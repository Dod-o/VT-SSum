{
    "id": "mc5ejmhwt6lqc46fbkay64qga2lmxllm",
    "title": "Combining Graphical Models and Deep Learning",
    "info": {
        "author": [
            "Matthew James Johnson, Google, Inc."
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_johnson_graphical_models/",
    "segmentation": [
        [
            "Thanks, you guys can hear me OK, OK so yeah I think the similarity is I think Max is talk.",
            "Also said something like composing graphical models with neural networks, which is the first 5 words in my title as well.",
            "Six so.",
            "However, this is like a poorly scoped title to some extent, because people have been using some graphical model ideas with deep learning for forever.",
            "So in fact I think that Joshua was probably doing work in the early 90s on things that we later called conditional random fields that were basically like using neural networks to predict potentials in a graph and then doing some kind of message passing.",
            "So really there's been a lot of work on broadly composing graphical models with neural networks.",
            "So what I'm going to tell you about, though, is some some of our work that is different from what's come before in a few ways.",
            "So one is that.",
            "I'm going to use more graphical model technology.",
            "Then maybe Max was telling you about yesterday, so graphical models actually refers to a lot of different ideas, but there are big textbooks written on these things, right?",
            "And long monographs so.",
            "You know, sometimes we see graph models and it's just like a visualization tool, right?",
            "Or you know a description of some simple relationships between the three random variables in R model or something and everything else.",
            "All the technologies in the in the neural networks and how it's pushing those probability masses around.",
            "But I'm going to talk to you about today is going to be about using a lot more graphical model stuff, in particular using a lot of like exponential family math to get really fast inference algorithms.",
            "That exploits a lot of the structure that we can put into models.",
            "The other part is going to you that this is a good idea that we should write down models this way and pair them with deep learning in a particular way.",
            "So I'm going to be talking about actually a line of work that directly builds on Dirking Liz and Max Wellings work on various autoencoders and the explosion of research that's come out of that.",
            "And the similar papers.",
            "So basically another title.",
            "Another version of this title, more descriptive would be like very slow encoders, plus probably graphical models in the latent space that we're going to see.",
            "So this is work that I did.",
            "In large part during my postdoc.",
            "And also that I'm continuing at Google Brain where I am now.",
            "So.",
            "I think one reason that a lot of the graphical models that we write down in deep learning are simple is that usually we have things that are just.",
            "I sort of are.",
            "We don't want to constrain our representations at all, and we have to make fairly simple predictions, like the inferences we have to make our not so complicated, but I want to tell you about problem that I think is 1, in which we do want to constrain our representation or particular way while still being flexible in other ways.",
            "So that starts with this guy."
        ],
        [
            "Here.",
            "Maybe not as cute as Nando's video, but this is a mouse on a lab table top in I said not as not as cute, so this is still kinda cute, though this is a mouse on a lab bench in.",
            "Harvard Medical School in the Neurobiology lab so they have colonies of hundreds of these mice and of course they study them for many reasons, but this neurobiology lab wants to learn things about how the mouse is brain works, and in particular this.",
            "This lab that I was a postdoc in wanted to sort of have developed new tools for analyzing this mouse is behavior because the brain ultimately gives rise to behavior and this is a huge undertaking.",
            "Understanding how the mouse responds to its environment and maybe learns.",
            "Those are all sort of like downstream things.",
            "We just want a way to sort of represent this guys behavior.",
            "So if you watch videos of this, so this is actually not interacting with very much.",
            "This is where we wanted to start.",
            "Just sort of like trying to escape these, exploring his surroundings.",
            "They like being kind of cooped up.",
            "So this mouse, if you watch its behavior for awhile, you might sort of naturally start to form your own description for what it's doing.",
            "So what I would do in what in fact ethologist have done for a long time is sort of naturally see these brief, stereotyped units of action, right?",
            "These sort of behavioral primitives so he'll sort of clean his nose Hill dark forward, he'll rear up in the air.",
            "Sometimes, depending on what we're doing to him, he'll like curl up into a ball and fear.",
            "Or you know, if you think there's a Fox nearby or something else, or press himself against the ground.",
            "So basically there are these little motifs of action, right?",
            "These stereotypes, motifs of action that we'd like to discover automatically from some kind of high bandwidth like video kind of data.",
            "So then we can study how that representation might change as a way to study how the mouse is.",
            "Behavior is changing."
        ],
        [
            "So in particular, you know I said that we can think of this mouse is behavior as decomposing into these brief, stereotyped actions like darts and rears and pauses if we want to discover these sorts of units, right?",
            "This is kind of like discovering syllabic units from sound.",
            "If I give you an audio recording of some language, maybe you don't speak listening to it.",
            "You might imagine that by picking out regularity's like picking out these stereotypes, phonetic units, you might be able to discover.",
            "Some kind of phoneme from the language, right?",
            "In fact, this is a problem that people have studied in sort of speech processing.",
            "Given recordings, try to discover these phonetic units.",
            "And so, in analogy to this, I'm going to refer to these behavioral primitives as behavioral syllables, right?",
            "So the content of language is actually not in the phoneme themselves so much as in their statistics, like how they are composed into sequences, right?",
            "So we have this alphabet of phonemes as it were, but the way we communicate the actual information content is really through quantizing these things and then seeing how they are made into sequences.",
            "And so we want to do something similar for mice we want to.",
            "Discover what these syllables are in an unsupervised way from video.",
            "And then we want to use that to study how these statistics of these things change.",
            "Maybe how the unigrams or bigrams statistics change as we for example, do some optogenetics and kind of zapped the mouse in the brain with the laser to make it move?",
            "Or think something we might change its genetic code or expose it to some kind of Fox odors?",
            "Or give it some pharmaceutical drugs right?",
            "So drug companies are really interested in high throughput behavioral phenotyping because they want to be able to do screening and pre clinical trials.",
            "They want to do some explorations on what drug compounds they should use to change human behavior or treat medical conditions so they have mouse models for things like autism, and they would like a way to represent the complex set of behaviors that we described as this being a mouse model for autism and then understand how it changes when we give it a drug that reverts to baseline.",
            "This sort of thing.",
            "So."
        ],
        [
            "I hear this mouse again.",
            "Here's our friend.",
            "We have to get him into computer somehow and So what we did is we used connect."
        ],
        [
            "Depth cameras, so this is a connect two video shot from above of the mouse in sort of larger enclosure, but there's sort of nothing interesting going on in the enclosure, so it's basically like freely behaving, so this is the circular enclosure we can remove these sorts of specularities and in fact we tracked the mouse in 2D to produce a sort of aligned video sequence where the mouse is always facing to the right.",
            "So this is sort of the egocentric view of the mouse right?",
            "Sort of the body aligned view of the mouse, and so when I say we want to model.",
            "Mouse behavior, model, videos and mouse behavior.",
            "What I mean is we want to model video sequences that are like this sequences of frames like this box 1.",
            "So how should we think about this problem?",
            "Well, just think about."
        ],
        [
            "The data you know one way to think about it is to refer to this object.",
            "We often think about this image manifold right?",
            "Somehow these images might be very high dimensional.",
            "Nominally.",
            "There you know 80 by 80 or something.",
            "So maybe 6400 dimensions, very high dimensional images, but we don't see every possible configuration of those images right?",
            "Every possible pixel pattern we see a very small subset, the ones that look like mice, right?",
            "The background sort of always dark.",
            "And then there's some well shaped.",
            "Blob in the middle and in fact we know that the mouse because we're recording it in time and the mouse moves smoothly through time.",
            "We should believe that you know, maybe we can think of these images is lying on or near some low dimensional image manifold and part of the modeling task is to discover this.",
            "So that is to say that every frame of this depth video.",
            "Is sitting on or near some low dimensional manifold.",
            "We might like to discover, but we're not done.",
            "We don't just want to find the image manifold of mice, we want to describe videos.",
            "So a video is a trajectory along this manifold.",
            "In fact, we're not even done then.",
            "We don't just want to discard the manifold and have some dynamical system.",
            "I actually said that we wanted to have a very structured dynamical system, right?",
            "We wanted to say learn this manifold not just as a set, but in coordinates, where we can describe the dynamics using relatively simple dynamical systems, like.",
            "Perhaps there's a simple dynamical pattern, as it's in one behavior like a dart and another simple dynamical mode when it goes into a rearing action.",
            "Yes, question.",
            "Facing.",
            "Yeah, the question was, is there a reason you keep the mouse facing right?",
            "There's a scientific reason which is that they were interested in egocentric behavior modeling, meaning they wanted the mouse.",
            "You know this is how the mouse sees itself.",
            "We see ourselves, you know from one preferred coordinate frame.",
            "And so that's sort of what we wanted to study.",
            "Was egocentric behavior you could, I mean this work is really like a starting point of trying to build models for behaving mice.",
            "There's all kinds of other stuff you want to do.",
            "So for example, you shouldn't forget about the position because the position could be meaningful if it's fleeing from something or interacting with something right?",
            "Of course, that's meaningful.",
            "You might want to do this all end to end.",
            "Don't have any tracking right, just like to be able to have an attention on the mouse and like decide the alignment.",
            "There's all kinds of stuff to do, but this is just sort of the first problem that we studied.",
            "So.",
            "The big picture here is we have some ingredients we want to do something like learning manifolds of images right?",
            "Which sounds familiar sounds like hey, deep learning gives us a lot of tools for that.",
            "Explicitly parameterising manifold images.",
            "But then we also want to have something where we're simultaneously saying actually I want to learn these manifold coordinates so that I can organize the information organized representation in a particular way.",
            "So."
        ],
        [
            "What kind of tools can we look for here?",
            "So obviously we can look for recurrent neural networks, right?",
            "So referral networks if you want to predict video and your ultimate performance is going to be measured on how well you predict frames of video, right?",
            "Recurrent neural Nets there's many different variations, but these are sort of the best technologies we have.",
            "I think to predict video.",
            "So if we had a sort of prediction problem where we needed to estimate the density of video or draw a new samples of video, you know go deep learning all the way.",
            "However, that's not the problem that the scientists have, so the scientists don't want to build a machine that can generate new mouse videos because they already have that machine.",
            "It's undergrads, undergrads connect, and a mouse.",
            "You'll get as many mouse videos as you want.",
            "They have the mouse videos.",
            "What they don't have is, you know, they want to understand the data and.",
            "To some extent, like when we fit something like on our net and RNN, and certainly this work on making these things interpretable, but you fit in our end you got, you got?",
            "You got like hundreds of megabytes of parameters and it's like all these weights and bias bicis these matrices?",
            "What have you learned about your data?",
            "Your network is learn about your data 'cause it can predict it well.",
            "It's learned the way in which mouse behaviors multimodal and stuff, right?",
            "But you haven't learned that you would have to sort of take those parameters and unpack them somehow and try to like inspect them.",
            "To try to figure out what the neural net has learned in its prediction rule, how to like how to understand behavior.",
            "But if we want to understand that neural Nets might not be vanilla, neural networks might not be exactly the right tool, because the representation internally of how it's forming these predictions, too unconstrained.",
            "So another set of tools we can think about that are really specifically about constraining probability distributions right constraining representations.",
            "Those are probabilistic graphical models, and indeed in this talk, when I say probably graphical models, so probably graphical models formally just talk about independent relationships or factorization relationships in densities.",
            "When I say probably graphical models in this talk, what I mean is actually more restricted models.",
            "Things that are built out of sort of tractable exponential family structure.",
            "And I can say more technical details about that a bit later, but sort of.",
            "I mean, you know building things out of simple components that we have a lot of understanding about what's going on because it is simple problem with probably graphical models is there are two restricted right there, too hard to fit video data with.",
            "So in fact, to set up a bit of how I think about this contrast, sort of exponential family problems, graphical model type tools versus sort of generic deep learning tools, I want to show you the most stylized version of a similar problem.",
            "Write a similar sort."
        ],
        [
            "Representation learning problem.",
            "So here I've gotten rid of the mouse and the video like the time series and I'm just saying here are some data points in 2D.",
            "Learn a representation of it that you can then show a scientist who's interested in these data.",
            "You know what do you?",
            "What do you see here?",
            "Clusters, right?",
            "Obviously we want to find clusters."
        ],
        [
            "If you fit this with just a density model, that wouldn't be very interesting, but you want to discover, like hey, these clusters in the data, maybe that scientifically meaningful so you could fit these data with a Gaussian mixture model.",
            "And if someone asked you if a scientist friend approached you and had data look like this, you would fit in with Gaussian mixture model.",
            "You would not say download Tensorflow an run a very autoencoder or something, but of course this data is really simple and this ability to fit with these simple parametric forms like a mixture of Gaussians breaks down really quickly.",
            "So in."
        ],
        [
            "Even just staying in 2D right in Datsun 2D, here's another set of points.",
            "I don't know about you, but I also see clusters that I would think are meaningful and I want to discover automatically.",
            "But if you try to fit Gaussian mixture model."
        ],
        [
            "Depending on how you set the hyperparameters, because the model is misspecified, right?",
            "Because the cluster components don't look like the cluster shape I've prescribed to them, by saying a mixture of Gaussians, it'll just have to fit the density with these blobs, and this is bad because not only have we not discovered the clustering structure in the data, these cluster labels now are not very meaningful, but also this is not a very good density model, right?",
            "It's just going to try to dial things.",
            "You could use Gaussian mixture models as density models.",
            "In some cases you do, but.",
            "Usually you just have to deal with so many Gaussians and discrete variables now, not a good idea.",
            "So another thing you could do to these data, this like."
        ],
        [
            "Warped mixture model data is you could fit a density network of some kind, so in particular you could fit a variational autoencoder right the generative part of various auto coder says take some Gaussian probability, some Gaussian random variable and then push it forward through some complicated mapping that you're going to fit to make you know.",
            "Basically any complicated density you like additive Gaussian noise at the end.",
            "All right, this is a super flexible model 'cause a neural network can push around probability mass in a really complicated way.",
            "Certainly can take a unimodal Gaussian and push it to be multimodal.",
            "So in fact, that's what I did here.",
            "And like other than some maybe should train it a bit longer.",
            "It's learned the density of the data, and so if your problem were to sample new samples that looked like your datasets because it's a bedroom or a mouse video or something, this would be great.",
            "You just sample, you get something from the purple density mass and it would look like your data would be happy.",
            "But now again, we don't see the clustering structure right.",
            "The information that there are five clusters here that's buried somewhere in the weights and biases of your network, right?",
            "If I give you a very solid encoder and say how many modes has it learned we like, I don't know.",
            "Maybe you could be clever and come up with some way of trying to slice open its guts and say like, oh, you know, here's the.",
            "Entrails are very favorable for us, but I think in general it's quite hard to know what your neural net is.",
            "Learn about your data.",
            "So we really want is some kind of high."
        ],
        [
            "It approach right?",
            "So for this cartoon problem, the hybrid approach would say learn the structure of the data in this case meaning discrete clusters of data points.",
            "But also we don't want to have this specification issue where when we tried to fit mixtures of Gaussians that meant that we weren't solving anything.",
            "In fact we want to have the flexibility maybe of neural network models to model complicated cluster shapes, for example but.",
            "We simultaneously want to be able to learn in our latent representation some organized structure.",
            "So."
        ],
        [
            "Oh one high level way of thinking about this is that the way we've been very successful in supervised learning is by combining deep learning with very simple linear classifiers.",
            "So here's what I mean.",
            "One way of thinking about your deep network is that it's doing.",
            "It's got a feature mapping right that we're learning that's taking your data in your data space.",
            "Your decision boundaries could be super complicated, but we're going to learn features.",
            "We're going to learn a way to transform that data space.",
            "Maybe will blow up the dimension.",
            "Will process it until at the top layer we're going to demand the things be linearly separable right at the top layer.",
            "You just do linear logistic regression, so we've solved before you know.",
            "Really tough nonlinear problems by using deep learning to learn how to map it to an easy problem.",
            "And we simultaneously learn our linear separation and our features.",
            "Because we're simultaneously learning these things right, we're learning how to make the problem easy.",
            "We're learning features in which our simple model works well.",
            "So the analogy here maybe for these problems that have a more unsupervised character is to say, let's take our complicated problems and learn nonlinear feature transformations, in which simpler, more organized structures fit well, right?",
            "So this is like, let's learn features to organize our space so that now things over here when they're straightened out now they fit our simpler priors."
        ],
        [
            "So.",
            "Just to draw a high level summary on all these things, probably graphical models.",
            "And remember by that I mean.",
            "Things built out of tractable, exponential family structures things like Gaussian mixtures of Gaussians, linear dynamical systems, hidden Markov models with finite numbers of states.",
            "They're good for some things.",
            "This thing that I'm calling structured representations where I am manually prescribing I want this part of the representation to be organized a specific way.",
            "They're good at thinking about priors and uncertainty.",
            "I know how to specify a prior on a linear dynamical system so that I can say it's, you know, we believe that there is this some frequency content.",
            "There are these time scales that are present in my process and not these others.",
            "I know right those priors so PGM's are good tools for that.",
            "The big problem is that the rigid assumptions that we bake into these strong priors may not fit our data well, right?",
            "It's like how the Gaussian mixture model being misspecified and then not sure if all of you around for this, but like you know, feature engineering used to be very common place in machine learning and this is like the worst part of machine learning.",
            "Maybe we still do it sometimes, but we much prefer to instead of manually engineers, we prefer to somehow learn the men tend.",
            "So those are sort of modeling reasons.",
            "PDF's are good.",
            "Here's some inference reasons that will say more about later, probably graphical models.",
            "Let us answer arbitrary inference queries, so if you have missing data or something, PM's are basically exactly the tool for how to think about stitching together partial information.",
            "Also, if your prior is a good one, then you can get data efficiency right?",
            "If your prior is a good one then you don't have to learn from scratch quite as much as when you have a sort of non informative.",
            "Network prior.",
            "Also they can give us computational efficiency if we stick to these rigid model classes, but the downside of course is that these things, because they lack flexibility.",
            "If we try to make them more flexible, we end up breaking all the nice fast inference that we have.",
            "OK, so those are things PDM's are good at in terms of modeling and then in terms of inference, deep learning is also quite good at some things.",
            "Let me tell you some things.",
            "I think that standard deep learning techniques are not so good at one as opposed to sort of getting the structure representations out so that we can understand.",
            "I think more often we just get neural net do right.",
            "We get hundreds of megabytes of weights and biases and then the problem begins to try to decipher what it's learned.",
            "And also I think the parameterisation is quite difficult to put priors over.",
            "It's not clear to me what a Gaussian prior on.",
            "Neural net weights is supposed to mean or if that's like a reasonable inductive bias, certainly I don't know how to do things like put timescale priors on RNN, so I'm sure there's been some work on it.",
            "I don't think it's a solved problem, but of course we love deep learning, right?",
            "The reason why we're in deep learning Summer school is because these things are so flexible in such high capacity, and this character feature learning from our data.",
            "Deep learning also sort of has more unlimited inference queries.",
            "I'll say a bit more about this later, but when you write down an RN or something Wavenet or something if you want to be able to infer missing, you know, given partial observations in some arbitrary pattern.",
            "Oftentimes you have to redesign your neural net and then retrain it for every inference pattern the demand, whereas graphical models sort of give you exponentially many combinatorially many inference programs out of your one model specification.",
            "Of course they can also be data and compute hungry.",
            "But one really exciting thing about inference that for me, I first learned from reading Dierking was in maximum paper on virtual autoencoders.",
            "Is this idea of recognition networks that learn how to do inference for you?",
            "Much more about that later, but that's super exciting for approximate inference, so stepping back and looking at this table, I think it's clear that I set this up to say that I think these tools from exponential family problems, graphical models, and these sort of deep learning tools these ideas are actually quite complementary, and in fact, the extent to which we can put these together is like chocolate and peanut butter, right?",
            "We should be really excited for the problems where we're not just solving a prediction problem, but we need to make a complex.",
            "More complex inference in our prediction, or we demand a more structured representation to our machine that's making the prediction.",
            "So I want to put these two together in emphasizing these sort of like sailing sailing points.",
            "So.",
            "That was all motivation.",
            "Now let me tell you the rest of what I'm going to talk about.",
            "First, I'm going to tell you a modeling idea, which is kind of an obvious one.",
            "Basically, take graphical models, you put graphical models at these nicely structured exponential family graphical models.",
            "Put them on the latent variables inside something like a variational auto encoder and then have neural networks for observation models to connect them to data and this will have this nice balance of sort of organized latent space.",
            "Kind of like the Platonic realm of organized objects, and then the flexibility to describe complex data that don't fit those hypothesis.",
            "That's."
        ],
        [
            "Calling idea then the main content of this talk.",
            "Certainly the main technical contribution of our work is how to do inference in these models.",
            "So in fact it's not hard to write down a model class like the one I described.",
            "Tricky thing is doing effective inference, and in particular we want to do inference that uses all of the best.",
            "Recent approximate inference technology from deep learning.",
            "Deep learning like tools.",
            "But we also because we built all the structure into our latent variable model.",
            "We want to be able to use that structure an have really fast inference algorithms as a result.",
            "So the inference part of this talk, which is really the technical part, really comes down to having recognition networks and pairing them with fast graphical model.",
            "Fast exponential, family graph model algorithms.",
            "And then finally I'll say a bit and show a few pictures about.",
            "This application of learning this sort of syllabic representation of mouse behavior from video."
        ],
        [
            "OK.",
            "So first modeling idea.",
            "So."
        ],
        [
            "Already described it a bit, but let me say a bit more detail.",
            "Specifically in this this one application, by the way, it's meaningful.",
            "Our application is a science.",
            "One.",
            "Write like scientists want to learn about the world and about the data.",
            "They don't just want to have a machine that forms predictions and then they ask no questions about how it's coming to those predictions.",
            "So this scientific problem was a great sort of way to spur thinking in these terms.",
            "So if you remember, we want to discover from video this like syllabic representation.",
            "So how do we write a latent variable model again using very restricted exponential family type tools?",
            "To describe generating these data, we want to come up with a generative story.",
            "So the first step is, let's think of having a finite state hidden Markov model, right?",
            "This is going to model, so think of time is going to write.",
            "This is going to model switching between different behavioral modes.",
            "So while the mouse sort of is in this one state, this blue states he's going to be darting and then it will switch to another behavior.",
            "This green thing.",
            "So these discrete mode sequence is going to represent these are the latent variables in coding are our structure or prior.",
            "To say we want to have this switching dynamic switching between these different syllables.",
            "So of course we'll write down a prior on transition matrices right and then hidden state Markov model for how those states unroll.",
            "The next step is what do we want to each states to be described as so in each state we want it to be a dynamical pattern, right?",
            "Like when the mouse is running we want to correspond to something happening to those manifold coordinates, right?",
            "Something happening to the mouse puppet strings in the image model.",
            "So when it's running there will be some dynamical system which will just write as this linear dynamical system.",
            "So we have written this.",
            "I mean.",
            "So think of this process here.",
            "As discrete time sampling of you know a low dimensional linear dynamical system.",
            "So at each time T there's some latent variable X which describes the mouse is configuration in the in the in the coordinates, the low dimensional coordinates, and while we're in state, blue in the blue state, we're evolving according to one set of these linear dynamical parameters, and then we switch into another state, will have another set of linear dynamical parameters.",
            "You see the parameters are indexed by disease, so we have a prior over generating.",
            "These matrices, one for each state.",
            "And those will describe the dynamics.",
            "This is like, you know, an RN, except everything is linear and we're injecting Gaussian noise.",
            "We can do all kinds of interesting things if we want to interpret these matrices after we fit them in terms of the time scale information, we just look at the spectral content of these a matrices from the priors that have to do time scales.",
            "We can do that nicely as well.",
            "So let me just clean this up and write it."
        ],
        [
            "Is a actual graphical model, so I'm saying that there's this discrete states that plugs into a continuous state that drives its dynamics.",
            "In"
        ],
        [
            "We just collect all the parameters into a single symbol Theta and write the graphical model like this.",
            "So then the last thing to do is have some way of producing images right?",
            "So every low dimensional code if you will.",
            "We want to be able to generate a.",
            "Mouse image from it.",
            "So this is where we maybe like to use some deep learning technology.",
            "So in particular I will clear off the bottom of the screen to make."
        ],
        [
            "Room"
        ],
        [
            "Also draw this as a graph and the basic idea is to use the."
        ],
        [
            "Is decoder type models that I'm sure you've heard of.",
            "In particular, this is like a density network that would fit from a very autoencoder.",
            "This just says let's write our image are high dimensional vector like our pixels, maybe 6400 mental image condition on the latent code and some neural net parameters gamma.",
            "We just want to say that comes from a Gaussian distribution where the mean is some rich nonlinear function of the latent code X and maybe so is the covariance.",
            "Right, so if you want an image manifold, mu is kind of like the mapping from our coordinates to the points on the image manifold.",
            "Question, yeah.",
            "How do you find the boundaries?",
            "Yeah, we're going to learn that from scratch, so we're just writing a prior that says there is some transition matrix from some prior.",
            "It could even be a Bayesian nonparametric prior.",
            "You can do all your hierarchical Dirichlet processes as I did Once Upon a time, so you can have an unbounded number of States and then you just say they follow that transition matrix and sample.",
            "Sample states from it, so we're going to learn that segmentation automatically in an unsupervised way.",
            "If you want to be semi supervised like you want to have some hand labels which does not work well for this problem, by the way, like people have tried to make hand labeled datasets, but they can only train humans to recognize a few different classes, like a few different behaviors because the mice moved kind of fast and humans aren't very high throughput machines.",
            "Also they have very low agreement if you ask two different human labelers that you train the same way you ask them to label new videos.",
            "Very low agreement.",
            "So you could be semi supervised here if you want to.",
            "But we're going to talk about totally unsupervised mode.",
            "Also, one other thing that I've started to think more and more is important.",
            "These states were just were generated in an open loop.",
            "They didn't depend on what the mouses configuration was or any external environment variables.",
            "But with graphical models, in principle at least it might break some of our inference, but we can draw edges from X1 to Z2 and X2 to Z3 to say oh, the mouse might switch its behavior based on what config configuration its body is in, or if we want to have covariates that we list upstairs is kind of like inputs plugging into our transition matrices or into the dynamics, we could do all of that.",
            "So this is actually a relatively simple model that I think would be interesting to embellish.",
            "Other questions.",
            "Yeah yeah, yeah.",
            "Why have to be random number 2 exquisite lattice domestic from Mexico?",
            "So the question is why is why random?",
            "I guess there are a few different answers.",
            "It's sort of the usual answer for why do we model high dimensional things as noisy versions of low dimensional things?",
            "It's because we don't expect the high dimensional objects to lie exactly on a manifold.",
            "We're sort of saying that to do dimensionality reduction we say they live near it.",
            "I'm sure that you know because of noise in the sensors or the fact that our model is wrong.",
            "We have to sort of say that some things are unexplained in our model, and the way we do that is we just say that it's noise that we don't care bout modeling further.",
            "So some of it is just sort of like having slop in our model noise term.",
            "Another way to think about it is that we want this thing to define a density over all possible images, because we want to be able to get any image and maybe ask questions about what the probability is under that.",
            "And we don't want the answer to almost always be 0.",
            "So you know whenever we want to have a density on output space, we usually have an explicit model for some low dimensional part and then some some way to make that have.",
            "To be absolutely continuous with respect to the big measure on the output space.",
            "So just the usual reasons.",
            "So last thing on this I'm going to draw a box because this is a model for one video sequence, but in fact we have many video sequences.",
            "We had like 3 million or 20,000,000 frames or something.",
            "I don't even remember.",
            "We also want to get this thing at some scale.",
            "So this is an example of for this particular application, by the way, I would call this a switching linear dynamical system.",
            "My favorite graphical model.",
            "It includes a special cases many things that you might like, like PCA, isn't there somewhere.",
            "So I'm going to draw a box to represent the repeated.",
            "The fact that you know there are many video sequences that were fitting.",
            "So."
        ],
        [
            "So that was here's a less colorful switching linear dynamical system model with nonlinear observations.",
            "I want to abstract abit.",
            "This is 1 example and to make the notation simple I'm going to stop tracking some details even though they're still there under the hood.",
            "So in particular let's collapse out time.",
            "So we're just thinking of the sequences themselves.",
            "This is just for rotational purposes and then also let's collapse all of our latent variables into one.",
            "So when I draw this notation, I mean that this structure is still there, but we're not interested in tracking it right now, but there's all kinds of interesting structure going on in our latent variable model, so the point of doing this is to really sort of generalize abstract things.",
            "Draw model like this one that I want to think of in four parts.",
            "So in the sort of top half, we're going to have the nice latent variable models an what nice means to me is.",
            "We're going to have some exponential family on local variables.",
            "That is nice in some way that I can make precise later, but I'll just say that it means it's built out of tractable exponential families.",
            "And then we're going to have a conjugate prior on the global variables.",
            "These are sort of the parameters for our mouse dynamics.",
            "And then these are the actual mouse configurations for a particular video, the local invariables.",
            "So that's the top half and then the bottom half.",
            "The top half is like the very idealized simple modeling land latent variables and the bottom half is how we connected up to complicated data.",
            "So in particular, we want to think of having a neural network observation model, and we could have a prior on the observation parameters.",
            "But really I think we're going to just do sort of variational EM and fix those two points and just maximize them.",
            "Any questions on this?",
            "OK, so this pattern, yeah.",
            "Why are you looking right?",
            "Are you keeping track of procedure?",
            "I am I'm just I'm just hiding them from the notation.",
            "Because for this example we want to have these, but actually the next slide is to say that this scheme where we just have some latent variables.",
            "There could be all kinds of stuff going on, like having discrete chains and continuous chains.",
            "I want to abstract and say the things that I'm going to tell you about."
        ],
        [
            "Applied to a lot more than just this example, so in fact here a bunch of graphical models.",
            "Each one is sort of interesting, and some of them have inputs, right?",
            "Some of them are like mixtures of experts or complicated driven dynamical systems.",
            "Or you know, hmm, that I think Joshua worked on originally, or like you know, some kind of LDA.",
            "Basically there used to be at these machine learning conferences.",
            "They were filled with like people developing specific structured models and then inference algorithms that would exploit all of that structure in the model.",
            "And these things would include interesting like hypothesis about the structure of your data, and correspondingly it would be very very fast.",
            "Algorithms for doing inference.",
            "So I claim that there are a lot of graphical models sitting there that are sort of untapped when we do a lot of deep learning research.",
            "But the framework I'm going to describe to you is going to be able to use all of these, right?",
            "So any of these things you can plug them in, not only the model which is interesting, but more importantly, the inference.",
            "So the inference algorithms that you've seen before for anything like LDA, we can use all of those cool inference techniques.",
            "So."
        ],
        [
            "Inference, that's really the big question.",
            "Like I said, anybody can write down a complicated model.",
            "The question is, can you do effective inference in it?",
            "And that's really the main story here.",
            "So to tell you about how we're going to exploit structure and inference, I have to give you some idea.",
            "First of what it means to exploit structure.",
            "Now that we have these constrained parts to our model, what are we going to do with them?",
            "So to explain that."
        ],
        [
            "In a very so many pages of textbooks written on these things, I'm just going to give you high level view.",
            "In particular, I'm going to tell you about natural gradient SPI, which was work done by Matt Hoffman who's now with me at Google Brain and followed up on a lot of subsequent works.",
            "So this is sort of when we have all this exponential family and maybe conjugacy structure.",
            "What can we do with it if we're trying to do SVD style large scale training?"
        ],
        [
            "So let me tell you about a model.",
            "Remember our models are going to have nonlinear neural network observation models and complexities.",
            "But to tell you about how to exploit the structure, let's first investigate what we can do when we have this kind of model structure, and then we'll break it and see how much we can recover.",
            "So here's our example model.",
            "I don't even have switching here to make it really simple, so these taxes form a linear Gaussian linear dynamical system, right?",
            "So each X is marginally Gaussian variable an we're just evolving these things according to linear Gaussian dynamics.",
            "So apply linear matrix, add Gaussian noise.",
            "Simple dynamical system model.",
            "More importantly, the observations were going to be linear in Gaussian, so sort of everything downstairs.",
            "Everything in these chains is linear and Gaussian, and so we should expect to be able to do a lot of stuff.",
            "And then I haven't spelled out the details, but the prior on Theta is going to be a conjugate prior, which is going to be sort of more more nice stuff that we can use so.",
            "How do we do inference in this kind of model?",
            "So when I say inference, I mean including inference over Theta.",
            "So Bayesian inference kind of like learning fitting the parameters were going to be fitting some kind of a representation of a posterior not only over these latent states, but also over the parameter.",
            "So what do we do?",
            "I'm going to talk about variational inference, in particular structured variational inference and.",
            "Who here is familiar with structured variational inference?",
            "Everybody cool.",
            "OK, so there's a lot more that can be said about this that I'm not going to go into.",
            "This is going to be like a high level view.",
            "So anyone read Kevin Murphy's book?",
            "I have Kevin Murphy's book.",
            "Yeah, so Kevin Murphy's book is amazing.",
            "Also, I think the best references this monograph by Wainwright and Jordan in 2008.",
            "I've been praying on that since I think I started grad school.",
            "They handed me that book and I've been like reading it over and over ever since.",
            "So there's a lot of interesting like convex analysis and stuff that I'm not going to go into, but I will tell you the punch lines about how we can exploit this structure.",
            "So we do variational inference.",
            "The idea is that we want to approximate this posterior over our parameter and our local latent variables X, and we're going to approximate it by factorizing.",
            "So this is somehow difficult to deal with, but we can break it down into pieces and represent variational distribution, Theta and then independent variational distribution on our local latent variables.",
            "And we're going to fit that thing to approximate the posterior.",
            "OK.",
            "So as I'm sure you've seen it at various points to make to make our variational approximation try to approximate the posterior, we're going to maximize this objective, which corresponds to minimizing the KL divergent from our approximation to the posterior, because this thing plus the KL divergences are constant.",
            "So actually, just by writing these little parentheses here as opposed to brackets, I've done something.",
            "Interesting, so it turns out that just by saying that I'm going to factorize my variational posterior this way, it's already a fact that now we can choose these variational factors to look like the corresponding factors in the prior.",
            "That's interesting because I'm saying the optimal form of these densities over all possible densities like satisfying the earlier LaGrange equations, is to choose these things to look like the prior.",
            "OK, that was just the technical side, so we have this objective there right down, and we're going to parameterise these variational factors.",
            "So look something like our prior and I'm going to write their natural parameters as Ada sub Theta.",
            "That's the parameter for Q Theta and aid us of X.",
            "That's the parameter for QX.",
            "OK, so to do inference to to fit our model, we want to optimize this objective, which is just some number with an expectation that this is just some number over here.",
            "Want to search over these parameters, right?",
            "OK, now we're in the land of optimization and we know what we can do right?",
            "Oh, you see an expectation list.",
            "Monte Carlo, it don't worry.",
            "And then do gradient descent over these parameters, right?",
            "So that's the naive thing that we could do without exploiting any structure.",
            "Turns out we can do a lot more in this setting.",
            "So turns out we can actually optimize out.",
            "One of our variational parameters completely so that is to say so.",
            "Remember, 80 X was a variational parameter for all of our latent variables.",
            "There could be a lot of local latent variables.",
            "Sorry, our local in variables X.",
            "We're going to optimize that out completely and just write a single argument function where we substitute in that optimizing value.",
            "OK, so you see now, I claim that we can do this optimization efficiently, and in fact we can get access to this function in a way we can get its derivatives.",
            "So we've optimized out.",
            "Probably the much larger variational parameter completely.",
            "We don't even have to search over it using local search methods like like gradient descent.",
            "Well, at least not in the outer loop.",
            "So this can be done super efficiently.",
            "Now we have an objective that's much smaller dimensionality.",
            "Now we can do gradient descent on this thing, right?",
            "It turns out you can still do something better.",
            "Actually, one more point is that we can do this optimization.",
            "Evaluating these expectations like the expectation over QX, exactly so we don't even have to Monte Carlo over X to perform this optimization.",
            "That's also super cool.",
            "It's an integral that we can actually do without just Monte Carlo and everything.",
            "In fact, we can evaluate this thing doing exactly over over Theta as well.",
            "So we have this objective now.",
            "We've optimized out some of the parameters perfectly.",
            "Can you do more?",
            "Indeed we can.",
            "It turns out that you can write the natural gradient of this objective.",
            "Computing all the integrals exactly on optimizing out your local latent variable factor, we can write that natural gradient as a very simple expression.",
            "OK, so the details here are not super important with some notation to unpack.",
            "Here's the big idea.",
            "We could have done flat gradient descent on this objective that would have already been a big win because we're doing integrals exactly, and because we are optimizing out our local variational factors, we can do even better.",
            "We can compute natural gradients, which are kind of sort of curvature corrected.",
            "2nd order optimization.",
            "Technique so we can compute these.",
            "Another crazy thing is we can actually compute these faster than you can compute the flat gradient, so there's no backwards pass.",
            "This expression is only in terms of things that we compute in the forward pass to evaluate the objective.",
            "Wow, that's super cool right?",
            "We used this very special structure in this very like crystalline restrictive structure, but we got a lot of mileage out of it in terms of an inference algorithm, right?",
            "One last point is that when we have many sequences, we can just Monte Carlo sample.",
            "We write this sum and then we can Monte Carlo sampling in the usual way to scale this fitting to big datasets.",
            "So what does that algorithm look like?"
        ],
        [
            "Let me just show you what it looks like for these time series models.",
            "Step one is we sample data mini batch, right?",
            "Maybe it's just one sequence of the sequence for modeling.",
            "We then compute evidence potentials using our nice linear Gaussian observation model.",
            "We can compute evidence potentials on our local invariables no problem.",
            "Then we use those potentials that we see that we use those potentials and do fast."
        ],
        [
            "Passing algorithms, so this is like a Kalman smoother details aren't important, but like your cell phones doing it all the time, these are super fast and it's the kind of thing that you're willing to support in inside your inference your learning algorithm.",
            "So it's 2 fast message passing to sort of aggregate the local information into a globally coherent inference over what the latent states were, and then step three is almost there is no step three.",
            "It turns out that using this information we already computed, we can.",
            "We can compute a simple expression for natural gradient on the variational.",
            "The variational parameters for this very often organize so natural gradient SPI in these very nice models, where you had all kinds of specific restricted structure.",
            "Tons of algorithmic tools that we got out of it."
        ],
        [
            "One other and maybe even the most important idea is.",
            "Doing this kind of inference, we can support arbitrary inference queries, so if you give me any missing data pattern right because I have a template for how to do inference which is through message passing, right?",
            "You give me any pattern of observations or non observations and I can do inferences over what was going on.",
            "When we look at things like inference networks right?",
            "In fact, those things essentially only support a limited number of inference queries.",
            "So if you have an RNN for example, maybe it supports linearly many inference queries.",
            "You do inference over the sequence, but if I give you an arbitrary missing data pattern, you might have to train a new inference network to handle that.",
            "So."
        ],
        [
            "That was what we can do in the nice case, but it really relied on all kinds of specific special model structure.",
            "And of course as soon as we break that structure a little bit, things are going horribly wrong.",
            "So let's see what goes wrong.",
            "Let's imagine having again the same prior on latent variables, so it's still nice and structured upstairs, but then we have this neural network decoder model.",
            "So maybe we're modeling a video using latent linear dynamical system, but then nonlinear neural network observation model.",
            "So let's try to do what we did before setup of factorized posterior.",
            "In this case we don't have, we know the optimal forms of these things to parameterise, but we're just going to say anyway, let's make you avexa.",
            "Gaussian seems reasonable and then we can write down the objective we did before.",
            "And the problem now is that we have this likelihood term from our neural network decoder model an it does not play nicely with the other terms.",
            "So because of the linear Gaussian observations in the other case, we could actually take this term which I've drawn in red here and fold it back into the prior.",
            "So somehow, by conditioning on this special kind of evidence, we could just put it back in the prior.",
            "Nothing became complicated and we stayed in our nice structured prior.",
            "We can do all kinds of inference tricks, But here now we've got these nonlinear observations.",
            "They've totally changed all of our assumptions about what this prior looks like.",
            "The prior was nice.",
            "We condition on complicated evidence.",
            "Now we have a complicated posterior.",
            "So in particular, if we tried to write down what the optimal local factor is, right?",
            "We tried to do this argmax over R. Maxing over our local variational parameters afex, this is now something hard to do.",
            "So instead of having a sort of very fast inference, things like message passing to compute this.",
            "Max and get the information we need out of it.",
            "Now we'd be stuck with a sort of nonlinear optimization problem, right?",
            "If you want to fit Q of X like, choose cubex to maximize this objective.",
            "That's a pretty generic smooth nonlinear programming problem.",
            "First, you have to do Monte Carlo over Q of X to even evaluate the objective, and then you have to do a bunch of generic gradient descent or something.",
            "This is a huge problem because this thing to do inference over local in variables that would be in our inner loop.",
            "So if we try to form this SPI objective which were free to define this SPI objective now is very difficult to compute because when we sample the data mini batch instead of just having something very fast and then being able to compute gradient with respect to other parameters.",
            "Now when we sample data many data mini batch we have to do some expensive non linear programming problem in our inner loop, run it to convergence and then do an update.",
            "So this basically breaks the entire SVI picture.",
            "And so you know, as far as I know, we just sort of didn't try to do things like this, or at least I didn't try to do things like this."
        ],
        [
            "Until there were some really interesting advances, in particular from this pair of papers that came out at about the same time, by Kingman, Max Welling and also resented Mohammed and this amortized inference paper, there was a big idea for a way to still come up with a good factor on our local link variables, but efficiently so these are various autumn colors and amortized inference.",
            "This is a big idea for handling sort of nonlinear observations."
        ],
        [
            "So here's the sort of variational autoencoder model.",
            "The initial one.",
            "Just think of the of course have been developed in a lot of work since the original paper, but in this case, let's just think of a simple prior on our latent variables.",
            "The X ends just in IID Gaussian, very simple, and the entire complexity of the model is in the fact that we're pushing that forward to be a complicated density by using a neural network observation model.",
            "So what if we try to do something like stochastic variational inference too?",
            "Maximize over these gamma parameters.",
            "We have the same problem where we sampled data mini batch to do our local variable.",
            "Inference is we have to solve a generic non linear programming problem just to compute our outer loop update on our parameters.",
            "But the insight of the various autoencoder work was instead of trying to run some iterative optimization algorithm that could be super expensive in our inner loop, why don't we just parameterize some function to take in the data mini batch?",
            "We sample and spit out a variational factor.",
            "The parameters of a variational factor for us, right?",
            "This thing might be just a fixed depth circuit, right?",
            "It's a five layer neural network or something like this and it's just going to take in data and spit out variational parameters.",
            "And the insight was that we can train that thing.",
            "We can train the parameters of that function approximator in the outer loop.",
            "Right, so we can search over those things and by doing that we're going to be learning and inference.",
            "Network learning a network that does our inference for us, and so in particular.",
            "If we define, we just define our local inference is not as the optimal inference is solving this optimization problem, but just the output of some neural network with some parameters Phi.",
            "And then we say yes, and you know it.",
            "Sort of looks kind of like the other network did where we take in Hawaiian output variational parameters on our latent variables.",
            "Then we can define this variational autoencoder objective, which just looks like saying taking our original elbow and then plugging in.",
            "These aren't the optimal local parameters anymore, we're not sort of searching over them perfectly, but these are just the output of our recognition network.",
            "So this means that our local inference now is fast.",
            "It's OK to run in our inner loop again, right?",
            "Sample Data mini batch applying inference network?",
            "That's just a few.",
            "You know, Matt moves cons.",
            "This sort of thing.",
            "Then we have our local inferences so we can take another step on our parameter.",
            "That's good.",
            "No more inner loop optimization.",
            "These are super cool and David you know talked about these in and Lt. Like looking at my graphical model problems and then looking at the various autumn colors like that's amazing.",
            "Why don't I take this and put it in there?",
            "So."
        ],
        [
            "Just to say a bit about some of the other work that's come around on inference networks that are adapted to time series, so there are other things people have done that I won't go into detail about in this talk to combine sort of ideas from graph models with various automotive.",
            "So one line of work is from some folks at Columbia, like Evan Archer, and basically the idea was let's have a sort of CRF kind of recognition model so that instead of.",
            "Taking our entire sequence, if we have time series, let's apply recognition networks just to get local information and then sort of like stitch it together.",
            "This is quite similar to the framework that I'm this is, I'd say, an instance of the framework that I'm going to describe to you, though not fully, is general, so that's a very interesting line of work.",
            "There's other work by David Sontag and students on sort of using.",
            "Let's use the fact that we have time series and make a bidirectional RNN that kind of looks like message passing to do our non linear inferences.",
            "Super interesting work.",
            "You should definitely check that out."
        ],
        [
            "So.",
            "Let me just summarize the big picture.",
            "We had a natural gradient SPI for these really special exponential family models.",
            "Very restricted they can't model a whole lot, but when you had them you can exploit all that mathematical structure.",
            "So in particular these things became.",
            "This SVI became very expensive for general observation models, but it was nice while we had it because we got the optimal local factor.",
            "If we can perform the optimization efficiently, was able to exploit all of our graph structure in our project structure to do that optimization.",
            "I was able to support arbitrary missing data patterns, and of course we got natural gradients essentially for free.",
            "But then we talk about these various auto encoders, which were super awesome because they handle general observation models.",
            "And there's still quite fast, so that's a huge win.",
            "And that's why we use them all the time just to say a bit about what I think there's autoencoders can lack is, of course, when we have an inference network instead of solving optimization problem for inference, we're just getting the output of some network.",
            "Approximator that means that we are doing suboptimal local inference.",
            "We also have to learn our network has to do all of our inference for us right?",
            "And so it has to learn how to do message passing, right?",
            "It has to learn how to propagate information from one into the other.",
            "Latent variable models become more complex.",
            "That's going to be harder and harder.",
            "Maybe you believe that deep learning can solve everything and learn how to like approximate any function, even if it's got matrix inversion and all kinds of stuff that it should be doing.",
            "That may be true, but at the very least it will be data intensive right?",
            "At the very least, if you're having to learn how to do all your inference from scratch, it's going to take a lot of data to train that inference network, whereas if you're in the case where you want to write down model structure in your latent variables, maybe.",
            "You know it would be better to be able to use that instead of learning from scratch how to do all that information aggregation.",
            "Also, as I said, when you have function approximators, but then you're missing some of the inputs your function needs, you might not be able to answer some inference queries.",
            "And of course you know this small technical detail of hey we used to get natural gradients another slightly worse.",
            "So of course, as you might infer from me, setting up this plot, I'm going to tell you about something that we worked on building on both of these lines of work really.",
            "Just, you know, trying to put them together as best we can.",
            "We, we called this sort of model an inference framework, structured variational encoders structured in the sense of structured variational inference from graphical models.",
            "And so the idea of this chocolate and peanut butter set up is that we wanted still to be quite fast for general observation models, right?",
            "Like no one wants to model their data with linear observations, linear sensations from linear Gaussian dynamics.",
            "But we don't have general observation models, neural network observation models.",
            "We also wanted to say more about later we wanted to be able to sort of optimize as best we can.",
            "What are local variational inference objective was and make that fast by exploiting all of the config structure, reusing all those inference algorithms that people published at NIPS and Icml's past, and of course support arbitrary inference queries?",
            "So this is, you know what I claim we're trying to do, and I think we can do.",
            "Any questions on?",
            "On this.",
            "OK, so."
        ],
        [
            "How do we put these things together?",
            "These inference tools are developed for you, so I said it before.",
            "The basic idea is to have recognition networks, but instead of them doing all of our local variable inference for us.",
            "Just solve part of the problem right?",
            "Kind of like a CRF would have recognition.",
            "Networks output conjugate graphical model potentials and then apply all the fast graphical model inference algorithms that you can in your latent variable model.",
            "That's the idea so."
        ],
        [
            "Just to give you a bit of detail on how that works.",
            "Again, we have this sort of setup where you know we have a nice latent variable model and ice prior to this tough term, right?",
            "This observation likelihood term that we don't like very much.",
            "Hi, what should we do about it?",
            "Well, let's just replace it.",
            "Never mind, that's just shifted over a little bit.",
            "We should replace it.",
            "So here's this complicated term for every time in our time series.",
            "Maybe right, we have this evidence potential that's saying, like, oh, that's the image.",
            "I have a lot of complicated beliefs about what the mouses pose was given that image.",
            "Let's replace those difficult terms with a function approximator, right?",
            "So just take those difficult likely terms and replace them with function approximators and let's structure those function approximators so that they give us conjugate potentials for our prior."
        ],
        [
            "Basically, what that means is we're going to approximate complicated.",
            "We're going to learn to approximate complicated evidence potentials by a function that takes in our data and then spits out a Gaussian local inference of Gaussian evidence potential.",
            "Or in general, a conjugate evidence potential.",
            "So idea is complicated evidence, but kind of analogous to how invariant autoencoders we just spit out the inference on the link variables.",
            "And it's just some Gaussian.",
            "Here we're doing that, but just locally were saying I.",
            "If that was the frame of video from the mouse, I would guess that it poses this, and my guess is the form of a Gaussian.",
            "But then we're going to stitch together all those guesses and synthesize that information with our prior using graphical model technology enabled by the fact that this is a conjugate potential.",
            "Yeah.",
            "What's different in this verse is within your game.",
            "Before the period I thought I kind of thought this is what you're already doing, I guess.",
            "So it depends on what you're asking, but in vanilla, so in sort of standard variational auto encoder's.",
            "The I mean specifically the you have the slide like 2 slides going there was.",
            "And then there was using the, and now there's a structured VSO, yes.",
            "In this kind of yeah, that's what this is.",
            "This is just details about it, so this is the structured via setup, so there's just the FBI and the structure.",
            "Option here.",
            "So question is, is there some third option?",
            "So here are the two inputs, sort of to work.",
            "Sure this this.",
            "You want this slide?",
            "Yeah yeah.",
            "OK in the middle, let's just yeah, just via is right.",
            "So here's like natural gradient Sky that was not in deep learning land so much, but it was in like exponential family graph, model end and then various auto coders.",
            "Super cool with inference networks that do all of our local inference with deep learning and then structured VSR trying to do the chocolate peanut butter thing where we're going to put these two together.",
            "So this is our work and it's trying to put together very autumn colors and ideas from this world.",
            "Local.",
            "I mean, in some sense we're trying to do inference over these guys and these guys, so these are local local in the sense that there are extensive variables like for every mouse video it gets a new copy of all these variables and then there's like global variables which apply to all the mouse videos.",
            "Sure."
        ],
        [
            "So.",
            "The big idea is just approximating complicated evidence potentials.",
            "Learning to approximate them with conjugate nice ones.",
            "Then we can essentially set up a similar setup we had before where we can optimize out against our approximate objective and then proceed.",
            "So let me give you an idea of what this algorithm looks like."
        ],
        [
            "Let's see when do we end?",
            "I think it's been an hour alright.",
            "So we ended like we have 30 minutes left cool, so here's what the algorithm looks like.",
            "Step one.",
            "We sample Ridata mini batch.",
            "There it is now.",
            "Instead of using our observation models, this is where we apply our recognition networks.",
            "So we apply our recognition network to get node potentials.",
            "This is very much like a conditional random field.",
            "So then the next step, which is not so much like a conditional random field, is we're going to run our fast PGM inference algorithms like message passing.",
            "Sort of fast proximal operators if you will to.",
            "Optimize out all of the other things using this conjugate evidence as a surrogate, so we've made a circuit with this conjugate.",
            "It's not the real evidence, but we're still optimizing out against that evidence to sort of synthesize together all of those local information together with each other and through all of our latent variables.",
            "So this is the step that's quite different from fairy tale autoencoders in that instead of sort of feedforward computation, this is an optimization.",
            "So if you guys were here for my.",
            "Earlier talk where I talked about differentiating through optimization.",
            "This is what got me into doing that was I was doing optimization of, you know, to do various elements in graphical models and then I wanted to differentiate through it to fit the parameters of the of the inference network of the recognition at work.",
            "So step one apply recognition or to get potentials Step 2 fast.",
            "PGM inference algorithms Step 3.",
            "Is that we can sample and compute flat gradients.",
            "So I just mean gradients of the neural network parameters and those are not curvature corrected in any way and then Step 4 is we can use the information that we've already computed to get a cheap estimate of the natural gradient with respect to the upstairs nice parameters.",
            "Questions yeah, something different here than in the FBI.",
            "Yes, the difference with SPI.",
            "So in some ways this is so that's why I described to you only applied to when you had conjugate observation models, so you couldn't handle neural network observations.",
            "After you've done step one, yes, right in step one, we use recognition works, so step one was like the VE stuff.",
            "So take recognition works will be no and that's what we want.",
            "So actually it is so the question was, is it different after step one from SV?",
            "I know this step is exactly what you doing SPI.",
            "In SBI.",
            "You wouldn't have to sample these things.",
            "Because you don't have to do the re parameterisation trick, you can integrate out the you can compute the expectations you have in the objective exactly.",
            "This is a subtle technical point, but somehow the reason we have to sample the reason we have to Monte Carlo is because we can't compute the integrals that we need against the nonlinear observation models.",
            "So somehow in SVI we never sampled, we only sampled data, but we never sampled expectations over variational factors in various autoencoders.",
            "We always sample whenever we see an expectation for variational factors like.",
            "Replace that with a sample and we're let's roll.",
            "So this step three is different in that way.",
            "Cool, let me show you some fun videos of this working."
        ],
        [
            "First on a couple simple problems because I think they sort of illustrate the idea pretty well.",
            "So this is, uh, the data from that warp mixture model that I showed you before.",
            "Over here is the data space.",
            "This is a view of the warps data space, and here is a latent space, so I made it a 2 dimensional latent space, and I initialized the mapping to be the identity or close to the identity.",
            "That's why the data points look the same, but this is the view from the latent space.",
            "You can see all of our Gaussian mixture model clusters.",
            "This is the data space and you can see these densities here.",
            "So I was using.",
            "The prior on the latent space is something like learning a Gaussian mixture model.",
            "It's using a Dirichlet process.",
            "Prior on the weights of the mixture model, and so it's going to prune out.",
            "I gave it 30 or 50 or something components.",
            "I think maybe just 20 or 30 here to make it not so messy.",
            "But it's going to learn the number of components.",
            "That's kind of a cool trick.",
            "So over here we want to see it straighten out and start to look like data that can be fit with the Gaussian mixture model, right?",
            "We're trying to learn features so that our Gaussian mixture model prior fits well and then over here we want to see it modeling the data accurately and learning a sort of structure representation with clusters.",
            "So I'll hit play.",
            "It's super quickly learns to prune out clusters, so basically fitting the Gaussian mixture model is easy relative to fitting these neural networks, and so it's really fast at doing that.",
            "It turns out the clusters and start straighten them out quite quickly, and then it takes awhile to get the details right.",
            "I think if I kept running this would probably continue to refine things, but these greens are different greens, though that seems impossible to see on the projector.",
            "So this did what we wanted, right?",
            "It's not only learned the density of the data pretty well, that's pretty good.",
            "That's certainly something we had before.",
            "We fit and color to it, but more importantly, it's learned this clustering structure.",
            "It's learned a representation that finds these clusters, and then even more interesting, it's learned how to map our complicated problem.",
            "It's learned a transformation so that the problem looks like a Gaussian mixture model problem in the latent space.",
            "Cool, here's an."
        ],
        [
            "Other nice simple example.",
            "This is like this is like the simplest video modeling problem, so here's the idea.",
            "This is the frame index and these images.",
            "The frames of this video or 1 dimensional.",
            "So I've just stacked them up so each column.",
            "Think of it like a frame in a video and so this is like a 1 dimensional video, a bouncing ball.",
            "We can visualize it unrolling it quite nicely, right?",
            "So this is frame index horizontally.",
            "This is what the data looks like.",
            "It's like a noisy pattern of a ball bouncing.",
            "Here are the predictions is like the data reconstruction in the middle panel.",
            "Right now it's initialized to pretty much junk.",
            "And then here's the latent states down here.",
            "So these are lower dimensional.",
            "And they look like junk.",
            "Right now they're just initialized somewhere, and then they fuzz out.",
            "I'm conditioning the model on the data so it's trained on like 80 sequences.",
            "That looks something like this, right?",
            "And then I'm going to hit play.",
            "It's going to start running the training.",
            "This is some held out test sequence and we're going to give it the data up to the red line, right?",
            "So up until the red line in this predictions area it's solving a filtering problem where it's saying you've seen the data.",
            "What do you think?",
            "A cleaned up version of the data looks like solving to the left of the red line and then to the right of the redline?",
            "It's doing a prediction, it doesn't get to see this data, it's just trying to predict out what it thinks the data will look like.",
            "And then, correspondingly, you'll see the sort of the states that things are going on, and then the predictive states.",
            "Because everything is Bayesian here, these different lines are a bunch of samples from the variational posterior so you can see the sort of uncertainty estimate that way, so play.",
            "Did I imply?",
            "Who play?",
            "Super fun so.",
            "What I think is interesting is that it goes through sort of two phases.",
            "At first it learns how to maybe try to play one more time.",
            "At first it tries to can I see?",
            "OK, I'll play one more time.",
            "At first it sort of learns how to encode the data, but not predict very well.",
            "But then it sort of refines it.",
            "So I interpret that as saying it sort of learned the image manifold.",
            "This is a just so story, so don't read too much into this, but this is consistent with sort of it learning the image manifold quickly.",
            "It can also include the data, but it hasn't organized it in a way so that the data look like.",
            "Can be predicted by linear dynamical system, but then it revises an you can actually see these states for more time.",
            "These states sort of don't look like in LDS and then they pop into these sinusoidal things.",
            "So linear dynamical system states in a generic basis will look like these sinusoidal patterns, like these harmonic oscillator type things.",
            "So you can see that it's sort of now learn to make the latent space look like our prior.",
            "It's describing this complicated image video modeling problem, not super complicated this toy, but.",
            "As this nonlinear observation model, but it's learned to make the link space look like a linear dynamical system, and it's representing uncertainty nicely.",
            "You can see we're really confident about what the States are here.",
            "We've denoised it pretty much completely, but then as we get further from where we give it data, the predictions are less and less certain.",
            "You can see it start to fuzz in the image, but it's actually pretty confident over a very, very long ranges.",
            "So.",
            "This is."
        ],
        [
            "Just a very simple experiment just on, I think that Doc data of justice running training with flat gradients versus with these natural gradient estimates that we get out of the algorithm and the elbow improves much faster if we use natural gradients.",
            "Basically it means that the GMM or the LDS sort of latent variable part can be fit much faster if we use these things."
        ],
        [
            "So also let me emphasize that this lets us do inference with arbitrary missing data patterns.",
            "So before the difference with the slide before is, we had arrows pointing down here these are like CRF like arrows in that we have these recognition networks.",
            "So if you give me a video the way I will infer what the latent states were was, I will apply to any frames you gave me.",
            "I'll apply my recognition networks to get those local guesses as to what the configuration of the system was, and then do fast filtering or smoothing.",
            "Basically message passing to stitch together those guesses.",
            "So that's great.",
            "We still have arbitrary inference queries.",
            "I say see next slide because what is the next slide?",
            "Yeah, so it depends on what your recognition network.",
            "Architecture is for example, if you gave me a partially occluded frame.",
            "Right then we can't deal with that because that is the thing we're trying to plug into the recognition network.",
            "And if we haven't trained it with missing data on half of the frame, it won't know how to make an inference out of that.",
            "So it's sort of at the level of granularity and expressed in our graphical model.",
            "I would throw out that frame and just say I'll use the other frames.",
            "So if you give me a subset of times and you give me the complete frames at those times, then we can answer inference queries like that.",
            "If you delete half the pixels in every single frame, we can do inference because our graphical model doesn't see that, that's.",
            "Recognition network territory."
        ],
        [
            "I don't know why I wrote, see next slide.",
            "I can't remember why is related.",
            "I think the idea was that.",
            "So SV is we can plug in any inference network architecture.",
            "So if you wanted to do all of your inference for the bottom component, this graphical model using a bidirectional RNN, because maybe you don't want linear dynamics, you want nonlinear dynamics down here, But then still some other nice latent variables up top.",
            "You can essentially nothing.",
            "I told you was peculiar to only outputing node potentials or something.",
            "You could output edge potentials would have sketched here.",
            "Or you could have an or.",
            "Basically this is you know you get to choose how much information you aggregate and how much inference you do with your neural networks.",
            "Sort of learned parametric function approximator versus these optimization techniques.",
            "So of course, the more you do with your recognition networks, you get the advantages of recognition networks like sort of a fewer constraints on your model structure.",
            "You also get the disadvantages because that might mean that you have less and less access to complicated inference queries so."
        ],
        [
            "One way to think about this is like a giant nob where we have like a controller where we have published graphical model type techniques.",
            "Again, I mean like exponential family problems, removal type techniques over on one side and then we have pure deep learning techniques on the other side and I hope that these kinds of tools and ones that are developed in the spirit can sort of act as a way to interpolate where if we want to say this part of my model I want to be organized in a very specific way, but this other part of my model.",
            "This should be very flexible, you know some.",
            "Image model or something like this?",
            "We should have those modeling tools and then hopefully correspondingly we have inference tools that are able to leverage all of that model structure.",
            "Whatever model structure we do put in, we should be able to leverage and then for other things we should use more generic techniques.",
            "OK, so."
        ],
        [
            "We just tell you a little bit about this mouse application.",
            "I think I took out the slides on the like.",
            "Giving mice drugs, but I think I just have some picture slides in here.",
            "So.",
            "Let's learn a model for the mouse video.",
            "I'm going to build up the model sequentially.",
            "So first, if you fit a variation."
        ],
        [
            "So here's what the raw data looks like.",
            "Apologize if that's a little small, but these are these mouse video frames.",
            "The mouse head is pointing to the right.",
            "I've looked at this for a long time, so there's sort of easy for me to parse, but this is a depth mapping and so you can see this is his head is sort of raised up in his ears, are sticking out, his head is sticking almost straight up there.",
            "He's sort of running around, another one, sometimes curled up into a ball."
        ],
        [
            "So that was real data.",
            "This is if you fit a variational auto encoder to that data is like these are great images to work with.",
            "This is like the M mistreated video 'cause you're just little mouse blobs.",
            "But if it a very solid encoder it does a great job of learning and image model, but it hasn't learned any time series structure and it hasn't learned how to explain that time series structure in terms of switching system will get to that in a second.",
            "But it's really good at modeling images.",
            "This is why we love various autoencoders.",
            "I was really excited when I fit that sort of denoised mice here."
        ],
        [
            "You can also do the thing where you lay out regular lattice in on a 2 dimensional subspace of the latent space and then see what these different axes look like.",
            "So here in the top left this is the mouse he's huddled up in.",
            "His ears are sticking out a little bit, but he's kind of like curled up into a ball as the image goes to the right.",
            "He sort of extending right.",
            "This is like what this puppet string in the very solid color space does.",
            "It extends the mouses body and then sort of down here.",
            "Especially down here is sort of raising his head up and becoming hot in the heat map.",
            "So it's under pretty good representation of this image manifold.",
            "Now we want to learn dynamics on it, so let's start by fitting.",
            "So this was like IID Gaussian prior.",
            "Let's embellish the prior structure that we have in our living space.",
            "Let's fit a linear dynamical system."
        ],
        [
            "So here are just three samples of predictions, so the top line is a. Oh man, it's hard to see the top line is a prediction and the bottom line is real data, and again, we're conditioning on data.",
            "Up until this red line.",
            "So these are three different video subsequences.",
            "Also, this data is 30 frames per second, but I subsampled every 4 frames, so I think this is I don't know total of.",
            "Maybe 100 seconds or something, so it's actually a fairly long range prediction that we're making here, so up until the red line that you can see that it's sort of like tracking the mouse right here, he's putting his head down and curling up.",
            "And then to the right of the red lines, it's forming a prediction which doesn't need to track with the mouse.",
            "Does it just needs to be a plausable completion of what was going on.",
            "So here you can see the top one.",
            "It said that oh, maybe the mouse goes into a rear right then here, like in the bottom one, it's saying, oh the mouse was inner ear whose actually climbing that's his foot is climbing on the side of the cage.",
            "He's coming down out of the tree, ran, then he's going to curl up into a ball and you can see his little his butt in his ears sticking out.",
            "So this is basically a validation to say we can model at least the local structure of these videos with latent linear dynamical systems, right?",
            "It's doing a pretty good job of not only encoding the data, but making pretty long range predictions.",
            "So then the last embellishment is to add in some switching structure.",
            "Let's see if we can discover some behavioral primitives, right?",
            "Let's go from the linear dynamical system to a switching linear dynamical system.",
            "And I'll just show you some examples of states."
        ],
        [
            "That come out.",
            "So this is a panel of videos.",
            "What we did was we ran this inference and then we took where it was likely to for the where the inference decided that, oh, it's in State 5 or something, right?",
            "The states don't come with English labels, we just add those later.",
            "So this is some states.",
            "What I did is I took a bunch of examples of where that state was used and then I lined them up and made this composite video.",
            "So when I hit play it's going to be, you know, different my sanna different times.",
            "But then a little white box is going to appear in the corner, and that's when Interstate 5 or whatever state this was.",
            "So this is the start of a rear, so you can see that when you know when the white box goes on, you can see that this is when he starts to raise his head and go up here.",
            "He sort of started here and I think it does look a little head Bob, but for these other ones you can see that when the white square goes on, this is when he's rearing up or climbing up on the wall.",
            "Here's another one this fall from a rear.",
            "So here you can see that he generally starts with head high.",
            "In fact, this one in most of them seems to be climbing on the wall, and then he falls down, right?",
            "So this is like atomizing all the all.",
            "The behavior for us.",
            "Here's a here's a really interesting one.",
            "This is a grooming syllable, which is a little hard to see because they all look like little balls, so they're balled up here.",
            "But then the White Square turns on.",
            "You can see they're like ears start to wiggle right?",
            "And basically when he's curled up.",
            "And his ears are wiggling.",
            "That means he's grooming his tummy, right?",
            "So this is the mouse.",
            "It's discovered a sort of grooming syllable.",
            "And then when he leaves it, or he extends his body, the syllable turns off.",
            "So I think those are all the slides I have on that app."
        ],
        [
            "Kacian here is you know what I told you about modeling idea, which is sort of a relatively obvious 1.",
            "Instead of having our latent spaces be just Gaussian and having all the capacity of our models in our neural networks will certainly have a lot of capacity.",
            "Another thing we could do which might be appropriate, especially in some scientific applications, is let's write down more interesting latent variable structure, but then gets pushed through the neural Nets for capacity.",
            "And maybe there are interesting games we can play where we decide, for example here.",
            "I don't care how you model the image generation frame by frame of a mouse, right?",
            "That's just the neural Nets, it can.",
            "It can do essentially anything, but all the temporal correlation across frames, right?",
            "I didn't have an RNN, there's no dynamical neural network, so all the correlation between frames across time had to be explained through our graphical model latent variable structure.",
            "And so we sort of set up the structure and the model has to learn how to explain mouse videos as organized through this, like PGM.",
            "Representation.",
            "So that was the modeling idea.",
            "The big technical bit was how inference works actually have.",
            "So another way too.",
            "So this paper is like 20 pages of appendices with a lot of like calculations, and I have a new way of organizing this stuff that makes it a lot easier in terms of sort of like proximal operators and sort of thing, so you know, hopefully will post that sometime in the next couple months, but.",
            "Let me just say that inference we were certainly able to exploit a lot of the graphical model structure and still exploit also recognition networks to handle the hard parts.",
            "If you go and download the paper now you're going to like this is so complicated and it is sort of.",
            "It works, but it's like there's a lot of calculation to it.",
            "I think we have a new way that I'm excited about it, sort of thinking about things much more clearly and generalizing at the same time.",
            "So, but this inference mechanism is able to exploit structure will also be general, and I told you about this new application in sort of neurobiology.",
            "I think actually is a quite an interesting tool for not only practical things like drug development, but more importantly science and neuroscience people are interested in studying mouse behavior with.",
            "So with that, let me think my."
        ],
        [
            "Collaborators, of which there are many.",
            "David Ave Alex, Moscow Bob Dado is the Pi and neurobiology and Ryan Adams and then more recently I've been collaborating with Sergey 11 Scott Linderman.",
            "Can't members name and Matt Hoffman as well on this stuff.",
            "So thank you very much.",
            "Yeah.",
            "Can you say it again?",
            "Oh, why is death images?",
            "There are a few reasons, so actually.",
            "The mouse is behavior in a 3D matters right matters alot.",
            "The mouse does a lot of rearing and it'll flatten itself to the ground when it scared of predators.",
            "So we wanted to kind of 3D input.",
            "In principle you could reconstruct that from 2D like you as a human.",
            "So maybe in principle we could model that, but I think to infer the 3D pose a mouse you might have to learn a lot of hard things, like how to synthesize the mouse is texture and stuff.",
            "Basically it's a much easier problem to have a good sensor to do that and we were trying to do science.",
            "I think it would be super interesting though if you could train a system that looks a mouse videos, maybe multiple perspectives, it could learn how to track from scratch and it could learn how to reconstruct the mouse.",
            "Suppose there are elements mouse behavior that we don't see from the top like where its feet are or something.",
            "If its limping sort of hard to see.",
            "So we can either attack that with better measurement modalities or with you know better inferential techniques and the ultimate limit is whatever we can do by watching this video and use our mouse priors to understand it.",
            "Yeah, just something that hasn't been explored, I think.",
            "Yeah no, no.",
            "The reasons for doing it this way as well.",
            "They said most of the time you don't use mice, we use the OR the rats and Lab Rats and not one color.",
            "Multicolor.",
            "Yes, right?",
            "This completely breaks down with vision.",
            "Yeah, right?",
            "Certainly because complex.",
            "Yeah, and the other thing is that these are nocturnal animals, so you might want to use him for writers.",
            "That's right, I forgot about that, so I Fortunately just dealt with code and numbers.",
            "Now with the mice themselves, Biondo made the two points one that rats are different colors.",
            "These mice are all C57 Black 6 and so they were all genetically identical.",
            "Twins so, but yeah, certainly the other point that actually all those recordings were done in the dark, so the Connect uses infrared pattern to sense things that presumably the mouse can't see.",
            "But yeah, it's hard to do.",
            "Color vision in the dark, certainly.",
            "Yeah.",
            "Other questions yeah.",
            "Supervised supervised learning.",
            "Yeah.",
            "So in principle you can just sort of shade any graphical model nodes you want and condition on those.",
            "So if you had some labels that you wanted to use, the sort of bootstrap and bias.",
            "As a prior and empirical prior for what these things should look like, you can run all these these inference algorithms and they still apply with semi supervision.",
            "I think that's another advantage of graphical models is you can sort of.",
            "It goes hand in hand with being able to support any inference query.",
            "You can sort of shade things in and instead of having an inference network is a way of answering one inference query or maybe a linear nested sequence of inference queries.",
            "But with graphical models are about in part is being able to.",
            "Have a template to generate a program from any shaded node pattern to how to do inference with shaded pattern?",
            "Yeah.",
            "Thanks.",
            "And then.",
            "But what can you do for this during that country competition?",
            "So I think your question is, hey, you were talking about some examples and then saying that there's some generality to it in the variational inference and like factorization, and like where does this structure like what?",
            "What is the general theory like?",
            "How do you generally describe when you can do this sort of thing?",
            "And there is an answer that this is, you know, as I was saying, we have different ways of thinking about this stuff.",
            "Now that I think is a lot cleaner, I'm going to give you an answer that's a bit technical so up.",
            "Essentially because we built our generative model in terms of sort of conditional exponential family structure.",
            "It turns out that when we do that, we end up with an overall exponential family.",
            "That is the energy term in it.",
            "The sort of linear term is a is in fact linear in its overall statistics, but it is a multilinear polynomial in the statistics of the component factors, so we have a TX times, TZ, right and other stuff we never have TX squares in it, so it's a multilinear polynomial of these simpler statistical families.",
            "The magic trick we did there with when we can factorize it and decompose it and things became nice was basically saying let's write something.",
            "Let's her variational family be a product of things that are linear in those statistics and those statistics.",
            "The overall energy is multilinear in an because those things are tractable by definition.",
            "We built it out of components like a Gaussian linear dynamical system that on its own I can handle it.",
            "Sort of.",
            "This multi linear structure that lets us decompose it then into a bunch of tractable pieces.",
            "So somehow being multilinear in statistics functions that correspond to exponential families for which we can compute the log normalizer exactly, that's sort of the technical jargon answer.",
            "High school.",
            "Are we good?",
            "Where the Clock alright.",
            "Thanks guys."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks, you guys can hear me OK, OK so yeah I think the similarity is I think Max is talk.",
                    "label": 0
                },
                {
                    "sent": "Also said something like composing graphical models with neural networks, which is the first 5 words in my title as well.",
                    "label": 0
                },
                {
                    "sent": "Six so.",
                    "label": 0
                },
                {
                    "sent": "However, this is like a poorly scoped title to some extent, because people have been using some graphical model ideas with deep learning for forever.",
                    "label": 0
                },
                {
                    "sent": "So in fact I think that Joshua was probably doing work in the early 90s on things that we later called conditional random fields that were basically like using neural networks to predict potentials in a graph and then doing some kind of message passing.",
                    "label": 0
                },
                {
                    "sent": "So really there's been a lot of work on broadly composing graphical models with neural networks.",
                    "label": 1
                },
                {
                    "sent": "So what I'm going to tell you about, though, is some some of our work that is different from what's come before in a few ways.",
                    "label": 0
                },
                {
                    "sent": "So one is that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use more graphical model technology.",
                    "label": 0
                },
                {
                    "sent": "Then maybe Max was telling you about yesterday, so graphical models actually refers to a lot of different ideas, but there are big textbooks written on these things, right?",
                    "label": 0
                },
                {
                    "sent": "And long monographs so.",
                    "label": 0
                },
                {
                    "sent": "You know, sometimes we see graph models and it's just like a visualization tool, right?",
                    "label": 0
                },
                {
                    "sent": "Or you know a description of some simple relationships between the three random variables in R model or something and everything else.",
                    "label": 0
                },
                {
                    "sent": "All the technologies in the in the neural networks and how it's pushing those probability masses around.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to talk to you about today is going to be about using a lot more graphical model stuff, in particular using a lot of like exponential family math to get really fast inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "That exploits a lot of the structure that we can put into models.",
                    "label": 0
                },
                {
                    "sent": "The other part is going to you that this is a good idea that we should write down models this way and pair them with deep learning in a particular way.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about actually a line of work that directly builds on Dirking Liz and Max Wellings work on various autoencoders and the explosion of research that's come out of that.",
                    "label": 0
                },
                {
                    "sent": "And the similar papers.",
                    "label": 0
                },
                {
                    "sent": "So basically another title.",
                    "label": 0
                },
                {
                    "sent": "Another version of this title, more descriptive would be like very slow encoders, plus probably graphical models in the latent space that we're going to see.",
                    "label": 0
                },
                {
                    "sent": "So this is work that I did.",
                    "label": 0
                },
                {
                    "sent": "In large part during my postdoc.",
                    "label": 0
                },
                {
                    "sent": "And also that I'm continuing at Google Brain where I am now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think one reason that a lot of the graphical models that we write down in deep learning are simple is that usually we have things that are just.",
                    "label": 0
                },
                {
                    "sent": "I sort of are.",
                    "label": 0
                },
                {
                    "sent": "We don't want to constrain our representations at all, and we have to make fairly simple predictions, like the inferences we have to make our not so complicated, but I want to tell you about problem that I think is 1, in which we do want to constrain our representation or particular way while still being flexible in other ways.",
                    "label": 0
                },
                {
                    "sent": "So that starts with this guy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Maybe not as cute as Nando's video, but this is a mouse on a lab table top in I said not as not as cute, so this is still kinda cute, though this is a mouse on a lab bench in.",
                    "label": 0
                },
                {
                    "sent": "Harvard Medical School in the Neurobiology lab so they have colonies of hundreds of these mice and of course they study them for many reasons, but this neurobiology lab wants to learn things about how the mouse is brain works, and in particular this.",
                    "label": 0
                },
                {
                    "sent": "This lab that I was a postdoc in wanted to sort of have developed new tools for analyzing this mouse is behavior because the brain ultimately gives rise to behavior and this is a huge undertaking.",
                    "label": 0
                },
                {
                    "sent": "Understanding how the mouse responds to its environment and maybe learns.",
                    "label": 0
                },
                {
                    "sent": "Those are all sort of like downstream things.",
                    "label": 0
                },
                {
                    "sent": "We just want a way to sort of represent this guys behavior.",
                    "label": 0
                },
                {
                    "sent": "So if you watch videos of this, so this is actually not interacting with very much.",
                    "label": 0
                },
                {
                    "sent": "This is where we wanted to start.",
                    "label": 0
                },
                {
                    "sent": "Just sort of like trying to escape these, exploring his surroundings.",
                    "label": 0
                },
                {
                    "sent": "They like being kind of cooped up.",
                    "label": 0
                },
                {
                    "sent": "So this mouse, if you watch its behavior for awhile, you might sort of naturally start to form your own description for what it's doing.",
                    "label": 0
                },
                {
                    "sent": "So what I would do in what in fact ethologist have done for a long time is sort of naturally see these brief, stereotyped units of action, right?",
                    "label": 0
                },
                {
                    "sent": "These sort of behavioral primitives so he'll sort of clean his nose Hill dark forward, he'll rear up in the air.",
                    "label": 0
                },
                {
                    "sent": "Sometimes, depending on what we're doing to him, he'll like curl up into a ball and fear.",
                    "label": 0
                },
                {
                    "sent": "Or you know, if you think there's a Fox nearby or something else, or press himself against the ground.",
                    "label": 0
                },
                {
                    "sent": "So basically there are these little motifs of action, right?",
                    "label": 0
                },
                {
                    "sent": "These stereotypes, motifs of action that we'd like to discover automatically from some kind of high bandwidth like video kind of data.",
                    "label": 0
                },
                {
                    "sent": "So then we can study how that representation might change as a way to study how the mouse is.",
                    "label": 0
                },
                {
                    "sent": "Behavior is changing.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, you know I said that we can think of this mouse is behavior as decomposing into these brief, stereotyped actions like darts and rears and pauses if we want to discover these sorts of units, right?",
                    "label": 0
                },
                {
                    "sent": "This is kind of like discovering syllabic units from sound.",
                    "label": 0
                },
                {
                    "sent": "If I give you an audio recording of some language, maybe you don't speak listening to it.",
                    "label": 0
                },
                {
                    "sent": "You might imagine that by picking out regularity's like picking out these stereotypes, phonetic units, you might be able to discover.",
                    "label": 0
                },
                {
                    "sent": "Some kind of phoneme from the language, right?",
                    "label": 0
                },
                {
                    "sent": "In fact, this is a problem that people have studied in sort of speech processing.",
                    "label": 0
                },
                {
                    "sent": "Given recordings, try to discover these phonetic units.",
                    "label": 0
                },
                {
                    "sent": "And so, in analogy to this, I'm going to refer to these behavioral primitives as behavioral syllables, right?",
                    "label": 0
                },
                {
                    "sent": "So the content of language is actually not in the phoneme themselves so much as in their statistics, like how they are composed into sequences, right?",
                    "label": 0
                },
                {
                    "sent": "So we have this alphabet of phonemes as it were, but the way we communicate the actual information content is really through quantizing these things and then seeing how they are made into sequences.",
                    "label": 0
                },
                {
                    "sent": "And so we want to do something similar for mice we want to.",
                    "label": 0
                },
                {
                    "sent": "Discover what these syllables are in an unsupervised way from video.",
                    "label": 0
                },
                {
                    "sent": "And then we want to use that to study how these statistics of these things change.",
                    "label": 0
                },
                {
                    "sent": "Maybe how the unigrams or bigrams statistics change as we for example, do some optogenetics and kind of zapped the mouse in the brain with the laser to make it move?",
                    "label": 0
                },
                {
                    "sent": "Or think something we might change its genetic code or expose it to some kind of Fox odors?",
                    "label": 0
                },
                {
                    "sent": "Or give it some pharmaceutical drugs right?",
                    "label": 0
                },
                {
                    "sent": "So drug companies are really interested in high throughput behavioral phenotyping because they want to be able to do screening and pre clinical trials.",
                    "label": 0
                },
                {
                    "sent": "They want to do some explorations on what drug compounds they should use to change human behavior or treat medical conditions so they have mouse models for things like autism, and they would like a way to represent the complex set of behaviors that we described as this being a mouse model for autism and then understand how it changes when we give it a drug that reverts to baseline.",
                    "label": 0
                },
                {
                    "sent": "This sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hear this mouse again.",
                    "label": 0
                },
                {
                    "sent": "Here's our friend.",
                    "label": 0
                },
                {
                    "sent": "We have to get him into computer somehow and So what we did is we used connect.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Depth cameras, so this is a connect two video shot from above of the mouse in sort of larger enclosure, but there's sort of nothing interesting going on in the enclosure, so it's basically like freely behaving, so this is the circular enclosure we can remove these sorts of specularities and in fact we tracked the mouse in 2D to produce a sort of aligned video sequence where the mouse is always facing to the right.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the egocentric view of the mouse right?",
                    "label": 0
                },
                {
                    "sent": "Sort of the body aligned view of the mouse, and so when I say we want to model.",
                    "label": 0
                },
                {
                    "sent": "Mouse behavior, model, videos and mouse behavior.",
                    "label": 0
                },
                {
                    "sent": "What I mean is we want to model video sequences that are like this sequences of frames like this box 1.",
                    "label": 0
                },
                {
                    "sent": "So how should we think about this problem?",
                    "label": 0
                },
                {
                    "sent": "Well, just think about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The data you know one way to think about it is to refer to this object.",
                    "label": 0
                },
                {
                    "sent": "We often think about this image manifold right?",
                    "label": 1
                },
                {
                    "sent": "Somehow these images might be very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Nominally.",
                    "label": 0
                },
                {
                    "sent": "There you know 80 by 80 or something.",
                    "label": 0
                },
                {
                    "sent": "So maybe 6400 dimensions, very high dimensional images, but we don't see every possible configuration of those images right?",
                    "label": 0
                },
                {
                    "sent": "Every possible pixel pattern we see a very small subset, the ones that look like mice, right?",
                    "label": 0
                },
                {
                    "sent": "The background sort of always dark.",
                    "label": 0
                },
                {
                    "sent": "And then there's some well shaped.",
                    "label": 0
                },
                {
                    "sent": "Blob in the middle and in fact we know that the mouse because we're recording it in time and the mouse moves smoothly through time.",
                    "label": 0
                },
                {
                    "sent": "We should believe that you know, maybe we can think of these images is lying on or near some low dimensional image manifold and part of the modeling task is to discover this.",
                    "label": 1
                },
                {
                    "sent": "So that is to say that every frame of this depth video.",
                    "label": 0
                },
                {
                    "sent": "Is sitting on or near some low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "We might like to discover, but we're not done.",
                    "label": 0
                },
                {
                    "sent": "We don't just want to find the image manifold of mice, we want to describe videos.",
                    "label": 0
                },
                {
                    "sent": "So a video is a trajectory along this manifold.",
                    "label": 0
                },
                {
                    "sent": "In fact, we're not even done then.",
                    "label": 0
                },
                {
                    "sent": "We don't just want to discard the manifold and have some dynamical system.",
                    "label": 0
                },
                {
                    "sent": "I actually said that we wanted to have a very structured dynamical system, right?",
                    "label": 0
                },
                {
                    "sent": "We wanted to say learn this manifold not just as a set, but in coordinates, where we can describe the dynamics using relatively simple dynamical systems, like.",
                    "label": 0
                },
                {
                    "sent": "Perhaps there's a simple dynamical pattern, as it's in one behavior like a dart and another simple dynamical mode when it goes into a rearing action.",
                    "label": 0
                },
                {
                    "sent": "Yes, question.",
                    "label": 0
                },
                {
                    "sent": "Facing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the question was, is there a reason you keep the mouse facing right?",
                    "label": 0
                },
                {
                    "sent": "There's a scientific reason which is that they were interested in egocentric behavior modeling, meaning they wanted the mouse.",
                    "label": 0
                },
                {
                    "sent": "You know this is how the mouse sees itself.",
                    "label": 0
                },
                {
                    "sent": "We see ourselves, you know from one preferred coordinate frame.",
                    "label": 0
                },
                {
                    "sent": "And so that's sort of what we wanted to study.",
                    "label": 0
                },
                {
                    "sent": "Was egocentric behavior you could, I mean this work is really like a starting point of trying to build models for behaving mice.",
                    "label": 0
                },
                {
                    "sent": "There's all kinds of other stuff you want to do.",
                    "label": 0
                },
                {
                    "sent": "So for example, you shouldn't forget about the position because the position could be meaningful if it's fleeing from something or interacting with something right?",
                    "label": 0
                },
                {
                    "sent": "Of course, that's meaningful.",
                    "label": 0
                },
                {
                    "sent": "You might want to do this all end to end.",
                    "label": 0
                },
                {
                    "sent": "Don't have any tracking right, just like to be able to have an attention on the mouse and like decide the alignment.",
                    "label": 0
                },
                {
                    "sent": "There's all kinds of stuff to do, but this is just sort of the first problem that we studied.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The big picture here is we have some ingredients we want to do something like learning manifolds of images right?",
                    "label": 0
                },
                {
                    "sent": "Which sounds familiar sounds like hey, deep learning gives us a lot of tools for that.",
                    "label": 0
                },
                {
                    "sent": "Explicitly parameterising manifold images.",
                    "label": 0
                },
                {
                    "sent": "But then we also want to have something where we're simultaneously saying actually I want to learn these manifold coordinates so that I can organize the information organized representation in a particular way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What kind of tools can we look for here?",
                    "label": 0
                },
                {
                    "sent": "So obviously we can look for recurrent neural networks, right?",
                    "label": 0
                },
                {
                    "sent": "So referral networks if you want to predict video and your ultimate performance is going to be measured on how well you predict frames of video, right?",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural Nets there's many different variations, but these are sort of the best technologies we have.",
                    "label": 0
                },
                {
                    "sent": "I think to predict video.",
                    "label": 0
                },
                {
                    "sent": "So if we had a sort of prediction problem where we needed to estimate the density of video or draw a new samples of video, you know go deep learning all the way.",
                    "label": 0
                },
                {
                    "sent": "However, that's not the problem that the scientists have, so the scientists don't want to build a machine that can generate new mouse videos because they already have that machine.",
                    "label": 0
                },
                {
                    "sent": "It's undergrads, undergrads connect, and a mouse.",
                    "label": 0
                },
                {
                    "sent": "You'll get as many mouse videos as you want.",
                    "label": 0
                },
                {
                    "sent": "They have the mouse videos.",
                    "label": 0
                },
                {
                    "sent": "What they don't have is, you know, they want to understand the data and.",
                    "label": 0
                },
                {
                    "sent": "To some extent, like when we fit something like on our net and RNN, and certainly this work on making these things interpretable, but you fit in our end you got, you got?",
                    "label": 0
                },
                {
                    "sent": "You got like hundreds of megabytes of parameters and it's like all these weights and bias bicis these matrices?",
                    "label": 0
                },
                {
                    "sent": "What have you learned about your data?",
                    "label": 0
                },
                {
                    "sent": "Your network is learn about your data 'cause it can predict it well.",
                    "label": 0
                },
                {
                    "sent": "It's learned the way in which mouse behaviors multimodal and stuff, right?",
                    "label": 0
                },
                {
                    "sent": "But you haven't learned that you would have to sort of take those parameters and unpack them somehow and try to like inspect them.",
                    "label": 0
                },
                {
                    "sent": "To try to figure out what the neural net has learned in its prediction rule, how to like how to understand behavior.",
                    "label": 0
                },
                {
                    "sent": "But if we want to understand that neural Nets might not be vanilla, neural networks might not be exactly the right tool, because the representation internally of how it's forming these predictions, too unconstrained.",
                    "label": 0
                },
                {
                    "sent": "So another set of tools we can think about that are really specifically about constraining probability distributions right constraining representations.",
                    "label": 0
                },
                {
                    "sent": "Those are probabilistic graphical models, and indeed in this talk, when I say probably graphical models, so probably graphical models formally just talk about independent relationships or factorization relationships in densities.",
                    "label": 0
                },
                {
                    "sent": "When I say probably graphical models in this talk, what I mean is actually more restricted models.",
                    "label": 0
                },
                {
                    "sent": "Things that are built out of sort of tractable exponential family structure.",
                    "label": 0
                },
                {
                    "sent": "And I can say more technical details about that a bit later, but sort of.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know building things out of simple components that we have a lot of understanding about what's going on because it is simple problem with probably graphical models is there are two restricted right there, too hard to fit video data with.",
                    "label": 0
                },
                {
                    "sent": "So in fact, to set up a bit of how I think about this contrast, sort of exponential family problems, graphical model type tools versus sort of generic deep learning tools, I want to show you the most stylized version of a similar problem.",
                    "label": 0
                },
                {
                    "sent": "Write a similar sort.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representation learning problem.",
                    "label": 0
                },
                {
                    "sent": "So here I've gotten rid of the mouse and the video like the time series and I'm just saying here are some data points in 2D.",
                    "label": 0
                },
                {
                    "sent": "Learn a representation of it that you can then show a scientist who's interested in these data.",
                    "label": 0
                },
                {
                    "sent": "You know what do you?",
                    "label": 0
                },
                {
                    "sent": "What do you see here?",
                    "label": 0
                },
                {
                    "sent": "Clusters, right?",
                    "label": 0
                },
                {
                    "sent": "Obviously we want to find clusters.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you fit this with just a density model, that wouldn't be very interesting, but you want to discover, like hey, these clusters in the data, maybe that scientifically meaningful so you could fit these data with a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "And if someone asked you if a scientist friend approached you and had data look like this, you would fit in with Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "You would not say download Tensorflow an run a very autoencoder or something, but of course this data is really simple and this ability to fit with these simple parametric forms like a mixture of Gaussians breaks down really quickly.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even just staying in 2D right in Datsun 2D, here's another set of points.",
                    "label": 0
                },
                {
                    "sent": "I don't know about you, but I also see clusters that I would think are meaningful and I want to discover automatically.",
                    "label": 0
                },
                {
                    "sent": "But if you try to fit Gaussian mixture model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Depending on how you set the hyperparameters, because the model is misspecified, right?",
                    "label": 0
                },
                {
                    "sent": "Because the cluster components don't look like the cluster shape I've prescribed to them, by saying a mixture of Gaussians, it'll just have to fit the density with these blobs, and this is bad because not only have we not discovered the clustering structure in the data, these cluster labels now are not very meaningful, but also this is not a very good density model, right?",
                    "label": 0
                },
                {
                    "sent": "It's just going to try to dial things.",
                    "label": 0
                },
                {
                    "sent": "You could use Gaussian mixture models as density models.",
                    "label": 0
                },
                {
                    "sent": "In some cases you do, but.",
                    "label": 0
                },
                {
                    "sent": "Usually you just have to deal with so many Gaussians and discrete variables now, not a good idea.",
                    "label": 0
                },
                {
                    "sent": "So another thing you could do to these data, this like.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Warped mixture model data is you could fit a density network of some kind, so in particular you could fit a variational autoencoder right the generative part of various auto coder says take some Gaussian probability, some Gaussian random variable and then push it forward through some complicated mapping that you're going to fit to make you know.",
                    "label": 0
                },
                {
                    "sent": "Basically any complicated density you like additive Gaussian noise at the end.",
                    "label": 0
                },
                {
                    "sent": "All right, this is a super flexible model 'cause a neural network can push around probability mass in a really complicated way.",
                    "label": 0
                },
                {
                    "sent": "Certainly can take a unimodal Gaussian and push it to be multimodal.",
                    "label": 0
                },
                {
                    "sent": "So in fact, that's what I did here.",
                    "label": 0
                },
                {
                    "sent": "And like other than some maybe should train it a bit longer.",
                    "label": 0
                },
                {
                    "sent": "It's learned the density of the data, and so if your problem were to sample new samples that looked like your datasets because it's a bedroom or a mouse video or something, this would be great.",
                    "label": 0
                },
                {
                    "sent": "You just sample, you get something from the purple density mass and it would look like your data would be happy.",
                    "label": 0
                },
                {
                    "sent": "But now again, we don't see the clustering structure right.",
                    "label": 0
                },
                {
                    "sent": "The information that there are five clusters here that's buried somewhere in the weights and biases of your network, right?",
                    "label": 0
                },
                {
                    "sent": "If I give you a very solid encoder and say how many modes has it learned we like, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe you could be clever and come up with some way of trying to slice open its guts and say like, oh, you know, here's the.",
                    "label": 0
                },
                {
                    "sent": "Entrails are very favorable for us, but I think in general it's quite hard to know what your neural net is.",
                    "label": 0
                },
                {
                    "sent": "Learn about your data.",
                    "label": 0
                },
                {
                    "sent": "So we really want is some kind of high.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It approach right?",
                    "label": 0
                },
                {
                    "sent": "So for this cartoon problem, the hybrid approach would say learn the structure of the data in this case meaning discrete clusters of data points.",
                    "label": 0
                },
                {
                    "sent": "But also we don't want to have this specification issue where when we tried to fit mixtures of Gaussians that meant that we weren't solving anything.",
                    "label": 0
                },
                {
                    "sent": "In fact we want to have the flexibility maybe of neural network models to model complicated cluster shapes, for example but.",
                    "label": 0
                },
                {
                    "sent": "We simultaneously want to be able to learn in our latent representation some organized structure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh one high level way of thinking about this is that the way we've been very successful in supervised learning is by combining deep learning with very simple linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "So here's what I mean.",
                    "label": 0
                },
                {
                    "sent": "One way of thinking about your deep network is that it's doing.",
                    "label": 0
                },
                {
                    "sent": "It's got a feature mapping right that we're learning that's taking your data in your data space.",
                    "label": 0
                },
                {
                    "sent": "Your decision boundaries could be super complicated, but we're going to learn features.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn a way to transform that data space.",
                    "label": 0
                },
                {
                    "sent": "Maybe will blow up the dimension.",
                    "label": 0
                },
                {
                    "sent": "Will process it until at the top layer we're going to demand the things be linearly separable right at the top layer.",
                    "label": 0
                },
                {
                    "sent": "You just do linear logistic regression, so we've solved before you know.",
                    "label": 0
                },
                {
                    "sent": "Really tough nonlinear problems by using deep learning to learn how to map it to an easy problem.",
                    "label": 0
                },
                {
                    "sent": "And we simultaneously learn our linear separation and our features.",
                    "label": 0
                },
                {
                    "sent": "Because we're simultaneously learning these things right, we're learning how to make the problem easy.",
                    "label": 0
                },
                {
                    "sent": "We're learning features in which our simple model works well.",
                    "label": 0
                },
                {
                    "sent": "So the analogy here maybe for these problems that have a more unsupervised character is to say, let's take our complicated problems and learn nonlinear feature transformations, in which simpler, more organized structures fit well, right?",
                    "label": 0
                },
                {
                    "sent": "So this is like, let's learn features to organize our space so that now things over here when they're straightened out now they fit our simpler priors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just to draw a high level summary on all these things, probably graphical models.",
                    "label": 0
                },
                {
                    "sent": "And remember by that I mean.",
                    "label": 0
                },
                {
                    "sent": "Things built out of tractable, exponential family structures things like Gaussian mixtures of Gaussians, linear dynamical systems, hidden Markov models with finite numbers of states.",
                    "label": 0
                },
                {
                    "sent": "They're good for some things.",
                    "label": 0
                },
                {
                    "sent": "This thing that I'm calling structured representations where I am manually prescribing I want this part of the representation to be organized a specific way.",
                    "label": 0
                },
                {
                    "sent": "They're good at thinking about priors and uncertainty.",
                    "label": 1
                },
                {
                    "sent": "I know how to specify a prior on a linear dynamical system so that I can say it's, you know, we believe that there is this some frequency content.",
                    "label": 0
                },
                {
                    "sent": "There are these time scales that are present in my process and not these others.",
                    "label": 0
                },
                {
                    "sent": "I know right those priors so PGM's are good tools for that.",
                    "label": 1
                },
                {
                    "sent": "The big problem is that the rigid assumptions that we bake into these strong priors may not fit our data well, right?",
                    "label": 0
                },
                {
                    "sent": "It's like how the Gaussian mixture model being misspecified and then not sure if all of you around for this, but like you know, feature engineering used to be very common place in machine learning and this is like the worst part of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe we still do it sometimes, but we much prefer to instead of manually engineers, we prefer to somehow learn the men tend.",
                    "label": 0
                },
                {
                    "sent": "So those are sort of modeling reasons.",
                    "label": 0
                },
                {
                    "sent": "PDF's are good.",
                    "label": 0
                },
                {
                    "sent": "Here's some inference reasons that will say more about later, probably graphical models.",
                    "label": 0
                },
                {
                    "sent": "Let us answer arbitrary inference queries, so if you have missing data or something, PM's are basically exactly the tool for how to think about stitching together partial information.",
                    "label": 0
                },
                {
                    "sent": "Also, if your prior is a good one, then you can get data efficiency right?",
                    "label": 0
                },
                {
                    "sent": "If your prior is a good one then you don't have to learn from scratch quite as much as when you have a sort of non informative.",
                    "label": 0
                },
                {
                    "sent": "Network prior.",
                    "label": 0
                },
                {
                    "sent": "Also they can give us computational efficiency if we stick to these rigid model classes, but the downside of course is that these things, because they lack flexibility.",
                    "label": 0
                },
                {
                    "sent": "If we try to make them more flexible, we end up breaking all the nice fast inference that we have.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are things PDM's are good at in terms of modeling and then in terms of inference, deep learning is also quite good at some things.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you some things.",
                    "label": 0
                },
                {
                    "sent": "I think that standard deep learning techniques are not so good at one as opposed to sort of getting the structure representations out so that we can understand.",
                    "label": 0
                },
                {
                    "sent": "I think more often we just get neural net do right.",
                    "label": 0
                },
                {
                    "sent": "We get hundreds of megabytes of weights and biases and then the problem begins to try to decipher what it's learned.",
                    "label": 0
                },
                {
                    "sent": "And also I think the parameterisation is quite difficult to put priors over.",
                    "label": 0
                },
                {
                    "sent": "It's not clear to me what a Gaussian prior on.",
                    "label": 0
                },
                {
                    "sent": "Neural net weights is supposed to mean or if that's like a reasonable inductive bias, certainly I don't know how to do things like put timescale priors on RNN, so I'm sure there's been some work on it.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's a solved problem, but of course we love deep learning, right?",
                    "label": 0
                },
                {
                    "sent": "The reason why we're in deep learning Summer school is because these things are so flexible in such high capacity, and this character feature learning from our data.",
                    "label": 1
                },
                {
                    "sent": "Deep learning also sort of has more unlimited inference queries.",
                    "label": 0
                },
                {
                    "sent": "I'll say a bit more about this later, but when you write down an RN or something Wavenet or something if you want to be able to infer missing, you know, given partial observations in some arbitrary pattern.",
                    "label": 1
                },
                {
                    "sent": "Oftentimes you have to redesign your neural net and then retrain it for every inference pattern the demand, whereas graphical models sort of give you exponentially many combinatorially many inference programs out of your one model specification.",
                    "label": 0
                },
                {
                    "sent": "Of course they can also be data and compute hungry.",
                    "label": 0
                },
                {
                    "sent": "But one really exciting thing about inference that for me, I first learned from reading Dierking was in maximum paper on virtual autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Is this idea of recognition networks that learn how to do inference for you?",
                    "label": 1
                },
                {
                    "sent": "Much more about that later, but that's super exciting for approximate inference, so stepping back and looking at this table, I think it's clear that I set this up to say that I think these tools from exponential family problems, graphical models, and these sort of deep learning tools these ideas are actually quite complementary, and in fact, the extent to which we can put these together is like chocolate and peanut butter, right?",
                    "label": 0
                },
                {
                    "sent": "We should be really excited for the problems where we're not just solving a prediction problem, but we need to make a complex.",
                    "label": 0
                },
                {
                    "sent": "More complex inference in our prediction, or we demand a more structured representation to our machine that's making the prediction.",
                    "label": 0
                },
                {
                    "sent": "So I want to put these two together in emphasizing these sort of like sailing sailing points.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That was all motivation.",
                    "label": 0
                },
                {
                    "sent": "Now let me tell you the rest of what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "First, I'm going to tell you a modeling idea, which is kind of an obvious one.",
                    "label": 0
                },
                {
                    "sent": "Basically, take graphical models, you put graphical models at these nicely structured exponential family graphical models.",
                    "label": 0
                },
                {
                    "sent": "Put them on the latent variables inside something like a variational auto encoder and then have neural networks for observation models to connect them to data and this will have this nice balance of sort of organized latent space.",
                    "label": 0
                },
                {
                    "sent": "Kind of like the Platonic realm of organized objects, and then the flexibility to describe complex data that don't fit those hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Calling idea then the main content of this talk.",
                    "label": 0
                },
                {
                    "sent": "Certainly the main technical contribution of our work is how to do inference in these models.",
                    "label": 0
                },
                {
                    "sent": "So in fact it's not hard to write down a model class like the one I described.",
                    "label": 0
                },
                {
                    "sent": "Tricky thing is doing effective inference, and in particular we want to do inference that uses all of the best.",
                    "label": 0
                },
                {
                    "sent": "Recent approximate inference technology from deep learning.",
                    "label": 0
                },
                {
                    "sent": "Deep learning like tools.",
                    "label": 0
                },
                {
                    "sent": "But we also because we built all the structure into our latent variable model.",
                    "label": 1
                },
                {
                    "sent": "We want to be able to use that structure an have really fast inference algorithms as a result.",
                    "label": 0
                },
                {
                    "sent": "So the inference part of this talk, which is really the technical part, really comes down to having recognition networks and pairing them with fast graphical model.",
                    "label": 1
                },
                {
                    "sent": "Fast exponential, family graph model algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then finally I'll say a bit and show a few pictures about.",
                    "label": 1
                },
                {
                    "sent": "This application of learning this sort of syllabic representation of mouse behavior from video.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So first modeling idea.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already described it a bit, but let me say a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "Specifically in this this one application, by the way, it's meaningful.",
                    "label": 0
                },
                {
                    "sent": "Our application is a science.",
                    "label": 0
                },
                {
                    "sent": "One.",
                    "label": 0
                },
                {
                    "sent": "Write like scientists want to learn about the world and about the data.",
                    "label": 0
                },
                {
                    "sent": "They don't just want to have a machine that forms predictions and then they ask no questions about how it's coming to those predictions.",
                    "label": 0
                },
                {
                    "sent": "So this scientific problem was a great sort of way to spur thinking in these terms.",
                    "label": 0
                },
                {
                    "sent": "So if you remember, we want to discover from video this like syllabic representation.",
                    "label": 0
                },
                {
                    "sent": "So how do we write a latent variable model again using very restricted exponential family type tools?",
                    "label": 0
                },
                {
                    "sent": "To describe generating these data, we want to come up with a generative story.",
                    "label": 0
                },
                {
                    "sent": "So the first step is, let's think of having a finite state hidden Markov model, right?",
                    "label": 0
                },
                {
                    "sent": "This is going to model, so think of time is going to write.",
                    "label": 0
                },
                {
                    "sent": "This is going to model switching between different behavioral modes.",
                    "label": 0
                },
                {
                    "sent": "So while the mouse sort of is in this one state, this blue states he's going to be darting and then it will switch to another behavior.",
                    "label": 0
                },
                {
                    "sent": "This green thing.",
                    "label": 0
                },
                {
                    "sent": "So these discrete mode sequence is going to represent these are the latent variables in coding are our structure or prior.",
                    "label": 0
                },
                {
                    "sent": "To say we want to have this switching dynamic switching between these different syllables.",
                    "label": 0
                },
                {
                    "sent": "So of course we'll write down a prior on transition matrices right and then hidden state Markov model for how those states unroll.",
                    "label": 0
                },
                {
                    "sent": "The next step is what do we want to each states to be described as so in each state we want it to be a dynamical pattern, right?",
                    "label": 0
                },
                {
                    "sent": "Like when the mouse is running we want to correspond to something happening to those manifold coordinates, right?",
                    "label": 0
                },
                {
                    "sent": "Something happening to the mouse puppet strings in the image model.",
                    "label": 0
                },
                {
                    "sent": "So when it's running there will be some dynamical system which will just write as this linear dynamical system.",
                    "label": 0
                },
                {
                    "sent": "So we have written this.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "So think of this process here.",
                    "label": 0
                },
                {
                    "sent": "As discrete time sampling of you know a low dimensional linear dynamical system.",
                    "label": 0
                },
                {
                    "sent": "So at each time T there's some latent variable X which describes the mouse is configuration in the in the in the coordinates, the low dimensional coordinates, and while we're in state, blue in the blue state, we're evolving according to one set of these linear dynamical parameters, and then we switch into another state, will have another set of linear dynamical parameters.",
                    "label": 0
                },
                {
                    "sent": "You see the parameters are indexed by disease, so we have a prior over generating.",
                    "label": 0
                },
                {
                    "sent": "These matrices, one for each state.",
                    "label": 0
                },
                {
                    "sent": "And those will describe the dynamics.",
                    "label": 0
                },
                {
                    "sent": "This is like, you know, an RN, except everything is linear and we're injecting Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "We can do all kinds of interesting things if we want to interpret these matrices after we fit them in terms of the time scale information, we just look at the spectral content of these a matrices from the priors that have to do time scales.",
                    "label": 0
                },
                {
                    "sent": "We can do that nicely as well.",
                    "label": 0
                },
                {
                    "sent": "So let me just clean this up and write it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a actual graphical model, so I'm saying that there's this discrete states that plugs into a continuous state that drives its dynamics.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just collect all the parameters into a single symbol Theta and write the graphical model like this.",
                    "label": 0
                },
                {
                    "sent": "So then the last thing to do is have some way of producing images right?",
                    "label": 0
                },
                {
                    "sent": "So every low dimensional code if you will.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to generate a.",
                    "label": 0
                },
                {
                    "sent": "Mouse image from it.",
                    "label": 0
                },
                {
                    "sent": "So this is where we maybe like to use some deep learning technology.",
                    "label": 0
                },
                {
                    "sent": "So in particular I will clear off the bottom of the screen to make.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Room",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also draw this as a graph and the basic idea is to use the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is decoder type models that I'm sure you've heard of.",
                    "label": 0
                },
                {
                    "sent": "In particular, this is like a density network that would fit from a very autoencoder.",
                    "label": 0
                },
                {
                    "sent": "This just says let's write our image are high dimensional vector like our pixels, maybe 6400 mental image condition on the latent code and some neural net parameters gamma.",
                    "label": 0
                },
                {
                    "sent": "We just want to say that comes from a Gaussian distribution where the mean is some rich nonlinear function of the latent code X and maybe so is the covariance.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you want an image manifold, mu is kind of like the mapping from our coordinates to the points on the image manifold.",
                    "label": 0
                },
                {
                    "sent": "Question, yeah.",
                    "label": 0
                },
                {
                    "sent": "How do you find the boundaries?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we're going to learn that from scratch, so we're just writing a prior that says there is some transition matrix from some prior.",
                    "label": 0
                },
                {
                    "sent": "It could even be a Bayesian nonparametric prior.",
                    "label": 0
                },
                {
                    "sent": "You can do all your hierarchical Dirichlet processes as I did Once Upon a time, so you can have an unbounded number of States and then you just say they follow that transition matrix and sample.",
                    "label": 0
                },
                {
                    "sent": "Sample states from it, so we're going to learn that segmentation automatically in an unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "If you want to be semi supervised like you want to have some hand labels which does not work well for this problem, by the way, like people have tried to make hand labeled datasets, but they can only train humans to recognize a few different classes, like a few different behaviors because the mice moved kind of fast and humans aren't very high throughput machines.",
                    "label": 0
                },
                {
                    "sent": "Also they have very low agreement if you ask two different human labelers that you train the same way you ask them to label new videos.",
                    "label": 0
                },
                {
                    "sent": "Very low agreement.",
                    "label": 0
                },
                {
                    "sent": "So you could be semi supervised here if you want to.",
                    "label": 0
                },
                {
                    "sent": "But we're going to talk about totally unsupervised mode.",
                    "label": 0
                },
                {
                    "sent": "Also, one other thing that I've started to think more and more is important.",
                    "label": 0
                },
                {
                    "sent": "These states were just were generated in an open loop.",
                    "label": 0
                },
                {
                    "sent": "They didn't depend on what the mouses configuration was or any external environment variables.",
                    "label": 0
                },
                {
                    "sent": "But with graphical models, in principle at least it might break some of our inference, but we can draw edges from X1 to Z2 and X2 to Z3 to say oh, the mouse might switch its behavior based on what config configuration its body is in, or if we want to have covariates that we list upstairs is kind of like inputs plugging into our transition matrices or into the dynamics, we could do all of that.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a relatively simple model that I think would be interesting to embellish.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Why have to be random number 2 exquisite lattice domestic from Mexico?",
                    "label": 0
                },
                {
                    "sent": "So the question is why is why random?",
                    "label": 0
                },
                {
                    "sent": "I guess there are a few different answers.",
                    "label": 0
                },
                {
                    "sent": "It's sort of the usual answer for why do we model high dimensional things as noisy versions of low dimensional things?",
                    "label": 0
                },
                {
                    "sent": "It's because we don't expect the high dimensional objects to lie exactly on a manifold.",
                    "label": 0
                },
                {
                    "sent": "We're sort of saying that to do dimensionality reduction we say they live near it.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that you know because of noise in the sensors or the fact that our model is wrong.",
                    "label": 0
                },
                {
                    "sent": "We have to sort of say that some things are unexplained in our model, and the way we do that is we just say that it's noise that we don't care bout modeling further.",
                    "label": 0
                },
                {
                    "sent": "So some of it is just sort of like having slop in our model noise term.",
                    "label": 0
                },
                {
                    "sent": "Another way to think about it is that we want this thing to define a density over all possible images, because we want to be able to get any image and maybe ask questions about what the probability is under that.",
                    "label": 0
                },
                {
                    "sent": "And we don't want the answer to almost always be 0.",
                    "label": 0
                },
                {
                    "sent": "So you know whenever we want to have a density on output space, we usually have an explicit model for some low dimensional part and then some some way to make that have.",
                    "label": 0
                },
                {
                    "sent": "To be absolutely continuous with respect to the big measure on the output space.",
                    "label": 0
                },
                {
                    "sent": "So just the usual reasons.",
                    "label": 0
                },
                {
                    "sent": "So last thing on this I'm going to draw a box because this is a model for one video sequence, but in fact we have many video sequences.",
                    "label": 0
                },
                {
                    "sent": "We had like 3 million or 20,000,000 frames or something.",
                    "label": 0
                },
                {
                    "sent": "I don't even remember.",
                    "label": 0
                },
                {
                    "sent": "We also want to get this thing at some scale.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of for this particular application, by the way, I would call this a switching linear dynamical system.",
                    "label": 0
                },
                {
                    "sent": "My favorite graphical model.",
                    "label": 0
                },
                {
                    "sent": "It includes a special cases many things that you might like, like PCA, isn't there somewhere.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to draw a box to represent the repeated.",
                    "label": 0
                },
                {
                    "sent": "The fact that you know there are many video sequences that were fitting.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was here's a less colorful switching linear dynamical system model with nonlinear observations.",
                    "label": 0
                },
                {
                    "sent": "I want to abstract abit.",
                    "label": 0
                },
                {
                    "sent": "This is 1 example and to make the notation simple I'm going to stop tracking some details even though they're still there under the hood.",
                    "label": 0
                },
                {
                    "sent": "So in particular let's collapse out time.",
                    "label": 0
                },
                {
                    "sent": "So we're just thinking of the sequences themselves.",
                    "label": 0
                },
                {
                    "sent": "This is just for rotational purposes and then also let's collapse all of our latent variables into one.",
                    "label": 0
                },
                {
                    "sent": "So when I draw this notation, I mean that this structure is still there, but we're not interested in tracking it right now, but there's all kinds of interesting structure going on in our latent variable model, so the point of doing this is to really sort of generalize abstract things.",
                    "label": 0
                },
                {
                    "sent": "Draw model like this one that I want to think of in four parts.",
                    "label": 0
                },
                {
                    "sent": "So in the sort of top half, we're going to have the nice latent variable models an what nice means to me is.",
                    "label": 0
                },
                {
                    "sent": "We're going to have some exponential family on local variables.",
                    "label": 1
                },
                {
                    "sent": "That is nice in some way that I can make precise later, but I'll just say that it means it's built out of tractable exponential families.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have a conjugate prior on the global variables.",
                    "label": 1
                },
                {
                    "sent": "These are sort of the parameters for our mouse dynamics.",
                    "label": 0
                },
                {
                    "sent": "And then these are the actual mouse configurations for a particular video, the local invariables.",
                    "label": 0
                },
                {
                    "sent": "So that's the top half and then the bottom half.",
                    "label": 1
                },
                {
                    "sent": "The top half is like the very idealized simple modeling land latent variables and the bottom half is how we connected up to complicated data.",
                    "label": 0
                },
                {
                    "sent": "So in particular, we want to think of having a neural network observation model, and we could have a prior on the observation parameters.",
                    "label": 0
                },
                {
                    "sent": "But really I think we're going to just do sort of variational EM and fix those two points and just maximize them.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this?",
                    "label": 0
                },
                {
                    "sent": "OK, so this pattern, yeah.",
                    "label": 0
                },
                {
                    "sent": "Why are you looking right?",
                    "label": 0
                },
                {
                    "sent": "Are you keeping track of procedure?",
                    "label": 0
                },
                {
                    "sent": "I am I'm just I'm just hiding them from the notation.",
                    "label": 0
                },
                {
                    "sent": "Because for this example we want to have these, but actually the next slide is to say that this scheme where we just have some latent variables.",
                    "label": 0
                },
                {
                    "sent": "There could be all kinds of stuff going on, like having discrete chains and continuous chains.",
                    "label": 0
                },
                {
                    "sent": "I want to abstract and say the things that I'm going to tell you about.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applied to a lot more than just this example, so in fact here a bunch of graphical models.",
                    "label": 0
                },
                {
                    "sent": "Each one is sort of interesting, and some of them have inputs, right?",
                    "label": 0
                },
                {
                    "sent": "Some of them are like mixtures of experts or complicated driven dynamical systems.",
                    "label": 0
                },
                {
                    "sent": "Or you know, hmm, that I think Joshua worked on originally, or like you know, some kind of LDA.",
                    "label": 0
                },
                {
                    "sent": "Basically there used to be at these machine learning conferences.",
                    "label": 0
                },
                {
                    "sent": "They were filled with like people developing specific structured models and then inference algorithms that would exploit all of that structure in the model.",
                    "label": 0
                },
                {
                    "sent": "And these things would include interesting like hypothesis about the structure of your data, and correspondingly it would be very very fast.",
                    "label": 0
                },
                {
                    "sent": "Algorithms for doing inference.",
                    "label": 0
                },
                {
                    "sent": "So I claim that there are a lot of graphical models sitting there that are sort of untapped when we do a lot of deep learning research.",
                    "label": 0
                },
                {
                    "sent": "But the framework I'm going to describe to you is going to be able to use all of these, right?",
                    "label": 0
                },
                {
                    "sent": "So any of these things you can plug them in, not only the model which is interesting, but more importantly, the inference.",
                    "label": 0
                },
                {
                    "sent": "So the inference algorithms that you've seen before for anything like LDA, we can use all of those cool inference techniques.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inference, that's really the big question.",
                    "label": 0
                },
                {
                    "sent": "Like I said, anybody can write down a complicated model.",
                    "label": 0
                },
                {
                    "sent": "The question is, can you do effective inference in it?",
                    "label": 0
                },
                {
                    "sent": "And that's really the main story here.",
                    "label": 0
                },
                {
                    "sent": "So to tell you about how we're going to exploit structure and inference, I have to give you some idea.",
                    "label": 0
                },
                {
                    "sent": "First of what it means to exploit structure.",
                    "label": 0
                },
                {
                    "sent": "Now that we have these constrained parts to our model, what are we going to do with them?",
                    "label": 0
                },
                {
                    "sent": "So to explain that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a very so many pages of textbooks written on these things, I'm just going to give you high level view.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm going to tell you about natural gradient SPI, which was work done by Matt Hoffman who's now with me at Google Brain and followed up on a lot of subsequent works.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of when we have all this exponential family and maybe conjugacy structure.",
                    "label": 0
                },
                {
                    "sent": "What can we do with it if we're trying to do SVD style large scale training?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me tell you about a model.",
                    "label": 0
                },
                {
                    "sent": "Remember our models are going to have nonlinear neural network observation models and complexities.",
                    "label": 0
                },
                {
                    "sent": "But to tell you about how to exploit the structure, let's first investigate what we can do when we have this kind of model structure, and then we'll break it and see how much we can recover.",
                    "label": 0
                },
                {
                    "sent": "So here's our example model.",
                    "label": 0
                },
                {
                    "sent": "I don't even have switching here to make it really simple, so these taxes form a linear Gaussian linear dynamical system, right?",
                    "label": 1
                },
                {
                    "sent": "So each X is marginally Gaussian variable an we're just evolving these things according to linear Gaussian dynamics.",
                    "label": 0
                },
                {
                    "sent": "So apply linear matrix, add Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Simple dynamical system model.",
                    "label": 0
                },
                {
                    "sent": "More importantly, the observations were going to be linear in Gaussian, so sort of everything downstairs.",
                    "label": 0
                },
                {
                    "sent": "Everything in these chains is linear and Gaussian, and so we should expect to be able to do a lot of stuff.",
                    "label": 1
                },
                {
                    "sent": "And then I haven't spelled out the details, but the prior on Theta is going to be a conjugate prior, which is going to be sort of more more nice stuff that we can use so.",
                    "label": 0
                },
                {
                    "sent": "How do we do inference in this kind of model?",
                    "label": 0
                },
                {
                    "sent": "So when I say inference, I mean including inference over Theta.",
                    "label": 0
                },
                {
                    "sent": "So Bayesian inference kind of like learning fitting the parameters were going to be fitting some kind of a representation of a posterior not only over these latent states, but also over the parameter.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about variational inference, in particular structured variational inference and.",
                    "label": 0
                },
                {
                    "sent": "Who here is familiar with structured variational inference?",
                    "label": 0
                },
                {
                    "sent": "Everybody cool.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a lot more that can be said about this that I'm not going to go into.",
                    "label": 0
                },
                {
                    "sent": "This is going to be like a high level view.",
                    "label": 0
                },
                {
                    "sent": "So anyone read Kevin Murphy's book?",
                    "label": 0
                },
                {
                    "sent": "I have Kevin Murphy's book.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so Kevin Murphy's book is amazing.",
                    "label": 0
                },
                {
                    "sent": "Also, I think the best references this monograph by Wainwright and Jordan in 2008.",
                    "label": 0
                },
                {
                    "sent": "I've been praying on that since I think I started grad school.",
                    "label": 0
                },
                {
                    "sent": "They handed me that book and I've been like reading it over and over ever since.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of interesting like convex analysis and stuff that I'm not going to go into, but I will tell you the punch lines about how we can exploit this structure.",
                    "label": 0
                },
                {
                    "sent": "So we do variational inference.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we want to approximate this posterior over our parameter and our local latent variables X, and we're going to approximate it by factorizing.",
                    "label": 0
                },
                {
                    "sent": "So this is somehow difficult to deal with, but we can break it down into pieces and represent variational distribution, Theta and then independent variational distribution on our local latent variables.",
                    "label": 0
                },
                {
                    "sent": "And we're going to fit that thing to approximate the posterior.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as I'm sure you've seen it at various points to make to make our variational approximation try to approximate the posterior, we're going to maximize this objective, which corresponds to minimizing the KL divergent from our approximation to the posterior, because this thing plus the KL divergences are constant.",
                    "label": 0
                },
                {
                    "sent": "So actually, just by writing these little parentheses here as opposed to brackets, I've done something.",
                    "label": 0
                },
                {
                    "sent": "Interesting, so it turns out that just by saying that I'm going to factorize my variational posterior this way, it's already a fact that now we can choose these variational factors to look like the corresponding factors in the prior.",
                    "label": 0
                },
                {
                    "sent": "That's interesting because I'm saying the optimal form of these densities over all possible densities like satisfying the earlier LaGrange equations, is to choose these things to look like the prior.",
                    "label": 0
                },
                {
                    "sent": "OK, that was just the technical side, so we have this objective there right down, and we're going to parameterise these variational factors.",
                    "label": 0
                },
                {
                    "sent": "So look something like our prior and I'm going to write their natural parameters as Ada sub Theta.",
                    "label": 0
                },
                {
                    "sent": "That's the parameter for Q Theta and aid us of X.",
                    "label": 0
                },
                {
                    "sent": "That's the parameter for QX.",
                    "label": 0
                },
                {
                    "sent": "OK, so to do inference to to fit our model, we want to optimize this objective, which is just some number with an expectation that this is just some number over here.",
                    "label": 0
                },
                {
                    "sent": "Want to search over these parameters, right?",
                    "label": 0
                },
                {
                    "sent": "OK, now we're in the land of optimization and we know what we can do right?",
                    "label": 0
                },
                {
                    "sent": "Oh, you see an expectation list.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo, it don't worry.",
                    "label": 0
                },
                {
                    "sent": "And then do gradient descent over these parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the naive thing that we could do without exploiting any structure.",
                    "label": 0
                },
                {
                    "sent": "Turns out we can do a lot more in this setting.",
                    "label": 0
                },
                {
                    "sent": "So turns out we can actually optimize out.",
                    "label": 0
                },
                {
                    "sent": "One of our variational parameters completely so that is to say so.",
                    "label": 0
                },
                {
                    "sent": "Remember, 80 X was a variational parameter for all of our latent variables.",
                    "label": 0
                },
                {
                    "sent": "There could be a lot of local latent variables.",
                    "label": 0
                },
                {
                    "sent": "Sorry, our local in variables X.",
                    "label": 0
                },
                {
                    "sent": "We're going to optimize that out completely and just write a single argument function where we substitute in that optimizing value.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see now, I claim that we can do this optimization efficiently, and in fact we can get access to this function in a way we can get its derivatives.",
                    "label": 0
                },
                {
                    "sent": "So we've optimized out.",
                    "label": 0
                },
                {
                    "sent": "Probably the much larger variational parameter completely.",
                    "label": 0
                },
                {
                    "sent": "We don't even have to search over it using local search methods like like gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Well, at least not in the outer loop.",
                    "label": 0
                },
                {
                    "sent": "So this can be done super efficiently.",
                    "label": 0
                },
                {
                    "sent": "Now we have an objective that's much smaller dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Now we can do gradient descent on this thing, right?",
                    "label": 0
                },
                {
                    "sent": "It turns out you can still do something better.",
                    "label": 0
                },
                {
                    "sent": "Actually, one more point is that we can do this optimization.",
                    "label": 0
                },
                {
                    "sent": "Evaluating these expectations like the expectation over QX, exactly so we don't even have to Monte Carlo over X to perform this optimization.",
                    "label": 0
                },
                {
                    "sent": "That's also super cool.",
                    "label": 0
                },
                {
                    "sent": "It's an integral that we can actually do without just Monte Carlo and everything.",
                    "label": 0
                },
                {
                    "sent": "In fact, we can evaluate this thing doing exactly over over Theta as well.",
                    "label": 0
                },
                {
                    "sent": "So we have this objective now.",
                    "label": 0
                },
                {
                    "sent": "We've optimized out some of the parameters perfectly.",
                    "label": 0
                },
                {
                    "sent": "Can you do more?",
                    "label": 1
                },
                {
                    "sent": "Indeed we can.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can write the natural gradient of this objective.",
                    "label": 0
                },
                {
                    "sent": "Computing all the integrals exactly on optimizing out your local latent variable factor, we can write that natural gradient as a very simple expression.",
                    "label": 0
                },
                {
                    "sent": "OK, so the details here are not super important with some notation to unpack.",
                    "label": 0
                },
                {
                    "sent": "Here's the big idea.",
                    "label": 0
                },
                {
                    "sent": "We could have done flat gradient descent on this objective that would have already been a big win because we're doing integrals exactly, and because we are optimizing out our local variational factors, we can do even better.",
                    "label": 0
                },
                {
                    "sent": "We can compute natural gradients, which are kind of sort of curvature corrected.",
                    "label": 0
                },
                {
                    "sent": "2nd order optimization.",
                    "label": 0
                },
                {
                    "sent": "Technique so we can compute these.",
                    "label": 0
                },
                {
                    "sent": "Another crazy thing is we can actually compute these faster than you can compute the flat gradient, so there's no backwards pass.",
                    "label": 0
                },
                {
                    "sent": "This expression is only in terms of things that we compute in the forward pass to evaluate the objective.",
                    "label": 0
                },
                {
                    "sent": "Wow, that's super cool right?",
                    "label": 0
                },
                {
                    "sent": "We used this very special structure in this very like crystalline restrictive structure, but we got a lot of mileage out of it in terms of an inference algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "One last point is that when we have many sequences, we can just Monte Carlo sample.",
                    "label": 0
                },
                {
                    "sent": "We write this sum and then we can Monte Carlo sampling in the usual way to scale this fitting to big datasets.",
                    "label": 0
                },
                {
                    "sent": "So what does that algorithm look like?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just show you what it looks like for these time series models.",
                    "label": 1
                },
                {
                    "sent": "Step one is we sample data mini batch, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe it's just one sequence of the sequence for modeling.",
                    "label": 0
                },
                {
                    "sent": "We then compute evidence potentials using our nice linear Gaussian observation model.",
                    "label": 1
                },
                {
                    "sent": "We can compute evidence potentials on our local invariables no problem.",
                    "label": 0
                },
                {
                    "sent": "Then we use those potentials that we see that we use those potentials and do fast.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Passing algorithms, so this is like a Kalman smoother details aren't important, but like your cell phones doing it all the time, these are super fast and it's the kind of thing that you're willing to support in inside your inference your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's 2 fast message passing to sort of aggregate the local information into a globally coherent inference over what the latent states were, and then step three is almost there is no step three.",
                    "label": 1
                },
                {
                    "sent": "It turns out that using this information we already computed, we can.",
                    "label": 1
                },
                {
                    "sent": "We can compute a simple expression for natural gradient on the variational.",
                    "label": 0
                },
                {
                    "sent": "The variational parameters for this very often organize so natural gradient SPI in these very nice models, where you had all kinds of specific restricted structure.",
                    "label": 0
                },
                {
                    "sent": "Tons of algorithmic tools that we got out of it.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One other and maybe even the most important idea is.",
                    "label": 0
                },
                {
                    "sent": "Doing this kind of inference, we can support arbitrary inference queries, so if you give me any missing data pattern right because I have a template for how to do inference which is through message passing, right?",
                    "label": 0
                },
                {
                    "sent": "You give me any pattern of observations or non observations and I can do inferences over what was going on.",
                    "label": 0
                },
                {
                    "sent": "When we look at things like inference networks right?",
                    "label": 0
                },
                {
                    "sent": "In fact, those things essentially only support a limited number of inference queries.",
                    "label": 0
                },
                {
                    "sent": "So if you have an RNN for example, maybe it supports linearly many inference queries.",
                    "label": 0
                },
                {
                    "sent": "You do inference over the sequence, but if I give you an arbitrary missing data pattern, you might have to train a new inference network to handle that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That was what we can do in the nice case, but it really relied on all kinds of specific special model structure.",
                    "label": 0
                },
                {
                    "sent": "And of course as soon as we break that structure a little bit, things are going horribly wrong.",
                    "label": 0
                },
                {
                    "sent": "So let's see what goes wrong.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine having again the same prior on latent variables, so it's still nice and structured upstairs, but then we have this neural network decoder model.",
                    "label": 0
                },
                {
                    "sent": "So maybe we're modeling a video using latent linear dynamical system, but then nonlinear neural network observation model.",
                    "label": 1
                },
                {
                    "sent": "So let's try to do what we did before setup of factorized posterior.",
                    "label": 0
                },
                {
                    "sent": "In this case we don't have, we know the optimal forms of these things to parameterise, but we're just going to say anyway, let's make you avexa.",
                    "label": 0
                },
                {
                    "sent": "Gaussian seems reasonable and then we can write down the objective we did before.",
                    "label": 1
                },
                {
                    "sent": "And the problem now is that we have this likelihood term from our neural network decoder model an it does not play nicely with the other terms.",
                    "label": 0
                },
                {
                    "sent": "So because of the linear Gaussian observations in the other case, we could actually take this term which I've drawn in red here and fold it back into the prior.",
                    "label": 0
                },
                {
                    "sent": "So somehow, by conditioning on this special kind of evidence, we could just put it back in the prior.",
                    "label": 0
                },
                {
                    "sent": "Nothing became complicated and we stayed in our nice structured prior.",
                    "label": 0
                },
                {
                    "sent": "We can do all kinds of inference tricks, But here now we've got these nonlinear observations.",
                    "label": 0
                },
                {
                    "sent": "They've totally changed all of our assumptions about what this prior looks like.",
                    "label": 0
                },
                {
                    "sent": "The prior was nice.",
                    "label": 0
                },
                {
                    "sent": "We condition on complicated evidence.",
                    "label": 0
                },
                {
                    "sent": "Now we have a complicated posterior.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if we tried to write down what the optimal local factor is, right?",
                    "label": 0
                },
                {
                    "sent": "We tried to do this argmax over R. Maxing over our local variational parameters afex, this is now something hard to do.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a sort of very fast inference, things like message passing to compute this.",
                    "label": 0
                },
                {
                    "sent": "Max and get the information we need out of it.",
                    "label": 0
                },
                {
                    "sent": "Now we'd be stuck with a sort of nonlinear optimization problem, right?",
                    "label": 0
                },
                {
                    "sent": "If you want to fit Q of X like, choose cubex to maximize this objective.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty generic smooth nonlinear programming problem.",
                    "label": 1
                },
                {
                    "sent": "First, you have to do Monte Carlo over Q of X to even evaluate the objective, and then you have to do a bunch of generic gradient descent or something.",
                    "label": 0
                },
                {
                    "sent": "This is a huge problem because this thing to do inference over local in variables that would be in our inner loop.",
                    "label": 0
                },
                {
                    "sent": "So if we try to form this SPI objective which were free to define this SPI objective now is very difficult to compute because when we sample the data mini batch instead of just having something very fast and then being able to compute gradient with respect to other parameters.",
                    "label": 0
                },
                {
                    "sent": "Now when we sample data many data mini batch we have to do some expensive non linear programming problem in our inner loop, run it to convergence and then do an update.",
                    "label": 0
                },
                {
                    "sent": "So this basically breaks the entire SVI picture.",
                    "label": 0
                },
                {
                    "sent": "And so you know, as far as I know, we just sort of didn't try to do things like this, or at least I didn't try to do things like this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until there were some really interesting advances, in particular from this pair of papers that came out at about the same time, by Kingman, Max Welling and also resented Mohammed and this amortized inference paper, there was a big idea for a way to still come up with a good factor on our local link variables, but efficiently so these are various autumn colors and amortized inference.",
                    "label": 0
                },
                {
                    "sent": "This is a big idea for handling sort of nonlinear observations.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the sort of variational autoencoder model.",
                    "label": 0
                },
                {
                    "sent": "The initial one.",
                    "label": 0
                },
                {
                    "sent": "Just think of the of course have been developed in a lot of work since the original paper, but in this case, let's just think of a simple prior on our latent variables.",
                    "label": 0
                },
                {
                    "sent": "The X ends just in IID Gaussian, very simple, and the entire complexity of the model is in the fact that we're pushing that forward to be a complicated density by using a neural network observation model.",
                    "label": 0
                },
                {
                    "sent": "So what if we try to do something like stochastic variational inference too?",
                    "label": 0
                },
                {
                    "sent": "Maximize over these gamma parameters.",
                    "label": 0
                },
                {
                    "sent": "We have the same problem where we sampled data mini batch to do our local variable.",
                    "label": 0
                },
                {
                    "sent": "Inference is we have to solve a generic non linear programming problem just to compute our outer loop update on our parameters.",
                    "label": 0
                },
                {
                    "sent": "But the insight of the various autoencoder work was instead of trying to run some iterative optimization algorithm that could be super expensive in our inner loop, why don't we just parameterize some function to take in the data mini batch?",
                    "label": 0
                },
                {
                    "sent": "We sample and spit out a variational factor.",
                    "label": 0
                },
                {
                    "sent": "The parameters of a variational factor for us, right?",
                    "label": 0
                },
                {
                    "sent": "This thing might be just a fixed depth circuit, right?",
                    "label": 0
                },
                {
                    "sent": "It's a five layer neural network or something like this and it's just going to take in data and spit out variational parameters.",
                    "label": 0
                },
                {
                    "sent": "And the insight was that we can train that thing.",
                    "label": 0
                },
                {
                    "sent": "We can train the parameters of that function approximator in the outer loop.",
                    "label": 0
                },
                {
                    "sent": "Right, so we can search over those things and by doing that we're going to be learning and inference.",
                    "label": 0
                },
                {
                    "sent": "Network learning a network that does our inference for us, and so in particular.",
                    "label": 0
                },
                {
                    "sent": "If we define, we just define our local inference is not as the optimal inference is solving this optimization problem, but just the output of some neural network with some parameters Phi.",
                    "label": 0
                },
                {
                    "sent": "And then we say yes, and you know it.",
                    "label": 0
                },
                {
                    "sent": "Sort of looks kind of like the other network did where we take in Hawaiian output variational parameters on our latent variables.",
                    "label": 0
                },
                {
                    "sent": "Then we can define this variational autoencoder objective, which just looks like saying taking our original elbow and then plugging in.",
                    "label": 0
                },
                {
                    "sent": "These aren't the optimal local parameters anymore, we're not sort of searching over them perfectly, but these are just the output of our recognition network.",
                    "label": 0
                },
                {
                    "sent": "So this means that our local inference now is fast.",
                    "label": 0
                },
                {
                    "sent": "It's OK to run in our inner loop again, right?",
                    "label": 0
                },
                {
                    "sent": "Sample Data mini batch applying inference network?",
                    "label": 0
                },
                {
                    "sent": "That's just a few.",
                    "label": 0
                },
                {
                    "sent": "You know, Matt moves cons.",
                    "label": 0
                },
                {
                    "sent": "This sort of thing.",
                    "label": 0
                },
                {
                    "sent": "Then we have our local inferences so we can take another step on our parameter.",
                    "label": 0
                },
                {
                    "sent": "That's good.",
                    "label": 0
                },
                {
                    "sent": "No more inner loop optimization.",
                    "label": 0
                },
                {
                    "sent": "These are super cool and David you know talked about these in and Lt. Like looking at my graphical model problems and then looking at the various autumn colors like that's amazing.",
                    "label": 0
                },
                {
                    "sent": "Why don't I take this and put it in there?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to say a bit about some of the other work that's come around on inference networks that are adapted to time series, so there are other things people have done that I won't go into detail about in this talk to combine sort of ideas from graph models with various automotive.",
                    "label": 0
                },
                {
                    "sent": "So one line of work is from some folks at Columbia, like Evan Archer, and basically the idea was let's have a sort of CRF kind of recognition model so that instead of.",
                    "label": 0
                },
                {
                    "sent": "Taking our entire sequence, if we have time series, let's apply recognition networks just to get local information and then sort of like stitch it together.",
                    "label": 0
                },
                {
                    "sent": "This is quite similar to the framework that I'm this is, I'd say, an instance of the framework that I'm going to describe to you, though not fully, is general, so that's a very interesting line of work.",
                    "label": 0
                },
                {
                    "sent": "There's other work by David Sontag and students on sort of using.",
                    "label": 0
                },
                {
                    "sent": "Let's use the fact that we have time series and make a bidirectional RNN that kind of looks like message passing to do our non linear inferences.",
                    "label": 0
                },
                {
                    "sent": "Super interesting work.",
                    "label": 0
                },
                {
                    "sent": "You should definitely check that out.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me just summarize the big picture.",
                    "label": 0
                },
                {
                    "sent": "We had a natural gradient SPI for these really special exponential family models.",
                    "label": 0
                },
                {
                    "sent": "Very restricted they can't model a whole lot, but when you had them you can exploit all that mathematical structure.",
                    "label": 0
                },
                {
                    "sent": "So in particular these things became.",
                    "label": 0
                },
                {
                    "sent": "This SVI became very expensive for general observation models, but it was nice while we had it because we got the optimal local factor.",
                    "label": 1
                },
                {
                    "sent": "If we can perform the optimization efficiently, was able to exploit all of our graph structure in our project structure to do that optimization.",
                    "label": 1
                },
                {
                    "sent": "I was able to support arbitrary missing data patterns, and of course we got natural gradients essentially for free.",
                    "label": 0
                },
                {
                    "sent": "But then we talk about these various auto encoders, which were super awesome because they handle general observation models.",
                    "label": 0
                },
                {
                    "sent": "And there's still quite fast, so that's a huge win.",
                    "label": 0
                },
                {
                    "sent": "And that's why we use them all the time just to say a bit about what I think there's autoencoders can lack is, of course, when we have an inference network instead of solving optimization problem for inference, we're just getting the output of some network.",
                    "label": 0
                },
                {
                    "sent": "Approximator that means that we are doing suboptimal local inference.",
                    "label": 1
                },
                {
                    "sent": "We also have to learn our network has to do all of our inference for us right?",
                    "label": 0
                },
                {
                    "sent": "And so it has to learn how to do message passing, right?",
                    "label": 0
                },
                {
                    "sent": "It has to learn how to propagate information from one into the other.",
                    "label": 0
                },
                {
                    "sent": "Latent variable models become more complex.",
                    "label": 0
                },
                {
                    "sent": "That's going to be harder and harder.",
                    "label": 0
                },
                {
                    "sent": "Maybe you believe that deep learning can solve everything and learn how to like approximate any function, even if it's got matrix inversion and all kinds of stuff that it should be doing.",
                    "label": 0
                },
                {
                    "sent": "That may be true, but at the very least it will be data intensive right?",
                    "label": 0
                },
                {
                    "sent": "At the very least, if you're having to learn how to do all your inference from scratch, it's going to take a lot of data to train that inference network, whereas if you're in the case where you want to write down model structure in your latent variables, maybe.",
                    "label": 0
                },
                {
                    "sent": "You know it would be better to be able to use that instead of learning from scratch how to do all that information aggregation.",
                    "label": 1
                },
                {
                    "sent": "Also, as I said, when you have function approximators, but then you're missing some of the inputs your function needs, you might not be able to answer some inference queries.",
                    "label": 0
                },
                {
                    "sent": "And of course you know this small technical detail of hey we used to get natural gradients another slightly worse.",
                    "label": 0
                },
                {
                    "sent": "So of course, as you might infer from me, setting up this plot, I'm going to tell you about something that we worked on building on both of these lines of work really.",
                    "label": 1
                },
                {
                    "sent": "Just, you know, trying to put them together as best we can.",
                    "label": 0
                },
                {
                    "sent": "We, we called this sort of model an inference framework, structured variational encoders structured in the sense of structured variational inference from graphical models.",
                    "label": 0
                },
                {
                    "sent": "And so the idea of this chocolate and peanut butter set up is that we wanted still to be quite fast for general observation models, right?",
                    "label": 0
                },
                {
                    "sent": "Like no one wants to model their data with linear observations, linear sensations from linear Gaussian dynamics.",
                    "label": 0
                },
                {
                    "sent": "But we don't have general observation models, neural network observation models.",
                    "label": 1
                },
                {
                    "sent": "We also wanted to say more about later we wanted to be able to sort of optimize as best we can.",
                    "label": 0
                },
                {
                    "sent": "What are local variational inference objective was and make that fast by exploiting all of the config structure, reusing all those inference algorithms that people published at NIPS and Icml's past, and of course support arbitrary inference queries?",
                    "label": 0
                },
                {
                    "sent": "So this is, you know what I claim we're trying to do, and I think we can do.",
                    "label": 0
                },
                {
                    "sent": "Any questions on?",
                    "label": 0
                },
                {
                    "sent": "On this.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we put these things together?",
                    "label": 0
                },
                {
                    "sent": "These inference tools are developed for you, so I said it before.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is to have recognition networks, but instead of them doing all of our local variable inference for us.",
                    "label": 0
                },
                {
                    "sent": "Just solve part of the problem right?",
                    "label": 0
                },
                {
                    "sent": "Kind of like a CRF would have recognition.",
                    "label": 0
                },
                {
                    "sent": "Networks output conjugate graphical model potentials and then apply all the fast graphical model inference algorithms that you can in your latent variable model.",
                    "label": 1
                },
                {
                    "sent": "That's the idea so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you a bit of detail on how that works.",
                    "label": 0
                },
                {
                    "sent": "Again, we have this sort of setup where you know we have a nice latent variable model and ice prior to this tough term, right?",
                    "label": 0
                },
                {
                    "sent": "This observation likelihood term that we don't like very much.",
                    "label": 0
                },
                {
                    "sent": "Hi, what should we do about it?",
                    "label": 0
                },
                {
                    "sent": "Well, let's just replace it.",
                    "label": 0
                },
                {
                    "sent": "Never mind, that's just shifted over a little bit.",
                    "label": 0
                },
                {
                    "sent": "We should replace it.",
                    "label": 0
                },
                {
                    "sent": "So here's this complicated term for every time in our time series.",
                    "label": 0
                },
                {
                    "sent": "Maybe right, we have this evidence potential that's saying, like, oh, that's the image.",
                    "label": 0
                },
                {
                    "sent": "I have a lot of complicated beliefs about what the mouses pose was given that image.",
                    "label": 0
                },
                {
                    "sent": "Let's replace those difficult terms with a function approximator, right?",
                    "label": 0
                },
                {
                    "sent": "So just take those difficult likely terms and replace them with function approximators and let's structure those function approximators so that they give us conjugate potentials for our prior.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, what that means is we're going to approximate complicated.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn to approximate complicated evidence potentials by a function that takes in our data and then spits out a Gaussian local inference of Gaussian evidence potential.",
                    "label": 0
                },
                {
                    "sent": "Or in general, a conjugate evidence potential.",
                    "label": 0
                },
                {
                    "sent": "So idea is complicated evidence, but kind of analogous to how invariant autoencoders we just spit out the inference on the link variables.",
                    "label": 0
                },
                {
                    "sent": "And it's just some Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Here we're doing that, but just locally were saying I.",
                    "label": 0
                },
                {
                    "sent": "If that was the frame of video from the mouse, I would guess that it poses this, and my guess is the form of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But then we're going to stitch together all those guesses and synthesize that information with our prior using graphical model technology enabled by the fact that this is a conjugate potential.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What's different in this verse is within your game.",
                    "label": 0
                },
                {
                    "sent": "Before the period I thought I kind of thought this is what you're already doing, I guess.",
                    "label": 0
                },
                {
                    "sent": "So it depends on what you're asking, but in vanilla, so in sort of standard variational auto encoder's.",
                    "label": 0
                },
                {
                    "sent": "The I mean specifically the you have the slide like 2 slides going there was.",
                    "label": 0
                },
                {
                    "sent": "And then there was using the, and now there's a structured VSO, yes.",
                    "label": 0
                },
                {
                    "sent": "In this kind of yeah, that's what this is.",
                    "label": 0
                },
                {
                    "sent": "This is just details about it, so this is the structured via setup, so there's just the FBI and the structure.",
                    "label": 0
                },
                {
                    "sent": "Option here.",
                    "label": 0
                },
                {
                    "sent": "So question is, is there some third option?",
                    "label": 0
                },
                {
                    "sent": "So here are the two inputs, sort of to work.",
                    "label": 0
                },
                {
                    "sent": "Sure this this.",
                    "label": 0
                },
                {
                    "sent": "You want this slide?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "OK in the middle, let's just yeah, just via is right.",
                    "label": 0
                },
                {
                    "sent": "So here's like natural gradient Sky that was not in deep learning land so much, but it was in like exponential family graph, model end and then various auto coders.",
                    "label": 0
                },
                {
                    "sent": "Super cool with inference networks that do all of our local inference with deep learning and then structured VSR trying to do the chocolate peanut butter thing where we're going to put these two together.",
                    "label": 0
                },
                {
                    "sent": "So this is our work and it's trying to put together very autumn colors and ideas from this world.",
                    "label": 0
                },
                {
                    "sent": "Local.",
                    "label": 0
                },
                {
                    "sent": "I mean, in some sense we're trying to do inference over these guys and these guys, so these are local local in the sense that there are extensive variables like for every mouse video it gets a new copy of all these variables and then there's like global variables which apply to all the mouse videos.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The big idea is just approximating complicated evidence potentials.",
                    "label": 0
                },
                {
                    "sent": "Learning to approximate them with conjugate nice ones.",
                    "label": 0
                },
                {
                    "sent": "Then we can essentially set up a similar setup we had before where we can optimize out against our approximate objective and then proceed.",
                    "label": 0
                },
                {
                    "sent": "So let me give you an idea of what this algorithm looks like.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see when do we end?",
                    "label": 0
                },
                {
                    "sent": "I think it's been an hour alright.",
                    "label": 0
                },
                {
                    "sent": "So we ended like we have 30 minutes left cool, so here's what the algorithm looks like.",
                    "label": 0
                },
                {
                    "sent": "Step one.",
                    "label": 0
                },
                {
                    "sent": "We sample Ridata mini batch.",
                    "label": 0
                },
                {
                    "sent": "There it is now.",
                    "label": 0
                },
                {
                    "sent": "Instead of using our observation models, this is where we apply our recognition networks.",
                    "label": 0
                },
                {
                    "sent": "So we apply our recognition network to get node potentials.",
                    "label": 0
                },
                {
                    "sent": "This is very much like a conditional random field.",
                    "label": 0
                },
                {
                    "sent": "So then the next step, which is not so much like a conditional random field, is we're going to run our fast PGM inference algorithms like message passing.",
                    "label": 0
                },
                {
                    "sent": "Sort of fast proximal operators if you will to.",
                    "label": 0
                },
                {
                    "sent": "Optimize out all of the other things using this conjugate evidence as a surrogate, so we've made a circuit with this conjugate.",
                    "label": 0
                },
                {
                    "sent": "It's not the real evidence, but we're still optimizing out against that evidence to sort of synthesize together all of those local information together with each other and through all of our latent variables.",
                    "label": 0
                },
                {
                    "sent": "So this is the step that's quite different from fairy tale autoencoders in that instead of sort of feedforward computation, this is an optimization.",
                    "label": 0
                },
                {
                    "sent": "So if you guys were here for my.",
                    "label": 0
                },
                {
                    "sent": "Earlier talk where I talked about differentiating through optimization.",
                    "label": 0
                },
                {
                    "sent": "This is what got me into doing that was I was doing optimization of, you know, to do various elements in graphical models and then I wanted to differentiate through it to fit the parameters of the of the inference network of the recognition at work.",
                    "label": 0
                },
                {
                    "sent": "So step one apply recognition or to get potentials Step 2 fast.",
                    "label": 1
                },
                {
                    "sent": "PGM inference algorithms Step 3.",
                    "label": 1
                },
                {
                    "sent": "Is that we can sample and compute flat gradients.",
                    "label": 0
                },
                {
                    "sent": "So I just mean gradients of the neural network parameters and those are not curvature corrected in any way and then Step 4 is we can use the information that we've already computed to get a cheap estimate of the natural gradient with respect to the upstairs nice parameters.",
                    "label": 0
                },
                {
                    "sent": "Questions yeah, something different here than in the FBI.",
                    "label": 0
                },
                {
                    "sent": "Yes, the difference with SPI.",
                    "label": 0
                },
                {
                    "sent": "So in some ways this is so that's why I described to you only applied to when you had conjugate observation models, so you couldn't handle neural network observations.",
                    "label": 0
                },
                {
                    "sent": "After you've done step one, yes, right in step one, we use recognition works, so step one was like the VE stuff.",
                    "label": 0
                },
                {
                    "sent": "So take recognition works will be no and that's what we want.",
                    "label": 0
                },
                {
                    "sent": "So actually it is so the question was, is it different after step one from SV?",
                    "label": 0
                },
                {
                    "sent": "I know this step is exactly what you doing SPI.",
                    "label": 0
                },
                {
                    "sent": "In SBI.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't have to sample these things.",
                    "label": 0
                },
                {
                    "sent": "Because you don't have to do the re parameterisation trick, you can integrate out the you can compute the expectations you have in the objective exactly.",
                    "label": 0
                },
                {
                    "sent": "This is a subtle technical point, but somehow the reason we have to sample the reason we have to Monte Carlo is because we can't compute the integrals that we need against the nonlinear observation models.",
                    "label": 0
                },
                {
                    "sent": "So somehow in SVI we never sampled, we only sampled data, but we never sampled expectations over variational factors in various autoencoders.",
                    "label": 0
                },
                {
                    "sent": "We always sample whenever we see an expectation for variational factors like.",
                    "label": 0
                },
                {
                    "sent": "Replace that with a sample and we're let's roll.",
                    "label": 0
                },
                {
                    "sent": "So this step three is different in that way.",
                    "label": 0
                },
                {
                    "sent": "Cool, let me show you some fun videos of this working.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First on a couple simple problems because I think they sort of illustrate the idea pretty well.",
                    "label": 0
                },
                {
                    "sent": "So this is, uh, the data from that warp mixture model that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "Over here is the data space.",
                    "label": 0
                },
                {
                    "sent": "This is a view of the warps data space, and here is a latent space, so I made it a 2 dimensional latent space, and I initialized the mapping to be the identity or close to the identity.",
                    "label": 0
                },
                {
                    "sent": "That's why the data points look the same, but this is the view from the latent space.",
                    "label": 0
                },
                {
                    "sent": "You can see all of our Gaussian mixture model clusters.",
                    "label": 0
                },
                {
                    "sent": "This is the data space and you can see these densities here.",
                    "label": 0
                },
                {
                    "sent": "So I was using.",
                    "label": 0
                },
                {
                    "sent": "The prior on the latent space is something like learning a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "It's using a Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "Prior on the weights of the mixture model, and so it's going to prune out.",
                    "label": 0
                },
                {
                    "sent": "I gave it 30 or 50 or something components.",
                    "label": 0
                },
                {
                    "sent": "I think maybe just 20 or 30 here to make it not so messy.",
                    "label": 0
                },
                {
                    "sent": "But it's going to learn the number of components.",
                    "label": 0
                },
                {
                    "sent": "That's kind of a cool trick.",
                    "label": 0
                },
                {
                    "sent": "So over here we want to see it straighten out and start to look like data that can be fit with the Gaussian mixture model, right?",
                    "label": 0
                },
                {
                    "sent": "We're trying to learn features so that our Gaussian mixture model prior fits well and then over here we want to see it modeling the data accurately and learning a sort of structure representation with clusters.",
                    "label": 0
                },
                {
                    "sent": "So I'll hit play.",
                    "label": 0
                },
                {
                    "sent": "It's super quickly learns to prune out clusters, so basically fitting the Gaussian mixture model is easy relative to fitting these neural networks, and so it's really fast at doing that.",
                    "label": 0
                },
                {
                    "sent": "It turns out the clusters and start straighten them out quite quickly, and then it takes awhile to get the details right.",
                    "label": 0
                },
                {
                    "sent": "I think if I kept running this would probably continue to refine things, but these greens are different greens, though that seems impossible to see on the projector.",
                    "label": 0
                },
                {
                    "sent": "So this did what we wanted, right?",
                    "label": 0
                },
                {
                    "sent": "It's not only learned the density of the data pretty well, that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "That's certainly something we had before.",
                    "label": 0
                },
                {
                    "sent": "We fit and color to it, but more importantly, it's learned this clustering structure.",
                    "label": 0
                },
                {
                    "sent": "It's learned a representation that finds these clusters, and then even more interesting, it's learned how to map our complicated problem.",
                    "label": 0
                },
                {
                    "sent": "It's learned a transformation so that the problem looks like a Gaussian mixture model problem in the latent space.",
                    "label": 0
                },
                {
                    "sent": "Cool, here's an.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other nice simple example.",
                    "label": 0
                },
                {
                    "sent": "This is like this is like the simplest video modeling problem, so here's the idea.",
                    "label": 0
                },
                {
                    "sent": "This is the frame index and these images.",
                    "label": 0
                },
                {
                    "sent": "The frames of this video or 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So I've just stacked them up so each column.",
                    "label": 0
                },
                {
                    "sent": "Think of it like a frame in a video and so this is like a 1 dimensional video, a bouncing ball.",
                    "label": 0
                },
                {
                    "sent": "We can visualize it unrolling it quite nicely, right?",
                    "label": 0
                },
                {
                    "sent": "So this is frame index horizontally.",
                    "label": 1
                },
                {
                    "sent": "This is what the data looks like.",
                    "label": 0
                },
                {
                    "sent": "It's like a noisy pattern of a ball bouncing.",
                    "label": 0
                },
                {
                    "sent": "Here are the predictions is like the data reconstruction in the middle panel.",
                    "label": 0
                },
                {
                    "sent": "Right now it's initialized to pretty much junk.",
                    "label": 0
                },
                {
                    "sent": "And then here's the latent states down here.",
                    "label": 1
                },
                {
                    "sent": "So these are lower dimensional.",
                    "label": 0
                },
                {
                    "sent": "And they look like junk.",
                    "label": 0
                },
                {
                    "sent": "Right now they're just initialized somewhere, and then they fuzz out.",
                    "label": 0
                },
                {
                    "sent": "I'm conditioning the model on the data so it's trained on like 80 sequences.",
                    "label": 0
                },
                {
                    "sent": "That looks something like this, right?",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to hit play.",
                    "label": 0
                },
                {
                    "sent": "It's going to start running the training.",
                    "label": 0
                },
                {
                    "sent": "This is some held out test sequence and we're going to give it the data up to the red line, right?",
                    "label": 0
                },
                {
                    "sent": "So up until the red line in this predictions area it's solving a filtering problem where it's saying you've seen the data.",
                    "label": 0
                },
                {
                    "sent": "What do you think?",
                    "label": 0
                },
                {
                    "sent": "A cleaned up version of the data looks like solving to the left of the red line and then to the right of the redline?",
                    "label": 0
                },
                {
                    "sent": "It's doing a prediction, it doesn't get to see this data, it's just trying to predict out what it thinks the data will look like.",
                    "label": 0
                },
                {
                    "sent": "And then, correspondingly, you'll see the sort of the states that things are going on, and then the predictive states.",
                    "label": 0
                },
                {
                    "sent": "Because everything is Bayesian here, these different lines are a bunch of samples from the variational posterior so you can see the sort of uncertainty estimate that way, so play.",
                    "label": 0
                },
                {
                    "sent": "Did I imply?",
                    "label": 0
                },
                {
                    "sent": "Who play?",
                    "label": 0
                },
                {
                    "sent": "Super fun so.",
                    "label": 0
                },
                {
                    "sent": "What I think is interesting is that it goes through sort of two phases.",
                    "label": 0
                },
                {
                    "sent": "At first it learns how to maybe try to play one more time.",
                    "label": 0
                },
                {
                    "sent": "At first it tries to can I see?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll play one more time.",
                    "label": 0
                },
                {
                    "sent": "At first it sort of learns how to encode the data, but not predict very well.",
                    "label": 0
                },
                {
                    "sent": "But then it sort of refines it.",
                    "label": 0
                },
                {
                    "sent": "So I interpret that as saying it sort of learned the image manifold.",
                    "label": 0
                },
                {
                    "sent": "This is a just so story, so don't read too much into this, but this is consistent with sort of it learning the image manifold quickly.",
                    "label": 0
                },
                {
                    "sent": "It can also include the data, but it hasn't organized it in a way so that the data look like.",
                    "label": 0
                },
                {
                    "sent": "Can be predicted by linear dynamical system, but then it revises an you can actually see these states for more time.",
                    "label": 0
                },
                {
                    "sent": "These states sort of don't look like in LDS and then they pop into these sinusoidal things.",
                    "label": 0
                },
                {
                    "sent": "So linear dynamical system states in a generic basis will look like these sinusoidal patterns, like these harmonic oscillator type things.",
                    "label": 0
                },
                {
                    "sent": "So you can see that it's sort of now learn to make the latent space look like our prior.",
                    "label": 0
                },
                {
                    "sent": "It's describing this complicated image video modeling problem, not super complicated this toy, but.",
                    "label": 0
                },
                {
                    "sent": "As this nonlinear observation model, but it's learned to make the link space look like a linear dynamical system, and it's representing uncertainty nicely.",
                    "label": 0
                },
                {
                    "sent": "You can see we're really confident about what the States are here.",
                    "label": 0
                },
                {
                    "sent": "We've denoised it pretty much completely, but then as we get further from where we give it data, the predictions are less and less certain.",
                    "label": 0
                },
                {
                    "sent": "You can see it start to fuzz in the image, but it's actually pretty confident over a very, very long ranges.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a very simple experiment just on, I think that Doc data of justice running training with flat gradients versus with these natural gradient estimates that we get out of the algorithm and the elbow improves much faster if we use natural gradients.",
                    "label": 0
                },
                {
                    "sent": "Basically it means that the GMM or the LDS sort of latent variable part can be fit much faster if we use these things.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So also let me emphasize that this lets us do inference with arbitrary missing data patterns.",
                    "label": 0
                },
                {
                    "sent": "So before the difference with the slide before is, we had arrows pointing down here these are like CRF like arrows in that we have these recognition networks.",
                    "label": 0
                },
                {
                    "sent": "So if you give me a video the way I will infer what the latent states were was, I will apply to any frames you gave me.",
                    "label": 0
                },
                {
                    "sent": "I'll apply my recognition networks to get those local guesses as to what the configuration of the system was, and then do fast filtering or smoothing.",
                    "label": 0
                },
                {
                    "sent": "Basically message passing to stitch together those guesses.",
                    "label": 0
                },
                {
                    "sent": "So that's great.",
                    "label": 0
                },
                {
                    "sent": "We still have arbitrary inference queries.",
                    "label": 1
                },
                {
                    "sent": "I say see next slide because what is the next slide?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it depends on what your recognition network.",
                    "label": 0
                },
                {
                    "sent": "Architecture is for example, if you gave me a partially occluded frame.",
                    "label": 0
                },
                {
                    "sent": "Right then we can't deal with that because that is the thing we're trying to plug into the recognition network.",
                    "label": 0
                },
                {
                    "sent": "And if we haven't trained it with missing data on half of the frame, it won't know how to make an inference out of that.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of at the level of granularity and expressed in our graphical model.",
                    "label": 0
                },
                {
                    "sent": "I would throw out that frame and just say I'll use the other frames.",
                    "label": 0
                },
                {
                    "sent": "So if you give me a subset of times and you give me the complete frames at those times, then we can answer inference queries like that.",
                    "label": 0
                },
                {
                    "sent": "If you delete half the pixels in every single frame, we can do inference because our graphical model doesn't see that, that's.",
                    "label": 0
                },
                {
                    "sent": "Recognition network territory.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't know why I wrote, see next slide.",
                    "label": 0
                },
                {
                    "sent": "I can't remember why is related.",
                    "label": 0
                },
                {
                    "sent": "I think the idea was that.",
                    "label": 0
                },
                {
                    "sent": "So SV is we can plug in any inference network architecture.",
                    "label": 1
                },
                {
                    "sent": "So if you wanted to do all of your inference for the bottom component, this graphical model using a bidirectional RNN, because maybe you don't want linear dynamics, you want nonlinear dynamics down here, But then still some other nice latent variables up top.",
                    "label": 0
                },
                {
                    "sent": "You can essentially nothing.",
                    "label": 0
                },
                {
                    "sent": "I told you was peculiar to only outputing node potentials or something.",
                    "label": 0
                },
                {
                    "sent": "You could output edge potentials would have sketched here.",
                    "label": 0
                },
                {
                    "sent": "Or you could have an or.",
                    "label": 0
                },
                {
                    "sent": "Basically this is you know you get to choose how much information you aggregate and how much inference you do with your neural networks.",
                    "label": 0
                },
                {
                    "sent": "Sort of learned parametric function approximator versus these optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "So of course, the more you do with your recognition networks, you get the advantages of recognition networks like sort of a fewer constraints on your model structure.",
                    "label": 0
                },
                {
                    "sent": "You also get the disadvantages because that might mean that you have less and less access to complicated inference queries so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way to think about this is like a giant nob where we have like a controller where we have published graphical model type techniques.",
                    "label": 0
                },
                {
                    "sent": "Again, I mean like exponential family problems, removal type techniques over on one side and then we have pure deep learning techniques on the other side and I hope that these kinds of tools and ones that are developed in the spirit can sort of act as a way to interpolate where if we want to say this part of my model I want to be organized in a very specific way, but this other part of my model.",
                    "label": 0
                },
                {
                    "sent": "This should be very flexible, you know some.",
                    "label": 0
                },
                {
                    "sent": "Image model or something like this?",
                    "label": 0
                },
                {
                    "sent": "We should have those modeling tools and then hopefully correspondingly we have inference tools that are able to leverage all of that model structure.",
                    "label": 0
                },
                {
                    "sent": "Whatever model structure we do put in, we should be able to leverage and then for other things we should use more generic techniques.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just tell you a little bit about this mouse application.",
                    "label": 0
                },
                {
                    "sent": "I think I took out the slides on the like.",
                    "label": 0
                },
                {
                    "sent": "Giving mice drugs, but I think I just have some picture slides in here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's learn a model for the mouse video.",
                    "label": 0
                },
                {
                    "sent": "I'm going to build up the model sequentially.",
                    "label": 0
                },
                {
                    "sent": "So first, if you fit a variation.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's what the raw data looks like.",
                    "label": 0
                },
                {
                    "sent": "Apologize if that's a little small, but these are these mouse video frames.",
                    "label": 0
                },
                {
                    "sent": "The mouse head is pointing to the right.",
                    "label": 0
                },
                {
                    "sent": "I've looked at this for a long time, so there's sort of easy for me to parse, but this is a depth mapping and so you can see this is his head is sort of raised up in his ears, are sticking out, his head is sticking almost straight up there.",
                    "label": 0
                },
                {
                    "sent": "He's sort of running around, another one, sometimes curled up into a ball.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was real data.",
                    "label": 0
                },
                {
                    "sent": "This is if you fit a variational auto encoder to that data is like these are great images to work with.",
                    "label": 0
                },
                {
                    "sent": "This is like the M mistreated video 'cause you're just little mouse blobs.",
                    "label": 0
                },
                {
                    "sent": "But if it a very solid encoder it does a great job of learning and image model, but it hasn't learned any time series structure and it hasn't learned how to explain that time series structure in terms of switching system will get to that in a second.",
                    "label": 0
                },
                {
                    "sent": "But it's really good at modeling images.",
                    "label": 0
                },
                {
                    "sent": "This is why we love various autoencoders.",
                    "label": 0
                },
                {
                    "sent": "I was really excited when I fit that sort of denoised mice here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do the thing where you lay out regular lattice in on a 2 dimensional subspace of the latent space and then see what these different axes look like.",
                    "label": 0
                },
                {
                    "sent": "So here in the top left this is the mouse he's huddled up in.",
                    "label": 0
                },
                {
                    "sent": "His ears are sticking out a little bit, but he's kind of like curled up into a ball as the image goes to the right.",
                    "label": 0
                },
                {
                    "sent": "He sort of extending right.",
                    "label": 0
                },
                {
                    "sent": "This is like what this puppet string in the very solid color space does.",
                    "label": 0
                },
                {
                    "sent": "It extends the mouses body and then sort of down here.",
                    "label": 0
                },
                {
                    "sent": "Especially down here is sort of raising his head up and becoming hot in the heat map.",
                    "label": 0
                },
                {
                    "sent": "So it's under pretty good representation of this image manifold.",
                    "label": 0
                },
                {
                    "sent": "Now we want to learn dynamics on it, so let's start by fitting.",
                    "label": 0
                },
                {
                    "sent": "So this was like IID Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "Let's embellish the prior structure that we have in our living space.",
                    "label": 0
                },
                {
                    "sent": "Let's fit a linear dynamical system.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are just three samples of predictions, so the top line is a. Oh man, it's hard to see the top line is a prediction and the bottom line is real data, and again, we're conditioning on data.",
                    "label": 0
                },
                {
                    "sent": "Up until this red line.",
                    "label": 0
                },
                {
                    "sent": "So these are three different video subsequences.",
                    "label": 0
                },
                {
                    "sent": "Also, this data is 30 frames per second, but I subsampled every 4 frames, so I think this is I don't know total of.",
                    "label": 0
                },
                {
                    "sent": "Maybe 100 seconds or something, so it's actually a fairly long range prediction that we're making here, so up until the red line that you can see that it's sort of like tracking the mouse right here, he's putting his head down and curling up.",
                    "label": 0
                },
                {
                    "sent": "And then to the right of the red lines, it's forming a prediction which doesn't need to track with the mouse.",
                    "label": 0
                },
                {
                    "sent": "Does it just needs to be a plausable completion of what was going on.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the top one.",
                    "label": 0
                },
                {
                    "sent": "It said that oh, maybe the mouse goes into a rear right then here, like in the bottom one, it's saying, oh the mouse was inner ear whose actually climbing that's his foot is climbing on the side of the cage.",
                    "label": 0
                },
                {
                    "sent": "He's coming down out of the tree, ran, then he's going to curl up into a ball and you can see his little his butt in his ears sticking out.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a validation to say we can model at least the local structure of these videos with latent linear dynamical systems, right?",
                    "label": 0
                },
                {
                    "sent": "It's doing a pretty good job of not only encoding the data, but making pretty long range predictions.",
                    "label": 0
                },
                {
                    "sent": "So then the last embellishment is to add in some switching structure.",
                    "label": 0
                },
                {
                    "sent": "Let's see if we can discover some behavioral primitives, right?",
                    "label": 0
                },
                {
                    "sent": "Let's go from the linear dynamical system to a switching linear dynamical system.",
                    "label": 0
                },
                {
                    "sent": "And I'll just show you some examples of states.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That come out.",
                    "label": 0
                },
                {
                    "sent": "So this is a panel of videos.",
                    "label": 0
                },
                {
                    "sent": "What we did was we ran this inference and then we took where it was likely to for the where the inference decided that, oh, it's in State 5 or something, right?",
                    "label": 0
                },
                {
                    "sent": "The states don't come with English labels, we just add those later.",
                    "label": 0
                },
                {
                    "sent": "So this is some states.",
                    "label": 0
                },
                {
                    "sent": "What I did is I took a bunch of examples of where that state was used and then I lined them up and made this composite video.",
                    "label": 0
                },
                {
                    "sent": "So when I hit play it's going to be, you know, different my sanna different times.",
                    "label": 0
                },
                {
                    "sent": "But then a little white box is going to appear in the corner, and that's when Interstate 5 or whatever state this was.",
                    "label": 0
                },
                {
                    "sent": "So this is the start of a rear, so you can see that when you know when the white box goes on, you can see that this is when he starts to raise his head and go up here.",
                    "label": 0
                },
                {
                    "sent": "He sort of started here and I think it does look a little head Bob, but for these other ones you can see that when the white square goes on, this is when he's rearing up or climbing up on the wall.",
                    "label": 0
                },
                {
                    "sent": "Here's another one this fall from a rear.",
                    "label": 0
                },
                {
                    "sent": "So here you can see that he generally starts with head high.",
                    "label": 0
                },
                {
                    "sent": "In fact, this one in most of them seems to be climbing on the wall, and then he falls down, right?",
                    "label": 0
                },
                {
                    "sent": "So this is like atomizing all the all.",
                    "label": 0
                },
                {
                    "sent": "The behavior for us.",
                    "label": 0
                },
                {
                    "sent": "Here's a here's a really interesting one.",
                    "label": 0
                },
                {
                    "sent": "This is a grooming syllable, which is a little hard to see because they all look like little balls, so they're balled up here.",
                    "label": 0
                },
                {
                    "sent": "But then the White Square turns on.",
                    "label": 0
                },
                {
                    "sent": "You can see they're like ears start to wiggle right?",
                    "label": 0
                },
                {
                    "sent": "And basically when he's curled up.",
                    "label": 0
                },
                {
                    "sent": "And his ears are wiggling.",
                    "label": 0
                },
                {
                    "sent": "That means he's grooming his tummy, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the mouse.",
                    "label": 0
                },
                {
                    "sent": "It's discovered a sort of grooming syllable.",
                    "label": 0
                },
                {
                    "sent": "And then when he leaves it, or he extends his body, the syllable turns off.",
                    "label": 0
                },
                {
                    "sent": "So I think those are all the slides I have on that app.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kacian here is you know what I told you about modeling idea, which is sort of a relatively obvious 1.",
                    "label": 0
                },
                {
                    "sent": "Instead of having our latent spaces be just Gaussian and having all the capacity of our models in our neural networks will certainly have a lot of capacity.",
                    "label": 0
                },
                {
                    "sent": "Another thing we could do which might be appropriate, especially in some scientific applications, is let's write down more interesting latent variable structure, but then gets pushed through the neural Nets for capacity.",
                    "label": 0
                },
                {
                    "sent": "And maybe there are interesting games we can play where we decide, for example here.",
                    "label": 0
                },
                {
                    "sent": "I don't care how you model the image generation frame by frame of a mouse, right?",
                    "label": 0
                },
                {
                    "sent": "That's just the neural Nets, it can.",
                    "label": 0
                },
                {
                    "sent": "It can do essentially anything, but all the temporal correlation across frames, right?",
                    "label": 0
                },
                {
                    "sent": "I didn't have an RNN, there's no dynamical neural network, so all the correlation between frames across time had to be explained through our graphical model latent variable structure.",
                    "label": 0
                },
                {
                    "sent": "And so we sort of set up the structure and the model has to learn how to explain mouse videos as organized through this, like PGM.",
                    "label": 0
                },
                {
                    "sent": "Representation.",
                    "label": 0
                },
                {
                    "sent": "So that was the modeling idea.",
                    "label": 0
                },
                {
                    "sent": "The big technical bit was how inference works actually have.",
                    "label": 0
                },
                {
                    "sent": "So another way too.",
                    "label": 0
                },
                {
                    "sent": "So this paper is like 20 pages of appendices with a lot of like calculations, and I have a new way of organizing this stuff that makes it a lot easier in terms of sort of like proximal operators and sort of thing, so you know, hopefully will post that sometime in the next couple months, but.",
                    "label": 0
                },
                {
                    "sent": "Let me just say that inference we were certainly able to exploit a lot of the graphical model structure and still exploit also recognition networks to handle the hard parts.",
                    "label": 0
                },
                {
                    "sent": "If you go and download the paper now you're going to like this is so complicated and it is sort of.",
                    "label": 0
                },
                {
                    "sent": "It works, but it's like there's a lot of calculation to it.",
                    "label": 0
                },
                {
                    "sent": "I think we have a new way that I'm excited about it, sort of thinking about things much more clearly and generalizing at the same time.",
                    "label": 0
                },
                {
                    "sent": "So, but this inference mechanism is able to exploit structure will also be general, and I told you about this new application in sort of neurobiology.",
                    "label": 0
                },
                {
                    "sent": "I think actually is a quite an interesting tool for not only practical things like drug development, but more importantly science and neuroscience people are interested in studying mouse behavior with.",
                    "label": 0
                },
                {
                    "sent": "So with that, let me think my.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Collaborators, of which there are many.",
                    "label": 0
                },
                {
                    "sent": "David Ave Alex, Moscow Bob Dado is the Pi and neurobiology and Ryan Adams and then more recently I've been collaborating with Sergey 11 Scott Linderman.",
                    "label": 0
                },
                {
                    "sent": "Can't members name and Matt Hoffman as well on this stuff.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Can you say it again?",
                    "label": 0
                },
                {
                    "sent": "Oh, why is death images?",
                    "label": 0
                },
                {
                    "sent": "There are a few reasons, so actually.",
                    "label": 0
                },
                {
                    "sent": "The mouse is behavior in a 3D matters right matters alot.",
                    "label": 0
                },
                {
                    "sent": "The mouse does a lot of rearing and it'll flatten itself to the ground when it scared of predators.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to kind of 3D input.",
                    "label": 0
                },
                {
                    "sent": "In principle you could reconstruct that from 2D like you as a human.",
                    "label": 0
                },
                {
                    "sent": "So maybe in principle we could model that, but I think to infer the 3D pose a mouse you might have to learn a lot of hard things, like how to synthesize the mouse is texture and stuff.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a much easier problem to have a good sensor to do that and we were trying to do science.",
                    "label": 0
                },
                {
                    "sent": "I think it would be super interesting though if you could train a system that looks a mouse videos, maybe multiple perspectives, it could learn how to track from scratch and it could learn how to reconstruct the mouse.",
                    "label": 0
                },
                {
                    "sent": "Suppose there are elements mouse behavior that we don't see from the top like where its feet are or something.",
                    "label": 0
                },
                {
                    "sent": "If its limping sort of hard to see.",
                    "label": 0
                },
                {
                    "sent": "So we can either attack that with better measurement modalities or with you know better inferential techniques and the ultimate limit is whatever we can do by watching this video and use our mouse priors to understand it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just something that hasn't been explored, I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah no, no.",
                    "label": 0
                },
                {
                    "sent": "The reasons for doing it this way as well.",
                    "label": 0
                },
                {
                    "sent": "They said most of the time you don't use mice, we use the OR the rats and Lab Rats and not one color.",
                    "label": 0
                },
                {
                    "sent": "Multicolor.",
                    "label": 0
                },
                {
                    "sent": "Yes, right?",
                    "label": 0
                },
                {
                    "sent": "This completely breaks down with vision.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Certainly because complex.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the other thing is that these are nocturnal animals, so you might want to use him for writers.",
                    "label": 0
                },
                {
                    "sent": "That's right, I forgot about that, so I Fortunately just dealt with code and numbers.",
                    "label": 0
                },
                {
                    "sent": "Now with the mice themselves, Biondo made the two points one that rats are different colors.",
                    "label": 0
                },
                {
                    "sent": "These mice are all C57 Black 6 and so they were all genetically identical.",
                    "label": 0
                },
                {
                    "sent": "Twins so, but yeah, certainly the other point that actually all those recordings were done in the dark, so the Connect uses infrared pattern to sense things that presumably the mouse can't see.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's hard to do.",
                    "label": 0
                },
                {
                    "sent": "Color vision in the dark, certainly.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Other questions yeah.",
                    "label": 0
                },
                {
                    "sent": "Supervised supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So in principle you can just sort of shade any graphical model nodes you want and condition on those.",
                    "label": 0
                },
                {
                    "sent": "So if you had some labels that you wanted to use, the sort of bootstrap and bias.",
                    "label": 0
                },
                {
                    "sent": "As a prior and empirical prior for what these things should look like, you can run all these these inference algorithms and they still apply with semi supervision.",
                    "label": 0
                },
                {
                    "sent": "I think that's another advantage of graphical models is you can sort of.",
                    "label": 0
                },
                {
                    "sent": "It goes hand in hand with being able to support any inference query.",
                    "label": 0
                },
                {
                    "sent": "You can sort of shade things in and instead of having an inference network is a way of answering one inference query or maybe a linear nested sequence of inference queries.",
                    "label": 0
                },
                {
                    "sent": "But with graphical models are about in part is being able to.",
                    "label": 0
                },
                {
                    "sent": "Have a template to generate a program from any shaded node pattern to how to do inference with shaded pattern?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "But what can you do for this during that country competition?",
                    "label": 0
                },
                {
                    "sent": "So I think your question is, hey, you were talking about some examples and then saying that there's some generality to it in the variational inference and like factorization, and like where does this structure like what?",
                    "label": 0
                },
                {
                    "sent": "What is the general theory like?",
                    "label": 0
                },
                {
                    "sent": "How do you generally describe when you can do this sort of thing?",
                    "label": 0
                },
                {
                    "sent": "And there is an answer that this is, you know, as I was saying, we have different ways of thinking about this stuff.",
                    "label": 0
                },
                {
                    "sent": "Now that I think is a lot cleaner, I'm going to give you an answer that's a bit technical so up.",
                    "label": 0
                },
                {
                    "sent": "Essentially because we built our generative model in terms of sort of conditional exponential family structure.",
                    "label": 0
                },
                {
                    "sent": "It turns out that when we do that, we end up with an overall exponential family.",
                    "label": 0
                },
                {
                    "sent": "That is the energy term in it.",
                    "label": 0
                },
                {
                    "sent": "The sort of linear term is a is in fact linear in its overall statistics, but it is a multilinear polynomial in the statistics of the component factors, so we have a TX times, TZ, right and other stuff we never have TX squares in it, so it's a multilinear polynomial of these simpler statistical families.",
                    "label": 0
                },
                {
                    "sent": "The magic trick we did there with when we can factorize it and decompose it and things became nice was basically saying let's write something.",
                    "label": 0
                },
                {
                    "sent": "Let's her variational family be a product of things that are linear in those statistics and those statistics.",
                    "label": 0
                },
                {
                    "sent": "The overall energy is multilinear in an because those things are tractable by definition.",
                    "label": 0
                },
                {
                    "sent": "We built it out of components like a Gaussian linear dynamical system that on its own I can handle it.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "This multi linear structure that lets us decompose it then into a bunch of tractable pieces.",
                    "label": 0
                },
                {
                    "sent": "So somehow being multilinear in statistics functions that correspond to exponential families for which we can compute the log normalizer exactly, that's sort of the technical jargon answer.",
                    "label": 0
                },
                {
                    "sent": "High school.",
                    "label": 0
                },
                {
                    "sent": "Are we good?",
                    "label": 0
                },
                {
                    "sent": "Where the Clock alright.",
                    "label": 0
                },
                {
                    "sent": "Thanks guys.",
                    "label": 0
                }
            ]
        }
    }
}