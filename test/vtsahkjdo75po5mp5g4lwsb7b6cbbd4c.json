{
    "id": "vtsahkjdo75po5mp5g4lwsb7b6cbbd4c",
    "title": "Learning to Reconstruct 3D Human Pose and Motion from Silhouettes",
    "info": {
        "author": [
            "Ankur Agarwal, INRIA - The French National Institute for Research in Computer Science and Control"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2004",
        "category": [
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/lmcv04_agarwal_lr3hp/",
    "segmentation": [
        [
            "So how I'm ready.",
            "My name is incredible and I'm going to present some work that Bill and I have been doing on learning to reconstruct 3D human pose in motion from silhouettes."
        ],
        [
            "So basic goal here is to recover 3D human body posts from image silhouettes and when we talk of 3D pose we basically mean joint angles so parameterized the body by trying to recover joint angles for major body joints.",
            "So we have 3 degrees of freedom for 18 joints on the body and we'd like to recover this using individual images which would enable things like self initialization or recovering from tracking losses and whenever possible also utilize video sequence information.",
            "Temporal currency for increased accuracy results.",
            "Applications with motion capture human computer interaction action recognition smart solar systems."
        ],
        [
            "OK so I won't go over the base of this lot of work that's being done on 3D human tracking and and poses timation won't go through all that, but very broadly speaking there are kind of two classes of approaches.",
            "People been working on and classify the most model based approaches and learning based approaches.",
            "So in model based approaches, we kind of presuppose an explicit known parametric body model which which the variety of types of it could be a complete 3 body model complete 2D versions, tree based representations, and these techniques generally involve inverting kinda mattix and followed by numerical optimization, which basically means we're trying to map this physical body model onto an image, and we're defining some kind of an error function.",
            "So this numerical optimization helps us to estimate parameters of this body model by overlaying it on the image.",
            "Random number of different kinds of works in that in some cases model based tracking where this model exactly fits 2 images across the sequence of.",
            "A sequence of images.",
            "Recently we've seen some new kinds of methods which are based on learning approaches.",
            "And the idea here being trying to avoid actually building an explicit body model and try to capture as much general information as we can directly by learning methods.",
            "So given lots and lots of data and how much information can we extract directly from this?",
            "So we avoid accurate 3D modeling, rendering, and.",
            "An important class of these are examples based methods, so from what we try to do is from this large amounts of training data that we have, we try to select maybe all or possibly some of the data as examples and given a new image.",
            "Or given you sequence of images, we try to compare to these examples and see how much information we can get from that.",
            "I mean for example the keyframes somehow fits in that scenario."
        ],
        [
            "So we propose a model free learning based approach.",
            "And study different in the sense that it recovers three posts, which is basically our joint angles by direct regression on robust silver descriptors.",
            "So our images are represented as silhouettes and we directly regress three posts from them.",
            "It's based on a sparse kernel based regression which is trained using motion capture data.",
            "The advantage is that we avoid the need to build an explicit 3D model.",
            "Answer Most of the most of the relationships between the image and the 3D model.",
            "Disposal dispose space is learn directly from data, easily adapted to different people, different appearances, possibly also non humans, animals or any kind of parametrizable will configuration.",
            "And slowly maybe also more robust than a model based approach.",
            "It's kind of more generic.",
            "On the negative side, it's.",
            "Little harder to interpret than exclusive body model and may also be slightly less accurate."
        ],
        [
            "So the basic idea is to learn a compact system that directly outputs pose from an image.",
            "And how do we do this?",
            "Represent the input which is our image by descriptor vector.",
            "We call it said.",
            "Let's see how we do that.",
            "And we write a multi parameter output post.",
            "We're trying to estimate is a vector X which is basically a list of all the body angles.",
            "So it's a huge high dimensional vector.",
            "And the basic goal is to learn this regressor here, which says X is basically nothing but a function of Z.",
            "And this highly complex non relationship will see how this model.",
            "Let's see basic idea given Z given our image somehow computers Ed and can we directly recover X from it?",
            "So not here.",
            "This does assume a functional relationship between our Z&X places, which is not really the case, but.",
            "Characters."
        ],
        [
            "Solving problems.",
            "So we started with similar descriptors represent our images silhouettes."
        ],
        [
            "So, given a human, let's assume for the moment that we've segmented, so we have a silhouette.",
            "And while you silhouettes well, they capture most of the information on post.",
            "As you can see here just by silhouette, we actually get a very good idea of what's going on in this image.",
            "They can often be extracted from real images.",
            "The incentive to color changes, textures, clothing.",
            "So we really don't care what texture we have on the clothes, what color they are and kind of looking at the shape broadly, which makes it more generic to variations amongst people and we require no kind of prior labeling of.",
            "Which leg is which or any kind of hand clicking it's no labeling at all.",
            "We're just trying to deal with this shape as an input, and of course they do have limitations and we do lose some kind of information.",
            "What was one major limitation is that the segmentation we're going to have is never going to be perfect, so we do have artifacts like attached shadows and in accuracies holding the silhouettes possibly.",
            "And we didn't lose this depth ordering or sightedness information.",
            "Some parts of the body which included like we don't see anything about the arm here.",
            "We don't know which leg is which is which and this information which is lost in going from an image to just a silhouette."
        ],
        [
            "OK, then we do have this problem of ambiguities as well.",
            "For example, we have shown two different poses which have very similar silhouettes an from silver.",
            "It's not really very easy to tell which leg is which are exactly next to impossible to tell which is the right thing, which is the left leg with two different cases here.",
            "And which arm is is left on the right arm?",
            "Where is the included ARM and sometimes back poses infront poses also confused of in silhouettes.",
            "So here's kind of evident bottom those can be confused times as well.",
            "And then these parameters.",
            "For example in this image, how much is an event we really?",
            "Well, that's a different issues because there's lots of information when we go to 2D, but there all these kinds of ambiguity issues and.",
            "Which are associated with dealing with silhouettes.",
            "So the silhouette supposed problems inherently multivalued given silver does not always a well defined 3D pose.",
            "Which is why single valued, regressive sometimes behave radically."
        ],
        [
            "So we describe ourselves using um shape context histograms.",
            "The idea is that we'd like to capture the silhouette shape, but the same time be robust against occlusions, segmentation failures, and stuff, so we really don't want to use global shape descriptors, for example, moments or.",
            "We'd like to base our representation on some local kind of features, so here we use shape context histograms, which is basically a distribution of local shape context responses, so shape contexts are shape descriptors, which were introduced Berkeley Group few years ago, and view here is.",
            "So start with the silhouette and we treat all the points on the edge edge points as interest points.",
            "So we've got a bunch of points here and we're trying to describe this sort of points which correspond to the silhouette shape.",
            "So the shape contexts do is place a small window, a small mask basically on each point and have a histogram here, which is in the.",
            "Like radio space and theater space.",
            "So the advantage of having something like this descriptor advantage over here being that.",
            "We insensitive to kind of less sensitive to scale and.",
            "And also is controlling the size of this this mask we can we can have control in the locality.",
            "So I actually find that keeping this mask roughly the size of a limp kind of captures captures video of local information while not losing all global information as well.",
            "So for each point to consider what we compute, this actually is a 60 dimensional histogram because we have 5 bins in the radial space and 12 discretizations on the angle space.",
            "So we have 5 * 1260 bins over here for each point in the silhouette, we compute assisted dimensional histogram.",
            "And so we have a distribution in this shape context space, which is basically 160 dimensional vector for each of these points.",
            "And then what we do here is basically vector quantize in this space to eventually get.",
            "A histogram in the shape context space, so we use.",
            "We do a clustering in this space, which kind of takes advantage of.",
            "We know what kind of shapes are dealing with their basically different forms of human poses, which is going to be a very reduced area volume in this.",
            "In this dimension shape, context space.",
            "So we do a clustering here using K means and obtain 100 centers which are basically a second level of pins.",
            "And this distribution is shape context spaces.",
            "Quantized to a single vector, which now encodes pretty robustly the entire shape of that silhouette.",
            "So it's a single vector here, which represent as did we say that we can capture most of the information on our image by the single vector."
        ],
        [
            "We shape contexts.",
            "Grams actually in code.",
            "Locality very well and.",
            "But they don't lose global information.",
            "So here I've just shown on analysis, dimensional shape, context, space.",
            "These are just the first 2 principal components.",
            "And all the blue points correspond to.",
            "This 60 dimensional histograms computed for all points on silhouettes over complete video sequence, so this is roughly the kind of distribution we have in the shape, context, space and these black centers.",
            "Here they came in centers which are using to vector quantizer space.",
            "If you look at the distribution of points for a single silhouette, for example this one, this is what we get in the shape context space in the first 2 principal components of shape context space.",
            "So we see that, roughly speaking, it doesn't code.",
            "The global shape of the silhouette.",
            "At the same time, it's kind of robust too.",
            "Tomato segmentation and stuff.",
            "Because it descriptive R. In the end, local."
        ],
        [
            "OK, so having obtained this center descriptor said, we're going to address this and obtain 3D post from it."
        ],
        [
            "So regression model is an only new one.",
            "The goal is to predict the output vector X which 3D human pose given the input vector Z which we just computed and represented by nonlinear weighted combination of basis functions.",
            "So we say that X is a sum of a weighted sum over basis functions in the zed space, which we compactly right like this.",
            "So here.",
            "The file is here.",
            "Basically basis functions which are computed in the silhouettes in the silver descriptor space.",
            "And they are the weights for the respective basis functions.",
            "So we just stack up the files together throughout the vector F. And stack up the weights together to get a big matrix A.",
            "And what we'd like to do here is from training data.",
            "Try to estimate this matrix a the dependence the relationship between.",
            "Our descriptor space and 3D post space.",
            "The basis here we use well could be kernel basis.",
            "We've tried different kinds of bases and, um, the ones we finally end up using a kernel basis where five K 5K of Zedd's, basically a kernel function defined between zero and the center.",
            "Zed K. For a given center points and we end up using radial basis functions.",
            "For the kernels."
        ],
        [
            "So the goal is basically trying to estimate that the coefficient matrix A and we do that by regularised least squares, which means that we'd like to minimize this error over all training points, the prediction error.",
            "Um?",
            "Right, so any times F minus XI.",
            "And then we have this regularization term which we call RFA.",
            "So basically which is a regularizer or penalty function, which we see often in learning methods to avoid overfitting on train data.",
            "So here we can impose different kinds of regularizers and get different behaviors from a regression function.",
            "A simple form progression is Ridge regression where what we try to do is.",
            "Impose a size limit on the.",
            "On the norm of a on the autonomic way.",
            "So that keeps all the coefficients mall trying to keep the surface flat and avoids overfitting in that sense."
        ],
        [
            "We actually use for our regression is the relevance vector machine and.",
            "It's got compared to, we've seen you, Sir, like the support vector machines being used for aggression and.",
            "This is a slightly different approach, which is based on Bayesian method for its use, both for classification and regression was introduced a couple of years ago with Michael Tipping.",
            "Phone.",
            "The idea for a certain machine is basically that we're going to learn the parameter.",
            "This coefficient matrix A and the kind of priors we impose here.",
            "On each parameter, we impose a Gaussian prior.",
            "So basically on each parameter or group of parameters.",
            "And it's a form of this prior which is actually important under.",
            "Which makes the relevant sector machine behave the way it does.",
            "The form here is, it's basically a nonconvex prior for logarithmic on the modulus.",
            "Um?",
            "Of the coefficient matrix.",
            "So what happens here is by input.",
            "By adding this term to our minimizations function.",
            "We not only forcing 80 with small, which helps in generalization, but also it helps in appearing sparse solutions.",
            "So the way that happens if we look at the force here, the regularization for switches.",
            "To the derivative of this regularizer respect to a, it's inversely proportional to the size of a.",
            "So what happens here is that as the coefficients tend to get smaller.",
            "The force increases and these coefficients are further pushed to reason size, so this ineffective very strong pruning going on of the parameters as parameters tend to as the coefficients tend to get go to zero.",
            "There actually limited from the system.",
            "And.",
            "So.",
            "We automatically we essentially select only the relevant features that we need, and.",
            "And lots of the coefficients in our final matrix A are essentially zeros, which means that we do not need the corresponding basis functions.",
            "So this shrinkage strengths are this pruning semantic rule that we want is controlled by parameter new over here.",
            "And by increasing you, we actually get sparser solutions."
        ],
        [
            "We made advantage of the relevance vector machines that get sparse solutions, then only generalized.",
            "But no sparse.",
            "So in terms of behavior is pretty similar to the reservation in terms of performance, but the solutions here are much, much sparser.",
            "So the.",
            "Would we did vantage we get from sparsity is that when we're dealing with kernel basis, it's only the relevant examples that are obtained from the training set.",
            "So typically have very, very large training sets, and we don't want to retain all those examples for computations later on.",
            "So there's an automatic pruning going on, and we select only relevant examples to computer basis functions.",
            "And if we use linear basis, which basically means that the basis function is nothing but the input, the silver descriptor.",
            "What we do is a relevant feature selection.",
            "So from from older have very high dimensional input vector.",
            "It's not all those dimensions are going to be retained for the regression and only a subset of them would be used to actually estimate pose.",
            "So it's an automatic relevant feature extraction going on inside."
        ],
        [
            "OK, so this is a briefing on the relevance vector machine.",
            "The way we use this."
        ],
        [
            "To get pose from static images.",
            "Stretching for experiment.",
            "So what we do is we use real human motion capture data for training our system.",
            "So the advantage of using such data is that we capture typical human motions and rather looking at all kinds of possibilities.",
            "So with this extremely high number of degrees of freedom in the human body, we have 54 parameters to estimate for the complete 3D human post.",
            "It's very, very dense space, but what we do is by using training data from real motions.",
            "We kind of capture more typical human movements and not just all possible poses.",
            "We got this data from a public website given here, so there's a lot of motion capture data which is being recorded for different kinds of activities.",
            "And what we use is to generate examples of.",
            "Silhouette's corresponding 3D pose.",
            "Now, Fortunately we don't have the the corresponding silhouettes for these, so we use a graphics package called Poser, which very nicely enables us to to render human motion capture data.",
            "So it's somewhat artificial.",
            "But for all this for all this real motion capture data we are able to generate silhouettes from different viewpoints using different kinds of models.",
            "So we kind of have pretty good coverage of the training space.",
            "So we have a lot of ground truth data to test and train and test our system on.",
            "And also we test on real sequences of another person."
        ],
        [
            "So we've looked at quite a few methods on these lines in terms of progress is we've tried regeneration and relevance vector regression.",
            "In terms of basis functions, we've tried linear basis functions, several different kinds of kernel functions.",
            "And it turns out that.",
            "The technique is not really very very dependent on the kind of progression used.",
            "The performance is pretty much similar, but we do benefit from the relevance vector regression.",
            "For example in terms of much more sparse solutions.",
            "And Gaussian kernels turn out to be the best in terms of our choice of basis functions, and they get slightly better.",
            "Slightly better performance then.",
            "A simple linear regression."
        ],
        [
            "So here are some results which we.",
            "Where we estimate post from a synthetic spider walk sequence.",
            "So we've got a number of such corresponding poses in our training data and these are syllabus which were not included in the training set.",
            "So there's a new sellers which have been given as a test and from single images we've actually obtained pretty accurate 3D poses.",
            "For these images, and for this particular constructions with relevance vector regression with caution kernel and were able to obtain.",
            "We retain only 6% of the training examples, which is basically so we have like 156 support vectors are from 600 training examples so extremely extremely sparse in the sense that we retain a very small percentage of our training data are very compact model.",
            "And we can pretty accurately get post from a single image, a single silhouette.",
            "In terms of numbers, the mean angular error per degree of freedom here is 6 degrees."
        ],
        [
            "OK, so this.",
            "So so basically the entire video sequence for some of those frames that we saw.",
            "As you see here, well in most of the cases there's an accurate reconstruction, but there lots of problems, especially if you look at it overtime.",
            "The left temporal jitter.",
            "And you'll see a number of instances of these glitches where estimation seems to be completely wrong with the person.",
            "Will simply heading angle the direction in which the person is working.",
            "For example, sometimes the totally it's wrong estimate.",
            "But more or less, it's able to get posed out of and note that there's no temporal currency imposed here, so it's getting each pose out of a single silhouette.",
            "So we do mostly OK, but we realize that we have 15% around what we call glitches, where the computer oscillations in the estimation.",
            "And the problem here is because of pose ambiguities, often from a single silhouette we have lots of confusing poses and which tends to give wrong estimations."
        ],
        [
            "So if you can do a couple of graphs for the same sequence, a couple of individual angles which plot here.",
            "For example here, plot the hip angle of the person while he's walking in circles.",
            "And.",
            "So the dotted curve shows the true value of the hip angle which we have from ground truth.",
            "And here's the estimation.",
            "So we kind of we tend to follow the curve basically overtime, so there's no loss in time as estimating from individual images, but you'll see several instances of glitch and cases where this completeness estimations, for example.",
            "Here is a very large error and.",
            "This is another parameter where we plot the overall heading angle of the person, so this varies from minus 1 to 2 + 90 degrees.",
            "The persons working complete circles and as you can see it's kind of stable with time, but lots of these instances of glitch or estimations."
        ],
        [
            "So these glitches there is also OK, but we have these glitches and the reason is because the regressive chooses wrong around case of animals, pair or remains undecided when we don't have enough information, the silhouette.",
            "This problem is actually it's very evident in the case of overhead angle, which is the most visible parameter direction with the person is working now.",
            "This thing is extremely hard to estimate because various through a complete 360 views range.",
            "What we do here is to avoid them.",
            "This rewrapping discontinuous thing actually addressing cost & Theta.",
            "So we can see that.",
            "The normalization of this unit vector which so this costs.",
            "With science we should be in a vector and denormalization, often causes problems."
        ],
        [
            "This shows the errors in the same thing and we can associate instances of high error which is shown in this graph with slower plot here shows the norm of the cost assigned to the vector.",
            "And whoever this vector is Miss estimated basically causes high error estimation."
        ],
        [
            "Here's some examples of same method on real images.",
            "Pretty much accurate estimation some cases of.",
            "Confusion between legs or misestimation."
        ],
        [
            "OK, so we've actually seen this already.",
            "The problem is coming from the fact that.",
            "But it's actually a multi brand surface in the Xbox.",
            "Is Dead Space and the cases where we have multiple values for silhouette, so it's a function treatment.",
            "The assumption that we're treating as a function is actually round to give problems in some cases.",
            "So where the real solution to it would be to try to set up a multivalued regressor.",
            "Which we should possibly give more than one solutions from a single silver.",
            "What we do is a possible solution is to resolve these ambiguities using temporal information.",
            "So we made the same thing in."
        ],
        [
            "Which I recover post from."
        ],
        [
            "Sequences and which is able to reduce glitches by embedding the thing in a tracking framework.",
            "So the idea here is that since we have information about the previous time state, we can use this temporal information too as a hint to kind of select the correct solution from if there are more than one possible solutions.",
            "Under Tinker State information with the typical dynamical model to kind of give an estimate of the state at a time T, and we incorporate that into a regression framework to kind of get a correct answer."
        ],
        [
            "So I'll just go over this very brief.",
            "The way we do that is that we learn a dynamical model, which is a second order model which helps us to predict the state at time T given two previous States and so this is again learn from the data.",
            "And the observation update is a slightly complicated model.",
            "Here.",
            "What it does is that.",
            "Within the regression framework, within the kernel, we not only have the observation at that time T which is coming from the image, but we also have this estimate of the pose which is given as an input in the regressor.",
            "So what happens is that we selecting examples in the kernel space which are not only closing silver space but also the state space.",
            "And through this indirect dependence on state estimate, we're not really very sensitive to temporal miss tracking, but it does help in resolving this ambiguity's.",
            "At the same time, we have a dependence on the linear dependence on the estimate, which helps in ensuring."
        ],
        [
            "False witness.",
            "So bearing in tracking framework, we see the errors go down by a couple of degrees and the same graph become much much smoother with highly hardly any situations or glitches."
        ],
        [
            "So same sequence by.",
            "Regressing in this joint.",
            "Regression framework we were addressing on not only the silhouette but also some little bit of temporal information to resolve ambiguities.",
            "We get pretty smart smooth reconstruction, so from a single camera, so that space and.",
            "We're not very sensitive to.",
            "Temporal miss tracking.",
            "Because we're looking at the observation at every step that we are using temporal information to some extent to resolve ambiguities and also helps in giving smoothness."
        ],
        [
            "Same method applied to several images.",
            "Well quality of construction is still worse.",
            "I'm going to number of factors about segmentation and stuff, but we are more or less able to achieve.",
            "Track from a single camera and real images."
        ],
        [
            "OK, conclusion so kind of formula, compact model for direct regression on image observations.",
            "We don't need anything.",
            "Any explicit 3D model.",
            "Which ensures easy adaptability to different problems.",
            "We do an exploit temporal currency in sequences by modeling dynamics, but it's an indirect dependencies, so we're not really very susceptible to miss tracking and the fact that we can actually estimate pose not always accurately, but mostly accurately from individual images makes it potentially self initialized tracking method.",
            "And rightly so, if in the same sequence, if we initialize automatically by estimating from single pose in around 84% of the cases, actually were able to estimate able to track successfully by automatically initializing.",
            "The smallest advantage that we do it assumes segmentation for our seller input.",
            "But if you can go by that, then the method works pretty good.",
            "Alright, thank you.",
            "Yeah, it's a comment more about the use you mentioned that you want this multivalued function approximation, right?",
            "The good news is that people have worked on that for a long time, so things called mixture density networks or mixtures of experts, right?",
            "Actually, you know, have been around to do that, so you could easily plug that in.",
            "Basically makes your conditional on your zed.",
            "You get a mixture distribution OK, and so you know the idea is you'll be in one of those modes and then also.",
            "For hooking up the things through time, you have those bunch of modes or bunch of whatever.",
            "Yes, you're right.",
            "Yeah yeah.",
            "So the things that we have been seeing.",
            "The only problem is that it's definitely possible and we actually working towards trying to prove that.",
            "But the problem is that it just kind of becomes a little more complicated.",
            "Extremely high dimensional spaces, so trying to see if we can actually workout this problem.",
            "Those issues that come in will try and do this clustering and these mixture methods in this high dimension spaces.",
            "But it's true that that's something we can explore too and some instruments.",
            "Showing that they're going to pults.",
            "That's right, there's not just just you know two or three solutions.",
            "What happens is that there are thousands of different branches.",
            "Which branch and rejoin in principle thousands, at least in practice.",
            "Probably only 10 or so, but the space is not just a multiple regression problem, it's more complicated than that.",
            "Well, in theory the the the one could deal with that once.",
            "Yeah, it would be easy, but the theory would allow that.",
            "You know the point that in practice this more issues which come up.",
            "But yes, it is potentially.",
            "Consult.",
            "How serious is your disadvantage?",
            "Can you get away with a poor segmentation?",
            "Yes, so actually the fact that we are presenting silhouettes by these robust local descriptors does show that we are in some sense, some robust too.",
            "Not very good segmentation as you can see in the real."
        ],
        [
            "In this example, so it's not really ideal.",
            "Silhouettes the kind of solution we've trained on.",
            "We've trained on very different silhouettes, which look much cleaner, and we're actually pretty accurately able to get post even with such noises, silhouettes, but.",
            "Right we do need segmentation even to get service quality, so that's the basic point.",
            "But we are quite robust.",
            "Two holes into local segmentation errors and stuff like that.",
            "One thing we've tried is training on part of the silhouette only, and you get less good but reasonable results.",
            "Training on working on just part of the solid.",
            "Anymore questions.",
            "OK, let's stop living.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how I'm ready.",
                    "label": 0
                },
                {
                    "sent": "My name is incredible and I'm going to present some work that Bill and I have been doing on learning to reconstruct 3D human pose in motion from silhouettes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basic goal here is to recover 3D human body posts from image silhouettes and when we talk of 3D pose we basically mean joint angles so parameterized the body by trying to recover joint angles for major body joints.",
                    "label": 1
                },
                {
                    "sent": "So we have 3 degrees of freedom for 18 joints on the body and we'd like to recover this using individual images which would enable things like self initialization or recovering from tracking losses and whenever possible also utilize video sequence information.",
                    "label": 0
                },
                {
                    "sent": "Temporal currency for increased accuracy results.",
                    "label": 0
                },
                {
                    "sent": "Applications with motion capture human computer interaction action recognition smart solar systems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I won't go over the base of this lot of work that's being done on 3D human tracking and and poses timation won't go through all that, but very broadly speaking there are kind of two classes of approaches.",
                    "label": 0
                },
                {
                    "sent": "People been working on and classify the most model based approaches and learning based approaches.",
                    "label": 1
                },
                {
                    "sent": "So in model based approaches, we kind of presuppose an explicit known parametric body model which which the variety of types of it could be a complete 3 body model complete 2D versions, tree based representations, and these techniques generally involve inverting kinda mattix and followed by numerical optimization, which basically means we're trying to map this physical body model onto an image, and we're defining some kind of an error function.",
                    "label": 0
                },
                {
                    "sent": "So this numerical optimization helps us to estimate parameters of this body model by overlaying it on the image.",
                    "label": 0
                },
                {
                    "sent": "Random number of different kinds of works in that in some cases model based tracking where this model exactly fits 2 images across the sequence of.",
                    "label": 0
                },
                {
                    "sent": "A sequence of images.",
                    "label": 0
                },
                {
                    "sent": "Recently we've seen some new kinds of methods which are based on learning approaches.",
                    "label": 0
                },
                {
                    "sent": "And the idea here being trying to avoid actually building an explicit body model and try to capture as much general information as we can directly by learning methods.",
                    "label": 0
                },
                {
                    "sent": "So given lots and lots of data and how much information can we extract directly from this?",
                    "label": 1
                },
                {
                    "sent": "So we avoid accurate 3D modeling, rendering, and.",
                    "label": 0
                },
                {
                    "sent": "An important class of these are examples based methods, so from what we try to do is from this large amounts of training data that we have, we try to select maybe all or possibly some of the data as examples and given a new image.",
                    "label": 0
                },
                {
                    "sent": "Or given you sequence of images, we try to compare to these examples and see how much information we can get from that.",
                    "label": 0
                },
                {
                    "sent": "I mean for example the keyframes somehow fits in that scenario.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we propose a model free learning based approach.",
                    "label": 1
                },
                {
                    "sent": "And study different in the sense that it recovers three posts, which is basically our joint angles by direct regression on robust silver descriptors.",
                    "label": 0
                },
                {
                    "sent": "So our images are represented as silhouettes and we directly regress three posts from them.",
                    "label": 0
                },
                {
                    "sent": "It's based on a sparse kernel based regression which is trained using motion capture data.",
                    "label": 0
                },
                {
                    "sent": "The advantage is that we avoid the need to build an explicit 3D model.",
                    "label": 1
                },
                {
                    "sent": "Answer Most of the most of the relationships between the image and the 3D model.",
                    "label": 1
                },
                {
                    "sent": "Disposal dispose space is learn directly from data, easily adapted to different people, different appearances, possibly also non humans, animals or any kind of parametrizable will configuration.",
                    "label": 0
                },
                {
                    "sent": "And slowly maybe also more robust than a model based approach.",
                    "label": 0
                },
                {
                    "sent": "It's kind of more generic.",
                    "label": 0
                },
                {
                    "sent": "On the negative side, it's.",
                    "label": 1
                },
                {
                    "sent": "Little harder to interpret than exclusive body model and may also be slightly less accurate.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic idea is to learn a compact system that directly outputs pose from an image.",
                    "label": 1
                },
                {
                    "sent": "And how do we do this?",
                    "label": 1
                },
                {
                    "sent": "Represent the input which is our image by descriptor vector.",
                    "label": 0
                },
                {
                    "sent": "We call it said.",
                    "label": 0
                },
                {
                    "sent": "Let's see how we do that.",
                    "label": 0
                },
                {
                    "sent": "And we write a multi parameter output post.",
                    "label": 0
                },
                {
                    "sent": "We're trying to estimate is a vector X which is basically a list of all the body angles.",
                    "label": 0
                },
                {
                    "sent": "So it's a huge high dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And the basic goal is to learn this regressor here, which says X is basically nothing but a function of Z.",
                    "label": 0
                },
                {
                    "sent": "And this highly complex non relationship will see how this model.",
                    "label": 0
                },
                {
                    "sent": "Let's see basic idea given Z given our image somehow computers Ed and can we directly recover X from it?",
                    "label": 0
                },
                {
                    "sent": "So not here.",
                    "label": 0
                },
                {
                    "sent": "This does assume a functional relationship between our Z&X places, which is not really the case, but.",
                    "label": 0
                },
                {
                    "sent": "Characters.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solving problems.",
                    "label": 0
                },
                {
                    "sent": "So we started with similar descriptors represent our images silhouettes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given a human, let's assume for the moment that we've segmented, so we have a silhouette.",
                    "label": 0
                },
                {
                    "sent": "And while you silhouettes well, they capture most of the information on post.",
                    "label": 1
                },
                {
                    "sent": "As you can see here just by silhouette, we actually get a very good idea of what's going on in this image.",
                    "label": 0
                },
                {
                    "sent": "They can often be extracted from real images.",
                    "label": 1
                },
                {
                    "sent": "The incentive to color changes, textures, clothing.",
                    "label": 0
                },
                {
                    "sent": "So we really don't care what texture we have on the clothes, what color they are and kind of looking at the shape broadly, which makes it more generic to variations amongst people and we require no kind of prior labeling of.",
                    "label": 0
                },
                {
                    "sent": "Which leg is which or any kind of hand clicking it's no labeling at all.",
                    "label": 0
                },
                {
                    "sent": "We're just trying to deal with this shape as an input, and of course they do have limitations and we do lose some kind of information.",
                    "label": 0
                },
                {
                    "sent": "What was one major limitation is that the segmentation we're going to have is never going to be perfect, so we do have artifacts like attached shadows and in accuracies holding the silhouettes possibly.",
                    "label": 0
                },
                {
                    "sent": "And we didn't lose this depth ordering or sightedness information.",
                    "label": 0
                },
                {
                    "sent": "Some parts of the body which included like we don't see anything about the arm here.",
                    "label": 0
                },
                {
                    "sent": "We don't know which leg is which is which and this information which is lost in going from an image to just a silhouette.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, then we do have this problem of ambiguities as well.",
                    "label": 0
                },
                {
                    "sent": "For example, we have shown two different poses which have very similar silhouettes an from silver.",
                    "label": 0
                },
                {
                    "sent": "It's not really very easy to tell which leg is which are exactly next to impossible to tell which is the right thing, which is the left leg with two different cases here.",
                    "label": 0
                },
                {
                    "sent": "And which arm is is left on the right arm?",
                    "label": 1
                },
                {
                    "sent": "Where is the included ARM and sometimes back poses infront poses also confused of in silhouettes.",
                    "label": 0
                },
                {
                    "sent": "So here's kind of evident bottom those can be confused times as well.",
                    "label": 0
                },
                {
                    "sent": "And then these parameters.",
                    "label": 0
                },
                {
                    "sent": "For example in this image, how much is an event we really?",
                    "label": 1
                },
                {
                    "sent": "Well, that's a different issues because there's lots of information when we go to 2D, but there all these kinds of ambiguity issues and.",
                    "label": 0
                },
                {
                    "sent": "Which are associated with dealing with silhouettes.",
                    "label": 0
                },
                {
                    "sent": "So the silhouette supposed problems inherently multivalued given silver does not always a well defined 3D pose.",
                    "label": 1
                },
                {
                    "sent": "Which is why single valued, regressive sometimes behave radically.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we describe ourselves using um shape context histograms.",
                    "label": 1
                },
                {
                    "sent": "The idea is that we'd like to capture the silhouette shape, but the same time be robust against occlusions, segmentation failures, and stuff, so we really don't want to use global shape descriptors, for example, moments or.",
                    "label": 0
                },
                {
                    "sent": "We'd like to base our representation on some local kind of features, so here we use shape context histograms, which is basically a distribution of local shape context responses, so shape contexts are shape descriptors, which were introduced Berkeley Group few years ago, and view here is.",
                    "label": 1
                },
                {
                    "sent": "So start with the silhouette and we treat all the points on the edge edge points as interest points.",
                    "label": 0
                },
                {
                    "sent": "So we've got a bunch of points here and we're trying to describe this sort of points which correspond to the silhouette shape.",
                    "label": 0
                },
                {
                    "sent": "So the shape contexts do is place a small window, a small mask basically on each point and have a histogram here, which is in the.",
                    "label": 0
                },
                {
                    "sent": "Like radio space and theater space.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of having something like this descriptor advantage over here being that.",
                    "label": 0
                },
                {
                    "sent": "We insensitive to kind of less sensitive to scale and.",
                    "label": 0
                },
                {
                    "sent": "And also is controlling the size of this this mask we can we can have control in the locality.",
                    "label": 0
                },
                {
                    "sent": "So I actually find that keeping this mask roughly the size of a limp kind of captures captures video of local information while not losing all global information as well.",
                    "label": 0
                },
                {
                    "sent": "So for each point to consider what we compute, this actually is a 60 dimensional histogram because we have 5 bins in the radial space and 12 discretizations on the angle space.",
                    "label": 0
                },
                {
                    "sent": "So we have 5 * 1260 bins over here for each point in the silhouette, we compute assisted dimensional histogram.",
                    "label": 0
                },
                {
                    "sent": "And so we have a distribution in this shape context space, which is basically 160 dimensional vector for each of these points.",
                    "label": 0
                },
                {
                    "sent": "And then what we do here is basically vector quantize in this space to eventually get.",
                    "label": 0
                },
                {
                    "sent": "A histogram in the shape context space, so we use.",
                    "label": 0
                },
                {
                    "sent": "We do a clustering in this space, which kind of takes advantage of.",
                    "label": 0
                },
                {
                    "sent": "We know what kind of shapes are dealing with their basically different forms of human poses, which is going to be a very reduced area volume in this.",
                    "label": 0
                },
                {
                    "sent": "In this dimension shape, context space.",
                    "label": 0
                },
                {
                    "sent": "So we do a clustering here using K means and obtain 100 centers which are basically a second level of pins.",
                    "label": 0
                },
                {
                    "sent": "And this distribution is shape context spaces.",
                    "label": 0
                },
                {
                    "sent": "Quantized to a single vector, which now encodes pretty robustly the entire shape of that silhouette.",
                    "label": 0
                },
                {
                    "sent": "So it's a single vector here, which represent as did we say that we can capture most of the information on our image by the single vector.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We shape contexts.",
                    "label": 0
                },
                {
                    "sent": "Grams actually in code.",
                    "label": 0
                },
                {
                    "sent": "Locality very well and.",
                    "label": 0
                },
                {
                    "sent": "But they don't lose global information.",
                    "label": 0
                },
                {
                    "sent": "So here I've just shown on analysis, dimensional shape, context, space.",
                    "label": 0
                },
                {
                    "sent": "These are just the first 2 principal components.",
                    "label": 0
                },
                {
                    "sent": "And all the blue points correspond to.",
                    "label": 0
                },
                {
                    "sent": "This 60 dimensional histograms computed for all points on silhouettes over complete video sequence, so this is roughly the kind of distribution we have in the shape, context, space and these black centers.",
                    "label": 0
                },
                {
                    "sent": "Here they came in centers which are using to vector quantizer space.",
                    "label": 0
                },
                {
                    "sent": "If you look at the distribution of points for a single silhouette, for example this one, this is what we get in the shape context space in the first 2 principal components of shape context space.",
                    "label": 1
                },
                {
                    "sent": "So we see that, roughly speaking, it doesn't code.",
                    "label": 0
                },
                {
                    "sent": "The global shape of the silhouette.",
                    "label": 0
                },
                {
                    "sent": "At the same time, it's kind of robust too.",
                    "label": 0
                },
                {
                    "sent": "Tomato segmentation and stuff.",
                    "label": 0
                },
                {
                    "sent": "Because it descriptive R. In the end, local.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so having obtained this center descriptor said, we're going to address this and obtain 3D post from it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So regression model is an only new one.",
                    "label": 0
                },
                {
                    "sent": "The goal is to predict the output vector X which 3D human pose given the input vector Z which we just computed and represented by nonlinear weighted combination of basis functions.",
                    "label": 1
                },
                {
                    "sent": "So we say that X is a sum of a weighted sum over basis functions in the zed space, which we compactly right like this.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "The file is here.",
                    "label": 0
                },
                {
                    "sent": "Basically basis functions which are computed in the silhouettes in the silver descriptor space.",
                    "label": 0
                },
                {
                    "sent": "And they are the weights for the respective basis functions.",
                    "label": 0
                },
                {
                    "sent": "So we just stack up the files together throughout the vector F. And stack up the weights together to get a big matrix A.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like to do here is from training data.",
                    "label": 0
                },
                {
                    "sent": "Try to estimate this matrix a the dependence the relationship between.",
                    "label": 0
                },
                {
                    "sent": "Our descriptor space and 3D post space.",
                    "label": 0
                },
                {
                    "sent": "The basis here we use well could be kernel basis.",
                    "label": 0
                },
                {
                    "sent": "We've tried different kinds of bases and, um, the ones we finally end up using a kernel basis where five K 5K of Zedd's, basically a kernel function defined between zero and the center.",
                    "label": 0
                },
                {
                    "sent": "Zed K. For a given center points and we end up using radial basis functions.",
                    "label": 0
                },
                {
                    "sent": "For the kernels.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the goal is basically trying to estimate that the coefficient matrix A and we do that by regularised least squares, which means that we'd like to minimize this error over all training points, the prediction error.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so any times F minus XI.",
                    "label": 0
                },
                {
                    "sent": "And then we have this regularization term which we call RFA.",
                    "label": 0
                },
                {
                    "sent": "So basically which is a regularizer or penalty function, which we see often in learning methods to avoid overfitting on train data.",
                    "label": 0
                },
                {
                    "sent": "So here we can impose different kinds of regularizers and get different behaviors from a regression function.",
                    "label": 0
                },
                {
                    "sent": "A simple form progression is Ridge regression where what we try to do is.",
                    "label": 0
                },
                {
                    "sent": "Impose a size limit on the.",
                    "label": 0
                },
                {
                    "sent": "On the norm of a on the autonomic way.",
                    "label": 0
                },
                {
                    "sent": "So that keeps all the coefficients mall trying to keep the surface flat and avoids overfitting in that sense.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We actually use for our regression is the relevance vector machine and.",
                    "label": 1
                },
                {
                    "sent": "It's got compared to, we've seen you, Sir, like the support vector machines being used for aggression and.",
                    "label": 0
                },
                {
                    "sent": "This is a slightly different approach, which is based on Bayesian method for its use, both for classification and regression was introduced a couple of years ago with Michael Tipping.",
                    "label": 0
                },
                {
                    "sent": "Phone.",
                    "label": 0
                },
                {
                    "sent": "The idea for a certain machine is basically that we're going to learn the parameter.",
                    "label": 0
                },
                {
                    "sent": "This coefficient matrix A and the kind of priors we impose here.",
                    "label": 0
                },
                {
                    "sent": "On each parameter, we impose a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "So basically on each parameter or group of parameters.",
                    "label": 1
                },
                {
                    "sent": "And it's a form of this prior which is actually important under.",
                    "label": 0
                },
                {
                    "sent": "Which makes the relevant sector machine behave the way it does.",
                    "label": 0
                },
                {
                    "sent": "The form here is, it's basically a nonconvex prior for logarithmic on the modulus.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Of the coefficient matrix.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is by input.",
                    "label": 0
                },
                {
                    "sent": "By adding this term to our minimizations function.",
                    "label": 0
                },
                {
                    "sent": "We not only forcing 80 with small, which helps in generalization, but also it helps in appearing sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "So the way that happens if we look at the force here, the regularization for switches.",
                    "label": 0
                },
                {
                    "sent": "To the derivative of this regularizer respect to a, it's inversely proportional to the size of a.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that as the coefficients tend to get smaller.",
                    "label": 0
                },
                {
                    "sent": "The force increases and these coefficients are further pushed to reason size, so this ineffective very strong pruning going on of the parameters as parameters tend to as the coefficients tend to get go to zero.",
                    "label": 0
                },
                {
                    "sent": "There actually limited from the system.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We automatically we essentially select only the relevant features that we need, and.",
                    "label": 0
                },
                {
                    "sent": "And lots of the coefficients in our final matrix A are essentially zeros, which means that we do not need the corresponding basis functions.",
                    "label": 0
                },
                {
                    "sent": "So this shrinkage strengths are this pruning semantic rule that we want is controlled by parameter new over here.",
                    "label": 0
                },
                {
                    "sent": "And by increasing you, we actually get sparser solutions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We made advantage of the relevance vector machines that get sparse solutions, then only generalized.",
                    "label": 1
                },
                {
                    "sent": "But no sparse.",
                    "label": 0
                },
                {
                    "sent": "So in terms of behavior is pretty similar to the reservation in terms of performance, but the solutions here are much, much sparser.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Would we did vantage we get from sparsity is that when we're dealing with kernel basis, it's only the relevant examples that are obtained from the training set.",
                    "label": 0
                },
                {
                    "sent": "So typically have very, very large training sets, and we don't want to retain all those examples for computations later on.",
                    "label": 0
                },
                {
                    "sent": "So there's an automatic pruning going on, and we select only relevant examples to computer basis functions.",
                    "label": 1
                },
                {
                    "sent": "And if we use linear basis, which basically means that the basis function is nothing but the input, the silver descriptor.",
                    "label": 0
                },
                {
                    "sent": "What we do is a relevant feature selection.",
                    "label": 0
                },
                {
                    "sent": "So from from older have very high dimensional input vector.",
                    "label": 0
                },
                {
                    "sent": "It's not all those dimensions are going to be retained for the regression and only a subset of them would be used to actually estimate pose.",
                    "label": 0
                },
                {
                    "sent": "So it's an automatic relevant feature extraction going on inside.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a briefing on the relevance vector machine.",
                    "label": 0
                },
                {
                    "sent": "The way we use this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get pose from static images.",
                    "label": 0
                },
                {
                    "sent": "Stretching for experiment.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we use real human motion capture data for training our system.",
                    "label": 1
                },
                {
                    "sent": "So the advantage of using such data is that we capture typical human motions and rather looking at all kinds of possibilities.",
                    "label": 0
                },
                {
                    "sent": "So with this extremely high number of degrees of freedom in the human body, we have 54 parameters to estimate for the complete 3D human post.",
                    "label": 0
                },
                {
                    "sent": "It's very, very dense space, but what we do is by using training data from real motions.",
                    "label": 1
                },
                {
                    "sent": "We kind of capture more typical human movements and not just all possible poses.",
                    "label": 0
                },
                {
                    "sent": "We got this data from a public website given here, so there's a lot of motion capture data which is being recorded for different kinds of activities.",
                    "label": 0
                },
                {
                    "sent": "And what we use is to generate examples of.",
                    "label": 0
                },
                {
                    "sent": "Silhouette's corresponding 3D pose.",
                    "label": 0
                },
                {
                    "sent": "Now, Fortunately we don't have the the corresponding silhouettes for these, so we use a graphics package called Poser, which very nicely enables us to to render human motion capture data.",
                    "label": 0
                },
                {
                    "sent": "So it's somewhat artificial.",
                    "label": 0
                },
                {
                    "sent": "But for all this for all this real motion capture data we are able to generate silhouettes from different viewpoints using different kinds of models.",
                    "label": 0
                },
                {
                    "sent": "So we kind of have pretty good coverage of the training space.",
                    "label": 0
                },
                {
                    "sent": "So we have a lot of ground truth data to test and train and test our system on.",
                    "label": 1
                },
                {
                    "sent": "And also we test on real sequences of another person.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've looked at quite a few methods on these lines in terms of progress is we've tried regeneration and relevance vector regression.",
                    "label": 0
                },
                {
                    "sent": "In terms of basis functions, we've tried linear basis functions, several different kinds of kernel functions.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that.",
                    "label": 0
                },
                {
                    "sent": "The technique is not really very very dependent on the kind of progression used.",
                    "label": 0
                },
                {
                    "sent": "The performance is pretty much similar, but we do benefit from the relevance vector regression.",
                    "label": 0
                },
                {
                    "sent": "For example in terms of much more sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "And Gaussian kernels turn out to be the best in terms of our choice of basis functions, and they get slightly better.",
                    "label": 1
                },
                {
                    "sent": "Slightly better performance then.",
                    "label": 0
                },
                {
                    "sent": "A simple linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some results which we.",
                    "label": 0
                },
                {
                    "sent": "Where we estimate post from a synthetic spider walk sequence.",
                    "label": 0
                },
                {
                    "sent": "So we've got a number of such corresponding poses in our training data and these are syllabus which were not included in the training set.",
                    "label": 0
                },
                {
                    "sent": "So there's a new sellers which have been given as a test and from single images we've actually obtained pretty accurate 3D poses.",
                    "label": 0
                },
                {
                    "sent": "For these images, and for this particular constructions with relevance vector regression with caution kernel and were able to obtain.",
                    "label": 0
                },
                {
                    "sent": "We retain only 6% of the training examples, which is basically so we have like 156 support vectors are from 600 training examples so extremely extremely sparse in the sense that we retain a very small percentage of our training data are very compact model.",
                    "label": 0
                },
                {
                    "sent": "And we can pretty accurately get post from a single image, a single silhouette.",
                    "label": 0
                },
                {
                    "sent": "In terms of numbers, the mean angular error per degree of freedom here is 6 degrees.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "So so basically the entire video sequence for some of those frames that we saw.",
                    "label": 0
                },
                {
                    "sent": "As you see here, well in most of the cases there's an accurate reconstruction, but there lots of problems, especially if you look at it overtime.",
                    "label": 0
                },
                {
                    "sent": "The left temporal jitter.",
                    "label": 0
                },
                {
                    "sent": "And you'll see a number of instances of these glitches where estimation seems to be completely wrong with the person.",
                    "label": 0
                },
                {
                    "sent": "Will simply heading angle the direction in which the person is working.",
                    "label": 0
                },
                {
                    "sent": "For example, sometimes the totally it's wrong estimate.",
                    "label": 0
                },
                {
                    "sent": "But more or less, it's able to get posed out of and note that there's no temporal currency imposed here, so it's getting each pose out of a single silhouette.",
                    "label": 0
                },
                {
                    "sent": "So we do mostly OK, but we realize that we have 15% around what we call glitches, where the computer oscillations in the estimation.",
                    "label": 0
                },
                {
                    "sent": "And the problem here is because of pose ambiguities, often from a single silhouette we have lots of confusing poses and which tends to give wrong estimations.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you can do a couple of graphs for the same sequence, a couple of individual angles which plot here.",
                    "label": 0
                },
                {
                    "sent": "For example here, plot the hip angle of the person while he's walking in circles.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the dotted curve shows the true value of the hip angle which we have from ground truth.",
                    "label": 0
                },
                {
                    "sent": "And here's the estimation.",
                    "label": 0
                },
                {
                    "sent": "So we kind of we tend to follow the curve basically overtime, so there's no loss in time as estimating from individual images, but you'll see several instances of glitch and cases where this completeness estimations, for example.",
                    "label": 0
                },
                {
                    "sent": "Here is a very large error and.",
                    "label": 0
                },
                {
                    "sent": "This is another parameter where we plot the overall heading angle of the person, so this varies from minus 1 to 2 + 90 degrees.",
                    "label": 1
                },
                {
                    "sent": "The persons working complete circles and as you can see it's kind of stable with time, but lots of these instances of glitch or estimations.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these glitches there is also OK, but we have these glitches and the reason is because the regressive chooses wrong around case of animals, pair or remains undecided when we don't have enough information, the silhouette.",
                    "label": 1
                },
                {
                    "sent": "This problem is actually it's very evident in the case of overhead angle, which is the most visible parameter direction with the person is working now.",
                    "label": 0
                },
                {
                    "sent": "This thing is extremely hard to estimate because various through a complete 360 views range.",
                    "label": 0
                },
                {
                    "sent": "What we do here is to avoid them.",
                    "label": 0
                },
                {
                    "sent": "This rewrapping discontinuous thing actually addressing cost & Theta.",
                    "label": 1
                },
                {
                    "sent": "So we can see that.",
                    "label": 1
                },
                {
                    "sent": "The normalization of this unit vector which so this costs.",
                    "label": 0
                },
                {
                    "sent": "With science we should be in a vector and denormalization, often causes problems.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This shows the errors in the same thing and we can associate instances of high error which is shown in this graph with slower plot here shows the norm of the cost assigned to the vector.",
                    "label": 0
                },
                {
                    "sent": "And whoever this vector is Miss estimated basically causes high error estimation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some examples of same method on real images.",
                    "label": 0
                },
                {
                    "sent": "Pretty much accurate estimation some cases of.",
                    "label": 0
                },
                {
                    "sent": "Confusion between legs or misestimation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've actually seen this already.",
                    "label": 0
                },
                {
                    "sent": "The problem is coming from the fact that.",
                    "label": 1
                },
                {
                    "sent": "But it's actually a multi brand surface in the Xbox.",
                    "label": 0
                },
                {
                    "sent": "Is Dead Space and the cases where we have multiple values for silhouette, so it's a function treatment.",
                    "label": 0
                },
                {
                    "sent": "The assumption that we're treating as a function is actually round to give problems in some cases.",
                    "label": 0
                },
                {
                    "sent": "So where the real solution to it would be to try to set up a multivalued regressor.",
                    "label": 1
                },
                {
                    "sent": "Which we should possibly give more than one solutions from a single silver.",
                    "label": 0
                },
                {
                    "sent": "What we do is a possible solution is to resolve these ambiguities using temporal information.",
                    "label": 1
                },
                {
                    "sent": "So we made the same thing in.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I recover post from.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sequences and which is able to reduce glitches by embedding the thing in a tracking framework.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that since we have information about the previous time state, we can use this temporal information too as a hint to kind of select the correct solution from if there are more than one possible solutions.",
                    "label": 0
                },
                {
                    "sent": "Under Tinker State information with the typical dynamical model to kind of give an estimate of the state at a time T, and we incorporate that into a regression framework to kind of get a correct answer.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just go over this very brief.",
                    "label": 0
                },
                {
                    "sent": "The way we do that is that we learn a dynamical model, which is a second order model which helps us to predict the state at time T given two previous States and so this is again learn from the data.",
                    "label": 0
                },
                {
                    "sent": "And the observation update is a slightly complicated model.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "What it does is that.",
                    "label": 0
                },
                {
                    "sent": "Within the regression framework, within the kernel, we not only have the observation at that time T which is coming from the image, but we also have this estimate of the pose which is given as an input in the regressor.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that we selecting examples in the kernel space which are not only closing silver space but also the state space.",
                    "label": 0
                },
                {
                    "sent": "And through this indirect dependence on state estimate, we're not really very sensitive to temporal miss tracking, but it does help in resolving this ambiguity's.",
                    "label": 1
                },
                {
                    "sent": "At the same time, we have a dependence on the linear dependence on the estimate, which helps in ensuring.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "False witness.",
                    "label": 0
                },
                {
                    "sent": "So bearing in tracking framework, we see the errors go down by a couple of degrees and the same graph become much much smoother with highly hardly any situations or glitches.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So same sequence by.",
                    "label": 0
                },
                {
                    "sent": "Regressing in this joint.",
                    "label": 0
                },
                {
                    "sent": "Regression framework we were addressing on not only the silhouette but also some little bit of temporal information to resolve ambiguities.",
                    "label": 0
                },
                {
                    "sent": "We get pretty smart smooth reconstruction, so from a single camera, so that space and.",
                    "label": 0
                },
                {
                    "sent": "We're not very sensitive to.",
                    "label": 0
                },
                {
                    "sent": "Temporal miss tracking.",
                    "label": 0
                },
                {
                    "sent": "Because we're looking at the observation at every step that we are using temporal information to some extent to resolve ambiguities and also helps in giving smoothness.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same method applied to several images.",
                    "label": 0
                },
                {
                    "sent": "Well quality of construction is still worse.",
                    "label": 0
                },
                {
                    "sent": "I'm going to number of factors about segmentation and stuff, but we are more or less able to achieve.",
                    "label": 0
                },
                {
                    "sent": "Track from a single camera and real images.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, conclusion so kind of formula, compact model for direct regression on image observations.",
                    "label": 1
                },
                {
                    "sent": "We don't need anything.",
                    "label": 0
                },
                {
                    "sent": "Any explicit 3D model.",
                    "label": 0
                },
                {
                    "sent": "Which ensures easy adaptability to different problems.",
                    "label": 0
                },
                {
                    "sent": "We do an exploit temporal currency in sequences by modeling dynamics, but it's an indirect dependencies, so we're not really very susceptible to miss tracking and the fact that we can actually estimate pose not always accurately, but mostly accurately from individual images makes it potentially self initialized tracking method.",
                    "label": 0
                },
                {
                    "sent": "And rightly so, if in the same sequence, if we initialize automatically by estimating from single pose in around 84% of the cases, actually were able to estimate able to track successfully by automatically initializing.",
                    "label": 0
                },
                {
                    "sent": "The smallest advantage that we do it assumes segmentation for our seller input.",
                    "label": 0
                },
                {
                    "sent": "But if you can go by that, then the method works pretty good.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a comment more about the use you mentioned that you want this multivalued function approximation, right?",
                    "label": 0
                },
                {
                    "sent": "The good news is that people have worked on that for a long time, so things called mixture density networks or mixtures of experts, right?",
                    "label": 0
                },
                {
                    "sent": "Actually, you know, have been around to do that, so you could easily plug that in.",
                    "label": 0
                },
                {
                    "sent": "Basically makes your conditional on your zed.",
                    "label": 0
                },
                {
                    "sent": "You get a mixture distribution OK, and so you know the idea is you'll be in one of those modes and then also.",
                    "label": 0
                },
                {
                    "sent": "For hooking up the things through time, you have those bunch of modes or bunch of whatever.",
                    "label": 0
                },
                {
                    "sent": "Yes, you're right.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So the things that we have been seeing.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that it's definitely possible and we actually working towards trying to prove that.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that it just kind of becomes a little more complicated.",
                    "label": 0
                },
                {
                    "sent": "Extremely high dimensional spaces, so trying to see if we can actually workout this problem.",
                    "label": 0
                },
                {
                    "sent": "Those issues that come in will try and do this clustering and these mixture methods in this high dimension spaces.",
                    "label": 0
                },
                {
                    "sent": "But it's true that that's something we can explore too and some instruments.",
                    "label": 0
                },
                {
                    "sent": "Showing that they're going to pults.",
                    "label": 0
                },
                {
                    "sent": "That's right, there's not just just you know two or three solutions.",
                    "label": 0
                },
                {
                    "sent": "What happens is that there are thousands of different branches.",
                    "label": 0
                },
                {
                    "sent": "Which branch and rejoin in principle thousands, at least in practice.",
                    "label": 0
                },
                {
                    "sent": "Probably only 10 or so, but the space is not just a multiple regression problem, it's more complicated than that.",
                    "label": 0
                },
                {
                    "sent": "Well, in theory the the the one could deal with that once.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it would be easy, but the theory would allow that.",
                    "label": 0
                },
                {
                    "sent": "You know the point that in practice this more issues which come up.",
                    "label": 0
                },
                {
                    "sent": "But yes, it is potentially.",
                    "label": 0
                },
                {
                    "sent": "Consult.",
                    "label": 0
                },
                {
                    "sent": "How serious is your disadvantage?",
                    "label": 0
                },
                {
                    "sent": "Can you get away with a poor segmentation?",
                    "label": 0
                },
                {
                    "sent": "Yes, so actually the fact that we are presenting silhouettes by these robust local descriptors does show that we are in some sense, some robust too.",
                    "label": 0
                },
                {
                    "sent": "Not very good segmentation as you can see in the real.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this example, so it's not really ideal.",
                    "label": 0
                },
                {
                    "sent": "Silhouettes the kind of solution we've trained on.",
                    "label": 0
                },
                {
                    "sent": "We've trained on very different silhouettes, which look much cleaner, and we're actually pretty accurately able to get post even with such noises, silhouettes, but.",
                    "label": 0
                },
                {
                    "sent": "Right we do need segmentation even to get service quality, so that's the basic point.",
                    "label": 0
                },
                {
                    "sent": "But we are quite robust.",
                    "label": 0
                },
                {
                    "sent": "Two holes into local segmentation errors and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "One thing we've tried is training on part of the silhouette only, and you get less good but reasonable results.",
                    "label": 0
                },
                {
                    "sent": "Training on working on just part of the solid.",
                    "label": 1
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK, let's stop living.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}