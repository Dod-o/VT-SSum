{
    "id": "hq6ieyrhf2jdyxhfy5o4tdyr2xzi4skn",
    "title": "Experiments with Non-parametric Topic Models",
    "info": {
        "author": [
            "Wray Buntine, Faculty of Information Technology, Monash University"
        ],
        "published": "Feb. 20, 2015",
        "recorded": "January 2015",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/solomon_buntine_topic_models/",
    "segmentation": [
        [
            "So this is a longer talk and I'll be cutting through bits of it, 'cause it's not all relevant.",
            "So don't be confused if you see some funny slides and material flypast.",
            "Also do interrupt so because we've got, there's all sorts of stuff here that you may or may not have seen before.",
            "I'm not very familiar with your background, so mostly people are machine learning.",
            "Yeah, automatics computer science, yes, but they are.",
            "In general, I should say the number of students and ex students who are involved in this car wise just finishing landus been out a few years but he got this whole area going with me.",
            "Some time ago and Swapnil.",
            "He's done more recent work, but actually his big thing with us was getting the multicore version working so.",
            "Which makes a huge difference.",
            "I must say it's so good having stuff run multicore.",
            "Makes a big."
        ],
        [
            "Difference so I want to just talk briefly about motivation and our general what we're trying to achieve.",
            "The real part of this talk is our topic model system, which has it's a fairly fancy theory and there's a lot of aspects to it, but what I'll be giving you is the basic ideas.",
            "This is as far as I can tell from our experiments.",
            "Is the best performing topic model system currently in.",
            "In on a number of dimensions, it's not as fast as Mallett, but it is on all the other metrics.",
            "It works the best, so the whole point of this talk is to wrap around this.",
            "At the end I've got this some other bits of fun we've been having, and these I'm not going to go into detail, but I'll just describe briefly.",
            "So motivate."
        ],
        [
            "And the usual stuff information overload.",
            "There's way too much."
        ],
        [
            "Information warfare.",
            "Everyone's out there trying to fight everyone else.",
            "You know the Democrats are editing the Republican Wikipedia pages and vice versa so, but these are the things we face at the moment."
        ],
        [
            "It's way too much information now, in largely in language processing, but in other areas related we have the one of the core tools.",
            "We have a probability vectors.",
            "And this is what we've figured out how to work with really well."
        ],
        [
            "So just to give you an idea, you can use a probability vector to represent the parts of speech of a word.",
            "Maybe you've got 10 part 60 parts of speech hashtags.",
            "Maybe there's 5000 hashtags you might want to use.",
            "Well, you can have a probability vector over them so.",
            "You can see many things we, in an intelligent system, will have probability vectors, or we can have probably look at probability vectors underneath what we'd like to do with them.",
            "The things like inheritance.",
            "Sharing of information.",
            "Networks passing networks around and then general inference and learning with these probably."
        ],
        [
            "Vectors, just to give you an example.",
            "What I mean by inheritance and or sharing.",
            "Doesn't matter what is the content here, but maybe its words.",
            "Maybe it's subject matter of some kind.",
            "What is in the book you expect to be related to what's in the chapter?",
            "What's in the paragraphs?",
            "You expect to be related to what's in the chapter.",
            "If you know the content here and here, you should get a rough idea of the content here.",
            "If you've got the content here, here and here, you should have a better idea of the content here.",
            "So there's inheritance and sharing up and down the hierarchy.",
            "Now the traditional way of doing inheritance and sharing or dealing with probability vectors in statistics is something called a richlite distribution.",
            "I'm not going to be going into the details of that.",
            "Some of you may recognize it.",
            "It looks very much like a multinomial.",
            "Form, but the main thing I want to point out is that if you put these into a hierarchy so we start with a.",
            "With the parent we get a first child that first child is then used as the parent for the grandchild.",
            "So it's this situation here.",
            "When you have this, these are the and you've got to ritualize.",
            "This is the functions you get.",
            "This is a distribution for everybody and it's a mess.",
            "It's not something we can deal with cleanly from a mathematical perspective, so dealing with Irish lies is something we can't do.",
            "But nevertheless AI and machine learning spend a large part of the 90s dressing everything up into rich lies, whether it be grammar, learning or decision tree learning or those net learning, it's one of the main tools we."
        ],
        [
            "So.",
            "What we've figured out is how to do this hierarchical learning quickly, well, moderately efficiently.",
            "Now, this is a.",
            "This is really something I do.",
            "I do want to point out it's not going to be going into the details of this in this overview talk.",
            "But the current standards if you attend a standard machine learning tutorial or if you read some of the more recent papers, you'll see what we're using is something all the Pitman Yor process display process.",
            "You'll see a lot of terms like Chinese restaurant processes and stick breaking.",
            "And what we've found is that these techniques I'm saying this is a punchline, and probably 90% of you have no idea what I'm talking about.",
            "But those of you who do not I'm talking about.",
            "The punchline is don't use them.",
            "So we've got better replacements or a lot more efficient.",
            "OK, so maybe some of you.",
            "Got that, but want to be doing is showing the sort of things we're doing with these.",
            "I won't be going into the algorithms that's that would be another whole tutorial.",
            "But what we're using this for is modeling hierarchies, doing inheritance, bringing in semantic resources like sentiment dictionaries and Wordnet, bringing them into the probability calculations.",
            "And one thing I realized as students had been giving me all these material on deep neural networks was that the about 2/3 of the opening introduction of many deep neural network tutorials for exactly the same as what we were doing.",
            "So I realized we really addressing the same kind of techniques but with different web addressing the same problems but with different.",
            "Techniques.",
            "And now there's some people trying to sort of bring some of the deep neural network technology into the probabilistic.",
            "Some of the more nonparametric methods, which is what I do so."
        ],
        [
            "Anyway, I'm not going to."
        ],
        [
            "Go into the richley distributions here, that's a.",
            "Part of the longer talk."
        ],
        [
            "Though this is the one slide which is probably important to so you can get an idea of things.",
            "Here we have a probability vector and that's going to be the mean.",
            "Of our distribution, and we're going to produce samples from it.",
            "So here we have.",
            "Three probability vectors that have been sampled using this as a mean, this one.",
            "Has what's called a high concentration.",
            "Concentration is the inverse variance in statistics.",
            "With so if it's a high concentration, it's a low variance, so this sample is virtually the same as the mean.",
            "This one with the low concentration means it's got a high variance.",
            "And this sample is very different to the main, so we can have our.",
            "We can have Dirichlet distributions and we can make them have very low variance or very high variance by changing the key parameters.",
            "When you say sample samples, we mean so this is really just.",
            "This is kind of an empirical distribution sampled from some underlying this yes, so this is the.",
            "Sampling is a distribution of probability vector, so it's not like sampling a real number X will sampling an entire vector.",
            "Yeah, that's what makes them harder to work with because you're dealing with vectors."
        ],
        [
            "So we'll skip through."
        ],
        [
            "Through"
        ],
        [
            "This other stuff and now I'll do what I call old school probabilistic reasoning, which might be old school for you.",
            "This is going way back to the 90s, which some of you I know.",
            "Some of my students were in primary school back then, so or earlier."
        ],
        [
            "So this is this is a topic model, so a latent original allocation and.",
            "Some of you may have seen that, and my whole point of this is, well, here's a here's a structure which represents the model.",
            "What it's actually is.",
            "This is a matrix of word data, and this matrix has been approximated by a matrix of.",
            "Of word probabilities in a matrix of topic probabilities.",
            "Now when we model it with our darish lies and we do some analysis, this is.",
            "The formula we end up that we want to optimize and this is just two Dirichlet's back to back.",
            "So.",
            "What happens is whenever you're doing learning Bayes networks, learning decision trees probabilistic.",
            "Context free grammars.",
            "Hidden Markov models when if you're doing any of these things, what you get is a holiday formula that look exactly like this strung together and what you're fiddling with is the numbers here.",
            "You're fiddling with the some indicator variables inside these counts.",
            "Here you're sampling them, but so this is the sort of formula that that many of us spent a whole decade playing with in the 90s."
        ],
        [
            "As I said, trees, graphs, networks, context free grammars.",
            "It's all done with this sort of formula.",
            "And once you've got the formula down, then you do search do something called model averaging.",
            "There's a number of things you can do.",
            "Now."
        ],
        [
            "Now, but the key I've got one key idea about this.",
            "I want to get across."
        ],
        [
            "And that is that the simple Dirichlet distribution doesn't really have the.",
            "Capability that we want to do this deal with this sort of reasoning."
        ],
        [
            "Now, to illustrate that, here's a context free grammar so.",
            "The idea is you you apply these rules, but when you apply the rule you apply it.",
            "You ignore the context around.",
            "So when we say we've got this so far, there's a verb phrase here.",
            "When we go to expand the verb phrase, we ignore.",
            "That context it's irrelevant to us.",
            "It's irrelevant to the theory of context free grammars.",
            "We ignore that and then we apply our rule to get to the next level and again.",
            "So that's the context.",
            "Free grammar.",
            "The problem with this is that, well, they do."
        ],
        [
            "Work Google bought YouTube.",
            "What can go in there?",
            "Probably not an ice cream, or, you know, a pencil.",
            "Google bought something.",
            "We have an idea.",
            "It's going to be something that a big Corporation buys.",
            "So context is everything.",
            "It's really important and this applies all over inference.",
            "This is why the so whereas."
        ],
        [
            "These models have a beautiful statistical theory with Dirichlet distributions.",
            "It's no good.",
            "You can't really use it.",
            "So a lot of work in LA."
        ],
        [
            "Eating grammars is how to make a context sensitive that all started in about 1990 will probably earlier.",
            "I'm not a language expert by the way."
        ],
        [
            "Learning based networks is similar."
        ],
        [
            "Um?",
            "I'll show you an example.",
            "Now of this is called an engram, so an engram is they used in language model, so it's used in speech recognition.",
            "1 gram is where you look at the.",
            "The word by itself for 2 gram, you look at the previous word.",
            "The three gram you look at the two previous words in making your prediction of the next word.",
            "So an ngram you look at the previous N -- 1 words.",
            "Now when you build this up with Darish lies, it works OK.",
            "It's not too bad.",
            "But but it ignores context again, which which can be important.",
            "And this is the context for the 2 gram context for the three gram, you can see it's important we'll get to this in a bit."
        ],
        [
            "So in 1995 one of the state of the art.",
            "Algorithms and this was actually for.",
            "Language compression of N grams with something called context tree weighting.",
            "So in this uses a technique called Bayesian model averaging.",
            "This is taking Darish lies to a ridiculous extreme."
        ],
        [
            "But, and this is where my recent work comes in, in 2006, Uyt developed something called the Hierarchical Pitman.",
            "Your model, and he then published a paper with some reason.",
            "I don't have it here.",
            "I'll do that afterwards.",
            "They figured out how to use that in something called the sequence memorizer and this.",
            "Beats CTW now.",
            "CTW is was a very important compression algorithm because it had a nice theory and it worked very well.",
            "If you work in, say, language, images, anything you know if something has a really good theory and it works well.",
            "That's really hard to do.",
            "But the sequence MEMOIZER, which is based on this theory from Morocco Pittman, yours of Tay, that actually beats CTW, so this is a whole new era was starting a whole new technological capability came in.",
            "And the the reason is that it now let's you model context in the.",
            "In the statistical world."
        ],
        [
            "It's probably the one message.",
            "I would like to take away, I'll give you."
        ],
        [
            "The basic idea of this modeling of context."
        ],
        [
            "Here.",
            "And then we'll go onto how we're using this in our topic models.",
            "So here is our.",
            "Part of part of US language again and.",
            "What goes here?",
            "If you've just seen an A?",
            "You know something?",
            "Well, it maybe it's singular, or maybe it's an adjective, but it's probably not a plural noun, so you know there's some restriction.",
            "When you've actually got a context of three words, now caught a well that narrows it down a bit more.",
            "But the key idea here, other than the context is important is it's hierarchical.",
            "So.",
            "Having what's the context when you know he caught a well, it should be related to this context.",
            "Here it should be similar.",
            "So whatever probability vector goes here over the possible words here, it's probably similar to this.",
            "In fact, as you get to say, a 10 gram.",
            "It's going to be very similar.",
            "You expect it to be a low, high low variance.",
            "Relationship with this probably has a higher variance relationship.",
            "This probability vector over the words there should be somewhat similar to that the general vocabulary of probability vectors, and here it will be probably a bit more restricted, so it will be.",
            "Lower variance, but you expect that this distribution.",
            "For the word here has this distribution as a mean.",
            "So that's the idea of context and the mean of probability vectors."
        ],
        [
            "And if we put that into an abstract sense, what we end up?"
        ],
        [
            "With is.",
            "The probability vectors are related and the probability vector with three bits of context has the one with the first 2 bits of context as its mean, which has the one with one as its mean, which has the general probability vectors.",
            "So this is the idea of context, so this is what you YT had introduce."
        ],
        [
            "Together with Michael Jordan and Bielen Bly, they they developed a variation of LDA called.",
            "HDP, LDA.",
            "It's absolutely been called, and they developed hierarchical Dirichlet processes, and this this appeared in a statistics Journal disappeared in a ACL conference.",
            "Then Nips took over.",
            "This was the the darling of nips for couple of years.",
            "If you you just have to put, you know, Chinese restaurants somewhere in your paper and you up the chances of acceptance dramatically.",
            "Bye."
        ],
        [
            "But we we were playing with something else probably happened because I wasn't doing great deal of research at the time, so I was going through.",
            "I was dealing with other things and but we we came at things from another angle.",
            "We actually showed that these techniques aren't as good as you can get.",
            "You can really improve on them.",
            "So since then, we've been applying our technology to a whole bunch of problems every time we apply it.",
            "We substantially beat the Chinese restaurant process equivalent, so this is, if you like this talk is part of a sales pitch to try and sell you the techniques the techniques are written up in.",
            "Some of our tutorials, but I won't be going into them in this in this talk."
        ],
        [
            "Yeah, the wording they had."
        ],
        [
            "Loving the ones you listed here.",
            "Franchise multifloor, oh it gets it gets really complex, yeah?",
            "So we have a we what we do is we eliminate some of this stuff and we basically really all we're doing is marginalizing in fact, really all we're doing is, if you were cynical, you'd say we'd taken one of the algorithms out of TE and figured out we could apply caching to speed it up.",
            "So that's probably one of my big contributions to this field was to use caching, so.",
            "I'm actually there's a bit more than that, but I was surprised, so we'll just flip through."
        ],
        [
            "Through this formula probably explains everything, but."
        ],
        [
            "We'll get back to the situation.",
            "We've got our parent probability vector.",
            "The probability vector that we may we're dealing with and our data.",
            "What people do is you want to integrate this out so you can do reasoning and eliminate a whole vector of parameters.",
            "If we've got 10,000 words in our vocabulary, we love to eliminate 10,000 real valued parameters from our inference, it makes things a lot more reliable.",
            "Statistics always works better when you've got 10,000 less real values to estimate, so you want to get rid of this if you can.",
            "And when you do this in the classic Dirichlet multinomial situation.",
            "And I'm skipping a bit here."
        ],
        [
            "You get this formula.",
            "Now this is a funny notation, but what this is, it's a polynomial.",
            "So what you get is the probability of your data.",
            "Given the grandparent, probably, or the parent probability, and having an integrated out P, the probability vector.",
            "Is this your polynomial?",
            "So you're passing a message up to the parent with a polynomial in it, and that makes things difficult to deal with.",
            "What we have is we we introduce a new set of latent variables which mirrors the counts and we pass up a multinomial.",
            "So that's the key thing.",
            "We're passing up a multinomial as data, and that can then be processed.",
            "That's about as mathematical as I'll be getting.",
            "So.",
            "Skip this."
        ],
        [
            "So I'll now go on to our topic models."
        ],
        [
            "This is a topic model you're given some.",
            "Bonnie Pelton yeah.",
            "So you're given these are called components in the word.",
            "Case of words, components would be words with a similar semantic meaning, and you're going to do a linear combination of these to get whatever it is you're trying to estimate in this case.",
            "To get a bag of words.",
            "So you take a linear combination of them.",
            "That gives you a probability vector for generating words.",
            "You generate a bag of words.",
            "So that's what a."
        ],
        [
            "Opponent models are there's a whole bunch, maybe familiar with ICA.",
            "For speech recognition, PCA, principal components analysis.",
            "Learning Code books is used in image recognition, non negative matrix factorization.",
            "Another variation of it topic modeling and they all changing the data may be non negative.",
            "You may use least squares or cross entropy.",
            "So depending on how you set it up, topic modeling is usually done with words which means you got non negative integers 'cause their counts.",
            "So the data is counts and what you like is probability vectors as your components.",
            "So these accounts, most of them are zero.",
            "And you'll have probability vectors in here and here.",
            "So that's the.",
            "The set up."
        ],
        [
            "Now.",
            "Oh why should you care about this abstract problem?",
            "In fact, why should I care about this abstract problem?",
            "I'm doing this for two reasons.",
            "One reason is we've had this code sitting around for awhile and we never really published it properly.",
            "We knew we had good methods, but we never really shown how good they were, so we thought.",
            "Well, maybe we have to go the next step.",
            "But the other reason is that you actually use topic models in."
        ],
        [
            "In many interesting ways now.",
            "For one thing that they can come up with really good semantic associations.",
            "There's a lot of ways of doing this, but for example, this comes from the New York Times News from a 20 year.",
            "One of the components you can build about 2000 topics out of this.",
            "So it's like PCA with a 2000 vectors.",
            "Here is one of those topics.",
            "Is the anthrax event, but it picks it out perfectly, so the ability to get clear, coherent word lists is quite remarkable."
        ],
        [
            "But This is why it's important.",
            "Here's a topic model here.",
            "The rest of this this is currently the best document segmentation model in practice, so if you want to break a document up into meaningful chunks, the best way to do it is with an algorithm developed by Landu.",
            "And Mark Johnson and myself.",
            "It's substantially better than quite a few others.",
            "And it has a topic model inside.",
            "Now, that doesn't mean it's a subroutine, because it's the model.",
            "What it means is that the model is inside.",
            "The algorithm is very different.",
            "But we're using basically the technology that we've developed for topic models to do the document segmentation."
        ],
        [
            "So there's good reasons to do.",
            "This and people are using this was at KDD this year.",
            "This was a very good piece of work.",
            "Showing how to connect ratings and sentiment done from sort of a CMU alliance with Singapore Management University and.",
            "This is an extension of the topic model in a way."
        ],
        [
            "Now.",
            "We had David Lewis visiting recently.",
            "He's an IR guy who did machine learning.",
            "He's a very practical guy.",
            "He helped start the IR for legal discovery.",
            "Track if that's a huge business now in the US especially.",
            "Yes, yes.",
            "So we we encounter David work all the time.",
            "He did two Reuters datasets and but he was visiting and he was saying he was raving about topic models are it's a terrible field, you know they.",
            "It's like a Rorschach inkblot test.",
            "If you stare at a bunch of words long enough, you're eventually going to see something.",
            "Well, you know I was laughing at this stage because I was doing topic models and and he was sitting there ripping in."
        ],
        [
            "I was laughing 'cause I agreed with him because there was an awful lot of you know we're coming out of that phase basically.",
            "But there was an awful lot of earlier work where it was, well, look at this cool stuff.",
            "To a degree, machine learning has done that in various stages.",
            "When a field gets going.",
            "But the two measures we use currently, this one probably isn't that meaningful, and I know some people who refuse to use it.",
            "It's perplexity, and it's basically a measure of surprise, and it's also used in speech recognition and some other tasks.",
            "The other measure is PMO, which is pointwise mutual information, which basically is a measure of how comprehensible the topics are."
        ],
        [
            "This has a very good PMI score.",
            "'cause each of these words is highly related.",
            "Though you gotta be careful because you know Dash Lee.",
            "Now, if you know the news, you know this is highly related to anthrax spores.",
            "But in a general semantic sense, it isn't.",
            "PMI is is is a tough."
        ],
        [
            "Measure to use.",
            "It measures coherence in some areas.",
            "In medical data it doesn't work very well because there's so many strange specifics that you might have in that field that that you can't go and collect coherence.",
            "From but nevertheless, these are the two sort of for good or for bad.",
            "These are current."
        ],
        [
            "Scores used"
        ],
        [
            "So we looked at a bunch of work.",
            "Some of that we're extending some of it away comparing against, so we compared against a number of state of the art algorithms.",
            "Um?",
            "Extending topic models has become a bit of a.",
            "It's a fashionable thing to do if you like at NIPS or I smell, you know to have a new fancy nonparametric model that is better than others.",
            "And and there's a lot of papers along this line.",
            "Nevertheless, so so we'll look at where we went with this, the.",
            "This is an interesting one, which we'll get to."
        ],
        [
            "So there's a key technology that that we didn't introduce.",
            "In fact, we haven't done a whole lot.",
            "That's that's knew.",
            "I could say if I was cynical, we just put it together.",
            "Well, so.",
            "This idea comes from Charles Elkan, who's who's left.",
            "San Diego told and is now Amazon.",
            "Charles Elkan and a student.",
            "So if you take a bag of words so this is a fragment of a news article, you bag it up.",
            "Now you expect the stop words to occur multiple times.",
            "Cabinet occurs twice.",
            "If you have about 20 words, what's the chance that a moderately rare word would occur twice?",
            "This isn't the secretary problem.",
            "Not the birthdays problem.",
            "It's not the birthdays problem.",
            "It's it's pretty low, it's just not going to happen, so the fact and this always happens with say, news articles or something.",
            "The thing is, there topical news articles are not random words.",
            "Someone's got a set of related topics and they're writing about it, and words are going to repeat.",
            "So this is called burstiness.",
            "The words burst, and in fact there's a statistical.",
            "There's a whole theory of statistics developed to deal with this.",
            "That as it happens, is what do rich like processes and Pitman yor processes before.",
            "Sir.",
            "These guys developed a topic public model that dealt with burstiness, unfortunately.",
            "It was pretty slow, so they couldn't really scale it up even though it worked very well.",
            "People kind of didn't.",
            "It didn't go.",
            "It wasn't taken up, you might say, and I think it was for computational reasons."
        ],
        [
            "What's more interesting about this is that there's been some work in the IR community, and this is a survey article.",
            "And.",
            "Where the ideas of burstiness is very closely related to the theory of IR.",
            "So they have an ocean called Eliteness.",
            "Birth burstiness is somehow fundamentally related to TF IDF.",
            "It's more like conceptual term, right?",
            "More than the like BM 25 is the actual formula yes yeah.",
            "So BAM 25 originally was motivated by two plus on model which you can play around with it and show that the you can get a related model using the Pitman you're so.",
            "So there is some relationship between the notion of burstiness and modeling it with with the nonparametric methods and the TF IDF formulas, which we're still trying to get to the bottom of this.",
            "But this is just an interesting."
        ],
        [
            "Connection, but nevertheless burstiness is something that we've added to our model because we know how to do these things efficiently.",
            "We've made it run very fast.",
            "So let's look."
        ],
        [
            "The evolution of topic models and what we've done.",
            "What sort of is makes our new system work?",
            "Well?",
            "This is probably the punchline of the talk.",
            "So this is classic LDA, which I showed earlier you might have.",
            "You have a matrix here which would be millions.",
            "Probably billions.",
            "This matrix who counts.",
            "And it's words by by documents here you've got a matrix of topics by words.",
            "And this is millions the size of it, and this one is.",
            "This matrix is topics by documents and you're multiplying these two matrices to estimate this.",
            "Now, if you if you've got a million.",
            "Parameters in this matrix.",
            "There's only one hyperparameter here.",
            "So as a sort of standard Bayesian reasoning, in fact, standard convex, any sort of standard optimization million parameters, one parameter is all you have to fiddle hyperparameter.",
            "Not good enough, you should have 1000 hyperparameters.",
            "'cause if you got so much data there so many numbers you should be able to have more numbers in your.",
            "So the first thing we do is we make these vectors.",
            "And that is in fact what the very influential paper in 2009 by Minoan Wall akin.",
            "Someone else did."
        ],
        [
            "They and the one that worked for them was something called asymmetric.",
            "They used an asymmetric dress like prior, and it's a symmetric symmetric LDA.",
            "It was implemented in 2007, available in the code 2008, and as it happens it's the 1st and in many ways still the best implementation of nonparametric LDA.",
            "And most of these other papers here keep in mind."
        ],
        [
            "Here's all the.",
            "This 2009 is beats most of these or its own parliament, but no one realized it was tackling the same problem.",
            "We sort of had a better look at it and realize it's just the same thing.",
            "It's a truncated.",
            "Method for doing what these guys are doing.",
            "So that's the next level."
        ],
        [
            "And this is a related thing.",
            "What we're doing is we're putting a darish light process, and these are parameters for the rousselet process on these two.",
            "Now."
        ],
        [
            "Now we go and.",
            "Learn this vector as well, so we're going to make this vector accessible to the machine learning machinery.",
            "And it's got parameters for the for this vector here, and we put a Pitman your distribution on these.",
            "So here we've got a hierarchy of probability vectors, a small one.",
            "Here and here."
        ],
        [
            "The next level is.",
            "We add this burstiness capability and.",
            "This is exactly why I believe it's exactly the same idea as the original paper from Doyle and Alcam.",
            "I'll try and explain to you what you're doing here, so you've got a bunch of topic vectors.",
            "What you're going to do is specialize every one of those topic vectors for the document.",
            "So imagine you got a topic that's all about things in Russia.",
            "Well, it's a whole.",
            "It's a maybe newspaper collection so you won't have a bunch of Russian topics.",
            "You're just going to have one.",
            "But maybe you'll have an article about the Crimea or an article about Moscow or my article about Siberia.",
            "This document is going to specialize the Russian topic into a. Siberia topic or a Moscow topic?",
            "So that's what this capability for specializing is doing.",
            "And if you don't have enough need for your own Siberian topic or your own Moscow topic.",
            "And.",
            "Then you do this.",
            "You'd have this ability to specialize a topic just for the document.",
            "Now, from a statistical perspective, this is crazy because you've got, say, 100 words in here, 200 words.",
            "And with those 200 words, you're going to estimate a matrix whose size is about a million parameters.",
            "So 200 words.",
            "You estimate a million real valued parameters impossible, and This is why for about a month after I'd read their work, I thought they were crazy and in one morning I woke up their geniuses.",
            "This is brilliant.",
            "I realized what they were doing, figured out we could implement it very quickly using our our tricks, the way we implemented, it's about 50% slower than classic LDA, both in memory an in time.",
            "So that's the next level."
        ],
        [
            "I'll show you the."
        ],
        [
            "The bits of this these are the probability vector hierarchies.",
            "That we're processing with our.",
            "Out techniques."
        ],
        [
            "These are the hyperparameters.",
            "That we're learning with generic.",
            "Methods you could maybe slice sampling or one of these sorts of generic sampling methods."
        ],
        [
            "And this this is a matrix that is the topic by Doc topic by word matrix specialized for the document 'cause it's in the document box.",
            "But we don't actually keep that.",
            "We only re computed as needed, so it's a virtual matrix.",
            "All we have is the regular matrices that are done in LDA.",
            "So we get to do this efficiently.",
            "We rebuild it as need."
        ],
        [
            "Anyway."
        ],
        [
            "That's the thing."
        ],
        [
            "Let me show you how this."
        ],
        [
            "Works.",
            "Perplexity higher is worse, lower is better.",
            "Let me see.",
            "This is the burst.",
            "The two bursty versions.",
            "Of classic LDA versus the nonparametric one hour full nonparametric one.",
            "This is the non bursty version of classic LDA and our nonparametric LDA.",
            "So what happens is the nonparametric one is always better than.",
            "The standard LDA, so adding those tricks to optimize or estimate all of those other parameters.",
            "Works.",
            "And adding in burstiness gives you a huge drop in perplexity.",
            "Basically, when you get repeated words, you're estimating and better.",
            "And you get a lot of repeated words in news articles and stuff.",
            "Here's the comprehensibility.",
            "So in this case.",
            "You can see the comprehensibility.",
            "For the nonparametric one is always better than the compensability for the regular one, and the burstiness makes things jump up.",
            "So the measure of compensability improves.",
            "Exactly the same way as it did here."
        ],
        [
            "That happens in a hole."
        ],
        [
            "Bunch of datasets.",
            "Here's a comparative.",
            "Evaluation of some of the current algorithms.",
            "These datasets I got from Sato who did?",
            "He's got a very nice.",
            "Variational method, which here is PC VB0 it's.",
            "Like version 4 of a whole bunch of there used to be there.",
            "I think there was PC zero and PCB and then this is the eventual one they settled on.",
            "Lower is better.",
            "This is perplexity our non parametric one is a green one.",
            "Meletis is the yellow one.",
            "The asymmetric Mallett and it's basically about as good as the variational one.",
            "Keep in mind this was written in 2007.",
            "This was written in 2010.",
            "I think in published in 2012 as the state of the art, this one already around, by the way, I wrote to David, known as did you realize that you would implement a truncated HDP?",
            "LDA and everyone else had been no one else had done it and he no.",
            "No, they never realized."
        ],
        [
            "This is against a split merge.",
            "This was a NIPS paper, I think about a year and a half ago.",
            "I guess it's just over a year ago.",
            "I've tried to line up their access with ours.",
            "Not only do we learn faster, we also converge to a better result.",
            "This is purely becausw of the statistical algorithm.",
            "Exactly the same model.",
            "Our statistical algorithm is doing a better job of estimating things.",
            "And it runs faster, so that's the technology we're using for the working with the Pittman.",
            "Yours.",
            "Here's the bursty 1.",
            "It's better again."
        ],
        [
            "Um?"
        ],
        [
            "So.",
            "We"
        ],
        [
            "Do normally I still haven't found another algorithm, so we're winning.",
            "Basically in this current race.",
            "He doesn't mean too much, but we're using the same technology for other interesting problems, so that's probably more.",
            "More relevant, we've got a multi core version, which is what Swapnil did, which I have to say is wonderful.",
            "You know like a 7 * 6 times speedup.",
            "It's so good to only use it.",
            "It wasn't that difficult to do in hindsight.",
            "Swapnil's honest thesis.",
            "Um?",
            "And it's not great.",
            "It's not slowed down a great deal.",
            "I remember I think it was David McCallum who's who's a language researcher at Pittsburgh, Massachusetts.",
            "Yeah, I ran into him about a year ago and he you know, what are you doing?",
            "Any well, I said I was doing nonparametric methods and he, oh I don't use them anymore.",
            "They're way too slow where they grind to a halt.",
            "They're no good.",
            "So a lot of people are taking that view, and they're not playing with these as much.",
            "We've now shown that that's no longer the case, so."
        ],
        [
            "There are some other algorithms of interest, but this is on GitHub.",
            "If anyone is interested, it's written in C. It has no dependencies, but I wouldn't.",
            "It is written under.",
            "Linix so it actually we've built it and compiled it on a Windows machine.",
            "I don't run Windows myself.",
            "The only problem with this, I realize is what I really should have done was a clean, simple iaccessible bit of code for students to edit.",
            "So 'cause I've had quite a few requests now for people who want to build on the code.",
            "And frankly this is full of all sorts of.",
            "Horrible little.",
            "Shortcuts and you know it.",
            "It runs very quickly.",
            "But it's hard to explain to someone the statistics is nontrivial as well, though we right now trying to do tutorials so.",
            "We're using these same techniques more in the general sense.",
            "I've been talking about, which is semantic networks and and trying to mimic some of the goals of deep neural networks.",
            "So that's some of the other."
        ],
        [
            "Things which we've been doing, and I'll just give you a brief rundown of that in the last few."
        ],
        [
            "It's.",
            "Sentiment.",
            "One of the problems is that sentiment in different areas tends to be different.",
            "You just kind of 1 sentiment dictionary.",
            "You really need to specialize it for different areas, so that's one thing you can do.",
            "Works well with these probability."
        ],
        [
            "Connect to hierarchies.",
            "This is our network.",
            "I won't try and explain it in a short period of time, but we we input a senti word net or one of these sentiment dictionaries.",
            "We operationalize it into a set of probability vectors, using with some.",
            "Parameters in here that we learn and then we specialize this probability vector for different facets.",
            "Cameras, mobiles.",
            "So you can see the vectors being the probability vectors being related."
        ],
        [
            "This is an example of what comes out.",
            "Um?",
            "So cold sausage is no good, apparently.",
            "Cold is not always are a.",
            "A bad word.",
            "It can be good sometimes and supervised.",
            "So the supervision is weak, it comes from."
        ],
        [
            "An occasional :).",
            "Yeah, yeah.",
            "Yeah yeah, this is off tweet this is Twitter so.",
            "And so the categories you define this through."
        ],
        [
            "If the word phone appears so, these have actually come out of these are topic.",
            "These are coming out as topics and their needs are coming out as.",
            "The specializations of the.",
            "This is a sentiment dictionary for that topic.",
            "Correct, but the topic was manually somehow.",
            "There are probably many more, so there's many more.",
            "There's hundreds and hundreds.",
            "Well, there's two.",
            "We actually tried it with two or three.",
            "Yeah.",
            "So.",
            "In the game, it should be considered positive.",
            "Yeah, you could be right there.",
            "Yeah yeah, yeah.",
            "True, yeah these aren't.",
            "I mean, I'm not sure if all of them are right or wrong, but Argentinian I couldn't workout and silly course, but.",
            "Hope we don't have any Argentinians here.",
            "Offended by."
        ],
        [
            "Um?",
            "So that's one sort of playing.",
            "This same technique we used for clustering Twitter, so we were doing very well at clustering Twitter on datasets.",
            "Using this sort of same technique but that never got published."
        ],
        [
            "This is the one I was telling you earlier, where you've got a block of text you wanna break it up into meaningful chunks.",
            "So what was the I wanted to ask before?",
            "So what's the let's application of this this?",
            "Usually it's say you want to identify the.",
            "The chunks who would use the need the chance.",
            "So 1 version I've heard is for retrieval.",
            "So you're just passage retrieval and you're going to return bits of it so, but it's usually done as a as a as a testing where people take text transcripts of speech.",
            "Yeah yeah, and each of these would be one person speaking.",
            "And then you're trying to break it up trying to get this works.",
            "Well, yeah it it.",
            "It fits pretty well.",
            "I'll show you well it is the best.",
            "Yeah currently it's the best and he did a parallel version of it as well, but."
        ],
        [
            "This is the model and there's the topic model inside.",
            "It's a pretty complex algo."
        ],
        [
            "We're not going to go into it, but here's the here.",
            "It is on a typical meeting transcript.",
            "Red is the ground truth where the where the known breaks are, and this is our probability of a break.",
            "So this is one of the earlier better algorithms.",
            "See it's having a lot more trouble, so let's see segmenting.",
            "Of a feature vector would be probably possibly.",
            "Segmenting video like extracting well, you know the trouble is images are entirely different.",
            "You know text there are these wonderful semantic tokens.",
            "But segmenting video there's lots of good information on there, and it's usually just a complete change of color or something.",
            "Rather, you know, so it.",
            "This is really only I always say topic models are only good if you got semantic tokens.",
            "You know things like words.",
            "In images, you can go to NMF where you just got a different.",
            "Technology really."
        ],
        [
            "This we are presented in Vietnam at a Asian conference on machine learning, but we put the authors into the so you got the author links from the citations as well as the authors.",
            "And you're building a network, so this.",
            "Thurston jochims this is the areas he works in, so these are topics it's coming up with and then you got the authors.",
            "One thing I noticed here is if I lowered the parameters enough, I could actually get myself to appear on the graph.",
            "But had to go way down there, but.",
            "You know, Tom Dietrich, Nir Friedman, Zuben Ghahramani, so you know we only put the bigger ones on here, but this is doing citation analysis."
        ],
        [
            "So I'd like to know this."
        ],
        [
            "I got duplicated up anyway that's it because Monash was still struggling with its web organization.",
            "I decided to make my own web page.",
            "And this is on WordPress, but you can find I've got a tutorial up here on the actual techniques if.",
            "Anyone is wants to hear more.",
            "So if you want to get be happy to give sort of go more into the math.",
            "I've always had the view that mathematics is something that adults do in the consenting adults do in the privacy of their office.",
            "So rather than a public lecture.",
            "Yeah.",
            "Good are OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a longer talk and I'll be cutting through bits of it, 'cause it's not all relevant.",
                    "label": 0
                },
                {
                    "sent": "So don't be confused if you see some funny slides and material flypast.",
                    "label": 0
                },
                {
                    "sent": "Also do interrupt so because we've got, there's all sorts of stuff here that you may or may not have seen before.",
                    "label": 0
                },
                {
                    "sent": "I'm not very familiar with your background, so mostly people are machine learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, automatics computer science, yes, but they are.",
                    "label": 0
                },
                {
                    "sent": "In general, I should say the number of students and ex students who are involved in this car wise just finishing landus been out a few years but he got this whole area going with me.",
                    "label": 0
                },
                {
                    "sent": "Some time ago and Swapnil.",
                    "label": 0
                },
                {
                    "sent": "He's done more recent work, but actually his big thing with us was getting the multicore version working so.",
                    "label": 0
                },
                {
                    "sent": "Which makes a huge difference.",
                    "label": 0
                },
                {
                    "sent": "I must say it's so good having stuff run multicore.",
                    "label": 0
                },
                {
                    "sent": "Makes a big.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference so I want to just talk briefly about motivation and our general what we're trying to achieve.",
                    "label": 0
                },
                {
                    "sent": "The real part of this talk is our topic model system, which has it's a fairly fancy theory and there's a lot of aspects to it, but what I'll be giving you is the basic ideas.",
                    "label": 0
                },
                {
                    "sent": "This is as far as I can tell from our experiments.",
                    "label": 0
                },
                {
                    "sent": "Is the best performing topic model system currently in.",
                    "label": 0
                },
                {
                    "sent": "In on a number of dimensions, it's not as fast as Mallett, but it is on all the other metrics.",
                    "label": 0
                },
                {
                    "sent": "It works the best, so the whole point of this talk is to wrap around this.",
                    "label": 0
                },
                {
                    "sent": "At the end I've got this some other bits of fun we've been having, and these I'm not going to go into detail, but I'll just describe briefly.",
                    "label": 0
                },
                {
                    "sent": "So motivate.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the usual stuff information overload.",
                    "label": 0
                },
                {
                    "sent": "There's way too much.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information warfare.",
                    "label": 0
                },
                {
                    "sent": "Everyone's out there trying to fight everyone else.",
                    "label": 0
                },
                {
                    "sent": "You know the Democrats are editing the Republican Wikipedia pages and vice versa so, but these are the things we face at the moment.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's way too much information now, in largely in language processing, but in other areas related we have the one of the core tools.",
                    "label": 0
                },
                {
                    "sent": "We have a probability vectors.",
                    "label": 0
                },
                {
                    "sent": "And this is what we've figured out how to work with really well.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give you an idea, you can use a probability vector to represent the parts of speech of a word.",
                    "label": 1
                },
                {
                    "sent": "Maybe you've got 10 part 60 parts of speech hashtags.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's 5000 hashtags you might want to use.",
                    "label": 0
                },
                {
                    "sent": "Well, you can have a probability vector over them so.",
                    "label": 1
                },
                {
                    "sent": "You can see many things we, in an intelligent system, will have probability vectors, or we can have probably look at probability vectors underneath what we'd like to do with them.",
                    "label": 0
                },
                {
                    "sent": "The things like inheritance.",
                    "label": 1
                },
                {
                    "sent": "Sharing of information.",
                    "label": 0
                },
                {
                    "sent": "Networks passing networks around and then general inference and learning with these probably.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vectors, just to give you an example.",
                    "label": 0
                },
                {
                    "sent": "What I mean by inheritance and or sharing.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter what is the content here, but maybe its words.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's subject matter of some kind.",
                    "label": 0
                },
                {
                    "sent": "What is in the book you expect to be related to what's in the chapter?",
                    "label": 0
                },
                {
                    "sent": "What's in the paragraphs?",
                    "label": 0
                },
                {
                    "sent": "You expect to be related to what's in the chapter.",
                    "label": 0
                },
                {
                    "sent": "If you know the content here and here, you should get a rough idea of the content here.",
                    "label": 0
                },
                {
                    "sent": "If you've got the content here, here and here, you should have a better idea of the content here.",
                    "label": 0
                },
                {
                    "sent": "So there's inheritance and sharing up and down the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Now the traditional way of doing inheritance and sharing or dealing with probability vectors in statistics is something called a richlite distribution.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be going into the details of that.",
                    "label": 0
                },
                {
                    "sent": "Some of you may recognize it.",
                    "label": 0
                },
                {
                    "sent": "It looks very much like a multinomial.",
                    "label": 0
                },
                {
                    "sent": "Form, but the main thing I want to point out is that if you put these into a hierarchy so we start with a.",
                    "label": 0
                },
                {
                    "sent": "With the parent we get a first child that first child is then used as the parent for the grandchild.",
                    "label": 0
                },
                {
                    "sent": "So it's this situation here.",
                    "label": 0
                },
                {
                    "sent": "When you have this, these are the and you've got to ritualize.",
                    "label": 0
                },
                {
                    "sent": "This is the functions you get.",
                    "label": 0
                },
                {
                    "sent": "This is a distribution for everybody and it's a mess.",
                    "label": 0
                },
                {
                    "sent": "It's not something we can deal with cleanly from a mathematical perspective, so dealing with Irish lies is something we can't do.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless AI and machine learning spend a large part of the 90s dressing everything up into rich lies, whether it be grammar, learning or decision tree learning or those net learning, it's one of the main tools we.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we've figured out is how to do this hierarchical learning quickly, well, moderately efficiently.",
                    "label": 0
                },
                {
                    "sent": "Now, this is a.",
                    "label": 0
                },
                {
                    "sent": "This is really something I do.",
                    "label": 0
                },
                {
                    "sent": "I do want to point out it's not going to be going into the details of this in this overview talk.",
                    "label": 0
                },
                {
                    "sent": "But the current standards if you attend a standard machine learning tutorial or if you read some of the more recent papers, you'll see what we're using is something all the Pitman Yor process display process.",
                    "label": 0
                },
                {
                    "sent": "You'll see a lot of terms like Chinese restaurant processes and stick breaking.",
                    "label": 1
                },
                {
                    "sent": "And what we've found is that these techniques I'm saying this is a punchline, and probably 90% of you have no idea what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "But those of you who do not I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "The punchline is don't use them.",
                    "label": 0
                },
                {
                    "sent": "So we've got better replacements or a lot more efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe some of you.",
                    "label": 0
                },
                {
                    "sent": "Got that, but want to be doing is showing the sort of things we're doing with these.",
                    "label": 0
                },
                {
                    "sent": "I won't be going into the algorithms that's that would be another whole tutorial.",
                    "label": 1
                },
                {
                    "sent": "But what we're using this for is modeling hierarchies, doing inheritance, bringing in semantic resources like sentiment dictionaries and Wordnet, bringing them into the probability calculations.",
                    "label": 1
                },
                {
                    "sent": "And one thing I realized as students had been giving me all these material on deep neural networks was that the about 2/3 of the opening introduction of many deep neural network tutorials for exactly the same as what we were doing.",
                    "label": 0
                },
                {
                    "sent": "So I realized we really addressing the same kind of techniques but with different web addressing the same problems but with different.",
                    "label": 0
                },
                {
                    "sent": "Techniques.",
                    "label": 0
                },
                {
                    "sent": "And now there's some people trying to sort of bring some of the deep neural network technology into the probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Some of the more nonparametric methods, which is what I do so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, I'm not going to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go into the richley distributions here, that's a.",
                    "label": 0
                },
                {
                    "sent": "Part of the longer talk.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Though this is the one slide which is probably important to so you can get an idea of things.",
                    "label": 0
                },
                {
                    "sent": "Here we have a probability vector and that's going to be the mean.",
                    "label": 0
                },
                {
                    "sent": "Of our distribution, and we're going to produce samples from it.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "Three probability vectors that have been sampled using this as a mean, this one.",
                    "label": 0
                },
                {
                    "sent": "Has what's called a high concentration.",
                    "label": 0
                },
                {
                    "sent": "Concentration is the inverse variance in statistics.",
                    "label": 0
                },
                {
                    "sent": "With so if it's a high concentration, it's a low variance, so this sample is virtually the same as the mean.",
                    "label": 0
                },
                {
                    "sent": "This one with the low concentration means it's got a high variance.",
                    "label": 0
                },
                {
                    "sent": "And this sample is very different to the main, so we can have our.",
                    "label": 0
                },
                {
                    "sent": "We can have Dirichlet distributions and we can make them have very low variance or very high variance by changing the key parameters.",
                    "label": 0
                },
                {
                    "sent": "When you say sample samples, we mean so this is really just.",
                    "label": 0
                },
                {
                    "sent": "This is kind of an empirical distribution sampled from some underlying this yes, so this is the.",
                    "label": 0
                },
                {
                    "sent": "Sampling is a distribution of probability vector, so it's not like sampling a real number X will sampling an entire vector.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what makes them harder to work with because you're dealing with vectors.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'll skip through.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This other stuff and now I'll do what I call old school probabilistic reasoning, which might be old school for you.",
                    "label": 0
                },
                {
                    "sent": "This is going way back to the 90s, which some of you I know.",
                    "label": 0
                },
                {
                    "sent": "Some of my students were in primary school back then, so or earlier.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is this is a topic model, so a latent original allocation and.",
                    "label": 0
                },
                {
                    "sent": "Some of you may have seen that, and my whole point of this is, well, here's a here's a structure which represents the model.",
                    "label": 0
                },
                {
                    "sent": "What it's actually is.",
                    "label": 0
                },
                {
                    "sent": "This is a matrix of word data, and this matrix has been approximated by a matrix of.",
                    "label": 0
                },
                {
                    "sent": "Of word probabilities in a matrix of topic probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now when we model it with our darish lies and we do some analysis, this is.",
                    "label": 0
                },
                {
                    "sent": "The formula we end up that we want to optimize and this is just two Dirichlet's back to back.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What happens is whenever you're doing learning Bayes networks, learning decision trees probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Context free grammars.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models when if you're doing any of these things, what you get is a holiday formula that look exactly like this strung together and what you're fiddling with is the numbers here.",
                    "label": 0
                },
                {
                    "sent": "You're fiddling with the some indicator variables inside these counts.",
                    "label": 0
                },
                {
                    "sent": "Here you're sampling them, but so this is the sort of formula that that many of us spent a whole decade playing with in the 90s.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said, trees, graphs, networks, context free grammars.",
                    "label": 0
                },
                {
                    "sent": "It's all done with this sort of formula.",
                    "label": 0
                },
                {
                    "sent": "And once you've got the formula down, then you do search do something called model averaging.",
                    "label": 0
                },
                {
                    "sent": "There's a number of things you can do.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, but the key I've got one key idea about this.",
                    "label": 0
                },
                {
                    "sent": "I want to get across.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that is that the simple Dirichlet distribution doesn't really have the.",
                    "label": 0
                },
                {
                    "sent": "Capability that we want to do this deal with this sort of reasoning.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, to illustrate that, here's a context free grammar so.",
                    "label": 1
                },
                {
                    "sent": "The idea is you you apply these rules, but when you apply the rule you apply it.",
                    "label": 0
                },
                {
                    "sent": "You ignore the context around.",
                    "label": 0
                },
                {
                    "sent": "So when we say we've got this so far, there's a verb phrase here.",
                    "label": 0
                },
                {
                    "sent": "When we go to expand the verb phrase, we ignore.",
                    "label": 0
                },
                {
                    "sent": "That context it's irrelevant to us.",
                    "label": 0
                },
                {
                    "sent": "It's irrelevant to the theory of context free grammars.",
                    "label": 1
                },
                {
                    "sent": "We ignore that and then we apply our rule to get to the next level and again.",
                    "label": 0
                },
                {
                    "sent": "So that's the context.",
                    "label": 0
                },
                {
                    "sent": "Free grammar.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is that, well, they do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work Google bought YouTube.",
                    "label": 0
                },
                {
                    "sent": "What can go in there?",
                    "label": 0
                },
                {
                    "sent": "Probably not an ice cream, or, you know, a pencil.",
                    "label": 0
                },
                {
                    "sent": "Google bought something.",
                    "label": 0
                },
                {
                    "sent": "We have an idea.",
                    "label": 0
                },
                {
                    "sent": "It's going to be something that a big Corporation buys.",
                    "label": 0
                },
                {
                    "sent": "So context is everything.",
                    "label": 0
                },
                {
                    "sent": "It's really important and this applies all over inference.",
                    "label": 0
                },
                {
                    "sent": "This is why the so whereas.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These models have a beautiful statistical theory with Dirichlet distributions.",
                    "label": 0
                },
                {
                    "sent": "It's no good.",
                    "label": 0
                },
                {
                    "sent": "You can't really use it.",
                    "label": 0
                },
                {
                    "sent": "So a lot of work in LA.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eating grammars is how to make a context sensitive that all started in about 1990 will probably earlier.",
                    "label": 0
                },
                {
                    "sent": "I'm not a language expert by the way.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning based networks is similar.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'll show you an example.",
                    "label": 0
                },
                {
                    "sent": "Now of this is called an engram, so an engram is they used in language model, so it's used in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "1 gram is where you look at the.",
                    "label": 0
                },
                {
                    "sent": "The word by itself for 2 gram, you look at the previous word.",
                    "label": 0
                },
                {
                    "sent": "The three gram you look at the two previous words in making your prediction of the next word.",
                    "label": 0
                },
                {
                    "sent": "So an ngram you look at the previous N -- 1 words.",
                    "label": 0
                },
                {
                    "sent": "Now when you build this up with Darish lies, it works OK.",
                    "label": 0
                },
                {
                    "sent": "It's not too bad.",
                    "label": 0
                },
                {
                    "sent": "But but it ignores context again, which which can be important.",
                    "label": 0
                },
                {
                    "sent": "And this is the context for the 2 gram context for the three gram, you can see it's important we'll get to this in a bit.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in 1995 one of the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Algorithms and this was actually for.",
                    "label": 0
                },
                {
                    "sent": "Language compression of N grams with something called context tree weighting.",
                    "label": 0
                },
                {
                    "sent": "So in this uses a technique called Bayesian model averaging.",
                    "label": 0
                },
                {
                    "sent": "This is taking Darish lies to a ridiculous extreme.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But, and this is where my recent work comes in, in 2006, Uyt developed something called the Hierarchical Pitman.",
                    "label": 0
                },
                {
                    "sent": "Your model, and he then published a paper with some reason.",
                    "label": 0
                },
                {
                    "sent": "I don't have it here.",
                    "label": 0
                },
                {
                    "sent": "I'll do that afterwards.",
                    "label": 0
                },
                {
                    "sent": "They figured out how to use that in something called the sequence memorizer and this.",
                    "label": 0
                },
                {
                    "sent": "Beats CTW now.",
                    "label": 0
                },
                {
                    "sent": "CTW is was a very important compression algorithm because it had a nice theory and it worked very well.",
                    "label": 0
                },
                {
                    "sent": "If you work in, say, language, images, anything you know if something has a really good theory and it works well.",
                    "label": 0
                },
                {
                    "sent": "That's really hard to do.",
                    "label": 0
                },
                {
                    "sent": "But the sequence MEMOIZER, which is based on this theory from Morocco Pittman, yours of Tay, that actually beats CTW, so this is a whole new era was starting a whole new technological capability came in.",
                    "label": 0
                },
                {
                    "sent": "And the the reason is that it now let's you model context in the.",
                    "label": 0
                },
                {
                    "sent": "In the statistical world.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's probably the one message.",
                    "label": 0
                },
                {
                    "sent": "I would like to take away, I'll give you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basic idea of this modeling of context.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And then we'll go onto how we're using this in our topic models.",
                    "label": 0
                },
                {
                    "sent": "So here is our.",
                    "label": 0
                },
                {
                    "sent": "Part of part of US language again and.",
                    "label": 0
                },
                {
                    "sent": "What goes here?",
                    "label": 0
                },
                {
                    "sent": "If you've just seen an A?",
                    "label": 0
                },
                {
                    "sent": "You know something?",
                    "label": 0
                },
                {
                    "sent": "Well, it maybe it's singular, or maybe it's an adjective, but it's probably not a plural noun, so you know there's some restriction.",
                    "label": 0
                },
                {
                    "sent": "When you've actually got a context of three words, now caught a well that narrows it down a bit more.",
                    "label": 0
                },
                {
                    "sent": "But the key idea here, other than the context is important is it's hierarchical.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Having what's the context when you know he caught a well, it should be related to this context.",
                    "label": 0
                },
                {
                    "sent": "Here it should be similar.",
                    "label": 0
                },
                {
                    "sent": "So whatever probability vector goes here over the possible words here, it's probably similar to this.",
                    "label": 1
                },
                {
                    "sent": "In fact, as you get to say, a 10 gram.",
                    "label": 0
                },
                {
                    "sent": "It's going to be very similar.",
                    "label": 0
                },
                {
                    "sent": "You expect it to be a low, high low variance.",
                    "label": 0
                },
                {
                    "sent": "Relationship with this probably has a higher variance relationship.",
                    "label": 0
                },
                {
                    "sent": "This probability vector over the words there should be somewhat similar to that the general vocabulary of probability vectors, and here it will be probably a bit more restricted, so it will be.",
                    "label": 0
                },
                {
                    "sent": "Lower variance, but you expect that this distribution.",
                    "label": 0
                },
                {
                    "sent": "For the word here has this distribution as a mean.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea of context and the mean of probability vectors.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we put that into an abstract sense, what we end up?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With is.",
                    "label": 0
                },
                {
                    "sent": "The probability vectors are related and the probability vector with three bits of context has the one with the first 2 bits of context as its mean, which has the one with one as its mean, which has the general probability vectors.",
                    "label": 0
                },
                {
                    "sent": "So this is the idea of context, so this is what you YT had introduce.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together with Michael Jordan and Bielen Bly, they they developed a variation of LDA called.",
                    "label": 0
                },
                {
                    "sent": "HDP, LDA.",
                    "label": 0
                },
                {
                    "sent": "It's absolutely been called, and they developed hierarchical Dirichlet processes, and this this appeared in a statistics Journal disappeared in a ACL conference.",
                    "label": 0
                },
                {
                    "sent": "Then Nips took over.",
                    "label": 0
                },
                {
                    "sent": "This was the the darling of nips for couple of years.",
                    "label": 0
                },
                {
                    "sent": "If you you just have to put, you know, Chinese restaurants somewhere in your paper and you up the chances of acceptance dramatically.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we we were playing with something else probably happened because I wasn't doing great deal of research at the time, so I was going through.",
                    "label": 0
                },
                {
                    "sent": "I was dealing with other things and but we we came at things from another angle.",
                    "label": 0
                },
                {
                    "sent": "We actually showed that these techniques aren't as good as you can get.",
                    "label": 0
                },
                {
                    "sent": "You can really improve on them.",
                    "label": 0
                },
                {
                    "sent": "So since then, we've been applying our technology to a whole bunch of problems every time we apply it.",
                    "label": 0
                },
                {
                    "sent": "We substantially beat the Chinese restaurant process equivalent, so this is, if you like this talk is part of a sales pitch to try and sell you the techniques the techniques are written up in.",
                    "label": 0
                },
                {
                    "sent": "Some of our tutorials, but I won't be going into them in this in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, the wording they had.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loving the ones you listed here.",
                    "label": 0
                },
                {
                    "sent": "Franchise multifloor, oh it gets it gets really complex, yeah?",
                    "label": 0
                },
                {
                    "sent": "So we have a we what we do is we eliminate some of this stuff and we basically really all we're doing is marginalizing in fact, really all we're doing is, if you were cynical, you'd say we'd taken one of the algorithms out of TE and figured out we could apply caching to speed it up.",
                    "label": 0
                },
                {
                    "sent": "So that's probably one of my big contributions to this field was to use caching, so.",
                    "label": 0
                },
                {
                    "sent": "I'm actually there's a bit more than that, but I was surprised, so we'll just flip through.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through this formula probably explains everything, but.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll get back to the situation.",
                    "label": 0
                },
                {
                    "sent": "We've got our parent probability vector.",
                    "label": 0
                },
                {
                    "sent": "The probability vector that we may we're dealing with and our data.",
                    "label": 0
                },
                {
                    "sent": "What people do is you want to integrate this out so you can do reasoning and eliminate a whole vector of parameters.",
                    "label": 0
                },
                {
                    "sent": "If we've got 10,000 words in our vocabulary, we love to eliminate 10,000 real valued parameters from our inference, it makes things a lot more reliable.",
                    "label": 0
                },
                {
                    "sent": "Statistics always works better when you've got 10,000 less real values to estimate, so you want to get rid of this if you can.",
                    "label": 0
                },
                {
                    "sent": "And when you do this in the classic Dirichlet multinomial situation.",
                    "label": 0
                },
                {
                    "sent": "And I'm skipping a bit here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get this formula.",
                    "label": 0
                },
                {
                    "sent": "Now this is a funny notation, but what this is, it's a polynomial.",
                    "label": 0
                },
                {
                    "sent": "So what you get is the probability of your data.",
                    "label": 0
                },
                {
                    "sent": "Given the grandparent, probably, or the parent probability, and having an integrated out P, the probability vector.",
                    "label": 0
                },
                {
                    "sent": "Is this your polynomial?",
                    "label": 0
                },
                {
                    "sent": "So you're passing a message up to the parent with a polynomial in it, and that makes things difficult to deal with.",
                    "label": 0
                },
                {
                    "sent": "What we have is we we introduce a new set of latent variables which mirrors the counts and we pass up a multinomial.",
                    "label": 0
                },
                {
                    "sent": "So that's the key thing.",
                    "label": 0
                },
                {
                    "sent": "We're passing up a multinomial as data, and that can then be processed.",
                    "label": 0
                },
                {
                    "sent": "That's about as mathematical as I'll be getting.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Skip this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll now go on to our topic models.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a topic model you're given some.",
                    "label": 0
                },
                {
                    "sent": "Bonnie Pelton yeah.",
                    "label": 0
                },
                {
                    "sent": "So you're given these are called components in the word.",
                    "label": 0
                },
                {
                    "sent": "Case of words, components would be words with a similar semantic meaning, and you're going to do a linear combination of these to get whatever it is you're trying to estimate in this case.",
                    "label": 0
                },
                {
                    "sent": "To get a bag of words.",
                    "label": 0
                },
                {
                    "sent": "So you take a linear combination of them.",
                    "label": 0
                },
                {
                    "sent": "That gives you a probability vector for generating words.",
                    "label": 0
                },
                {
                    "sent": "You generate a bag of words.",
                    "label": 0
                },
                {
                    "sent": "So that's what a.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Opponent models are there's a whole bunch, maybe familiar with ICA.",
                    "label": 0
                },
                {
                    "sent": "For speech recognition, PCA, principal components analysis.",
                    "label": 0
                },
                {
                    "sent": "Learning Code books is used in image recognition, non negative matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Another variation of it topic modeling and they all changing the data may be non negative.",
                    "label": 0
                },
                {
                    "sent": "You may use least squares or cross entropy.",
                    "label": 0
                },
                {
                    "sent": "So depending on how you set it up, topic modeling is usually done with words which means you got non negative integers 'cause their counts.",
                    "label": 0
                },
                {
                    "sent": "So the data is counts and what you like is probability vectors as your components.",
                    "label": 0
                },
                {
                    "sent": "So these accounts, most of them are zero.",
                    "label": 0
                },
                {
                    "sent": "And you'll have probability vectors in here and here.",
                    "label": 0
                },
                {
                    "sent": "So that's the.",
                    "label": 0
                },
                {
                    "sent": "The set up.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Oh why should you care about this abstract problem?",
                    "label": 0
                },
                {
                    "sent": "In fact, why should I care about this abstract problem?",
                    "label": 0
                },
                {
                    "sent": "I'm doing this for two reasons.",
                    "label": 0
                },
                {
                    "sent": "One reason is we've had this code sitting around for awhile and we never really published it properly.",
                    "label": 0
                },
                {
                    "sent": "We knew we had good methods, but we never really shown how good they were, so we thought.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe we have to go the next step.",
                    "label": 0
                },
                {
                    "sent": "But the other reason is that you actually use topic models in.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In many interesting ways now.",
                    "label": 0
                },
                {
                    "sent": "For one thing that they can come up with really good semantic associations.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of ways of doing this, but for example, this comes from the New York Times News from a 20 year.",
                    "label": 0
                },
                {
                    "sent": "One of the components you can build about 2000 topics out of this.",
                    "label": 0
                },
                {
                    "sent": "So it's like PCA with a 2000 vectors.",
                    "label": 0
                },
                {
                    "sent": "Here is one of those topics.",
                    "label": 0
                },
                {
                    "sent": "Is the anthrax event, but it picks it out perfectly, so the ability to get clear, coherent word lists is quite remarkable.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But This is why it's important.",
                    "label": 0
                },
                {
                    "sent": "Here's a topic model here.",
                    "label": 0
                },
                {
                    "sent": "The rest of this this is currently the best document segmentation model in practice, so if you want to break a document up into meaningful chunks, the best way to do it is with an algorithm developed by Landu.",
                    "label": 0
                },
                {
                    "sent": "And Mark Johnson and myself.",
                    "label": 0
                },
                {
                    "sent": "It's substantially better than quite a few others.",
                    "label": 0
                },
                {
                    "sent": "And it has a topic model inside.",
                    "label": 0
                },
                {
                    "sent": "Now, that doesn't mean it's a subroutine, because it's the model.",
                    "label": 0
                },
                {
                    "sent": "What it means is that the model is inside.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is very different.",
                    "label": 0
                },
                {
                    "sent": "But we're using basically the technology that we've developed for topic models to do the document segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's good reasons to do.",
                    "label": 0
                },
                {
                    "sent": "This and people are using this was at KDD this year.",
                    "label": 0
                },
                {
                    "sent": "This was a very good piece of work.",
                    "label": 0
                },
                {
                    "sent": "Showing how to connect ratings and sentiment done from sort of a CMU alliance with Singapore Management University and.",
                    "label": 0
                },
                {
                    "sent": "This is an extension of the topic model in a way.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We had David Lewis visiting recently.",
                    "label": 0
                },
                {
                    "sent": "He's an IR guy who did machine learning.",
                    "label": 0
                },
                {
                    "sent": "He's a very practical guy.",
                    "label": 0
                },
                {
                    "sent": "He helped start the IR for legal discovery.",
                    "label": 0
                },
                {
                    "sent": "Track if that's a huge business now in the US especially.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So we we encounter David work all the time.",
                    "label": 0
                },
                {
                    "sent": "He did two Reuters datasets and but he was visiting and he was saying he was raving about topic models are it's a terrible field, you know they.",
                    "label": 0
                },
                {
                    "sent": "It's like a Rorschach inkblot test.",
                    "label": 0
                },
                {
                    "sent": "If you stare at a bunch of words long enough, you're eventually going to see something.",
                    "label": 0
                },
                {
                    "sent": "Well, you know I was laughing at this stage because I was doing topic models and and he was sitting there ripping in.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was laughing 'cause I agreed with him because there was an awful lot of you know we're coming out of that phase basically.",
                    "label": 0
                },
                {
                    "sent": "But there was an awful lot of earlier work where it was, well, look at this cool stuff.",
                    "label": 0
                },
                {
                    "sent": "To a degree, machine learning has done that in various stages.",
                    "label": 0
                },
                {
                    "sent": "When a field gets going.",
                    "label": 0
                },
                {
                    "sent": "But the two measures we use currently, this one probably isn't that meaningful, and I know some people who refuse to use it.",
                    "label": 0
                },
                {
                    "sent": "It's perplexity, and it's basically a measure of surprise, and it's also used in speech recognition and some other tasks.",
                    "label": 0
                },
                {
                    "sent": "The other measure is PMO, which is pointwise mutual information, which basically is a measure of how comprehensible the topics are.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This has a very good PMI score.",
                    "label": 0
                },
                {
                    "sent": "'cause each of these words is highly related.",
                    "label": 0
                },
                {
                    "sent": "Though you gotta be careful because you know Dash Lee.",
                    "label": 0
                },
                {
                    "sent": "Now, if you know the news, you know this is highly related to anthrax spores.",
                    "label": 0
                },
                {
                    "sent": "But in a general semantic sense, it isn't.",
                    "label": 0
                },
                {
                    "sent": "PMI is is is a tough.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measure to use.",
                    "label": 0
                },
                {
                    "sent": "It measures coherence in some areas.",
                    "label": 0
                },
                {
                    "sent": "In medical data it doesn't work very well because there's so many strange specifics that you might have in that field that that you can't go and collect coherence.",
                    "label": 0
                },
                {
                    "sent": "From but nevertheless, these are the two sort of for good or for bad.",
                    "label": 0
                },
                {
                    "sent": "These are current.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scores used",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we looked at a bunch of work.",
                    "label": 0
                },
                {
                    "sent": "Some of that we're extending some of it away comparing against, so we compared against a number of state of the art algorithms.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Extending topic models has become a bit of a.",
                    "label": 0
                },
                {
                    "sent": "It's a fashionable thing to do if you like at NIPS or I smell, you know to have a new fancy nonparametric model that is better than others.",
                    "label": 0
                },
                {
                    "sent": "And and there's a lot of papers along this line.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, so so we'll look at where we went with this, the.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting one, which we'll get to.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a key technology that that we didn't introduce.",
                    "label": 0
                },
                {
                    "sent": "In fact, we haven't done a whole lot.",
                    "label": 0
                },
                {
                    "sent": "That's that's knew.",
                    "label": 0
                },
                {
                    "sent": "I could say if I was cynical, we just put it together.",
                    "label": 0
                },
                {
                    "sent": "Well, so.",
                    "label": 0
                },
                {
                    "sent": "This idea comes from Charles Elkan, who's who's left.",
                    "label": 0
                },
                {
                    "sent": "San Diego told and is now Amazon.",
                    "label": 0
                },
                {
                    "sent": "Charles Elkan and a student.",
                    "label": 0
                },
                {
                    "sent": "So if you take a bag of words so this is a fragment of a news article, you bag it up.",
                    "label": 0
                },
                {
                    "sent": "Now you expect the stop words to occur multiple times.",
                    "label": 0
                },
                {
                    "sent": "Cabinet occurs twice.",
                    "label": 0
                },
                {
                    "sent": "If you have about 20 words, what's the chance that a moderately rare word would occur twice?",
                    "label": 0
                },
                {
                    "sent": "This isn't the secretary problem.",
                    "label": 0
                },
                {
                    "sent": "Not the birthdays problem.",
                    "label": 0
                },
                {
                    "sent": "It's not the birthdays problem.",
                    "label": 0
                },
                {
                    "sent": "It's it's pretty low, it's just not going to happen, so the fact and this always happens with say, news articles or something.",
                    "label": 0
                },
                {
                    "sent": "The thing is, there topical news articles are not random words.",
                    "label": 0
                },
                {
                    "sent": "Someone's got a set of related topics and they're writing about it, and words are going to repeat.",
                    "label": 0
                },
                {
                    "sent": "So this is called burstiness.",
                    "label": 0
                },
                {
                    "sent": "The words burst, and in fact there's a statistical.",
                    "label": 0
                },
                {
                    "sent": "There's a whole theory of statistics developed to deal with this.",
                    "label": 0
                },
                {
                    "sent": "That as it happens, is what do rich like processes and Pitman yor processes before.",
                    "label": 0
                },
                {
                    "sent": "Sir.",
                    "label": 0
                },
                {
                    "sent": "These guys developed a topic public model that dealt with burstiness, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "It was pretty slow, so they couldn't really scale it up even though it worked very well.",
                    "label": 0
                },
                {
                    "sent": "People kind of didn't.",
                    "label": 0
                },
                {
                    "sent": "It didn't go.",
                    "label": 0
                },
                {
                    "sent": "It wasn't taken up, you might say, and I think it was for computational reasons.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's more interesting about this is that there's been some work in the IR community, and this is a survey article.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Where the ideas of burstiness is very closely related to the theory of IR.",
                    "label": 0
                },
                {
                    "sent": "So they have an ocean called Eliteness.",
                    "label": 0
                },
                {
                    "sent": "Birth burstiness is somehow fundamentally related to TF IDF.",
                    "label": 0
                },
                {
                    "sent": "It's more like conceptual term, right?",
                    "label": 0
                },
                {
                    "sent": "More than the like BM 25 is the actual formula yes yeah.",
                    "label": 0
                },
                {
                    "sent": "So BAM 25 originally was motivated by two plus on model which you can play around with it and show that the you can get a related model using the Pitman you're so.",
                    "label": 0
                },
                {
                    "sent": "So there is some relationship between the notion of burstiness and modeling it with with the nonparametric methods and the TF IDF formulas, which we're still trying to get to the bottom of this.",
                    "label": 0
                },
                {
                    "sent": "But this is just an interesting.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Connection, but nevertheless burstiness is something that we've added to our model because we know how to do these things efficiently.",
                    "label": 0
                },
                {
                    "sent": "We've made it run very fast.",
                    "label": 0
                },
                {
                    "sent": "So let's look.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The evolution of topic models and what we've done.",
                    "label": 1
                },
                {
                    "sent": "What sort of is makes our new system work?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "This is probably the punchline of the talk.",
                    "label": 0
                },
                {
                    "sent": "So this is classic LDA, which I showed earlier you might have.",
                    "label": 0
                },
                {
                    "sent": "You have a matrix here which would be millions.",
                    "label": 0
                },
                {
                    "sent": "Probably billions.",
                    "label": 0
                },
                {
                    "sent": "This matrix who counts.",
                    "label": 0
                },
                {
                    "sent": "And it's words by by documents here you've got a matrix of topics by words.",
                    "label": 0
                },
                {
                    "sent": "And this is millions the size of it, and this one is.",
                    "label": 0
                },
                {
                    "sent": "This matrix is topics by documents and you're multiplying these two matrices to estimate this.",
                    "label": 0
                },
                {
                    "sent": "Now, if you if you've got a million.",
                    "label": 0
                },
                {
                    "sent": "Parameters in this matrix.",
                    "label": 0
                },
                {
                    "sent": "There's only one hyperparameter here.",
                    "label": 0
                },
                {
                    "sent": "So as a sort of standard Bayesian reasoning, in fact, standard convex, any sort of standard optimization million parameters, one parameter is all you have to fiddle hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "Not good enough, you should have 1000 hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "'cause if you got so much data there so many numbers you should be able to have more numbers in your.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we do is we make these vectors.",
                    "label": 0
                },
                {
                    "sent": "And that is in fact what the very influential paper in 2009 by Minoan Wall akin.",
                    "label": 0
                },
                {
                    "sent": "Someone else did.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They and the one that worked for them was something called asymmetric.",
                    "label": 0
                },
                {
                    "sent": "They used an asymmetric dress like prior, and it's a symmetric symmetric LDA.",
                    "label": 0
                },
                {
                    "sent": "It was implemented in 2007, available in the code 2008, and as it happens it's the 1st and in many ways still the best implementation of nonparametric LDA.",
                    "label": 0
                },
                {
                    "sent": "And most of these other papers here keep in mind.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's all the.",
                    "label": 0
                },
                {
                    "sent": "This 2009 is beats most of these or its own parliament, but no one realized it was tackling the same problem.",
                    "label": 0
                },
                {
                    "sent": "We sort of had a better look at it and realize it's just the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's a truncated.",
                    "label": 0
                },
                {
                    "sent": "Method for doing what these guys are doing.",
                    "label": 0
                },
                {
                    "sent": "So that's the next level.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a related thing.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is we're putting a darish light process, and these are parameters for the rousselet process on these two.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we go and.",
                    "label": 0
                },
                {
                    "sent": "Learn this vector as well, so we're going to make this vector accessible to the machine learning machinery.",
                    "label": 0
                },
                {
                    "sent": "And it's got parameters for the for this vector here, and we put a Pitman your distribution on these.",
                    "label": 0
                },
                {
                    "sent": "So here we've got a hierarchy of probability vectors, a small one.",
                    "label": 0
                },
                {
                    "sent": "Here and here.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next level is.",
                    "label": 0
                },
                {
                    "sent": "We add this burstiness capability and.",
                    "label": 0
                },
                {
                    "sent": "This is exactly why I believe it's exactly the same idea as the original paper from Doyle and Alcam.",
                    "label": 0
                },
                {
                    "sent": "I'll try and explain to you what you're doing here, so you've got a bunch of topic vectors.",
                    "label": 0
                },
                {
                    "sent": "What you're going to do is specialize every one of those topic vectors for the document.",
                    "label": 0
                },
                {
                    "sent": "So imagine you got a topic that's all about things in Russia.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a whole.",
                    "label": 0
                },
                {
                    "sent": "It's a maybe newspaper collection so you won't have a bunch of Russian topics.",
                    "label": 0
                },
                {
                    "sent": "You're just going to have one.",
                    "label": 0
                },
                {
                    "sent": "But maybe you'll have an article about the Crimea or an article about Moscow or my article about Siberia.",
                    "label": 0
                },
                {
                    "sent": "This document is going to specialize the Russian topic into a. Siberia topic or a Moscow topic?",
                    "label": 0
                },
                {
                    "sent": "So that's what this capability for specializing is doing.",
                    "label": 0
                },
                {
                    "sent": "And if you don't have enough need for your own Siberian topic or your own Moscow topic.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Then you do this.",
                    "label": 0
                },
                {
                    "sent": "You'd have this ability to specialize a topic just for the document.",
                    "label": 0
                },
                {
                    "sent": "Now, from a statistical perspective, this is crazy because you've got, say, 100 words in here, 200 words.",
                    "label": 0
                },
                {
                    "sent": "And with those 200 words, you're going to estimate a matrix whose size is about a million parameters.",
                    "label": 0
                },
                {
                    "sent": "So 200 words.",
                    "label": 0
                },
                {
                    "sent": "You estimate a million real valued parameters impossible, and This is why for about a month after I'd read their work, I thought they were crazy and in one morning I woke up their geniuses.",
                    "label": 0
                },
                {
                    "sent": "This is brilliant.",
                    "label": 0
                },
                {
                    "sent": "I realized what they were doing, figured out we could implement it very quickly using our our tricks, the way we implemented, it's about 50% slower than classic LDA, both in memory an in time.",
                    "label": 0
                },
                {
                    "sent": "So that's the next level.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you the.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bits of this these are the probability vector hierarchies.",
                    "label": 0
                },
                {
                    "sent": "That we're processing with our.",
                    "label": 0
                },
                {
                    "sent": "Out techniques.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "That we're learning with generic.",
                    "label": 0
                },
                {
                    "sent": "Methods you could maybe slice sampling or one of these sorts of generic sampling methods.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this this is a matrix that is the topic by Doc topic by word matrix specialized for the document 'cause it's in the document box.",
                    "label": 0
                },
                {
                    "sent": "But we don't actually keep that.",
                    "label": 0
                },
                {
                    "sent": "We only re computed as needed, so it's a virtual matrix.",
                    "label": 0
                },
                {
                    "sent": "All we have is the regular matrices that are done in LDA.",
                    "label": 0
                },
                {
                    "sent": "So we get to do this efficiently.",
                    "label": 0
                },
                {
                    "sent": "We rebuild it as need.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the thing.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you how this.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works.",
                    "label": 0
                },
                {
                    "sent": "Perplexity higher is worse, lower is better.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "This is the burst.",
                    "label": 0
                },
                {
                    "sent": "The two bursty versions.",
                    "label": 0
                },
                {
                    "sent": "Of classic LDA versus the nonparametric one hour full nonparametric one.",
                    "label": 0
                },
                {
                    "sent": "This is the non bursty version of classic LDA and our nonparametric LDA.",
                    "label": 0
                },
                {
                    "sent": "So what happens is the nonparametric one is always better than.",
                    "label": 0
                },
                {
                    "sent": "The standard LDA, so adding those tricks to optimize or estimate all of those other parameters.",
                    "label": 0
                },
                {
                    "sent": "Works.",
                    "label": 0
                },
                {
                    "sent": "And adding in burstiness gives you a huge drop in perplexity.",
                    "label": 0
                },
                {
                    "sent": "Basically, when you get repeated words, you're estimating and better.",
                    "label": 0
                },
                {
                    "sent": "And you get a lot of repeated words in news articles and stuff.",
                    "label": 0
                },
                {
                    "sent": "Here's the comprehensibility.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "You can see the comprehensibility.",
                    "label": 0
                },
                {
                    "sent": "For the nonparametric one is always better than the compensability for the regular one, and the burstiness makes things jump up.",
                    "label": 0
                },
                {
                    "sent": "So the measure of compensability improves.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same way as it did here.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That happens in a hole.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bunch of datasets.",
                    "label": 0
                },
                {
                    "sent": "Here's a comparative.",
                    "label": 0
                },
                {
                    "sent": "Evaluation of some of the current algorithms.",
                    "label": 0
                },
                {
                    "sent": "These datasets I got from Sato who did?",
                    "label": 0
                },
                {
                    "sent": "He's got a very nice.",
                    "label": 0
                },
                {
                    "sent": "Variational method, which here is PC VB0 it's.",
                    "label": 0
                },
                {
                    "sent": "Like version 4 of a whole bunch of there used to be there.",
                    "label": 0
                },
                {
                    "sent": "I think there was PC zero and PCB and then this is the eventual one they settled on.",
                    "label": 0
                },
                {
                    "sent": "Lower is better.",
                    "label": 0
                },
                {
                    "sent": "This is perplexity our non parametric one is a green one.",
                    "label": 0
                },
                {
                    "sent": "Meletis is the yellow one.",
                    "label": 0
                },
                {
                    "sent": "The asymmetric Mallett and it's basically about as good as the variational one.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind this was written in 2007.",
                    "label": 0
                },
                {
                    "sent": "This was written in 2010.",
                    "label": 0
                },
                {
                    "sent": "I think in published in 2012 as the state of the art, this one already around, by the way, I wrote to David, known as did you realize that you would implement a truncated HDP?",
                    "label": 0
                },
                {
                    "sent": "LDA and everyone else had been no one else had done it and he no.",
                    "label": 0
                },
                {
                    "sent": "No, they never realized.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is against a split merge.",
                    "label": 0
                },
                {
                    "sent": "This was a NIPS paper, I think about a year and a half ago.",
                    "label": 0
                },
                {
                    "sent": "I guess it's just over a year ago.",
                    "label": 0
                },
                {
                    "sent": "I've tried to line up their access with ours.",
                    "label": 0
                },
                {
                    "sent": "Not only do we learn faster, we also converge to a better result.",
                    "label": 0
                },
                {
                    "sent": "This is purely becausw of the statistical algorithm.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same model.",
                    "label": 0
                },
                {
                    "sent": "Our statistical algorithm is doing a better job of estimating things.",
                    "label": 0
                },
                {
                    "sent": "And it runs faster, so that's the technology we're using for the working with the Pittman.",
                    "label": 0
                },
                {
                    "sent": "Yours.",
                    "label": 0
                },
                {
                    "sent": "Here's the bursty 1.",
                    "label": 0
                },
                {
                    "sent": "It's better again.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do normally I still haven't found another algorithm, so we're winning.",
                    "label": 0
                },
                {
                    "sent": "Basically in this current race.",
                    "label": 0
                },
                {
                    "sent": "He doesn't mean too much, but we're using the same technology for other interesting problems, so that's probably more.",
                    "label": 0
                },
                {
                    "sent": "More relevant, we've got a multi core version, which is what Swapnil did, which I have to say is wonderful.",
                    "label": 0
                },
                {
                    "sent": "You know like a 7 * 6 times speedup.",
                    "label": 0
                },
                {
                    "sent": "It's so good to only use it.",
                    "label": 0
                },
                {
                    "sent": "It wasn't that difficult to do in hindsight.",
                    "label": 0
                },
                {
                    "sent": "Swapnil's honest thesis.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it's not great.",
                    "label": 0
                },
                {
                    "sent": "It's not slowed down a great deal.",
                    "label": 0
                },
                {
                    "sent": "I remember I think it was David McCallum who's who's a language researcher at Pittsburgh, Massachusetts.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I ran into him about a year ago and he you know, what are you doing?",
                    "label": 0
                },
                {
                    "sent": "Any well, I said I was doing nonparametric methods and he, oh I don't use them anymore.",
                    "label": 0
                },
                {
                    "sent": "They're way too slow where they grind to a halt.",
                    "label": 0
                },
                {
                    "sent": "They're no good.",
                    "label": 0
                },
                {
                    "sent": "So a lot of people are taking that view, and they're not playing with these as much.",
                    "label": 0
                },
                {
                    "sent": "We've now shown that that's no longer the case, so.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are some other algorithms of interest, but this is on GitHub.",
                    "label": 0
                },
                {
                    "sent": "If anyone is interested, it's written in C. It has no dependencies, but I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "It is written under.",
                    "label": 0
                },
                {
                    "sent": "Linix so it actually we've built it and compiled it on a Windows machine.",
                    "label": 0
                },
                {
                    "sent": "I don't run Windows myself.",
                    "label": 0
                },
                {
                    "sent": "The only problem with this, I realize is what I really should have done was a clean, simple iaccessible bit of code for students to edit.",
                    "label": 0
                },
                {
                    "sent": "So 'cause I've had quite a few requests now for people who want to build on the code.",
                    "label": 0
                },
                {
                    "sent": "And frankly this is full of all sorts of.",
                    "label": 0
                },
                {
                    "sent": "Horrible little.",
                    "label": 0
                },
                {
                    "sent": "Shortcuts and you know it.",
                    "label": 0
                },
                {
                    "sent": "It runs very quickly.",
                    "label": 0
                },
                {
                    "sent": "But it's hard to explain to someone the statistics is nontrivial as well, though we right now trying to do tutorials so.",
                    "label": 0
                },
                {
                    "sent": "We're using these same techniques more in the general sense.",
                    "label": 0
                },
                {
                    "sent": "I've been talking about, which is semantic networks and and trying to mimic some of the goals of deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "So that's some of the other.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things which we've been doing, and I'll just give you a brief rundown of that in the last few.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Sentiment.",
                    "label": 0
                },
                {
                    "sent": "One of the problems is that sentiment in different areas tends to be different.",
                    "label": 0
                },
                {
                    "sent": "You just kind of 1 sentiment dictionary.",
                    "label": 0
                },
                {
                    "sent": "You really need to specialize it for different areas, so that's one thing you can do.",
                    "label": 0
                },
                {
                    "sent": "Works well with these probability.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Connect to hierarchies.",
                    "label": 0
                },
                {
                    "sent": "This is our network.",
                    "label": 0
                },
                {
                    "sent": "I won't try and explain it in a short period of time, but we we input a senti word net or one of these sentiment dictionaries.",
                    "label": 0
                },
                {
                    "sent": "We operationalize it into a set of probability vectors, using with some.",
                    "label": 0
                },
                {
                    "sent": "Parameters in here that we learn and then we specialize this probability vector for different facets.",
                    "label": 0
                },
                {
                    "sent": "Cameras, mobiles.",
                    "label": 0
                },
                {
                    "sent": "So you can see the vectors being the probability vectors being related.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example of what comes out.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So cold sausage is no good, apparently.",
                    "label": 0
                },
                {
                    "sent": "Cold is not always are a.",
                    "label": 0
                },
                {
                    "sent": "A bad word.",
                    "label": 0
                },
                {
                    "sent": "It can be good sometimes and supervised.",
                    "label": 0
                },
                {
                    "sent": "So the supervision is weak, it comes from.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An occasional :).",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, this is off tweet this is Twitter so.",
                    "label": 0
                },
                {
                    "sent": "And so the categories you define this through.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the word phone appears so, these have actually come out of these are topic.",
                    "label": 0
                },
                {
                    "sent": "These are coming out as topics and their needs are coming out as.",
                    "label": 0
                },
                {
                    "sent": "The specializations of the.",
                    "label": 0
                },
                {
                    "sent": "This is a sentiment dictionary for that topic.",
                    "label": 0
                },
                {
                    "sent": "Correct, but the topic was manually somehow.",
                    "label": 0
                },
                {
                    "sent": "There are probably many more, so there's many more.",
                    "label": 0
                },
                {
                    "sent": "There's hundreds and hundreds.",
                    "label": 0
                },
                {
                    "sent": "Well, there's two.",
                    "label": 0
                },
                {
                    "sent": "We actually tried it with two or three.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the game, it should be considered positive.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could be right there.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "True, yeah these aren't.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not sure if all of them are right or wrong, but Argentinian I couldn't workout and silly course, but.",
                    "label": 0
                },
                {
                    "sent": "Hope we don't have any Argentinians here.",
                    "label": 0
                },
                {
                    "sent": "Offended by.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's one sort of playing.",
                    "label": 0
                },
                {
                    "sent": "This same technique we used for clustering Twitter, so we were doing very well at clustering Twitter on datasets.",
                    "label": 0
                },
                {
                    "sent": "Using this sort of same technique but that never got published.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the one I was telling you earlier, where you've got a block of text you wanna break it up into meaningful chunks.",
                    "label": 0
                },
                {
                    "sent": "So what was the I wanted to ask before?",
                    "label": 0
                },
                {
                    "sent": "So what's the let's application of this this?",
                    "label": 0
                },
                {
                    "sent": "Usually it's say you want to identify the.",
                    "label": 0
                },
                {
                    "sent": "The chunks who would use the need the chance.",
                    "label": 0
                },
                {
                    "sent": "So 1 version I've heard is for retrieval.",
                    "label": 0
                },
                {
                    "sent": "So you're just passage retrieval and you're going to return bits of it so, but it's usually done as a as a as a testing where people take text transcripts of speech.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, and each of these would be one person speaking.",
                    "label": 0
                },
                {
                    "sent": "And then you're trying to break it up trying to get this works.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah it it.",
                    "label": 0
                },
                {
                    "sent": "It fits pretty well.",
                    "label": 0
                },
                {
                    "sent": "I'll show you well it is the best.",
                    "label": 0
                },
                {
                    "sent": "Yeah currently it's the best and he did a parallel version of it as well, but.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the model and there's the topic model inside.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty complex algo.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're not going to go into it, but here's the here.",
                    "label": 0
                },
                {
                    "sent": "It is on a typical meeting transcript.",
                    "label": 0
                },
                {
                    "sent": "Red is the ground truth where the where the known breaks are, and this is our probability of a break.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the earlier better algorithms.",
                    "label": 0
                },
                {
                    "sent": "See it's having a lot more trouble, so let's see segmenting.",
                    "label": 0
                },
                {
                    "sent": "Of a feature vector would be probably possibly.",
                    "label": 0
                },
                {
                    "sent": "Segmenting video like extracting well, you know the trouble is images are entirely different.",
                    "label": 0
                },
                {
                    "sent": "You know text there are these wonderful semantic tokens.",
                    "label": 0
                },
                {
                    "sent": "But segmenting video there's lots of good information on there, and it's usually just a complete change of color or something.",
                    "label": 0
                },
                {
                    "sent": "Rather, you know, so it.",
                    "label": 0
                },
                {
                    "sent": "This is really only I always say topic models are only good if you got semantic tokens.",
                    "label": 0
                },
                {
                    "sent": "You know things like words.",
                    "label": 0
                },
                {
                    "sent": "In images, you can go to NMF where you just got a different.",
                    "label": 0
                },
                {
                    "sent": "Technology really.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This we are presented in Vietnam at a Asian conference on machine learning, but we put the authors into the so you got the author links from the citations as well as the authors.",
                    "label": 0
                },
                {
                    "sent": "And you're building a network, so this.",
                    "label": 0
                },
                {
                    "sent": "Thurston jochims this is the areas he works in, so these are topics it's coming up with and then you got the authors.",
                    "label": 0
                },
                {
                    "sent": "One thing I noticed here is if I lowered the parameters enough, I could actually get myself to appear on the graph.",
                    "label": 0
                },
                {
                    "sent": "But had to go way down there, but.",
                    "label": 0
                },
                {
                    "sent": "You know, Tom Dietrich, Nir Friedman, Zuben Ghahramani, so you know we only put the bigger ones on here, but this is doing citation analysis.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'd like to know this.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I got duplicated up anyway that's it because Monash was still struggling with its web organization.",
                    "label": 0
                },
                {
                    "sent": "I decided to make my own web page.",
                    "label": 0
                },
                {
                    "sent": "And this is on WordPress, but you can find I've got a tutorial up here on the actual techniques if.",
                    "label": 0
                },
                {
                    "sent": "Anyone is wants to hear more.",
                    "label": 0
                },
                {
                    "sent": "So if you want to get be happy to give sort of go more into the math.",
                    "label": 0
                },
                {
                    "sent": "I've always had the view that mathematics is something that adults do in the consenting adults do in the privacy of their office.",
                    "label": 0
                },
                {
                    "sent": "So rather than a public lecture.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Good are OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}