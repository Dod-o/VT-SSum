{
    "id": "znkg7kjebav5oiahe4mnpe7ao4gnw672",
    "title": "Relational Learning with One Network: An Asymptotic Analysis",
    "info": {
        "author": [
            "Rongjing Xiang, Purdue University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Network Analysis",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_xiang_network/",
    "segmentation": [
        [
            "OK, so good morning everyone I'm running and this is joint work with my advisor Gen level and where from produce.",
            "So let me begin with the sorry."
        ],
        [
            "Sorry.",
            "Let me begin with an overview of the key issues we are trying to address in this work."
        ],
        [
            "So the sentences in this slide will be made more precise in the moment, but here our first motivation is that although relational learning methods have been applied mainly to sync to a single network, there has been no theoretical study of the properties of these learning algorithms, so."
        ],
        [
            "I will so we attempt to address this issue by a first study in the asymptomatic behaviors of these relational estimated hers in the sense that the training graph grows to Infinity.",
            "Based on this analysis, we attempt to further address the practical issue in relational learning, which is combinatorial blowup of computational complexity.",
            "Since the relational models grows with the training data size, an structures in the data needs to be better explored."
        ],
        [
            "Our asymptotically analysis result indicates that this exploration can be done in the context of joint learning and disjoint learning.",
            "We will improve, try to improve the quality of this joint learning by using Asymptotical results.",
            "So with these goals."
        ],
        [
            "In mind, let me now introduce some background in relational learning first."
        ],
        [
            "Here is the conventional setting of machine learning where we assume that the data instances are samples from the underlying lying population of ID instances where."
        ],
        [
            "Each instance has.",
            "An attribute vector X and."
        ],
        [
            "It's the labels, so these two aspects carry over to the relational setting.",
            "However, in relational domains we have, we can observe the natural links between data instances which encode the probability distribution probability dependencies between the data instances, so they are no longer ID."
        ],
        [
            "And this is a key difference between relational learning and ideal learning.",
            "We attempt to use the dependency to improve prediction performance."
        ],
        [
            "More specifically, in our setting, we assume that the the domain is consists of a single graph, meaning that each instance is potentially dependent on and everything else.",
            "There are actually many examples of such domains, including World Wide Web and social networks."
        ],
        [
            "Now let's do this.",
            "Let me describe the learning and inference tasks in single network domains."
        ],
        [
            "So during training we observe fully labeled training subgraphs from the underlying network and we learn relational model."
        ],
        [
            "During application we observe another sub."
        ],
        [
            "Graph from the."
        ],
        [
            "Underlying network an it could be."
        ],
        [
            "Partially labeled or."
        ],
        [
            "Fully unlabeled, we apply the learning model to collectively classify the the unlabeled in."
        ],
        [
            "Senses so in this work we will focus on the training aspect.",
            "Our analysis is built on previous work and the most popular approaching relational learning.",
            "The Markovian relational model."
        ],
        [
            "Most."
        ],
        [
            "There has been a number of previous work in this model, for example, relational Markov networks, relation Markov logic networks, and relational dependency networks.",
            "Two major aspects of these Markovian relational models, Markov assumptions and click templating.",
            "The Markov properties, just as in a Markov network model, the."
        ],
        [
            "As for the click templates, each click type is defined on a small repetitive sub graph structure."
        ],
        [
            "For example, edgewise."
        ],
        [
            "Clicks are defined on the edges."
        ],
        [
            "And a node centric click is defined on each instance and its related instances.",
            "Each click type is."
        ],
        [
            "Going to be rolled out all over the training graph and this results in."
        ],
        [
            "Joint probability distribution of all the labels of the in."
        ],
        [
            "And seeing the training network and."
        ],
        [
            "Here we can see that T is template."
        ],
        [
            "See other clicks."
        ],
        [
            "And we assume the potential function to be of log linear form to simplify the notations.",
            "In our analysis.",
            "We further assume that the we have only one."
        ],
        [
            "Templates and the joint distribution reduced to this form."
        ],
        [
            "And we have explicitly written out the log linear potential potential function here."
        ],
        [
            "We use file to denote the feature extracted from each click also."
        ],
        [
            "So we use N to denote."
        ],
        [
            "Number of clicks in the training graph."
        ],
        [
            "And we can see that the Markov network model different.",
            "Formulated here is actually actually grows with the training graph size.",
            "This is very different from the conventional use of Markov network to ID data where the size of Markov network is always fixed."
        ],
        [
            "So again, here is a joint distribution of the labels given the attributes and direct learning approach."
        ],
        [
            "You would be maximizing this likelihood."
        ],
        [
            "And this will involve enumerating all of the labels in the graph jointly.",
            "You can see very clearly from the partition function where the summation is over the space of all possible joint labelings, and the complexity grows exponentially with the number of instances in the training graph, which is generally intractable."
        ],
        [
            "So another approach which has been explored in relational learning for single networks is pseudo likelihood."
        ],
        [
            "In the work of relational dependency network here we consider a generalized version of it where we partition."
        ],
        [
            "And the graph into components and then the pseudo likelihood objective will be factorized over."
        ],
        [
            "The components so each factor is going to be a conditional probability over the joint labeling with."
        ],
        [
            "Each component an given given the labeling."
        ],
        [
            "In the mall."
        ],
        [
            "Of blanket so the."
        ],
        [
            "Key difference between employee and Emily is in the partition function.",
            "We can see that the.",
            "Partition function of mpoe is only integrated over the space of the joint labeling within each component, so that the computation."
        ],
        [
            "Does not remain fixed when the training graph size grows, so here we use.",
            "Now we use M to denote the number of pseudo likelihood components and.",
            "The number of similar likelihood components will grow with the training graph size becausw.",
            "We assume that the pseudo likelihood components to be of bounded size.",
            "So by an asymptomatic analysis we."
        ],
        [
            "Meaning that both end and then grow to Infinity."
        ],
        [
            "OK, now with this background we can take a closer look."
        ],
        [
            "At the key in the first key issue so.",
            "Although these Markovian relational models have been successfully applied to many single network domains, there it is not clear whether they are theoretically justified.",
            "First question to ask is whether these estimators will converge properly as we acquire more and more training data."
        ],
        [
            "So in single network domains, acquiring more and more training data will correspond to acquiring a larger and larger training graph, so this is different from the conventional asymptotic analysis."
        ],
        [
            "So now we are going to outline the conditions in which the estimators are syntactically consistent and normal.",
            "Smith"
        ],
        [
            "To do an asymptomatic analysis, we."
        ],
        [
            "First need to need the probability measure of Markov network to be to exist for infinite networks.",
            "So this can be satisfied by a sufficient condition which is local finiteness.",
            "It has two aspects.",
            "First we assume each node has a bounded degree.",
            "Then we assume the click potential function to be bounded, so."
        ],
        [
            "So this local finiteness assumption ensure the existence of an infinite Markov network for the model to converge we also need a."
        ],
        [
            "Further assumption which we call weak dependence.",
            "It's assumed that the total covariance of each click with all the other clicks in the network is finite.",
            "Intuitively, this assumption means that the correlation is concentrated quite locali so."
        ],
        [
            "For each, for any local click, if the correlation with all the other clicks in the graph decays very rapidly."
        ],
        [
            "With respect to graph distance.",
            "Then this total finite total variance assumption assumption will be satisfied."
        ],
        [
            "In the literature there are several other weak dependence assumptions.",
            "Especially in sequential data there there are various mixing conditions, and in fact these mixing conditions usually imply our finite code.",
            "Total covariance assumption here."
        ],
        [
            "OK, so now we explain why we need this weak dependence assumption."
        ],
        [
            "First, some notations we review from the literature of standard exponential families.",
            "The gradients of the log."
        ],
        [
            "Play hood function the only."
        ],
        [
            "So here is that we normalize the log likelihood function by the number of clicks in the graph so that the log likelihood function does not go to infinite."
        ],
        [
            "And similarly we have the gradient of the log pseudo likelihood function, where the expectation is taken is conditioned on the Markov blanket of each component.",
            "So the immediately implication of weak dependence is a law of large number or gold density type of result.",
            "It says that the."
        ],
        [
            "The average of feature values will converge to their expectation in probability, so we can see that both."
        ],
        [
            "These two gradients will converge to zero in probability.",
            "And we can further characterize the."
        ],
        [
            "Sympathetic convergence rates by Central Limit theorem type of results.",
            "We have proved in the paper that the convergent."
        ],
        [
            "Distribution of the Group of the gradient is normal."
        ],
        [
            "And also for the pseudo likelihood, so the."
        ],
        [
            "Asymptotical variance of the Murray here can be interpreted as the article variance of feature values within the network for the."
        ],
        [
            "MPL yet involves two covariance matrices that KDV here can be interpreted as covariate.",
            "The article variance of feature within each component and see can be interpreted as."
        ],
        [
            "The between component covariance."
        ],
        [
            "OK, so now we have a first justification of relational estimators for single network domains.",
            "Both the Emily and NPR asymptomatically consistent, so both."
        ],
        [
            "Estimators will converge to their true parameter value in probability based on the previously mention."
        ],
        [
            "Assumption local finiteness, weak dependence and technical assumption.",
            "The identifiability of parameters which is standard."
        ],
        [
            "For the analysis of exponential families.",
            "So again to remind you here, Theta N is estimated from training graph of size of N clicks while the employee is indexed by the number of pseudo likelihood components."
        ],
        [
            "So our proof of consistency is based on the textbook lemma.",
            "And it consists of three conditions.",
            "Here we only mention the key ingredients for the proof of.",
            "The ME for the sake of time so."
        ],
        [
            "If we let the MN the random function MN to be the log joint likelihood of of the of the data.",
            "So here note that this is the joint data likelihood to be differentiated from our conditional estimator, which is conditioned on X.",
            "This is for the purpose of linking the conditional estimator to the generative joint distribution."
        ],
        [
            "So the first condition can be verified by the previously mentioned law of large numbers, and the second condition the right hand side minus obstructed by the left hand side, is essentially the KL divergences between the two global distributions, parameterized by Sita Zero and the."
        ],
        [
            "And by using local finiteness we can reduce it into local KL divergent switches.",
            "The KL divergences between the two local conditional probability distributions because these local KL divergences strictly positive.",
            "This second condition follows and thus."
        ],
        [
            "3rd condition can be verified based by using the convexity of log partition function."
        ],
        [
            "So."
        ],
        [
            "So now we have the same topic as consistency.",
            "Based on this we can use the Delta method to convert the central limit theorem for gradients into the asymptotic normality of the estimators.",
            "So we have the asymptotic normality of the Emily here.",
            "The symbiotic variance is the same as in the central limit theorem.",
            "V can be interpreted as the within component auto covariance.",
            "And similarly we have the asymptotic."
        ],
        [
            "Normality of the employee an again till the V."
        ],
        [
            "Is the within component."
        ],
        [
            "Auto covariance see it."
        ],
        [
            "The beat."
        ],
        [
            "Component covariance, so now we can compare these two estimators by."
        ],
        [
            "Comparing their their eyes."
        ],
        [
            "Arctic variants.",
            "So the 1st result is quite intuitive.",
            "The smell is asymptotically more efficient than the employee and special cases that when the PL components."
        ],
        [
            "Independent of each other, the equality will hold, so Secondly."
        ],
        [
            "We can also compare the different pseudo likelihood constructions.",
            "Basically the employee with cause the partition is asymptomatically more efficient than an employee with."
        ],
        [
            "The partition, so at this point we."
        ],
        [
            "We have finished the discussion of the first issue.",
            "As for the second issue we mentioned in the beginning that we are going to explore the structure in the data by a tradeoff between disjoint and joint learning.",
            "So by disjoint learning conceptually it."
        ],
        [
            "Means that we can partition the graph into."
        ],
        [
            "Conan's and then the joint objective."
        ],
        [
            "Be approximately optimized by optimizing a factorized objective.",
            "So in the asymptotical analysis we have by comparing the join the app same tatic variances we have already.",
            "Partially addressed the disjoint learning issue, we know that the final partition will be worse than the.",
            "Then the cause of partition, but in."
        ],
        [
            "Practice.",
            "Usually the partitions are not strictly final causes and each other, so these results are not immediately useful.",
            "We are now going to further explore the asymptotical variance to improve."
        ],
        [
            "MPL"
        ],
        [
            "So ideally we want to optimize the asymptotic variance."
        ],
        [
            "So all of the involved matrixes are not cannot be efficiently evaluated."
        ],
        [
            "But we can approximately optimize either the within component covariance or the between component covariance."
        ],
        [
            "This might be related to the graph clustering algorithms.",
            "However our pseudo likelihood components typically very small or and possibly overlapping clusters of nodes which are not typically dealt with by component by graph clustering algo."
        ],
        [
            "So in this work we simply investigate a simple heuristic which maximizes the within component linkage greedily, and we compare this approach to a random construction based on breadth first search or depth first search."
        ],
        [
            "Over the same model.",
            "So here are the estimation results on the on the synthetic data.",
            "We evaluate the mean square error of the estimation.",
            "We can."
        ],
        [
            "See that usually the MLB."
        ],
        [
            "Without seeing the lowest estimation error, and but unfortunately it can only be applied to very small networks, the heuristic approach usually achieves fastest estimation convergence."
        ],
        [
            "Among all the mpoe constructions, and it significantly up for."
        ],
        [
            "Perform the random construction an."
        ],
        [
            "Not surprisingly, the lower order pseudo likelihood components converge most slowly."
        ],
        [
            "So in conclusion, the heuristically constructed employee improve estimation accuracy."
        ],
        [
            "So estimation convergence, so we also want to know in real data whether the our pseudo likelihood component heuristic construction can improve classification accuracy at all.",
            "So we apply our method to the Purdue Facebook."
        ],
        [
            "Network and here are two classification examples where we predict the gender and political view of the instances so."
        ],
        [
            "We also compared to the baseline independent learning approach.",
            "We can see that relational learning indeed improve over."
        ],
        [
            "And learning and then we see that the random construction doesn't really achieve good performance relative to the Singleton and pairwise construction construction in real data."
        ],
        [
            "But our heuristic achieves consistently better or equivalent performance across the number of training graphs."
        ],
        [
            "So inclusion we can say that heuristically constructed mpoe can improve classification as accuracy as well."
        ],
        [
            "To conclude."
        ],
        [
            "In this work, we have done an asymptotical analysis.",
            "Of the employee and the MLB for relational learning.",
            "Single network domain."
        ],
        [
            "Things and our study indicates that a better tradeoff between joint learning and disjoint learning can be achieved by maximizing within component."
        ],
        [
            "Clarence and we experimentally investigated heuristic peer component construction algorithm and show that it improves the estimation accuracy and classification performance.",
            "This only provides a starting point for more sophisticated algorithms based on asymptotically covariance."
        ],
        [
            "We believe that further exploration in this direction will be very helpful for the relational learning community.",
            "Becausw, the relational model, grows with the training graph size and the real network has a lot of structures, especially the high clustering coefficient, which has not been well explored in relational learning algorithms.",
            "On the."
        ],
        [
            "Theoretical side we are currently trying to characterize the convergence rates in the finite data region.",
            "This can."
        ],
        [
            "Use the talk, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so good morning everyone I'm running and this is joint work with my advisor Gen level and where from produce.",
                    "label": 0
                },
                {
                    "sent": "So let me begin with the sorry.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Let me begin with an overview of the key issues we are trying to address in this work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the sentences in this slide will be made more precise in the moment, but here our first motivation is that although relational learning methods have been applied mainly to sync to a single network, there has been no theoretical study of the properties of these learning algorithms, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will so we attempt to address this issue by a first study in the asymptomatic behaviors of these relational estimated hers in the sense that the training graph grows to Infinity.",
                    "label": 1
                },
                {
                    "sent": "Based on this analysis, we attempt to further address the practical issue in relational learning, which is combinatorial blowup of computational complexity.",
                    "label": 1
                },
                {
                    "sent": "Since the relational models grows with the training data size, an structures in the data needs to be better explored.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our asymptotically analysis result indicates that this exploration can be done in the context of joint learning and disjoint learning.",
                    "label": 1
                },
                {
                    "sent": "We will improve, try to improve the quality of this joint learning by using Asymptotical results.",
                    "label": 0
                },
                {
                    "sent": "So with these goals.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In mind, let me now introduce some background in relational learning first.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the conventional setting of machine learning where we assume that the data instances are samples from the underlying lying population of ID instances where.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each instance has.",
                    "label": 0
                },
                {
                    "sent": "An attribute vector X and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the labels, so these two aspects carry over to the relational setting.",
                    "label": 0
                },
                {
                    "sent": "However, in relational domains we have, we can observe the natural links between data instances which encode the probability distribution probability dependencies between the data instances, so they are no longer ID.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a key difference between relational learning and ideal learning.",
                    "label": 0
                },
                {
                    "sent": "We attempt to use the dependency to improve prediction performance.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More specifically, in our setting, we assume that the the domain is consists of a single graph, meaning that each instance is potentially dependent on and everything else.",
                    "label": 0
                },
                {
                    "sent": "There are actually many examples of such domains, including World Wide Web and social networks.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's do this.",
                    "label": 0
                },
                {
                    "sent": "Let me describe the learning and inference tasks in single network domains.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So during training we observe fully labeled training subgraphs from the underlying network and we learn relational model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "During application we observe another sub.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph from the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Underlying network an it could be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partially labeled or.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fully unlabeled, we apply the learning model to collectively classify the the unlabeled in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Senses so in this work we will focus on the training aspect.",
                    "label": 0
                },
                {
                    "sent": "Our analysis is built on previous work and the most popular approaching relational learning.",
                    "label": 1
                },
                {
                    "sent": "The Markovian relational model.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There has been a number of previous work in this model, for example, relational Markov networks, relation Markov logic networks, and relational dependency networks.",
                    "label": 0
                },
                {
                    "sent": "Two major aspects of these Markovian relational models, Markov assumptions and click templating.",
                    "label": 0
                },
                {
                    "sent": "The Markov properties, just as in a Markov network model, the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As for the click templates, each click type is defined on a small repetitive sub graph structure.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, edgewise.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clicks are defined on the edges.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a node centric click is defined on each instance and its related instances.",
                    "label": 0
                },
                {
                    "sent": "Each click type is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to be rolled out all over the training graph and this results in.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Joint probability distribution of all the labels of the in.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And seeing the training network and.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we can see that T is template.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See other clicks.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we assume the potential function to be of log linear form to simplify the notations.",
                    "label": 0
                },
                {
                    "sent": "In our analysis.",
                    "label": 0
                },
                {
                    "sent": "We further assume that the we have only one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Templates and the joint distribution reduced to this form.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have explicitly written out the log linear potential potential function here.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use file to denote the feature extracted from each click also.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use N to denote.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Number of clicks in the training graph.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can see that the Markov network model different.",
                    "label": 0
                },
                {
                    "sent": "Formulated here is actually actually grows with the training graph size.",
                    "label": 0
                },
                {
                    "sent": "This is very different from the conventional use of Markov network to ID data where the size of Markov network is always fixed.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, here is a joint distribution of the labels given the attributes and direct learning approach.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You would be maximizing this likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this will involve enumerating all of the labels in the graph jointly.",
                    "label": 0
                },
                {
                    "sent": "You can see very clearly from the partition function where the summation is over the space of all possible joint labelings, and the complexity grows exponentially with the number of instances in the training graph, which is generally intractable.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another approach which has been explored in relational learning for single networks is pseudo likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the work of relational dependency network here we consider a generalized version of it where we partition.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the graph into components and then the pseudo likelihood objective will be factorized over.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The components so each factor is going to be a conditional probability over the joint labeling with.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each component an given given the labeling.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the mall.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of blanket so the.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Key difference between employee and Emily is in the partition function.",
                    "label": 1
                },
                {
                    "sent": "We can see that the.",
                    "label": 1
                },
                {
                    "sent": "Partition function of mpoe is only integrated over the space of the joint labeling within each component, so that the computation.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does not remain fixed when the training graph size grows, so here we use.",
                    "label": 0
                },
                {
                    "sent": "Now we use M to denote the number of pseudo likelihood components and.",
                    "label": 0
                },
                {
                    "sent": "The number of similar likelihood components will grow with the training graph size becausw.",
                    "label": 0
                },
                {
                    "sent": "We assume that the pseudo likelihood components to be of bounded size.",
                    "label": 0
                },
                {
                    "sent": "So by an asymptomatic analysis we.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meaning that both end and then grow to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now with this background we can take a closer look.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the key in the first key issue so.",
                    "label": 0
                },
                {
                    "sent": "Although these Markovian relational models have been successfully applied to many single network domains, there it is not clear whether they are theoretically justified.",
                    "label": 0
                },
                {
                    "sent": "First question to ask is whether these estimators will converge properly as we acquire more and more training data.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in single network domains, acquiring more and more training data will correspond to acquiring a larger and larger training graph, so this is different from the conventional asymptotic analysis.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we are going to outline the conditions in which the estimators are syntactically consistent and normal.",
                    "label": 0
                },
                {
                    "sent": "Smith",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do an asymptomatic analysis, we.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First need to need the probability measure of Markov network to be to exist for infinite networks.",
                    "label": 0
                },
                {
                    "sent": "So this can be satisfied by a sufficient condition which is local finiteness.",
                    "label": 0
                },
                {
                    "sent": "It has two aspects.",
                    "label": 0
                },
                {
                    "sent": "First we assume each node has a bounded degree.",
                    "label": 0
                },
                {
                    "sent": "Then we assume the click potential function to be bounded, so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this local finiteness assumption ensure the existence of an infinite Markov network for the model to converge we also need a.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further assumption which we call weak dependence.",
                    "label": 0
                },
                {
                    "sent": "It's assumed that the total covariance of each click with all the other clicks in the network is finite.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, this assumption means that the correlation is concentrated quite locali so.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each, for any local click, if the correlation with all the other clicks in the graph decays very rapidly.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With respect to graph distance.",
                    "label": 0
                },
                {
                    "sent": "Then this total finite total variance assumption assumption will be satisfied.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the literature there are several other weak dependence assumptions.",
                    "label": 0
                },
                {
                    "sent": "Especially in sequential data there there are various mixing conditions, and in fact these mixing conditions usually imply our finite code.",
                    "label": 0
                },
                {
                    "sent": "Total covariance assumption here.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we explain why we need this weak dependence assumption.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, some notations we review from the literature of standard exponential families.",
                    "label": 0
                },
                {
                    "sent": "The gradients of the log.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play hood function the only.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is that we normalize the log likelihood function by the number of clicks in the graph so that the log likelihood function does not go to infinite.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And similarly we have the gradient of the log pseudo likelihood function, where the expectation is taken is conditioned on the Markov blanket of each component.",
                    "label": 0
                },
                {
                    "sent": "So the immediately implication of weak dependence is a law of large number or gold density type of result.",
                    "label": 1
                },
                {
                    "sent": "It says that the.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The average of feature values will converge to their expectation in probability, so we can see that both.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These two gradients will converge to zero in probability.",
                    "label": 0
                },
                {
                    "sent": "And we can further characterize the.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sympathetic convergence rates by Central Limit theorem type of results.",
                    "label": 0
                },
                {
                    "sent": "We have proved in the paper that the convergent.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution of the Group of the gradient is normal.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also for the pseudo likelihood, so the.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asymptotical variance of the Murray here can be interpreted as the article variance of feature values within the network for the.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "MPL yet involves two covariance matrices that KDV here can be interpreted as covariate.",
                    "label": 0
                },
                {
                    "sent": "The article variance of feature within each component and see can be interpreted as.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The between component covariance.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have a first justification of relational estimators for single network domains.",
                    "label": 0
                },
                {
                    "sent": "Both the Emily and NPR asymptomatically consistent, so both.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimators will converge to their true parameter value in probability based on the previously mention.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assumption local finiteness, weak dependence and technical assumption.",
                    "label": 0
                },
                {
                    "sent": "The identifiability of parameters which is standard.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the analysis of exponential families.",
                    "label": 0
                },
                {
                    "sent": "So again to remind you here, Theta N is estimated from training graph of size of N clicks while the employee is indexed by the number of pseudo likelihood components.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our proof of consistency is based on the textbook lemma.",
                    "label": 0
                },
                {
                    "sent": "And it consists of three conditions.",
                    "label": 0
                },
                {
                    "sent": "Here we only mention the key ingredients for the proof of.",
                    "label": 0
                },
                {
                    "sent": "The ME for the sake of time so.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we let the MN the random function MN to be the log joint likelihood of of the of the data.",
                    "label": 0
                },
                {
                    "sent": "So here note that this is the joint data likelihood to be differentiated from our conditional estimator, which is conditioned on X.",
                    "label": 0
                },
                {
                    "sent": "This is for the purpose of linking the conditional estimator to the generative joint distribution.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first condition can be verified by the previously mentioned law of large numbers, and the second condition the right hand side minus obstructed by the left hand side, is essentially the KL divergences between the two global distributions, parameterized by Sita Zero and the.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by using local finiteness we can reduce it into local KL divergent switches.",
                    "label": 0
                },
                {
                    "sent": "The KL divergences between the two local conditional probability distributions because these local KL divergences strictly positive.",
                    "label": 0
                },
                {
                    "sent": "This second condition follows and thus.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd condition can be verified based by using the convexity of log partition function.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have the same topic as consistency.",
                    "label": 0
                },
                {
                    "sent": "Based on this we can use the Delta method to convert the central limit theorem for gradients into the asymptotic normality of the estimators.",
                    "label": 0
                },
                {
                    "sent": "So we have the asymptotic normality of the Emily here.",
                    "label": 0
                },
                {
                    "sent": "The symbiotic variance is the same as in the central limit theorem.",
                    "label": 0
                },
                {
                    "sent": "V can be interpreted as the within component auto covariance.",
                    "label": 0
                },
                {
                    "sent": "And similarly we have the asymptotic.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normality of the employee an again till the V.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the within component.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Auto covariance see it.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The beat.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Component covariance, so now we can compare these two estimators by.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comparing their their eyes.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arctic variants.",
                    "label": 0
                },
                {
                    "sent": "So the 1st result is quite intuitive.",
                    "label": 0
                },
                {
                    "sent": "The smell is asymptotically more efficient than the employee and special cases that when the PL components.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independent of each other, the equality will hold, so Secondly.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also compare the different pseudo likelihood constructions.",
                    "label": 0
                },
                {
                    "sent": "Basically the employee with cause the partition is asymptomatically more efficient than an employee with.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The partition, so at this point we.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have finished the discussion of the first issue.",
                    "label": 0
                },
                {
                    "sent": "As for the second issue we mentioned in the beginning that we are going to explore the structure in the data by a tradeoff between disjoint and joint learning.",
                    "label": 0
                },
                {
                    "sent": "So by disjoint learning conceptually it.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means that we can partition the graph into.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conan's and then the joint objective.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be approximately optimized by optimizing a factorized objective.",
                    "label": 0
                },
                {
                    "sent": "So in the asymptotical analysis we have by comparing the join the app same tatic variances we have already.",
                    "label": 0
                },
                {
                    "sent": "Partially addressed the disjoint learning issue, we know that the final partition will be worse than the.",
                    "label": 0
                },
                {
                    "sent": "Then the cause of partition, but in.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practice.",
                    "label": 0
                },
                {
                    "sent": "Usually the partitions are not strictly final causes and each other, so these results are not immediately useful.",
                    "label": 0
                },
                {
                    "sent": "We are now going to further explore the asymptotical variance to improve.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "MPL",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So ideally we want to optimize the asymptotic variance.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all of the involved matrixes are not cannot be efficiently evaluated.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can approximately optimize either the within component covariance or the between component covariance.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This might be related to the graph clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "However our pseudo likelihood components typically very small or and possibly overlapping clusters of nodes which are not typically dealt with by component by graph clustering algo.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work we simply investigate a simple heuristic which maximizes the within component linkage greedily, and we compare this approach to a random construction based on breadth first search or depth first search.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the same model.",
                    "label": 0
                },
                {
                    "sent": "So here are the estimation results on the on the synthetic data.",
                    "label": 0
                },
                {
                    "sent": "We evaluate the mean square error of the estimation.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See that usually the MLB.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Without seeing the lowest estimation error, and but unfortunately it can only be applied to very small networks, the heuristic approach usually achieves fastest estimation convergence.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Among all the mpoe constructions, and it significantly up for.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perform the random construction an.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not surprisingly, the lower order pseudo likelihood components converge most slowly.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion, the heuristically constructed employee improve estimation accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So estimation convergence, so we also want to know in real data whether the our pseudo likelihood component heuristic construction can improve classification accuracy at all.",
                    "label": 0
                },
                {
                    "sent": "So we apply our method to the Purdue Facebook.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network and here are two classification examples where we predict the gender and political view of the instances so.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compared to the baseline independent learning approach.",
                    "label": 0
                },
                {
                    "sent": "We can see that relational learning indeed improve over.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And learning and then we see that the random construction doesn't really achieve good performance relative to the Singleton and pairwise construction construction in real data.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But our heuristic achieves consistently better or equivalent performance across the number of training graphs.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So inclusion we can say that heuristically constructed mpoe can improve classification as accuracy as well.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this work, we have done an asymptotical analysis.",
                    "label": 0
                },
                {
                    "sent": "Of the employee and the MLB for relational learning.",
                    "label": 0
                },
                {
                    "sent": "Single network domain.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things and our study indicates that a better tradeoff between joint learning and disjoint learning can be achieved by maximizing within component.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clarence and we experimentally investigated heuristic peer component construction algorithm and show that it improves the estimation accuracy and classification performance.",
                    "label": 0
                },
                {
                    "sent": "This only provides a starting point for more sophisticated algorithms based on asymptotically covariance.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We believe that further exploration in this direction will be very helpful for the relational learning community.",
                    "label": 0
                },
                {
                    "sent": "Becausw, the relational model, grows with the training graph size and the real network has a lot of structures, especially the high clustering coefficient, which has not been well explored in relational learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "On the.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theoretical side we are currently trying to characterize the convergence rates in the finite data region.",
                    "label": 0
                },
                {
                    "sent": "This can.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use the talk, thank you.",
                    "label": 0
                }
            ]
        }
    }
}