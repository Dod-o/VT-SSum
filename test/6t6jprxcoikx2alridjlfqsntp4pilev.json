{
    "id": "6t6jprxcoikx2alridjlfqsntp4pilev",
    "title": "On probabilistic hypergraph matching",
    "info": {
        "author": [
            "Amnon Shashua, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "July 20, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/icml2010_shashua_ophm/",
    "segmentation": [
        [
            "So I'll be talking about the similarity.",
            "And non geometric similarity.",
            "Embedded into a graph matching problem.",
            "So we have two 2 point sets.",
            "The points are points, sets are undergoing nonrigid transformation.",
            "Some general nonrigid transformation.",
            "The matching that we would like to do is not complete.",
            "There could be some points in one set that are not matched to another two other points in another set.",
            "And I'll present to you Ristic very simple, but very powerful heuristic that we presented two years ago in CPR, and since then created a lot of follow up work.",
            "It's very simple that it involves a suspicious assumption which when we get there, I'll show it to you, but if you ignore the correctness of the assumption you get out a very nice algorithm and works."
        ],
        [
            "Very nicely so we have.",
            "In computer vision, you know we take images.",
            "We find the matching.",
            "We find points and then would like to match points between 2 images or track points.",
            "And in many cases the transfer.",
            "It's not an isometry, right?",
            "It's not a rigid transformation, translation and rotation that describes how the point matches behave, but it's some non rigid transformation as we can see here, the leg is in different position and so forth.",
            "But we could have some local local structure.",
            "So globally the point sets.",
            "Would not behave under some global nice transformation, but locali things could behave.",
            "Things would behave well like local affine transformations.",
            "Now the idea of putting it into a graph structure.",
            "Allows us to so.",
            "We have edges or hyperedges representing relationships between points in each set, and we're presenting this as in height as a hypergraph allows us to put arbitrary complex relationships among points in each in each set.",
            "So we have an arbitrary relationship represented by hyper edges in the graph, and then we would like to match these hyperedges in between."
        ],
        [
            "Two in between the two hyper graphs.",
            "So we have a collection of points in one area, so this is 1 graph, a collection of points in the other set, which is another graph and would like to match these two.",
            "But instead of describing relationships only between pairwise pair pairs of points, a hypergraph allows us to establish relationships among triplets among N couples of points.",
            "So we still have the number of vertices remains the same, but the edges now represent complex relationships among multiple points, and each tuple is represented by by hyper edge, and on that hyperedge we put in number, which would be an invariant, which I'll show."
        ],
        [
            "In a moment.",
            "So for example, if we would like to.",
            "This establish a fun invariant properties.",
            "Then we will need each hyperedge will need to work on four points, so with four points, or each invariant if you have an invite of endpoints you'll need N + 1 points in order to describe that invariant.",
            "So for example, if we have local IAFF and invariants and we have a representation of four points as a hyperedge, then we can calculate the ratio of the areas and we get enough fine invariant if indeed there is enough.",
            "Local ER fine invariant.",
            "So if we have a collection of points, and we describe hyperedges among four couples of points, and if there is a local invariants in the matching local, often inventing the magic between the two sets, the numbers on the hyper edges would match completely accurately between the two between the datasets.",
            "The problem is matching hypergraphs is an NP hard problem, so even though you can describe it very nicely, finding a solution.",
            "Would be very would be very difficult."
        ],
        [
            "The way we're approaching it is we're looking at at the probabilistic way of defining the matching, so it's an inexact graph matching.",
            "The conventional way is to describe a hard matching problem and then to relax it.",
            "We are going the other way around.",
            "We are defining a soft matching criteria.",
            "We're going to find the global optimum of the soft matching criteria.",
            "Once we have a soft matching, we can then go to a hard matching using the linear assignment problem.",
            "Overall, it will not give us the optimal solution because there could be another route not going through a soft matching which would give us a better solution.",
            "But what will show that the soft matching idea that we're putting here is a very simple idea with a very simple algorithm.",
            "That algorithm describes and in particular cases.",
            "For example, when you have complete matches when you have.",
            "The entire set of points in one set is matched to the entire set of points in the other set.",
            "There are no free points in the set.",
            "You get known existing algorithms.",
            "Also, there are certain heuristics that people applied in normalizing the measurement matrices, which come out very naturally in what will be doing so.",
            "We're going through a soft match."
        ],
        [
            "Pink soft matching idea.",
            "If we have a software solution, which is a probabilistic assignment between vertices between the two between the two sets, one can then go to a hard matching using a linear assignment problem.",
            "But what we also note that in many application domains, your goal is really the soft assignment.",
            "For example, in in tracking problems where you have multiple frames and you want to track points along multiple frames, you wouldn't necessarily want that in between each two frames you have a hard assignment you would like to keep the ambiguities until the end of the process, and then make a hard decision.",
            "So in a tracking process you would like to have a soft assignment or a probabilistic assignment of points along the sequence.",
            "And then later when you have more than three 4 frames to make a hard decision.",
            "So sometimes soft assignment is not a byproduct, it's really the product of what you what you are looking for.",
            "But in any case, if you are looking for a hard assignment then what we claim go for a soft assignment and then do a linear assignment algorithm."
        ],
        [
            "OK, so the hypergraph matching problem, the way we describe it, so we have two hyper graphs of degree D. That means each hyperedge is defined on D points.",
            "So when D = 2, you have a graph.",
            "The set of vertices is as normally defined in graph, so we have a set of points which compared the set of vertices and the set of hyper edges are defined between the temples of points.",
            "What we are interested is the matching function between the set of points in the first graph to the set of points in the second graph, or this matching is not necessarily complete.",
            "There could be points in the first set of parts that are not matched to the second and vice versa.",
            "Now, once we have a matching between, if we have established the matching between points, then clearly we have also established the matching between the hyperedges.",
            "Because if V1 to VDR, the couple of a hyperedge, then the matching hyper air just simply the mapping of the one and the map at Milda mapping of VD.",
            "So once we establish the matching between the points, clearly the matching between the edges naturally."
        ],
        [
            "It is well defined.",
            "So what is our input the input?",
            "And that's what I'm defining now.",
            "So what is our input?",
            "The inputs are weights on the hyper edges, so the way we define it is that the probability that an edge.",
            "From the first graph, matches an edge E prime in the second.",
            "In the second graph, and this would be written here as a matrix.",
            "Between Edge E and edge E Prime, which is the probability that the matching of E gives us the prime and the matching of E is defined."
        ],
        [
            "As I said in the previous slide, if we have a matching function between the vertice is, it defines the matching between between edges."
        ],
        [
            "So, in in case where we have a graph that couples are pairwise, this is a matrix.",
            "So this is a well, it's it's a matrix where it's have N squared by N square, where N is the number of vertice is.",
            "Now what we are interested as the output is the probability that two verticies match, so let's call that the matrix X between vertex V and vertex V prime, which the probability that the matching of V gives us a week right?",
            "So the output is always an N by N matrix, the input.",
            "Is an end to the power of D to the end to the power of the matrix, where D is the size of the couples that we have.",
            "But will show will define a simple algebraic connection between S and and X.",
            "Once we define this connection will have a very simple optimization algorithm that will recover X from from S."
        ],
        [
            "First, a simple definition of what is a Chronicle product between two matrices.",
            "It's very simple if you have a metrics be you have copies of the matrix B multiplied by the entries of a.",
            "Let's say a simple definition and then you can apply this definition incrementally.",
            "If you have more than two elements in the product, you do that pairwise and you simply concatenate them.",
            "So that will be a.",
            "If we have D matrices A1 to lady, the product of all of them together is defined.",
            "This way, we're then you can go back to this definition for every pair."
        ],
        [
            "OK, so.",
            "So this is this suspicious assumption.",
            "I'm going to make an assumption that given the graphs, the decision the mapping of V1.",
            "Is independent, conditionally independent the mapping of V2?",
            "Now I'm not going to spend time on convincing you that this assumption is correct, because this assumption is normally not correct.",
            "OK, so let's.",
            "For every one V2.",
            "So this is making assumption of conditional independence that if when you observe the graphs and you make a decision on the mapping of V1 and then you observe the graph again and make a mapping of V2, these two are independent and I'm not going to defend it because normally it's not correct.",
            "But let's make this assumption and see where this assumption leads us because it's going to lead us to known algorithms in particular cases and to new algorithm in the general case where the matching is not.",
            "Is not complete.",
            "So where does this lead us in this algebraic connection that the that the matrix S Again S is 10 to the power of D by enter the power of D, where D is the number of vertices in an edge D equal to give us a graph, right?",
            "Is the Chronicle product D wise of the Matrix X?",
            "So in the case of a graph, this will be X multiplied by itself X.",
            "So X multiplied by itself X if X is an N by N matrix.",
            "This Chronicle Chronicle product will give us an N squared by N square matrix, which is exactly the dimensions of office.",
            "OK, so from this assumption comes out this relationship and it's very simple because.",
            "Because of the conditional independence E here is V1 to VD has so because of conditional independence.",
            "It's a product of each one separately.",
            "It's since it's a product for each one separately you get this product, so it's a very simple relationship."
        ],
        [
            "Now let's take this and see where."
        ],
        [
            "Well, this is just a graphical.",
            "So if S is equal to the product of of X, let's.",
            "Take any ticket, distance function and a later take the.",
            "The KL Divergent as the distance I take a distance function, minimize the distance between S and the product of S. Under what assumptions?",
            "The assumption since X?",
            "Is is the is a probabilistic matching.",
            "It has to be doubly stochastic means if it's a complete matching all the purchases in one set are matched all the vertices and then another in another set.",
            "It has to be doubly stochastic that means the sum of rows and the sum of columns must sum up to one, so negative and the summer frozen summer must sum up to one.",
            "Now since it's not a complete matching, some of the vertices may not be matched to other vertices we have here the inequality.",
            "So the sum of rows is smaller or equal to 1 and the sum of columns is smaller or equal to 1 and it's supposed to be non negative.",
            "You will end up.",
            "It's not many to many because it could be.",
            "When some vertices are unmatched, could be some verticies, no, it just allows you to have some vertices being unmatched.",
            "But the matrix itself gives you a probability for each entry in the matrix.",
            "What is the probability of vertex one vertex vertex too?",
            "So the matching is well defined.",
            "The fact that it's less or equal to 1 allows you to have some verticies with a zero probability.",
            "OK, that's so some entries in X could be 0.",
            "That's all it says.",
            "OK, so here we have an optimization criteria where what we will.",
            "Well, this is also just a note.",
            "Some people have noticed from a you Ristic point of view that if they take the input matrix S, the weight matrix and they find the closest doubly stochastic matrix to it.",
            "Then they get better matching algorithms and if they start with the original matrix.",
            "So what this criteria basically tells you why this is so?",
            "Because if you replace if S is doubly stochastic, then X must satisfy these conditions."
        ],
        [
            "Which is within here, but if X is doubly stochastic then the product is also doubly stochastic.",
            "Therefore, S will be doubly stochastic.",
            "Now if you want to go the opposite way, if you take S and before you start your algorithm, find the closest doubly stochastic matrix to it.",
            "You'll be closer to satisfying the doubly stochastic constraints of the metrics is and this is what people notice from heuristic POV, like Cora tell in 2000."
        ],
        [
            "Anyway, let's go back to.",
            "For the distance function will take the relative entropy, the KL divergent.",
            "Between two sets we have these two terms becausw these are not necessarily probabilities, they don't sum up to sum up to one, so we'll take that the relative entropy measure between the two between the two sets white at the relative entropy, because then we'll get a globally optimal algorithm out of all of this, instead of the L2."
        ],
        [
            "OK, now what we'll do.",
            "We'll take again SSAN by DN BI D matrix.",
            "We'll take this matrix an marginalise it will take some of rows and sum of columns and create a matrix Y which is an N by N matrix and this is the definition going from X to Y.",
            "So it's a marginalization of all the entries in S and then one can show.",
            "I'm not trying to hear that if we're looking at the optimal, the global minimizer of the relative entropy distance between S and the Chronicle product of S, it's equivalent to solving this problem.",
            "Well, now you don't have S, you have Y, so we are now with matrices that are N by N matrices rather than an N by D and end by the matrix.",
            "Can get rid of these two and get a convex problem.",
            "What is 1 transpose X11?",
            "Transpose X One is the number of matches.",
            "We said that some of the points may not be matched might not necessarily be matched.",
            "And here they are.",
            "The number of matches that we have because if some of the points are not matched, the entry in X would be 0.",
            "So one transpose X one will have lesser than N, then N matches, so it."
        ],
        [
            "We defined by K. The number of matches.",
            "And write an algorithm writing optimization as a function of chaos.",
            "Since K is from 1:00 to 1:00 to end, we can solve this problem end times and then choose the global optimum.",
            "So if we look at.",
            "A function.",
            "No, it is the number of matches, right?",
            "You you have end points, so you could have any matches or you can have less than 10 matches.",
            "So K is a number between one 2 and we don't know how many matches are going to be.",
            "So let's solve this end times and then choose the best solution.",
            "Now if we do this we have here a relative entropy which is convex problem and we have linear constraints, some of them in equalities.",
            "Some of them equalities.",
            "Now this problem is convex in K. And we can easily find the global optimizer using a break man Bregman projection."
        ],
        [
            "Algorithm.",
            "Which we can define as following.",
            "We can look at this problem as we would like to find X closes as possible to Y in relative entropy, where X is in the intersection of three convex sets.",
            "The first set is the set of X, non negative and X1 smaller equal to 1.",
            "It's a convex set.",
            "The second convex set is X transpose, one smaller equal to 1 and the third one is the equality one transpose X one is equal to K. Now these kinds of problems we know how to solve them incrementally.",
            "This is the Bregman divergent we can solve this problem if we know how to solve a surrogate problem, which is a simplified one which is minimum of the objective function, subject to only one of the sets with an additional dot product that we have here.",
            "So if we know how to solve the."
        ],
        [
            "This one.",
            "Then this is the Bregman projection.",
            "We start incrementally with a guest on X.",
            "And then we iterate.",
            "We solve the local the circuit problem with our guest plus the gradient of our of our distance function with the previous solution and we iterate this again and again.",
            "Since the problem is convex, it will converge to the optimal solution."
        ],
        [
            "And solving this problem is easy.",
            "I'm not going to show why, but it is.",
            "It is easy."
        ],
        [
            "So altogether we have a simple algorithm to recover X.",
            "Now this simple algorithm in case where the number of matches is equal to N, that means we know that every point in set one is matched to some point in set two.",
            "This problem reduces to the well known sinkhorn algorithm of taking a matrix and by normalizing rows and columns you get a doubly stochastic matrix and that doubly stochastic matrix is the closest under KL divergent and This is why we use the KL divergent distance metric.",
            "In what we have here.",
            "So in case of a complete matching, this algorithm is very simple.",
            "You take your end by DN BI, D matrix or N squared by N square.",
            "If we have a graph matrix, you marginalise it to get this matrix Y which is N by N and then find the closest doubly stochastic matrix under KL divergent which is simply running the sinkhorn algorithm to why.",
            "And this gives you the matrix X.",
            "It's a bit more complicated running this algorithm here when the number of matches is smaller than N. And then you get to the Bregman a diversion, so the sync an algorithm which is very well known since the 50s is a particular case of what we see here.",
            "When the when you have complete matches."
        ],
        [
            "OK, and this is what we have here.",
            "When the hyper graphs are of the same size and all the vertices has to be matched.",
            "Then the algorithm reduces the sinkhorn for the nearest doubly stochastic matrix to a given matrix and that given matrix is the matrix matrix, why?"
        ],
        [
            "OK, one thing that we should do since we are so the algorithm in the matrix.",
            "Why is very simple?",
            "Why is an N by N matrix?",
            "From an N by N matrix we get another N by N matrix which is our output matrix X.",
            "The problem is going from S to YS as an end by D, by N by D matrix.",
            "So just representing it creates an issue.",
            "We don't need to do complicated.",
            "Calculations on it we are just marginalizing it.",
            "But still, it's an end.",
            "By the by and by the metrics.",
            "So what we do when D is high.",
            "We sample it.",
            "So we sample some of the hyper edges for each vertex and we have a way to do it in kind of a clever way.",
            "But basically it's sampling.",
            "Anne.",
            "No, no, we have a sampling that on average with guaranteed that for any two vertices that are going to be matched, you have a reasonable sample of the hyper edges between those those two verticies, but it's not a theoretical guarantee or anything of that sense, so if we if we sample Z hyperedges for vertex, this is the number of this.",
            "Is the complexity of the algae."
        ],
        [
            "Item.",
            "So here if we look at the runtime, this is.",
            "Edge correlation that means we're not calculating the time it takes to build a matrix is only given the metrics S how much time it takes to run the algorithm, and we're comparing this to, say, spectral matching algorithm, and you can see here.",
            "This is the runtime.",
            "This is our algorithm, which is basically many orders of magnitude less in terms of runtime than any spectral algorithm, because again, the algorithm doesn't do much.",
            "It takes the matrix S marginalized, it gets the matrix Y.",
            "Once you have why, it's an iterative algorithm of normalization, that's it.",
            "And here we're using sampling, so if we if we are using samples, the more samplings.",
            "This is the number of hyper edges per vertex that we have.",
            "So when you have 200 and this is a 50 point set.",
            "Then see the runtime goes here to 7 seconds and so forth."
        ],
        [
            "If we take some synthetic experiments, if we take.",
            "Like a set of 25 points, we duplicated and we do a rigid transformation and then add noise to the rigid transformation.",
            "And then we add additional random points because it's not a complete set.",
            "Could be less than an matches that we compare the performance in terms of the number of matches that are correct.",
            "Compared to a spectral matching, again it's much much higher in terms of performance, and if we add random points to both graphs, not only to one of the one of the graphs, the more random points we add, the performance starts to drop, but still much, much higher than any spectral method."
        ],
        [
            "That we."
        ],
        [
            "Can do here."
        ],
        [
            "I'll jump here too.",
            "Sticky matching would like to find matches between these two sets where one of the sets undergoes a transformation.",
            "So we look for points in each set."
        ],
        [
            "OK, so we took it again.",
            "It's part of synthetic examples.",
            "We took the image and we created an app, find distortion of the image and then we did a spectral matching.",
            "We get 10 out of the 3033 points are mismatches.",
            "If we use this algorithm that I described, you have no mismatches, so."
        ],
        [
            "Very robust.",
            "Now if we take tracking sequences.",
            "So here we have a number of frames.",
            "Overall it's a nonrigid transformation that the person is talking and moving the lips and so forth and in terms of matches we get very accurate matches all along."
        ],
        [
            "The sequence and the same here.",
            "The same here from other sequences you get very very good matches.",
            "Much, much better by order of magnitude than any standard conventional method, which are also more complicated."
        ],
        [
            "In terms of implementation.",
            "So we take an image.",
            "We look for Sift feature points using difference of Gaussians for the interest points and those are the points that we are using and then we're using D equal to four.",
            "So we're looking for local affine invariants, so we have hyper edges of degree 4, and that allows for local affine transformations.",
            "And This is why you'll get a much better performance than a spectral matching, which looks only at distances, that's the.",
            "That's the key to go to a hypergraph.",
            "OK, so the structure translates to hypergraphs.",
            "Again, a hypergraph will allow one to put arbitrary complex relationships among couples of points.",
            "The only point is how to then find the matching because it's an NP hard problem.",
            "A probabilistic interpretation with a suspicious assumption leads to a very simple algebraic relationship.",
            "Once we have this algebraic relationship, and if we put it under the KL divergent, we get a very simple optimization criteria that one can find the global optimal of that optimization criteria.",
            "And in particular cases, one gets known you risztics will stick that people have noticed in the past and also known algorithms like the sink on algorithms when you have a complete matching.",
            "So it's globally optimal for the soft matching, and it's very efficient as you saw.",
            "And then if you want to hard matching, you do the linear assignment problem.",
            "So this is it.",
            "Question.",
            "How do you control the smoothness?",
            "You don't control the smoothness.",
            "The only constraint is that the sum of the rows and columns must be bounded by by 1.",
            "So you have a matrix where each entry in the matrix is your probability of matching the row and column indices of that matrix, and there isn't room for smoothness because originally you got simply a collection of data points in one graph is a collection of data points in another graph.",
            "Nobody gave you smoothness information to enforce them.",
            "So it does not mean that if it point point index I is matched by pointing index J, the point index type plus one should be matched to a point near the index J.",
            "There's no such information in the problem itself.",
            "So there's no smoothness to be enforced here.",
            "Successful.",
            "Well, they are simple cases where this assumption is correct.",
            "But in many complicated and challenging cases, this assumption is not correct.",
            "I think what it says is that in general the problem, since the problem is NP hard, you cannot find a rule that would be simple and find a simple algorithm.",
            "So the approximate algorithm comes out from a very approximate assumption about how the world behaves and the world behaves in a conditionally independent manner.",
            "So so instead of saying OK, this is a very complicated algorithm.",
            "Let's relax it.",
            "To a much simpler algorithm which we can find a solution.",
            "We come and say, OK, this is a very complicated relationship between the point matches.",
            "Let's make a very simple assumption on how this behaves, not on the algorithm on how the probability behaves.",
            "And then you get a simple algorithm, so it's the other way around, starting instead of starting from complicated algorithm and relaxing it, we start with the real world behavior of the probabilities and make a strong assumption on how those probabilities behave and the assumption is the conditional independence, and from there the algorithm emerges in a simple manner.",
            "Features.",
            "Yeah, so the idea of not complete matches is exactly to handle the issue of occlusions wherein tracking problems.",
            "You have points that are not matched to any other points because of occlusions.",
            "So the key of all this work is to handle the case where you don't have matching points.",
            "Well, I showed in the tracking in the tracking there were many occlusions, but I showed only the end image of the tracking.",
            "But along the tracking I should have showed a video of it, but along the tracking that many occlusions going on point that there cannot be matched to other points.",
            "I'm actually interpret your assumption is a mean field approximation, because that's usually what you do.",
            "When you do mean field that you assume that things are statistically well.",
            "You know you usually use mean field when describing things as a graphical model, right?",
            "So you have.",
            "You have a joint probability and you want to do inference and then you apply them in fields on the way.",
            "The joint probability behaves.",
            "Here we make it very simple assumption.",
            "Yet it's conditionally independent, and from there comes algebraic relationship.",
            "I think it's even simpler than a mean field assumption.",
            "It's I think it's the most basic basic assumption in terms of this simplicity level that one can make think mean field is 1 level up in terms of complexity.",
            "?",
            "Generalizing this framework, when the hyper reps have different number of life, not necessarily deep to have an average of seven more general now we haven't yet worked on that.",
            "In principle, yes.",
            "The problem is that now you cannot write it nicely as a matrix, right as an end by D. But in the end by the Matrix.",
            "But the algebraic relationship should be there, but now it's not a matrix and Chronicle product and could be messy, that's the.",
            "Question.",
            "The overlaps between two hypernova.",
            "They share node and so you can have like net local features with someone overlap and then you can maybe get a horse.",
            "You can, you can share.",
            "That means you can have two hyperedges which share some of the nodes right in hyperedge, say of four nodes could have three nodes.",
            "They share three nodes, and the 4th node, so it's in the definition of a hyperedge.",
            "Nobody made the assumption that they need to be mutually exclusive.",
            "Very high dependency.",
            "Yeah and then yeah, but it still works.",
            "Structure.",
            "Rigid transformation.",
            "Very nicely.",
            "Lionel Richie.",
            "No.",
            "Well we haven't, and the reason that we haven't looked at it is that.",
            "Some years ago we had an algorithm.",
            "Nice algorithm for feature selection and then said OK, we have an ice hammer.",
            "Let's look for nails for this and one of the things we went to computational biology and there I find out that working in computational biology it's not enough to have an algorithm you really need to understand the biology.",
            "You really need to understand the data, how you massage the data and so forth.",
            "So we end up doing it.",
            "We found a collaborator who really understands the the biology and it's one of those papers that I spend much more time than I wanted to spend on.",
            "And since then I stay away from computational biology.",
            "Is either either make the effort and understand the biology, or don't get into it, OK?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll be talking about the similarity.",
                    "label": 0
                },
                {
                    "sent": "And non geometric similarity.",
                    "label": 0
                },
                {
                    "sent": "Embedded into a graph matching problem.",
                    "label": 0
                },
                {
                    "sent": "So we have two 2 point sets.",
                    "label": 0
                },
                {
                    "sent": "The points are points, sets are undergoing nonrigid transformation.",
                    "label": 0
                },
                {
                    "sent": "Some general nonrigid transformation.",
                    "label": 0
                },
                {
                    "sent": "The matching that we would like to do is not complete.",
                    "label": 0
                },
                {
                    "sent": "There could be some points in one set that are not matched to another two other points in another set.",
                    "label": 0
                },
                {
                    "sent": "And I'll present to you Ristic very simple, but very powerful heuristic that we presented two years ago in CPR, and since then created a lot of follow up work.",
                    "label": 0
                },
                {
                    "sent": "It's very simple that it involves a suspicious assumption which when we get there, I'll show it to you, but if you ignore the correctness of the assumption you get out a very nice algorithm and works.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very nicely so we have.",
                    "label": 0
                },
                {
                    "sent": "In computer vision, you know we take images.",
                    "label": 0
                },
                {
                    "sent": "We find the matching.",
                    "label": 0
                },
                {
                    "sent": "We find points and then would like to match points between 2 images or track points.",
                    "label": 0
                },
                {
                    "sent": "And in many cases the transfer.",
                    "label": 0
                },
                {
                    "sent": "It's not an isometry, right?",
                    "label": 0
                },
                {
                    "sent": "It's not a rigid transformation, translation and rotation that describes how the point matches behave, but it's some non rigid transformation as we can see here, the leg is in different position and so forth.",
                    "label": 0
                },
                {
                    "sent": "But we could have some local local structure.",
                    "label": 1
                },
                {
                    "sent": "So globally the point sets.",
                    "label": 0
                },
                {
                    "sent": "Would not behave under some global nice transformation, but locali things could behave.",
                    "label": 1
                },
                {
                    "sent": "Things would behave well like local affine transformations.",
                    "label": 0
                },
                {
                    "sent": "Now the idea of putting it into a graph structure.",
                    "label": 0
                },
                {
                    "sent": "Allows us to so.",
                    "label": 0
                },
                {
                    "sent": "We have edges or hyperedges representing relationships between points in each set, and we're presenting this as in height as a hypergraph allows us to put arbitrary complex relationships among points in each in each set.",
                    "label": 0
                },
                {
                    "sent": "So we have an arbitrary relationship represented by hyper edges in the graph, and then we would like to match these hyperedges in between.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two in between the two hyper graphs.",
                    "label": 0
                },
                {
                    "sent": "So we have a collection of points in one area, so this is 1 graph, a collection of points in the other set, which is another graph and would like to match these two.",
                    "label": 0
                },
                {
                    "sent": "But instead of describing relationships only between pairwise pair pairs of points, a hypergraph allows us to establish relationships among triplets among N couples of points.",
                    "label": 0
                },
                {
                    "sent": "So we still have the number of vertices remains the same, but the edges now represent complex relationships among multiple points, and each tuple is represented by by hyper edge, and on that hyperedge we put in number, which would be an invariant, which I'll show.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a moment.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we would like to.",
                    "label": 0
                },
                {
                    "sent": "This establish a fun invariant properties.",
                    "label": 0
                },
                {
                    "sent": "Then we will need each hyperedge will need to work on four points, so with four points, or each invariant if you have an invite of endpoints you'll need N + 1 points in order to describe that invariant.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have local IAFF and invariants and we have a representation of four points as a hyperedge, then we can calculate the ratio of the areas and we get enough fine invariant if indeed there is enough.",
                    "label": 0
                },
                {
                    "sent": "Local ER fine invariant.",
                    "label": 0
                },
                {
                    "sent": "So if we have a collection of points, and we describe hyperedges among four couples of points, and if there is a local invariants in the matching local, often inventing the magic between the two sets, the numbers on the hyper edges would match completely accurately between the two between the datasets.",
                    "label": 0
                },
                {
                    "sent": "The problem is matching hypergraphs is an NP hard problem, so even though you can describe it very nicely, finding a solution.",
                    "label": 0
                },
                {
                    "sent": "Would be very would be very difficult.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way we're approaching it is we're looking at at the probabilistic way of defining the matching, so it's an inexact graph matching.",
                    "label": 1
                },
                {
                    "sent": "The conventional way is to describe a hard matching problem and then to relax it.",
                    "label": 0
                },
                {
                    "sent": "We are going the other way around.",
                    "label": 0
                },
                {
                    "sent": "We are defining a soft matching criteria.",
                    "label": 0
                },
                {
                    "sent": "We're going to find the global optimum of the soft matching criteria.",
                    "label": 1
                },
                {
                    "sent": "Once we have a soft matching, we can then go to a hard matching using the linear assignment problem.",
                    "label": 0
                },
                {
                    "sent": "Overall, it will not give us the optimal solution because there could be another route not going through a soft matching which would give us a better solution.",
                    "label": 0
                },
                {
                    "sent": "But what will show that the soft matching idea that we're putting here is a very simple idea with a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "That algorithm describes and in particular cases.",
                    "label": 0
                },
                {
                    "sent": "For example, when you have complete matches when you have.",
                    "label": 0
                },
                {
                    "sent": "The entire set of points in one set is matched to the entire set of points in the other set.",
                    "label": 0
                },
                {
                    "sent": "There are no free points in the set.",
                    "label": 0
                },
                {
                    "sent": "You get known existing algorithms.",
                    "label": 0
                },
                {
                    "sent": "Also, there are certain heuristics that people applied in normalizing the measurement matrices, which come out very naturally in what will be doing so.",
                    "label": 0
                },
                {
                    "sent": "We're going through a soft match.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pink soft matching idea.",
                    "label": 0
                },
                {
                    "sent": "If we have a software solution, which is a probabilistic assignment between vertices between the two between the two sets, one can then go to a hard matching using a linear assignment problem.",
                    "label": 1
                },
                {
                    "sent": "But what we also note that in many application domains, your goal is really the soft assignment.",
                    "label": 0
                },
                {
                    "sent": "For example, in in tracking problems where you have multiple frames and you want to track points along multiple frames, you wouldn't necessarily want that in between each two frames you have a hard assignment you would like to keep the ambiguities until the end of the process, and then make a hard decision.",
                    "label": 0
                },
                {
                    "sent": "So in a tracking process you would like to have a soft assignment or a probabilistic assignment of points along the sequence.",
                    "label": 0
                },
                {
                    "sent": "And then later when you have more than three 4 frames to make a hard decision.",
                    "label": 0
                },
                {
                    "sent": "So sometimes soft assignment is not a byproduct, it's really the product of what you what you are looking for.",
                    "label": 0
                },
                {
                    "sent": "But in any case, if you are looking for a hard assignment then what we claim go for a soft assignment and then do a linear assignment algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the hypergraph matching problem, the way we describe it, so we have two hyper graphs of degree D. That means each hyperedge is defined on D points.",
                    "label": 1
                },
                {
                    "sent": "So when D = 2, you have a graph.",
                    "label": 0
                },
                {
                    "sent": "The set of vertices is as normally defined in graph, so we have a set of points which compared the set of vertices and the set of hyper edges are defined between the temples of points.",
                    "label": 0
                },
                {
                    "sent": "What we are interested is the matching function between the set of points in the first graph to the set of points in the second graph, or this matching is not necessarily complete.",
                    "label": 0
                },
                {
                    "sent": "There could be points in the first set of parts that are not matched to the second and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Now, once we have a matching between, if we have established the matching between points, then clearly we have also established the matching between the hyperedges.",
                    "label": 0
                },
                {
                    "sent": "Because if V1 to VDR, the couple of a hyperedge, then the matching hyper air just simply the mapping of the one and the map at Milda mapping of VD.",
                    "label": 0
                },
                {
                    "sent": "So once we establish the matching between the points, clearly the matching between the edges naturally.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is well defined.",
                    "label": 0
                },
                {
                    "sent": "So what is our input the input?",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm defining now.",
                    "label": 0
                },
                {
                    "sent": "So what is our input?",
                    "label": 0
                },
                {
                    "sent": "The inputs are weights on the hyper edges, so the way we define it is that the probability that an edge.",
                    "label": 1
                },
                {
                    "sent": "From the first graph, matches an edge E prime in the second.",
                    "label": 0
                },
                {
                    "sent": "In the second graph, and this would be written here as a matrix.",
                    "label": 0
                },
                {
                    "sent": "Between Edge E and edge E Prime, which is the probability that the matching of E gives us the prime and the matching of E is defined.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said in the previous slide, if we have a matching function between the vertice is, it defines the matching between between edges.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, in in case where we have a graph that couples are pairwise, this is a matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is a well, it's it's a matrix where it's have N squared by N square, where N is the number of vertice is.",
                    "label": 0
                },
                {
                    "sent": "Now what we are interested as the output is the probability that two verticies match, so let's call that the matrix X between vertex V and vertex V prime, which the probability that the matching of V gives us a week right?",
                    "label": 0
                },
                {
                    "sent": "So the output is always an N by N matrix, the input.",
                    "label": 0
                },
                {
                    "sent": "Is an end to the power of D to the end to the power of the matrix, where D is the size of the couples that we have.",
                    "label": 0
                },
                {
                    "sent": "But will show will define a simple algebraic connection between S and and X.",
                    "label": 1
                },
                {
                    "sent": "Once we define this connection will have a very simple optimization algorithm that will recover X from from S.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, a simple definition of what is a Chronicle product between two matrices.",
                    "label": 1
                },
                {
                    "sent": "It's very simple if you have a metrics be you have copies of the matrix B multiplied by the entries of a.",
                    "label": 0
                },
                {
                    "sent": "Let's say a simple definition and then you can apply this definition incrementally.",
                    "label": 0
                },
                {
                    "sent": "If you have more than two elements in the product, you do that pairwise and you simply concatenate them.",
                    "label": 0
                },
                {
                    "sent": "So that will be a.",
                    "label": 0
                },
                {
                    "sent": "If we have D matrices A1 to lady, the product of all of them together is defined.",
                    "label": 0
                },
                {
                    "sent": "This way, we're then you can go back to this definition for every pair.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is this suspicious assumption.",
                    "label": 0
                },
                {
                    "sent": "I'm going to make an assumption that given the graphs, the decision the mapping of V1.",
                    "label": 0
                },
                {
                    "sent": "Is independent, conditionally independent the mapping of V2?",
                    "label": 0
                },
                {
                    "sent": "Now I'm not going to spend time on convincing you that this assumption is correct, because this assumption is normally not correct.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                },
                {
                    "sent": "For every one V2.",
                    "label": 0
                },
                {
                    "sent": "So this is making assumption of conditional independence that if when you observe the graphs and you make a decision on the mapping of V1 and then you observe the graph again and make a mapping of V2, these two are independent and I'm not going to defend it because normally it's not correct.",
                    "label": 0
                },
                {
                    "sent": "But let's make this assumption and see where this assumption leads us because it's going to lead us to known algorithms in particular cases and to new algorithm in the general case where the matching is not.",
                    "label": 0
                },
                {
                    "sent": "Is not complete.",
                    "label": 0
                },
                {
                    "sent": "So where does this lead us in this algebraic connection that the that the matrix S Again S is 10 to the power of D by enter the power of D, where D is the number of vertices in an edge D equal to give us a graph, right?",
                    "label": 0
                },
                {
                    "sent": "Is the Chronicle product D wise of the Matrix X?",
                    "label": 0
                },
                {
                    "sent": "So in the case of a graph, this will be X multiplied by itself X.",
                    "label": 0
                },
                {
                    "sent": "So X multiplied by itself X if X is an N by N matrix.",
                    "label": 0
                },
                {
                    "sent": "This Chronicle Chronicle product will give us an N squared by N square matrix, which is exactly the dimensions of office.",
                    "label": 0
                },
                {
                    "sent": "OK, so from this assumption comes out this relationship and it's very simple because.",
                    "label": 0
                },
                {
                    "sent": "Because of the conditional independence E here is V1 to VD has so because of conditional independence.",
                    "label": 0
                },
                {
                    "sent": "It's a product of each one separately.",
                    "label": 0
                },
                {
                    "sent": "It's since it's a product for each one separately you get this product, so it's a very simple relationship.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's take this and see where.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, this is just a graphical.",
                    "label": 0
                },
                {
                    "sent": "So if S is equal to the product of of X, let's.",
                    "label": 0
                },
                {
                    "sent": "Take any ticket, distance function and a later take the.",
                    "label": 0
                },
                {
                    "sent": "The KL Divergent as the distance I take a distance function, minimize the distance between S and the product of S. Under what assumptions?",
                    "label": 0
                },
                {
                    "sent": "The assumption since X?",
                    "label": 0
                },
                {
                    "sent": "Is is the is a probabilistic matching.",
                    "label": 0
                },
                {
                    "sent": "It has to be doubly stochastic means if it's a complete matching all the purchases in one set are matched all the vertices and then another in another set.",
                    "label": 0
                },
                {
                    "sent": "It has to be doubly stochastic that means the sum of rows and the sum of columns must sum up to one, so negative and the summer frozen summer must sum up to one.",
                    "label": 0
                },
                {
                    "sent": "Now since it's not a complete matching, some of the vertices may not be matched to other vertices we have here the inequality.",
                    "label": 0
                },
                {
                    "sent": "So the sum of rows is smaller or equal to 1 and the sum of columns is smaller or equal to 1 and it's supposed to be non negative.",
                    "label": 0
                },
                {
                    "sent": "You will end up.",
                    "label": 0
                },
                {
                    "sent": "It's not many to many because it could be.",
                    "label": 0
                },
                {
                    "sent": "When some vertices are unmatched, could be some verticies, no, it just allows you to have some vertices being unmatched.",
                    "label": 0
                },
                {
                    "sent": "But the matrix itself gives you a probability for each entry in the matrix.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of vertex one vertex vertex too?",
                    "label": 0
                },
                {
                    "sent": "So the matching is well defined.",
                    "label": 0
                },
                {
                    "sent": "The fact that it's less or equal to 1 allows you to have some verticies with a zero probability.",
                    "label": 0
                },
                {
                    "sent": "OK, that's so some entries in X could be 0.",
                    "label": 0
                },
                {
                    "sent": "That's all it says.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we have an optimization criteria where what we will.",
                    "label": 0
                },
                {
                    "sent": "Well, this is also just a note.",
                    "label": 0
                },
                {
                    "sent": "Some people have noticed from a you Ristic point of view that if they take the input matrix S, the weight matrix and they find the closest doubly stochastic matrix to it.",
                    "label": 0
                },
                {
                    "sent": "Then they get better matching algorithms and if they start with the original matrix.",
                    "label": 0
                },
                {
                    "sent": "So what this criteria basically tells you why this is so?",
                    "label": 0
                },
                {
                    "sent": "Because if you replace if S is doubly stochastic, then X must satisfy these conditions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is within here, but if X is doubly stochastic then the product is also doubly stochastic.",
                    "label": 1
                },
                {
                    "sent": "Therefore, S will be doubly stochastic.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to go the opposite way, if you take S and before you start your algorithm, find the closest doubly stochastic matrix to it.",
                    "label": 0
                },
                {
                    "sent": "You'll be closer to satisfying the doubly stochastic constraints of the metrics is and this is what people notice from heuristic POV, like Cora tell in 2000.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, let's go back to.",
                    "label": 0
                },
                {
                    "sent": "For the distance function will take the relative entropy, the KL divergent.",
                    "label": 1
                },
                {
                    "sent": "Between two sets we have these two terms becausw these are not necessarily probabilities, they don't sum up to sum up to one, so we'll take that the relative entropy measure between the two between the two sets white at the relative entropy, because then we'll get a globally optimal algorithm out of all of this, instead of the L2.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now what we'll do.",
                    "label": 0
                },
                {
                    "sent": "We'll take again SSAN by DN BI D matrix.",
                    "label": 0
                },
                {
                    "sent": "We'll take this matrix an marginalise it will take some of rows and sum of columns and create a matrix Y which is an N by N matrix and this is the definition going from X to Y.",
                    "label": 0
                },
                {
                    "sent": "So it's a marginalization of all the entries in S and then one can show.",
                    "label": 0
                },
                {
                    "sent": "I'm not trying to hear that if we're looking at the optimal, the global minimizer of the relative entropy distance between S and the Chronicle product of S, it's equivalent to solving this problem.",
                    "label": 0
                },
                {
                    "sent": "Well, now you don't have S, you have Y, so we are now with matrices that are N by N matrices rather than an N by D and end by the matrix.",
                    "label": 0
                },
                {
                    "sent": "Can get rid of these two and get a convex problem.",
                    "label": 0
                },
                {
                    "sent": "What is 1 transpose X11?",
                    "label": 0
                },
                {
                    "sent": "Transpose X One is the number of matches.",
                    "label": 0
                },
                {
                    "sent": "We said that some of the points may not be matched might not necessarily be matched.",
                    "label": 0
                },
                {
                    "sent": "And here they are.",
                    "label": 0
                },
                {
                    "sent": "The number of matches that we have because if some of the points are not matched, the entry in X would be 0.",
                    "label": 0
                },
                {
                    "sent": "So one transpose X one will have lesser than N, then N matches, so it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We defined by K. The number of matches.",
                    "label": 1
                },
                {
                    "sent": "And write an algorithm writing optimization as a function of chaos.",
                    "label": 0
                },
                {
                    "sent": "Since K is from 1:00 to 1:00 to end, we can solve this problem end times and then choose the global optimum.",
                    "label": 0
                },
                {
                    "sent": "So if we look at.",
                    "label": 0
                },
                {
                    "sent": "A function.",
                    "label": 0
                },
                {
                    "sent": "No, it is the number of matches, right?",
                    "label": 0
                },
                {
                    "sent": "You you have end points, so you could have any matches or you can have less than 10 matches.",
                    "label": 0
                },
                {
                    "sent": "So K is a number between one 2 and we don't know how many matches are going to be.",
                    "label": 0
                },
                {
                    "sent": "So let's solve this end times and then choose the best solution.",
                    "label": 0
                },
                {
                    "sent": "Now if we do this we have here a relative entropy which is convex problem and we have linear constraints, some of them in equalities.",
                    "label": 0
                },
                {
                    "sent": "Some of them equalities.",
                    "label": 0
                },
                {
                    "sent": "Now this problem is convex in K. And we can easily find the global optimizer using a break man Bregman projection.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which we can define as following.",
                    "label": 0
                },
                {
                    "sent": "We can look at this problem as we would like to find X closes as possible to Y in relative entropy, where X is in the intersection of three convex sets.",
                    "label": 0
                },
                {
                    "sent": "The first set is the set of X, non negative and X1 smaller equal to 1.",
                    "label": 0
                },
                {
                    "sent": "It's a convex set.",
                    "label": 0
                },
                {
                    "sent": "The second convex set is X transpose, one smaller equal to 1 and the third one is the equality one transpose X one is equal to K. Now these kinds of problems we know how to solve them incrementally.",
                    "label": 0
                },
                {
                    "sent": "This is the Bregman divergent we can solve this problem if we know how to solve a surrogate problem, which is a simplified one which is minimum of the objective function, subject to only one of the sets with an additional dot product that we have here.",
                    "label": 0
                },
                {
                    "sent": "So if we know how to solve the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Then this is the Bregman projection.",
                    "label": 0
                },
                {
                    "sent": "We start incrementally with a guest on X.",
                    "label": 0
                },
                {
                    "sent": "And then we iterate.",
                    "label": 0
                },
                {
                    "sent": "We solve the local the circuit problem with our guest plus the gradient of our of our distance function with the previous solution and we iterate this again and again.",
                    "label": 0
                },
                {
                    "sent": "Since the problem is convex, it will converge to the optimal solution.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And solving this problem is easy.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to show why, but it is.",
                    "label": 0
                },
                {
                    "sent": "It is easy.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So altogether we have a simple algorithm to recover X.",
                    "label": 0
                },
                {
                    "sent": "Now this simple algorithm in case where the number of matches is equal to N, that means we know that every point in set one is matched to some point in set two.",
                    "label": 0
                },
                {
                    "sent": "This problem reduces to the well known sinkhorn algorithm of taking a matrix and by normalizing rows and columns you get a doubly stochastic matrix and that doubly stochastic matrix is the closest under KL divergent and This is why we use the KL divergent distance metric.",
                    "label": 0
                },
                {
                    "sent": "In what we have here.",
                    "label": 0
                },
                {
                    "sent": "So in case of a complete matching, this algorithm is very simple.",
                    "label": 0
                },
                {
                    "sent": "You take your end by DN BI, D matrix or N squared by N square.",
                    "label": 0
                },
                {
                    "sent": "If we have a graph matrix, you marginalise it to get this matrix Y which is N by N and then find the closest doubly stochastic matrix under KL divergent which is simply running the sinkhorn algorithm to why.",
                    "label": 0
                },
                {
                    "sent": "And this gives you the matrix X.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more complicated running this algorithm here when the number of matches is smaller than N. And then you get to the Bregman a diversion, so the sync an algorithm which is very well known since the 50s is a particular case of what we see here.",
                    "label": 0
                },
                {
                    "sent": "When the when you have complete matches.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and this is what we have here.",
                    "label": 0
                },
                {
                    "sent": "When the hyper graphs are of the same size and all the vertices has to be matched.",
                    "label": 1
                },
                {
                    "sent": "Then the algorithm reduces the sinkhorn for the nearest doubly stochastic matrix to a given matrix and that given matrix is the matrix matrix, why?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one thing that we should do since we are so the algorithm in the matrix.",
                    "label": 0
                },
                {
                    "sent": "Why is very simple?",
                    "label": 0
                },
                {
                    "sent": "Why is an N by N matrix?",
                    "label": 0
                },
                {
                    "sent": "From an N by N matrix we get another N by N matrix which is our output matrix X.",
                    "label": 0
                },
                {
                    "sent": "The problem is going from S to YS as an end by D, by N by D matrix.",
                    "label": 1
                },
                {
                    "sent": "So just representing it creates an issue.",
                    "label": 0
                },
                {
                    "sent": "We don't need to do complicated.",
                    "label": 0
                },
                {
                    "sent": "Calculations on it we are just marginalizing it.",
                    "label": 0
                },
                {
                    "sent": "But still, it's an end.",
                    "label": 0
                },
                {
                    "sent": "By the by and by the metrics.",
                    "label": 0
                },
                {
                    "sent": "So what we do when D is high.",
                    "label": 0
                },
                {
                    "sent": "We sample it.",
                    "label": 0
                },
                {
                    "sent": "So we sample some of the hyper edges for each vertex and we have a way to do it in kind of a clever way.",
                    "label": 1
                },
                {
                    "sent": "But basically it's sampling.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "No, no, we have a sampling that on average with guaranteed that for any two vertices that are going to be matched, you have a reasonable sample of the hyper edges between those those two verticies, but it's not a theoretical guarantee or anything of that sense, so if we if we sample Z hyperedges for vertex, this is the number of this.",
                    "label": 0
                },
                {
                    "sent": "Is the complexity of the algae.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Item.",
                    "label": 0
                },
                {
                    "sent": "So here if we look at the runtime, this is.",
                    "label": 0
                },
                {
                    "sent": "Edge correlation that means we're not calculating the time it takes to build a matrix is only given the metrics S how much time it takes to run the algorithm, and we're comparing this to, say, spectral matching algorithm, and you can see here.",
                    "label": 0
                },
                {
                    "sent": "This is the runtime.",
                    "label": 0
                },
                {
                    "sent": "This is our algorithm, which is basically many orders of magnitude less in terms of runtime than any spectral algorithm, because again, the algorithm doesn't do much.",
                    "label": 0
                },
                {
                    "sent": "It takes the matrix S marginalized, it gets the matrix Y.",
                    "label": 0
                },
                {
                    "sent": "Once you have why, it's an iterative algorithm of normalization, that's it.",
                    "label": 0
                },
                {
                    "sent": "And here we're using sampling, so if we if we are using samples, the more samplings.",
                    "label": 0
                },
                {
                    "sent": "This is the number of hyper edges per vertex that we have.",
                    "label": 1
                },
                {
                    "sent": "So when you have 200 and this is a 50 point set.",
                    "label": 0
                },
                {
                    "sent": "Then see the runtime goes here to 7 seconds and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we take some synthetic experiments, if we take.",
                    "label": 0
                },
                {
                    "sent": "Like a set of 25 points, we duplicated and we do a rigid transformation and then add noise to the rigid transformation.",
                    "label": 1
                },
                {
                    "sent": "And then we add additional random points because it's not a complete set.",
                    "label": 0
                },
                {
                    "sent": "Could be less than an matches that we compare the performance in terms of the number of matches that are correct.",
                    "label": 0
                },
                {
                    "sent": "Compared to a spectral matching, again it's much much higher in terms of performance, and if we add random points to both graphs, not only to one of the one of the graphs, the more random points we add, the performance starts to drop, but still much, much higher than any spectral method.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can do here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll jump here too.",
                    "label": 0
                },
                {
                    "sent": "Sticky matching would like to find matches between these two sets where one of the sets undergoes a transformation.",
                    "label": 0
                },
                {
                    "sent": "So we look for points in each set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we took it again.",
                    "label": 0
                },
                {
                    "sent": "It's part of synthetic examples.",
                    "label": 0
                },
                {
                    "sent": "We took the image and we created an app, find distortion of the image and then we did a spectral matching.",
                    "label": 0
                },
                {
                    "sent": "We get 10 out of the 3033 points are mismatches.",
                    "label": 0
                },
                {
                    "sent": "If we use this algorithm that I described, you have no mismatches, so.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very robust.",
                    "label": 0
                },
                {
                    "sent": "Now if we take tracking sequences.",
                    "label": 0
                },
                {
                    "sent": "So here we have a number of frames.",
                    "label": 0
                },
                {
                    "sent": "Overall it's a nonrigid transformation that the person is talking and moving the lips and so forth and in terms of matches we get very accurate matches all along.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sequence and the same here.",
                    "label": 0
                },
                {
                    "sent": "The same here from other sequences you get very very good matches.",
                    "label": 0
                },
                {
                    "sent": "Much, much better by order of magnitude than any standard conventional method, which are also more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of implementation.",
                    "label": 0
                },
                {
                    "sent": "So we take an image.",
                    "label": 0
                },
                {
                    "sent": "We look for Sift feature points using difference of Gaussians for the interest points and those are the points that we are using and then we're using D equal to four.",
                    "label": 0
                },
                {
                    "sent": "So we're looking for local affine invariants, so we have hyper edges of degree 4, and that allows for local affine transformations.",
                    "label": 0
                },
                {
                    "sent": "And This is why you'll get a much better performance than a spectral matching, which looks only at distances, that's the.",
                    "label": 0
                },
                {
                    "sent": "That's the key to go to a hypergraph.",
                    "label": 0
                },
                {
                    "sent": "OK, so the structure translates to hypergraphs.",
                    "label": 1
                },
                {
                    "sent": "Again, a hypergraph will allow one to put arbitrary complex relationships among couples of points.",
                    "label": 0
                },
                {
                    "sent": "The only point is how to then find the matching because it's an NP hard problem.",
                    "label": 1
                },
                {
                    "sent": "A probabilistic interpretation with a suspicious assumption leads to a very simple algebraic relationship.",
                    "label": 0
                },
                {
                    "sent": "Once we have this algebraic relationship, and if we put it under the KL divergent, we get a very simple optimization criteria that one can find the global optimal of that optimization criteria.",
                    "label": 0
                },
                {
                    "sent": "And in particular cases, one gets known you risztics will stick that people have noticed in the past and also known algorithms like the sink on algorithms when you have a complete matching.",
                    "label": 0
                },
                {
                    "sent": "So it's globally optimal for the soft matching, and it's very efficient as you saw.",
                    "label": 0
                },
                {
                    "sent": "And then if you want to hard matching, you do the linear assignment problem.",
                    "label": 0
                },
                {
                    "sent": "So this is it.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "How do you control the smoothness?",
                    "label": 0
                },
                {
                    "sent": "You don't control the smoothness.",
                    "label": 0
                },
                {
                    "sent": "The only constraint is that the sum of the rows and columns must be bounded by by 1.",
                    "label": 0
                },
                {
                    "sent": "So you have a matrix where each entry in the matrix is your probability of matching the row and column indices of that matrix, and there isn't room for smoothness because originally you got simply a collection of data points in one graph is a collection of data points in another graph.",
                    "label": 0
                },
                {
                    "sent": "Nobody gave you smoothness information to enforce them.",
                    "label": 0
                },
                {
                    "sent": "So it does not mean that if it point point index I is matched by pointing index J, the point index type plus one should be matched to a point near the index J.",
                    "label": 0
                },
                {
                    "sent": "There's no such information in the problem itself.",
                    "label": 0
                },
                {
                    "sent": "So there's no smoothness to be enforced here.",
                    "label": 0
                },
                {
                    "sent": "Successful.",
                    "label": 0
                },
                {
                    "sent": "Well, they are simple cases where this assumption is correct.",
                    "label": 0
                },
                {
                    "sent": "But in many complicated and challenging cases, this assumption is not correct.",
                    "label": 0
                },
                {
                    "sent": "I think what it says is that in general the problem, since the problem is NP hard, you cannot find a rule that would be simple and find a simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the approximate algorithm comes out from a very approximate assumption about how the world behaves and the world behaves in a conditionally independent manner.",
                    "label": 0
                },
                {
                    "sent": "So so instead of saying OK, this is a very complicated algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let's relax it.",
                    "label": 0
                },
                {
                    "sent": "To a much simpler algorithm which we can find a solution.",
                    "label": 0
                },
                {
                    "sent": "We come and say, OK, this is a very complicated relationship between the point matches.",
                    "label": 0
                },
                {
                    "sent": "Let's make a very simple assumption on how this behaves, not on the algorithm on how the probability behaves.",
                    "label": 0
                },
                {
                    "sent": "And then you get a simple algorithm, so it's the other way around, starting instead of starting from complicated algorithm and relaxing it, we start with the real world behavior of the probabilities and make a strong assumption on how those probabilities behave and the assumption is the conditional independence, and from there the algorithm emerges in a simple manner.",
                    "label": 0
                },
                {
                    "sent": "Features.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the idea of not complete matches is exactly to handle the issue of occlusions wherein tracking problems.",
                    "label": 0
                },
                {
                    "sent": "You have points that are not matched to any other points because of occlusions.",
                    "label": 0
                },
                {
                    "sent": "So the key of all this work is to handle the case where you don't have matching points.",
                    "label": 0
                },
                {
                    "sent": "Well, I showed in the tracking in the tracking there were many occlusions, but I showed only the end image of the tracking.",
                    "label": 0
                },
                {
                    "sent": "But along the tracking I should have showed a video of it, but along the tracking that many occlusions going on point that there cannot be matched to other points.",
                    "label": 0
                },
                {
                    "sent": "I'm actually interpret your assumption is a mean field approximation, because that's usually what you do.",
                    "label": 0
                },
                {
                    "sent": "When you do mean field that you assume that things are statistically well.",
                    "label": 0
                },
                {
                    "sent": "You know you usually use mean field when describing things as a graphical model, right?",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "You have a joint probability and you want to do inference and then you apply them in fields on the way.",
                    "label": 0
                },
                {
                    "sent": "The joint probability behaves.",
                    "label": 0
                },
                {
                    "sent": "Here we make it very simple assumption.",
                    "label": 0
                },
                {
                    "sent": "Yet it's conditionally independent, and from there comes algebraic relationship.",
                    "label": 0
                },
                {
                    "sent": "I think it's even simpler than a mean field assumption.",
                    "label": 0
                },
                {
                    "sent": "It's I think it's the most basic basic assumption in terms of this simplicity level that one can make think mean field is 1 level up in terms of complexity.",
                    "label": 0
                },
                {
                    "sent": "?",
                    "label": 0
                },
                {
                    "sent": "Generalizing this framework, when the hyper reps have different number of life, not necessarily deep to have an average of seven more general now we haven't yet worked on that.",
                    "label": 0
                },
                {
                    "sent": "In principle, yes.",
                    "label": 0
                },
                {
                    "sent": "The problem is that now you cannot write it nicely as a matrix, right as an end by D. But in the end by the Matrix.",
                    "label": 0
                },
                {
                    "sent": "But the algebraic relationship should be there, but now it's not a matrix and Chronicle product and could be messy, that's the.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "The overlaps between two hypernova.",
                    "label": 0
                },
                {
                    "sent": "They share node and so you can have like net local features with someone overlap and then you can maybe get a horse.",
                    "label": 0
                },
                {
                    "sent": "You can, you can share.",
                    "label": 0
                },
                {
                    "sent": "That means you can have two hyperedges which share some of the nodes right in hyperedge, say of four nodes could have three nodes.",
                    "label": 0
                },
                {
                    "sent": "They share three nodes, and the 4th node, so it's in the definition of a hyperedge.",
                    "label": 0
                },
                {
                    "sent": "Nobody made the assumption that they need to be mutually exclusive.",
                    "label": 0
                },
                {
                    "sent": "Very high dependency.",
                    "label": 0
                },
                {
                    "sent": "Yeah and then yeah, but it still works.",
                    "label": 0
                },
                {
                    "sent": "Structure.",
                    "label": 0
                },
                {
                    "sent": "Rigid transformation.",
                    "label": 0
                },
                {
                    "sent": "Very nicely.",
                    "label": 0
                },
                {
                    "sent": "Lionel Richie.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Well we haven't, and the reason that we haven't looked at it is that.",
                    "label": 0
                },
                {
                    "sent": "Some years ago we had an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Nice algorithm for feature selection and then said OK, we have an ice hammer.",
                    "label": 0
                },
                {
                    "sent": "Let's look for nails for this and one of the things we went to computational biology and there I find out that working in computational biology it's not enough to have an algorithm you really need to understand the biology.",
                    "label": 0
                },
                {
                    "sent": "You really need to understand the data, how you massage the data and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we end up doing it.",
                    "label": 0
                },
                {
                    "sent": "We found a collaborator who really understands the the biology and it's one of those papers that I spend much more time than I wanted to spend on.",
                    "label": 0
                },
                {
                    "sent": "And since then I stay away from computational biology.",
                    "label": 0
                },
                {
                    "sent": "Is either either make the effort and understand the biology, or don't get into it, OK?",
                    "label": 0
                }
            ]
        }
    }
}