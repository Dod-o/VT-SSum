{
    "id": "gwqrf3yzumqhjuogjqsmrrqyrmnpjbvt",
    "title": "Gentle Introduction to Signal Processing and Classification for Single-Trial ERP Analysis",
    "info": {
        "author": [
            "Benjamin Blankertz, Machine Learning and Intelligent Data Analysis Group, TU Berlin"
        ],
        "published": "April 3, 2014",
        "recorded": "February 2014",
        "category": [
            "Top->Technology->Neurotechnology",
            "Top->Medicine"
        ]
    },
    "url": "http://videolectures.net/bbci2014_blankertz_signal_processing/",
    "segmentation": [
        [
            "OK, this is overview.",
            "What are presenting today so?",
            "To set the scene for European analysis, I will shortly introduce a paradigm in of PCI where we need this single trial European analysis.",
            "And Yep, eBay seller.",
            "And I will show some standard analysis and then motivate Y for successful BCI classification.",
            "We should move from so-called univariate multivariate features and then I will tell you how to classify these features and.",
            "And.",
            "We'll discuss some things about understanding what things are going on, their important remarks.",
            "So this is a core part of the lecture, which intend to cover.",
            "In any case, this.",
            "I for sure I will not.",
            "Be able to show everything that is here, but I put it on the slide so you can can look at look at those presentation afterwards and maybe one or two of these topics will also be.",
            "Included in the lecture."
        ],
        [
            "OK, we Arkansas."
        ],
        [
            "And with noninvasive BCI.",
            "So I guess you know all the settings the electrical brain activity is measured with electrodes from from this carb and transmitted to the computers and so there."
        ],
        [
            "The part that is discussed today starts.",
            "But first I should give an overview of what kind of occurrences of neural activity we measure there in the noninvasive.",
            "Measurements so there are so called spontaneous oscillations.",
            "Most importantly for Bcis sensory motor rhythms and Fabio ever talk about this in the next lecture?",
            "So for us here in this topic, European analysis, so oscillations are more or less just noise and disturbing what we want to classify here.",
            "And it's the most critical part.",
            "Here is a visual Alpha risen.",
            "This is strong 20 Hertz activity in the visual area.",
            "That is most prominent when the eyes are closed, but even with eyes open there still.",
            "Quite a lot of this 10 Hertz activity present.",
            "So I will not talk about this.",
            "I will also not talk about the second point here.",
            "These are induced oscillations, for example, steady state visual evoked potentials.",
            "If you look at a similar set.",
            "Blinking with some periodicity will find signal oscillation of the same frequency in the visual cortex, so I will also not talk about this.",
            "Our topic is a transient activity event related potentials chart.",
            "I'm locked to Eve."
        ],
        [
            "And usually external stimulus.",
            "And if you look for description of ERP S you often see figure like this, which shows a typical time cause of ERP.",
            "But typical here doesn't mean that you were in your day to observe something like this because he is a put in many different components.",
            "So usually only observe few of those.",
            "But this shows how come?",
            "Concept so there are in this time course after the stimulus presentation that we are interested in, you see some peaks in the time cause positive and negative peaks, and this correspond to some neural processing of the of the event.",
            "So typically these are different aspects and probably.",
            "Originate from from different regions in the brain.",
            "So if you would look at topography's here of these components, I would probably look different and generally very early components reflect the processing of the physical stimulus and the mid latency components.",
            "Lower level cognitive.",
            "Processes and later ones are higher cognitive processes.",
            "So but"
        ],
        [
            "You don't need to know much about this, so here I will show you the.",
            "Thing that is important for this lecture.",
            "Um?",
            "I will motivate the paradigm of ERP based BCI spellers.",
            "So the the basic paradigm is called oddball paradigm and in a oddball paradigm you have, let's say acoustic stimuli.",
            "Here you have two types of stimuli.",
            "There's a low tone and more infrequent high tone.",
            "So there's a sequence of low tones, interferes with his infrequent occurrences of high tones, and the participant pays attention to the tones, and so there will be some neural processes that.",
            "Correspond to the detection of this deviation that there is deviation in sequence on the regularity.",
            "So in this case we will at the central electrodes is at sisam.",
            "Typically positive deflection.",
            "Which is the P300 component?",
            "Um?",
            "And."
        ],
        [
            "So the same concept works in the visual domain, so here we have frequently green dots presented and interspersed with some red squares.",
            "So and this time it reverses, which are shaded here correspond to the time intervals where we expect specific processing of these stimuli, so these are called the pox or single trials, and the technical domain, so our interests would would be to discriminate these two types of epox the types that correspond to processing of a standard stimulus, and those two examples here.",
            "So epoch set correspond to processing of deviant stimuli.",
            "But even if you this is, this is possible in this paradigm, so this doesn't make a BCI yet.",
            "'cause if even if we can decode this and discriminate, then we would only get some information that is already in the sequence.",
            "So we already from presenting the sequence.",
            "We already know the time points of the divisions.",
            "So.",
            "In order to make it interesting for BC."
        ],
        [
            "I-1 is a different paradigm, so here are.",
            "More groups of possible stimuli.",
            "And now there's a specific attention task for the participant's attention, for example.",
            "To attend, for example the Red Square.",
            "Then we would call this stimulus the target stimulus and all other which of different of different types or others are non targets.",
            "And if we can.",
            "Get this information from the from the e.g.",
            "This is not something that we can use for BCI because now we can.",
            "Transmit some information to it so that there's a fixed or.",
            "Similar sequence, and it's only by the attention of the participant makes it discrimination between targets and non targets.",
            "So."
        ],
        [
            "Therefore, this leads to a simple design of a spell offer BCI where you can write letters just with attention measured brain activities.",
            "So I'll say the user here wants to write the letter N to complete the word brain here.",
            "Then with the very same paradigm we could just present or letters of the alphabet in rapid succession and the user would concentrate on the occurrences of the N. And each time there's a N, we will see this bingo component and CR.",
            "He did they think he or she detected?",
            "The occurrence of the target letter, and then if we can find this information in the EG and successfully classified, then this letter can be selected.",
            "I go back so 11 difference to the to the standard paradigm or paradigm.",
            "Here is also that user presentation is much faster than the typical paradigms, so therefore I didn't shave.",
            "Here's a nontarget epoch's because they are strongly overlapping, so you do the first presentation to have fast communication that is clear.",
            "But this makes the interpretation and the classification of the components of course more difficult.",
            "Because they are mixed and overlapped."
        ],
        [
            "So and so just another figure.",
            "This is the same but just showing that we will have this information from from many channels which will be will become very important."
        ],
        [
            "Later.",
            "So what would be a standard analysis of such an European experiment?",
            "So you look at this.",
            "Event related potentials.",
            "So here you.",
            "And collect, or the pox from the experiment.",
            "So you do this for a long time and then you have many epox corresponding to target stimuli and many box corresponding to nontarget stimuli.",
            "And then you average across all these epoch's.",
            "So we will see this in more detail later but this.",
            "Everything we get rid of the non task related components, so this will average out and we will only see the source components that are related to the processing of the event.",
            "So now we can compare.",
            "So we are interested in the difference between these two conditions, targets and non targets.",
            "So therefore it might be useful to shade here the area where the two curves differ.",
            "So here's a thread shading.",
            "If the targets are more positive and blue shading is there more, more negative.",
            "So here one can already see that.",
            "Here's a prominent difference, centrally, where the targets are more positive and he left at the back occipitale in the vision area, it's more negative.",
            "But in order to get a better overview of this.",
            "Topographic."
        ],
        [
            "Distribution sorry, one slide before so often run.",
            "This place is also in a grid plot.",
            "These are the same plots as before, but now just arranged in a grid."
        ],
        [
            "Um?",
            "Then to get a better impression of topography of these components, one plus is as Maps.",
            "So here on top you again see the time course of European two channels.",
            "And here you see is shaded for different time intervals and for exactly this time intervals you see the topography.",
            "Here that means we we average within this time interval and then we get one value for each channel.",
            "One voltage value and this is now interpolated as its topography, so this is a top view in the head and this is color coded.",
            "Negative potential blue positive in red and this is done separately for the two conditions and for all this for shaded.",
            "Intervals so all these.",
            "Components now give information that is important for classification tasks.",
            "You see that for each of these time intervals, there's a clear difference between the two conditions."
        ],
        [
            "So this was the first part of the standard European analysis.",
            "Then if you look at papers of your peer analysis when they quantify the difference between targets and non targets, you will often see graphs like this.",
            "So this is a amplitude of the P3 component.",
            "Here at one channel I didn't put it here, so at channel sees it for example and then you have fears of mean value for the targets is 7 microvolts and for non targets 0.5 and.",
            "This gives the standard deviation and you can also do statistical tests.",
            "So this is, this is what I would call the standard analysis.",
            "Of your piece, and this already gives both gives important information, but for Bcis we have to go one step further because we need to classify single trials.",
            "So not not see the difference on averages, but we need for each single trial we need to decide whether it's a belongs to the target or non target.",
            "In order to control the feedback."
        ],
        [
            "Application.",
            "So and why we call this what we've seen before univariate features.",
            "I will now introduce and motivate why we have to use those multivariate features."
        ],
        [
            "So, but we start from this.",
            "Univariate features, so our task is to distinguish target from non target epox or trials.",
            "And from from the literature and also the graph that I've shown you, we see that.",
            "The best discrimination is probably can be gained from the P3 component, which has its focus on the electrode set and.",
            "The peak latency of about 300 milliseconds.",
            "So.",
            "If we want to discriminate targets and non targets of best first guess would be that we use this amplitude.",
            "In the channels is set at the peak time, also P3, so this is what this barplot before was based on.",
            "And of course, we could also use this for."
        ],
        [
            "Suffication so here you see average year piece for the targets and non targets.",
            "And here's a time point of the peak.",
            "The highest voltage here in this time course.",
            "Which is in this experiment at 220 milliseconds, electrodes he said.",
            "And you see that there is a difference.",
            "Indeed, the amplitude is higher.",
            "For targets and for non targets, or there's hope that we can classify something and so this would be called univariate feature.",
            "Univariate means it's just one value that you have there for each trial.",
            "But so these are again averages.",
            "So the strategy would be, for example to put a threshold between the two means here and say everything.",
            "Above is classified as target and everything below is non target.",
            "So, but here you see the.",
            "The causes time costs for the single choice.",
            "So I hear randomly selected 10 single trials and here you see the voltage value at this peak latency.",
            "Where we would.",
            "Base or classification based on this simple strategy on so and the single trials look very much different and deviate very much from the from the average variable becausw.",
            "The brain is very, very active each moment in time.",
            "There are many processes going on.",
            "Each time that are not related to the events that we are interested in.",
            "So therefore we have.",
            "This large varieties, so let's let's see how classification strategy would look like now.",
            "So here you have the points of this single trials here, so we've seen that.",
            "So there are two targets classified with this strategy as non targets.",
            "And also here are two non targets classified as targets.",
            "So it's.",
            "It is for sure better than guessing, but not very powerful, so we should improve on that.",
            "But before I will show you how to improve, we will discuss measures of.",
            "Separability, so here we could say, for example this.",
            "60% correct and 40% wrong, but often 1 uses."
        ],
        [
            "Are there also other measures for discriminability?",
            "So I just mentioned those so it's not.",
            "The details are not important here, so they are often used as a measure, so ask fair value which is defined like this.",
            "So this is essentially the square difference of the of the means of in these two conditions divided by the by the variance of all values where you put all two classes together and there's some normalization such that this.",
            "Coefficient is between minus one and one, so this is not important importance that you get this message below.",
            "So if.",
            "They put two distributions are clearly separated and don't have much variance within each condition.",
            "And if the.",
            "One class says that the English class here is more negatives and they ask where value will be something like.",
            "Minus one if they are completely mixed then they ask where value will be around 0.",
            "This is non discriminative and the other extreme of separation spends orange classes positive and then the coefficient will be around.",
            "Plus one.",
            "And I."
        ],
        [
            "So introduce another measure of separability.",
            "Which is the area under the RC curve?",
            "And so this is just a.",
            "Very short introduction on how you could calculate it, so this depends only on the on the order.",
            "Office this values so this other.",
            "Amplitudes that we would.",
            "Use for classification, so once you start from below here and you go in this grid.",
            "Up one step each time you see a blue dot here and if you see a orange dot, you go to the right.",
            "And then at some point if you have so, the grid depends on the number of points and then you arrive at the upper right corner.",
            "And this area under the curve is.",
            "Yeah, this is an area under the curve, so here you would just count the number of of squares and this also gives a good measure of separation."
        ],
        [
            "And again, this is what you need to remember.",
            "So if there are in the order such that the orange are all below the blue.",
            "Then you will get this curve and the area is 0 below the curve.",
            "If they are completely mixed then you go randomly up, right, up right and you will get something like the diagonal line.",
            "So the area will be 1/2.",
            "And if the orders like this, then you will go up, up, up and then right, right, right and the area is 1.",
            "So these are the extreme about separation, and this is the random case.",
            "So this is a little bit and different since you asked fair value 'cause here it doesn't matter.",
            "You're just the order matters, not how much variance is within the classes.",
            "And in our data sets, the curve looks like this and we get our see value of 0.7."
        ],
        [
            "So now we can.",
            "Add something some more information to our standard analysis.",
            "So and in order to understand that so first.",
            "Note that we so this is like a one one single trial, one epoch.",
            "So here you have the time.",
            "Here's stimulus presentation and this is like 1 second around.",
            "The stimulus presentation.",
            "And here's a time courses for all channels, but of course we can also display this as a matrix, so this is the same here, but you have to roll for each channel and now these amplitude values here are color coded.",
            "So we will often use this representation also.",
            "And so now we have such matrices for each trial for each epoch.",
            "So we have a sequence here and we know whether it's corresponds to a target or non target.",
            "And now for each element in this matrix for each channel and each timepoint.",
            "We have no values for targets and non targets and then we can apply this measure of discriminability.",
            "Then we would get one AOC.",
            "Value and we put it as the corresponding point in this matrix.",
            "So for each time point on each channel, this value here says how well does exactly this time point in this channel discriminate between the two conditions?",
            "So if there's something right here, we know this channel or you know here around 0 there's no discriminability.",
            "And so we get a nice overview."
        ],
        [
            "So we can now enhance our standard analysis by putting below each channel.",
            "Here one row of symmetric matrix that we've seen before.",
            "So for each channel.",
            "We know for every time points our see value and we put it below here and this gives send some more statistic, reliably indication of how well.",
            "Each channel and each time point discriminates between the targets and non targets."
        ],
        [
            "And of course we can do the very same thing also photographies.",
            "So we have this ask where values for each time point and each channel, so we can use the same time intervals that we had before for plotting.",
            "And then plot this ask where Values's topography's and then this gives quite clear.",
            "Indication of the difference."
        ],
        [
            "And just one final example of how to display this this analysis.",
            "So here's this topografie's are exactly the same as before, but here I put on top so this full matrix of AOC values.",
            "Time points cross channels.",
            "And then here you get.",
            "So overview of all information that we have available and you can see also how this evolves in time, the differences and so on.",
            "So you could use this matrix also to select the most discriminative univariate feature and say maybe six years of area with a with a higher score.",
            "So that is most near to 0 to one and say OK, no, I don't know we have this time point and this channel and this is the best univariate feature.",
            "So this is certainly."
        ],
        [
            "Useful, but I will.",
            "Show you that one should use multivariate features and how and in order to provide the background for this multivariate features.",
            "I will tell you a little bit about the generation of e.g signals."
        ],
        [
            "So the.",
            "Signals that we measure with EG electrodes.",
            "This is something electrical currents from that is generated by pyramidal neurons in the cortex.",
            "And.",
            "The reason that we can we can measure this activity on this carpet at these these special kinds is pyramidal cells orientated in parallel.",
            "In this this cortex, so the current sum up and become measurable.",
            "Yet this club and also their their dendrites, run perpendicular to the.",
            "To this cult.",
            "So therefore we can.",
            "Measure this very well and."
        ],
        [
            "Now comes the important point here.",
            "So of course in the in the brain we don't have wires that go from the source here, but the electrical activity.",
            "Has to arrive somehow.",
            "Here we want to measure it and it's transmitted here by so called volume conduction.",
            "So the tissues here the brain, the fluid and the scars.",
            "Are they all have some conductivity and say transmit?",
            "The information but, and this is not important to conductivity is very different between the tissues.",
            "So the brain inside the wet part is a very good conductor.",
            "While this car is a very bad conductor and this is a very important consequence.",
            "So if we look at this sales here, this population and how we would measure their electrical signal at these electrodes.",
            "And the intensity with which it arrives at CG sensors doesn't differ very much cause the most difference in the distance is here inside the brain.",
            "But there is a conductivity is so good that it doesn't make a big difference whether the signal has to travel only the short way or the wrong way, because compared to the hard way of the signal to get through the.",
            "Skull.",
            "It has a conductivity is very bad.",
            "This is.",
            "This is comparably nothing or easy and the way through the skull doesn't depend on this.",
            "Distances.",
            "So therefore we will get the signal from from one source with almost the same intensity at different electrodes.",
            "So that means we don't have much spatial information in it."
        ],
        [
            "And.",
            "This you can also check if you just take some e.g data and here I calculated the correlation coefficient of the signal that is measured centrally at these at with the signals from all other electrodes and this correlation coefficient here.",
            "Is put here in true colors and you see that almost all channels have a correlation coefficient of more than .9 Mrs Central electron.",
            "So that means the spatial information is very limited, at least in raiji."
        ],
        [
            "So with this as a background, we can all discuss how to proceed from the univariate feature to multivariate features.",
            "So one thing where we can extend that we are not only used one time point, not only.",
            "The P3 peak time point, for example.",
            "But you can use multiple time points or intervals.",
            "These are called temporal features, or you can put together the information from many many channels, but still only one time for and this would be called special features.",
            "All we do both these are called Special Temple."
        ],
        [
            "Feature.",
            "So why does this information help?",
            "So now we do.",
            "The comparison only takes the univariate information from one channel, one time point and compared to multivariate.",
            "So assume this setting you have a, so the ERP speller setting where we want to discriminate targets from non targets.",
            "And this is a cognitive task.",
            "So assume we have to.",
            "Assess to the to the source signal in the brain so we don't have, but assume we have this and this might look.",
            "Like this in a single trial, you have to speak peak.",
            "So that in the G is seen as P300 component.",
            "And so here we would have the threshold as in the example before.",
            "So this point here is above the threshold.",
            "So this is correctly classified.",
            "But it's the same time in the brain.",
            "There's this spontaneous activity.",
            "For example, this visual Alpha rhythm that I talked about in the beginning.",
            "So you have 10 Hertz or solation here, originating from another place.",
            "And now due to this volume conduction that I explained before.",
            "At these electrodes, that which is above his cognitive source and Atos at which is above the visual area, we don't measure exactly this source signal, but a mixture between the signals.",
            "So if we only take into 'cause this tool, you are made an example that I deliberately take Factor 1 here.",
            "For the source to be transmitted to the electrode, buffet and factor 1/2 that here, for example, is the visual.",
            "Rhythm also.",
            "Is measured at these at.",
            "So what we measure here is now this mixtures it is shown here in the magenta color.",
            "And also knows that we receive a mixture so he is not so clearly visible, but it's the same mixture.",
            "Just with different weightings.",
            "So no, no if we are unlucky and this.",
            "Time point where we we take our univariate features exactly where we are.",
            "We have a minimum value here and the.",
            "Ongoing oscillation.",
            "Then this time point might be below our threshold and this would be a misclassification.",
            "And if we only have this univariate feature, there's no chance to discriminators.",
            "But if we take a multivariate feature and we take into account also what we measure its channels it.",
            "Then there would be a awaiting spatial filter which here just calculated because I know the mixture where we can reconstruct the original cognitive source.",
            "So if I take.",
            "For over 3 times channel these add and subtract 2 / 3 the information from ozette.",
            "We would exactly reconstruct this and then we would be able to classify correctly.",
            "So but here's we could save the classification only be cause we had a multivariate feature.",
            "We also talks information from another channel.",
            "And."
        ],
        [
            "Similarly, if we.",
            "Don't use this channel all set, but we use multiple time points.",
            "We could also be safe here in this context, so it's again the same setting.",
            "Mrs Miss classification here.",
            "But if you know would use for example, four time points here around this peak and just average across those time points and we would get rid of these ups and downs that come from the visual Idol rhythm and we would obtain.",
            "Good classification."
        ],
        [
            "And this temporal information also holds for other disturbances, like what we often see in e.g.",
            "In particular, if the person is, if it's hot and it is there some sweat on the skin on the Skype and you often see this drift.",
            "And this drifts can also, of course, cause Mystic misclassification.",
            "But if you have a temporal filter that somehow weights different time points, you could can also get rid of the drift and savior classification."
        ],
        [
            "So this means it's it's a good idea to include more channels and more time points to the classification, and so generally for EPS we use so-called spatial temporal features, which.",
            "Exactly means.",
            "That you use more channels and more time points.",
            "And this shows how.",
            "How this is done.",
            "You just take several channels.",
            "This may be a subset, or maybe all channels and then you take several time intervals.",
            "Or you just do subsampling or what else?",
            "And then you have this matrix of values for all your channels and all the time points.",
            "And all these various in the matrix you put on a vector and this is in what we call feature vector and this is now.",
            "The quantification of the information.",
            "About this one, try.",
            "So this contains temporal information, spatial information and therefore.",
            "Should be a good basis for classification."
        ],
        [
            "So special cases are the temporal features.",
            "If you only take one channel, so we then take just one time course.",
            "As a feature vector, this is called temporal feature."
        ],
        [
            "Well.",
            "If you take many channels or say all channels but just one time point then we get one value per channel, so this is called special feature."
        ],
        [
            "So to sum this up, if you have one, one epoch of EG.",
            "So this is 1 short time segment related to similar processing.",
            "We would.",
            "Displaying in the signals as as a matrix and then the features would be if we take one row of this matrix.",
            "This is a temporal feature.",
            "If you take a column of this matrix, it's a special feature.",
            "And if you take the full matrix, or maybe subsample, use only few channels and few time points, this is a spatial temporal feature.",
            "So it's a temporary feature correspond to a time 'cause the spatial feature to a topography and spatial temporal feature to a sequence of topography's."
        ],
        [
            "So not too.",
            "Put a bit.",
            "Backgrounds to it so we can.",
            "So often one assumes the following model for earpiece that we have.",
            "In the brain signals we have so-called timelocked activity.",
            "And non time logged activity.",
            "So time log means if.",
            "If the stimulus is repeated several times, we will get the same similar potential at certain latency's, say always 300 milliseconds after the stimulus, we have a more positive peaks.",
            "This is an timelocked activity.",
            "And all spontaneous activity is non timelocked.",
            "So in this model, so X of K is the signal in the case trials that we measure.",
            "And P of T know now would be the time locked activity, the ERP and the residual activities called here are of K and the model or the simplifying assumption here for earpieces.",
            "At the time locked activity the ERP is the same in every trial so therefore I didn't put the index K here.",
            "So in every trial you have this the same, say cognitive source active and every time in the same way.",
            "So this is a bit simplified cause might depends also on vigilance and different factors, but simplifying you can say this is the same and we have this residual activity which is a background, EG.",
            "And.",
            "Then if we have this splitting, then we know that the if we average across trials the residual activity there should be 0, because this exactly means non time lock.",
            "If we average across trials, nothing is left is just random activities that cancels out.",
            "And therefore this residual activity.",
            "If we put this in a probability framework, this is.",
            "Distribution here is a Gaussian distribution with mean zero.",
            "This is long time locked and some covariance matrix, so this is often assumed and empirically found that this noise in the EG is Gaussian distributed.",
            "And so now comes the important consequence.",
            "If you know, look at the distribution of the e.g signals itself.",
            "Say we take one tick fixed time point and look at the distribution over trials.",
            "And if you calculate the mean of this X of T0.",
            "Then we get out only the piece the ERP becausw.",
            "There are has been zero, so this cancels out, so the mean.",
            "Is that your P itself?",
            "And so covariance.",
            "Of this this trial.",
            "So if you calculate the covariance, you subtract the mean so we would subtract the mean here.",
            "If we subtract the P from the X, we only.",
            "Have left the residual.",
            "So therefore the covariance is.",
            "Covariance of this residual activity.",
            "So this is important consequences if you have this ERP model you say the mean of your distribution is the ERP itself, and so this is a signal we are interested in and the covariance is the noise.",
            "So the covariance has nothing to do with the signal."
        ],
        [
            "So here for illustration, same thing.",
            "So here on the left we have the signal X the signal itself that we measure.",
            "And here we have the ERP and we're here.",
            "We have serves, annoys the residual so that sex is always the sum of the European to residual.",
            "So this is a magenta colored line.",
            "And this these are different trials.",
            "So in each trial the ERP is the same and the noise is different.",
            "And if we average.",
            "Vertically across trials.",
            "Then the residual activity would average out and would be.",
            "Eventually be a zero line, so he only took eight trials, so therefore it's not exactly 0, but this would be zero line.",
            "Everything the constant signal gives the appear.",
            "So therefore, if you do this for the X.",
            "If we average across enough large number of trials, we will get out the P. So this is what was shown in formulas here.",
            "So this is a basic office standard ERP analysis we average across trials and then.",
            "We recall what we get here.",
            "We call the ERP."
        ],
        [
            "So no fuck for classification.",
            "It's important that we know the distributions of the two conditions, so you will see this in detail later, But so we need to know distributions.",
            "So I will show you how to visualize distributions and find out characterize distributions.",
            "So first here we start very simple with a 2 dimensional feature.",
            "So we take care.",
            "One channel here took 55 and two time points.",
            "And now our feature is this vector.",
            "Value at P5 at time point 180 and value at P 5 * .450.",
            "So we have a 2 dimensional feature vector."
        ],
        [
            "And now we want to make a so called scatter plot.",
            "So here we will make a point in.",
            "This Patch is better scatter plot for each trial.",
            "And on the 1 axis we have one feature and on the other axis we have certain feature.",
            "So how would this look like?",
            "So here we have channel, he remarked the early time point and he remarked the late time point.",
            "Now it turns this by 90 degrees I rotated.",
            "Now we can.",
            "The value that we have here we take as Y axis and the value we have here we take as X axis.",
            "So we put the cross here.",
            "So this one other ear piece.",
            "So this is a mean.",
            "Now here I add some single trials Sony pokes and now we do the same thing for the points.",
            "If we get here for each trial.",
            "So for each trial we get a point here in this scatter plot.",
            "So now I do it for the rest.",
            "And then we see something like this distribution.",
            "This is called scatter plot.",
            "So, and as I showed shown before, for your piece, we assume that.",
            "The noise is Gaussian distributed.",
            "And Gaussian distributions are indicated by Ellipse.",
            "And I will show you."
        ],
        [
            "What's the background of this is so here we have a 2 dimensional Gaussian distribution.",
            "These thoughts and this is this nice surface is a density which says.",
            "Something like the probability density.",
            "So here around the mean it is most density and outsides.",
            "It's get less and.",
            "This aligns here are the eizo density lines here.",
            "So which has the same height and this surface plot?",
            "So this means all points on one line has the same probability density.",
            "And for Gaussian distributions, or these, if you look from above, these are ellipse.",
            "And so forth too.",
            "To easily indicate a Gaussian distribution, you just take one of these..."
        ],
        [
            "And a particular one as I will show, you know.",
            "But how do we know how this ellipse looks like?",
            "Um?",
            "So this can be obtained with the so-called eigenvalue decomposition.",
            "Um?",
            "So I explained the formula but so.",
            "Given the matrix, it is symmetric and positive definite.",
            "We can do the eigenvalue decomposition, which means we get another matrix V which is also normal matrix and holds the eigenvectors.",
            "And the diagonal matrix D such that we can represent the Matrix Sigma as a product V times ditrans be transpose.",
            "So don't don't worry, assist, looks complicated to you.",
            "So first thing is you can forget about the prerequisites because we will always apply this for covariance matrices.",
            "And self info covariance matrix is always fulfilled.",
            "So if you have a covariance matrix, you can always make this decomposition.",
            "And the meaning of the decomposition is also very simple, it just says.",
            "So this.",
            "These matrices this correspond to... from the Gaussian distributions and.",
            "Diagonal matrices correspond to... that have their access.",
            "Along with the coordinate axis, so the axis of this ellipse is wrong.",
            "First this along X axis of the coordinate system and another Y.",
            "If you have non diagonal matrices, these... are tilted somehow and the eigenvalue decomposition just gives you gives you this correspondence it says.",
            "So the general matrix that you have Sigma here correspondence to an ellipse that you get if you take this basic ellipse that is defined by the diagonal matrix D. And apply this matrix V which is a rotation.",
            "OK, so this is D and this 2 dimensional case would just have the diagonal matrix is just two entries, one is the radius in horizontal direction and the others in the vertical direction."
        ],
        [
            "So here for plotting this nice ellipse, if you get the data here from many trials X.",
            "Then you would get this ellipse just by calculating this eigenvalue decomposition.",
            "And then in this matrix V. So this is 2 by two and the two dimensional case you have two vectors, 22 dimensional vectors.",
            "And one vector gives you this axis and the other vector gives you that access.",
            "But this vectors are normalized this half length one, so this doesn't say how long is the ellipsoid in that direction.",
            "This is given by the Matrix D. These are quotes are eigenvalues, so in this diagonal matrix two by two we have two values and one says.",
            "How, how long is this axis and the other?",
            "How long is it set axes and this correspond also to your original distribution?",
            "Causes eigenvalues D. For example, this eigen values that correspond to this V1 to this direction.",
            "This is exactly the variance of the data in this direction.",
            "So then this is eigenvalue decomposition.",
            "We know the variance in this direction and in that direction.",
            "OK, so.",
            "I guess this was already quite a. Exhausting for you and therefore so that you don't get into it too low vigilance state.",
            "We will make some active part now.",
            "And.",
            "So therefore first, so this will be a group work, so you will have to leave.",
            "Your place is so and I will explain what happens.",
            "So first we make a distribution into several groups of about five people and then I will explain the task.",
            "So splitting in the in the groups will be as follows.",
            "So all this part will go to the to the left and that part we go to the right.",
            "Maybe first I explain.",
            "It is then you have, so you will take some time, but it doesn't matter.",
            "So then in each group you have to arrange yourself in a line that corresponds to the to the distance of your birth place to Berlin.",
            "So in the end the one one in Berlin should be on this side and the people from far away should be that side.",
            "OK, we have these two lines and then but I will expand this later again and then you will count on this side.",
            "I think from from 1 to 8, the first one says one, the second 28 and then you repeat and then all the ones will be in one group or the tools and so on so that you are mixed between the distances and I think on this site you will count to 12.",
            "OK, but first of all so, but it's also the intention that you get to know each other.",
            "So take your time to discuss where you where you originate from.",
            "OK, so.",
            "Say thank you so now just get into a linear order.",
            "So even if not everything was was clarified.",
            "So now I mention you counting, so I think yeah, you need to count to 7 and on that side to 10.",
            "So you just start and you remember your number and all the ones here are one group and all the tools and the same.",
            "On the other side, and then you have to find places in within your group here to do the group work.",
            "So did you get up this evening?",
            "OK, so only two 7, so let's do again so.",
            "Five OK. OK. OK 6.",
            "You have to count, so he was six or seven.",
            "No, we only took yeah, correct?",
            "Yeah, but remember your number right?",
            "No, you have to start from one again.",
            "I change this thing so one.",
            "2.",
            "You have to count.",
            "So yoga.",
            "56 OK, great so now.",
            "Now find your group members so.",
            "So.",
            "So did you succeed?",
            "Do you have all your number?",
            "So, do you have your numbers or not?",
            "So who was the last 1 eleven 1213?",
            "No, no.",
            "I need to 10.",
            "So I change I changed.",
            "To 10 so you are 9.",
            "81 she.",
            "7.",
            "Yeah, 10 minutes.",
            "The next one is 1 again.",
            "OK, so now you have to find your partners.",
            "Sorry sorry.",
            "Don't don't mix between the parts then, then the groups are too big just.",
            "Don't that just within the side?",
            "Sorry I didn't say that.",
            "So now you can also distribute a little bit more that you have some some space for discussion.",
            "So.",
            "So OK, I hope you attract approximately find your group.",
            "So just so now the office.",
            "Please attention to the task so it's a task that you would usually do is a piece of paper because it's about drawing some distributions, but you can.",
            "In this case I think it's easier if you just discuss and think about how it could look like, but of course you can also get a piece of paper if if you like, so this is a task, so I've shown you how to how you do this, scatter plots and now you should.",
            "So I showed a scatter plot.",
            "With one channel in two time points and also to say the counterpart, so you should do a scatter plot.",
            "Now for one time point here, where is the peak of the PS3 years?",
            "But for two electrodes.",
            "So on the X axis you put sevario.",
            "Of the electrodes, is that?",
            "And on the Y axis you put the electrode of the electrode ozet.",
            "And the difference to the what I've shown you before.",
            "Now you have to think about two distributions, so about you get one distribution for targets and another for non targets.",
            "So if you think about this scatter plot, so again you will have some Gaussian distribution so that you should know approximately about two edits or so.",
            "How these could look like.",
            "So.",
            "Yeah, I don't have a bot here, so you have mentioned this.",
            "Go back to this."
        ],
        [
            "So you should have something like this on this axis you now have.",
            "Potentially it sees it as it.",
            "This is all set and you should think about these two ellipsoids for targets and non targets, or they look like and.",
            "Corresponding to these signals that you get in this standard ERP.",
            "So now you should.",
            "Should discuss with your with your group members, so that means you.",
            "Yeah, OK, so maybe discuss like 4 minutes and then we will continue.",
            "OK, thank you very much.",
            "I'm sorry that we don't have more time.",
            "So I'm very happy that you so actively participated.",
            "So now I will tell you how it looks like so far from this graph.",
            "You can already easily see where the means of the two distributions are, so you have at channel all set in that direction.",
            "Both means are at zero and on the other axis the mean for non targets is around zero and the other one is around 5 microvolt.",
            "So this this is this solution, but this is from the.",
            "Little bit different data set and also you cannot see.",
            "You cannot say exactly how this looks like, but here you see.",
            "In the opposite direction, there both at zero and in the horizontal direction, yes is 5 microvolt difference.",
            "And then the distributions are Gaussian distributed.",
            "So you have this ellipsoids, and so now it's a question how this ellipsoid is tilted or looks like and this you could not see from the picture I've showed you because there's not.",
            "I didn't show which trial it belongs to which.",
            "So, but you from what I told before, you could know that the orientation is approximately like this, and this is due to this volume conduction.",
            "So I said that what you measure at different channels is highly correlated.",
            "That means that if the value is high at all set, then it's also high Aziz at.",
            "In principle, so therefore these are more or less orientated like this.",
            "So in this particular data set is very much tilted, so I don't know what so this is maybe a bit untypical, so it should take another solution picture, so usually it's they look they look more like this.",
            "OK, and so this is important because this is now the setting for classification.",
            "So our task is to kiss classify these distributions."
        ],
        [
            "So this is the next topic, so we have.",
            "To find a classifier, the classifier is a mapping just from from the space where we have some features.",
            "So from this vectors to say labels one and two, which corresponds to targets and non targets.",
            "So from the continuous signals we first cut out the box from each epoch, we calculate the feature vector.",
            "The spatial temporal feature, and then we have the classification."
        ],
        [
            "Step.",
            "So you see, this is the same calculated from a different data set.",
            "So just to repeat here, we have the two means and the means correspond to the smallest to the ERP itself.",
            "So this is the signal.",
            "And the covariance matrix, which makes this ellipsoid.",
            "This is just the noise as we've seen from the LP model, and because this is just background noise, we also know that this is the same in both conditions, so this doesn't depend on the condition, it's just background noise.",
            "So the Sigma is the same."
        ],
        [
            "So now how do we classify so?",
            "This is just we start with a very simple method to classifiers.",
            "So assume we only know the means of the two distributions.",
            "We only know mu one and MU 2.",
            "And now we have a sample X.",
            "So if you only know the mean there's really not much that we can do if we are asked to classify, we would measure the distance to the two means and then say 'cause it's the distance is smaller too.",
            "MU two we would classify it as two.",
            "And if we do this strategy for all points, we would separate the whole.",
            "Plane.",
            "Operation lines that that goes perpendicular to this connection.",
            "Also two means and cuts it exactly in the middle, and all these are classified as two and see other half plan as one.",
            "And this is a very simple classification scheme that you can also do in high dimensions, not only in two.",
            "As a short you here.",
            "And this is called the nearest centroid classifier.",
            "So that means also called centroid and.",
            "So this is the nearest."
        ],
        [
            "Android classifier.",
            "And if you this is formalized, so we don't really need to know this, But this is formalized usually in this way the classifier has a weight vector W. It's just the same dimensions as your feature.",
            "Features and one number of bias bias term that you subtract and so here from this graph you see.",
            "So essentially this V transpose times X is a distance of here.",
            "This X If you Project X on this vector W. This is a distance from the origin and then we have two subjects.",
            "Here the distance of this separation line from the origin, so therefore this distance here from the separation line.",
            "To the projection of this point is this expression, and if this is positive, this means it's on that side of the separation line.",
            "Then we would for example, classify.",
            "Then we would classify a Class 2 in this case and if it's.",
            "On this side, then classifier one.",
            "OK, so each classifier has his fate vector or enter bias term, and you can formalize all linear classifiers in this way, not only the nearest centroid classifier."
        ],
        [
            "So, but can we expect for Fiji this nearest simple near century classifier to perform very well, so as so these are distributions that we could expect to see in e.g.",
            "So in this case this would be like ozette and this is at and this is the third due to the.",
            "Coalition.",
            "Now the nearest centroid classifier.",
            "What makes this separation this side?",
            "It's orange class and this side is blue class and you immediately see that this is not a good idea that you should better have something like this.",
            "OK, so we see for each year if we have this high correlation between the channels then this NCC is now."
        ],
        [
            "Good.",
            "So therefore I will introduce another classifier, linear discriminant analysis.",
            "And one can show that linear discriminant analysis is optimal in a certain sense.",
            "So optimal means here that its hits minimum risk of miss classification for samples that come from the same distributions.",
            "And the LDA is optimal under these three assumptions.",
            "The first assumption is the features of all classes are Gaussian distributed.",
            "The second is that the Gaussian distributions all have the same covariance matrix and that we have these first two things we have already discussed to be true for LP's, and the third is set to true class distributions are known so that we have this new one and Sigma and U2 and Sigma is the same.",
            "In that case, if you have this given, we can define the weight vector of LDA.",
            "Here's the inverse of the covariance matrix, Sigma Times new 2 minus mu one and bias term."
        ],
        [
            "So now this is the same setting as we have seen before with the NCC and also the excesses at the same, exactly the same spot.",
            "Now, but here I've shown some typical ET urope distributions.",
            "So then we would have this ellipsoids and the separation line of LDA.",
            "In this case would look like this, and in this case very same X and the same users.",
            "Before.",
            "Here X would be classified as blue class.",
            "And this is if you.",
            "If we know this distributions, then it's obvious that this is a good choice."
        ],
        [
            "And there's also a correspondence between LDA and NCCS, or I will not go into detail here, but this this is transformed.",
            "It's called Whitening, which transforms distribution which is ellipsoidal distribution into a spherical one."
        ],
        [
            "And if you transform this whole space versus ellipsoid distribution by whitening into a spherical one, then here we can do the NCC classification.",
            "But here we removed, so to say this correlation structure.",
            "But this is just a side remark."
        ],
        [
            "This is not not important, you can just take LDA and do it."
        ],
        [
            "Um?",
            "OK, I based on the model, I already motivated the two covariance matrices of the two classes should be the same because they only depend on the background data.",
            "But here I also have some empirical evidence from the data here I.",
            "Calculated the covariance matrices from the two classes targets and non targets and these typographies visualize this eigenvectors which are the vectors of this.",
            "Covariance matrix of this ellipsoid corresponding to the highest eigenvalues, so to the most information is and here you see that the corresponding Maps look very similar between two classes.",
            "So."
        ],
        [
            "We can.",
            "But this is not generally the case, so this is the same visualization of covariance matrices from a different classification example.",
            "So this is handwritten digit recognition.",
            "So here's here's the zeros, and here's the 7th, this this other mean values.",
            "And these are against the eigenvectors of covariance matrix.",
            "And here you see, this doesn't look.",
            "Any similar to this one?",
            "So there's that means in the handwritten digits.",
            "The covariance matrix depends on the class.",
            "So if you draw the zero little bit different little bit tilted or little seeker pen or something like that.",
            "Then there's some variation, but how this variation is reflected in this feature in that case depends on the class.",
            "So this is not generally that I say oil.",
            "Covariance matrices are the same, but this is depends on what I explained before this year.",
            "P model.",
            "Because there's a noise is independent of the condition.",
            "But here the noise is it.",
            "Depend, so that means here for the digits we would probably need a different."
        ],
        [
            "Classifier.",
            "So that so now we can.",
            "We already discussed how to extract features, so now we and we discuss classifiers.",
            "So now we.",
            "Can evaluate classifiers and talk about that, so therefore just one word about validating classifiers.",
            "If you validate classifiers, it's important that you have separately a training set where you train your classifier on where you determine the distributions and this term.",
            "In this vector W and the bias B and then you take another set independent which must have nothing to do with the training set, and then you calculate the performance.",
            "So only then you get a reasonable estimate.",
            "So here I will not have time to.",
            "Talk about more details about this, but if you are interested in this topic, you can refer to this."
        ],
        [
            "Paper.",
            "So here I show you results classification validation based on spatial features.",
            "Corresponding to different time intervals.",
            "So this is the first step.",
            "So here I took.",
            "The various in this time interval and then like have a special feature.",
            "And I do a classification.",
            "Then I take the next component, calculated spatial feature for this time interval and.",
            "Makes a validation of the corresponding classes for her, so these are like trying out different spatial features.",
            "And here we see the performance.",
            "So we see for these two components we have best classification is error rate around 14% and the others also perform OK, but a bit worse.",
            "So, but as I motivated before, we should use all information, so we should use spatial temporal feature and put all this information here together.",
            "So but remember here is the best classification on this.",
            "Best intervals around."
        ],
        [
            "18%.",
            "And when I put now all information together into a spatial temporal feature and I apply my linear discriminant classifier, then I get an error of 25%.",
            "And which is worse than the result for the best?",
            "Interval so this looks strange at first sight because here we added some information because this feature includes this feature but has more information, so it should perform better, but it performs worse.",
            "So that's could be surprising because we have this optimality criterion, but if there's an optimal classifier and the optimal classifier gets more information than at least it shouldn't get."
        ],
        [
            "Worse, So what happens?",
            "So then we need to go back to the assumptions.",
            "Also, optimality and the third one I didn't talk about.",
            "The third assumption was a true distributions are known, which says we we know the true means and the covariance matrix.",
            "And of course this is never the case in real applications.",
            "So you don't know the true covariance matrix if you have an any PCI data sets, so you have to to estimate.",
            "This distribution para meters from the data and so there you make an error in this estimation.",
            "So therefore some optimality doesn't hold 'cause you don't know their distributions.",
            "And.",
            "Yeah, and these people see that by adding more information we have higher dimensional features.",
            "And then this estimation."
        ],
        [
            "Gets worse.",
            "So for a classifier we have to estimate these parameters, the mean and the covariance matrix.",
            "This is a formulas and now to discriminate between true and estimated parameters.",
            "These are also called empirical parameters.",
            "We put the head on these variables.",
            "Philadelphia observation if you have very high dimensional features and not so many examples where you can.",
            "And estimate your para meters from.",
            "Then there's a systematical bias.",
            "And so in particular, if we estimate this covariance matrix.",
            "High dimensional space.",
            "This Ms systematic biases like this.",
            "The large eigenvalues of this covariance matrix are too large and small eigenvalues are too small."
        ],
        [
            "Or so.",
            "And graphically this means so I can various hiking various meats.",
            "If you remember means large variance.",
            "So here's this would be the direction also.",
            "Not just variance and this of the smallest variance.",
            "So yeah, Green is a true covariance matrix, so if we estimate this from data, large eigenvalues would be too large.",
            "That means even estimates of variance, larger and smaller eigenvalues are estimated to small and means.",
            "Here there's a very Asian is smaller.",
            "We would even estimate it even smaller.",
            "Of course, this is just a cartoon, because if we this is in 2D.",
            "If you just have two dimensional features then this problem would not arise.",
            "This is only if F like 500 dimension.",
            "And this shows the same thing.",
            "So these are the eigenvectors.",
            "And he says I can values in black as a true eigenvalues.",
            "And here we estimate these eigenvalues from data and here we are only 50 data points and four blue 100 for Green 200.",
            "And here you see as a low eigenvalues are if you have too little samples and their estimated even lower and for high eigen various estimated even higher.",
            "And then you can easily imagine if you have such a bad estimate of the covariance matrix and you calculate the classifier based on that.",
            "Of course it's not."
        ],
        [
            "Optimal.",
            "So we need to.",
            "Counterbalance is to counteract this misestimation.",
            "And there's a quite nice easy solution to that.",
            "Um?",
            "So as we have described the bias we need to make the estimate the ellipse more spherical.",
            "So because the estimate is 2, ellipsoidal likes a blue one.",
            "So we just make apply a linear morphing between the empirical one and it's very clear.",
            "Covariance matrix.",
            "So here's our empirical one and the spherical.",
            "Covariance matrix corresponds to the identity vector, which is scaled here with the average eigenvalue.",
            "And with this parimeter gamma, we make a linear morphing.",
            "1 minus, we multiply the empirical covariance matrix and with gamma.",
            "This vehicle, so if if gamma is 0 then we would just have sick my head and if, is one we would completely forget about the estimate Sigma hat and just takes a."
        ],
        [
            "Spherical one.",
            "So one can calculate that this so this is called shrunk covariance matrix.",
            "You can calculate its shrunk covariance matrix has assign same eigenvectors.",
            "As a empirical one.",
            "So remember, the eigenvectors are the.",
            "Direction of how the axis of the ellipsoids are, so that means you don't turn the ellipsoid.",
            "We just change the eigenvalues.",
            "And these eigenvalues are just linear interpolated between the estimated iron, various D and the average eigenvalue new.",
            "So this just confirms what?"
        ],
        [
            "Shown in the picture.",
            "So now we can.",
            "Make an improved version of the LDA classifier.",
            "We don't invert.",
            "Here's an empirical covariance matrix that has is estimate problems, But here we use this shrunk covariance matrix.",
            "But otherwise the formulas are exactly the same, you just replace.",
            "The estimate also covariance matrix.",
            "Tell but success one open question here because he is still the parameter gamma, which is a number between zero and one which morphs between our estimate Sigma head and the spherical covariance matrix.",
            "Yeah, if we forgot my equals zero, we have normal LDA info, equals one.",
            "We have here the identity matrix, so this cancels out.",
            "We have here only mew two months me one and this is simple nearest centroid classifier.",
            "So if you use this shrunk covariance matrix, this is.",
            "The classifier is called shrinkage.",
            "LDA.",
            "So shrinkage LDA makes a compromise between LDA and nearest century classifier."
        ],
        [
            "Now we have to find a value appropriate value for gamma.",
            "So either we can do this by cross validation and just try out on the training set different values of, and then depending on the curves that we get pick, picks this, with the best performance.",
            "This is quite time consuming."
        ],
        [
            "So therefore we were happy to see that.",
            "There's now a new method or it's already around for 10 years.",
            "And."
        ],
        [
            "Which is like like this.",
            "So we can calculate with one formulas or so optimal gamma in a certain sense.",
            "So this is the optimal tradeoff.",
            "So here's a formula that's very easy to implement and also not very time consuming.",
            "That calculates the gamma.",
            "And the camera optimizes this difference.",
            "Here's this optimizes.",
            "The difference between the true covariance matrix that we don't know and the covariance matrix that we get with this shrinkage.",
            "And so it's it's a bit surprising, but under some assumptions you can solve this optimization problem, although we don't know this true covariance matrix Sigma.",
            "So this somehow cancels out in the calculation and we just get a formula that's based on the data that we estimate.",
            "So this is more or less what we see here is the covariance matrix of a single trial.",
            "So you calculate for each single trial occurrence matrix.",
            "And then you calculate.",
            "Along all trials, the variance that you have in this estimate and is this variance.",
            "So that means if this covariance matrix single trial covariance matrix is unstable over trials, you will get a high value of, And if this is stable.",
            "Then you get a low value of,"
        ],
        [
            "So now let's see how this works.",
            "So this is going back to what I was showing you before classification on.",
            "These seven single time intervals.",
            "As this was our first try, just putting information together and using LDA we had this bad performance.",
            "And now with shrinkage, Eleazar performance is only 4%.",
            "So now, as expected, it combines effectively all this information and we don't have this problem with a high dimensionality.",
            "And then this gives a very good performance.",
            "So this is not nothing like some general result.",
            "This is only from one data set.",
            "So here apparently it works very well.",
            "But whether how, how well this works of course depends on on your data.",
            "But but usually so our experiences for for this ERP datasets at least, you don't get worse with the shrinkage, but.",
            "You can only get better thoughts.",
            "It's not.",
            "It doesn't have to be the case, but so this is all."
        ],
        [
            "Experience.",
            "So too.",
            "Give you some more overview so we have at this two classifiers.",
            "First very simple nearest century classifier.",
            "There's the weight vector of the classifiers, just the difference.",
            "New 2 minus mu one.",
            "So therefore this if you.",
            "Calculate this on special features.",
            "That means you subtract, for example, the year, the history, topography from the targets and subtract and from the nontarget so you this fate vector of your classifier would be visualized, flexes, then we had Sadie, a classifier where we have the same difference but we multiply it with the inverse covariance matrix to take into account this spatial smearing and so covalent structure.",
            "So on this data set this further error rates.",
            "End."
        ],
        [
            "Now it's with shrinkage with you.",
            "Put in different various if, then we have continuum between these extreme classifiers.",
            "So for gamma equals zero we have 88, equals one.",
            "We have nearest centroid classifier and there's a continuum here in between.",
            "Somewhere so usually it looks like this first goes down because you make your estimate better.",
            "You counterbalance the error and in the end it gets worse again because then you stop exploiting the covalent structure.",
            "So usually I have optimum somewhere in between and hopefully with this shrinkage formula this puts out this best,"
        ],
        [
            "So this is another view in it so.",
            "The same means, so here we use empirical covariance matrix and LDA.",
            "Here use spherical covariance matrix and your central classifier.",
            "So these solutions only differ with what you expect.",
            "How the noise looks like.",
            "Or this covariance matrix looks like.",
            "So this would mean that in the near century classifier this means you ignore the information from the data because you don't trust it.",
            "So therefore you just.",
            "Base your classifier on the means.",
            "And with shrinkage you do something in between.",
            "So that means this amount of shrinkage somehow is related to your belief in the estimation of the noise.",
            "If you have enough data, then your estimate of the noise will be very good, so you should use LDA or a small gamma.",
            "And if you're if not so much data or very high dimensional features.",
            "Then your estimation of the noise is so bad and error prone that it's better not to use it.",
            "But usually you will be somewhere in the middle and.",
            "Take Cisco."
        ],
        [
            "See fire.",
            "Find also is a separating hyperplane with.",
            "This shrinkage Adss always has to be somewhere between NCC and earlier.",
            "And this camera is based on such a optimization.",
            "This is a.",
            "Error in the covariance estimation and this formula calculates this optimum point.",
            "OK.",
            "Regular.",
            "Yeah, this is.",
            "This is kind of regularization, so there are different kinds of regularization, so, but typically you try to make the estimator of the covariance better and.",
            "This yeah so the.",
            "So general shrinkage is in the exactly one way to regularize.",
            "But this is something new to it from this little Wolf and Schaefer swimmer.",
            "How to calculate?",
            "So, so usually if in the presentations of regularize LDA you're left alone of how to select your perimetre.",
            "OK, so as I said, I.",
            "Who's targeting taking this same time as?",
            "For the other, so then we have still 15 minutes, so I will present 1 short topic at the end and then we have room for more questions and."
        ],
        [
            "Pick.",
            "So this is."
        ],
        [
            "One important point of.",
            "To understand what is what is going on here with the classification.",
            "So this is a critical problem if you use this advanced methods from machine learning.",
            "So you there are many ways where you can.",
            "Can fail and do things wrong, so therefore it's in any case very important that you try to understand what is going on.",
            "What is a classifier doing?",
            "But it turns out that this is not so easy.",
            "So first of all, so I put this here under the framework of spatial filters.",
            "So we can say that linear classifier is a spatial filter, so literally this is only true if he uses spatial features.",
            "So if you have from one time point.",
            "We take various of all channels.",
            "If we then train the classifier on those special features and the way tractor of the classifier has exactly the same dimensions as a number of channels.",
            "And it's applied to the data as a special filter."
        ],
        [
            "So here's one example from our speller data set in the very beginning.",
            "So these are this is Alpha matrix.",
            "We see the two intervals where we have nice differences, so we could train a classifier, for example based on this.",
            "Time interval and this would be a special filter.",
            "Or we could train the classifier on this time interval.",
            "So if you."
        ],
        [
            "Rain.",
            "Such a classifier.",
            "Send send it's a spatial filter and we could.",
            "After that we could just apply it to continuous data so we get new data continuously and we could use this weight vector of the classifier.",
            "And uses weights to wait this channels and we would get just one time signal here.",
            "So this is also something that you're going to do in the exercises in the practical session.",
            "So then this would put the the output of the classifier and we see it.",
            "Of course it.",
            "Next the classifier here was designed to maximize the difference here at this time interval 250 or something and the other classifier on the P3 component maximizes and here.",
            "And we can can visualize this special filter.",
            "So the weight vector of the classifier.",
            "We can visualize this as a topography here.",
            "So therefore it's.",
            "It's tempting to have an interpretation of what was learned by the classifier based on this topography, so here you get you see well where there are high rates of the classifier and where there are low weights and you see this also in papers quite often that people say OK, Now we check what the classifier does.",
            "Here's the topography and then they make the interpretation.",
            "But"
        ],
        [
            "This has to be done really carefully as I will show you here so I will make a very simple example to show what the problem here is.",
            "So we assume now very simple case.",
            "We have just two sources S1 and S2.",
            "And these are the patterns vector A1 and A2.",
            "Which say how these sources are propagated to the e.g senders.",
            "So this is multiplied with a one and this is with a 2 and this add up to the signal that we measure at some sensor AT2 sensors X.",
            "So these are all vectors of.",
            "BF are vectors of dimensionality tool.",
            "So now assume our task is to design A spatial filter that we want to apply to the data X to extract, say, source one.",
            "We want to discover source one.",
            "So applying vector spatial filter means we multiply this with a W transpose.",
            "So if we apply this W transpose to X.",
            "This distributes linearly to the two.",
            "Terms here.",
            "So if we want to recover S1, then we can distinguish the following two cases.",
            "So if that means we get need to get rid of the second term because here's the interfering source as two.",
            "So this filter needs to cancel out this part.",
            "So in the 1st.",
            "For the first case, is set.",
            "Um?",
            "This these two propagation vectors A1 and A2 are orthogonal.",
            "That means H1 transpose times a two is 0.",
            "So in this case we would choose W just to be a one.",
            "So if you see if you plug in a one here SW then here we have a one transpose times a two and this is zero if it's orthogonal.",
            "So then this cancels out.",
            "And here we have a one transpose, a one.",
            "This is just a number, so just a scaling.",
            "So we would recover as one.",
            "But in the in the other case.",
            "That it's that it's not orthogonal.",
            "We would need to choose W somehow that this is orthogonal, so we need to cancel out the second term so we have to choose W such that W transpose times a 20.",
            "So now it doesn't matter how we do it.",
            "Just remember we need to choose W such that W. Transpose times 820.",
            "Because this means set the filter the best filter W only depends on source 2.",
            "So we want to recover source as one, but we need the filter only depends on S2.",
            "So this a two.",
            "So the distribution vector that relates to the topography of this second source."
        ],
        [
            "So that.",
            "So assume S1 as a cognitive component P3 that we want to extract to do the classification and say as two.",
            "Is a source from the visual area that makes interference.",
            "So that is the optimal filter to get the PS3 component.",
            "Depends.",
            "Under just on the interference as two.",
            "We have two tools.",
            "Select the filter W such that it fulfilled fulfills a criterion based on S2.",
            "It means if you know, make a make an interpretation and we look at that.",
            "Topography of this spatial filter W. It would make interpretation.",
            "Not of the components that we are extracting, also P3, but we would make an interpretation of of the interfering sources.",
            "So this is.",
            "So this is this is an extreme case or in practical examples you have many different interfering sources and impractical.",
            "There's a tradeoff.",
            "Your weight vector of the classifier will also depend on the signal of interest, because it has to trade off.",
            "But still, it remains true that it also depends and may strongly depend on on the other components that do the interference."
        ],
        [
            "And this is another way too.",
            "To illustrate this, so here I have an example is very clean data.",
            "Potentially two electrodes and.",
            "Here we have a very nice classification problem and we can obtain a good classifier and now I artificially add some noise to it.",
            "So I just generated 10 Hertz signal and I add noise.",
            "This factor 0.42 channel CPZ and this factor points two to FC set."
        ],
        [
            "Then we get this distributions here, so I added some some noise and therefore of course there are gets worse.",
            "So here we had 15% error and here 33%.",
            "So this is not surprising, but now.",
            "This is when we classify adjust based on these two channels.",
            "But now if we get gives the classifier the chance also to get the signal from ozette.",
            "Then you'll get back even with this disturbance you get back, so original classification accuracy.",
            "Cause that."
        ],
        [
            "Pacifier can just use this information from Ozerden SBTRKT this, so that's quite obvious."
        ],
        [
            "What's going on is quite obvious, but for the interpretation this means that this classifier on these three channels will use.",
            "Some will use a channel that although channels, it has no discriminative information.",
            "Remember, I just artificially generated this or that general send channel with just some 10 Hertz oscillation, which didn't have anything to do here with this discriminative task.",
            "So this again shows that the best classifier will have positive weights or non zero weights on components it.",
            "Don't for in itself have no discriminative information, so it may use this information just to subtract noise.",
            "And therefore, in the interpretation you cannot say if you see some positive weight somewhere, you say.",
            "Ah, this classifier users or you can say the classifier uses this information of course, but you cannot say that from this area comes discriminative information."
        ],
        [
            "OK, so this.",
            "Well, this is just one final word, so here I am.",
            "This lecture I tried to convince you.",
            "2 uses multivariate features and machine learning techniques like this shrinkage, LDA classifier.",
            "So this gives increased sensitivity and will lead to better performance.",
            "Hopefully if you apply it wisely but.",
            "So keep keep in mind that you should be if you try to make interpretations, you should be.",
            "Be careful, but you should do interpretations.",
            "Don't treat this as a black box, but be careful.",
            "And I think in a later lecture on Thursday you will also learn about how you can regain interpretability of these classifiers.",
            "So we've seen directly interpreting these filters is critical, but there are techniques how to do it.",
            "OK.",
            "Yes, so this this concludes the material that I was trying to present.",
            "So there's there's room for some questions, but.",
            "At the same time, I would already distribute this sheet, so in order.",
            "For me to help me improve my my lecturing.",
            "There's some some feedback.",
            "This is called the Muddiest point feedback, so this is takes just half a minute to fill out, so it just says two points or.",
            "One thing is where you list things that have remained unclear, or where you would have wished to see more details, and the other part is for additional informal comments that you would like to give.",
            "So you can if you go out, you just put it here on on the disk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is overview.",
                    "label": 0
                },
                {
                    "sent": "What are presenting today so?",
                    "label": 0
                },
                {
                    "sent": "To set the scene for European analysis, I will shortly introduce a paradigm in of PCI where we need this single trial European analysis.",
                    "label": 0
                },
                {
                    "sent": "And Yep, eBay seller.",
                    "label": 0
                },
                {
                    "sent": "And I will show some standard analysis and then motivate Y for successful BCI classification.",
                    "label": 0
                },
                {
                    "sent": "We should move from so-called univariate multivariate features and then I will tell you how to classify these features and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We'll discuss some things about understanding what things are going on, their important remarks.",
                    "label": 0
                },
                {
                    "sent": "So this is a core part of the lecture, which intend to cover.",
                    "label": 0
                },
                {
                    "sent": "In any case, this.",
                    "label": 0
                },
                {
                    "sent": "I for sure I will not.",
                    "label": 0
                },
                {
                    "sent": "Be able to show everything that is here, but I put it on the slide so you can can look at look at those presentation afterwards and maybe one or two of these topics will also be.",
                    "label": 0
                },
                {
                    "sent": "Included in the lecture.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we Arkansas.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with noninvasive BCI.",
                    "label": 0
                },
                {
                    "sent": "So I guess you know all the settings the electrical brain activity is measured with electrodes from from this carb and transmitted to the computers and so there.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The part that is discussed today starts.",
                    "label": 0
                },
                {
                    "sent": "But first I should give an overview of what kind of occurrences of neural activity we measure there in the noninvasive.",
                    "label": 1
                },
                {
                    "sent": "Measurements so there are so called spontaneous oscillations.",
                    "label": 0
                },
                {
                    "sent": "Most importantly for Bcis sensory motor rhythms and Fabio ever talk about this in the next lecture?",
                    "label": 0
                },
                {
                    "sent": "So for us here in this topic, European analysis, so oscillations are more or less just noise and disturbing what we want to classify here.",
                    "label": 0
                },
                {
                    "sent": "And it's the most critical part.",
                    "label": 1
                },
                {
                    "sent": "Here is a visual Alpha risen.",
                    "label": 0
                },
                {
                    "sent": "This is strong 20 Hertz activity in the visual area.",
                    "label": 0
                },
                {
                    "sent": "That is most prominent when the eyes are closed, but even with eyes open there still.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of this 10 Hertz activity present.",
                    "label": 0
                },
                {
                    "sent": "So I will not talk about this.",
                    "label": 0
                },
                {
                    "sent": "I will also not talk about the second point here.",
                    "label": 0
                },
                {
                    "sent": "These are induced oscillations, for example, steady state visual evoked potentials.",
                    "label": 1
                },
                {
                    "sent": "If you look at a similar set.",
                    "label": 0
                },
                {
                    "sent": "Blinking with some periodicity will find signal oscillation of the same frequency in the visual cortex, so I will also not talk about this.",
                    "label": 0
                },
                {
                    "sent": "Our topic is a transient activity event related potentials chart.",
                    "label": 0
                },
                {
                    "sent": "I'm locked to Eve.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And usually external stimulus.",
                    "label": 0
                },
                {
                    "sent": "And if you look for description of ERP S you often see figure like this, which shows a typical time cause of ERP.",
                    "label": 0
                },
                {
                    "sent": "But typical here doesn't mean that you were in your day to observe something like this because he is a put in many different components.",
                    "label": 0
                },
                {
                    "sent": "So usually only observe few of those.",
                    "label": 0
                },
                {
                    "sent": "But this shows how come?",
                    "label": 0
                },
                {
                    "sent": "Concept so there are in this time course after the stimulus presentation that we are interested in, you see some peaks in the time cause positive and negative peaks, and this correspond to some neural processing of the of the event.",
                    "label": 0
                },
                {
                    "sent": "So typically these are different aspects and probably.",
                    "label": 0
                },
                {
                    "sent": "Originate from from different regions in the brain.",
                    "label": 1
                },
                {
                    "sent": "So if you would look at topography's here of these components, I would probably look different and generally very early components reflect the processing of the physical stimulus and the mid latency components.",
                    "label": 0
                },
                {
                    "sent": "Lower level cognitive.",
                    "label": 0
                },
                {
                    "sent": "Processes and later ones are higher cognitive processes.",
                    "label": 0
                },
                {
                    "sent": "So but",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't need to know much about this, so here I will show you the.",
                    "label": 0
                },
                {
                    "sent": "Thing that is important for this lecture.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I will motivate the paradigm of ERP based BCI spellers.",
                    "label": 0
                },
                {
                    "sent": "So the the basic paradigm is called oddball paradigm and in a oddball paradigm you have, let's say acoustic stimuli.",
                    "label": 0
                },
                {
                    "sent": "Here you have two types of stimuli.",
                    "label": 0
                },
                {
                    "sent": "There's a low tone and more infrequent high tone.",
                    "label": 0
                },
                {
                    "sent": "So there's a sequence of low tones, interferes with his infrequent occurrences of high tones, and the participant pays attention to the tones, and so there will be some neural processes that.",
                    "label": 0
                },
                {
                    "sent": "Correspond to the detection of this deviation that there is deviation in sequence on the regularity.",
                    "label": 0
                },
                {
                    "sent": "So in this case we will at the central electrodes is at sisam.",
                    "label": 0
                },
                {
                    "sent": "Typically positive deflection.",
                    "label": 0
                },
                {
                    "sent": "Which is the P300 component?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the same concept works in the visual domain, so here we have frequently green dots presented and interspersed with some red squares.",
                    "label": 1
                },
                {
                    "sent": "So and this time it reverses, which are shaded here correspond to the time intervals where we expect specific processing of these stimuli, so these are called the pox or single trials, and the technical domain, so our interests would would be to discriminate these two types of epox the types that correspond to processing of a standard stimulus, and those two examples here.",
                    "label": 0
                },
                {
                    "sent": "So epoch set correspond to processing of deviant stimuli.",
                    "label": 0
                },
                {
                    "sent": "But even if you this is, this is possible in this paradigm, so this doesn't make a BCI yet.",
                    "label": 0
                },
                {
                    "sent": "'cause if even if we can decode this and discriminate, then we would only get some information that is already in the sequence.",
                    "label": 0
                },
                {
                    "sent": "So we already from presenting the sequence.",
                    "label": 0
                },
                {
                    "sent": "We already know the time points of the divisions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In order to make it interesting for BC.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I-1 is a different paradigm, so here are.",
                    "label": 0
                },
                {
                    "sent": "More groups of possible stimuli.",
                    "label": 0
                },
                {
                    "sent": "And now there's a specific attention task for the participant's attention, for example.",
                    "label": 1
                },
                {
                    "sent": "To attend, for example the Red Square.",
                    "label": 1
                },
                {
                    "sent": "Then we would call this stimulus the target stimulus and all other which of different of different types or others are non targets.",
                    "label": 0
                },
                {
                    "sent": "And if we can.",
                    "label": 0
                },
                {
                    "sent": "Get this information from the from the e.g.",
                    "label": 1
                },
                {
                    "sent": "This is not something that we can use for BCI because now we can.",
                    "label": 0
                },
                {
                    "sent": "Transmit some information to it so that there's a fixed or.",
                    "label": 0
                },
                {
                    "sent": "Similar sequence, and it's only by the attention of the participant makes it discrimination between targets and non targets.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, this leads to a simple design of a spell offer BCI where you can write letters just with attention measured brain activities.",
                    "label": 1
                },
                {
                    "sent": "So I'll say the user here wants to write the letter N to complete the word brain here.",
                    "label": 0
                },
                {
                    "sent": "Then with the very same paradigm we could just present or letters of the alphabet in rapid succession and the user would concentrate on the occurrences of the N. And each time there's a N, we will see this bingo component and CR.",
                    "label": 0
                },
                {
                    "sent": "He did they think he or she detected?",
                    "label": 1
                },
                {
                    "sent": "The occurrence of the target letter, and then if we can find this information in the EG and successfully classified, then this letter can be selected.",
                    "label": 0
                },
                {
                    "sent": "I go back so 11 difference to the to the standard paradigm or paradigm.",
                    "label": 0
                },
                {
                    "sent": "Here is also that user presentation is much faster than the typical paradigms, so therefore I didn't shave.",
                    "label": 0
                },
                {
                    "sent": "Here's a nontarget epoch's because they are strongly overlapping, so you do the first presentation to have fast communication that is clear.",
                    "label": 1
                },
                {
                    "sent": "But this makes the interpretation and the classification of the components of course more difficult.",
                    "label": 0
                },
                {
                    "sent": "Because they are mixed and overlapped.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and so just another figure.",
                    "label": 0
                },
                {
                    "sent": "This is the same but just showing that we will have this information from from many channels which will be will become very important.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "So what would be a standard analysis of such an European experiment?",
                    "label": 0
                },
                {
                    "sent": "So you look at this.",
                    "label": 0
                },
                {
                    "sent": "Event related potentials.",
                    "label": 0
                },
                {
                    "sent": "So here you.",
                    "label": 0
                },
                {
                    "sent": "And collect, or the pox from the experiment.",
                    "label": 0
                },
                {
                    "sent": "So you do this for a long time and then you have many epox corresponding to target stimuli and many box corresponding to nontarget stimuli.",
                    "label": 0
                },
                {
                    "sent": "And then you average across all these epoch's.",
                    "label": 0
                },
                {
                    "sent": "So we will see this in more detail later but this.",
                    "label": 0
                },
                {
                    "sent": "Everything we get rid of the non task related components, so this will average out and we will only see the source components that are related to the processing of the event.",
                    "label": 0
                },
                {
                    "sent": "So now we can compare.",
                    "label": 0
                },
                {
                    "sent": "So we are interested in the difference between these two conditions, targets and non targets.",
                    "label": 0
                },
                {
                    "sent": "So therefore it might be useful to shade here the area where the two curves differ.",
                    "label": 0
                },
                {
                    "sent": "So here's a thread shading.",
                    "label": 0
                },
                {
                    "sent": "If the targets are more positive and blue shading is there more, more negative.",
                    "label": 0
                },
                {
                    "sent": "So here one can already see that.",
                    "label": 0
                },
                {
                    "sent": "Here's a prominent difference, centrally, where the targets are more positive and he left at the back occipitale in the vision area, it's more negative.",
                    "label": 0
                },
                {
                    "sent": "But in order to get a better overview of this.",
                    "label": 0
                },
                {
                    "sent": "Topographic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution sorry, one slide before so often run.",
                    "label": 0
                },
                {
                    "sent": "This place is also in a grid plot.",
                    "label": 1
                },
                {
                    "sent": "These are the same plots as before, but now just arranged in a grid.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then to get a better impression of topography of these components, one plus is as Maps.",
                    "label": 0
                },
                {
                    "sent": "So here on top you again see the time course of European two channels.",
                    "label": 0
                },
                {
                    "sent": "And here you see is shaded for different time intervals and for exactly this time intervals you see the topography.",
                    "label": 0
                },
                {
                    "sent": "Here that means we we average within this time interval and then we get one value for each channel.",
                    "label": 0
                },
                {
                    "sent": "One voltage value and this is now interpolated as its topography, so this is a top view in the head and this is color coded.",
                    "label": 0
                },
                {
                    "sent": "Negative potential blue positive in red and this is done separately for the two conditions and for all this for shaded.",
                    "label": 0
                },
                {
                    "sent": "Intervals so all these.",
                    "label": 0
                },
                {
                    "sent": "Components now give information that is important for classification tasks.",
                    "label": 0
                },
                {
                    "sent": "You see that for each of these time intervals, there's a clear difference between the two conditions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was the first part of the standard European analysis.",
                    "label": 0
                },
                {
                    "sent": "Then if you look at papers of your peer analysis when they quantify the difference between targets and non targets, you will often see graphs like this.",
                    "label": 0
                },
                {
                    "sent": "So this is a amplitude of the P3 component.",
                    "label": 0
                },
                {
                    "sent": "Here at one channel I didn't put it here, so at channel sees it for example and then you have fears of mean value for the targets is 7 microvolts and for non targets 0.5 and.",
                    "label": 0
                },
                {
                    "sent": "This gives the standard deviation and you can also do statistical tests.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what I would call the standard analysis.",
                    "label": 0
                },
                {
                    "sent": "Of your piece, and this already gives both gives important information, but for Bcis we have to go one step further because we need to classify single trials.",
                    "label": 0
                },
                {
                    "sent": "So not not see the difference on averages, but we need for each single trial we need to decide whether it's a belongs to the target or non target.",
                    "label": 0
                },
                {
                    "sent": "In order to control the feedback.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Application.",
                    "label": 0
                },
                {
                    "sent": "So and why we call this what we've seen before univariate features.",
                    "label": 0
                },
                {
                    "sent": "I will now introduce and motivate why we have to use those multivariate features.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but we start from this.",
                    "label": 0
                },
                {
                    "sent": "Univariate features, so our task is to distinguish target from non target epox or trials.",
                    "label": 1
                },
                {
                    "sent": "And from from the literature and also the graph that I've shown you, we see that.",
                    "label": 0
                },
                {
                    "sent": "The best discrimination is probably can be gained from the P3 component, which has its focus on the electrode set and.",
                    "label": 1
                },
                {
                    "sent": "The peak latency of about 300 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "If we want to discriminate targets and non targets of best first guess would be that we use this amplitude.",
                    "label": 0
                },
                {
                    "sent": "In the channels is set at the peak time, also P3, so this is what this barplot before was based on.",
                    "label": 0
                },
                {
                    "sent": "And of course, we could also use this for.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suffication so here you see average year piece for the targets and non targets.",
                    "label": 0
                },
                {
                    "sent": "And here's a time point of the peak.",
                    "label": 0
                },
                {
                    "sent": "The highest voltage here in this time course.",
                    "label": 0
                },
                {
                    "sent": "Which is in this experiment at 220 milliseconds, electrodes he said.",
                    "label": 0
                },
                {
                    "sent": "And you see that there is a difference.",
                    "label": 1
                },
                {
                    "sent": "Indeed, the amplitude is higher.",
                    "label": 1
                },
                {
                    "sent": "For targets and for non targets, or there's hope that we can classify something and so this would be called univariate feature.",
                    "label": 0
                },
                {
                    "sent": "Univariate means it's just one value that you have there for each trial.",
                    "label": 0
                },
                {
                    "sent": "But so these are again averages.",
                    "label": 0
                },
                {
                    "sent": "So the strategy would be, for example to put a threshold between the two means here and say everything.",
                    "label": 1
                },
                {
                    "sent": "Above is classified as target and everything below is non target.",
                    "label": 0
                },
                {
                    "sent": "So, but here you see the.",
                    "label": 0
                },
                {
                    "sent": "The causes time costs for the single choice.",
                    "label": 0
                },
                {
                    "sent": "So I hear randomly selected 10 single trials and here you see the voltage value at this peak latency.",
                    "label": 0
                },
                {
                    "sent": "Where we would.",
                    "label": 0
                },
                {
                    "sent": "Base or classification based on this simple strategy on so and the single trials look very much different and deviate very much from the from the average variable becausw.",
                    "label": 0
                },
                {
                    "sent": "The brain is very, very active each moment in time.",
                    "label": 0
                },
                {
                    "sent": "There are many processes going on.",
                    "label": 0
                },
                {
                    "sent": "Each time that are not related to the events that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So therefore we have.",
                    "label": 0
                },
                {
                    "sent": "This large varieties, so let's let's see how classification strategy would look like now.",
                    "label": 0
                },
                {
                    "sent": "So here you have the points of this single trials here, so we've seen that.",
                    "label": 0
                },
                {
                    "sent": "So there are two targets classified with this strategy as non targets.",
                    "label": 0
                },
                {
                    "sent": "And also here are two non targets classified as targets.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "It is for sure better than guessing, but not very powerful, so we should improve on that.",
                    "label": 0
                },
                {
                    "sent": "But before I will show you how to improve, we will discuss measures of.",
                    "label": 0
                },
                {
                    "sent": "Separability, so here we could say, for example this.",
                    "label": 0
                },
                {
                    "sent": "60% correct and 40% wrong, but often 1 uses.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are there also other measures for discriminability?",
                    "label": 0
                },
                {
                    "sent": "So I just mentioned those so it's not.",
                    "label": 0
                },
                {
                    "sent": "The details are not important here, so they are often used as a measure, so ask fair value which is defined like this.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the square difference of the of the means of in these two conditions divided by the by the variance of all values where you put all two classes together and there's some normalization such that this.",
                    "label": 0
                },
                {
                    "sent": "Coefficient is between minus one and one, so this is not important importance that you get this message below.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "They put two distributions are clearly separated and don't have much variance within each condition.",
                    "label": 0
                },
                {
                    "sent": "And if the.",
                    "label": 0
                },
                {
                    "sent": "One class says that the English class here is more negatives and they ask where value will be something like.",
                    "label": 0
                },
                {
                    "sent": "Minus one if they are completely mixed then they ask where value will be around 0.",
                    "label": 0
                },
                {
                    "sent": "This is non discriminative and the other extreme of separation spends orange classes positive and then the coefficient will be around.",
                    "label": 0
                },
                {
                    "sent": "Plus one.",
                    "label": 0
                },
                {
                    "sent": "And I.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So introduce another measure of separability.",
                    "label": 1
                },
                {
                    "sent": "Which is the area under the RC curve?",
                    "label": 1
                },
                {
                    "sent": "And so this is just a.",
                    "label": 0
                },
                {
                    "sent": "Very short introduction on how you could calculate it, so this depends only on the on the order.",
                    "label": 0
                },
                {
                    "sent": "Office this values so this other.",
                    "label": 0
                },
                {
                    "sent": "Amplitudes that we would.",
                    "label": 0
                },
                {
                    "sent": "Use for classification, so once you start from below here and you go in this grid.",
                    "label": 0
                },
                {
                    "sent": "Up one step each time you see a blue dot here and if you see a orange dot, you go to the right.",
                    "label": 0
                },
                {
                    "sent": "And then at some point if you have so, the grid depends on the number of points and then you arrive at the upper right corner.",
                    "label": 0
                },
                {
                    "sent": "And this area under the curve is.",
                    "label": 1
                },
                {
                    "sent": "Yeah, this is an area under the curve, so here you would just count the number of of squares and this also gives a good measure of separation.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, this is what you need to remember.",
                    "label": 0
                },
                {
                    "sent": "So if there are in the order such that the orange are all below the blue.",
                    "label": 0
                },
                {
                    "sent": "Then you will get this curve and the area is 0 below the curve.",
                    "label": 0
                },
                {
                    "sent": "If they are completely mixed then you go randomly up, right, up right and you will get something like the diagonal line.",
                    "label": 0
                },
                {
                    "sent": "So the area will be 1/2.",
                    "label": 0
                },
                {
                    "sent": "And if the orders like this, then you will go up, up, up and then right, right, right and the area is 1.",
                    "label": 0
                },
                {
                    "sent": "So these are the extreme about separation, and this is the random case.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit and different since you asked fair value 'cause here it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You're just the order matters, not how much variance is within the classes.",
                    "label": 0
                },
                {
                    "sent": "And in our data sets, the curve looks like this and we get our see value of 0.7.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can.",
                    "label": 0
                },
                {
                    "sent": "Add something some more information to our standard analysis.",
                    "label": 0
                },
                {
                    "sent": "So and in order to understand that so first.",
                    "label": 0
                },
                {
                    "sent": "Note that we so this is like a one one single trial, one epoch.",
                    "label": 0
                },
                {
                    "sent": "So here you have the time.",
                    "label": 0
                },
                {
                    "sent": "Here's stimulus presentation and this is like 1 second around.",
                    "label": 0
                },
                {
                    "sent": "The stimulus presentation.",
                    "label": 0
                },
                {
                    "sent": "And here's a time courses for all channels, but of course we can also display this as a matrix, so this is the same here, but you have to roll for each channel and now these amplitude values here are color coded.",
                    "label": 0
                },
                {
                    "sent": "So we will often use this representation also.",
                    "label": 0
                },
                {
                    "sent": "And so now we have such matrices for each trial for each epoch.",
                    "label": 0
                },
                {
                    "sent": "So we have a sequence here and we know whether it's corresponds to a target or non target.",
                    "label": 0
                },
                {
                    "sent": "And now for each element in this matrix for each channel and each timepoint.",
                    "label": 0
                },
                {
                    "sent": "We have no values for targets and non targets and then we can apply this measure of discriminability.",
                    "label": 0
                },
                {
                    "sent": "Then we would get one AOC.",
                    "label": 0
                },
                {
                    "sent": "Value and we put it as the corresponding point in this matrix.",
                    "label": 0
                },
                {
                    "sent": "So for each time point on each channel, this value here says how well does exactly this time point in this channel discriminate between the two conditions?",
                    "label": 0
                },
                {
                    "sent": "So if there's something right here, we know this channel or you know here around 0 there's no discriminability.",
                    "label": 0
                },
                {
                    "sent": "And so we get a nice overview.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can now enhance our standard analysis by putting below each channel.",
                    "label": 0
                },
                {
                    "sent": "Here one row of symmetric matrix that we've seen before.",
                    "label": 0
                },
                {
                    "sent": "So for each channel.",
                    "label": 0
                },
                {
                    "sent": "We know for every time points our see value and we put it below here and this gives send some more statistic, reliably indication of how well.",
                    "label": 0
                },
                {
                    "sent": "Each channel and each time point discriminates between the targets and non targets.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course we can do the very same thing also photographies.",
                    "label": 0
                },
                {
                    "sent": "So we have this ask where values for each time point and each channel, so we can use the same time intervals that we had before for plotting.",
                    "label": 0
                },
                {
                    "sent": "And then plot this ask where Values's topography's and then this gives quite clear.",
                    "label": 0
                },
                {
                    "sent": "Indication of the difference.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just one final example of how to display this this analysis.",
                    "label": 0
                },
                {
                    "sent": "So here's this topografie's are exactly the same as before, but here I put on top so this full matrix of AOC values.",
                    "label": 0
                },
                {
                    "sent": "Time points cross channels.",
                    "label": 0
                },
                {
                    "sent": "And then here you get.",
                    "label": 0
                },
                {
                    "sent": "So overview of all information that we have available and you can see also how this evolves in time, the differences and so on.",
                    "label": 0
                },
                {
                    "sent": "So you could use this matrix also to select the most discriminative univariate feature and say maybe six years of area with a with a higher score.",
                    "label": 1
                },
                {
                    "sent": "So that is most near to 0 to one and say OK, no, I don't know we have this time point and this channel and this is the best univariate feature.",
                    "label": 0
                },
                {
                    "sent": "So this is certainly.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Useful, but I will.",
                    "label": 0
                },
                {
                    "sent": "Show you that one should use multivariate features and how and in order to provide the background for this multivariate features.",
                    "label": 1
                },
                {
                    "sent": "I will tell you a little bit about the generation of e.g signals.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Signals that we measure with EG electrodes.",
                    "label": 0
                },
                {
                    "sent": "This is something electrical currents from that is generated by pyramidal neurons in the cortex.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The reason that we can we can measure this activity on this carpet at these these special kinds is pyramidal cells orientated in parallel.",
                    "label": 1
                },
                {
                    "sent": "In this this cortex, so the current sum up and become measurable.",
                    "label": 0
                },
                {
                    "sent": "Yet this club and also their their dendrites, run perpendicular to the.",
                    "label": 0
                },
                {
                    "sent": "To this cult.",
                    "label": 0
                },
                {
                    "sent": "So therefore we can.",
                    "label": 0
                },
                {
                    "sent": "Measure this very well and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now comes the important point here.",
                    "label": 0
                },
                {
                    "sent": "So of course in the in the brain we don't have wires that go from the source here, but the electrical activity.",
                    "label": 0
                },
                {
                    "sent": "Has to arrive somehow.",
                    "label": 0
                },
                {
                    "sent": "Here we want to measure it and it's transmitted here by so called volume conduction.",
                    "label": 1
                },
                {
                    "sent": "So the tissues here the brain, the fluid and the scars.",
                    "label": 0
                },
                {
                    "sent": "Are they all have some conductivity and say transmit?",
                    "label": 0
                },
                {
                    "sent": "The information but, and this is not important to conductivity is very different between the tissues.",
                    "label": 0
                },
                {
                    "sent": "So the brain inside the wet part is a very good conductor.",
                    "label": 0
                },
                {
                    "sent": "While this car is a very bad conductor and this is a very important consequence.",
                    "label": 0
                },
                {
                    "sent": "So if we look at this sales here, this population and how we would measure their electrical signal at these electrodes.",
                    "label": 0
                },
                {
                    "sent": "And the intensity with which it arrives at CG sensors doesn't differ very much cause the most difference in the distance is here inside the brain.",
                    "label": 0
                },
                {
                    "sent": "But there is a conductivity is so good that it doesn't make a big difference whether the signal has to travel only the short way or the wrong way, because compared to the hard way of the signal to get through the.",
                    "label": 0
                },
                {
                    "sent": "Skull.",
                    "label": 0
                },
                {
                    "sent": "It has a conductivity is very bad.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This is comparably nothing or easy and the way through the skull doesn't depend on this.",
                    "label": 0
                },
                {
                    "sent": "Distances.",
                    "label": 0
                },
                {
                    "sent": "So therefore we will get the signal from from one source with almost the same intensity at different electrodes.",
                    "label": 1
                },
                {
                    "sent": "So that means we don't have much spatial information in it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This you can also check if you just take some e.g data and here I calculated the correlation coefficient of the signal that is measured centrally at these at with the signals from all other electrodes and this correlation coefficient here.",
                    "label": 0
                },
                {
                    "sent": "Is put here in true colors and you see that almost all channels have a correlation coefficient of more than .9 Mrs Central electron.",
                    "label": 0
                },
                {
                    "sent": "So that means the spatial information is very limited, at least in raiji.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this as a background, we can all discuss how to proceed from the univariate feature to multivariate features.",
                    "label": 1
                },
                {
                    "sent": "So one thing where we can extend that we are not only used one time point, not only.",
                    "label": 0
                },
                {
                    "sent": "The P3 peak time point, for example.",
                    "label": 1
                },
                {
                    "sent": "But you can use multiple time points or intervals.",
                    "label": 1
                },
                {
                    "sent": "These are called temporal features, or you can put together the information from many many channels, but still only one time for and this would be called special features.",
                    "label": 0
                },
                {
                    "sent": "All we do both these are called Special Temple.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature.",
                    "label": 0
                },
                {
                    "sent": "So why does this information help?",
                    "label": 0
                },
                {
                    "sent": "So now we do.",
                    "label": 0
                },
                {
                    "sent": "The comparison only takes the univariate information from one channel, one time point and compared to multivariate.",
                    "label": 0
                },
                {
                    "sent": "So assume this setting you have a, so the ERP speller setting where we want to discriminate targets from non targets.",
                    "label": 0
                },
                {
                    "sent": "And this is a cognitive task.",
                    "label": 1
                },
                {
                    "sent": "So assume we have to.",
                    "label": 0
                },
                {
                    "sent": "Assess to the to the source signal in the brain so we don't have, but assume we have this and this might look.",
                    "label": 0
                },
                {
                    "sent": "Like this in a single trial, you have to speak peak.",
                    "label": 0
                },
                {
                    "sent": "So that in the G is seen as P300 component.",
                    "label": 0
                },
                {
                    "sent": "And so here we would have the threshold as in the example before.",
                    "label": 0
                },
                {
                    "sent": "So this point here is above the threshold.",
                    "label": 0
                },
                {
                    "sent": "So this is correctly classified.",
                    "label": 0
                },
                {
                    "sent": "But it's the same time in the brain.",
                    "label": 0
                },
                {
                    "sent": "There's this spontaneous activity.",
                    "label": 0
                },
                {
                    "sent": "For example, this visual Alpha rhythm that I talked about in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So you have 10 Hertz or solation here, originating from another place.",
                    "label": 0
                },
                {
                    "sent": "And now due to this volume conduction that I explained before.",
                    "label": 0
                },
                {
                    "sent": "At these electrodes, that which is above his cognitive source and Atos at which is above the visual area, we don't measure exactly this source signal, but a mixture between the signals.",
                    "label": 0
                },
                {
                    "sent": "So if we only take into 'cause this tool, you are made an example that I deliberately take Factor 1 here.",
                    "label": 0
                },
                {
                    "sent": "For the source to be transmitted to the electrode, buffet and factor 1/2 that here, for example, is the visual.",
                    "label": 0
                },
                {
                    "sent": "Rhythm also.",
                    "label": 0
                },
                {
                    "sent": "Is measured at these at.",
                    "label": 0
                },
                {
                    "sent": "So what we measure here is now this mixtures it is shown here in the magenta color.",
                    "label": 0
                },
                {
                    "sent": "And also knows that we receive a mixture so he is not so clearly visible, but it's the same mixture.",
                    "label": 0
                },
                {
                    "sent": "Just with different weightings.",
                    "label": 0
                },
                {
                    "sent": "So no, no if we are unlucky and this.",
                    "label": 0
                },
                {
                    "sent": "Time point where we we take our univariate features exactly where we are.",
                    "label": 0
                },
                {
                    "sent": "We have a minimum value here and the.",
                    "label": 0
                },
                {
                    "sent": "Ongoing oscillation.",
                    "label": 0
                },
                {
                    "sent": "Then this time point might be below our threshold and this would be a misclassification.",
                    "label": 0
                },
                {
                    "sent": "And if we only have this univariate feature, there's no chance to discriminators.",
                    "label": 0
                },
                {
                    "sent": "But if we take a multivariate feature and we take into account also what we measure its channels it.",
                    "label": 1
                },
                {
                    "sent": "Then there would be a awaiting spatial filter which here just calculated because I know the mixture where we can reconstruct the original cognitive source.",
                    "label": 0
                },
                {
                    "sent": "So if I take.",
                    "label": 0
                },
                {
                    "sent": "For over 3 times channel these add and subtract 2 / 3 the information from ozette.",
                    "label": 0
                },
                {
                    "sent": "We would exactly reconstruct this and then we would be able to classify correctly.",
                    "label": 0
                },
                {
                    "sent": "So but here's we could save the classification only be cause we had a multivariate feature.",
                    "label": 0
                },
                {
                    "sent": "We also talks information from another channel.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similarly, if we.",
                    "label": 0
                },
                {
                    "sent": "Don't use this channel all set, but we use multiple time points.",
                    "label": 1
                },
                {
                    "sent": "We could also be safe here in this context, so it's again the same setting.",
                    "label": 0
                },
                {
                    "sent": "Mrs Miss classification here.",
                    "label": 0
                },
                {
                    "sent": "But if you know would use for example, four time points here around this peak and just average across those time points and we would get rid of these ups and downs that come from the visual Idol rhythm and we would obtain.",
                    "label": 0
                },
                {
                    "sent": "Good classification.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this temporal information also holds for other disturbances, like what we often see in e.g.",
                    "label": 0
                },
                {
                    "sent": "In particular, if the person is, if it's hot and it is there some sweat on the skin on the Skype and you often see this drift.",
                    "label": 0
                },
                {
                    "sent": "And this drifts can also, of course, cause Mystic misclassification.",
                    "label": 0
                },
                {
                    "sent": "But if you have a temporal filter that somehow weights different time points, you could can also get rid of the drift and savior classification.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this means it's it's a good idea to include more channels and more time points to the classification, and so generally for EPS we use so-called spatial temporal features, which.",
                    "label": 0
                },
                {
                    "sent": "Exactly means.",
                    "label": 0
                },
                {
                    "sent": "That you use more channels and more time points.",
                    "label": 0
                },
                {
                    "sent": "And this shows how.",
                    "label": 0
                },
                {
                    "sent": "How this is done.",
                    "label": 0
                },
                {
                    "sent": "You just take several channels.",
                    "label": 0
                },
                {
                    "sent": "This may be a subset, or maybe all channels and then you take several time intervals.",
                    "label": 0
                },
                {
                    "sent": "Or you just do subsampling or what else?",
                    "label": 0
                },
                {
                    "sent": "And then you have this matrix of values for all your channels and all the time points.",
                    "label": 0
                },
                {
                    "sent": "And all these various in the matrix you put on a vector and this is in what we call feature vector and this is now.",
                    "label": 0
                },
                {
                    "sent": "The quantification of the information.",
                    "label": 0
                },
                {
                    "sent": "About this one, try.",
                    "label": 0
                },
                {
                    "sent": "So this contains temporal information, spatial information and therefore.",
                    "label": 0
                },
                {
                    "sent": "Should be a good basis for classification.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So special cases are the temporal features.",
                    "label": 0
                },
                {
                    "sent": "If you only take one channel, so we then take just one time course.",
                    "label": 0
                },
                {
                    "sent": "As a feature vector, this is called temporal feature.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If you take many channels or say all channels but just one time point then we get one value per channel, so this is called special feature.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to sum this up, if you have one, one epoch of EG.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 short time segment related to similar processing.",
                    "label": 0
                },
                {
                    "sent": "We would.",
                    "label": 0
                },
                {
                    "sent": "Displaying in the signals as as a matrix and then the features would be if we take one row of this matrix.",
                    "label": 0
                },
                {
                    "sent": "This is a temporal feature.",
                    "label": 0
                },
                {
                    "sent": "If you take a column of this matrix, it's a special feature.",
                    "label": 0
                },
                {
                    "sent": "And if you take the full matrix, or maybe subsample, use only few channels and few time points, this is a spatial temporal feature.",
                    "label": 0
                },
                {
                    "sent": "So it's a temporary feature correspond to a time 'cause the spatial feature to a topography and spatial temporal feature to a sequence of topography's.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So not too.",
                    "label": 0
                },
                {
                    "sent": "Put a bit.",
                    "label": 0
                },
                {
                    "sent": "Backgrounds to it so we can.",
                    "label": 0
                },
                {
                    "sent": "So often one assumes the following model for earpiece that we have.",
                    "label": 0
                },
                {
                    "sent": "In the brain signals we have so-called timelocked activity.",
                    "label": 0
                },
                {
                    "sent": "And non time logged activity.",
                    "label": 0
                },
                {
                    "sent": "So time log means if.",
                    "label": 0
                },
                {
                    "sent": "If the stimulus is repeated several times, we will get the same similar potential at certain latency's, say always 300 milliseconds after the stimulus, we have a more positive peaks.",
                    "label": 0
                },
                {
                    "sent": "This is an timelocked activity.",
                    "label": 0
                },
                {
                    "sent": "And all spontaneous activity is non timelocked.",
                    "label": 0
                },
                {
                    "sent": "So in this model, so X of K is the signal in the case trials that we measure.",
                    "label": 0
                },
                {
                    "sent": "And P of T know now would be the time locked activity, the ERP and the residual activities called here are of K and the model or the simplifying assumption here for earpieces.",
                    "label": 1
                },
                {
                    "sent": "At the time locked activity the ERP is the same in every trial so therefore I didn't put the index K here.",
                    "label": 1
                },
                {
                    "sent": "So in every trial you have this the same, say cognitive source active and every time in the same way.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit simplified cause might depends also on vigilance and different factors, but simplifying you can say this is the same and we have this residual activity which is a background, EG.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Then if we have this splitting, then we know that the if we average across trials the residual activity there should be 0, because this exactly means non time lock.",
                    "label": 0
                },
                {
                    "sent": "If we average across trials, nothing is left is just random activities that cancels out.",
                    "label": 0
                },
                {
                    "sent": "And therefore this residual activity.",
                    "label": 0
                },
                {
                    "sent": "If we put this in a probability framework, this is.",
                    "label": 0
                },
                {
                    "sent": "Distribution here is a Gaussian distribution with mean zero.",
                    "label": 1
                },
                {
                    "sent": "This is long time locked and some covariance matrix, so this is often assumed and empirically found that this noise in the EG is Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "And so now comes the important consequence.",
                    "label": 0
                },
                {
                    "sent": "If you know, look at the distribution of the e.g signals itself.",
                    "label": 0
                },
                {
                    "sent": "Say we take one tick fixed time point and look at the distribution over trials.",
                    "label": 0
                },
                {
                    "sent": "And if you calculate the mean of this X of T0.",
                    "label": 0
                },
                {
                    "sent": "Then we get out only the piece the ERP becausw.",
                    "label": 0
                },
                {
                    "sent": "There are has been zero, so this cancels out, so the mean.",
                    "label": 0
                },
                {
                    "sent": "Is that your P itself?",
                    "label": 0
                },
                {
                    "sent": "And so covariance.",
                    "label": 0
                },
                {
                    "sent": "Of this this trial.",
                    "label": 0
                },
                {
                    "sent": "So if you calculate the covariance, you subtract the mean so we would subtract the mean here.",
                    "label": 0
                },
                {
                    "sent": "If we subtract the P from the X, we only.",
                    "label": 1
                },
                {
                    "sent": "Have left the residual.",
                    "label": 0
                },
                {
                    "sent": "So therefore the covariance is.",
                    "label": 0
                },
                {
                    "sent": "Covariance of this residual activity.",
                    "label": 0
                },
                {
                    "sent": "So this is important consequences if you have this ERP model you say the mean of your distribution is the ERP itself, and so this is a signal we are interested in and the covariance is the noise.",
                    "label": 0
                },
                {
                    "sent": "So the covariance has nothing to do with the signal.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here for illustration, same thing.",
                    "label": 0
                },
                {
                    "sent": "So here on the left we have the signal X the signal itself that we measure.",
                    "label": 0
                },
                {
                    "sent": "And here we have the ERP and we're here.",
                    "label": 0
                },
                {
                    "sent": "We have serves, annoys the residual so that sex is always the sum of the European to residual.",
                    "label": 0
                },
                {
                    "sent": "So this is a magenta colored line.",
                    "label": 0
                },
                {
                    "sent": "And this these are different trials.",
                    "label": 0
                },
                {
                    "sent": "So in each trial the ERP is the same and the noise is different.",
                    "label": 0
                },
                {
                    "sent": "And if we average.",
                    "label": 0
                },
                {
                    "sent": "Vertically across trials.",
                    "label": 0
                },
                {
                    "sent": "Then the residual activity would average out and would be.",
                    "label": 0
                },
                {
                    "sent": "Eventually be a zero line, so he only took eight trials, so therefore it's not exactly 0, but this would be zero line.",
                    "label": 0
                },
                {
                    "sent": "Everything the constant signal gives the appear.",
                    "label": 0
                },
                {
                    "sent": "So therefore, if you do this for the X.",
                    "label": 0
                },
                {
                    "sent": "If we average across enough large number of trials, we will get out the P. So this is what was shown in formulas here.",
                    "label": 0
                },
                {
                    "sent": "So this is a basic office standard ERP analysis we average across trials and then.",
                    "label": 1
                },
                {
                    "sent": "We recall what we get here.",
                    "label": 0
                },
                {
                    "sent": "We call the ERP.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So no fuck for classification.",
                    "label": 0
                },
                {
                    "sent": "It's important that we know the distributions of the two conditions, so you will see this in detail later, But so we need to know distributions.",
                    "label": 0
                },
                {
                    "sent": "So I will show you how to visualize distributions and find out characterize distributions.",
                    "label": 0
                },
                {
                    "sent": "So first here we start very simple with a 2 dimensional feature.",
                    "label": 0
                },
                {
                    "sent": "So we take care.",
                    "label": 0
                },
                {
                    "sent": "One channel here took 55 and two time points.",
                    "label": 0
                },
                {
                    "sent": "And now our feature is this vector.",
                    "label": 0
                },
                {
                    "sent": "Value at P5 at time point 180 and value at P 5 * .450.",
                    "label": 0
                },
                {
                    "sent": "So we have a 2 dimensional feature vector.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we want to make a so called scatter plot.",
                    "label": 0
                },
                {
                    "sent": "So here we will make a point in.",
                    "label": 0
                },
                {
                    "sent": "This Patch is better scatter plot for each trial.",
                    "label": 0
                },
                {
                    "sent": "And on the 1 axis we have one feature and on the other axis we have certain feature.",
                    "label": 0
                },
                {
                    "sent": "So how would this look like?",
                    "label": 0
                },
                {
                    "sent": "So here we have channel, he remarked the early time point and he remarked the late time point.",
                    "label": 0
                },
                {
                    "sent": "Now it turns this by 90 degrees I rotated.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "The value that we have here we take as Y axis and the value we have here we take as X axis.",
                    "label": 0
                },
                {
                    "sent": "So we put the cross here.",
                    "label": 0
                },
                {
                    "sent": "So this one other ear piece.",
                    "label": 0
                },
                {
                    "sent": "So this is a mean.",
                    "label": 0
                },
                {
                    "sent": "Now here I add some single trials Sony pokes and now we do the same thing for the points.",
                    "label": 0
                },
                {
                    "sent": "If we get here for each trial.",
                    "label": 0
                },
                {
                    "sent": "So for each trial we get a point here in this scatter plot.",
                    "label": 0
                },
                {
                    "sent": "So now I do it for the rest.",
                    "label": 0
                },
                {
                    "sent": "And then we see something like this distribution.",
                    "label": 0
                },
                {
                    "sent": "This is called scatter plot.",
                    "label": 0
                },
                {
                    "sent": "So, and as I showed shown before, for your piece, we assume that.",
                    "label": 0
                },
                {
                    "sent": "The noise is Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "And Gaussian distributions are indicated by Ellipse.",
                    "label": 0
                },
                {
                    "sent": "And I will show you.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the background of this is so here we have a 2 dimensional Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "These thoughts and this is this nice surface is a density which says.",
                    "label": 0
                },
                {
                    "sent": "Something like the probability density.",
                    "label": 0
                },
                {
                    "sent": "So here around the mean it is most density and outsides.",
                    "label": 0
                },
                {
                    "sent": "It's get less and.",
                    "label": 0
                },
                {
                    "sent": "This aligns here are the eizo density lines here.",
                    "label": 0
                },
                {
                    "sent": "So which has the same height and this surface plot?",
                    "label": 0
                },
                {
                    "sent": "So this means all points on one line has the same probability density.",
                    "label": 0
                },
                {
                    "sent": "And for Gaussian distributions, or these, if you look from above, these are ellipse.",
                    "label": 1
                },
                {
                    "sent": "And so forth too.",
                    "label": 0
                },
                {
                    "sent": "To easily indicate a Gaussian distribution, you just take one of these...",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And a particular one as I will show, you know.",
                    "label": 0
                },
                {
                    "sent": "But how do we know how this ellipse looks like?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this can be obtained with the so-called eigenvalue decomposition.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I explained the formula but so.",
                    "label": 0
                },
                {
                    "sent": "Given the matrix, it is symmetric and positive definite.",
                    "label": 0
                },
                {
                    "sent": "We can do the eigenvalue decomposition, which means we get another matrix V which is also normal matrix and holds the eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "And the diagonal matrix D such that we can represent the Matrix Sigma as a product V times ditrans be transpose.",
                    "label": 1
                },
                {
                    "sent": "So don't don't worry, assist, looks complicated to you.",
                    "label": 1
                },
                {
                    "sent": "So first thing is you can forget about the prerequisites because we will always apply this for covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "And self info covariance matrix is always fulfilled.",
                    "label": 0
                },
                {
                    "sent": "So if you have a covariance matrix, you can always make this decomposition.",
                    "label": 0
                },
                {
                    "sent": "And the meaning of the decomposition is also very simple, it just says.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "These matrices this correspond to... from the Gaussian distributions and.",
                    "label": 0
                },
                {
                    "sent": "Diagonal matrices correspond to... that have their access.",
                    "label": 0
                },
                {
                    "sent": "Along with the coordinate axis, so the axis of this ellipse is wrong.",
                    "label": 0
                },
                {
                    "sent": "First this along X axis of the coordinate system and another Y.",
                    "label": 0
                },
                {
                    "sent": "If you have non diagonal matrices, these... are tilted somehow and the eigenvalue decomposition just gives you gives you this correspondence it says.",
                    "label": 0
                },
                {
                    "sent": "So the general matrix that you have Sigma here correspondence to an ellipse that you get if you take this basic ellipse that is defined by the diagonal matrix D. And apply this matrix V which is a rotation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is D and this 2 dimensional case would just have the diagonal matrix is just two entries, one is the radius in horizontal direction and the others in the vertical direction.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here for plotting this nice ellipse, if you get the data here from many trials X.",
                    "label": 0
                },
                {
                    "sent": "Then you would get this ellipse just by calculating this eigenvalue decomposition.",
                    "label": 0
                },
                {
                    "sent": "And then in this matrix V. So this is 2 by two and the two dimensional case you have two vectors, 22 dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "And one vector gives you this axis and the other vector gives you that access.",
                    "label": 0
                },
                {
                    "sent": "But this vectors are normalized this half length one, so this doesn't say how long is the ellipsoid in that direction.",
                    "label": 0
                },
                {
                    "sent": "This is given by the Matrix D. These are quotes are eigenvalues, so in this diagonal matrix two by two we have two values and one says.",
                    "label": 0
                },
                {
                    "sent": "How, how long is this axis and the other?",
                    "label": 0
                },
                {
                    "sent": "How long is it set axes and this correspond also to your original distribution?",
                    "label": 0
                },
                {
                    "sent": "Causes eigenvalues D. For example, this eigen values that correspond to this V1 to this direction.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the variance of the data in this direction.",
                    "label": 0
                },
                {
                    "sent": "So then this is eigenvalue decomposition.",
                    "label": 0
                },
                {
                    "sent": "We know the variance in this direction and in that direction.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I guess this was already quite a. Exhausting for you and therefore so that you don't get into it too low vigilance state.",
                    "label": 0
                },
                {
                    "sent": "We will make some active part now.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So therefore first, so this will be a group work, so you will have to leave.",
                    "label": 0
                },
                {
                    "sent": "Your place is so and I will explain what happens.",
                    "label": 0
                },
                {
                    "sent": "So first we make a distribution into several groups of about five people and then I will explain the task.",
                    "label": 0
                },
                {
                    "sent": "So splitting in the in the groups will be as follows.",
                    "label": 0
                },
                {
                    "sent": "So all this part will go to the to the left and that part we go to the right.",
                    "label": 0
                },
                {
                    "sent": "Maybe first I explain.",
                    "label": 0
                },
                {
                    "sent": "It is then you have, so you will take some time, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So then in each group you have to arrange yourself in a line that corresponds to the to the distance of your birth place to Berlin.",
                    "label": 0
                },
                {
                    "sent": "So in the end the one one in Berlin should be on this side and the people from far away should be that side.",
                    "label": 0
                },
                {
                    "sent": "OK, we have these two lines and then but I will expand this later again and then you will count on this side.",
                    "label": 0
                },
                {
                    "sent": "I think from from 1 to 8, the first one says one, the second 28 and then you repeat and then all the ones will be in one group or the tools and so on so that you are mixed between the distances and I think on this site you will count to 12.",
                    "label": 0
                },
                {
                    "sent": "OK, but first of all so, but it's also the intention that you get to know each other.",
                    "label": 0
                },
                {
                    "sent": "So take your time to discuss where you where you originate from.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Say thank you so now just get into a linear order.",
                    "label": 0
                },
                {
                    "sent": "So even if not everything was was clarified.",
                    "label": 0
                },
                {
                    "sent": "So now I mention you counting, so I think yeah, you need to count to 7 and on that side to 10.",
                    "label": 0
                },
                {
                    "sent": "So you just start and you remember your number and all the ones here are one group and all the tools and the same.",
                    "label": 0
                },
                {
                    "sent": "On the other side, and then you have to find places in within your group here to do the group work.",
                    "label": 0
                },
                {
                    "sent": "So did you get up this evening?",
                    "label": 0
                },
                {
                    "sent": "OK, so only two 7, so let's do again so.",
                    "label": 0
                },
                {
                    "sent": "Five OK. OK. OK 6.",
                    "label": 0
                },
                {
                    "sent": "You have to count, so he was six or seven.",
                    "label": 0
                },
                {
                    "sent": "No, we only took yeah, correct?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but remember your number right?",
                    "label": 0
                },
                {
                    "sent": "No, you have to start from one again.",
                    "label": 0
                },
                {
                    "sent": "I change this thing so one.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "You have to count.",
                    "label": 0
                },
                {
                    "sent": "So yoga.",
                    "label": 0
                },
                {
                    "sent": "56 OK, great so now.",
                    "label": 0
                },
                {
                    "sent": "Now find your group members so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So did you succeed?",
                    "label": 0
                },
                {
                    "sent": "Do you have all your number?",
                    "label": 0
                },
                {
                    "sent": "So, do you have your numbers or not?",
                    "label": 0
                },
                {
                    "sent": "So who was the last 1 eleven 1213?",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "I need to 10.",
                    "label": 0
                },
                {
                    "sent": "So I change I changed.",
                    "label": 0
                },
                {
                    "sent": "To 10 so you are 9.",
                    "label": 0
                },
                {
                    "sent": "81 she.",
                    "label": 0
                },
                {
                    "sent": "7.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "The next one is 1 again.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you have to find your partners.",
                    "label": 0
                },
                {
                    "sent": "Sorry sorry.",
                    "label": 0
                },
                {
                    "sent": "Don't don't mix between the parts then, then the groups are too big just.",
                    "label": 0
                },
                {
                    "sent": "Don't that just within the side?",
                    "label": 0
                },
                {
                    "sent": "Sorry I didn't say that.",
                    "label": 0
                },
                {
                    "sent": "So now you can also distribute a little bit more that you have some some space for discussion.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So OK, I hope you attract approximately find your group.",
                    "label": 0
                },
                {
                    "sent": "So just so now the office.",
                    "label": 0
                },
                {
                    "sent": "Please attention to the task so it's a task that you would usually do is a piece of paper because it's about drawing some distributions, but you can.",
                    "label": 0
                },
                {
                    "sent": "In this case I think it's easier if you just discuss and think about how it could look like, but of course you can also get a piece of paper if if you like, so this is a task, so I've shown you how to how you do this, scatter plots and now you should.",
                    "label": 0
                },
                {
                    "sent": "So I showed a scatter plot.",
                    "label": 0
                },
                {
                    "sent": "With one channel in two time points and also to say the counterpart, so you should do a scatter plot.",
                    "label": 0
                },
                {
                    "sent": "Now for one time point here, where is the peak of the PS3 years?",
                    "label": 0
                },
                {
                    "sent": "But for two electrodes.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis you put sevario.",
                    "label": 0
                },
                {
                    "sent": "Of the electrodes, is that?",
                    "label": 0
                },
                {
                    "sent": "And on the Y axis you put the electrode of the electrode ozet.",
                    "label": 0
                },
                {
                    "sent": "And the difference to the what I've shown you before.",
                    "label": 0
                },
                {
                    "sent": "Now you have to think about two distributions, so about you get one distribution for targets and another for non targets.",
                    "label": 0
                },
                {
                    "sent": "So if you think about this scatter plot, so again you will have some Gaussian distribution so that you should know approximately about two edits or so.",
                    "label": 0
                },
                {
                    "sent": "How these could look like.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't have a bot here, so you have mentioned this.",
                    "label": 0
                },
                {
                    "sent": "Go back to this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you should have something like this on this axis you now have.",
                    "label": 0
                },
                {
                    "sent": "Potentially it sees it as it.",
                    "label": 0
                },
                {
                    "sent": "This is all set and you should think about these two ellipsoids for targets and non targets, or they look like and.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to these signals that you get in this standard ERP.",
                    "label": 0
                },
                {
                    "sent": "So now you should.",
                    "label": 0
                },
                {
                    "sent": "Should discuss with your with your group members, so that means you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so maybe discuss like 4 minutes and then we will continue.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry that we don't have more time.",
                    "label": 0
                },
                {
                    "sent": "So I'm very happy that you so actively participated.",
                    "label": 0
                },
                {
                    "sent": "So now I will tell you how it looks like so far from this graph.",
                    "label": 0
                },
                {
                    "sent": "You can already easily see where the means of the two distributions are, so you have at channel all set in that direction.",
                    "label": 0
                },
                {
                    "sent": "Both means are at zero and on the other axis the mean for non targets is around zero and the other one is around 5 microvolt.",
                    "label": 0
                },
                {
                    "sent": "So this this is this solution, but this is from the.",
                    "label": 0
                },
                {
                    "sent": "Little bit different data set and also you cannot see.",
                    "label": 0
                },
                {
                    "sent": "You cannot say exactly how this looks like, but here you see.",
                    "label": 0
                },
                {
                    "sent": "In the opposite direction, there both at zero and in the horizontal direction, yes is 5 microvolt difference.",
                    "label": 0
                },
                {
                    "sent": "And then the distributions are Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "So you have this ellipsoids, and so now it's a question how this ellipsoid is tilted or looks like and this you could not see from the picture I've showed you because there's not.",
                    "label": 0
                },
                {
                    "sent": "I didn't show which trial it belongs to which.",
                    "label": 0
                },
                {
                    "sent": "So, but you from what I told before, you could know that the orientation is approximately like this, and this is due to this volume conduction.",
                    "label": 0
                },
                {
                    "sent": "So I said that what you measure at different channels is highly correlated.",
                    "label": 0
                },
                {
                    "sent": "That means that if the value is high at all set, then it's also high Aziz at.",
                    "label": 0
                },
                {
                    "sent": "In principle, so therefore these are more or less orientated like this.",
                    "label": 0
                },
                {
                    "sent": "So in this particular data set is very much tilted, so I don't know what so this is maybe a bit untypical, so it should take another solution picture, so usually it's they look they look more like this.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is important because this is now the setting for classification.",
                    "label": 0
                },
                {
                    "sent": "So our task is to kiss classify these distributions.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the next topic, so we have.",
                    "label": 0
                },
                {
                    "sent": "To find a classifier, the classifier is a mapping just from from the space where we have some features.",
                    "label": 1
                },
                {
                    "sent": "So from this vectors to say labels one and two, which corresponds to targets and non targets.",
                    "label": 1
                },
                {
                    "sent": "So from the continuous signals we first cut out the box from each epoch, we calculate the feature vector.",
                    "label": 1
                },
                {
                    "sent": "The spatial temporal feature, and then we have the classification.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step.",
                    "label": 0
                },
                {
                    "sent": "So you see, this is the same calculated from a different data set.",
                    "label": 0
                },
                {
                    "sent": "So just to repeat here, we have the two means and the means correspond to the smallest to the ERP itself.",
                    "label": 0
                },
                {
                    "sent": "So this is the signal.",
                    "label": 0
                },
                {
                    "sent": "And the covariance matrix, which makes this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "This is just the noise as we've seen from the LP model, and because this is just background noise, we also know that this is the same in both conditions, so this doesn't depend on the condition, it's just background noise.",
                    "label": 0
                },
                {
                    "sent": "So the Sigma is the same.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now how do we classify so?",
                    "label": 0
                },
                {
                    "sent": "This is just we start with a very simple method to classifiers.",
                    "label": 0
                },
                {
                    "sent": "So assume we only know the means of the two distributions.",
                    "label": 1
                },
                {
                    "sent": "We only know mu one and MU 2.",
                    "label": 0
                },
                {
                    "sent": "And now we have a sample X.",
                    "label": 0
                },
                {
                    "sent": "So if you only know the mean there's really not much that we can do if we are asked to classify, we would measure the distance to the two means and then say 'cause it's the distance is smaller too.",
                    "label": 0
                },
                {
                    "sent": "MU two we would classify it as two.",
                    "label": 0
                },
                {
                    "sent": "And if we do this strategy for all points, we would separate the whole.",
                    "label": 0
                },
                {
                    "sent": "Plane.",
                    "label": 0
                },
                {
                    "sent": "Operation lines that that goes perpendicular to this connection.",
                    "label": 1
                },
                {
                    "sent": "Also two means and cuts it exactly in the middle, and all these are classified as two and see other half plan as one.",
                    "label": 0
                },
                {
                    "sent": "And this is a very simple classification scheme that you can also do in high dimensions, not only in two.",
                    "label": 1
                },
                {
                    "sent": "As a short you here.",
                    "label": 0
                },
                {
                    "sent": "And this is called the nearest centroid classifier.",
                    "label": 0
                },
                {
                    "sent": "So that means also called centroid and.",
                    "label": 0
                },
                {
                    "sent": "So this is the nearest.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Android classifier.",
                    "label": 0
                },
                {
                    "sent": "And if you this is formalized, so we don't really need to know this, But this is formalized usually in this way the classifier has a weight vector W. It's just the same dimensions as your feature.",
                    "label": 0
                },
                {
                    "sent": "Features and one number of bias bias term that you subtract and so here from this graph you see.",
                    "label": 0
                },
                {
                    "sent": "So essentially this V transpose times X is a distance of here.",
                    "label": 0
                },
                {
                    "sent": "This X If you Project X on this vector W. This is a distance from the origin and then we have two subjects.",
                    "label": 0
                },
                {
                    "sent": "Here the distance of this separation line from the origin, so therefore this distance here from the separation line.",
                    "label": 0
                },
                {
                    "sent": "To the projection of this point is this expression, and if this is positive, this means it's on that side of the separation line.",
                    "label": 0
                },
                {
                    "sent": "Then we would for example, classify.",
                    "label": 0
                },
                {
                    "sent": "Then we would classify a Class 2 in this case and if it's.",
                    "label": 1
                },
                {
                    "sent": "On this side, then classifier one.",
                    "label": 0
                },
                {
                    "sent": "OK, so each classifier has his fate vector or enter bias term, and you can formalize all linear classifiers in this way, not only the nearest centroid classifier.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but can we expect for Fiji this nearest simple near century classifier to perform very well, so as so these are distributions that we could expect to see in e.g.",
                    "label": 1
                },
                {
                    "sent": "So in this case this would be like ozette and this is at and this is the third due to the.",
                    "label": 0
                },
                {
                    "sent": "Coalition.",
                    "label": 0
                },
                {
                    "sent": "Now the nearest centroid classifier.",
                    "label": 0
                },
                {
                    "sent": "What makes this separation this side?",
                    "label": 0
                },
                {
                    "sent": "It's orange class and this side is blue class and you immediately see that this is not a good idea that you should better have something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so we see for each year if we have this high correlation between the channels then this NCC is now.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So therefore I will introduce another classifier, linear discriminant analysis.",
                    "label": 0
                },
                {
                    "sent": "And one can show that linear discriminant analysis is optimal in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "So optimal means here that its hits minimum risk of miss classification for samples that come from the same distributions.",
                    "label": 1
                },
                {
                    "sent": "And the LDA is optimal under these three assumptions.",
                    "label": 0
                },
                {
                    "sent": "The first assumption is the features of all classes are Gaussian distributed.",
                    "label": 1
                },
                {
                    "sent": "The second is that the Gaussian distributions all have the same covariance matrix and that we have these first two things we have already discussed to be true for LP's, and the third is set to true class distributions are known so that we have this new one and Sigma and U2 and Sigma is the same.",
                    "label": 1
                },
                {
                    "sent": "In that case, if you have this given, we can define the weight vector of LDA.",
                    "label": 0
                },
                {
                    "sent": "Here's the inverse of the covariance matrix, Sigma Times new 2 minus mu one and bias term.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now this is the same setting as we have seen before with the NCC and also the excesses at the same, exactly the same spot.",
                    "label": 0
                },
                {
                    "sent": "Now, but here I've shown some typical ET urope distributions.",
                    "label": 0
                },
                {
                    "sent": "So then we would have this ellipsoids and the separation line of LDA.",
                    "label": 0
                },
                {
                    "sent": "In this case would look like this, and in this case very same X and the same users.",
                    "label": 0
                },
                {
                    "sent": "Before.",
                    "label": 0
                },
                {
                    "sent": "Here X would be classified as blue class.",
                    "label": 0
                },
                {
                    "sent": "And this is if you.",
                    "label": 0
                },
                {
                    "sent": "If we know this distributions, then it's obvious that this is a good choice.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's also a correspondence between LDA and NCCS, or I will not go into detail here, but this this is transformed.",
                    "label": 0
                },
                {
                    "sent": "It's called Whitening, which transforms distribution which is ellipsoidal distribution into a spherical one.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you transform this whole space versus ellipsoid distribution by whitening into a spherical one, then here we can do the NCC classification.",
                    "label": 0
                },
                {
                    "sent": "But here we removed, so to say this correlation structure.",
                    "label": 0
                },
                {
                    "sent": "But this is just a side remark.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is not not important, you can just take LDA and do it.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, I based on the model, I already motivated the two covariance matrices of the two classes should be the same because they only depend on the background data.",
                    "label": 1
                },
                {
                    "sent": "But here I also have some empirical evidence from the data here I.",
                    "label": 0
                },
                {
                    "sent": "Calculated the covariance matrices from the two classes targets and non targets and these typographies visualize this eigenvectors which are the vectors of this.",
                    "label": 1
                },
                {
                    "sent": "Covariance matrix of this ellipsoid corresponding to the highest eigenvalues, so to the most information is and here you see that the corresponding Maps look very similar between two classes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "But this is not generally the case, so this is the same visualization of covariance matrices from a different classification example.",
                    "label": 0
                },
                {
                    "sent": "So this is handwritten digit recognition.",
                    "label": 0
                },
                {
                    "sent": "So here's here's the zeros, and here's the 7th, this this other mean values.",
                    "label": 0
                },
                {
                    "sent": "And these are against the eigenvectors of covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And here you see, this doesn't look.",
                    "label": 0
                },
                {
                    "sent": "Any similar to this one?",
                    "label": 0
                },
                {
                    "sent": "So there's that means in the handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "The covariance matrix depends on the class.",
                    "label": 0
                },
                {
                    "sent": "So if you draw the zero little bit different little bit tilted or little seeker pen or something like that.",
                    "label": 0
                },
                {
                    "sent": "Then there's some variation, but how this variation is reflected in this feature in that case depends on the class.",
                    "label": 0
                },
                {
                    "sent": "So this is not generally that I say oil.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrices are the same, but this is depends on what I explained before this year.",
                    "label": 0
                },
                {
                    "sent": "P model.",
                    "label": 0
                },
                {
                    "sent": "Because there's a noise is independent of the condition.",
                    "label": 0
                },
                {
                    "sent": "But here the noise is it.",
                    "label": 0
                },
                {
                    "sent": "Depend, so that means here for the digits we would probably need a different.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classifier.",
                    "label": 0
                },
                {
                    "sent": "So that so now we can.",
                    "label": 0
                },
                {
                    "sent": "We already discussed how to extract features, so now we and we discuss classifiers.",
                    "label": 0
                },
                {
                    "sent": "So now we.",
                    "label": 0
                },
                {
                    "sent": "Can evaluate classifiers and talk about that, so therefore just one word about validating classifiers.",
                    "label": 0
                },
                {
                    "sent": "If you validate classifiers, it's important that you have separately a training set where you train your classifier on where you determine the distributions and this term.",
                    "label": 0
                },
                {
                    "sent": "In this vector W and the bias B and then you take another set independent which must have nothing to do with the training set, and then you calculate the performance.",
                    "label": 0
                },
                {
                    "sent": "So only then you get a reasonable estimate.",
                    "label": 0
                },
                {
                    "sent": "So here I will not have time to.",
                    "label": 0
                },
                {
                    "sent": "Talk about more details about this, but if you are interested in this topic, you can refer to this.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "So here I show you results classification validation based on spatial features.",
                    "label": 1
                },
                {
                    "sent": "Corresponding to different time intervals.",
                    "label": 0
                },
                {
                    "sent": "So this is the first step.",
                    "label": 1
                },
                {
                    "sent": "So here I took.",
                    "label": 0
                },
                {
                    "sent": "The various in this time interval and then like have a special feature.",
                    "label": 1
                },
                {
                    "sent": "And I do a classification.",
                    "label": 0
                },
                {
                    "sent": "Then I take the next component, calculated spatial feature for this time interval and.",
                    "label": 0
                },
                {
                    "sent": "Makes a validation of the corresponding classes for her, so these are like trying out different spatial features.",
                    "label": 0
                },
                {
                    "sent": "And here we see the performance.",
                    "label": 0
                },
                {
                    "sent": "So we see for these two components we have best classification is error rate around 14% and the others also perform OK, but a bit worse.",
                    "label": 0
                },
                {
                    "sent": "So, but as I motivated before, we should use all information, so we should use spatial temporal feature and put all this information here together.",
                    "label": 0
                },
                {
                    "sent": "So but remember here is the best classification on this.",
                    "label": 0
                },
                {
                    "sent": "Best intervals around.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "18%.",
                    "label": 0
                },
                {
                    "sent": "And when I put now all information together into a spatial temporal feature and I apply my linear discriminant classifier, then I get an error of 25%.",
                    "label": 0
                },
                {
                    "sent": "And which is worse than the result for the best?",
                    "label": 1
                },
                {
                    "sent": "Interval so this looks strange at first sight because here we added some information because this feature includes this feature but has more information, so it should perform better, but it performs worse.",
                    "label": 0
                },
                {
                    "sent": "So that's could be surprising because we have this optimality criterion, but if there's an optimal classifier and the optimal classifier gets more information than at least it shouldn't get.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Worse, So what happens?",
                    "label": 0
                },
                {
                    "sent": "So then we need to go back to the assumptions.",
                    "label": 0
                },
                {
                    "sent": "Also, optimality and the third one I didn't talk about.",
                    "label": 0
                },
                {
                    "sent": "The third assumption was a true distributions are known, which says we we know the true means and the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "And of course this is never the case in real applications.",
                    "label": 0
                },
                {
                    "sent": "So you don't know the true covariance matrix if you have an any PCI data sets, so you have to to estimate.",
                    "label": 0
                },
                {
                    "sent": "This distribution para meters from the data and so there you make an error in this estimation.",
                    "label": 0
                },
                {
                    "sent": "So therefore some optimality doesn't hold 'cause you don't know their distributions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and these people see that by adding more information we have higher dimensional features.",
                    "label": 0
                },
                {
                    "sent": "And then this estimation.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gets worse.",
                    "label": 0
                },
                {
                    "sent": "So for a classifier we have to estimate these parameters, the mean and the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "This is a formulas and now to discriminate between true and estimated parameters.",
                    "label": 0
                },
                {
                    "sent": "These are also called empirical parameters.",
                    "label": 0
                },
                {
                    "sent": "We put the head on these variables.",
                    "label": 0
                },
                {
                    "sent": "Philadelphia observation if you have very high dimensional features and not so many examples where you can.",
                    "label": 0
                },
                {
                    "sent": "And estimate your para meters from.",
                    "label": 0
                },
                {
                    "sent": "Then there's a systematical bias.",
                    "label": 1
                },
                {
                    "sent": "And so in particular, if we estimate this covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "High dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This Ms systematic biases like this.",
                    "label": 0
                },
                {
                    "sent": "The large eigenvalues of this covariance matrix are too large and small eigenvalues are too small.",
                    "label": 1
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or so.",
                    "label": 0
                },
                {
                    "sent": "And graphically this means so I can various hiking various meats.",
                    "label": 0
                },
                {
                    "sent": "If you remember means large variance.",
                    "label": 0
                },
                {
                    "sent": "So here's this would be the direction also.",
                    "label": 0
                },
                {
                    "sent": "Not just variance and this of the smallest variance.",
                    "label": 0
                },
                {
                    "sent": "So yeah, Green is a true covariance matrix, so if we estimate this from data, large eigenvalues would be too large.",
                    "label": 0
                },
                {
                    "sent": "That means even estimates of variance, larger and smaller eigenvalues are estimated to small and means.",
                    "label": 0
                },
                {
                    "sent": "Here there's a very Asian is smaller.",
                    "label": 0
                },
                {
                    "sent": "We would even estimate it even smaller.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is just a cartoon, because if we this is in 2D.",
                    "label": 0
                },
                {
                    "sent": "If you just have two dimensional features then this problem would not arise.",
                    "label": 0
                },
                {
                    "sent": "This is only if F like 500 dimension.",
                    "label": 0
                },
                {
                    "sent": "And this shows the same thing.",
                    "label": 0
                },
                {
                    "sent": "So these are the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And he says I can values in black as a true eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And here we estimate these eigenvalues from data and here we are only 50 data points and four blue 100 for Green 200.",
                    "label": 0
                },
                {
                    "sent": "And here you see as a low eigenvalues are if you have too little samples and their estimated even lower and for high eigen various estimated even higher.",
                    "label": 0
                },
                {
                    "sent": "And then you can easily imagine if you have such a bad estimate of the covariance matrix and you calculate the classifier based on that.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimal.",
                    "label": 0
                },
                {
                    "sent": "So we need to.",
                    "label": 0
                },
                {
                    "sent": "Counterbalance is to counteract this misestimation.",
                    "label": 0
                },
                {
                    "sent": "And there's a quite nice easy solution to that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So as we have described the bias we need to make the estimate the ellipse more spherical.",
                    "label": 1
                },
                {
                    "sent": "So because the estimate is 2, ellipsoidal likes a blue one.",
                    "label": 0
                },
                {
                    "sent": "So we just make apply a linear morphing between the empirical one and it's very clear.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So here's our empirical one and the spherical.",
                    "label": 1
                },
                {
                    "sent": "Covariance matrix corresponds to the identity vector, which is scaled here with the average eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And with this parimeter gamma, we make a linear morphing.",
                    "label": 0
                },
                {
                    "sent": "1 minus, we multiply the empirical covariance matrix and with gamma.",
                    "label": 1
                },
                {
                    "sent": "This vehicle, so if if gamma is 0 then we would just have sick my head and if, is one we would completely forget about the estimate Sigma hat and just takes a.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spherical one.",
                    "label": 0
                },
                {
                    "sent": "So one can calculate that this so this is called shrunk covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "You can calculate its shrunk covariance matrix has assign same eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "As a empirical one.",
                    "label": 1
                },
                {
                    "sent": "So remember, the eigenvectors are the.",
                    "label": 0
                },
                {
                    "sent": "Direction of how the axis of the ellipsoids are, so that means you don't turn the ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "We just change the eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And these eigenvalues are just linear interpolated between the estimated iron, various D and the average eigenvalue new.",
                    "label": 0
                },
                {
                    "sent": "So this just confirms what?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shown in the picture.",
                    "label": 0
                },
                {
                    "sent": "So now we can.",
                    "label": 0
                },
                {
                    "sent": "Make an improved version of the LDA classifier.",
                    "label": 0
                },
                {
                    "sent": "We don't invert.",
                    "label": 0
                },
                {
                    "sent": "Here's an empirical covariance matrix that has is estimate problems, But here we use this shrunk covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "But otherwise the formulas are exactly the same, you just replace.",
                    "label": 0
                },
                {
                    "sent": "The estimate also covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Tell but success one open question here because he is still the parameter gamma, which is a number between zero and one which morphs between our estimate Sigma head and the spherical covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if we forgot my equals zero, we have normal LDA info, equals one.",
                    "label": 0
                },
                {
                    "sent": "We have here the identity matrix, so this cancels out.",
                    "label": 0
                },
                {
                    "sent": "We have here only mew two months me one and this is simple nearest centroid classifier.",
                    "label": 0
                },
                {
                    "sent": "So if you use this shrunk covariance matrix, this is.",
                    "label": 0
                },
                {
                    "sent": "The classifier is called shrinkage.",
                    "label": 0
                },
                {
                    "sent": "LDA.",
                    "label": 0
                },
                {
                    "sent": "So shrinkage LDA makes a compromise between LDA and nearest century classifier.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we have to find a value appropriate value for gamma.",
                    "label": 0
                },
                {
                    "sent": "So either we can do this by cross validation and just try out on the training set different values of, and then depending on the curves that we get pick, picks this, with the best performance.",
                    "label": 0
                },
                {
                    "sent": "This is quite time consuming.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So therefore we were happy to see that.",
                    "label": 0
                },
                {
                    "sent": "There's now a new method or it's already around for 10 years.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is like like this.",
                    "label": 0
                },
                {
                    "sent": "So we can calculate with one formulas or so optimal gamma in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimal tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So here's a formula that's very easy to implement and also not very time consuming.",
                    "label": 0
                },
                {
                    "sent": "That calculates the gamma.",
                    "label": 0
                },
                {
                    "sent": "And the camera optimizes this difference.",
                    "label": 0
                },
                {
                    "sent": "Here's this optimizes.",
                    "label": 0
                },
                {
                    "sent": "The difference between the true covariance matrix that we don't know and the covariance matrix that we get with this shrinkage.",
                    "label": 0
                },
                {
                    "sent": "And so it's it's a bit surprising, but under some assumptions you can solve this optimization problem, although we don't know this true covariance matrix Sigma.",
                    "label": 0
                },
                {
                    "sent": "So this somehow cancels out in the calculation and we just get a formula that's based on the data that we estimate.",
                    "label": 0
                },
                {
                    "sent": "So this is more or less what we see here is the covariance matrix of a single trial.",
                    "label": 0
                },
                {
                    "sent": "So you calculate for each single trial occurrence matrix.",
                    "label": 0
                },
                {
                    "sent": "And then you calculate.",
                    "label": 0
                },
                {
                    "sent": "Along all trials, the variance that you have in this estimate and is this variance.",
                    "label": 0
                },
                {
                    "sent": "So that means if this covariance matrix single trial covariance matrix is unstable over trials, you will get a high value of, And if this is stable.",
                    "label": 0
                },
                {
                    "sent": "Then you get a low value of,",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's see how this works.",
                    "label": 0
                },
                {
                    "sent": "So this is going back to what I was showing you before classification on.",
                    "label": 0
                },
                {
                    "sent": "These seven single time intervals.",
                    "label": 0
                },
                {
                    "sent": "As this was our first try, just putting information together and using LDA we had this bad performance.",
                    "label": 0
                },
                {
                    "sent": "And now with shrinkage, Eleazar performance is only 4%.",
                    "label": 0
                },
                {
                    "sent": "So now, as expected, it combines effectively all this information and we don't have this problem with a high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And then this gives a very good performance.",
                    "label": 0
                },
                {
                    "sent": "So this is not nothing like some general result.",
                    "label": 0
                },
                {
                    "sent": "This is only from one data set.",
                    "label": 0
                },
                {
                    "sent": "So here apparently it works very well.",
                    "label": 0
                },
                {
                    "sent": "But whether how, how well this works of course depends on on your data.",
                    "label": 0
                },
                {
                    "sent": "But but usually so our experiences for for this ERP datasets at least, you don't get worse with the shrinkage, but.",
                    "label": 0
                },
                {
                    "sent": "You can only get better thoughts.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be the case, but so this is all.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experience.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                },
                {
                    "sent": "Give you some more overview so we have at this two classifiers.",
                    "label": 0
                },
                {
                    "sent": "First very simple nearest century classifier.",
                    "label": 0
                },
                {
                    "sent": "There's the weight vector of the classifiers, just the difference.",
                    "label": 0
                },
                {
                    "sent": "New 2 minus mu one.",
                    "label": 0
                },
                {
                    "sent": "So therefore this if you.",
                    "label": 0
                },
                {
                    "sent": "Calculate this on special features.",
                    "label": 0
                },
                {
                    "sent": "That means you subtract, for example, the year, the history, topography from the targets and subtract and from the nontarget so you this fate vector of your classifier would be visualized, flexes, then we had Sadie, a classifier where we have the same difference but we multiply it with the inverse covariance matrix to take into account this spatial smearing and so covalent structure.",
                    "label": 0
                },
                {
                    "sent": "So on this data set this further error rates.",
                    "label": 0
                },
                {
                    "sent": "End.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it's with shrinkage with you.",
                    "label": 0
                },
                {
                    "sent": "Put in different various if, then we have continuum between these extreme classifiers.",
                    "label": 0
                },
                {
                    "sent": "So for gamma equals zero we have 88, equals one.",
                    "label": 0
                },
                {
                    "sent": "We have nearest centroid classifier and there's a continuum here in between.",
                    "label": 0
                },
                {
                    "sent": "Somewhere so usually it looks like this first goes down because you make your estimate better.",
                    "label": 0
                },
                {
                    "sent": "You counterbalance the error and in the end it gets worse again because then you stop exploiting the covalent structure.",
                    "label": 0
                },
                {
                    "sent": "So usually I have optimum somewhere in between and hopefully with this shrinkage formula this puts out this best,",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is another view in it so.",
                    "label": 0
                },
                {
                    "sent": "The same means, so here we use empirical covariance matrix and LDA.",
                    "label": 1
                },
                {
                    "sent": "Here use spherical covariance matrix and your central classifier.",
                    "label": 0
                },
                {
                    "sent": "So these solutions only differ with what you expect.",
                    "label": 0
                },
                {
                    "sent": "How the noise looks like.",
                    "label": 0
                },
                {
                    "sent": "Or this covariance matrix looks like.",
                    "label": 0
                },
                {
                    "sent": "So this would mean that in the near century classifier this means you ignore the information from the data because you don't trust it.",
                    "label": 0
                },
                {
                    "sent": "So therefore you just.",
                    "label": 0
                },
                {
                    "sent": "Base your classifier on the means.",
                    "label": 0
                },
                {
                    "sent": "And with shrinkage you do something in between.",
                    "label": 0
                },
                {
                    "sent": "So that means this amount of shrinkage somehow is related to your belief in the estimation of the noise.",
                    "label": 1
                },
                {
                    "sent": "If you have enough data, then your estimate of the noise will be very good, so you should use LDA or a small gamma.",
                    "label": 0
                },
                {
                    "sent": "And if you're if not so much data or very high dimensional features.",
                    "label": 0
                },
                {
                    "sent": "Then your estimation of the noise is so bad and error prone that it's better not to use it.",
                    "label": 0
                },
                {
                    "sent": "But usually you will be somewhere in the middle and.",
                    "label": 0
                },
                {
                    "sent": "Take Cisco.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See fire.",
                    "label": 0
                },
                {
                    "sent": "Find also is a separating hyperplane with.",
                    "label": 0
                },
                {
                    "sent": "This shrinkage Adss always has to be somewhere between NCC and earlier.",
                    "label": 0
                },
                {
                    "sent": "And this camera is based on such a optimization.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "Error in the covariance estimation and this formula calculates this optimum point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Regular.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of regularization, so there are different kinds of regularization, so, but typically you try to make the estimator of the covariance better and.",
                    "label": 0
                },
                {
                    "sent": "This yeah so the.",
                    "label": 0
                },
                {
                    "sent": "So general shrinkage is in the exactly one way to regularize.",
                    "label": 0
                },
                {
                    "sent": "But this is something new to it from this little Wolf and Schaefer swimmer.",
                    "label": 0
                },
                {
                    "sent": "How to calculate?",
                    "label": 0
                },
                {
                    "sent": "So, so usually if in the presentations of regularize LDA you're left alone of how to select your perimetre.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said, I.",
                    "label": 0
                },
                {
                    "sent": "Who's targeting taking this same time as?",
                    "label": 0
                },
                {
                    "sent": "For the other, so then we have still 15 minutes, so I will present 1 short topic at the end and then we have room for more questions and.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pick.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One important point of.",
                    "label": 0
                },
                {
                    "sent": "To understand what is what is going on here with the classification.",
                    "label": 1
                },
                {
                    "sent": "So this is a critical problem if you use this advanced methods from machine learning.",
                    "label": 0
                },
                {
                    "sent": "So you there are many ways where you can.",
                    "label": 0
                },
                {
                    "sent": "Can fail and do things wrong, so therefore it's in any case very important that you try to understand what is going on.",
                    "label": 0
                },
                {
                    "sent": "What is a classifier doing?",
                    "label": 0
                },
                {
                    "sent": "But it turns out that this is not so easy.",
                    "label": 0
                },
                {
                    "sent": "So first of all, so I put this here under the framework of spatial filters.",
                    "label": 0
                },
                {
                    "sent": "So we can say that linear classifier is a spatial filter, so literally this is only true if he uses spatial features.",
                    "label": 1
                },
                {
                    "sent": "So if you have from one time point.",
                    "label": 1
                },
                {
                    "sent": "We take various of all channels.",
                    "label": 0
                },
                {
                    "sent": "If we then train the classifier on those special features and the way tractor of the classifier has exactly the same dimensions as a number of channels.",
                    "label": 0
                },
                {
                    "sent": "And it's applied to the data as a special filter.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's one example from our speller data set in the very beginning.",
                    "label": 0
                },
                {
                    "sent": "So these are this is Alpha matrix.",
                    "label": 0
                },
                {
                    "sent": "We see the two intervals where we have nice differences, so we could train a classifier, for example based on this.",
                    "label": 0
                },
                {
                    "sent": "Time interval and this would be a special filter.",
                    "label": 0
                },
                {
                    "sent": "Or we could train the classifier on this time interval.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rain.",
                    "label": 0
                },
                {
                    "sent": "Such a classifier.",
                    "label": 0
                },
                {
                    "sent": "Send send it's a spatial filter and we could.",
                    "label": 0
                },
                {
                    "sent": "After that we could just apply it to continuous data so we get new data continuously and we could use this weight vector of the classifier.",
                    "label": 0
                },
                {
                    "sent": "And uses weights to wait this channels and we would get just one time signal here.",
                    "label": 0
                },
                {
                    "sent": "So this is also something that you're going to do in the exercises in the practical session.",
                    "label": 0
                },
                {
                    "sent": "So then this would put the the output of the classifier and we see it.",
                    "label": 0
                },
                {
                    "sent": "Of course it.",
                    "label": 0
                },
                {
                    "sent": "Next the classifier here was designed to maximize the difference here at this time interval 250 or something and the other classifier on the P3 component maximizes and here.",
                    "label": 0
                },
                {
                    "sent": "And we can can visualize this special filter.",
                    "label": 0
                },
                {
                    "sent": "So the weight vector of the classifier.",
                    "label": 0
                },
                {
                    "sent": "We can visualize this as a topography here.",
                    "label": 0
                },
                {
                    "sent": "So therefore it's.",
                    "label": 0
                },
                {
                    "sent": "It's tempting to have an interpretation of what was learned by the classifier based on this topography, so here you get you see well where there are high rates of the classifier and where there are low weights and you see this also in papers quite often that people say OK, Now we check what the classifier does.",
                    "label": 0
                },
                {
                    "sent": "Here's the topography and then they make the interpretation.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This has to be done really carefully as I will show you here so I will make a very simple example to show what the problem here is.",
                    "label": 0
                },
                {
                    "sent": "So we assume now very simple case.",
                    "label": 0
                },
                {
                    "sent": "We have just two sources S1 and S2.",
                    "label": 0
                },
                {
                    "sent": "And these are the patterns vector A1 and A2.",
                    "label": 0
                },
                {
                    "sent": "Which say how these sources are propagated to the e.g senders.",
                    "label": 0
                },
                {
                    "sent": "So this is multiplied with a one and this is with a 2 and this add up to the signal that we measure at some sensor AT2 sensors X.",
                    "label": 0
                },
                {
                    "sent": "So these are all vectors of.",
                    "label": 0
                },
                {
                    "sent": "BF are vectors of dimensionality tool.",
                    "label": 0
                },
                {
                    "sent": "So now assume our task is to design A spatial filter that we want to apply to the data X to extract, say, source one.",
                    "label": 1
                },
                {
                    "sent": "We want to discover source one.",
                    "label": 0
                },
                {
                    "sent": "So applying vector spatial filter means we multiply this with a W transpose.",
                    "label": 0
                },
                {
                    "sent": "So if we apply this W transpose to X.",
                    "label": 0
                },
                {
                    "sent": "This distributes linearly to the two.",
                    "label": 0
                },
                {
                    "sent": "Terms here.",
                    "label": 1
                },
                {
                    "sent": "So if we want to recover S1, then we can distinguish the following two cases.",
                    "label": 0
                },
                {
                    "sent": "So if that means we get need to get rid of the second term because here's the interfering source as two.",
                    "label": 0
                },
                {
                    "sent": "So this filter needs to cancel out this part.",
                    "label": 0
                },
                {
                    "sent": "So in the 1st.",
                    "label": 0
                },
                {
                    "sent": "For the first case, is set.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This these two propagation vectors A1 and A2 are orthogonal.",
                    "label": 0
                },
                {
                    "sent": "That means H1 transpose times a two is 0.",
                    "label": 0
                },
                {
                    "sent": "So in this case we would choose W just to be a one.",
                    "label": 0
                },
                {
                    "sent": "So if you see if you plug in a one here SW then here we have a one transpose times a two and this is zero if it's orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So then this cancels out.",
                    "label": 0
                },
                {
                    "sent": "And here we have a one transpose, a one.",
                    "label": 0
                },
                {
                    "sent": "This is just a number, so just a scaling.",
                    "label": 0
                },
                {
                    "sent": "So we would recover as one.",
                    "label": 0
                },
                {
                    "sent": "But in the in the other case.",
                    "label": 0
                },
                {
                    "sent": "That it's that it's not orthogonal.",
                    "label": 1
                },
                {
                    "sent": "We would need to choose W somehow that this is orthogonal, so we need to cancel out the second term so we have to choose W such that W transpose times a 20.",
                    "label": 0
                },
                {
                    "sent": "So now it doesn't matter how we do it.",
                    "label": 0
                },
                {
                    "sent": "Just remember we need to choose W such that W. Transpose times 820.",
                    "label": 0
                },
                {
                    "sent": "Because this means set the filter the best filter W only depends on source 2.",
                    "label": 0
                },
                {
                    "sent": "So we want to recover source as one, but we need the filter only depends on S2.",
                    "label": 0
                },
                {
                    "sent": "So this a two.",
                    "label": 0
                },
                {
                    "sent": "So the distribution vector that relates to the topography of this second source.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "So assume S1 as a cognitive component P3 that we want to extract to do the classification and say as two.",
                    "label": 0
                },
                {
                    "sent": "Is a source from the visual area that makes interference.",
                    "label": 1
                },
                {
                    "sent": "So that is the optimal filter to get the PS3 component.",
                    "label": 0
                },
                {
                    "sent": "Depends.",
                    "label": 1
                },
                {
                    "sent": "Under just on the interference as two.",
                    "label": 0
                },
                {
                    "sent": "We have two tools.",
                    "label": 0
                },
                {
                    "sent": "Select the filter W such that it fulfilled fulfills a criterion based on S2.",
                    "label": 0
                },
                {
                    "sent": "It means if you know, make a make an interpretation and we look at that.",
                    "label": 0
                },
                {
                    "sent": "Topography of this spatial filter W. It would make interpretation.",
                    "label": 0
                },
                {
                    "sent": "Not of the components that we are extracting, also P3, but we would make an interpretation of of the interfering sources.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "So this is this is an extreme case or in practical examples you have many different interfering sources and impractical.",
                    "label": 0
                },
                {
                    "sent": "There's a tradeoff.",
                    "label": 1
                },
                {
                    "sent": "Your weight vector of the classifier will also depend on the signal of interest, because it has to trade off.",
                    "label": 0
                },
                {
                    "sent": "But still, it remains true that it also depends and may strongly depend on on the other components that do the interference.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is another way too.",
                    "label": 0
                },
                {
                    "sent": "To illustrate this, so here I have an example is very clean data.",
                    "label": 0
                },
                {
                    "sent": "Potentially two electrodes and.",
                    "label": 0
                },
                {
                    "sent": "Here we have a very nice classification problem and we can obtain a good classifier and now I artificially add some noise to it.",
                    "label": 0
                },
                {
                    "sent": "So I just generated 10 Hertz signal and I add noise.",
                    "label": 0
                },
                {
                    "sent": "This factor 0.42 channel CPZ and this factor points two to FC set.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we get this distributions here, so I added some some noise and therefore of course there are gets worse.",
                    "label": 0
                },
                {
                    "sent": "So here we had 15% error and here 33%.",
                    "label": 0
                },
                {
                    "sent": "So this is not surprising, but now.",
                    "label": 0
                },
                {
                    "sent": "This is when we classify adjust based on these two channels.",
                    "label": 0
                },
                {
                    "sent": "But now if we get gives the classifier the chance also to get the signal from ozette.",
                    "label": 0
                },
                {
                    "sent": "Then you'll get back even with this disturbance you get back, so original classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "Cause that.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pacifier can just use this information from Ozerden SBTRKT this, so that's quite obvious.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's going on is quite obvious, but for the interpretation this means that this classifier on these three channels will use.",
                    "label": 0
                },
                {
                    "sent": "Some will use a channel that although channels, it has no discriminative information.",
                    "label": 0
                },
                {
                    "sent": "Remember, I just artificially generated this or that general send channel with just some 10 Hertz oscillation, which didn't have anything to do here with this discriminative task.",
                    "label": 0
                },
                {
                    "sent": "So this again shows that the best classifier will have positive weights or non zero weights on components it.",
                    "label": 0
                },
                {
                    "sent": "Don't for in itself have no discriminative information, so it may use this information just to subtract noise.",
                    "label": 0
                },
                {
                    "sent": "And therefore, in the interpretation you cannot say if you see some positive weight somewhere, you say.",
                    "label": 0
                },
                {
                    "sent": "Ah, this classifier users or you can say the classifier uses this information of course, but you cannot say that from this area comes discriminative information.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Well, this is just one final word, so here I am.",
                    "label": 0
                },
                {
                    "sent": "This lecture I tried to convince you.",
                    "label": 0
                },
                {
                    "sent": "2 uses multivariate features and machine learning techniques like this shrinkage, LDA classifier.",
                    "label": 1
                },
                {
                    "sent": "So this gives increased sensitivity and will lead to better performance.",
                    "label": 1
                },
                {
                    "sent": "Hopefully if you apply it wisely but.",
                    "label": 0
                },
                {
                    "sent": "So keep keep in mind that you should be if you try to make interpretations, you should be.",
                    "label": 1
                },
                {
                    "sent": "Be careful, but you should do interpretations.",
                    "label": 0
                },
                {
                    "sent": "Don't treat this as a black box, but be careful.",
                    "label": 1
                },
                {
                    "sent": "And I think in a later lecture on Thursday you will also learn about how you can regain interpretability of these classifiers.",
                    "label": 0
                },
                {
                    "sent": "So we've seen directly interpreting these filters is critical, but there are techniques how to do it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this this concludes the material that I was trying to present.",
                    "label": 0
                },
                {
                    "sent": "So there's there's room for some questions, but.",
                    "label": 0
                },
                {
                    "sent": "At the same time, I would already distribute this sheet, so in order.",
                    "label": 0
                },
                {
                    "sent": "For me to help me improve my my lecturing.",
                    "label": 0
                },
                {
                    "sent": "There's some some feedback.",
                    "label": 0
                },
                {
                    "sent": "This is called the Muddiest point feedback, so this is takes just half a minute to fill out, so it just says two points or.",
                    "label": 0
                },
                {
                    "sent": "One thing is where you list things that have remained unclear, or where you would have wished to see more details, and the other part is for additional informal comments that you would like to give.",
                    "label": 0
                },
                {
                    "sent": "So you can if you go out, you just put it here on on the disk.",
                    "label": 0
                }
            ]
        }
    }
}