{
    "id": "nq7p34mku5nd6w5m5lbsuprkb6aunos7",
    "title": "A Dirty Model for Multi-task Learning",
    "info": {
        "author": [
            "Ali Jalali, University of Texas at Austin"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/nips2010_jalali_dmm/",
    "segmentation": [
        [
            "OK, good afternoon everyone.",
            "Thank you for coming.",
            "I'm going to present this work.",
            "This is a joint work with Pradeep Ravikumar so Jason Kavian children all from University of Texas Austin."
        ],
        [
            "So oftentimes, in practice in many areas of science and engineering like biology, vision, etc, we face a situation that we want to estimate.",
            "A number of variables P with very undersampled.",
            "Size N and the classical curse of dimensionality forces us to take more and more samples if we want to have.",
            "A consistent estimation.",
            "However, over the past decades, our research shows that if the parameter leaves has a low dimensional structure, then there might be a hope and people have looked at different low dimensional structures such as sparsity, block sparsity, etc.",
            "But the question is, what if the parameter that we are interested in does not?",
            "Have the clean structure, such as the ones I just counted.",
            "Then we want to propose a method which is a kind of zero approach to this problem.",
            "A simple superposition of structures which is still low dimensional but surprisingly very useful in dealing with dirty data."
        ],
        [
            "Let's look at the multitask learning problem.",
            "The Multi task learning problem is the problem that we have a number of tasks to learn.",
            "And they have some sort of shared structure.",
            "As an example, consider you have two writers, writer, one and writer, two and each one of them.",
            "We ask them to write the letter A and then we want to learn a profile for each one of these writers.",
            "So one way is to learn the profile individually for each of these writers.",
            "The other is to learn them together so the main question here is that is there any advantage if I learn individual profiles kind of jointly?",
            "Of course, this is a very general problem, so."
        ],
        [
            "So we look at a multiple linear regression set up.",
            "There for each task we have linear samples.",
            "And of course we have some noise.",
            "So in this figure that I have shown each column.",
            "In the response matrix, why an parameter matrix beta corresponds to 1 task?",
            "So we are sampling each task with them with matrix X.",
            "An look at the matrix Y.",
            "So the question is, given samples of wise and X is can we estimate beta?"
        ],
        [
            "So one way to look at this problem is to take advantage of the sparsity structure that each task might have, so we can treat each task independently.",
            "The famous lasso does that for us.",
            "If you look at the last solar so we can minimize a loss function plus norm one of.",
            "Each of these tasks.",
            "Here Norm 1 means some of the absolute values of each task and it is well known that it takes advantage of the sparsity of the underlying tasks."
        ],
        [
            "The other approach is to take advantage of the features that are shared across many tasks.",
            "If you look at the picture I gave you, you see that there are three features that are shared across all tasks.",
            "So if we can somehow.",
            "Find the regularizer that promotes this kind of structure as opposed to simple sparsity structure.",
            "We might be able to take advantage of this.",
            "And it's been studied that norms like L1 LP.",
            "Where by LP I mean we take LP norm over the rows and then we take L1 norm over the LP norms.",
            "Basically we add them up over all rows.",
            "It turned out this performs in these situations.",
            "If we have a lot of sparsity, it performs well, in particular, here I've shown norm one Infinity.",
            "Well by Infinity I mean taking the maximum over each row and then add them up.",
            "And you can imagine that if I push the maximum to be 0, then all the elements in a row will be 0.",
            "And if I have a non zero maximum then there is no price for.",
            "Each entry to be non 0 because we pay the price by picking just one maximum.",
            "So what really does it promotes this type of structure?",
            "So this is another approach to this."
        ],
        [
            "So it turned out in terms of the performance loss, so does not model the shared sparsity, so it's good for this type of the picture I'm showing on the top.",
            "And group lasso does not model individual sparsity, so it performs well, their features are shared across many tasks.",
            "As you can see."
        ],
        [
            "But what about a more realistic data?",
            "In more realistic data, we have cases where some features are shared across many tasks, some are shared across few tasks.",
            "What can we do in this case?"
        ],
        [
            "Well.",
            "Let's look at the real.",
            "Situation this paper that I've shown here and then let's somehow define what we mean by a shared feature.",
            "For instance, here I said if it's feature shared across among four tasks or more, I call it shared feature.",
            "If not not and then I put this shared feature in one matrix, call it B and I leave the rest in the other matrix S. OK.",
            "So then the hope is if I can regularize these two matrices properly then I might be able to do better than both of these two quote unquote clean models.",
            "And This is why we call this model as dirty model.",
            "So it's not either one of those."
        ],
        [
            "So I pick a threshold D here it's four and I split my matrix into these two matrices.",
            "And then motivated by this I proposed this algorithm.",
            "The algorithm is just minimizing the L2 loss plus regularize B&S.",
            "With respect to suitable regularizers.",
            "Notice that I'm only interested in recovery of beat Beta, so I don't care about any particular B or any particular S, so in the sense I don't care about the value of D that I split.",
            "If these two things, although in theory it turned out D is exactly equal to the ratio of Lambda over Lambda S. So that's ratio that gives us this split, but we don't care because.",
            "We only want to recover beta.",
            "We don't care about any particular B or any particular S."
        ],
        [
            "So let's move on to a special task, a special case of two tasks, where each task has S feature and a portion of Alpha of these two.",
            "Features are shared across the tasks.",
            "OK, so you see if Alpha is a small, a few portion is shared.",
            "If Alpha is large, larger portions."
        ],
        [
            "So if you have little overlap, say Alpha is .3, I plot the probability of success versus real scale sample size for these three algorithms that I told you, what is the probability of success.",
            "So for each value of sample size I produce a number of matches and then I try to recover this structure of beta and I see how many times I see succeed and how many times I failed.",
            "So for all these three algorithms, you will see that there is a sharp transition that the probability of success goes from zero to one in this case, since we have little overlap, you could intuitively imagine that last so would perform better than L1L Infinity, but you will see that dirty model performed better than both."
        ],
        [
            "If I have more overlap 2/3 you will see the last one, the one in Infinity, performs almost the same but still dirty model has better."
        ],
        [
            "Performance if I go to high overlap .8.",
            "As you can imagine, L1L Infinity becomes better.",
            "Then lasso and early model still outperforms both in the sense that it requires less samples to get into the probability of success equal to 1."
        ],
        [
            "Of course, these matches the theoretical results that we have for Lasso.",
            "Vanwright showed that this risk air sample size ANOVA reslock P is equal to 2 four group Lasso Negahban at all.",
            "They show that it's equal to 4 -- 3 Alpha."
        ],
        [
            "And as a consequence, you will see that if Alpha is less than 2/3, lasso is better.",
            "If Alpha is greater than 2/3 then group lasso or L1L Infinity in this case perform."
        ],
        [
            "Better, however.",
            "A priori I don't know what Alpha is and hence I don't know which one to choose.",
            "Should I choose last?",
            "So should I choose group lasso?",
            "I don't know because I don't know what the Alpha is.",
            "So here is our result for dirty model.",
            "So dirty model has the sample complexity of two minus Alpha, which theoretically outperforms both of those two methods for the range of Alpha between zero and one."
        ],
        [
            "In summary, if you look at this empirical plot, you will see that last.",
            "So I plotted their phase transition threshold versus different values of Alpha.",
            "You will see that Lasso is almost constant to L1 and Infinity is 4 -- 3 Alpha and you will see on the bottom that are dirty.",
            "Model outperformed both of them in all regimes and it has 2 minus Alpha sample complexity."
        ],
        [
            "So to give you a more formal statement of our theorems, just.",
            "Introduce the notation.",
            "I use UK to show the support of taskey.",
            "You is the Union of support.",
            "X is distributed normal, Zero Sigma and S is the sparsity is the maximum size of this support and these are two standard.",
            "Assumptions if you are being too Professor Bowman tutorial, you know these two things.",
            "These are incoherence and eigenvalue conditions very standard conditions."
        ],
        [
            "Then here is our general theorem for our test case.",
            "For Gaussian design we are saying that if Lambda B and lambdas are suitably lower bounded for any skills as Sr log P with high probability, there is no false exclusion.",
            "Which means the support of true beta is included in the support of True Beta Hat and we have this L Infinity bounds.",
            "So the values are very close.",
            "And Moreover, if we have this assumption that the minimum energy and matrix beta is greater than some this value, then there is no false inclusion.",
            "Considering false exclusion, you will get the support equals the support."
        ],
        [
            "Beta Hut.",
            "So here is our theory for two test case.",
            "I already told you here we showed that if N / S log P modulo that 2 minus Alpha, S is greater than two minus Alpha.",
            "You will always succeed with high probability, and if it's less than that you will always fail.",
            "So this shows it's a really sharp bound and slightly above it.",
            "You'll succeed slightly below that."
        ],
        [
            "Buffet.",
            "I just want to make a general comment on 30 models at the end.",
            "As you saw there, the model was just a super simple superposition of simple structures.",
            "People have looked at sparse, plus block sparse, but other people have looked at sparse post low rank block, sparse plus low rank and so on.",
            "And in fact there is a workshop.",
            "Book come up and robust statistical learning.",
            "So if you like this appetizer, go for an."
        ],
        [
            "In summary, multitask learning is challenging when there is partial overlap across tasks.",
            "Relevant structure is neither sparse nor block sparse.",
            "A superposition of simple structure I30 model as we call this, surprisingly useful for modeling such dirty structures.",
            "For the multi task learning problem, The Dirty model outperforms solo structure, lasso and group lasso, and thank you for your attention.",
            "Questions.",
            "So I had two questions.",
            "One is it looks like it's very nice at either extreme and I'm wondering about in the middle where neither lasso nor the one norm Q norm would be optimal in the middle.",
            "Do you think it's also a very effective procedure as well?",
            "Yes, in fact, in fact, this this figure that I kind of summarized the performance is if you look at this you will see that in all regimes of sparsity, at least for two test case that we studied, we show that we outperformed both of them, so it's good in all regimes in in between for all.",
            "I guess the question is, Can you imagine that there's another procedure that in this intermediate regime?",
            "Would actually be substantially more effective.",
            "Where do you think you're actually doing close to optimal?",
            "Well, there could be yes.",
            "Of course, if you come up with, I think the most important thing here is to find the right model that fits the data the best.",
            "So if you come up with a better model that fits the data better, of course you'll get better results.",
            "So actually in your model that non shared features, you have sparsity using L1.",
            "So I just like to get your comments that suppose you change your model such that the non shared feature have L2 norm regularization instead for one because there's sometimes the data you have low rank structure but that have actually more general noise model for the non shared parts.",
            "OK thank you.",
            "So.",
            "Yes there.",
            "You need to.",
            "You need to consider two things.",
            "One is that.",
            "If you have two types of L1LQ and you split it in two ways, then it might be the case that you are.",
            "You are penalizing only one of them, whichever is smaller.",
            "This could be one of them, but if your question is that, can we consider L2 instead of L Infinity?",
            "Because I didn't really hear your question well, but if your question is.",
            "Can we consider L2 instead of Infinity?",
            "The answer is yes, and that could be another analysis that can be done, but we considered a one and Infinity in contrast with L1L1 because it was giving us kind of two extremes.",
            "Question during the poster.",
            "Thank you, OK?",
            "Anyone else?",
            "But I had one other question which was sometimes multitest learning gives its maximum benefit when sample size is quite small and then we sometimes observe that for very large sample sizes that actually can be counterproductive.",
            "It can hurt.",
            "Do you ever observe that sort of phenomenon with algorithm or is it sort of always safe to use?",
            "I have not observed that phenomenon yet, but this is something probably I need to look at and think about.",
            "Thank you, thank you speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "Thank you for coming.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present this work.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with Pradeep Ravikumar so Jason Kavian children all from University of Texas Austin.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So oftentimes, in practice in many areas of science and engineering like biology, vision, etc, we face a situation that we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "A number of variables P with very undersampled.",
                    "label": 1
                },
                {
                    "sent": "Size N and the classical curse of dimensionality forces us to take more and more samples if we want to have.",
                    "label": 0
                },
                {
                    "sent": "A consistent estimation.",
                    "label": 0
                },
                {
                    "sent": "However, over the past decades, our research shows that if the parameter leaves has a low dimensional structure, then there might be a hope and people have looked at different low dimensional structures such as sparsity, block sparsity, etc.",
                    "label": 0
                },
                {
                    "sent": "But the question is, what if the parameter that we are interested in does not?",
                    "label": 1
                },
                {
                    "sent": "Have the clean structure, such as the ones I just counted.",
                    "label": 0
                },
                {
                    "sent": "Then we want to propose a method which is a kind of zero approach to this problem.",
                    "label": 0
                },
                {
                    "sent": "A simple superposition of structures which is still low dimensional but surprisingly very useful in dealing with dirty data.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the multitask learning problem.",
                    "label": 1
                },
                {
                    "sent": "The Multi task learning problem is the problem that we have a number of tasks to learn.",
                    "label": 1
                },
                {
                    "sent": "And they have some sort of shared structure.",
                    "label": 0
                },
                {
                    "sent": "As an example, consider you have two writers, writer, one and writer, two and each one of them.",
                    "label": 0
                },
                {
                    "sent": "We ask them to write the letter A and then we want to learn a profile for each one of these writers.",
                    "label": 0
                },
                {
                    "sent": "So one way is to learn the profile individually for each of these writers.",
                    "label": 0
                },
                {
                    "sent": "The other is to learn them together so the main question here is that is there any advantage if I learn individual profiles kind of jointly?",
                    "label": 0
                },
                {
                    "sent": "Of course, this is a very general problem, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we look at a multiple linear regression set up.",
                    "label": 1
                },
                {
                    "sent": "There for each task we have linear samples.",
                    "label": 0
                },
                {
                    "sent": "And of course we have some noise.",
                    "label": 0
                },
                {
                    "sent": "So in this figure that I have shown each column.",
                    "label": 0
                },
                {
                    "sent": "In the response matrix, why an parameter matrix beta corresponds to 1 task?",
                    "label": 0
                },
                {
                    "sent": "So we are sampling each task with them with matrix X.",
                    "label": 0
                },
                {
                    "sent": "An look at the matrix Y.",
                    "label": 0
                },
                {
                    "sent": "So the question is, given samples of wise and X is can we estimate beta?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way to look at this problem is to take advantage of the sparsity structure that each task might have, so we can treat each task independently.",
                    "label": 0
                },
                {
                    "sent": "The famous lasso does that for us.",
                    "label": 0
                },
                {
                    "sent": "If you look at the last solar so we can minimize a loss function plus norm one of.",
                    "label": 0
                },
                {
                    "sent": "Each of these tasks.",
                    "label": 0
                },
                {
                    "sent": "Here Norm 1 means some of the absolute values of each task and it is well known that it takes advantage of the sparsity of the underlying tasks.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other approach is to take advantage of the features that are shared across many tasks.",
                    "label": 0
                },
                {
                    "sent": "If you look at the picture I gave you, you see that there are three features that are shared across all tasks.",
                    "label": 0
                },
                {
                    "sent": "So if we can somehow.",
                    "label": 0
                },
                {
                    "sent": "Find the regularizer that promotes this kind of structure as opposed to simple sparsity structure.",
                    "label": 0
                },
                {
                    "sent": "We might be able to take advantage of this.",
                    "label": 0
                },
                {
                    "sent": "And it's been studied that norms like L1 LP.",
                    "label": 0
                },
                {
                    "sent": "Where by LP I mean we take LP norm over the rows and then we take L1 norm over the LP norms.",
                    "label": 0
                },
                {
                    "sent": "Basically we add them up over all rows.",
                    "label": 0
                },
                {
                    "sent": "It turned out this performs in these situations.",
                    "label": 0
                },
                {
                    "sent": "If we have a lot of sparsity, it performs well, in particular, here I've shown norm one Infinity.",
                    "label": 0
                },
                {
                    "sent": "Well by Infinity I mean taking the maximum over each row and then add them up.",
                    "label": 0
                },
                {
                    "sent": "And you can imagine that if I push the maximum to be 0, then all the elements in a row will be 0.",
                    "label": 0
                },
                {
                    "sent": "And if I have a non zero maximum then there is no price for.",
                    "label": 0
                },
                {
                    "sent": "Each entry to be non 0 because we pay the price by picking just one maximum.",
                    "label": 0
                },
                {
                    "sent": "So what really does it promotes this type of structure?",
                    "label": 0
                },
                {
                    "sent": "So this is another approach to this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turned out in terms of the performance loss, so does not model the shared sparsity, so it's good for this type of the picture I'm showing on the top.",
                    "label": 0
                },
                {
                    "sent": "And group lasso does not model individual sparsity, so it performs well, their features are shared across many tasks.",
                    "label": 1
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what about a more realistic data?",
                    "label": 1
                },
                {
                    "sent": "In more realistic data, we have cases where some features are shared across many tasks, some are shared across few tasks.",
                    "label": 0
                },
                {
                    "sent": "What can we do in this case?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the real.",
                    "label": 0
                },
                {
                    "sent": "Situation this paper that I've shown here and then let's somehow define what we mean by a shared feature.",
                    "label": 0
                },
                {
                    "sent": "For instance, here I said if it's feature shared across among four tasks or more, I call it shared feature.",
                    "label": 0
                },
                {
                    "sent": "If not not and then I put this shared feature in one matrix, call it B and I leave the rest in the other matrix S. OK.",
                    "label": 0
                },
                {
                    "sent": "So then the hope is if I can regularize these two matrices properly then I might be able to do better than both of these two quote unquote clean models.",
                    "label": 0
                },
                {
                    "sent": "And This is why we call this model as dirty model.",
                    "label": 0
                },
                {
                    "sent": "So it's not either one of those.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I pick a threshold D here it's four and I split my matrix into these two matrices.",
                    "label": 0
                },
                {
                    "sent": "And then motivated by this I proposed this algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is just minimizing the L2 loss plus regularize B&S.",
                    "label": 0
                },
                {
                    "sent": "With respect to suitable regularizers.",
                    "label": 0
                },
                {
                    "sent": "Notice that I'm only interested in recovery of beat Beta, so I don't care about any particular B or any particular S, so in the sense I don't care about the value of D that I split.",
                    "label": 0
                },
                {
                    "sent": "If these two things, although in theory it turned out D is exactly equal to the ratio of Lambda over Lambda S. So that's ratio that gives us this split, but we don't care because.",
                    "label": 0
                },
                {
                    "sent": "We only want to recover beta.",
                    "label": 0
                },
                {
                    "sent": "We don't care about any particular B or any particular S.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's move on to a special task, a special case of two tasks, where each task has S feature and a portion of Alpha of these two.",
                    "label": 1
                },
                {
                    "sent": "Features are shared across the tasks.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see if Alpha is a small, a few portion is shared.",
                    "label": 0
                },
                {
                    "sent": "If Alpha is large, larger portions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you have little overlap, say Alpha is .3, I plot the probability of success versus real scale sample size for these three algorithms that I told you, what is the probability of success.",
                    "label": 1
                },
                {
                    "sent": "So for each value of sample size I produce a number of matches and then I try to recover this structure of beta and I see how many times I see succeed and how many times I failed.",
                    "label": 0
                },
                {
                    "sent": "So for all these three algorithms, you will see that there is a sharp transition that the probability of success goes from zero to one in this case, since we have little overlap, you could intuitively imagine that last so would perform better than L1L Infinity, but you will see that dirty model performed better than both.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I have more overlap 2/3 you will see the last one, the one in Infinity, performs almost the same but still dirty model has better.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Performance if I go to high overlap .8.",
                    "label": 1
                },
                {
                    "sent": "As you can imagine, L1L Infinity becomes better.",
                    "label": 0
                },
                {
                    "sent": "Then lasso and early model still outperforms both in the sense that it requires less samples to get into the probability of success equal to 1.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, these matches the theoretical results that we have for Lasso.",
                    "label": 0
                },
                {
                    "sent": "Vanwright showed that this risk air sample size ANOVA reslock P is equal to 2 four group Lasso Negahban at all.",
                    "label": 1
                },
                {
                    "sent": "They show that it's equal to 4 -- 3 Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as a consequence, you will see that if Alpha is less than 2/3, lasso is better.",
                    "label": 0
                },
                {
                    "sent": "If Alpha is greater than 2/3 then group lasso or L1L Infinity in this case perform.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better, however.",
                    "label": 0
                },
                {
                    "sent": "A priori I don't know what Alpha is and hence I don't know which one to choose.",
                    "label": 0
                },
                {
                    "sent": "Should I choose last?",
                    "label": 0
                },
                {
                    "sent": "So should I choose group lasso?",
                    "label": 1
                },
                {
                    "sent": "I don't know because I don't know what the Alpha is.",
                    "label": 1
                },
                {
                    "sent": "So here is our result for dirty model.",
                    "label": 0
                },
                {
                    "sent": "So dirty model has the sample complexity of two minus Alpha, which theoretically outperforms both of those two methods for the range of Alpha between zero and one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In summary, if you look at this empirical plot, you will see that last.",
                    "label": 0
                },
                {
                    "sent": "So I plotted their phase transition threshold versus different values of Alpha.",
                    "label": 0
                },
                {
                    "sent": "You will see that Lasso is almost constant to L1 and Infinity is 4 -- 3 Alpha and you will see on the bottom that are dirty.",
                    "label": 0
                },
                {
                    "sent": "Model outperformed both of them in all regimes and it has 2 minus Alpha sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to give you a more formal statement of our theorems, just.",
                    "label": 0
                },
                {
                    "sent": "Introduce the notation.",
                    "label": 0
                },
                {
                    "sent": "I use UK to show the support of taskey.",
                    "label": 1
                },
                {
                    "sent": "You is the Union of support.",
                    "label": 0
                },
                {
                    "sent": "X is distributed normal, Zero Sigma and S is the sparsity is the maximum size of this support and these are two standard.",
                    "label": 0
                },
                {
                    "sent": "Assumptions if you are being too Professor Bowman tutorial, you know these two things.",
                    "label": 0
                },
                {
                    "sent": "These are incoherence and eigenvalue conditions very standard conditions.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then here is our general theorem for our test case.",
                    "label": 0
                },
                {
                    "sent": "For Gaussian design we are saying that if Lambda B and lambdas are suitably lower bounded for any skills as Sr log P with high probability, there is no false exclusion.",
                    "label": 1
                },
                {
                    "sent": "Which means the support of true beta is included in the support of True Beta Hat and we have this L Infinity bounds.",
                    "label": 0
                },
                {
                    "sent": "So the values are very close.",
                    "label": 1
                },
                {
                    "sent": "And Moreover, if we have this assumption that the minimum energy and matrix beta is greater than some this value, then there is no false inclusion.",
                    "label": 0
                },
                {
                    "sent": "Considering false exclusion, you will get the support equals the support.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Beta Hut.",
                    "label": 0
                },
                {
                    "sent": "So here is our theory for two test case.",
                    "label": 1
                },
                {
                    "sent": "I already told you here we showed that if N / S log P modulo that 2 minus Alpha, S is greater than two minus Alpha.",
                    "label": 0
                },
                {
                    "sent": "You will always succeed with high probability, and if it's less than that you will always fail.",
                    "label": 1
                },
                {
                    "sent": "So this shows it's a really sharp bound and slightly above it.",
                    "label": 0
                },
                {
                    "sent": "You'll succeed slightly below that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Buffet.",
                    "label": 0
                },
                {
                    "sent": "I just want to make a general comment on 30 models at the end.",
                    "label": 0
                },
                {
                    "sent": "As you saw there, the model was just a super simple superposition of simple structures.",
                    "label": 1
                },
                {
                    "sent": "People have looked at sparse, plus block sparse, but other people have looked at sparse post low rank block, sparse plus low rank and so on.",
                    "label": 0
                },
                {
                    "sent": "And in fact there is a workshop.",
                    "label": 1
                },
                {
                    "sent": "Book come up and robust statistical learning.",
                    "label": 0
                },
                {
                    "sent": "So if you like this appetizer, go for an.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In summary, multitask learning is challenging when there is partial overlap across tasks.",
                    "label": 1
                },
                {
                    "sent": "Relevant structure is neither sparse nor block sparse.",
                    "label": 1
                },
                {
                    "sent": "A superposition of simple structure I30 model as we call this, surprisingly useful for modeling such dirty structures.",
                    "label": 0
                },
                {
                    "sent": "For the multi task learning problem, The Dirty model outperforms solo structure, lasso and group lasso, and thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So I had two questions.",
                    "label": 0
                },
                {
                    "sent": "One is it looks like it's very nice at either extreme and I'm wondering about in the middle where neither lasso nor the one norm Q norm would be optimal in the middle.",
                    "label": 0
                },
                {
                    "sent": "Do you think it's also a very effective procedure as well?",
                    "label": 0
                },
                {
                    "sent": "Yes, in fact, in fact, this this figure that I kind of summarized the performance is if you look at this you will see that in all regimes of sparsity, at least for two test case that we studied, we show that we outperformed both of them, so it's good in all regimes in in between for all.",
                    "label": 0
                },
                {
                    "sent": "I guess the question is, Can you imagine that there's another procedure that in this intermediate regime?",
                    "label": 0
                },
                {
                    "sent": "Would actually be substantially more effective.",
                    "label": 0
                },
                {
                    "sent": "Where do you think you're actually doing close to optimal?",
                    "label": 0
                },
                {
                    "sent": "Well, there could be yes.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you come up with, I think the most important thing here is to find the right model that fits the data the best.",
                    "label": 0
                },
                {
                    "sent": "So if you come up with a better model that fits the data better, of course you'll get better results.",
                    "label": 0
                },
                {
                    "sent": "So actually in your model that non shared features, you have sparsity using L1.",
                    "label": 0
                },
                {
                    "sent": "So I just like to get your comments that suppose you change your model such that the non shared feature have L2 norm regularization instead for one because there's sometimes the data you have low rank structure but that have actually more general noise model for the non shared parts.",
                    "label": 0
                },
                {
                    "sent": "OK thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yes there.",
                    "label": 0
                },
                {
                    "sent": "You need to.",
                    "label": 0
                },
                {
                    "sent": "You need to consider two things.",
                    "label": 0
                },
                {
                    "sent": "One is that.",
                    "label": 0
                },
                {
                    "sent": "If you have two types of L1LQ and you split it in two ways, then it might be the case that you are.",
                    "label": 0
                },
                {
                    "sent": "You are penalizing only one of them, whichever is smaller.",
                    "label": 0
                },
                {
                    "sent": "This could be one of them, but if your question is that, can we consider L2 instead of L Infinity?",
                    "label": 0
                },
                {
                    "sent": "Because I didn't really hear your question well, but if your question is.",
                    "label": 0
                },
                {
                    "sent": "Can we consider L2 instead of Infinity?",
                    "label": 0
                },
                {
                    "sent": "The answer is yes, and that could be another analysis that can be done, but we considered a one and Infinity in contrast with L1L1 because it was giving us kind of two extremes.",
                    "label": 0
                },
                {
                    "sent": "Question during the poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you, OK?",
                    "label": 0
                },
                {
                    "sent": "Anyone else?",
                    "label": 0
                },
                {
                    "sent": "But I had one other question which was sometimes multitest learning gives its maximum benefit when sample size is quite small and then we sometimes observe that for very large sample sizes that actually can be counterproductive.",
                    "label": 0
                },
                {
                    "sent": "It can hurt.",
                    "label": 0
                },
                {
                    "sent": "Do you ever observe that sort of phenomenon with algorithm or is it sort of always safe to use?",
                    "label": 0
                },
                {
                    "sent": "I have not observed that phenomenon yet, but this is something probably I need to look at and think about.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you speaker again.",
                    "label": 0
                }
            ]
        }
    }
}