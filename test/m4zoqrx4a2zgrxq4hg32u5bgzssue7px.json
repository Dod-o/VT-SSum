{
    "id": "m4zoqrx4a2zgrxq4hg32u5bgzssue7px",
    "title": "Active Matching: Efficient Guided Search for Image Correspondence",
    "info": {
        "author": [
            "Andrew Davison, Department of Computing, Imperial College London"
        ],
        "published": "Nov. 8, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/rss2010_davison_ameg/",
    "segmentation": [
        [
            "I was never particularly satisfied with ransack 'cause it involved random sampling.",
            "Some kind of fixed thresholds and so on.",
            "There's an algorithm which is better which actually originated here instead of author, called JCBB Join Compatibility Branching Band, which is quite similar to our probabilistic kind of version of ranch ransack.",
            "If you like, basically it's still the same situation where you get some candidate matches and then you're going to resolve which are the right ones, but you're going to do that with a probabilistic interpretation tree, which basically gives you a joint probability.",
            "Which set of matches are consistent with each other and you can find the correct set like that."
        ],
        [
            "But we still wondered, can we do better?",
            "So basically the thing that I think is going on there that seems unnecessary is the fact that you're going to search all of these regions exhaustively for the different features and then sort it out afterwards.",
            "Can't we do something more efficient where we do a much more step by step search, so IE search for one feature, see what we find and then see what that tells us about searching for the other features.",
            "So this is what I'm thinking is the active component here.",
            "Rather than you know, bottom up processing.",
            "We're going to do a little bit of processing, then think about it, decide what to do next, and then do a bit more processing and have this loop like this.",
            "So I had my first kind of steps on this where a paper I presented I CV 2005.",
            "So this is showing a step by step search.",
            "In a simple kind of toy example where we're trying to track a box moving over a desk, for instance, and the numbers you can see here are actually mutual information, so the numbers in black for each of these features show basically the number of bits I expect to obtain.",
            "In reducing the uncertainty of the position of an object by measuring a certain feature, so we've got this object moving over a desk.",
            "I want to track it.",
            "There's several features I can possibly measure.",
            "We can evaluate mutual information to say.",
            "If I measure this feature, how much will it reduce my uncertainty in where that object is so that how much it reduces depends on exactly where the feature is on the object, and then by guiding a sequential search from feature to feature to feature, trying to always get the most.",
            "Information gain we can come up with a sequential strategy.",
            "So that was, you know, some some nice step."
        ],
        [
            "And and you know, we worked out the right way to evaluate mutual information, everything and these kind of tracking problems, but it wasn't a realistic algorithm because it couldn't cope with this sort of ambiguity that we have.",
            "So in the problem that that ransack resolves where some of the matches you you try first will be wrong and you've got to workout which are the right ones in which are the wrong ones that couldn't be coped with.",
            "So the next stage was an algorithm we call active matching.",
            "So now we're going to do the same kind of step by step thing, but we're going to use a mixture of.",
            "Gaussians representation for the search state and we're going to try and follow this step."
        ],
        [
            "By step approach.",
            "So this is basically how it works in an example, so here's my.",
            "My starting situation.",
            "I've made this prediction about where the features are in an image.",
            "I'm going to choose one of these features to search for first, so I'm going to choose this one here.",
            "I'm going to search this ellipse and see if I find any candidate features that look like the one I'm looking for.",
            "So suppose in this particular example there are two places in this ellipse that matched to their feature.",
            "Descriptor that I'm searching for.",
            "So what do we do in response to this where we basically now initialize a mixture distribution which has three components, so when I search this.",
            "Philips here and found two things.",
            "There are now three discrete possibilities, either that this one is correct or this one is correct, or that neither of them is correct, right?",
            "So those are the only three possibilities, so therefore we launch three Gaussian distributions and we give them weights.",
            "So they have three weights there that add up to one.",
            "So there's one Gaussian distribution, which is basically what would happen if this was the correct match.",
            "So in that case I can then take advantage of the correlation.",
            "Information I have between all the different predictions and I can shrink the search regions for all the other features like this so there small... on the left here for each feature are basically saying this is what's going on.",
            "If this match was correct.",
            "So if this match was correct I've reduced the search region for this feature from this to this.",
            "On the other hand, if this one was correct and I can shrink it from this to this so it shrunk by about the same amount but it shrunk in a different place if neither of them is correct, then I can't shrink them at all, so.",
            "I'm going to still keep that kind of base Gaussian there, so now we have a mixture of Gaussians representing our search state, and we're now going to follow a sequence of actions which we think are as efficiently as possible.",
            "Going to resolve this mixture into a single, highly piqued component.",
            "So at each step we're selecting one feature to search for, and one of these Gaussian search regions to search in.",
            "So what happens next?",
            "I choose this feature to measure next and I choose this small region to search so you will see that I'm never actually needing to go back and search big... again.",
            "I can now kind of concentrate on the small ones and see if they kind of confirm or reject the current hypothesis that I've got.",
            "So if I search this one so Luckily I've chosen.",
            "Actually this must have been the right match.",
            "I've now chosen to search this one and I found something there as well, so that's actually really good confirmation that the the hypothesis that this one was correct.",
            "It is true because that's told me to search a small region here and there was actually what I was expecting to see there.",
            "So that's really good confirmation.",
            "So when I act, I update what happens is with boosted the probability of this hypothesis.",
            "So Gaussian one has gone up to wait, not .99.",
            "And the probability of all the other hypothesis has got squashed down 'cause they will need to add up to one.",
            "And what's in fact happened here is that the probability of their like the background null hypothesis that neither of the initial matches with correct has gone down so low.",
            "So 0.00001 or something that we've just deleted it.",
            "It's basically become impossible.",
            "So we continue this sequence of actions.",
            "So at each step we're choosing something we're measuring it, so you'll see that that measurement was strong enough to basically delete all but one.",
            "Gaussian now and you'll see that we're in a very good situation now.",
            "Where to finish off?",
            "We just checking small... for the rest of the features, where more rest you know, making a very accurate prediction.",
            "Ticking off that it's there.",
            "So that kind of.",
            "Key result here is that we've achieved global matching on that on that image with a lot fewer image search operations than were needed with one of the algorithms like JCB that does get candidates and then resolve.",
            "So these big green... here are the regions we would have had to search with one of the other algorithms with active matching.",
            "We've only searched the yellow regions, so there's one big yellow region for the 1st.",
            "Feature but all the others are small, so in this particular example we were kind of lucky and we went straight down the right path, but the But the algorithm also has the ability to backtrack.",
            "So if you were chosen unlucky path to go down 1st and you weren't finding confirmation, then the probability of that hypothesis would start to drift down and the other ones would come up and you'd eventually switch your attention to some other possibilities."
        ],
        [
            "OK so I just explain a little bit more detail how active matching works, so this is our joint prediction.",
            "So if this is the set of features we're looking for it in an image, we represent what we call the search state.",
            "With the Gaussian mixture over, you know in the state dimension of the features that we're trying to find.",
            "So if we're trying to search for 10 features in an image, feature position has two components, so we have a 20 by 20.",
            "20 element state vector Anna 20 by 20 you know covariance matrix representing a joint Gaussian distribution over the positions of those features in the image.",
            "So the fact that it's joint is important, 'cause that's what captures all this correlation information, which is really important.",
            "So we've got a mixture of these Gaussians where each Gaussian has a weight.",
            "Lambda analla."
        ],
        [
            "Kilometres add up to one.",
            "So every time we choose to make a measurement, we're choosing one of the... to searching.",
            "So we choose one Gaussian and one feature, and that gives US1 ellipse in the image.",
            "To search, we search the ellipse.",
            "We see how many candidate matches we find that agree with the descriptor of the particular feature we're looking for, and then based on the result of that we update our mixture.",
            "So if this is the mixture before we've made that search, so Gaussian wine and Gaussian two.",
            "So here with showing.",
            "1D kind of projection really of what's really going on in 2D.",
            "We can we find you know one or more matches.",
            "We form a likelihood function.",
            "We multiply this by this to then get a posterior where we make some approximations to force the posterior to again be a mixture of gases."
        ],
        [
            "So just to explain what their likelihood function is there.",
            "So imagine this situation where again this is a 1D projection of a 2D search.",
            "There's this region that we're searching in, so that's the kind of Gray region here.",
            "In this particular example, we find you know one match within that region.",
            "So the question is, where has that match come from?",
            "If the actual feature that we're looking for is actually somewhere near to the position where we were searching, then that's probably a true positive match and.",
            "You know we were most likely to find it if it was exactly in that position, but there's also some probability of finding it if we are a couple of pixels either side 'cause you know feature matches aren't perfectly located, they have one or two pixels kind of uncertainty, so that gives us this kind of Gaussian bit here.",
            "If we found a match here, but the feature was actually somewhere else within that region, then that match we found must have been a false positive.",
            "So some kind of clutter and we must have missed the true positive.",
            "So we've got, you know, a false negative somewhere, a true positive, and then.",
            "Whereas if the actual feature we were looking for wasn't in the region we're researching at all, then.",
            "There's no false negative, but we've still found a false positive here.",
            "So basically that this tells us you know how to go about performing the Heights of these different things in the likelihood function.",
            "At the actual probabilities here that we built into this model could come from training.",
            "So, for instance, suppose you looked at a particular type of feature, so like a black corner against a white background, and you took loads of video data and on every frame you found how many corners are in an image you know which are black corners against white background.",
            "I mean, we actually did this and we found on average if we take loads of indoor images, there are 20 features like that in every single image.",
            "Black Corner against a white background.",
            "So therefore you can workout per pixel probability of there being some clutter.",
            "You know of something that looks like that feature in an image, so every time you search a region of a certain size, there's a certain probability that you will find a feature that looks like the one you're looking for, and that could be a different.",
            "You could model that differently for every sort of feature.",
            "Of course.",
            "I mean what we've?",
            "Make a certain sorts of features like Black Corner against black background are probably much more common than other sorts of features, but in our implementation we've just used some standard values.",
            "Same for all sorts."
        ],
        [
            "Features So what you know actually happens anecdotally, is if you.",
            "Search a region here and you find.",
            "At least one match.",
            "What will basically happen is you'll get a nice new stronger hypothesis at positions where you found matches and everything else will kind of get a bit squashed down if you search a big region and you don't find any matches, then basically the probability of that hypothesis goes down a bit, and the probability of everything else drifts up and you just keep doing that and it kind of sorts itself all out.",
            "And as I said, anytime we have weights of gas teams that are really really low, we just delete them.",
            "We could probably do something.",
            "Bit cleverer than that, but we just delete them at them."
        ],
        [
            "So the last part that I haven't talked about is how do we actually choose which feature to measure next, so that's a very important part of this strategy.",
            "How do we choose our sequence of measurements?",
            "And we're going to use information theoretic scores here.",
            "Basically, we want to find the mutual information for each candidate measurement, so that could be the mutual information of that candidate measurement and the state parameters were actually interested in finding, which could be the position of the camera, or perhaps the whole kind of slam state.",
            "Or something like that.",
            "So and something we can also possibly do is to think OK, I've got the mutual information.",
            "That's how much information this measurement I'm expecting to give me, but maybe I could also divide by kind of cost, so searching big regions is particularly bad in terms of image processing cost, so we could divide the expected mutual information by the area of the region we're expecting to search, and we call that like information efficiency.",
            "So how much work do you have to do?",
            "Puppet basically."
        ],
        [
            "You can make a measure like that, so the very nice thing about a mutual information.",
            "Measure is it's in this absolute units of bits, so you can directly compare different sorts of features and all kinds of things.",
            "And if you actually breakdown with our mixture of Gaussians representation, what the mutual information actually is.",
            "It turns out you can separate it quite nicely and intuitively into a discrete part and a continuous path.",
            "So the continuous part is all about.",
            "If I measure this feature in our launcher, nice nuthin Gaussian.",
            "How much thinner is that Gaussian than the old one?",
            "So basically, if you make a measurement that.",
            "That's going to squash the standard deviation of a Gaussian by a factor of two that you're gaining one bit of information.",
            "There's also a discrete part that says how much is this measurement going to change.",
            "The weights of my different hypothesis.",
            "So basically, if you've got like a bar chart, a histogram, you can evaluate the entropy of that, and you can if you make a measurement that's going to change the distribution from something that's kind of flatten even to something which is peaked, then there's also a gain in information.",
            "That so we can.",
            "So that's how we actually evaluate."
        ],
        [
            "Which feature to measure next?",
            "OK, so here's the whole thing running on some sequences, so this is within our kind of older.",
            "Eks"
        ],
        [
            "Slam system.",
            "So.",
            "The big kind of result here is, as we said before, we can get matching on each frame by searching a much smaller part of the image than we previously had to.",
            "So again, basically we're ignoring the whole image here, apart from the little bits within the yellow regions on each frame.",
            "So what that enables us to do in this example is track really quite fast motion.",
            "So to track fast motion we have relatively weak predictions, big search... and if we had to search all of those exhaustively, that would be an expensive image processing task.",
            "But with this active approach we can focus down much more quickly."
        ],
        [
            "And some timings breaks down so so this shows what JCB does on some of these you know sequences like this.",
            "You're spending an awful lot of time on the image processing.",
            "Basically worse with active matching we reduce image processing a lot.",
            "That's the blue kind of parts here.",
            "But actually another problem comes around because we've saved a lot of image processing, but we've kind of introduced a new cost, which is this kind of thinking time in between measurements after I've made one measurement, I need to update my mixture.",
            "Evaluate mention information, decide what to do next.",
            "It turns out in the end.",
            "Maybe that is worse to do that then just to do everything exhaustively in the 1st place.",
            "So we thought, OK, what are we?",
            "What do we do about this?",
            "So in fact, that gets particularly bad if you want to match really a lot of features per frame, and actually we do so five years ago in SLAM we were quite happy matching 1020 features per frame.",
            "But if we look at any modern system, everyone's matching hundreds of features per frame now and to really get accuracy in SLAM that that's that's well proven.",
            "Now that's what you want to do.",
            "So can we make an active active matching algorithm that was scaled to being able to match?",
            "Hunt"
        ],
        [
            "The features per frame."
        ],
        [
            "In real time.",
            "So to evaluate to look at this problem.",
            "In the meantime, we've been doing some work on thinking about the correlation structure in in visual Maps.",
            "So if you look at this video here and think about how the features are moving across the image as the camera moves, you'll see that the correlation between features.",
            "So how similarly they move depends on their positions in the scene.",
            "So features that are all in a similar position.",
            "So all of the stuff in the background they move in a very consistent way, so they're very highly correlated in the way they move things that are under foreground and also close to each other.",
            "Also have a very highly correlated movement, but there's relatively weak correlation between features in the foreground and features.",
            "In the background.",
            "So we thought OK."
        ],
        [
            "Can we understand this correlation structure?",
            "You know, in a nicer, deeper way that would then allow us to do something also clever in inactive matching.",
            "So rather than just thinking about mutual information of each feature with the state that you're trying to find, we thought let's think about mutual information between pairs of features.",
            "So basically, this illustration here shows you a set of predictions for features in an image and then pairwise mutual informations between them.",
            "So this basically says if I measure this feature, how much does it allow me to reduce?",
            "The search region for this feature, so in in terms of bits, so I if I measure this feature, there's 2.1 bits of information with this feature, so I can reduce the standard deviation of this search region by a factor of about two.",
            "So if we look at particular features here, so 2 features in the background here have a relatively high mutual information of 1.9 bits, whereas a feature in the background and a feature in the foreground have a low mutual information of North Point 6."
        ],
        [
            "OK, so then we discovered this.",
            "Kind of old stuff for how do you take her a dense graph here?",
            "So this is this is a graph of mutual information links between us set of variables.",
            "There's an algorithm for the child nutri which enables you to make the tree like approximation to this whole graph, which best captures the correlation structure here.",
            "So basically the child new tree is the factorizable distribution here, which most closely approximates the full distribution.",
            "Represented by this dense graph in terms of minimum KL divergent between the two."
        ],
        [
            "Distributions and actually.",
            "We already have this mutual information graph, so it's very easiest for us to calculate the child you treat and the actual algorithm to calculate the channel.",
            "New tree is almost trivial once you have the mutual information graph.",
            "All you have to do is look at your pairwise link."
        ],
        [
            "So this whole graph and start joining things together so you take every."
        ],
        [
            "Different feature.",
            "And you."
        ],
        [
            "Join it to the neighbor that has that it has the highest pairwise meet."
        ],
        [
            "And information with so you do that.",
            "Once you get one set of kind of clusters and then you look at each cluster and then join each one again during the clusters up to the one."
        ],
        [
            "I have the heist machine."
        ],
        [
            "Formation with."
        ],
        [
            "So they join up like this.",
            "And that's it.",
            "Is the Cherry Tree.",
            "It's super cheap to compute once you have them.",
            "The Mutual information graph so."
        ],
        [
            "We have this tree.",
            "So now we can do active matching in a much more efficient way that scales much better to large.",
            "Numbers of features.",
            "So basically every time we make a measurement of 1 feature rather than having to do like a full EKF type update over all the features jointly, we can now just passing messages along the tree, belief propagation and use that to update the joint distribution between features.",
            "And there's another very nice thing which is that to calculate mutual information so the mutual information measure we use now is what's the mutual information between a feature that we want to measure and all the other features that we want to.",
            "That are in our search state jointly and it turns out that in in a tree the mutual information between this feature and all the others is the same as the mutual information between this one and just it's it's neighbors.",
            "So that means we can also calculate mutual information very."
        ],
        [
            "Efficiently, so now we have this algorithm called clam chowder active matching, and this is how it works.",
            "So you'll see now hundreds of features per frame.",
            "We're trying to search for.",
            "And you'll see that every time we measure 1 feature we propagate the results of that along the tree and reduce the search regions for all the other features and you get this very nice kind of thing where it seems to relax gradually from the prior onto the final.",
            "Search state.",
            "And so again, the final result where you can see the big difference between the.",
            "The kind of exhaustive regions we would have had to search, and the regions we've."
        ],
        [
            "Being able to search with this active method.",
            "So here's the important graph that shows this is matching time, so this is the full computation time per frame.",
            "To match an image and how it scales with number of features.",
            "So this was original active matching, so remember, real time constraint is probably 30 milliseconds per frame.",
            "We're heading North very quickly.",
            "3040 fifty 60 features.",
            "You know this is already 5 seconds or something, so completely you know outside real time bounds.",
            "This is clam now.",
            "Much nicer behavior up to you know.",
            "Maybe 100 and 150 features, but if we look at how that graph expands, you know this is actually where we want to be.",
            "400 features in 30 milliseconds and a clam is still climbing much too high, so we made a further approximation with an algorithm called subam which actually does take us pretty much into real time performance for 400 features."
        ],
        [
            "The frames are sub ohm is is what we call subset active matching.",
            "So this goes further than clamp it does the Charlie Tree.",
            "Then it even starts chopping the Cherry Tree up into bits and basically deals with each of those separately.",
            "So it chooses a subset of features within the tree.",
            "So you make the tree and then you cut it into some little branches.",
            "You do your active matching independently on each one of those little clusters, and then pass the result of that onto the next cluster down the tree and it turns out that because this is always doing much more local operations, this scales better so that so this really can match hundreds of features at frame rate, and so that's kind of what we now planning to use in our."
        ],
        [
            "Main slam system so finish up so active matching we think it's very nice.",
            "Fully Bayesian robust multi hypothesis feature matching approach which you know importantly takes full benefit and advantage of the priors that you have available in most matching tasks.",
            "So interested in applying these algorithms further so when our area I've been interested in for a long time is high frame rate matching so could we make SLAM systems where you could throw a camera across the room or something like that and track it.",
            "Track it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was never particularly satisfied with ransack 'cause it involved random sampling.",
                    "label": 0
                },
                {
                    "sent": "Some kind of fixed thresholds and so on.",
                    "label": 0
                },
                {
                    "sent": "There's an algorithm which is better which actually originated here instead of author, called JCBB Join Compatibility Branching Band, which is quite similar to our probabilistic kind of version of ranch ransack.",
                    "label": 0
                },
                {
                    "sent": "If you like, basically it's still the same situation where you get some candidate matches and then you're going to resolve which are the right ones, but you're going to do that with a probabilistic interpretation tree, which basically gives you a joint probability.",
                    "label": 0
                },
                {
                    "sent": "Which set of matches are consistent with each other and you can find the correct set like that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we still wondered, can we do better?",
                    "label": 0
                },
                {
                    "sent": "So basically the thing that I think is going on there that seems unnecessary is the fact that you're going to search all of these regions exhaustively for the different features and then sort it out afterwards.",
                    "label": 0
                },
                {
                    "sent": "Can't we do something more efficient where we do a much more step by step search, so IE search for one feature, see what we find and then see what that tells us about searching for the other features.",
                    "label": 0
                },
                {
                    "sent": "So this is what I'm thinking is the active component here.",
                    "label": 0
                },
                {
                    "sent": "Rather than you know, bottom up processing.",
                    "label": 0
                },
                {
                    "sent": "We're going to do a little bit of processing, then think about it, decide what to do next, and then do a bit more processing and have this loop like this.",
                    "label": 0
                },
                {
                    "sent": "So I had my first kind of steps on this where a paper I presented I CV 2005.",
                    "label": 0
                },
                {
                    "sent": "So this is showing a step by step search.",
                    "label": 0
                },
                {
                    "sent": "In a simple kind of toy example where we're trying to track a box moving over a desk, for instance, and the numbers you can see here are actually mutual information, so the numbers in black for each of these features show basically the number of bits I expect to obtain.",
                    "label": 0
                },
                {
                    "sent": "In reducing the uncertainty of the position of an object by measuring a certain feature, so we've got this object moving over a desk.",
                    "label": 0
                },
                {
                    "sent": "I want to track it.",
                    "label": 0
                },
                {
                    "sent": "There's several features I can possibly measure.",
                    "label": 0
                },
                {
                    "sent": "We can evaluate mutual information to say.",
                    "label": 0
                },
                {
                    "sent": "If I measure this feature, how much will it reduce my uncertainty in where that object is so that how much it reduces depends on exactly where the feature is on the object, and then by guiding a sequential search from feature to feature to feature, trying to always get the most.",
                    "label": 0
                },
                {
                    "sent": "Information gain we can come up with a sequential strategy.",
                    "label": 0
                },
                {
                    "sent": "So that was, you know, some some nice step.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And and you know, we worked out the right way to evaluate mutual information, everything and these kind of tracking problems, but it wasn't a realistic algorithm because it couldn't cope with this sort of ambiguity that we have.",
                    "label": 1
                },
                {
                    "sent": "So in the problem that that ransack resolves where some of the matches you you try first will be wrong and you've got to workout which are the right ones in which are the wrong ones that couldn't be coped with.",
                    "label": 1
                },
                {
                    "sent": "So the next stage was an algorithm we call active matching.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to do the same kind of step by step thing, but we're going to use a mixture of.",
                    "label": 1
                },
                {
                    "sent": "Gaussians representation for the search state and we're going to try and follow this step.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By step approach.",
                    "label": 0
                },
                {
                    "sent": "So this is basically how it works in an example, so here's my.",
                    "label": 0
                },
                {
                    "sent": "My starting situation.",
                    "label": 0
                },
                {
                    "sent": "I've made this prediction about where the features are in an image.",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose one of these features to search for first, so I'm going to choose this one here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to search this ellipse and see if I find any candidate features that look like the one I'm looking for.",
                    "label": 0
                },
                {
                    "sent": "So suppose in this particular example there are two places in this ellipse that matched to their feature.",
                    "label": 0
                },
                {
                    "sent": "Descriptor that I'm searching for.",
                    "label": 0
                },
                {
                    "sent": "So what do we do in response to this where we basically now initialize a mixture distribution which has three components, so when I search this.",
                    "label": 0
                },
                {
                    "sent": "Philips here and found two things.",
                    "label": 0
                },
                {
                    "sent": "There are now three discrete possibilities, either that this one is correct or this one is correct, or that neither of them is correct, right?",
                    "label": 0
                },
                {
                    "sent": "So those are the only three possibilities, so therefore we launch three Gaussian distributions and we give them weights.",
                    "label": 0
                },
                {
                    "sent": "So they have three weights there that add up to one.",
                    "label": 0
                },
                {
                    "sent": "So there's one Gaussian distribution, which is basically what would happen if this was the correct match.",
                    "label": 0
                },
                {
                    "sent": "So in that case I can then take advantage of the correlation.",
                    "label": 0
                },
                {
                    "sent": "Information I have between all the different predictions and I can shrink the search regions for all the other features like this so there small... on the left here for each feature are basically saying this is what's going on.",
                    "label": 0
                },
                {
                    "sent": "If this match was correct.",
                    "label": 0
                },
                {
                    "sent": "So if this match was correct I've reduced the search region for this feature from this to this.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if this one was correct and I can shrink it from this to this so it shrunk by about the same amount but it shrunk in a different place if neither of them is correct, then I can't shrink them at all, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to still keep that kind of base Gaussian there, so now we have a mixture of Gaussians representing our search state, and we're now going to follow a sequence of actions which we think are as efficiently as possible.",
                    "label": 0
                },
                {
                    "sent": "Going to resolve this mixture into a single, highly piqued component.",
                    "label": 0
                },
                {
                    "sent": "So at each step we're selecting one feature to search for, and one of these Gaussian search regions to search in.",
                    "label": 0
                },
                {
                    "sent": "So what happens next?",
                    "label": 0
                },
                {
                    "sent": "I choose this feature to measure next and I choose this small region to search so you will see that I'm never actually needing to go back and search big... again.",
                    "label": 0
                },
                {
                    "sent": "I can now kind of concentrate on the small ones and see if they kind of confirm or reject the current hypothesis that I've got.",
                    "label": 0
                },
                {
                    "sent": "So if I search this one so Luckily I've chosen.",
                    "label": 0
                },
                {
                    "sent": "Actually this must have been the right match.",
                    "label": 0
                },
                {
                    "sent": "I've now chosen to search this one and I found something there as well, so that's actually really good confirmation that the the hypothesis that this one was correct.",
                    "label": 0
                },
                {
                    "sent": "It is true because that's told me to search a small region here and there was actually what I was expecting to see there.",
                    "label": 0
                },
                {
                    "sent": "So that's really good confirmation.",
                    "label": 0
                },
                {
                    "sent": "So when I act, I update what happens is with boosted the probability of this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian one has gone up to wait, not .99.",
                    "label": 0
                },
                {
                    "sent": "And the probability of all the other hypothesis has got squashed down 'cause they will need to add up to one.",
                    "label": 0
                },
                {
                    "sent": "And what's in fact happened here is that the probability of their like the background null hypothesis that neither of the initial matches with correct has gone down so low.",
                    "label": 0
                },
                {
                    "sent": "So 0.00001 or something that we've just deleted it.",
                    "label": 0
                },
                {
                    "sent": "It's basically become impossible.",
                    "label": 0
                },
                {
                    "sent": "So we continue this sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "So at each step we're choosing something we're measuring it, so you'll see that that measurement was strong enough to basically delete all but one.",
                    "label": 0
                },
                {
                    "sent": "Gaussian now and you'll see that we're in a very good situation now.",
                    "label": 0
                },
                {
                    "sent": "Where to finish off?",
                    "label": 0
                },
                {
                    "sent": "We just checking small... for the rest of the features, where more rest you know, making a very accurate prediction.",
                    "label": 0
                },
                {
                    "sent": "Ticking off that it's there.",
                    "label": 0
                },
                {
                    "sent": "So that kind of.",
                    "label": 0
                },
                {
                    "sent": "Key result here is that we've achieved global matching on that on that image with a lot fewer image search operations than were needed with one of the algorithms like JCB that does get candidates and then resolve.",
                    "label": 0
                },
                {
                    "sent": "So these big green... here are the regions we would have had to search with one of the other algorithms with active matching.",
                    "label": 0
                },
                {
                    "sent": "We've only searched the yellow regions, so there's one big yellow region for the 1st.",
                    "label": 0
                },
                {
                    "sent": "Feature but all the others are small, so in this particular example we were kind of lucky and we went straight down the right path, but the But the algorithm also has the ability to backtrack.",
                    "label": 0
                },
                {
                    "sent": "So if you were chosen unlucky path to go down 1st and you weren't finding confirmation, then the probability of that hypothesis would start to drift down and the other ones would come up and you'd eventually switch your attention to some other possibilities.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I just explain a little bit more detail how active matching works, so this is our joint prediction.",
                    "label": 0
                },
                {
                    "sent": "So if this is the set of features we're looking for it in an image, we represent what we call the search state.",
                    "label": 1
                },
                {
                    "sent": "With the Gaussian mixture over, you know in the state dimension of the features that we're trying to find.",
                    "label": 0
                },
                {
                    "sent": "So if we're trying to search for 10 features in an image, feature position has two components, so we have a 20 by 20.",
                    "label": 1
                },
                {
                    "sent": "20 element state vector Anna 20 by 20 you know covariance matrix representing a joint Gaussian distribution over the positions of those features in the image.",
                    "label": 0
                },
                {
                    "sent": "So the fact that it's joint is important, 'cause that's what captures all this correlation information, which is really important.",
                    "label": 0
                },
                {
                    "sent": "So we've got a mixture of these Gaussians where each Gaussian has a weight.",
                    "label": 1
                },
                {
                    "sent": "Lambda analla.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kilometres add up to one.",
                    "label": 0
                },
                {
                    "sent": "So every time we choose to make a measurement, we're choosing one of the... to searching.",
                    "label": 1
                },
                {
                    "sent": "So we choose one Gaussian and one feature, and that gives US1 ellipse in the image.",
                    "label": 0
                },
                {
                    "sent": "To search, we search the ellipse.",
                    "label": 0
                },
                {
                    "sent": "We see how many candidate matches we find that agree with the descriptor of the particular feature we're looking for, and then based on the result of that we update our mixture.",
                    "label": 0
                },
                {
                    "sent": "So if this is the mixture before we've made that search, so Gaussian wine and Gaussian two.",
                    "label": 0
                },
                {
                    "sent": "So here with showing.",
                    "label": 0
                },
                {
                    "sent": "1D kind of projection really of what's really going on in 2D.",
                    "label": 0
                },
                {
                    "sent": "We can we find you know one or more matches.",
                    "label": 0
                },
                {
                    "sent": "We form a likelihood function.",
                    "label": 0
                },
                {
                    "sent": "We multiply this by this to then get a posterior where we make some approximations to force the posterior to again be a mixture of gases.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to explain what their likelihood function is there.",
                    "label": 0
                },
                {
                    "sent": "So imagine this situation where again this is a 1D projection of a 2D search.",
                    "label": 0
                },
                {
                    "sent": "There's this region that we're searching in, so that's the kind of Gray region here.",
                    "label": 0
                },
                {
                    "sent": "In this particular example, we find you know one match within that region.",
                    "label": 0
                },
                {
                    "sent": "So the question is, where has that match come from?",
                    "label": 0
                },
                {
                    "sent": "If the actual feature that we're looking for is actually somewhere near to the position where we were searching, then that's probably a true positive match and.",
                    "label": 0
                },
                {
                    "sent": "You know we were most likely to find it if it was exactly in that position, but there's also some probability of finding it if we are a couple of pixels either side 'cause you know feature matches aren't perfectly located, they have one or two pixels kind of uncertainty, so that gives us this kind of Gaussian bit here.",
                    "label": 0
                },
                {
                    "sent": "If we found a match here, but the feature was actually somewhere else within that region, then that match we found must have been a false positive.",
                    "label": 0
                },
                {
                    "sent": "So some kind of clutter and we must have missed the true positive.",
                    "label": 0
                },
                {
                    "sent": "So we've got, you know, a false negative somewhere, a true positive, and then.",
                    "label": 0
                },
                {
                    "sent": "Whereas if the actual feature we were looking for wasn't in the region we're researching at all, then.",
                    "label": 0
                },
                {
                    "sent": "There's no false negative, but we've still found a false positive here.",
                    "label": 0
                },
                {
                    "sent": "So basically that this tells us you know how to go about performing the Heights of these different things in the likelihood function.",
                    "label": 1
                },
                {
                    "sent": "At the actual probabilities here that we built into this model could come from training.",
                    "label": 1
                },
                {
                    "sent": "So, for instance, suppose you looked at a particular type of feature, so like a black corner against a white background, and you took loads of video data and on every frame you found how many corners are in an image you know which are black corners against white background.",
                    "label": 0
                },
                {
                    "sent": "I mean, we actually did this and we found on average if we take loads of indoor images, there are 20 features like that in every single image.",
                    "label": 0
                },
                {
                    "sent": "Black Corner against a white background.",
                    "label": 0
                },
                {
                    "sent": "So therefore you can workout per pixel probability of there being some clutter.",
                    "label": 0
                },
                {
                    "sent": "You know of something that looks like that feature in an image, so every time you search a region of a certain size, there's a certain probability that you will find a feature that looks like the one you're looking for, and that could be a different.",
                    "label": 0
                },
                {
                    "sent": "You could model that differently for every sort of feature.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "I mean what we've?",
                    "label": 0
                },
                {
                    "sent": "Make a certain sorts of features like Black Corner against black background are probably much more common than other sorts of features, but in our implementation we've just used some standard values.",
                    "label": 0
                },
                {
                    "sent": "Same for all sorts.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Features So what you know actually happens anecdotally, is if you.",
                    "label": 0
                },
                {
                    "sent": "Search a region here and you find.",
                    "label": 0
                },
                {
                    "sent": "At least one match.",
                    "label": 0
                },
                {
                    "sent": "What will basically happen is you'll get a nice new stronger hypothesis at positions where you found matches and everything else will kind of get a bit squashed down if you search a big region and you don't find any matches, then basically the probability of that hypothesis goes down a bit, and the probability of everything else drifts up and you just keep doing that and it kind of sorts itself all out.",
                    "label": 0
                },
                {
                    "sent": "And as I said, anytime we have weights of gas teams that are really really low, we just delete them.",
                    "label": 0
                },
                {
                    "sent": "We could probably do something.",
                    "label": 0
                },
                {
                    "sent": "Bit cleverer than that, but we just delete them at them.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the last part that I haven't talked about is how do we actually choose which feature to measure next, so that's a very important part of this strategy.",
                    "label": 0
                },
                {
                    "sent": "How do we choose our sequence of measurements?",
                    "label": 0
                },
                {
                    "sent": "And we're going to use information theoretic scores here.",
                    "label": 0
                },
                {
                    "sent": "Basically, we want to find the mutual information for each candidate measurement, so that could be the mutual information of that candidate measurement and the state parameters were actually interested in finding, which could be the position of the camera, or perhaps the whole kind of slam state.",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "So and something we can also possibly do is to think OK, I've got the mutual information.",
                    "label": 0
                },
                {
                    "sent": "That's how much information this measurement I'm expecting to give me, but maybe I could also divide by kind of cost, so searching big regions is particularly bad in terms of image processing cost, so we could divide the expected mutual information by the area of the region we're expecting to search, and we call that like information efficiency.",
                    "label": 1
                },
                {
                    "sent": "So how much work do you have to do?",
                    "label": 0
                },
                {
                    "sent": "Puppet basically.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can make a measure like that, so the very nice thing about a mutual information.",
                    "label": 0
                },
                {
                    "sent": "Measure is it's in this absolute units of bits, so you can directly compare different sorts of features and all kinds of things.",
                    "label": 0
                },
                {
                    "sent": "And if you actually breakdown with our mixture of Gaussians representation, what the mutual information actually is.",
                    "label": 0
                },
                {
                    "sent": "It turns out you can separate it quite nicely and intuitively into a discrete part and a continuous path.",
                    "label": 0
                },
                {
                    "sent": "So the continuous part is all about.",
                    "label": 1
                },
                {
                    "sent": "If I measure this feature in our launcher, nice nuthin Gaussian.",
                    "label": 0
                },
                {
                    "sent": "How much thinner is that Gaussian than the old one?",
                    "label": 0
                },
                {
                    "sent": "So basically, if you make a measurement that.",
                    "label": 1
                },
                {
                    "sent": "That's going to squash the standard deviation of a Gaussian by a factor of two that you're gaining one bit of information.",
                    "label": 1
                },
                {
                    "sent": "There's also a discrete part that says how much is this measurement going to change.",
                    "label": 0
                },
                {
                    "sent": "The weights of my different hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you've got like a bar chart, a histogram, you can evaluate the entropy of that, and you can if you make a measurement that's going to change the distribution from something that's kind of flatten even to something which is peaked, then there's also a gain in information.",
                    "label": 1
                },
                {
                    "sent": "That so we can.",
                    "label": 0
                },
                {
                    "sent": "So that's how we actually evaluate.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which feature to measure next?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the whole thing running on some sequences, so this is within our kind of older.",
                    "label": 0
                },
                {
                    "sent": "Eks",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slam system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The big kind of result here is, as we said before, we can get matching on each frame by searching a much smaller part of the image than we previously had to.",
                    "label": 0
                },
                {
                    "sent": "So again, basically we're ignoring the whole image here, apart from the little bits within the yellow regions on each frame.",
                    "label": 0
                },
                {
                    "sent": "So what that enables us to do in this example is track really quite fast motion.",
                    "label": 0
                },
                {
                    "sent": "So to track fast motion we have relatively weak predictions, big search... and if we had to search all of those exhaustively, that would be an expensive image processing task.",
                    "label": 0
                },
                {
                    "sent": "But with this active approach we can focus down much more quickly.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And some timings breaks down so so this shows what JCB does on some of these you know sequences like this.",
                    "label": 0
                },
                {
                    "sent": "You're spending an awful lot of time on the image processing.",
                    "label": 0
                },
                {
                    "sent": "Basically worse with active matching we reduce image processing a lot.",
                    "label": 0
                },
                {
                    "sent": "That's the blue kind of parts here.",
                    "label": 0
                },
                {
                    "sent": "But actually another problem comes around because we've saved a lot of image processing, but we've kind of introduced a new cost, which is this kind of thinking time in between measurements after I've made one measurement, I need to update my mixture.",
                    "label": 0
                },
                {
                    "sent": "Evaluate mention information, decide what to do next.",
                    "label": 0
                },
                {
                    "sent": "It turns out in the end.",
                    "label": 0
                },
                {
                    "sent": "Maybe that is worse to do that then just to do everything exhaustively in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So we thought, OK, what are we?",
                    "label": 0
                },
                {
                    "sent": "What do we do about this?",
                    "label": 0
                },
                {
                    "sent": "So in fact, that gets particularly bad if you want to match really a lot of features per frame, and actually we do so five years ago in SLAM we were quite happy matching 1020 features per frame.",
                    "label": 1
                },
                {
                    "sent": "But if we look at any modern system, everyone's matching hundreds of features per frame now and to really get accuracy in SLAM that that's that's well proven.",
                    "label": 0
                },
                {
                    "sent": "Now that's what you want to do.",
                    "label": 0
                },
                {
                    "sent": "So can we make an active active matching algorithm that was scaled to being able to match?",
                    "label": 0
                },
                {
                    "sent": "Hunt",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The features per frame.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In real time.",
                    "label": 0
                },
                {
                    "sent": "So to evaluate to look at this problem.",
                    "label": 0
                },
                {
                    "sent": "In the meantime, we've been doing some work on thinking about the correlation structure in in visual Maps.",
                    "label": 1
                },
                {
                    "sent": "So if you look at this video here and think about how the features are moving across the image as the camera moves, you'll see that the correlation between features.",
                    "label": 0
                },
                {
                    "sent": "So how similarly they move depends on their positions in the scene.",
                    "label": 0
                },
                {
                    "sent": "So features that are all in a similar position.",
                    "label": 0
                },
                {
                    "sent": "So all of the stuff in the background they move in a very consistent way, so they're very highly correlated in the way they move things that are under foreground and also close to each other.",
                    "label": 0
                },
                {
                    "sent": "Also have a very highly correlated movement, but there's relatively weak correlation between features in the foreground and features.",
                    "label": 0
                },
                {
                    "sent": "In the background.",
                    "label": 0
                },
                {
                    "sent": "So we thought OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can we understand this correlation structure?",
                    "label": 1
                },
                {
                    "sent": "You know, in a nicer, deeper way that would then allow us to do something also clever in inactive matching.",
                    "label": 0
                },
                {
                    "sent": "So rather than just thinking about mutual information of each feature with the state that you're trying to find, we thought let's think about mutual information between pairs of features.",
                    "label": 1
                },
                {
                    "sent": "So basically, this illustration here shows you a set of predictions for features in an image and then pairwise mutual informations between them.",
                    "label": 0
                },
                {
                    "sent": "So this basically says if I measure this feature, how much does it allow me to reduce?",
                    "label": 0
                },
                {
                    "sent": "The search region for this feature, so in in terms of bits, so I if I measure this feature, there's 2.1 bits of information with this feature, so I can reduce the standard deviation of this search region by a factor of about two.",
                    "label": 0
                },
                {
                    "sent": "So if we look at particular features here, so 2 features in the background here have a relatively high mutual information of 1.9 bits, whereas a feature in the background and a feature in the foreground have a low mutual information of North Point 6.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then we discovered this.",
                    "label": 0
                },
                {
                    "sent": "Kind of old stuff for how do you take her a dense graph here?",
                    "label": 0
                },
                {
                    "sent": "So this is this is a graph of mutual information links between us set of variables.",
                    "label": 0
                },
                {
                    "sent": "There's an algorithm for the child nutri which enables you to make the tree like approximation to this whole graph, which best captures the correlation structure here.",
                    "label": 0
                },
                {
                    "sent": "So basically the child new tree is the factorizable distribution here, which most closely approximates the full distribution.",
                    "label": 1
                },
                {
                    "sent": "Represented by this dense graph in terms of minimum KL divergent between the two.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distributions and actually.",
                    "label": 0
                },
                {
                    "sent": "We already have this mutual information graph, so it's very easiest for us to calculate the child you treat and the actual algorithm to calculate the channel.",
                    "label": 0
                },
                {
                    "sent": "New tree is almost trivial once you have the mutual information graph.",
                    "label": 0
                },
                {
                    "sent": "All you have to do is look at your pairwise link.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this whole graph and start joining things together so you take every.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different feature.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Join it to the neighbor that has that it has the highest pairwise meet.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And information with so you do that.",
                    "label": 0
                },
                {
                    "sent": "Once you get one set of kind of clusters and then you look at each cluster and then join each one again during the clusters up to the one.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have the heist machine.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formation with.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they join up like this.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Is the Cherry Tree.",
                    "label": 0
                },
                {
                    "sent": "It's super cheap to compute once you have them.",
                    "label": 0
                },
                {
                    "sent": "The Mutual information graph so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have this tree.",
                    "label": 0
                },
                {
                    "sent": "So now we can do active matching in a much more efficient way that scales much better to large.",
                    "label": 1
                },
                {
                    "sent": "Numbers of features.",
                    "label": 1
                },
                {
                    "sent": "So basically every time we make a measurement of 1 feature rather than having to do like a full EKF type update over all the features jointly, we can now just passing messages along the tree, belief propagation and use that to update the joint distribution between features.",
                    "label": 0
                },
                {
                    "sent": "And there's another very nice thing which is that to calculate mutual information so the mutual information measure we use now is what's the mutual information between a feature that we want to measure and all the other features that we want to.",
                    "label": 0
                },
                {
                    "sent": "That are in our search state jointly and it turns out that in in a tree the mutual information between this feature and all the others is the same as the mutual information between this one and just it's it's neighbors.",
                    "label": 0
                },
                {
                    "sent": "So that means we can also calculate mutual information very.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficiently, so now we have this algorithm called clam chowder active matching, and this is how it works.",
                    "label": 0
                },
                {
                    "sent": "So you'll see now hundreds of features per frame.",
                    "label": 0
                },
                {
                    "sent": "We're trying to search for.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that every time we measure 1 feature we propagate the results of that along the tree and reduce the search regions for all the other features and you get this very nice kind of thing where it seems to relax gradually from the prior onto the final.",
                    "label": 0
                },
                {
                    "sent": "Search state.",
                    "label": 0
                },
                {
                    "sent": "And so again, the final result where you can see the big difference between the.",
                    "label": 0
                },
                {
                    "sent": "The kind of exhaustive regions we would have had to search, and the regions we've.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Being able to search with this active method.",
                    "label": 0
                },
                {
                    "sent": "So here's the important graph that shows this is matching time, so this is the full computation time per frame.",
                    "label": 0
                },
                {
                    "sent": "To match an image and how it scales with number of features.",
                    "label": 0
                },
                {
                    "sent": "So this was original active matching, so remember, real time constraint is probably 30 milliseconds per frame.",
                    "label": 0
                },
                {
                    "sent": "We're heading North very quickly.",
                    "label": 0
                },
                {
                    "sent": "3040 fifty 60 features.",
                    "label": 0
                },
                {
                    "sent": "You know this is already 5 seconds or something, so completely you know outside real time bounds.",
                    "label": 0
                },
                {
                    "sent": "This is clam now.",
                    "label": 0
                },
                {
                    "sent": "Much nicer behavior up to you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe 100 and 150 features, but if we look at how that graph expands, you know this is actually where we want to be.",
                    "label": 0
                },
                {
                    "sent": "400 features in 30 milliseconds and a clam is still climbing much too high, so we made a further approximation with an algorithm called subam which actually does take us pretty much into real time performance for 400 features.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The frames are sub ohm is is what we call subset active matching.",
                    "label": 0
                },
                {
                    "sent": "So this goes further than clamp it does the Charlie Tree.",
                    "label": 0
                },
                {
                    "sent": "Then it even starts chopping the Cherry Tree up into bits and basically deals with each of those separately.",
                    "label": 0
                },
                {
                    "sent": "So it chooses a subset of features within the tree.",
                    "label": 0
                },
                {
                    "sent": "So you make the tree and then you cut it into some little branches.",
                    "label": 0
                },
                {
                    "sent": "You do your active matching independently on each one of those little clusters, and then pass the result of that onto the next cluster down the tree and it turns out that because this is always doing much more local operations, this scales better so that so this really can match hundreds of features at frame rate, and so that's kind of what we now planning to use in our.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Main slam system so finish up so active matching we think it's very nice.",
                    "label": 0
                },
                {
                    "sent": "Fully Bayesian robust multi hypothesis feature matching approach which you know importantly takes full benefit and advantage of the priors that you have available in most matching tasks.",
                    "label": 1
                },
                {
                    "sent": "So interested in applying these algorithms further so when our area I've been interested in for a long time is high frame rate matching so could we make SLAM systems where you could throw a camera across the room or something like that and track it.",
                    "label": 0
                },
                {
                    "sent": "Track it.",
                    "label": 0
                }
            ]
        }
    }
}