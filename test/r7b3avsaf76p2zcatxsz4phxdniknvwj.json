{
    "id": "r7b3avsaf76p2zcatxsz4phxdniknvwj",
    "title": "Path coding penalties for directed acyclic graphs",
    "info": {
        "author": [
            "Julien Mairal, INRIA"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_mairal_penalties/",
    "segmentation": [
        [
            "So what?"
        ],
        [
            "We want this to do feature selection in graph in the sense which I'm going to define, which likely we are going to use some structured sparsity tools which will lead to some non convex combinatorial problems, some convex optimization problems and we are going to deal with all of that using network flows so."
        ],
        [
            "I'm going to just to show a motivating examples from a field which is not really my field of expertise, but so an example which I like.",
            "It's coming from Rapoport ET al and what you have.",
            "What you have here is the metabolic network of the building East.",
            "In other words, what you have is for every vertex you have a gene which encode protein, and each time you have an arc between two vertices.",
            "Basically it means that there is some relation between the two genes.",
            "So now the indices in this paper where the authors have is a binary classification task, which I'm not going to not going to give much detail, but they have many samples and for every sample they have gene expressions for every of these jeans and they want to find out which genes can explain.",
            "Maybe this phenomenon that they are observing.",
            "So they built a linear model and when they obtained so they will obtain a weight vector when wait for every gene and basically the green dots here represent genes which have a positive weights and read the ones which have a negative weights.",
            "So they get some performance in terms of classification and they want to be able to interpret this result.",
            "And as you see here this, this representation is a bit scattered, so they argue that you might want to do something to get something more interpretable, and they will."
        ],
        [
            "Use some penalties to induce smoothness on the graph, such that if you if they show that two biologies, they will maybe.",
            "Prefer this kind of results and see for example that here you get some kind of cluster which has a positive waves.",
            "So now a bit later, there have been other arguments in the literature which says that we might want to obtain a sparse solution or also to get better interpretability.",
            "And in the case of graphs, we might want to be able to automatically extract some, let's say a few number of connected subgraphs from this graph, which will explain the phenomenon.",
            "And this is what the this talk."
        ],
        [
            "About so, since we're in the optimization workshop, I have to maybe to write down this this optimization problem at some point, so we get a nice convex most loss function at some regularization, which will encode some a priori knowledge that we have on the solution W. So I'm not going to go through all of these penalties which have been proposed, and in fact I will focus on some recent work on structured sparsity which will try to.",
            "Encode some structure of the problem into Omega, and I write structured sparsity's because there have been many recent works on this topic, and in fact this works.",
            "Use the same kind of terminology, but they are significantly different and I'm going to use in this work as a building block.",
            "The penalties introduced by younger talent in 2009 and Jacob ET al in 2009, which was at ICML."
        ],
        [
            "So again, just in case that it was not clear enough when I was presenting these large graph, what we want is given for example an example optimization problem where we have P features.",
            "Each of the features can be organized on the graph, the kind of.",
            "Non zero patterns of the solution.",
            "We want to encourage are the ones which can be covered by a few connect."
        ],
        [
            "Subgraphs, for example.",
            "This one.",
            "Here we will select 2369, Eleven 12 and this is the kind of patterns that we want to obtain."
        ],
        [
            "So in fact the the so he won't get hard, have proposed the penalty, which is a bit marginal that supplies into the to some specific case of positive or graphs, but marginally they will consider a penalty that given a set of predefined groups of subsets of variables which perceived overlap, they built a penalty which encourage the pattern of non zeros of the solution W to be a union of a few number of groups.",
            "So if we had, for example, that all the groups were the connected sub graph, this would exactly lead to what we want.",
            "That is, to encourage the have disk graph sparsity inducing effect.",
            "So I'm going to ask you just to digest this formula.",
            "So basically this capital G is the predefined set of groups and given a vector W, the penalty will count.",
            "Basically how many groups we need to cover the.",
            "So these are the groups we just selected in this capital J.",
            "So we want the support of W to be in the Union of.",
            "If you groups and each group has a cost, so we look for the set of groups with minimum costs that can cover the support so it has exactly this effect we want as other properties.",
            "It's not convex, and what is a bit disappointing at first sight is that it's a this problem.",
            "Here is NP hard to compute.",
            "In general, so the authors used greedy algorithms to deal with that.",
            "Now let us we write this penalty in a different way, so we have here a selection program we want to select some groups so we can write this as an optimization problem where we look for binary vector that has as many entries as groups.",
            "And basically this some here becomes in this formula.",
            "This inner product and you can imagine with a bit of abuse of notation some binary matrix and here such that this constraint here is exactly this constraint here.",
            "OK, and this is a Boolean linear program which isn't that hard to compute."
        ],
        [
            "But Interestingly, we can.",
            "There are very standard ways of relaxing that.",
            "Ann, in particular.",
            "If you replace these constraints here by just saying that the entries of X should be positive and instead of the support you replace that by the absolute values of the entries of the W, so it's you get an LPR taxation.",
            "You get.",
            "In fact here on Norm.",
            "And this is exactly actually the same mechanism to go from zero to L1 an.",
            "So something interesting is that actually when you do that you get exactly.",
            "And Lt, which was introduced by Jacko, battle in HTML, but in a quite it's equivalent.",
            "But it was introducing a formal different point of view.",
            "And basically it says that I CL 2009 we had at the same time and convex penalty, and it's convex relaxation which were developed independently so."
        ],
        [
            "K so as I said before, natural choice to do what we want for graph is to consider as groups all connected subgraphs.",
            "The promise that we get an exponential number of them and basically this is intractable.",
            "We could use and this is what was used by Jacoby.",
            "Tell the old pairs of vertices linked by an arc to encourage a bit of connectivity of the solution in the graph, But this only models local interaction, so it gives more connected solutions than L1 for example, But still this is very low coolant.",
            "We ask yourself, can we find another structure which is a bit richer?",
            "And one example would be to consider all connected subgraphs up to a size L, But this actually doesn't scale very well, even for quite a small values of L. For example, three or four.",
            "This becomes quickly instructible as soon as your graph is connected enough.",
            "So the question is, can we replace connected subgraph by something different which is rich enough to model long range interactions in the graph and leads to things that we can compute?",
            "Anne."
        ],
        [
            "What we say is that we have a solution when the graph is a DAG directed acyclic graph an let us assume that this is the case.",
            "And if we define G to be the set of all paths in the DAG, so we still get at first sight an exponential number of groups.",
            "What I say is that assuming you define your weights correctly, you.",
            "Cost for every group correctly.",
            "Then you we can do things in polynomial time.",
            "OK, so we set the weights, eat at G so the basically the cost of selecting the group or the path G to be gamma fixed cost each time you select the group.",
            "Because you want a small number of groups and we want to Spar City so we add the length of the path here.",
            "Basically the as a sanity check if you set gamma equal to 0 this non complex.",
            "Becomes exactly L zero and this convex relaxation becomes exactly L1.",
            "OK, so as an example, if you select 236 using this path and you select.",
            "9/11 12 using this path.",
            "Basically, the cost the nonconvex penalty is exactly gamma plus three plus Gamma Pro 3, three being the length of each of these two paths."
        ],
        [
            "So the first observation we make.",
            "Is that if we add two nodes to this graph so well, DAG.",
            "Here is this very simple graph with four nodes.",
            "If we add node source node S on the sink node T. If you put some costs on the arcs, which are one every of these arcs on this part of the graph and gamma for each arc going from S2 every of the node.",
            "Basically this this cost for selecting a path into G. Can be seen as the sum of costs from the path that goes from S. Follow the path we have selected and then goes to the sink T. So this choice of course of notation for the source and terminology.",
            "So something is not innocent because we are going to use network flows to address them to compute these penalties in polynomial time and."
        ],
        [
            "Optimization.",
            "So very quick introduction to network flows if you understand me with that, I would say maybe very simplified introduction and just going to introduce what I need an there are very nice reference if you want to know more about network flows.",
            "So basically the flow is non negative function, an axe that has that respect, some conservation constraints.",
            "So the amount of flow going through a node is equal to the amount of flow going out of the node."
        ],
        [
            "In this talk, basically the flows usually go from a source and then end up at the sink.",
            "And the main property of these flows that we're going to."
        ],
        [
            "Uses that.",
            "These flows can be decomposed into pass flow.",
            "So what is a pass through?",
            "Basically you will understand with this simple example, so we have this flow and basically I said."
        ],
        [
            "This is the sum of 1 unit of flow going through this path."
        ],
        [
            "And when you need a flow going through this path."
        ],
        [
            "But you get the original fruit."
        ],
        [
            "And the main message.",
            "So this is something which is a which is very well known in the literature on network flows."
        ],
        [
            "Which we're going to exploit, and basically the main message here is that we get an optimization problem over all the paths in the DAG, so we get an exponential number of variables.",
            "We request that as.",
            "Selecting a path in the DAG is equivalent to sending some unit of flow going from the source to the sink along this path in the graph, and then we end up with network Proofer formulation which we can solve in polynomial time.",
            "So let's, uh.",
            "Basically these are two simple proposition.",
            "The first one is this is 1 nonconvex penalty, so if you remember, this was the optimization overpass on the graph.",
            "And now we want to find the flow F that minimize some linear cost here.",
            "So we have some cost.",
            "Now on the graph and a few constraints.",
            "So basically this this is just a constraint saying that for the node corresponding to the variable G, if the variable JS if Jay is in the support of W, you need to have at least one.",
            "You need to flow going through this node.",
            "An we have a similar proposition for the for the convex for laxation version, and now we can compute at least compute these penalties in polynomial time.",
            "And I guess that you are now all expert in proximal operators, since I'm very glad that we have plenty of talks before introducing that, so I'm not going to do it, but we can as well compute the proximal operators of the nonconvex penalty and of the convex penalty in polynomial time using some network optimization an.",
            "Also, this is less important, but this penalty here is the norm and we can compute as well.",
            "In polynomial time, which can be useful if you want to get duality gaps when you do your optimization."
        ],
        [
            "So in the last minutes I'm going quickly to two experiments, so the first one is we wanted to reproduce an experiment from Jacob ET al, who used some structure penalty where the groups were all.",
            "Pairs of vertices connected by an arc.",
            "So the one I briefly presented during the during the talk, and I say that it was only modeling local interactions in the graph.",
            "An so we get a gene expression data with about 8000 genes.",
            "About 300 samples.",
            "This is we have a binary classification problem.",
            "We want to classify meta static samples against nonmetastatic samples and this is a don't say that this is a breast cancer data.",
            "And for this data set there exists a graph built by some biologists which we use here.",
            "And since this is the graph, we do something which may be quite bad, but at the end we will just observe that we can still.",
            "Obtain good results in terms of connectivity in the original graph.",
            "So basically we just order, only choose our directions and we move cycles in the graph.",
            "So we keep about 80% of the arcs.",
            "So we are going to do, I think 20 runs.",
            "And to evaluate the error rates using 80% for each one, we use 20% of the data as a test set and 80% for training and.",
            "For each training set, we use 10 fold cross validation to select or permit."
        ],
        [
            "And basically what we observe is that.",
            "These are error rates on the 1st row and these are standard deviation, so the first thing we see is that something which is not surprising because it was observed before is that Ridge regression already performs quite well, which means that maybe it's not so useful to use the graph to obtain a good performance, and this is a data set which is quite nasty with a lot of noise and the standard deviation are quite large, so.",
            "Saying that one method is better than another is a tough call.",
            "But if you look at if you try to interpret and you look at what you what, which number of genes you have selected.",
            "You see that if you already want to spar solution than the lasso, select about 30 variables.",
            "This is the method of jacobita which selects about 70 variables the same as us, but internally in terms of connectivity of the solution.",
            "What is the number of connected sub graph in the original graph we have selected?",
            "So the less of course doesn't select something which disconnected jacobita select something which is more addicting than the last.",
            "So once we obtained, I mean the just show that if your purpose is to select a small number of connected sub graph, this is basically it does the right job and I don't have much more time to comment on the stability, but we get also more stable results that if used less or just pairs of vertices connected by your graph."
        ],
        [
            "And just let's the experiments.",
            "So image denoising just.",
            "So Patch based approach where we extract all overlapping patches from an image which are noisy.",
            "We do a sparse approximation of every Patch, so we can assume that we need a dictionary for that.",
            "So each Patch will be linear combination of elements of a dictionary and what we use is the nutritional what is called in the signal signal processing literature DCT dictionary, which is represented here, which is organized by frequencies on the grid so that we can use a DAG and maybe ask yourself.",
            "If we use the structure or do we get better results that if we do not?",
            "And."
        ],
        [
            "Basically, we hope so.",
            "We obtain significantly better results using this very simple dictionaries.",
            "When we add a structure, so it's not state of the art results for image noising, but it's far from being ridiculous.",
            "It's more or less at the same level as if you know the image processing literature as the Gaussian scale mixture.",
            "Data developed by particular Enright and Simoncelli, in 2003.",
            "Anne.",
            "Web server as well that this is an experiment where the convex nonconvex penalty there is much better than the convex one, so I wanted at least one experiment where we have convexity didn't help, and I'm a bit familiar with this image processing literature, and so I knew that.",
            "In this case, I knew already that L 0 does better than L1, and here we get as well the fact that the nonconvex penalty less bitter.",
            "So using the non convex or convex really depends on the application you want an."
        ],
        [
            "This will basically conclude my talk.",
            "I'll give you a bit of advertisement if you're interested.",
            "We have.",
            "An overview meadowcraft from space optimization coming soon and a lot of software which you can use under which are open source and free available.",
            "And that's it.",
            "So in terms of selecting subgraphs in this graph, so you have selected this paths in the diagram, there are also many.",
            "Methods for extracting different types of subgraphs orthograph.",
            "Efficiently, so I was wondering whether I have sensitive what kind of methods are you?",
            "Are you thinking about?",
            "Yes, I read a bit of graph kernel and there is something quite interesting which I didn't develop in this.",
            "In the short paper."
        ],
        [
            "About time.",
            "Trying to make some links with that which is.",
            "So basically for graph kernels, what we want is to build some a similarity measure to simplify between two graphs and."
        ],
        [
            "The standard ways which have been proposed well, let's say let us consider all possible connected subgraph and look at how many we have in common, which is NP hard to compute right?",
            "Or something like that?",
            "What?",
            "A complete yes and this is empty.",
            "How to compute and people have asked the exact same question here as this one.",
            "Can we replace replace subgraphs by something else and the unsquare was quite similar?",
            "Or basically people have used work kernels which can be computed in polynomial time.",
            "And so basically we have the same question and same on square for two different problems.",
            "So it seems quite a natural question too.",
            "I'm going to try to find some deeper links which I have not found ready yet.",
            "If you look at all subgraphs, of course it's yes.",
            "You can impose some restrictions.",
            "First of all, you can just consider some types of subgraphs that only occur in your graphs, not just every possible thing.",
            "And then in this graph also, but once where methods where you actually get the.",
            "Picture representation very cheaply so you know which subgraphs occur there.",
            "It's not just the in the random walk for knowledge, usually computer.",
            "There is also.",
            "So yeah, it would be interesting, but there is also a big difference between this literature of graph kernels, which is sometimes you have a different labels for every vertex, which here is not really the case, it's a.",
            "So that yes.",
            "Yes, so I didn't completely understand when you showed the experiment that you somehow explicitly remove cycles by breaking them and orienting that just randomly, right?",
            "So he said because you want the projection to be valid for all of your weight.",
            "So could you impose capacities?",
            "Other capacities on the network flow problems?",
            "Is the question related to why do I need a dog or I think it might also partially the question.",
            "So why do I need a DAG I?"
        ],
        [
            "I attended the question, but why do I need a dog is because?",
            "It didn't have the time to develop much, but when you get to flow on any graph, it can be decomposed into past flows flows that.",
            "I mean you need some flows going from the source, the sink and cycle flows, which just suck the cycle in the graph.",
            "And when I have my cost here, when I want to when I need this when I want to build this equivalence between this optimization problem path and the flows, I cannot afford to have this cycle flows.",
            "The effect which.",
            "I want to have some unit of flow going from the source such that the meat discussed gamma each time you select a cost.",
            "Here if you have some flow going on here, the path will only have a cost, which is the length of the path.",
            "It will just do not have the right weight gamma.",
            "So there's it.",
            "This wouldn't make sense in your applications for.",
            "So.",
            "It means that I can run the algorithm on graphs which have cycles.",
            "It will give something which I.",
            "But the only thing is that it will tend to select cycles because they will have a lower cost.",
            "But if you don't have too many cycles, I guess you can have something reasonable, but it might be hard to already know what you get.",
            "If you have undirected graph, basically cycles for every pairs, then it's really not adept."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want this to do feature selection in graph in the sense which I'm going to define, which likely we are going to use some structured sparsity tools which will lead to some non convex combinatorial problems, some convex optimization problems and we are going to deal with all of that using network flows so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to just to show a motivating examples from a field which is not really my field of expertise, but so an example which I like.",
                    "label": 0
                },
                {
                    "sent": "It's coming from Rapoport ET al and what you have.",
                    "label": 0
                },
                {
                    "sent": "What you have here is the metabolic network of the building East.",
                    "label": 1
                },
                {
                    "sent": "In other words, what you have is for every vertex you have a gene which encode protein, and each time you have an arc between two vertices.",
                    "label": 0
                },
                {
                    "sent": "Basically it means that there is some relation between the two genes.",
                    "label": 0
                },
                {
                    "sent": "So now the indices in this paper where the authors have is a binary classification task, which I'm not going to not going to give much detail, but they have many samples and for every sample they have gene expressions for every of these jeans and they want to find out which genes can explain.",
                    "label": 0
                },
                {
                    "sent": "Maybe this phenomenon that they are observing.",
                    "label": 0
                },
                {
                    "sent": "So they built a linear model and when they obtained so they will obtain a weight vector when wait for every gene and basically the green dots here represent genes which have a positive weights and read the ones which have a negative weights.",
                    "label": 0
                },
                {
                    "sent": "So they get some performance in terms of classification and they want to be able to interpret this result.",
                    "label": 0
                },
                {
                    "sent": "And as you see here this, this representation is a bit scattered, so they argue that you might want to do something to get something more interpretable, and they will.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use some penalties to induce smoothness on the graph, such that if you if they show that two biologies, they will maybe.",
                    "label": 0
                },
                {
                    "sent": "Prefer this kind of results and see for example that here you get some kind of cluster which has a positive waves.",
                    "label": 0
                },
                {
                    "sent": "So now a bit later, there have been other arguments in the literature which says that we might want to obtain a sparse solution or also to get better interpretability.",
                    "label": 0
                },
                {
                    "sent": "And in the case of graphs, we might want to be able to automatically extract some, let's say a few number of connected subgraphs from this graph, which will explain the phenomenon.",
                    "label": 0
                },
                {
                    "sent": "And this is what the this talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About so, since we're in the optimization workshop, I have to maybe to write down this this optimization problem at some point, so we get a nice convex most loss function at some regularization, which will encode some a priori knowledge that we have on the solution W. So I'm not going to go through all of these penalties which have been proposed, and in fact I will focus on some recent work on structured sparsity which will try to.",
                    "label": 0
                },
                {
                    "sent": "Encode some structure of the problem into Omega, and I write structured sparsity's because there have been many recent works on this topic, and in fact this works.",
                    "label": 0
                },
                {
                    "sent": "Use the same kind of terminology, but they are significantly different and I'm going to use in this work as a building block.",
                    "label": 0
                },
                {
                    "sent": "The penalties introduced by younger talent in 2009 and Jacob ET al in 2009, which was at ICML.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, just in case that it was not clear enough when I was presenting these large graph, what we want is given for example an example optimization problem where we have P features.",
                    "label": 0
                },
                {
                    "sent": "Each of the features can be organized on the graph, the kind of.",
                    "label": 0
                },
                {
                    "sent": "Non zero patterns of the solution.",
                    "label": 0
                },
                {
                    "sent": "We want to encourage are the ones which can be covered by a few connect.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subgraphs, for example.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Here we will select 2369, Eleven 12 and this is the kind of patterns that we want to obtain.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in fact the the so he won't get hard, have proposed the penalty, which is a bit marginal that supplies into the to some specific case of positive or graphs, but marginally they will consider a penalty that given a set of predefined groups of subsets of variables which perceived overlap, they built a penalty which encourage the pattern of non zeros of the solution W to be a union of a few number of groups.",
                    "label": 1
                },
                {
                    "sent": "So if we had, for example, that all the groups were the connected sub graph, this would exactly lead to what we want.",
                    "label": 0
                },
                {
                    "sent": "That is, to encourage the have disk graph sparsity inducing effect.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to ask you just to digest this formula.",
                    "label": 0
                },
                {
                    "sent": "So basically this capital G is the predefined set of groups and given a vector W, the penalty will count.",
                    "label": 0
                },
                {
                    "sent": "Basically how many groups we need to cover the.",
                    "label": 0
                },
                {
                    "sent": "So these are the groups we just selected in this capital J.",
                    "label": 0
                },
                {
                    "sent": "So we want the support of W to be in the Union of.",
                    "label": 0
                },
                {
                    "sent": "If you groups and each group has a cost, so we look for the set of groups with minimum costs that can cover the support so it has exactly this effect we want as other properties.",
                    "label": 0
                },
                {
                    "sent": "It's not convex, and what is a bit disappointing at first sight is that it's a this problem.",
                    "label": 0
                },
                {
                    "sent": "Here is NP hard to compute.",
                    "label": 0
                },
                {
                    "sent": "In general, so the authors used greedy algorithms to deal with that.",
                    "label": 0
                },
                {
                    "sent": "Now let us we write this penalty in a different way, so we have here a selection program we want to select some groups so we can write this as an optimization problem where we look for binary vector that has as many entries as groups.",
                    "label": 0
                },
                {
                    "sent": "And basically this some here becomes in this formula.",
                    "label": 0
                },
                {
                    "sent": "This inner product and you can imagine with a bit of abuse of notation some binary matrix and here such that this constraint here is exactly this constraint here.",
                    "label": 1
                },
                {
                    "sent": "OK, and this is a Boolean linear program which isn't that hard to compute.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But Interestingly, we can.",
                    "label": 0
                },
                {
                    "sent": "There are very standard ways of relaxing that.",
                    "label": 0
                },
                {
                    "sent": "Ann, in particular.",
                    "label": 0
                },
                {
                    "sent": "If you replace these constraints here by just saying that the entries of X should be positive and instead of the support you replace that by the absolute values of the entries of the W, so it's you get an LPR taxation.",
                    "label": 0
                },
                {
                    "sent": "You get.",
                    "label": 0
                },
                {
                    "sent": "In fact here on Norm.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly actually the same mechanism to go from zero to L1 an.",
                    "label": 0
                },
                {
                    "sent": "So something interesting is that actually when you do that you get exactly.",
                    "label": 0
                },
                {
                    "sent": "And Lt, which was introduced by Jacko, battle in HTML, but in a quite it's equivalent.",
                    "label": 0
                },
                {
                    "sent": "But it was introducing a formal different point of view.",
                    "label": 0
                },
                {
                    "sent": "And basically it says that I CL 2009 we had at the same time and convex penalty, and it's convex relaxation which were developed independently so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K so as I said before, natural choice to do what we want for graph is to consider as groups all connected subgraphs.",
                    "label": 0
                },
                {
                    "sent": "The promise that we get an exponential number of them and basically this is intractable.",
                    "label": 0
                },
                {
                    "sent": "We could use and this is what was used by Jacoby.",
                    "label": 0
                },
                {
                    "sent": "Tell the old pairs of vertices linked by an arc to encourage a bit of connectivity of the solution in the graph, But this only models local interaction, so it gives more connected solutions than L1 for example, But still this is very low coolant.",
                    "label": 1
                },
                {
                    "sent": "We ask yourself, can we find another structure which is a bit richer?",
                    "label": 1
                },
                {
                    "sent": "And one example would be to consider all connected subgraphs up to a size L, But this actually doesn't scale very well, even for quite a small values of L. For example, three or four.",
                    "label": 0
                },
                {
                    "sent": "This becomes quickly instructible as soon as your graph is connected enough.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we replace connected subgraph by something different which is rich enough to model long range interactions in the graph and leads to things that we can compute?",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we say is that we have a solution when the graph is a DAG directed acyclic graph an let us assume that this is the case.",
                    "label": 1
                },
                {
                    "sent": "And if we define G to be the set of all paths in the DAG, so we still get at first sight an exponential number of groups.",
                    "label": 1
                },
                {
                    "sent": "What I say is that assuming you define your weights correctly, you.",
                    "label": 0
                },
                {
                    "sent": "Cost for every group correctly.",
                    "label": 1
                },
                {
                    "sent": "Then you we can do things in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we set the weights, eat at G so the basically the cost of selecting the group or the path G to be gamma fixed cost each time you select the group.",
                    "label": 0
                },
                {
                    "sent": "Because you want a small number of groups and we want to Spar City so we add the length of the path here.",
                    "label": 0
                },
                {
                    "sent": "Basically the as a sanity check if you set gamma equal to 0 this non complex.",
                    "label": 0
                },
                {
                    "sent": "Becomes exactly L zero and this convex relaxation becomes exactly L1.",
                    "label": 0
                },
                {
                    "sent": "OK, so as an example, if you select 236 using this path and you select.",
                    "label": 0
                },
                {
                    "sent": "9/11 12 using this path.",
                    "label": 0
                },
                {
                    "sent": "Basically, the cost the nonconvex penalty is exactly gamma plus three plus Gamma Pro 3, three being the length of each of these two paths.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first observation we make.",
                    "label": 0
                },
                {
                    "sent": "Is that if we add two nodes to this graph so well, DAG.",
                    "label": 0
                },
                {
                    "sent": "Here is this very simple graph with four nodes.",
                    "label": 0
                },
                {
                    "sent": "If we add node source node S on the sink node T. If you put some costs on the arcs, which are one every of these arcs on this part of the graph and gamma for each arc going from S2 every of the node.",
                    "label": 0
                },
                {
                    "sent": "Basically this this cost for selecting a path into G. Can be seen as the sum of costs from the path that goes from S. Follow the path we have selected and then goes to the sink T. So this choice of course of notation for the source and terminology.",
                    "label": 0
                },
                {
                    "sent": "So something is not innocent because we are going to use network flows to address them to compute these penalties in polynomial time and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "So very quick introduction to network flows if you understand me with that, I would say maybe very simplified introduction and just going to introduce what I need an there are very nice reference if you want to know more about network flows.",
                    "label": 1
                },
                {
                    "sent": "So basically the flow is non negative function, an axe that has that respect, some conservation constraints.",
                    "label": 0
                },
                {
                    "sent": "So the amount of flow going through a node is equal to the amount of flow going out of the node.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this talk, basically the flows usually go from a source and then end up at the sink.",
                    "label": 0
                },
                {
                    "sent": "And the main property of these flows that we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uses that.",
                    "label": 0
                },
                {
                    "sent": "These flows can be decomposed into pass flow.",
                    "label": 1
                },
                {
                    "sent": "So what is a pass through?",
                    "label": 0
                },
                {
                    "sent": "Basically you will understand with this simple example, so we have this flow and basically I said.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the sum of 1 unit of flow going through this path.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you need a flow going through this path.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you get the original fruit.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the main message.",
                    "label": 0
                },
                {
                    "sent": "So this is something which is a which is very well known in the literature on network flows.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which we're going to exploit, and basically the main message here is that we get an optimization problem over all the paths in the DAG, so we get an exponential number of variables.",
                    "label": 1
                },
                {
                    "sent": "We request that as.",
                    "label": 0
                },
                {
                    "sent": "Selecting a path in the DAG is equivalent to sending some unit of flow going from the source to the sink along this path in the graph, and then we end up with network Proofer formulation which we can solve in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "So let's, uh.",
                    "label": 0
                },
                {
                    "sent": "Basically these are two simple proposition.",
                    "label": 0
                },
                {
                    "sent": "The first one is this is 1 nonconvex penalty, so if you remember, this was the optimization overpass on the graph.",
                    "label": 0
                },
                {
                    "sent": "And now we want to find the flow F that minimize some linear cost here.",
                    "label": 0
                },
                {
                    "sent": "So we have some cost.",
                    "label": 0
                },
                {
                    "sent": "Now on the graph and a few constraints.",
                    "label": 0
                },
                {
                    "sent": "So basically this this is just a constraint saying that for the node corresponding to the variable G, if the variable JS if Jay is in the support of W, you need to have at least one.",
                    "label": 0
                },
                {
                    "sent": "You need to flow going through this node.",
                    "label": 0
                },
                {
                    "sent": "An we have a similar proposition for the for the convex for laxation version, and now we can compute at least compute these penalties in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "And I guess that you are now all expert in proximal operators, since I'm very glad that we have plenty of talks before introducing that, so I'm not going to do it, but we can as well compute the proximal operators of the nonconvex penalty and of the convex penalty in polynomial time using some network optimization an.",
                    "label": 1
                },
                {
                    "sent": "Also, this is less important, but this penalty here is the norm and we can compute as well.",
                    "label": 0
                },
                {
                    "sent": "In polynomial time, which can be useful if you want to get duality gaps when you do your optimization.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the last minutes I'm going quickly to two experiments, so the first one is we wanted to reproduce an experiment from Jacob ET al, who used some structure penalty where the groups were all.",
                    "label": 0
                },
                {
                    "sent": "Pairs of vertices connected by an arc.",
                    "label": 0
                },
                {
                    "sent": "So the one I briefly presented during the during the talk, and I say that it was only modeling local interactions in the graph.",
                    "label": 0
                },
                {
                    "sent": "An so we get a gene expression data with about 8000 genes.",
                    "label": 1
                },
                {
                    "sent": "About 300 samples.",
                    "label": 0
                },
                {
                    "sent": "This is we have a binary classification problem.",
                    "label": 1
                },
                {
                    "sent": "We want to classify meta static samples against nonmetastatic samples and this is a don't say that this is a breast cancer data.",
                    "label": 0
                },
                {
                    "sent": "And for this data set there exists a graph built by some biologists which we use here.",
                    "label": 0
                },
                {
                    "sent": "And since this is the graph, we do something which may be quite bad, but at the end we will just observe that we can still.",
                    "label": 0
                },
                {
                    "sent": "Obtain good results in terms of connectivity in the original graph.",
                    "label": 0
                },
                {
                    "sent": "So basically we just order, only choose our directions and we move cycles in the graph.",
                    "label": 1
                },
                {
                    "sent": "So we keep about 80% of the arcs.",
                    "label": 0
                },
                {
                    "sent": "So we are going to do, I think 20 runs.",
                    "label": 0
                },
                {
                    "sent": "And to evaluate the error rates using 80% for each one, we use 20% of the data as a test set and 80% for training and.",
                    "label": 1
                },
                {
                    "sent": "For each training set, we use 10 fold cross validation to select or permit.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically what we observe is that.",
                    "label": 0
                },
                {
                    "sent": "These are error rates on the 1st row and these are standard deviation, so the first thing we see is that something which is not surprising because it was observed before is that Ridge regression already performs quite well, which means that maybe it's not so useful to use the graph to obtain a good performance, and this is a data set which is quite nasty with a lot of noise and the standard deviation are quite large, so.",
                    "label": 0
                },
                {
                    "sent": "Saying that one method is better than another is a tough call.",
                    "label": 0
                },
                {
                    "sent": "But if you look at if you try to interpret and you look at what you what, which number of genes you have selected.",
                    "label": 0
                },
                {
                    "sent": "You see that if you already want to spar solution than the lasso, select about 30 variables.",
                    "label": 0
                },
                {
                    "sent": "This is the method of jacobita which selects about 70 variables the same as us, but internally in terms of connectivity of the solution.",
                    "label": 0
                },
                {
                    "sent": "What is the number of connected sub graph in the original graph we have selected?",
                    "label": 0
                },
                {
                    "sent": "So the less of course doesn't select something which disconnected jacobita select something which is more addicting than the last.",
                    "label": 0
                },
                {
                    "sent": "So once we obtained, I mean the just show that if your purpose is to select a small number of connected sub graph, this is basically it does the right job and I don't have much more time to comment on the stability, but we get also more stable results that if used less or just pairs of vertices connected by your graph.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just let's the experiments.",
                    "label": 0
                },
                {
                    "sent": "So image denoising just.",
                    "label": 0
                },
                {
                    "sent": "So Patch based approach where we extract all overlapping patches from an image which are noisy.",
                    "label": 1
                },
                {
                    "sent": "We do a sparse approximation of every Patch, so we can assume that we need a dictionary for that.",
                    "label": 1
                },
                {
                    "sent": "So each Patch will be linear combination of elements of a dictionary and what we use is the nutritional what is called in the signal signal processing literature DCT dictionary, which is represented here, which is organized by frequencies on the grid so that we can use a DAG and maybe ask yourself.",
                    "label": 0
                },
                {
                    "sent": "If we use the structure or do we get better results that if we do not?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, we hope so.",
                    "label": 0
                },
                {
                    "sent": "We obtain significantly better results using this very simple dictionaries.",
                    "label": 0
                },
                {
                    "sent": "When we add a structure, so it's not state of the art results for image noising, but it's far from being ridiculous.",
                    "label": 0
                },
                {
                    "sent": "It's more or less at the same level as if you know the image processing literature as the Gaussian scale mixture.",
                    "label": 0
                },
                {
                    "sent": "Data developed by particular Enright and Simoncelli, in 2003.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Web server as well that this is an experiment where the convex nonconvex penalty there is much better than the convex one, so I wanted at least one experiment where we have convexity didn't help, and I'm a bit familiar with this image processing literature, and so I knew that.",
                    "label": 0
                },
                {
                    "sent": "In this case, I knew already that L 0 does better than L1, and here we get as well the fact that the nonconvex penalty less bitter.",
                    "label": 0
                },
                {
                    "sent": "So using the non convex or convex really depends on the application you want an.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This will basically conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a bit of advertisement if you're interested.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "An overview meadowcraft from space optimization coming soon and a lot of software which you can use under which are open source and free available.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "So in terms of selecting subgraphs in this graph, so you have selected this paths in the diagram, there are also many.",
                    "label": 0
                },
                {
                    "sent": "Methods for extracting different types of subgraphs orthograph.",
                    "label": 0
                },
                {
                    "sent": "Efficiently, so I was wondering whether I have sensitive what kind of methods are you?",
                    "label": 0
                },
                {
                    "sent": "Are you thinking about?",
                    "label": 0
                },
                {
                    "sent": "Yes, I read a bit of graph kernel and there is something quite interesting which I didn't develop in this.",
                    "label": 0
                },
                {
                    "sent": "In the short paper.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About time.",
                    "label": 0
                },
                {
                    "sent": "Trying to make some links with that which is.",
                    "label": 0
                },
                {
                    "sent": "So basically for graph kernels, what we want is to build some a similarity measure to simplify between two graphs and.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The standard ways which have been proposed well, let's say let us consider all possible connected subgraph and look at how many we have in common, which is NP hard to compute right?",
                    "label": 0
                },
                {
                    "sent": "Or something like that?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "A complete yes and this is empty.",
                    "label": 0
                },
                {
                    "sent": "How to compute and people have asked the exact same question here as this one.",
                    "label": 0
                },
                {
                    "sent": "Can we replace replace subgraphs by something else and the unsquare was quite similar?",
                    "label": 1
                },
                {
                    "sent": "Or basically people have used work kernels which can be computed in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "And so basically we have the same question and same on square for two different problems.",
                    "label": 0
                },
                {
                    "sent": "So it seems quite a natural question too.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to find some deeper links which I have not found ready yet.",
                    "label": 0
                },
                {
                    "sent": "If you look at all subgraphs, of course it's yes.",
                    "label": 0
                },
                {
                    "sent": "You can impose some restrictions.",
                    "label": 0
                },
                {
                    "sent": "First of all, you can just consider some types of subgraphs that only occur in your graphs, not just every possible thing.",
                    "label": 0
                },
                {
                    "sent": "And then in this graph also, but once where methods where you actually get the.",
                    "label": 0
                },
                {
                    "sent": "Picture representation very cheaply so you know which subgraphs occur there.",
                    "label": 0
                },
                {
                    "sent": "It's not just the in the random walk for knowledge, usually computer.",
                    "label": 0
                },
                {
                    "sent": "There is also.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it would be interesting, but there is also a big difference between this literature of graph kernels, which is sometimes you have a different labels for every vertex, which here is not really the case, it's a.",
                    "label": 0
                },
                {
                    "sent": "So that yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I didn't completely understand when you showed the experiment that you somehow explicitly remove cycles by breaking them and orienting that just randomly, right?",
                    "label": 0
                },
                {
                    "sent": "So he said because you want the projection to be valid for all of your weight.",
                    "label": 0
                },
                {
                    "sent": "So could you impose capacities?",
                    "label": 0
                },
                {
                    "sent": "Other capacities on the network flow problems?",
                    "label": 0
                },
                {
                    "sent": "Is the question related to why do I need a dog or I think it might also partially the question.",
                    "label": 0
                },
                {
                    "sent": "So why do I need a DAG I?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I attended the question, but why do I need a dog is because?",
                    "label": 0
                },
                {
                    "sent": "It didn't have the time to develop much, but when you get to flow on any graph, it can be decomposed into past flows flows that.",
                    "label": 0
                },
                {
                    "sent": "I mean you need some flows going from the source, the sink and cycle flows, which just suck the cycle in the graph.",
                    "label": 0
                },
                {
                    "sent": "And when I have my cost here, when I want to when I need this when I want to build this equivalence between this optimization problem path and the flows, I cannot afford to have this cycle flows.",
                    "label": 0
                },
                {
                    "sent": "The effect which.",
                    "label": 0
                },
                {
                    "sent": "I want to have some unit of flow going from the source such that the meat discussed gamma each time you select a cost.",
                    "label": 0
                },
                {
                    "sent": "Here if you have some flow going on here, the path will only have a cost, which is the length of the path.",
                    "label": 0
                },
                {
                    "sent": "It will just do not have the right weight gamma.",
                    "label": 0
                },
                {
                    "sent": "So there's it.",
                    "label": 0
                },
                {
                    "sent": "This wouldn't make sense in your applications for.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It means that I can run the algorithm on graphs which have cycles.",
                    "label": 0
                },
                {
                    "sent": "It will give something which I.",
                    "label": 0
                },
                {
                    "sent": "But the only thing is that it will tend to select cycles because they will have a lower cost.",
                    "label": 0
                },
                {
                    "sent": "But if you don't have too many cycles, I guess you can have something reasonable, but it might be hard to already know what you get.",
                    "label": 0
                },
                {
                    "sent": "If you have undirected graph, basically cycles for every pairs, then it's really not adept.",
                    "label": 0
                }
            ]
        }
    }
}