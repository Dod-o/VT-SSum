{
    "id": "7m5g3mkrqmfjpm5iluyxjfxf3xilra75",
    "title": "Variational Optimisation by Marginal Matching",
    "info": {
        "author": [
            "Neil D. Lawrence, School of Computer Science, University of Manchester"
        ],
        "published": "Dec. 31, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/abi07_lawrence_vap/",
    "segmentation": [
        [
            "OK, so this is just it's an exploration and really to help me try and understand EP bit better and what it's doing.",
            "What the idea is is that we can decompose the likelihood in the standard sort of close to the variational way.",
            "That's the KL divergent that's normally used in variational log likelihoods.",
            "But then we can introduce site functions in the same way you have any P. But we sort of proceed as if you're in variational situation.",
            "So we've got this extra term and then we've moved the site functions into the likelihood.",
            "Which is normally composed of those two terms there.",
            "Now, what's interesting, if we place certain constraints on what Q of F is in relation to these site functions, we can then it turns out rewrite the likelihood in the following form, which is the sum of three terms, the first of which is the sort of standard EP approximation to the likelihood, the second of which is the sum of KL divergences between the marginal distributions of our variational posterior approximation and the tilted distributions, which arise in EP.",
            "Plus the KL divergent between the full posterior approximation.",
            "The standard variational cover kale divergent and the true."
        ],
        [
            "Posterior.",
            "So what's interesting about that is that then we can look at what the difference between the well first of all.",
            "If we drop the last term, we get the standard variational lower bound on the likelihood.",
            "Now, since the negative EP Energy is given by this first portion, we can see that the difference between the standard variational bound is simply this.",
            "EP Energy is this KL divergent here, which is the sum of all the KL divergences."
        ],
        [
            "Next slide.",
            "So if you look at the difference between this EP Energy and the likelihood, what you see is it's the sum of two KL divergences, well, 1 subtracted from the other.",
            "Now what's very interesting is that the minimum of these KL divergences actually coincide.",
            "So by minimizing iteratively with respect to this KL divergent, you also minimize this KL diversions.",
            "So that seems very promising, and the way you do that is actually by doing the variational approximation.",
            "In this system, the standard variational Bayes.",
            "But zero forcing is a big problem, because if there's any zero forcing in this coming in, the likelihood to force these posteriors to have areas of 0, then both these kled vergence is go very, very large, so you end up with the difference between 2 numbers, which could be very large, and then the.",
            "The approximation you may get is.",
            "Could be poor, but so the question is, it seems like you should be doing the variational thing, but in practice perhaps EP is doing a much better job matching these two KL divergences to make their different small and therefore you're getting better approximations to the marginal likelihood with EP than you do."
        ],
        [
            "Variational bayes.",
            "Well, so with the reference because, well, it's in the poster.",
            "But multi Curse and Carl Rasmussen at use variational bounds to monitor.",
            "So I use variational bound with EP and it sort of gives insight as to when you should be able to do that, but I can tell you about it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is just it's an exploration and really to help me try and understand EP bit better and what it's doing.",
                    "label": 0
                },
                {
                    "sent": "What the idea is is that we can decompose the likelihood in the standard sort of close to the variational way.",
                    "label": 0
                },
                {
                    "sent": "That's the KL divergent that's normally used in variational log likelihoods.",
                    "label": 0
                },
                {
                    "sent": "But then we can introduce site functions in the same way you have any P. But we sort of proceed as if you're in variational situation.",
                    "label": 0
                },
                {
                    "sent": "So we've got this extra term and then we've moved the site functions into the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Which is normally composed of those two terms there.",
                    "label": 0
                },
                {
                    "sent": "Now, what's interesting, if we place certain constraints on what Q of F is in relation to these site functions, we can then it turns out rewrite the likelihood in the following form, which is the sum of three terms, the first of which is the sort of standard EP approximation to the likelihood, the second of which is the sum of KL divergences between the marginal distributions of our variational posterior approximation and the tilted distributions, which arise in EP.",
                    "label": 0
                },
                {
                    "sent": "Plus the KL divergent between the full posterior approximation.",
                    "label": 0
                },
                {
                    "sent": "The standard variational cover kale divergent and the true.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Posterior.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting about that is that then we can look at what the difference between the well first of all.",
                    "label": 0
                },
                {
                    "sent": "If we drop the last term, we get the standard variational lower bound on the likelihood.",
                    "label": 1
                },
                {
                    "sent": "Now, since the negative EP Energy is given by this first portion, we can see that the difference between the standard variational bound is simply this.",
                    "label": 0
                },
                {
                    "sent": "EP Energy is this KL divergent here, which is the sum of all the KL divergences.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next slide.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the difference between this EP Energy and the likelihood, what you see is it's the sum of two KL divergences, well, 1 subtracted from the other.",
                    "label": 0
                },
                {
                    "sent": "Now what's very interesting is that the minimum of these KL divergences actually coincide.",
                    "label": 0
                },
                {
                    "sent": "So by minimizing iteratively with respect to this KL divergent, you also minimize this KL diversions.",
                    "label": 0
                },
                {
                    "sent": "So that seems very promising, and the way you do that is actually by doing the variational approximation.",
                    "label": 0
                },
                {
                    "sent": "In this system, the standard variational Bayes.",
                    "label": 0
                },
                {
                    "sent": "But zero forcing is a big problem, because if there's any zero forcing in this coming in, the likelihood to force these posteriors to have areas of 0, then both these kled vergence is go very, very large, so you end up with the difference between 2 numbers, which could be very large, and then the.",
                    "label": 0
                },
                {
                    "sent": "The approximation you may get is.",
                    "label": 0
                },
                {
                    "sent": "Could be poor, but so the question is, it seems like you should be doing the variational thing, but in practice perhaps EP is doing a much better job matching these two KL divergences to make their different small and therefore you're getting better approximations to the marginal likelihood with EP than you do.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variational bayes.",
                    "label": 0
                },
                {
                    "sent": "Well, so with the reference because, well, it's in the poster.",
                    "label": 0
                },
                {
                    "sent": "But multi Curse and Carl Rasmussen at use variational bounds to monitor.",
                    "label": 0
                },
                {
                    "sent": "So I use variational bound with EP and it sort of gives insight as to when you should be able to do that, but I can tell you about it.",
                    "label": 0
                }
            ]
        }
    }
}