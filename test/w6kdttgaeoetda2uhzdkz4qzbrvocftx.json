{
    "id": "w6kdttgaeoetda2uhzdkz4qzbrvocftx",
    "title": "Bayesian Localized Multiple Kernel Learning",
    "info": {
        "author": [
            "C. Mario Christoudias, Linguistics and Philosophy, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Multiple Kernel Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_christoudias_blmk/",
    "segmentation": [
        [
            "Thank you for coming.",
            "Today I'm going to talk about work on localized multiple kernel learning with Gaussian processes, and this is joint work with Keller's and at Toyota Technological Institute of Chicago and Trevor Darryl at UC Berkeley."
        ],
        [
            "So the problem that we're interested in is doing classification for multiple information sources.",
            "So this can be this can be multiple descriptions of a single source.",
            "For example, in the case of scene, understanding or object categorization, we have multiple feature descriptors that you can use to describe an image.",
            "But it can also be the case we have actually physically independent sources that you want to integrate to perform classification, such as an audio and visual classification.",
            "And with challenging each of these scenarios is that you need to find some way of optimally combining these potentially very disparate sources of information and such that you can obtain accurate classification.",
            "And what we advocate for in this talk is in fact that this optimal combination, although it's typically thought of in terms of doing global combinations, that in fact an optimal combination can often be sample dependent.",
            "So basically, in the case of noise, or in the case of missing data, or when the discriminative properties of your classifier virus over your input space.",
            "So we like to find ways of."
        ],
        [
            "Mining the data in that way for that particular problem.",
            "So multiple kernel learning proposes a good framework for combining multiple sources of information, where you define a kernel function, interviews and then essentially you combine the different sources by combining those kernel functions, for example by adding them together or by multiplying them.",
            "A commonly multiple kernel learning you seek a global combination of your different sources.",
            "So here I have a kernel matrix defining few and have a weight over each of those views.",
            "So what we advocate for in this work is we say well in fact, in the cases we have per sample noise or you have missing data.",
            "Or in the case we just expect that given a pair of examples or an image that different features will be important for describing that image, and in fact you actually want a local combination over your kernel functions.",
            "So in this work, we propose a new multiple kernel learning algorithm with Gaussian process."
        ],
        [
            "Sees that learns a local waiting over space.",
            "In particular, we develop a Gaussian process model where the covariance function is formulated as a product of a parametric form which measures within view similarity and a nonparametric kernel which measures the importance of features across views.",
            "And for tractability, we assume that the local waiting is piecewise constant and perform a clustering of the space and learn a componentwise waiting over the space.",
            "So some related work.",
            "There's been a variety of work in the multiple kernel learning literature."
        ],
        [
            "There's been work on global kernel combination, SVM, learning frameworks as well as global kernel combination, probabilistic frames with Gaussian processes by Co portal, which is actually work developed in our group and collaborators.",
            "There's also been a variety of approaches for developing for performing local kernel combination.",
            "In particular, there's been work by Linda.",
            "Now that looks in sambol of SVM classifiers, where you have a single SVM classifier defined over each input.",
            "There's also been work by going in and alpide and that looked at learning a gating function for doing a sample dependent combination, and more recently there's been work by Yanga.",
            "Now that's that's done.",
            "A group sensitive combination of the difference of the different views, and this can be most similar.",
            "Team is most similar to our approach.",
            "I'm learning framework and here we do a localized model within a Gaussian process framework and probabilistic setting.",
            "So before describing our model, I just want to give a brief background of Gaussian processes, although."
        ],
        [
            "Those of you have seen first, William Stalker already had an introduction to this.",
            "So Gaussian processes are Bayesian approach for doing regression where you assume Gaussian process prior with the space of functions regressing too.",
            "It's expressed by the following graphical model, where X is your input, F is a latent function that you wish to learn.",
            "And why is your output so the latent function F?",
            "You would define the Gaussian process prior over that little space.",
            "Latent functions where the covariance of that Gaussian is defined using a kernel function or similarity function.",
            "Then this latent functions later output by IID Gaussian noise typically.",
            "So basically the parameters of this model or the parameters of your similarity function as well as the noise of your output and typically the way you learn these parameters in Gaussian processes is you essentially do this by Max likelihood on your data.",
            "And now for inference.",
            "You can marginalized out these late enough variables which which results the.",
            "You can do the marginalization in closed form and this results in a Gaussian predictive distribution whose mean is known as the mean prediction.",
            "The most likely estimate under the Gaussian process model and the covariance is the uncertainty.",
            "So in our approach we develop a Gaussian process model."
        ],
        [
            "The covariance function is parameterized as the sum of covariance functions or kernel functions one per view.",
            "So here visa number of views and we have case the kernel function in each view.",
            "And what we do is, we assume that the kernel function in this view has a particular form.",
            "In particular, we parameterized it as a product of a parametric kernel function in that view that measures.",
            "The similarity within that view.",
            "As well as a nonparametric weighting that that in fact weights pairs of examples so.",
            "I if I just expand out this form, what I'm showing here is a nonparametric kernel where I have."
        ],
        [
            "Pairwise waiting in each view saying how important is this feature for discriminate examples as well as a parametric kernel in that?",
            "I love you and I miss using simple Matlab notation to kind of show the element wise product.",
            "So although this kind of expresses the problem in kind of a very general way, the problem here is that you have N squared parameters per view.",
            "So we need some way of kind of making this problem tractable, because then squared parameters you overfit.",
            "So the way that we do that is we or one way you can do that is by assuming that this weighting matrix is somehow."
        ],
        [
            "Bank constraint.",
            "But there are set of G basis functions that in effect describe the important or the different ways that this nonparametric matrix can vary.",
            "And doing so reduces the problem complexity to M * N parameters.",
            "But in fact what we do is we consider a model where we assume that this nonparametric matrix is rank one."
        ],
        [
            "And under this scenario, essentially the basis function G is describing for each sample how confident we are in that sample in that view.",
            "So this gives us order N parameters, but still a fair number of parameters to learn under this model.",
            "So finally, the last assumption we make is that we assume that."
        ],
        [
            "This G vector.",
            "This weighting vector is locally smooth.",
            "That basically examples that are nearby in the input space will have a similar waiting right and we express this as essentially having a set of latent components or latent structure in the data that says different local regions of the space which are given by E and then for each latent component we have awaiting that we learn of how we optimally combine the kernels and over that component.",
            "So you can see in some way this kind of describing a slightly more complicated problem than just having a single weight proper sample.",
            "But in fact what we do is in our experiments we pre cluster to give the components and now we're left to optimize over is the Alpha parameters.",
            "OK, so now this this gives a componentwise waiting of the different kernels with the components are specified by our clustering E. And now we have V * P parameters, or P is the number of components visa number of use.",
            "Now, of course, if I take this a step further, then we're back down to kind of convention."
        ],
        [
            "Multiple kernel learning where we have a single weight per view, right?",
            "So here especially this matrix collapses to a single term here V parameters.",
            "But what we're doing here is looking at something slightly more complex where we're actually doing looking for componentwise combination.",
            "So then under our model."
        ],
        [
            "So the so the learning under model, essentially to learn these these weights in each of the components.",
            "And the way we do that is, we do that through maximum likelihood, where we minimize the following following log posterior, where here the first term is the likelihood of the Gaussian process model to handle multiple classes, and we have a prior that says.",
            "I want to favor non zero weights.",
            "An inference under model is done via standard gypsy mean prediction, where I'm just showing the equation for one versus all classification and see.",
            "Here is the class.",
            "So I'm just going to briefly go over a synthetic example, just to kind of give some intuition beyond our approach in some water."
        ],
        [
            "Nation.",
            "So here what I'm showing is a problem where I have a 2D space and with two classes where the circles there's one class and X is another class.",
            "And now from the studio space.",
            "Imagine these are the two inputs that we get to our algorithm, particularly at the first dimension II."
        ],
        [
            "Well first, First off, what we can see is that if we just project these examples down, but essentially the distributions of each class completely overlap, right?",
            "So it's difficult within each of you to discriminate one class from the other.",
            "So and also you can imagine that if you do a global combination of these views then you're not going to be able to account for the fact that these classes are overlapping and correct.",
            "Essentially for this poor projection.",
            "Where is the fighting a local combination you can affect try and separate out these two classes in each view and then then recover accurate performance.",
            "And that's pretty much our approach proceeds, so we had to get a clustering of the data where E1 to E4 different clusters."
        ],
        [
            "There are each cluster.",
            "We have a set of weights over each view, and then for example one waiting.",
            "You might learn is to learn that one class gets zero waiting, another class gets away."
        ],
        [
            "One waiting and that in effect separates out the different classes.",
            "In this case, although of course there are other weightings that would also separate out the classes and are learned by the algorithm.",
            "And then here are some experiments we did with this synthetic scenario.",
            "So."
        ],
        [
            "Here we compare our approach in red to variety of baseline approaches.",
            "In particular, we compare approach to Gaussian process classifier defining each view as well as the approach of global combination.",
            "So in particular global kernel combination summing globally the kernels as well as doing a late integration, we take the output of each Gaussian process in each view and we just average the mean prediction.",
            "And what we can see is that first of all from doing classification from other view alone is quite poorly as you'd expect, but also thinking global combination doesn't improve much either, whereas by doing a local combination with our approach or component combination were able to achieve and recover good performance.",
            "And here I'm showing the curve for a four clusters, but you can see that we increase the number of clusters and affect our algorithm is fairly insensitive to and over clustering of the space.",
            "So you can just kind of know an approximate estimate of what kind of structure you expect and then just can you can use an over clustering."
        ],
        [
            "So I'm just going to briefly talk about some experiments, so we looked at two tasks in this work.",
            "The first task was looking at a view disagreement data set from our previous work, which was an audio visual task that consisted of 15 subjects interacting with an avatar, answering a set of yes and no questions with head, gesture, and speech.",
            "Where the head gesture was head nods and head shakes, and the speech was yes and no utterances and one problem in in.",
            "In this kind of scenario is that you know person often can say yes and head nod.",
            "So you can classify from our view together.",
            "But it's also the case that a person may say yes, not head nod or head nod and not say yes right.",
            "So in essence you have kind of this per sample occlusion problem occurring, overview disagreement and in our experiments we simulate the view disagreement where we place a sample in the visual modality with background head motion.",
            "And in the audio modality with audio Babble noise.",
            "And then we experiment with various amounts of disagreement."
        ],
        [
            "So, so here's the result that we get on with no Visa interview disagreement, and essentially we can see is I'm showing the performance of the audio classifier as well as the visual classifier and the other baselines, as well as our approach and essentially in the case of no use to disagreement.",
            "Despite the poor performance of the audio classifier, were able to leverage the good performance of the visual one."
        ],
        [
            "Now if we increase the amount of view disagreement, then we see a few things.",
            "First of all, the single view classifiers degrade because if it's completely included in one view, then you can't reliably estimate from a single view.",
            "Similarly, the global combination is unable to correct this per sample occlusion, whereas our approach is able to get active performance."
        ],
        [
            "We also ran experiments on a category recognition data set, in particular Caltech 101, which consists of 1 two categories of various objects, and for this data set we looked at four different feature kernels that are commonly used in the literature and in particular geometric permanent match.",
            "In my experiments with those."
        ],
        [
            "And these are the results that we get comparing to the various baseline.",
            "So essentially what this plot shows, it shows that essentially with our approach we get improvement over any other view alone, and we also perform an integration similarly if."
        ],
        [
            "She.",
            "Compared to the state of the art, we see that our approach does fairly well over here compared to some of the more recent saved our approaches.",
            "So kind of 1 drawback that we found here is that we didn't get any improvement over global combination on this data set.",
            "In particular, we perform similarly to the global model, but the."
        ],
        [
            "Good news is that we don't do any worse.",
            "So increase the number of clusters.",
            "We specially get a stable performance."
        ],
        [
            "So in the interest of time, I'll just skip over."
        ],
        [
            "This.",
            "So in conclusion, we presented a localized multiple kernel learning algorithm with Gaussian processes, where we model the covariance function of Gaussian process with as a product of a parametric function, Anna nonparametric kernel that measures across feature importance.",
            "We demonstrate our approach on the task of audiovisual user agreement as well as object conditional feature types and we also are currently looking at alternative regularization schemes within our model.",
            "Then, instead of using a clustering of the space.",
            "Take some kind of regularizer that that you can still use to learn all the end samples or the the end.",
            "Measure the end confidence values.",
            "And here's kind of the result that we get from this."
        ],
        [
            "Getting a full covariance with those kind of regularizers, but it's kind of preliminary work.",
            "So."
        ],
        [
            "So thank you very much."
        ],
        [
            "Online."
        ],
        [
            "Is it raining down?",
            "I. Yeah, it's kind of arbitrary that the splits that we had is we generally we.",
            "Use kernels completed over 30 split of 30 images.",
            "So we essentially did splits over those.",
            "Those 30 images per class so.",
            "No, no, I mean, although it mean Gaussian process use, you know there are issues if you have large datasets and bother sparsification techniques for handling with that.",
            "But that wasn't the case the case here.",
            "But so.",
            "OK programs correctly.",
            "Specify that you will need to form a version of the brand.",
            "Now.",
            "You're actually working with approximation rank constraint, proximity to the public pool area.",
            "So my question is, how do you?",
            "Ensure that you end up with approximation ends up doing the right so so basically here we are essentially so one thing I didn't mention is that we assume that the weights are positive, so the only optimize were positive ratings.",
            "And then then, essentially because we're now summing over positive weightings over the parametric kernel function, each view then.",
            "Essentially, since those parametric kernel functions are are.",
            "Mercer kernels are valid kernels then.",
            "Yes.",
            "Right?",
            "So it's not Uncle screen.",
            "We have our brother.",
            "So.",
            "Further then he has the primary provider for this program.",
            "Right?",
            "Decisions.",
            "So it's not working.",
            "The waiting is rank constrained, yeah?",
            "More about the resulting sure compared with the market economy.",
            "Sure, this is spot."
        ],
        [
            "Hello.",
            "This one sure, especially what we're showing here, is.",
            "So one benefit we did see on the Celtic 101 data set for having a local waiting in the case of missing data.",
            "So in essence, what I'm showing here is the performance of Single View.",
            "Classifiers are the increasing amount of missing data or missing data.",
            "Here is removing.",
            "Channel from from from from each sample where essentially for each class we remove.",
            "A set of samples per view, and then I increase the amount of missing data.",
            "Very much so.",
            "Essentially what we see is that although the other classifier is kind of our steady across amount of missing data, what we see is actually multiple kernel learning because it can really only take advantage of fully observed data essence as we increase the amount of this examples, then that multiple kernel learning of global combination degrades, whereas by having a local combination our approach is able to take to essentially take into account both partial observed an fully observed data and maintain relatively good performance.",
            "Versions.",
            "What was the reversal in your room?",
            "You like the the marginal likelihood.",
            "You're welcome.",
            "I see you had a prior which she described as a private papers known as Zero, yes."
        ],
        [
            "Solutions.",
            "It's strange to me, so that is this is really necessary to.",
            "To impose non zero solutions to a prior and we shouldn't this kind of happen naturally even through the light went so well.",
            "So The thing is because we're having essentially a waiting on each on each of these kind of person Apple waiting on each of these.",
            "Our component was working through these kernels that essentially you know, we want to make sure that.",
            "In fact, we don't end up getting a zero kind of major wanting to make sure that the matrix is well conditioned so we can invert it in the end of the Gaussian process.",
            "But in addition, also this likelihood so you can see this turn here, for example, minimizing that term.",
            "One way to do that is to drive the parameters to 0, so you want to kind of event.",
            "Avoid that kind of degenerate solution.",
            "I'm.",
            "OK, but do you like her?",
            "So if it's a local stereo, like for predicting the wise, how can that be in terms of the low light it with a good solution and write everything to 00?",
            "Yeah, basically nonsense.",
            "Yeah, so it's basically the interplay of the data term and the prior term here and you want to look at that likelihood in terms of that decomposition, right?",
            "So if this waiting dominates and then then I want to minimize that term.",
            "So now I'll drive things to 0, right?",
            "But in general you're right if this term is dominating that then that would occur.",
            "OK, let's speak."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for coming.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about work on localized multiple kernel learning with Gaussian processes, and this is joint work with Keller's and at Toyota Technological Institute of Chicago and Trevor Darryl at UC Berkeley.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem that we're interested in is doing classification for multiple information sources.",
                    "label": 0
                },
                {
                    "sent": "So this can be this can be multiple descriptions of a single source.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of scene, understanding or object categorization, we have multiple feature descriptors that you can use to describe an image.",
                    "label": 1
                },
                {
                    "sent": "But it can also be the case we have actually physically independent sources that you want to integrate to perform classification, such as an audio and visual classification.",
                    "label": 0
                },
                {
                    "sent": "And with challenging each of these scenarios is that you need to find some way of optimally combining these potentially very disparate sources of information and such that you can obtain accurate classification.",
                    "label": 1
                },
                {
                    "sent": "And what we advocate for in this talk is in fact that this optimal combination, although it's typically thought of in terms of doing global combinations, that in fact an optimal combination can often be sample dependent.",
                    "label": 0
                },
                {
                    "sent": "So basically, in the case of noise, or in the case of missing data, or when the discriminative properties of your classifier virus over your input space.",
                    "label": 0
                },
                {
                    "sent": "So we like to find ways of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mining the data in that way for that particular problem.",
                    "label": 0
                },
                {
                    "sent": "So multiple kernel learning proposes a good framework for combining multiple sources of information, where you define a kernel function, interviews and then essentially you combine the different sources by combining those kernel functions, for example by adding them together or by multiplying them.",
                    "label": 1
                },
                {
                    "sent": "A commonly multiple kernel learning you seek a global combination of your different sources.",
                    "label": 1
                },
                {
                    "sent": "So here I have a kernel matrix defining few and have a weight over each of those views.",
                    "label": 0
                },
                {
                    "sent": "So what we advocate for in this work is we say well in fact, in the cases we have per sample noise or you have missing data.",
                    "label": 0
                },
                {
                    "sent": "Or in the case we just expect that given a pair of examples or an image that different features will be important for describing that image, and in fact you actually want a local combination over your kernel functions.",
                    "label": 0
                },
                {
                    "sent": "So in this work, we propose a new multiple kernel learning algorithm with Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sees that learns a local waiting over space.",
                    "label": 1
                },
                {
                    "sent": "In particular, we develop a Gaussian process model where the covariance function is formulated as a product of a parametric form which measures within view similarity and a nonparametric kernel which measures the importance of features across views.",
                    "label": 0
                },
                {
                    "sent": "And for tractability, we assume that the local waiting is piecewise constant and perform a clustering of the space and learn a componentwise waiting over the space.",
                    "label": 1
                },
                {
                    "sent": "So some related work.",
                    "label": 0
                },
                {
                    "sent": "There's been a variety of work in the multiple kernel learning literature.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's been work on global kernel combination, SVM, learning frameworks as well as global kernel combination, probabilistic frames with Gaussian processes by Co portal, which is actually work developed in our group and collaborators.",
                    "label": 1
                },
                {
                    "sent": "There's also been a variety of approaches for developing for performing local kernel combination.",
                    "label": 0
                },
                {
                    "sent": "In particular, there's been work by Linda.",
                    "label": 0
                },
                {
                    "sent": "Now that looks in sambol of SVM classifiers, where you have a single SVM classifier defined over each input.",
                    "label": 0
                },
                {
                    "sent": "There's also been work by going in and alpide and that looked at learning a gating function for doing a sample dependent combination, and more recently there's been work by Yanga.",
                    "label": 0
                },
                {
                    "sent": "Now that's that's done.",
                    "label": 0
                },
                {
                    "sent": "A group sensitive combination of the difference of the different views, and this can be most similar.",
                    "label": 0
                },
                {
                    "sent": "Team is most similar to our approach.",
                    "label": 0
                },
                {
                    "sent": "I'm learning framework and here we do a localized model within a Gaussian process framework and probabilistic setting.",
                    "label": 0
                },
                {
                    "sent": "So before describing our model, I just want to give a brief background of Gaussian processes, although.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those of you have seen first, William Stalker already had an introduction to this.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian processes are Bayesian approach for doing regression where you assume Gaussian process prior with the space of functions regressing too.",
                    "label": 1
                },
                {
                    "sent": "It's expressed by the following graphical model, where X is your input, F is a latent function that you wish to learn.",
                    "label": 0
                },
                {
                    "sent": "And why is your output so the latent function F?",
                    "label": 0
                },
                {
                    "sent": "You would define the Gaussian process prior over that little space.",
                    "label": 0
                },
                {
                    "sent": "Latent functions where the covariance of that Gaussian is defined using a kernel function or similarity function.",
                    "label": 0
                },
                {
                    "sent": "Then this latent functions later output by IID Gaussian noise typically.",
                    "label": 0
                },
                {
                    "sent": "So basically the parameters of this model or the parameters of your similarity function as well as the noise of your output and typically the way you learn these parameters in Gaussian processes is you essentially do this by Max likelihood on your data.",
                    "label": 0
                },
                {
                    "sent": "And now for inference.",
                    "label": 0
                },
                {
                    "sent": "You can marginalized out these late enough variables which which results the.",
                    "label": 1
                },
                {
                    "sent": "You can do the marginalization in closed form and this results in a Gaussian predictive distribution whose mean is known as the mean prediction.",
                    "label": 0
                },
                {
                    "sent": "The most likely estimate under the Gaussian process model and the covariance is the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So in our approach we develop a Gaussian process model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The covariance function is parameterized as the sum of covariance functions or kernel functions one per view.",
                    "label": 0
                },
                {
                    "sent": "So here visa number of views and we have case the kernel function in each view.",
                    "label": 0
                },
                {
                    "sent": "And what we do is, we assume that the kernel function in this view has a particular form.",
                    "label": 0
                },
                {
                    "sent": "In particular, we parameterized it as a product of a parametric kernel function in that view that measures.",
                    "label": 0
                },
                {
                    "sent": "The similarity within that view.",
                    "label": 0
                },
                {
                    "sent": "As well as a nonparametric weighting that that in fact weights pairs of examples so.",
                    "label": 0
                },
                {
                    "sent": "I if I just expand out this form, what I'm showing here is a nonparametric kernel where I have.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pairwise waiting in each view saying how important is this feature for discriminate examples as well as a parametric kernel in that?",
                    "label": 0
                },
                {
                    "sent": "I love you and I miss using simple Matlab notation to kind of show the element wise product.",
                    "label": 0
                },
                {
                    "sent": "So although this kind of expresses the problem in kind of a very general way, the problem here is that you have N squared parameters per view.",
                    "label": 0
                },
                {
                    "sent": "So we need some way of kind of making this problem tractable, because then squared parameters you overfit.",
                    "label": 0
                },
                {
                    "sent": "So the way that we do that is we or one way you can do that is by assuming that this weighting matrix is somehow.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bank constraint.",
                    "label": 0
                },
                {
                    "sent": "But there are set of G basis functions that in effect describe the important or the different ways that this nonparametric matrix can vary.",
                    "label": 0
                },
                {
                    "sent": "And doing so reduces the problem complexity to M * N parameters.",
                    "label": 1
                },
                {
                    "sent": "But in fact what we do is we consider a model where we assume that this nonparametric matrix is rank one.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And under this scenario, essentially the basis function G is describing for each sample how confident we are in that sample in that view.",
                    "label": 0
                },
                {
                    "sent": "So this gives us order N parameters, but still a fair number of parameters to learn under this model.",
                    "label": 1
                },
                {
                    "sent": "So finally, the last assumption we make is that we assume that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This G vector.",
                    "label": 0
                },
                {
                    "sent": "This weighting vector is locally smooth.",
                    "label": 0
                },
                {
                    "sent": "That basically examples that are nearby in the input space will have a similar waiting right and we express this as essentially having a set of latent components or latent structure in the data that says different local regions of the space which are given by E and then for each latent component we have awaiting that we learn of how we optimally combine the kernels and over that component.",
                    "label": 0
                },
                {
                    "sent": "So you can see in some way this kind of describing a slightly more complicated problem than just having a single weight proper sample.",
                    "label": 0
                },
                {
                    "sent": "But in fact what we do is in our experiments we pre cluster to give the components and now we're left to optimize over is the Alpha parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this this gives a componentwise waiting of the different kernels with the components are specified by our clustering E. And now we have V * P parameters, or P is the number of components visa number of use.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if I take this a step further, then we're back down to kind of convention.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multiple kernel learning where we have a single weight per view, right?",
                    "label": 1
                },
                {
                    "sent": "So here especially this matrix collapses to a single term here V parameters.",
                    "label": 0
                },
                {
                    "sent": "But what we're doing here is looking at something slightly more complex where we're actually doing looking for componentwise combination.",
                    "label": 0
                },
                {
                    "sent": "So then under our model.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the so the learning under model, essentially to learn these these weights in each of the components.",
                    "label": 0
                },
                {
                    "sent": "And the way we do that is, we do that through maximum likelihood, where we minimize the following following log posterior, where here the first term is the likelihood of the Gaussian process model to handle multiple classes, and we have a prior that says.",
                    "label": 0
                },
                {
                    "sent": "I want to favor non zero weights.",
                    "label": 0
                },
                {
                    "sent": "An inference under model is done via standard gypsy mean prediction, where I'm just showing the equation for one versus all classification and see.",
                    "label": 0
                },
                {
                    "sent": "Here is the class.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to briefly go over a synthetic example, just to kind of give some intuition beyond our approach in some water.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "So here what I'm showing is a problem where I have a 2D space and with two classes where the circles there's one class and X is another class.",
                    "label": 0
                },
                {
                    "sent": "And now from the studio space.",
                    "label": 0
                },
                {
                    "sent": "Imagine these are the two inputs that we get to our algorithm, particularly at the first dimension II.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well first, First off, what we can see is that if we just project these examples down, but essentially the distributions of each class completely overlap, right?",
                    "label": 0
                },
                {
                    "sent": "So it's difficult within each of you to discriminate one class from the other.",
                    "label": 0
                },
                {
                    "sent": "So and also you can imagine that if you do a global combination of these views then you're not going to be able to account for the fact that these classes are overlapping and correct.",
                    "label": 0
                },
                {
                    "sent": "Essentially for this poor projection.",
                    "label": 0
                },
                {
                    "sent": "Where is the fighting a local combination you can affect try and separate out these two classes in each view and then then recover accurate performance.",
                    "label": 0
                },
                {
                    "sent": "And that's pretty much our approach proceeds, so we had to get a clustering of the data where E1 to E4 different clusters.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are each cluster.",
                    "label": 0
                },
                {
                    "sent": "We have a set of weights over each view, and then for example one waiting.",
                    "label": 0
                },
                {
                    "sent": "You might learn is to learn that one class gets zero waiting, another class gets away.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One waiting and that in effect separates out the different classes.",
                    "label": 0
                },
                {
                    "sent": "In this case, although of course there are other weightings that would also separate out the classes and are learned by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then here are some experiments we did with this synthetic scenario.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we compare our approach in red to variety of baseline approaches.",
                    "label": 1
                },
                {
                    "sent": "In particular, we compare approach to Gaussian process classifier defining each view as well as the approach of global combination.",
                    "label": 1
                },
                {
                    "sent": "So in particular global kernel combination summing globally the kernels as well as doing a late integration, we take the output of each Gaussian process in each view and we just average the mean prediction.",
                    "label": 0
                },
                {
                    "sent": "And what we can see is that first of all from doing classification from other view alone is quite poorly as you'd expect, but also thinking global combination doesn't improve much either, whereas by doing a local combination with our approach or component combination were able to achieve and recover good performance.",
                    "label": 0
                },
                {
                    "sent": "And here I'm showing the curve for a four clusters, but you can see that we increase the number of clusters and affect our algorithm is fairly insensitive to and over clustering of the space.",
                    "label": 1
                },
                {
                    "sent": "So you can just kind of know an approximate estimate of what kind of structure you expect and then just can you can use an over clustering.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm just going to briefly talk about some experiments, so we looked at two tasks in this work.",
                    "label": 0
                },
                {
                    "sent": "The first task was looking at a view disagreement data set from our previous work, which was an audio visual task that consisted of 15 subjects interacting with an avatar, answering a set of yes and no questions with head, gesture, and speech.",
                    "label": 1
                },
                {
                    "sent": "Where the head gesture was head nods and head shakes, and the speech was yes and no utterances and one problem in in.",
                    "label": 0
                },
                {
                    "sent": "In this kind of scenario is that you know person often can say yes and head nod.",
                    "label": 0
                },
                {
                    "sent": "So you can classify from our view together.",
                    "label": 0
                },
                {
                    "sent": "But it's also the case that a person may say yes, not head nod or head nod and not say yes right.",
                    "label": 0
                },
                {
                    "sent": "So in essence you have kind of this per sample occlusion problem occurring, overview disagreement and in our experiments we simulate the view disagreement where we place a sample in the visual modality with background head motion.",
                    "label": 1
                },
                {
                    "sent": "And in the audio modality with audio Babble noise.",
                    "label": 0
                },
                {
                    "sent": "And then we experiment with various amounts of disagreement.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so here's the result that we get on with no Visa interview disagreement, and essentially we can see is I'm showing the performance of the audio classifier as well as the visual classifier and the other baselines, as well as our approach and essentially in the case of no use to disagreement.",
                    "label": 0
                },
                {
                    "sent": "Despite the poor performance of the audio classifier, were able to leverage the good performance of the visual one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if we increase the amount of view disagreement, then we see a few things.",
                    "label": 1
                },
                {
                    "sent": "First of all, the single view classifiers degrade because if it's completely included in one view, then you can't reliably estimate from a single view.",
                    "label": 1
                },
                {
                    "sent": "Similarly, the global combination is unable to correct this per sample occlusion, whereas our approach is able to get active performance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also ran experiments on a category recognition data set, in particular Caltech 101, which consists of 1 two categories of various objects, and for this data set we looked at four different feature kernels that are commonly used in the literature and in particular geometric permanent match.",
                    "label": 0
                },
                {
                    "sent": "In my experiments with those.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the results that we get comparing to the various baseline.",
                    "label": 0
                },
                {
                    "sent": "So essentially what this plot shows, it shows that essentially with our approach we get improvement over any other view alone, and we also perform an integration similarly if.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She.",
                    "label": 0
                },
                {
                    "sent": "Compared to the state of the art, we see that our approach does fairly well over here compared to some of the more recent saved our approaches.",
                    "label": 0
                },
                {
                    "sent": "So kind of 1 drawback that we found here is that we didn't get any improvement over global combination on this data set.",
                    "label": 0
                },
                {
                    "sent": "In particular, we perform similarly to the global model, but the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good news is that we don't do any worse.",
                    "label": 0
                },
                {
                    "sent": "So increase the number of clusters.",
                    "label": 1
                },
                {
                    "sent": "We specially get a stable performance.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the interest of time, I'll just skip over.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, we presented a localized multiple kernel learning algorithm with Gaussian processes, where we model the covariance function of Gaussian process with as a product of a parametric function, Anna nonparametric kernel that measures across feature importance.",
                    "label": 1
                },
                {
                    "sent": "We demonstrate our approach on the task of audiovisual user agreement as well as object conditional feature types and we also are currently looking at alternative regularization schemes within our model.",
                    "label": 0
                },
                {
                    "sent": "Then, instead of using a clustering of the space.",
                    "label": 0
                },
                {
                    "sent": "Take some kind of regularizer that that you can still use to learn all the end samples or the the end.",
                    "label": 0
                },
                {
                    "sent": "Measure the end confidence values.",
                    "label": 0
                },
                {
                    "sent": "And here's kind of the result that we get from this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getting a full covariance with those kind of regularizers, but it's kind of preliminary work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Online.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it raining down?",
                    "label": 0
                },
                {
                    "sent": "I. Yeah, it's kind of arbitrary that the splits that we had is we generally we.",
                    "label": 0
                },
                {
                    "sent": "Use kernels completed over 30 split of 30 images.",
                    "label": 0
                },
                {
                    "sent": "So we essentially did splits over those.",
                    "label": 0
                },
                {
                    "sent": "Those 30 images per class so.",
                    "label": 0
                },
                {
                    "sent": "No, no, I mean, although it mean Gaussian process use, you know there are issues if you have large datasets and bother sparsification techniques for handling with that.",
                    "label": 0
                },
                {
                    "sent": "But that wasn't the case the case here.",
                    "label": 0
                },
                {
                    "sent": "But so.",
                    "label": 0
                },
                {
                    "sent": "OK programs correctly.",
                    "label": 0
                },
                {
                    "sent": "Specify that you will need to form a version of the brand.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "You're actually working with approximation rank constraint, proximity to the public pool area.",
                    "label": 0
                },
                {
                    "sent": "So my question is, how do you?",
                    "label": 0
                },
                {
                    "sent": "Ensure that you end up with approximation ends up doing the right so so basically here we are essentially so one thing I didn't mention is that we assume that the weights are positive, so the only optimize were positive ratings.",
                    "label": 0
                },
                {
                    "sent": "And then then, essentially because we're now summing over positive weightings over the parametric kernel function, each view then.",
                    "label": 0
                },
                {
                    "sent": "Essentially, since those parametric kernel functions are are.",
                    "label": 0
                },
                {
                    "sent": "Mercer kernels are valid kernels then.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So it's not Uncle screen.",
                    "label": 0
                },
                {
                    "sent": "We have our brother.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Further then he has the primary provider for this program.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Decisions.",
                    "label": 0
                },
                {
                    "sent": "So it's not working.",
                    "label": 0
                },
                {
                    "sent": "The waiting is rank constrained, yeah?",
                    "label": 0
                },
                {
                    "sent": "More about the resulting sure compared with the market economy.",
                    "label": 0
                },
                {
                    "sent": "Sure, this is spot.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "This one sure, especially what we're showing here, is.",
                    "label": 0
                },
                {
                    "sent": "So one benefit we did see on the Celtic 101 data set for having a local waiting in the case of missing data.",
                    "label": 1
                },
                {
                    "sent": "So in essence, what I'm showing here is the performance of Single View.",
                    "label": 0
                },
                {
                    "sent": "Classifiers are the increasing amount of missing data or missing data.",
                    "label": 0
                },
                {
                    "sent": "Here is removing.",
                    "label": 0
                },
                {
                    "sent": "Channel from from from from each sample where essentially for each class we remove.",
                    "label": 0
                },
                {
                    "sent": "A set of samples per view, and then I increase the amount of missing data.",
                    "label": 0
                },
                {
                    "sent": "Very much so.",
                    "label": 1
                },
                {
                    "sent": "Essentially what we see is that although the other classifier is kind of our steady across amount of missing data, what we see is actually multiple kernel learning because it can really only take advantage of fully observed data essence as we increase the amount of this examples, then that multiple kernel learning of global combination degrades, whereas by having a local combination our approach is able to take to essentially take into account both partial observed an fully observed data and maintain relatively good performance.",
                    "label": 0
                },
                {
                    "sent": "Versions.",
                    "label": 0
                },
                {
                    "sent": "What was the reversal in your room?",
                    "label": 0
                },
                {
                    "sent": "You like the the marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "You're welcome.",
                    "label": 0
                },
                {
                    "sent": "I see you had a prior which she described as a private papers known as Zero, yes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solutions.",
                    "label": 0
                },
                {
                    "sent": "It's strange to me, so that is this is really necessary to.",
                    "label": 0
                },
                {
                    "sent": "To impose non zero solutions to a prior and we shouldn't this kind of happen naturally even through the light went so well.",
                    "label": 0
                },
                {
                    "sent": "So The thing is because we're having essentially a waiting on each on each of these kind of person Apple waiting on each of these.",
                    "label": 0
                },
                {
                    "sent": "Our component was working through these kernels that essentially you know, we want to make sure that.",
                    "label": 0
                },
                {
                    "sent": "In fact, we don't end up getting a zero kind of major wanting to make sure that the matrix is well conditioned so we can invert it in the end of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "But in addition, also this likelihood so you can see this turn here, for example, minimizing that term.",
                    "label": 0
                },
                {
                    "sent": "One way to do that is to drive the parameters to 0, so you want to kind of event.",
                    "label": 0
                },
                {
                    "sent": "Avoid that kind of degenerate solution.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "OK, but do you like her?",
                    "label": 0
                },
                {
                    "sent": "So if it's a local stereo, like for predicting the wise, how can that be in terms of the low light it with a good solution and write everything to 00?",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically nonsense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's basically the interplay of the data term and the prior term here and you want to look at that likelihood in terms of that decomposition, right?",
                    "label": 0
                },
                {
                    "sent": "So if this waiting dominates and then then I want to minimize that term.",
                    "label": 0
                },
                {
                    "sent": "So now I'll drive things to 0, right?",
                    "label": 0
                },
                {
                    "sent": "But in general you're right if this term is dominating that then that would occur.",
                    "label": 0
                },
                {
                    "sent": "OK, let's speak.",
                    "label": 0
                }
            ]
        }
    }
}