{
    "id": "74hp5bptubomofylxf4mjiwdkjvsblqd",
    "title": "Sparse Multi-output Gaussian Processes",
    "info": {
        "author": [
            "Mauricio Alvarez, School of Computer Science, University of Manchester"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "May 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/aispds08_alvarez_smogp/",
    "segmentation": [
        [
            "I'm.",
            "I'm going to talk about sparse movie local Gaussian processes.",
            "This is joint work.",
            "We need Lawrence.",
            "This school of Computer Science, University of Manchester."
        ],
        [
            "This is an outline of our presentation.",
            "So we're going to give a perspective of the convolution process.",
            "So we're going to talk about our approach for the sparse approximation, which includes some conditional independence assumptions.",
            "Some experiments on some conclusions."
        ],
        [
            "So we consider the problem of modeling correlated outputs from a single Gaussian process.",
            "Usually this is difficult because we are required to compute not only covariances for each output, but cross covariances between outputs, and those covariances must be such that they must be expressive in the sense that they capture dependencies and correlations between outputs for one side and the other side.",
            "They must lead to a positive definite matrix for the full Gaussian process.",
            "There are many applications on these, for example in just risztics.",
            "Literature where usually you have.",
            "Civil variables, but you have more emissions of 1 variable than the other.",
            "Usually the one that's less expensive to measure and you want to use those measures to find to try to predict the most expensive ones.",
            "There's one of the applications.",
            "One myth."
        ],
        [
            "Use for this kind of modeling task employees the convolution process.",
            "So convolution process is just moving average constructions, guaranteed symbolic covariance function.",
            "The problem is that when we construct the full Gaussian process using this approach with another with the matrix which goes as Ncube being and the number of points QQB&Q, the number outputs.",
            "So where we going to do is to introduce sparse approximation for the full covariance, exploiting the conditional independence assumption that it's.",
            "An implicit in the convolution process framework."
        ],
        [
            "So let's talk about little about the compilation, the convolution process.",
            "So we consider a set of functions FQ and we have capital Q of those functions and function can be expressed about.",
            "The function can be expressed using this generation mechanism, which consists in just solving this integral where KQ is a smoothing kernel function and you is a latent process or latent function.",
            "Now this is typical form in linear systems.",
            "For example, where this K function here.",
            "Smoothing kernel here usually is the impulse responses with system.",
            "And this latent function is just the input to the system.",
            "This is more fun.",
            "Function is related, for example with the scription in terms of differential equation of the system.",
            "We can allow the influence of more than one latent function.",
            "For example, our functions and include an independent process, so our final output constructed with this mechanism is just the sum of the process due to the.",
            "Convolution process an independent process WQ?"
        ],
        [
            "So here is a diagram of how do we construct the outputs.",
            "So we have this latent function."
        ],
        [
            "Then we make the convolution between this latent function and smoothing kernels.",
            "Which is the sole reason Teague?"
        ],
        [
            "We end up with this output functions.",
            "Which are correlated to they are both generated by the same process."
        ],
        [
            "Then we add an independent process and we."
        ],
        [
            "And finally, with the outputs we want to model."
        ],
        [
            "Since.",
            "OK, if we assume."
        ],
        [
            "Does the latent functions are Gaussian processes and the two this is a linear operation.",
            "The output is going to be a Gaussian process as well."
        ],
        [
            "And the covariance function for the outputs is giving us the sum of the covariance of the functions into the convolution process plus our auto covariance due to the independent process an.",
            "This covariance is expressed in this way.",
            "We see here a double sum and a double integral, which implicates the smoothing kernel.",
            "And the covariance important, the covariance of the latent functions."
        ],
        [
            "So.",
            "Depending on the form of these covariance, are the latent function that we assume for the process.",
            "We have different different forms for the actual covariance of the outputs.",
            "In the past, many literature for constructing special process is common.",
            "Option is to assume that the latent functions white noise process.",
            "So if you assume that this variance, these are ways processes, this is a Delta function.",
            "And we can eliminate one of the integrals of one of these sums.",
            "This should be the form of the covariance of the outputs of the convolution process.",
            "You know, work.",
            "We're going to assume that those processes are independent Gaussian processes, so.",
            "The independence assumption of the Gaussian processes allow us to eliminate one of the songs and we get this expression for the covariance of the functions constructed with this convolution process.",
            "And then we have two.",
            "We have also to take into account that resell relationship between the output functions and latent functions.",
            "So we expressed that through these covariance."
        ],
        [
            "OK, we use this model to construct the represent these several outputs and the likelihood of the model is given by this expression, which is the full covariance once we have the full Gaussian process or the covariance function.",
            "Here is KFF.",
            "We have Y server outputs and this KFF is just the full covariance matrix of the full process is with blocks given by the specific covariance between the outputs, and we saw before.",
            "Here we are allow noise term.",
            "This matrix Sigma Phi.",
            "Here are the parameters of the covariance function and the noise and we have this.",
            "Location input vectors where we evaluate these covariance function.",
            "A key issue here and the motivation.",
            "One of the motivations for this work is that when we have to learn the parameters, using this likelihood involves the inverse of this matrix, which as I said before, now grows with complexity and cube cube cube being, and the number of points for each output and being Q the number outputs."
        ],
        [
            "Wish in stop with the big amount and outputs.",
            "An observation puts points is going to be prohibitive.",
            "Anne.",
            "Using that model, the predictive distribution at some new points extar is given by this equation, where the mean is given by this expression and the covariance where this expression we see that this term here can be precomputed in the training process so that the prediction is in Q4 domain in square Q square for the variance."
        ],
        [
            "OK, so.",
            "Our strategy for approximate inference is to exploit the natural conditional dependencies in the model.",
            "So we go back again to the.",
            "Through our model of the convolution."
        ],
        [
            "We see that if we knew all the values of U for all values of say we have specific choice over kernel.",
            "Here we can know the value of F."
        ],
        [
            "So with this.",
            "Conditional independence assumption.",
            "We can say that the full set of outputs given the full set of inputs evaluated all values of, say.",
            "Can be factory side this way.",
            "So we end up saying that the outputs are independent, given that we know the conditionally independent given that we know the set of latent functions where Theta are the parameters of the kernels and covariance involved in this.",
            "Destroy."
        ],
        [
            "Fusion.",
            "Our key assumption is that we are going to say that that independence.",
            "We hold even if we have only observed M samples from their latent functions.",
            "So we have the intuition that that could be a valid approximation in the sense that if we'll put more more samples there, we're going to get closer to the full response.",
            "On the other hand, if there for the latent functions are sufficiently smooth just with a few of these samples, may be sufficient to describe the outputs.",
            "In terms of the convolution process.",
            "So we defined use samples from the latent functions KU as the cross covariance matrix between latent functions KUF as the cross covariance matrix between the latent and the output functions and the capital say as the set of input vectors, of which this latent functions are evaluated."
        ],
        [
            "We does it up so we have that the output functions are conditionally independent given the latent functions that we expressed again here, but look like here we have a discrete race version of the latent functions, so we can describe the distribution through normal distribution, which means given by this expression and covariance given by this expression, where D is a block diagonal matrix, which means that is equal to this expression in the main.",
            "Diagonal blocks and it's zero outside there.",
            "Those diagonal blocks.",
            "If we integrate all the latent functions, the marginal likelihood is given in this way.",
            "And we're going to compare both."
        ],
        [
            "Variances, so this is the expression for the full covariance.",
            "KFF and here we have the expression for the sparse approximation.",
            "So we see that we can approximate the full covariance using a low rank matrix.",
            "Plus a block diagonal matrix English the blocks.",
            "Of this matrix are going to be the covariances of each output."
        ],
        [
            "The predicted distribution is given by this expressions here or the mean.",
            "Is desperation again in the case of the full process, is the full process.",
            "We can precompute this factor here in the training process.",
            "And we have the same for the covariance function."
        ],
        [
            "OK, you know the computational required requirements of our approximation for learning the computational dim and using the calculation of this inverse, which grows as NQQ and which is related with the fact that we have to make the inverse of Q matrices of dimension N and this factor here, which is really with the multiplication in the low rank approximation.",
            "So this is the demand we have.",
            "Just one latent function.",
            "For inference."
        ],
        [
            "So we have to make this multiplication for the mean."
        ],
        [
            "We have the complexity, QM and the computation of the audience is QM Square.",
            "After sample computations.",
            "As I said, beef."
        ],
        [
            "Or which includes calculating this factor here?"
        ],
        [
            "OK, there are some remarks that we want to make the functional form well of the approximation is almost identical to the partial partially independent training conditional picks approximation which have been already introduced in the Gaussian processes machine learning framework.",
            "However, in the pizza approximation.",
            "There are some differences with respect to the approximation that we have presented here.",
            "The first one is that in the Pixie approximation is it is not obvious which variables should be together, and here there is a physical meaning of which variables to be grouped together.",
            "Variables are related with each output, and the other thing is that in the pit see inducing variables living in the same space as the observations.",
            "This is not necessarily true for our case, where we have used another letter here, say say to indicate the issue.",
            "Another important thing is that if we allow the conditional independence not only between the outputs, but within each output, we can come across with another approximation.",
            "Here the factorization is not over only over the outputs on over the observation points as well, and this leads to the fully independent training conditional approximation in which we are going to have a covariance matrix.",
            "This is not block diagonal but diagonal."
        ],
        [
            "So here we have a toy problem.",
            "We have four outputs.",
            "Q equal 4.",
            "And we generate.",
            "Some samples from this model of the we use the convolution process and there is some sample.",
            "Generate some samples.",
            "For outputs and one of the outputs, we are going to allow a missing range which goes from minus .8 to 0, and we're going to evaluate the quality of the prediction for the full Gaussian process for independent process.",
            "For the pizza approximation.",
            "For the fix approximation.",
            "We use 200 points for training and we see here for a prediction task or the full Gaussian process is able to recover the missing range, but that is not true for the independent process.",
            "And the case of the fizzie approximation this approximation.",
            "Again, we can recover the platform of the function.",
            "This crosses here are the locations of the.",
            "Latent functions."
        ],
        [
            "Here we have a table where we show the standardise mean square error.",
            "For ten runs of the toy problem.",
            "So we see that the approximations to every would job.",
            "Obtaining similar errors for the outputs."
        ],
        [
            "This is a problem with the real data, so we have this data set that had been used in the cheapest risztics literature.",
            "So.",
            "Consists of measurements of concentrations of seven heavy metals collected in the topsoil of some region in Swiss Jura.",
            "And we have a set of 359 locations where those consistent concentrations have been insured we.",
            "Take.",
            "259 locations for training.",
            "100 locations for testing and the motivation is that in some scenarios it is possible to have.",
            "Some expensive variables as I told I said before, which can be predicted using some secondary variables which are less expensive to measure in this case.",
            "We are going to predict the this metal here.",
            "This concentration of calcium using a more sample set of secondary variables an equal the sync and we're going to predict the behavior of the copper in some test locations using the full set of data for lead, nickel and see.",
            "For the case of the pizza approximation, we're going to optimize with respect to the positions or locations of where."
        ],
        [
            "Pseudo of the latent functions.",
            "So here we have the results.",
            "For commun, what we have here is the mean absolute error.",
            "For the first one is the independent Gaussian process.",
            "The second one is busy with 50 locations and so on 102 hundred 500.",
            "This is the prediction using the full Gaussian process and this is the result for a well known method in the literature known as ordinary Coke region.",
            "So we see in this case.",
            "Not even with 50 locations the bits approximation is almost equal to the.",
            "Prediction obtaining using the Queen G and every time we put more.",
            "Inputs in the latent function, we see that we approach the behavior of the full Gaussian process here.",
            "So.",
            "In the case of copper, we don't have.",
            "Such a good lock.",
            "And we have that the behavior of the full Gaussian process burden the equation prediction.",
            "But we see that even with 500 positions of the latent functions, we are far away from the prediction of the full Gaussian process."
        ],
        [
            "This is another example with real data.",
            "We have a sensor.",
            "Network data set is just weather data collected from a sensor network located in the South Coast, England.",
            "And then there were includes four sensors.",
            "Each of which measures about environment variables which include, for example H, temperature, speed of wind tide, and the motivation here is that sometimes the networking go down goes down and we would like to be able to predict the behavior of those variables using information from the variables that we already have.",
            "In this case, we have selected one of the signal sensor site signals they hide and apply the pizza approximation scheme with an independent process with square exponential kernel.",
            "We have 1000 training observations and we have 3324 for testing and this one's for training.",
            "So."
        ],
        [
            "Here in the left column we have the behavior with independent Gaussian process and in the right column we have the behavior with the PC approximation and again as in the toy problem we can see that the independent process is not able to capture the missing range in the figure.",
            "This is true for both sensors we see here that the pics approximation is able to capture the.",
            "The dependencies.",
            "Again, this crosses.",
            "Here are the locations of the latent function."
        ],
        [
            "OK, we have presented sparse approximation for multiple output Gaussian processes.",
            "That one side captures the correlated information inputs, outputs and at the same time we can really use the amount of computational for prediction and optimization purposes.",
            "The computational complexity for this approximation matches the computational complexity for modeling with independent Gaussian processes, or the problem is that the independence assumption is not able to capture those missing races that we saw before the experiments.",
            "There are some modeling issues that we have to.",
            "Study which is which is the size of the active server is going to give a good approximation and on the other hand how we can select that.",
            "Set of locations."
        ],
        [
            "OK, we think the authors of this paper, who kindly made the censored network database available."
        ],
        [
            "Thank you."
        ],
        [
            "Question.",
            "Zen.",
            "When you say the full Gaussian process using the full convolution kernel based Gaussian process, yeah, this is very near the actual question.",
            "Is some more to do with.",
            "So you mentioned you statistics is motivating example and then the linear model code visualizations the most frequently used method for building up multi output GPS.",
            "But the emphasis very much when placing prior were using prior knowledge defining the structure of the relationships between the underlying process is in this framework.",
            "How easy is it for you to build in prior knowledge?",
            "Because it seems it's quite a complex interaction between the underlying Gaussian process.",
            "Is the labor process is in the kernel.",
            "That you're using to convolute those, so is it easy for you to to take an expert and get prior knowledge in here.",
            "OK, since more tech."
        ],
        [
            "Called point of view.",
            "The problem is that we can solve in some sense we can solve this integral for a specific kernel function for a specific form of the latent process.",
            "So for example, as I said before, this is the typical model we encounter in linear systems.",
            "Actually, this is the expression of a linear time invariant system, so if we can in some sense character resize the response of a linear system, for example in terms of a differential equation, we're going to see the actual solution of the differential equation has this form.",
            "So I think that's the same thing which we can include some prior knowledge.",
            "She does on the last slide you compared with cream, but how does it actually?"
        ],
        [
            "OK, so we included that result just to compare the behavior of the convolution process against the equation, But here the kriging there is exactly the same that we have in mind is to predict the behavior of that particular variable using some secondary data.",
            "As well.",
            "What do you mean I don't get it?",
            "Creating actually does 'cause it out the same code, various assumptions and source, and if no no it said this completely different model using geostatistics literature.",
            "So.",
            "Yeah.",
            "Potentially say what kind of case with this function you are using.",
            "Yeah sorry OK for for these.",
            "For this presentation, for these experiments, we always."
        ],
        [
            "User.",
            "Gaussian kernel, Gaussian kernel for the for the smoothing kernel an for the covariance of the Gaussian process.",
            "If you guys on the type of the parameters of the kernel, yeah yeah.",
            "Spectrum active site for the moment you this actually this problem that we have to work more on that.",
            "But you know one dimensional case.",
            "You just look."
        ],
        [
            "People in these experiments.",
            "The one dimensional case just put the equally space well in high dimensional."
        ],
        [
            "Cases, for example in this data.",
            "So we initially side the positions using a clustering algorithm and then we use the marginal likelihood to optimize with respect to those.",
            "Locations.",
            "Has another person.",
            "You just mentioned the relation between this case function and sort of differential equations.",
            "But what was it again?",
            "OK.",
            "So.",
            "So."
        ],
        [
            "There are different different ways to express the behavior of a linear system.",
            "One of them is, for example using an ordinary differential equation.",
            "Another way is to use convolution process.",
            "So.",
            "When we solve if we have forget for a moment of this model, so we have differential equation to express the behavior of the linear system and we solve the differential equation.",
            "We're going to see that there is an integral involved in that, and the form of that is going in the form of that solution is similar to this one.",
            "So that looks.",
            "OK, the only the only condition that OK the only condition.",
            "If your kernel is gold.",
            "Is it true?",
            "I mean this is this is just a way to understand this framework.",
            "OK, I'm not saying that.",
            "Well, maybe it's possible that you don't get.",
            "Not every convolution corresponding.",
            "Yeah, yeah yeah, I understand your point, and actually there's two and has to be with the other question is this is really the solution of a linear dynamical system in the sense of causality in the sense of stability in all those senses that you're talking about.",
            "I understand that charges the only restrictions that we are putting here or the system is a linear time invariant.",
            "Does those are the only?",
            "Restrictions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about sparse movie local Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "This is joint work.",
                    "label": 0
                },
                {
                    "sent": "We need Lawrence.",
                    "label": 0
                },
                {
                    "sent": "This school of Computer Science, University of Manchester.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an outline of our presentation.",
                    "label": 0
                },
                {
                    "sent": "So we're going to give a perspective of the convolution process.",
                    "label": 1
                },
                {
                    "sent": "So we're going to talk about our approach for the sparse approximation, which includes some conditional independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "Some experiments on some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider the problem of modeling correlated outputs from a single Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "Usually this is difficult because we are required to compute not only covariances for each output, but cross covariances between outputs, and those covariances must be such that they must be expressive in the sense that they capture dependencies and correlations between outputs for one side and the other side.",
                    "label": 1
                },
                {
                    "sent": "They must lead to a positive definite matrix for the full Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "There are many applications on these, for example in just risztics.",
                    "label": 0
                },
                {
                    "sent": "Literature where usually you have.",
                    "label": 0
                },
                {
                    "sent": "Civil variables, but you have more emissions of 1 variable than the other.",
                    "label": 0
                },
                {
                    "sent": "Usually the one that's less expensive to measure and you want to use those measures to find to try to predict the most expensive ones.",
                    "label": 0
                },
                {
                    "sent": "There's one of the applications.",
                    "label": 0
                },
                {
                    "sent": "One myth.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use for this kind of modeling task employees the convolution process.",
                    "label": 1
                },
                {
                    "sent": "So convolution process is just moving average constructions, guaranteed symbolic covariance function.",
                    "label": 0
                },
                {
                    "sent": "The problem is that when we construct the full Gaussian process using this approach with another with the matrix which goes as Ncube being and the number of points QQB&Q, the number outputs.",
                    "label": 0
                },
                {
                    "sent": "So where we going to do is to introduce sparse approximation for the full covariance, exploiting the conditional independence assumption that it's.",
                    "label": 1
                },
                {
                    "sent": "An implicit in the convolution process framework.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about little about the compilation, the convolution process.",
                    "label": 0
                },
                {
                    "sent": "So we consider a set of functions FQ and we have capital Q of those functions and function can be expressed about.",
                    "label": 1
                },
                {
                    "sent": "The function can be expressed using this generation mechanism, which consists in just solving this integral where KQ is a smoothing kernel function and you is a latent process or latent function.",
                    "label": 0
                },
                {
                    "sent": "Now this is typical form in linear systems.",
                    "label": 0
                },
                {
                    "sent": "For example, where this K function here.",
                    "label": 0
                },
                {
                    "sent": "Smoothing kernel here usually is the impulse responses with system.",
                    "label": 0
                },
                {
                    "sent": "And this latent function is just the input to the system.",
                    "label": 0
                },
                {
                    "sent": "This is more fun.",
                    "label": 0
                },
                {
                    "sent": "Function is related, for example with the scription in terms of differential equation of the system.",
                    "label": 0
                },
                {
                    "sent": "We can allow the influence of more than one latent function.",
                    "label": 1
                },
                {
                    "sent": "For example, our functions and include an independent process, so our final output constructed with this mechanism is just the sum of the process due to the.",
                    "label": 0
                },
                {
                    "sent": "Convolution process an independent process WQ?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a diagram of how do we construct the outputs.",
                    "label": 0
                },
                {
                    "sent": "So we have this latent function.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we make the convolution between this latent function and smoothing kernels.",
                    "label": 0
                },
                {
                    "sent": "Which is the sole reason Teague?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We end up with this output functions.",
                    "label": 0
                },
                {
                    "sent": "Which are correlated to they are both generated by the same process.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we add an independent process and we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, with the outputs we want to model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since.",
                    "label": 0
                },
                {
                    "sent": "OK, if we assume.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does the latent functions are Gaussian processes and the two this is a linear operation.",
                    "label": 0
                },
                {
                    "sent": "The output is going to be a Gaussian process as well.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the covariance function for the outputs is giving us the sum of the covariance of the functions into the convolution process plus our auto covariance due to the independent process an.",
                    "label": 1
                },
                {
                    "sent": "This covariance is expressed in this way.",
                    "label": 0
                },
                {
                    "sent": "We see here a double sum and a double integral, which implicates the smoothing kernel.",
                    "label": 0
                },
                {
                    "sent": "And the covariance important, the covariance of the latent functions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Depending on the form of these covariance, are the latent function that we assume for the process.",
                    "label": 0
                },
                {
                    "sent": "We have different different forms for the actual covariance of the outputs.",
                    "label": 0
                },
                {
                    "sent": "In the past, many literature for constructing special process is common.",
                    "label": 0
                },
                {
                    "sent": "Option is to assume that the latent functions white noise process.",
                    "label": 1
                },
                {
                    "sent": "So if you assume that this variance, these are ways processes, this is a Delta function.",
                    "label": 0
                },
                {
                    "sent": "And we can eliminate one of the integrals of one of these sums.",
                    "label": 0
                },
                {
                    "sent": "This should be the form of the covariance of the outputs of the convolution process.",
                    "label": 1
                },
                {
                    "sent": "You know, work.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that those processes are independent Gaussian processes, so.",
                    "label": 0
                },
                {
                    "sent": "The independence assumption of the Gaussian processes allow us to eliminate one of the songs and we get this expression for the covariance of the functions constructed with this convolution process.",
                    "label": 0
                },
                {
                    "sent": "And then we have two.",
                    "label": 1
                },
                {
                    "sent": "We have also to take into account that resell relationship between the output functions and latent functions.",
                    "label": 0
                },
                {
                    "sent": "So we expressed that through these covariance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we use this model to construct the represent these several outputs and the likelihood of the model is given by this expression, which is the full covariance once we have the full Gaussian process or the covariance function.",
                    "label": 1
                },
                {
                    "sent": "Here is KFF.",
                    "label": 0
                },
                {
                    "sent": "We have Y server outputs and this KFF is just the full covariance matrix of the full process is with blocks given by the specific covariance between the outputs, and we saw before.",
                    "label": 0
                },
                {
                    "sent": "Here we are allow noise term.",
                    "label": 0
                },
                {
                    "sent": "This matrix Sigma Phi.",
                    "label": 0
                },
                {
                    "sent": "Here are the parameters of the covariance function and the noise and we have this.",
                    "label": 0
                },
                {
                    "sent": "Location input vectors where we evaluate these covariance function.",
                    "label": 0
                },
                {
                    "sent": "A key issue here and the motivation.",
                    "label": 0
                },
                {
                    "sent": "One of the motivations for this work is that when we have to learn the parameters, using this likelihood involves the inverse of this matrix, which as I said before, now grows with complexity and cube cube cube being, and the number of points for each output and being Q the number outputs.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wish in stop with the big amount and outputs.",
                    "label": 0
                },
                {
                    "sent": "An observation puts points is going to be prohibitive.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Using that model, the predictive distribution at some new points extar is given by this equation, where the mean is given by this expression and the covariance where this expression we see that this term here can be precomputed in the training process so that the prediction is in Q4 domain in square Q square for the variance.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Our strategy for approximate inference is to exploit the natural conditional dependencies in the model.",
                    "label": 1
                },
                {
                    "sent": "So we go back again to the.",
                    "label": 1
                },
                {
                    "sent": "Through our model of the convolution.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see that if we knew all the values of U for all values of say we have specific choice over kernel.",
                    "label": 0
                },
                {
                    "sent": "Here we can know the value of F.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this.",
                    "label": 0
                },
                {
                    "sent": "Conditional independence assumption.",
                    "label": 0
                },
                {
                    "sent": "We can say that the full set of outputs given the full set of inputs evaluated all values of, say.",
                    "label": 0
                },
                {
                    "sent": "Can be factory side this way.",
                    "label": 0
                },
                {
                    "sent": "So we end up saying that the outputs are independent, given that we know the conditionally independent given that we know the set of latent functions where Theta are the parameters of the kernels and covariance involved in this.",
                    "label": 1
                },
                {
                    "sent": "Destroy.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fusion.",
                    "label": 0
                },
                {
                    "sent": "Our key assumption is that we are going to say that that independence.",
                    "label": 0
                },
                {
                    "sent": "We hold even if we have only observed M samples from their latent functions.",
                    "label": 1
                },
                {
                    "sent": "So we have the intuition that that could be a valid approximation in the sense that if we'll put more more samples there, we're going to get closer to the full response.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if there for the latent functions are sufficiently smooth just with a few of these samples, may be sufficient to describe the outputs.",
                    "label": 0
                },
                {
                    "sent": "In terms of the convolution process.",
                    "label": 0
                },
                {
                    "sent": "So we defined use samples from the latent functions KU as the cross covariance matrix between latent functions KUF as the cross covariance matrix between the latent and the output functions and the capital say as the set of input vectors, of which this latent functions are evaluated.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We does it up so we have that the output functions are conditionally independent given the latent functions that we expressed again here, but look like here we have a discrete race version of the latent functions, so we can describe the distribution through normal distribution, which means given by this expression and covariance given by this expression, where D is a block diagonal matrix, which means that is equal to this expression in the main.",
                    "label": 1
                },
                {
                    "sent": "Diagonal blocks and it's zero outside there.",
                    "label": 0
                },
                {
                    "sent": "Those diagonal blocks.",
                    "label": 0
                },
                {
                    "sent": "If we integrate all the latent functions, the marginal likelihood is given in this way.",
                    "label": 1
                },
                {
                    "sent": "And we're going to compare both.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variances, so this is the expression for the full covariance.",
                    "label": 0
                },
                {
                    "sent": "KFF and here we have the expression for the sparse approximation.",
                    "label": 1
                },
                {
                    "sent": "So we see that we can approximate the full covariance using a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Plus a block diagonal matrix English the blocks.",
                    "label": 0
                },
                {
                    "sent": "Of this matrix are going to be the covariances of each output.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The predicted distribution is given by this expressions here or the mean.",
                    "label": 0
                },
                {
                    "sent": "Is desperation again in the case of the full process, is the full process.",
                    "label": 0
                },
                {
                    "sent": "We can precompute this factor here in the training process.",
                    "label": 0
                },
                {
                    "sent": "And we have the same for the covariance function.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, you know the computational required requirements of our approximation for learning the computational dim and using the calculation of this inverse, which grows as NQQ and which is related with the fact that we have to make the inverse of Q matrices of dimension N and this factor here, which is really with the multiplication in the low rank approximation.",
                    "label": 1
                },
                {
                    "sent": "So this is the demand we have.",
                    "label": 0
                },
                {
                    "sent": "Just one latent function.",
                    "label": 0
                },
                {
                    "sent": "For inference.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have to make this multiplication for the mean.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have the complexity, QM and the computation of the audience is QM Square.",
                    "label": 1
                },
                {
                    "sent": "After sample computations.",
                    "label": 0
                },
                {
                    "sent": "As I said, beef.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or which includes calculating this factor here?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there are some remarks that we want to make the functional form well of the approximation is almost identical to the partial partially independent training conditional picks approximation which have been already introduced in the Gaussian processes machine learning framework.",
                    "label": 1
                },
                {
                    "sent": "However, in the pizza approximation.",
                    "label": 0
                },
                {
                    "sent": "There are some differences with respect to the approximation that we have presented here.",
                    "label": 1
                },
                {
                    "sent": "The first one is that in the Pixie approximation is it is not obvious which variables should be together, and here there is a physical meaning of which variables to be grouped together.",
                    "label": 0
                },
                {
                    "sent": "Variables are related with each output, and the other thing is that in the pit see inducing variables living in the same space as the observations.",
                    "label": 0
                },
                {
                    "sent": "This is not necessarily true for our case, where we have used another letter here, say say to indicate the issue.",
                    "label": 0
                },
                {
                    "sent": "Another important thing is that if we allow the conditional independence not only between the outputs, but within each output, we can come across with another approximation.",
                    "label": 0
                },
                {
                    "sent": "Here the factorization is not over only over the outputs on over the observation points as well, and this leads to the fully independent training conditional approximation in which we are going to have a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "This is not block diagonal but diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we have a toy problem.",
                    "label": 1
                },
                {
                    "sent": "We have four outputs.",
                    "label": 0
                },
                {
                    "sent": "Q equal 4.",
                    "label": 0
                },
                {
                    "sent": "And we generate.",
                    "label": 0
                },
                {
                    "sent": "Some samples from this model of the we use the convolution process and there is some sample.",
                    "label": 1
                },
                {
                    "sent": "Generate some samples.",
                    "label": 0
                },
                {
                    "sent": "For outputs and one of the outputs, we are going to allow a missing range which goes from minus .8 to 0, and we're going to evaluate the quality of the prediction for the full Gaussian process for independent process.",
                    "label": 0
                },
                {
                    "sent": "For the pizza approximation.",
                    "label": 0
                },
                {
                    "sent": "For the fix approximation.",
                    "label": 0
                },
                {
                    "sent": "We use 200 points for training and we see here for a prediction task or the full Gaussian process is able to recover the missing range, but that is not true for the independent process.",
                    "label": 0
                },
                {
                    "sent": "And the case of the fizzie approximation this approximation.",
                    "label": 0
                },
                {
                    "sent": "Again, we can recover the platform of the function.",
                    "label": 0
                },
                {
                    "sent": "This crosses here are the locations of the.",
                    "label": 0
                },
                {
                    "sent": "Latent functions.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we have a table where we show the standardise mean square error.",
                    "label": 1
                },
                {
                    "sent": "For ten runs of the toy problem.",
                    "label": 1
                },
                {
                    "sent": "So we see that the approximations to every would job.",
                    "label": 0
                },
                {
                    "sent": "Obtaining similar errors for the outputs.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a problem with the real data, so we have this data set that had been used in the cheapest risztics literature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Consists of measurements of concentrations of seven heavy metals collected in the topsoil of some region in Swiss Jura.",
                    "label": 1
                },
                {
                    "sent": "And we have a set of 359 locations where those consistent concentrations have been insured we.",
                    "label": 0
                },
                {
                    "sent": "Take.",
                    "label": 0
                },
                {
                    "sent": "259 locations for training.",
                    "label": 0
                },
                {
                    "sent": "100 locations for testing and the motivation is that in some scenarios it is possible to have.",
                    "label": 0
                },
                {
                    "sent": "Some expensive variables as I told I said before, which can be predicted using some secondary variables which are less expensive to measure in this case.",
                    "label": 0
                },
                {
                    "sent": "We are going to predict the this metal here.",
                    "label": 0
                },
                {
                    "sent": "This concentration of calcium using a more sample set of secondary variables an equal the sync and we're going to predict the behavior of the copper in some test locations using the full set of data for lead, nickel and see.",
                    "label": 0
                },
                {
                    "sent": "For the case of the pizza approximation, we're going to optimize with respect to the positions or locations of where.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pseudo of the latent functions.",
                    "label": 0
                },
                {
                    "sent": "So here we have the results.",
                    "label": 0
                },
                {
                    "sent": "For commun, what we have here is the mean absolute error.",
                    "label": 1
                },
                {
                    "sent": "For the first one is the independent Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "The second one is busy with 50 locations and so on 102 hundred 500.",
                    "label": 0
                },
                {
                    "sent": "This is the prediction using the full Gaussian process and this is the result for a well known method in the literature known as ordinary Coke region.",
                    "label": 0
                },
                {
                    "sent": "So we see in this case.",
                    "label": 0
                },
                {
                    "sent": "Not even with 50 locations the bits approximation is almost equal to the.",
                    "label": 0
                },
                {
                    "sent": "Prediction obtaining using the Queen G and every time we put more.",
                    "label": 0
                },
                {
                    "sent": "Inputs in the latent function, we see that we approach the behavior of the full Gaussian process here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the case of copper, we don't have.",
                    "label": 0
                },
                {
                    "sent": "Such a good lock.",
                    "label": 0
                },
                {
                    "sent": "And we have that the behavior of the full Gaussian process burden the equation prediction.",
                    "label": 0
                },
                {
                    "sent": "But we see that even with 500 positions of the latent functions, we are far away from the prediction of the full Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is another example with real data.",
                    "label": 0
                },
                {
                    "sent": "We have a sensor.",
                    "label": 0
                },
                {
                    "sent": "Network data set is just weather data collected from a sensor network located in the South Coast, England.",
                    "label": 1
                },
                {
                    "sent": "And then there were includes four sensors.",
                    "label": 0
                },
                {
                    "sent": "Each of which measures about environment variables which include, for example H, temperature, speed of wind tide, and the motivation here is that sometimes the networking go down goes down and we would like to be able to predict the behavior of those variables using information from the variables that we already have.",
                    "label": 1
                },
                {
                    "sent": "In this case, we have selected one of the signal sensor site signals they hide and apply the pizza approximation scheme with an independent process with square exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "We have 1000 training observations and we have 3324 for testing and this one's for training.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here in the left column we have the behavior with independent Gaussian process and in the right column we have the behavior with the PC approximation and again as in the toy problem we can see that the independent process is not able to capture the missing range in the figure.",
                    "label": 0
                },
                {
                    "sent": "This is true for both sensors we see here that the pics approximation is able to capture the.",
                    "label": 0
                },
                {
                    "sent": "The dependencies.",
                    "label": 0
                },
                {
                    "sent": "Again, this crosses.",
                    "label": 0
                },
                {
                    "sent": "Here are the locations of the latent function.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we have presented sparse approximation for multiple output Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "That one side captures the correlated information inputs, outputs and at the same time we can really use the amount of computational for prediction and optimization purposes.",
                    "label": 0
                },
                {
                    "sent": "The computational complexity for this approximation matches the computational complexity for modeling with independent Gaussian processes, or the problem is that the independence assumption is not able to capture those missing races that we saw before the experiments.",
                    "label": 1
                },
                {
                    "sent": "There are some modeling issues that we have to.",
                    "label": 0
                },
                {
                    "sent": "Study which is which is the size of the active server is going to give a good approximation and on the other hand how we can select that.",
                    "label": 0
                },
                {
                    "sent": "Set of locations.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we think the authors of this paper, who kindly made the censored network database available.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Zen.",
                    "label": 0
                },
                {
                    "sent": "When you say the full Gaussian process using the full convolution kernel based Gaussian process, yeah, this is very near the actual question.",
                    "label": 0
                },
                {
                    "sent": "Is some more to do with.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned you statistics is motivating example and then the linear model code visualizations the most frequently used method for building up multi output GPS.",
                    "label": 0
                },
                {
                    "sent": "But the emphasis very much when placing prior were using prior knowledge defining the structure of the relationships between the underlying process is in this framework.",
                    "label": 0
                },
                {
                    "sent": "How easy is it for you to build in prior knowledge?",
                    "label": 0
                },
                {
                    "sent": "Because it seems it's quite a complex interaction between the underlying Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Is the labor process is in the kernel.",
                    "label": 0
                },
                {
                    "sent": "That you're using to convolute those, so is it easy for you to to take an expert and get prior knowledge in here.",
                    "label": 0
                },
                {
                    "sent": "OK, since more tech.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Called point of view.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we can solve in some sense we can solve this integral for a specific kernel function for a specific form of the latent process.",
                    "label": 0
                },
                {
                    "sent": "So for example, as I said before, this is the typical model we encounter in linear systems.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is the expression of a linear time invariant system, so if we can in some sense character resize the response of a linear system, for example in terms of a differential equation, we're going to see the actual solution of the differential equation has this form.",
                    "label": 0
                },
                {
                    "sent": "So I think that's the same thing which we can include some prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "She does on the last slide you compared with cream, but how does it actually?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we included that result just to compare the behavior of the convolution process against the equation, But here the kriging there is exactly the same that we have in mind is to predict the behavior of that particular variable using some secondary data.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "What do you mean I don't get it?",
                    "label": 0
                },
                {
                    "sent": "Creating actually does 'cause it out the same code, various assumptions and source, and if no no it said this completely different model using geostatistics literature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Potentially say what kind of case with this function you are using.",
                    "label": 0
                },
                {
                    "sent": "Yeah sorry OK for for these.",
                    "label": 0
                },
                {
                    "sent": "For this presentation, for these experiments, we always.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "User.",
                    "label": 0
                },
                {
                    "sent": "Gaussian kernel, Gaussian kernel for the for the smoothing kernel an for the covariance of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "If you guys on the type of the parameters of the kernel, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Spectrum active site for the moment you this actually this problem that we have to work more on that.",
                    "label": 0
                },
                {
                    "sent": "But you know one dimensional case.",
                    "label": 0
                },
                {
                    "sent": "You just look.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People in these experiments.",
                    "label": 0
                },
                {
                    "sent": "The one dimensional case just put the equally space well in high dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cases, for example in this data.",
                    "label": 0
                },
                {
                    "sent": "So we initially side the positions using a clustering algorithm and then we use the marginal likelihood to optimize with respect to those.",
                    "label": 0
                },
                {
                    "sent": "Locations.",
                    "label": 0
                },
                {
                    "sent": "Has another person.",
                    "label": 0
                },
                {
                    "sent": "You just mentioned the relation between this case function and sort of differential equations.",
                    "label": 0
                },
                {
                    "sent": "But what was it again?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are different different ways to express the behavior of a linear system.",
                    "label": 0
                },
                {
                    "sent": "One of them is, for example using an ordinary differential equation.",
                    "label": 0
                },
                {
                    "sent": "Another way is to use convolution process.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When we solve if we have forget for a moment of this model, so we have differential equation to express the behavior of the linear system and we solve the differential equation.",
                    "label": 0
                },
                {
                    "sent": "We're going to see that there is an integral involved in that, and the form of that is going in the form of that solution is similar to this one.",
                    "label": 0
                },
                {
                    "sent": "So that looks.",
                    "label": 0
                },
                {
                    "sent": "OK, the only the only condition that OK the only condition.",
                    "label": 0
                },
                {
                    "sent": "If your kernel is gold.",
                    "label": 0
                },
                {
                    "sent": "Is it true?",
                    "label": 1
                },
                {
                    "sent": "I mean this is this is just a way to understand this framework.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not saying that.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe it's possible that you don't get.",
                    "label": 0
                },
                {
                    "sent": "Not every convolution corresponding.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah, I understand your point, and actually there's two and has to be with the other question is this is really the solution of a linear dynamical system in the sense of causality in the sense of stability in all those senses that you're talking about.",
                    "label": 0
                },
                {
                    "sent": "I understand that charges the only restrictions that we are putting here or the system is a linear time invariant.",
                    "label": 0
                },
                {
                    "sent": "Does those are the only?",
                    "label": 0
                },
                {
                    "sent": "Restrictions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}