{
    "id": "ssocbw3za63ywkcsedskky2lem2wogmk",
    "title": "Informal sentiment analysis in multiple domains for English and Spanish",
    "info": {
        "author": [
            "Tadej \u0160tajner, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 16, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/is2012_stajner_sentiment_analysis/",
    "segmentation": [
        [
            "I'll talk about expanding on the already quite well researched sentiment analysis problem and comparing how this problem kind of comes up in different domains and in different languages.",
            "And what are the differences here?",
            "And above everything, how to best model this?",
            "In a way that works best for all these setting settings."
        ],
        [
            "So just for those of you who aren't familiar with exactly this is a topic of study.",
            "That's about opinions, sentiment, evaluations, emotions, and so on.",
            "So basically different things that people use to relate themselves to other things around them.",
            "Also other people.",
            "And there's also cinnamon synonym for this called opinion mining, which you some of you might have seen in other domains."
        ],
        [
            "Button then.",
            "That's because opinions are the things that kind of drive a lot of our behaviors.",
            "So a lot of stuff that we're doing we're doing because we've seen other people express positive or negative emotion or sentiment towards some particular thing.",
            "And this is a.",
            "So if you want to know how to influence others, you should follow this presentation.",
            "Because this is kind of a big decision making criterion and analyzing this in how people are expressing themselves can be really useful to predict how people will react in the future."
        ],
        [
            "So, and where can we find this?",
            "So of course, every other things can be found on the web.",
            "Or via word of mouth.",
            "So like people, privately conversing with each other.",
            "But would the interesting part for us is basically what we can gather.",
            "So for us, interesting stuff was.",
            "Found in social media, where people are pretty relaxed about expressing themselves and also in a bit more domain specific areas like products and movie reviews such as Epinions, IMDb, Rotten Tomatoes and such.",
            "There is also some sentiment in news, but it's a bit more subtle.",
            "And of course also internal data for various business domains for, for instance for customer feedback.",
            "So every decent business has a book of complaints somewhere.",
            "And that could be quite interesting to see.",
            "And one question is here, is that do these domains kind of act differently?",
            "So do they have different indicators?",
            "Or what are the kind of characteristics?"
        ],
        [
            "So just to go quickly through some work that this was done in this area.",
            "So initially this was done from new movie reviews because this was a quite an easy topic.",
            "You had positive and negative examples.",
            "They had scores attached and the domain was really straightforward and this was kind of the Seminole work here and.",
            "Indians this works well and is more or less a solved problem, but it turns out that in other domains you should first separate what is actually an opinion and what is just some statement because not every statement is also an opinion.",
            "Maybe movie reviews.",
            "It is because it's a review in the end, but that's definitely not true in general and all domains have wildly different distributions about.",
            "So what is subjective objective and what is actually positive or negative?",
            "So it turns out that solving this first and then figuring out what the positive and negative polarities can be more important than just going positive or negative.",
            "But we'll see how this turns out in our experiments."
        ],
        [
            "So it's and also since it's a quite clean text classification problem, in the end it's quite interesting area of research for also machine learning application.",
            "Is so the main problem of having really domain specific classifiers has also been.",
            "Tackles quite successfully nowadays, and it's also being.",
            "It also has some quite good results in applying deep learning to this.",
            "OK, but All in all, most approaches that are used in the wild nowadays.",
            "They usually encode domain knowledge in some lexicons, so that's kind of still knowledge from some existing source which assigns a unique, globally valid number, or in a class for each word that appear.",
            "So for instance, the word good is almost almost always somewhat positive.",
            "This works OK. You don't get perfect performance with this big cause.",
            "A lot of words are context sensitive, so if something is heavy, that might not be a good or bad thing.",
            "Right, so if some for instance some phone is like that might be a good thing, but if some for instance.",
            "Something with example that is inappropriate.",
            "Anyway, I had this the same one, but I forgot what was the other thing.",
            "Anyway, things are not context insensitive.",
            "There are quite some good lexical sources for this."
        ],
        [
            "Just kind of a quick snapshot.",
            "How is this problem actually formulated in general so it's an opinion.",
            "Is a quintuplets 05 points or one is Holder who has the opinion?",
            "Target is about what aspect about which property orientation is positive, negative mostly or also it could be gradient in gradient and also when was this opinion stated?",
            "So this can be also interesting playgrounds for some other problems.",
            "For instance holders and targets sometimes have to be extracted from the content itself, and aspect extraction is also a whole new area of research.",
            "OK, for simplification here will mostly talk about orientation here, and we're also dealing with Holder in target extraction, But this we treat this as a preprocessing step.",
            "So basically.",
            "What we do is."
        ],
        [
            "We replace the Holder in the target with some like OPEC.",
            "Kind of, for instance, we save up person, love that and we do.",
            "Actually instead of having the persons name, there is a feature.",
            "We actually said this is just a person or a Holder.",
            "So it's a bit cleaned up for further learning.",
            "So yes, this is the actual problem.",
            "We have objective, negative and positive.",
            "This is the simplified version, so we don't at first consider doing this in the settings.",
            "I'll explain later why.",
            "So it's basically a three class classification problem."
        ],
        [
            "And there we have some hypothesis here which would like to think about and evaluate.",
            "So one is, can we use external sources of information to increase performance or various lexicons?",
            "Various regular expressions, patterns and stuff like that?",
            "Things that we know that should have some information and may not be explicitly present in the data.",
            "So what's the best way to encode is so there's a different ways to model this as features and kind of how to actually incorporate it in a learning setting.",
            "Then when we have lexicons which work best for what domain, and of course in the end kind of the descriptive thing, what are the differences across domains and languages?"
        ],
        [
            "Just to go through the datasets here so we have two languages, Spanish, English and three domains.",
            "So new social media and movie reviews.",
            "So this was.",
            "1st for instance, go through the news domain.",
            "We use the JRC new sentiment corpus.",
            "It's around 2000, something 1200 something examples.",
            "This was originally done for English, but we have a translation done by Bing Translate for in Spanish and it's it's not a bad thing to do actually, at least for news it not not a lot is lost.",
            "We have the classic reviews data set from both Pang and leading Lee.",
            "This is the kernel data set, which is really often cited.",
            "And we also constructed some of our own datasets for social media for specific domains of telecommunication providers and how the people expressed the sentiment about those.",
            "So they are a bit different in size.",
            "Some are smaller, so some of them only have two classes that actually don't have any neutral examples.",
            "And the class distributions are also a bit varied.",
            "For instance, the new datasets has mostly objective statements.",
            "Whereas this one basically one has everything subjective and it's a bit different in others, but it's quite OK balanced."
        ],
        [
            "So it's not doesn't have any strict, really rare occurrences.",
            "So to represent all this data.",
            "So how to?",
            "What do we use to detect this?",
            "We have three main sources, so first is the content.",
            "So this is the baseline Model S. So if you have a.",
            "Piece of text.",
            "If you do this.",
            "Tokenization, preprocessing, stemming, lemmatization, stopword removal, and the classic stuff so.",
            "Here we actually Additionally perform removal of numbers.",
            "Removal of.",
            "Targets opinion holders plus removal of URLs.",
            "I mean, this is simply for cleaning up stuff for social media.",
            "And people got also really good results by cleaning up misspellings, so this was actually.",
            "Cleaning up data is brings a much bigger performance improvement than actually any other algorithm.",
            "Secondly, we use the sentiment lexicons which I mentioned earlier.",
            "This is a global list of mapping a word to some sentiment score.",
            "And also some surface patterns which are basically just saying how are things expressed that people are repeating vowels screaming using caps lock, lots of punctuation's, emoticons, smileys and so on.",
            "So we also have some pattern which are kind of non.",
            "Non lexical, but we it's kind of how words are actually expressed then."
        ],
        [
            "Today for the new spelling corrections, is this something that we're particularly well in the social media setting?",
            "Yes, we have a lot of them.",
            "Or yes, it's actually.",
            "I would also say that for Spanish you have more than than four English.",
            "Maybe it's the English that is from Ireland, so we'll see if that makes a difference.",
            "But yeah, some some parts.",
            "Some cultures actually have really well established slang for social media.",
            "I'm not sure how deep this goes, but yes, for English and for some domains on social media, cleaning up with simple spelling corrections worked wonders.",
            "No algorithm, language modeling and character modeling and so on.",
            "So this and some simple dictionary rules.",
            "OK, so OK you can see the water the presence feature.",
            "So mostly standard thing except for the masking and also ask normalization.",
            "This was quite important for Spanish because Spanish has a bit extra characters, but they're not consistently used, so some people actually use the ASK equivalent.",
            "So if you have a with this mark on top then just say in the end.",
            "Um?"
        ],
        [
            "OK force Lexicon features.",
            "So this is the second part.",
            "When we look at the sentiment lexicons, we take a look at every word.",
            "If it's in there and then, we generate like a sum of all scores in the in the.",
            "Example, some of the absolute scores, so that's kind of gives us this opinionated Ness.",
            "Then the ratio of positive to negative and all of the above for every individual simplified part of speech.",
            "For instance, what is the sum of scores for all nouns?",
            "And that's one feature here.",
            "So this is kind of a.",
            "Way to to generate an aggregate score for the whole example, given that you only have individual words that are detected."
        ],
        [
            "Some words about the lexicons that we used, so there are several existing ones.",
            "English, Spanish and we developed some new ones, so this is what inner contributed alot here.",
            "So we call this render Lex and render Lex links and there is a project that is being worked for and the links part is interesting 'cause we also include them.",
            "The links of that.",
            "So the Co occurrence links between words and known sentiment words.",
            "So for instance, if a word Co occurs with some other work which we know that is positive, we say this is a positive link, and if it Co occurs with a negative word but in a contrast setting, so this is.",
            "Good but slow and then we say OK.",
            "If this is a bad slow that means that good is a good thing.",
            "If so, is is a bad thing.",
            "So basically we tried to generate features about how these things Co occur.",
            "And."
        ],
        [
            "So for the surface features, or basically, how should I say this character patterns?",
            "These are the ones that we can take a look at, so looking at capitalization of individual words in digital characters, proportions of how many things are repeated.",
            "Also, if negation or contrast words are used emoticons, we actually have special lexicons for English and Spanish because they use totally different emoticons.",
            "And of course curse words and also punctuation is also quite a big.",
            "Indicator.",
            "Some go, you see in the end, but just to profanity, is not used a lot.",
            "It exists, but not enough to be picked up as a big feature."
        ],
        [
            "Um?",
            "I mean.",
            "Well, that it's in the models.",
            "It didn't pick up with a big weight, so it's like one of the small signals, but it doesn't make a big difference.",
            "Also in the score it didn't.",
            "Kind of contribute much, but included because I saw some people cursing in the data so.",
            "Let's so OK, this is 1 hypothesis here.",
            "How should we model this?",
            "So bag of words representation is usually really sparse, so we have an example which has like 50 words maximum and then.",
            "More or less in a space of like 100,000 dimensions, and that usually works OK for most models.",
            "But we also wanted to experiment whether these features, which are basically more like Co occurrence occurrence and TF IDF based, play well with these which are have a bit of a different scale.",
            "So these are proportional and indicator features and so on.",
            "And besides, trying just a simple way to concatenate all these things three we tried to do first intermediate model which first used only bag of words to predict what the sentiment and then use this prediction as an input feature for the final classifier along with lexical and surface features.",
            "So this was kind of a two step like 2 level pipeline in away and the idea was that this might kind of clean up something that.",
            "Maybe show up.",
            "It works actually.",
            "Separating these two further didn't really help a lot, maybe because I think it's more it has to do something with distributions and sparsity I guess.",
            "I didn't get too deep into this, but I just report on what worked."
        ],
        [
            "So OK, just to say what the setup here.",
            "So you have these three feature classes.",
            "We have various combinations of this evaluated and we have, so we valid models on this word plus features 2 levels think to level combination and also using feature scaling.",
            "So scaling the features to have a mean of zero and variance of 1.",
            "This reportedly worked well for some SVM problems, especially when you do stochastic learning.",
            "And we'll see if it works."
        ],
        [
            "So for news data, there's not much to see here.",
            "Becausw the baseline back before test VM was the best Model S nothing else actually came better.",
            "Things are kind of equivalent statistically, but not much is changed on a good note, this is better than what the people who released the datasets had.",
            "Like for three points or so, so I think it was in the same was also for English, more or less.",
            "Um?"
        ],
        [
            "Maybe this one is actually more interesting.",
            "Here we got better results by including this letter.",
            "OK, sure.",
            "You can look here.",
            "And we hear there wasn't a big difference, but it sounded, including all features here gave us a bit of a boost.",
            "Compared to other things, so one thing which is obvious not having a bag of words is a pretty bad idea because you actually lose like huge amount of information there.",
            "Lexicons themselves give some information, but it's hard to tell which one would be better.",
            "So we have some domain specific ones, and some general ones, and they all kind of contribute something.",
            "The interesting part is that if we use all of them, we don't actually get worse results, so this was quite shocking.",
            "London, so I guess, but it doesn't see I think the sparsity so, but this is social media.",
            "Social media.",
            "This is like it's also small data set is like eight 900 examples.",
            "So so even I was surprised that bag of words gave me anything actually.",
            "But it also shows that this combination model with.",
            "Two level features actually gives like a slight edge above the SVM one and also in other examples.",
            "So this is data set is on tweets on services and products from big telephone.",
            "Yeah, so my phone doesn't work.",
            "So I bought an iPhone and now it's cheaper so.",
            "And stuff like that, so yes."
        ],
        [
            "OK, just quit.",
            "This, uh, this one.",
            "So there are things that on news we don't do any better on social media we do better."
        ],
        [
            "Maybe we can go to the English part.",
            "So here we we didn't really introduce bag of words.",
            "Still basically the best thing, more or less.",
            "So the difference here.",
            "It wasn't that significant actually.",
            "But it shows that this combination and feature scaling really was kind of in general a good thing to do here, although the best result still came from just having a bag of words.",
            "And actually this.",
            "This one here is not that much below what was actually state of the art.",
            "Using really sophisticated parsers and stuff like that.",
            "So I think above .91 is what people get with really heavy machinery, so doing something pretty much which is tokenization more or less and getting that Paris was quite OK I guess.",
            "Come from this rule based.",
            "No, but they they have a specific.",
            "Something really complex involving parsers and not rule based.",
            "It's learned based, still learning, so this is more less people do machine learning on this one more.",
            "I actually don't remember what was the exact approach.",
            "The deep learning one got also around 91, so that one is also pattern based.",
            "And didn't use any of this parsing rules, but.",
            "The point the point I'm trying to make the yeah so comes from parsing more or less so.",
            "The data set is not that hard to maximize I guess, so it's easy task in the end and this one is 2002.",
            "So lot of people actually try this one and most everyone got basically up there somewhere."
        ],
        [
            "On the news data.",
            "So for the.",
            "For the English part of the news, surprisingly, we actually got some benefit out of using lexicons, so Surface features didn't bring up anything.",
            "Cause I mean certain features aren't used to express sentiment in use.",
            "People use!",
            "Marks really rarely.",
            "There mostly dots dashes are used where grammar says they should be used and people don't really scream a lot there.",
            "And I haven't seen any curse words here, so.",
            "But yeah, lexicons do help a bit cause the data set is still not that big.",
            "1300 examples and it shows."
        ],
        [
            "And for the social media data set, this one was actually pretty small.",
            "So here the the benefit of using lexical huge and actually using bag of words wasn't that good.",
            "So I think it was that was in some other experiments so.",
            "Here, because it's a small data set, we don't have a lot of to learn from, so we can actually benefit from having a generalized representation in the form of lexicons.",
            "So actually here it also helps to have this step version, so just having concatenation really not that impressive.",
            "And also just to clarify, so this is SVM, multinomial naive Bayes, SVM.",
            "With these two levels and two level plus scaling.",
            "So scaling helps a bit here, but not that I would be too happy about it.",
            "So."
        ],
        [
            "OK, just to summarize this one and kind of conclude because I think I'm almost out of time.",
            "Oh, so reviews are pretty hard.",
            "I think this one was for Spanish, so this is wrong.",
            "The lexicons in proof.",
            "Two layers help for.",
            "Especially for social media.",
            "And the lexicons which."
        ],
        [
            "So earlier here, so the links, parts and their other lexicons don't show a big difference, except for this one, which is domain specific, which works much better than the one which is general.",
            "So this is also an interesting thing to notice.",
            "So that only few few losing only lexicons which are domain specific on navbase you get 2.8, which is OK.",
            "I guess I mean the best result is about .88.",
            "But it's actually quite a good compromise, because sometimes you simply don't have training data, or you know for some other constraints.",
            "So for the first shot, doing lexicons on social media is a good idea."
        ],
        [
            "Um?",
            "And the other link features didn't help in any way.",
            "And."
        ],
        [
            "Maybe just looking at some models, I actually threw this into a decision tree learning so it's called predictive clustering tree.",
            "So the software is called clues.",
            "So The thing is, maybe it shows that what are the big features here so the.",
            "This one is actually not using bag of words, because if you throw in that you just get a big line of single single branches, so it's useless.",
            "But for lexicons an for surface you get quite a nice story.",
            "So here you see that lexicons kind of go on top, then you get some.",
            "Capitalization is the only surface feature which makes sense.",
            "Basically.",
            "If there are people being mentioned there that maybe has to do something with objectivity, because if capitalization.",
            "Is not higher than some percentage, then its objective.",
            "So if no one is being mentioned, then it's objective.",
            "So that makes some sense.",
            "And then there's basically if someone is asking a question.",
            "That that can be.",
            "Subject subjective because if someone is not then.",
            "No questions are objective, so this is one thing."
        ],
        [
            "OK for social media.",
            "So basically the the top layers are mostly number of rows, number of capitalizations, vows, repetitions of letters and there is also some lexicon sprinkled in, so it's.",
            "Smiley yes sad face.",
            "Of course, if sad face.",
            "Yes, positive.",
            "I have no idea how this happens, but.",
            "Maybe there's some cultural barrier here.",
            "I didn't cross quite."
        ],
        [
            "OK, the reviews Mace basically just simply lexical feature for the simple reason that this data set is quite well cleaned up so it doesn't have anything.",
            "Just like those configurations."
        ],
        [
            "I'm I have a lexicon of negation words and I figure out if it's mentioned there or not.",
            "I don't have any specific engineering for that.",
            "It's just a feature actually, so I don't.",
            "I don't have any specific processing, I just say OK, this is a negation word one."
        ],
        [
            "But without the context, just yeah, this one that's but bit simplification and also for contrast words.",
            "But those don't pop up a lot.",
            "But despite, however, this one didn't show much.",
            "So basically ratio of positive to negative words was quite interesting, and some lexicons."
        ],
        [
            "The newsletter set was also.",
            "Quite so lexicon heavy so not many surface features except for capitalization, which is kind of a proxy for mentioning people.",
            "And a bit more on adjectives then announce, so maybe that's a translation artifact."
        ],
        [
            "Yeah, for English social media, it's mostly.",
            "Lexicons, I was surprised, so apparent I think maybe in English.",
            "And so this was actually Irish people use less like heavy exclamation uppercasing stuff like that, and they actually say things in words, not in how they write it.",
            "So maybe that's some cultural thing here."
        ],
        [
            "OK, just to conclude this.",
            "We also tried hierarchical representation, so having this model into 2 levels like first objective and then positive negative and this didn't work really well.",
            "It's same or lower and I would say feature scaling would still be recommended but not in all feature spaces.",
            "So this would be kind of a thing to try, but not without consideration.",
            "And of course, having a two layer model was almost always.",
            "Either the same or better than having just a concatenated model."
        ],
        [
            "So yes, we go with below state of the art on the reviews above on the new data set, which is quite full surprise for me and.",
            "We went through some of these feature differences in different domains and languages and kind of shed some light on some possibly cultural cultural differences.",
            "Here I I'm not that expert in that area, so I'll kind of leave this a bit, but there are."
        ],
        [
            "There so finished.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll talk about expanding on the already quite well researched sentiment analysis problem and comparing how this problem kind of comes up in different domains and in different languages.",
                    "label": 0
                },
                {
                    "sent": "And what are the differences here?",
                    "label": 0
                },
                {
                    "sent": "And above everything, how to best model this?",
                    "label": 0
                },
                {
                    "sent": "In a way that works best for all these setting settings.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just for those of you who aren't familiar with exactly this is a topic of study.",
                    "label": 0
                },
                {
                    "sent": "That's about opinions, sentiment, evaluations, emotions, and so on.",
                    "label": 1
                },
                {
                    "sent": "So basically different things that people use to relate themselves to other things around them.",
                    "label": 0
                },
                {
                    "sent": "Also other people.",
                    "label": 0
                },
                {
                    "sent": "And there's also cinnamon synonym for this called opinion mining, which you some of you might have seen in other domains.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Button then.",
                    "label": 0
                },
                {
                    "sent": "That's because opinions are the things that kind of drive a lot of our behaviors.",
                    "label": 0
                },
                {
                    "sent": "So a lot of stuff that we're doing we're doing because we've seen other people express positive or negative emotion or sentiment towards some particular thing.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know how to influence others, you should follow this presentation.",
                    "label": 0
                },
                {
                    "sent": "Because this is kind of a big decision making criterion and analyzing this in how people are expressing themselves can be really useful to predict how people will react in the future.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, and where can we find this?",
                    "label": 1
                },
                {
                    "sent": "So of course, every other things can be found on the web.",
                    "label": 0
                },
                {
                    "sent": "Or via word of mouth.",
                    "label": 1
                },
                {
                    "sent": "So like people, privately conversing with each other.",
                    "label": 0
                },
                {
                    "sent": "But would the interesting part for us is basically what we can gather.",
                    "label": 0
                },
                {
                    "sent": "So for us, interesting stuff was.",
                    "label": 0
                },
                {
                    "sent": "Found in social media, where people are pretty relaxed about expressing themselves and also in a bit more domain specific areas like products and movie reviews such as Epinions, IMDb, Rotten Tomatoes and such.",
                    "label": 0
                },
                {
                    "sent": "There is also some sentiment in news, but it's a bit more subtle.",
                    "label": 0
                },
                {
                    "sent": "And of course also internal data for various business domains for, for instance for customer feedback.",
                    "label": 0
                },
                {
                    "sent": "So every decent business has a book of complaints somewhere.",
                    "label": 0
                },
                {
                    "sent": "And that could be quite interesting to see.",
                    "label": 0
                },
                {
                    "sent": "And one question is here, is that do these domains kind of act differently?",
                    "label": 0
                },
                {
                    "sent": "So do they have different indicators?",
                    "label": 0
                },
                {
                    "sent": "Or what are the kind of characteristics?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to go quickly through some work that this was done in this area.",
                    "label": 0
                },
                {
                    "sent": "So initially this was done from new movie reviews because this was a quite an easy topic.",
                    "label": 0
                },
                {
                    "sent": "You had positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "They had scores attached and the domain was really straightforward and this was kind of the Seminole work here and.",
                    "label": 0
                },
                {
                    "sent": "Indians this works well and is more or less a solved problem, but it turns out that in other domains you should first separate what is actually an opinion and what is just some statement because not every statement is also an opinion.",
                    "label": 0
                },
                {
                    "sent": "Maybe movie reviews.",
                    "label": 0
                },
                {
                    "sent": "It is because it's a review in the end, but that's definitely not true in general and all domains have wildly different distributions about.",
                    "label": 0
                },
                {
                    "sent": "So what is subjective objective and what is actually positive or negative?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that solving this first and then figuring out what the positive and negative polarities can be more important than just going positive or negative.",
                    "label": 0
                },
                {
                    "sent": "But we'll see how this turns out in our experiments.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's and also since it's a quite clean text classification problem, in the end it's quite interesting area of research for also machine learning application.",
                    "label": 1
                },
                {
                    "sent": "Is so the main problem of having really domain specific classifiers has also been.",
                    "label": 0
                },
                {
                    "sent": "Tackles quite successfully nowadays, and it's also being.",
                    "label": 0
                },
                {
                    "sent": "It also has some quite good results in applying deep learning to this.",
                    "label": 1
                },
                {
                    "sent": "OK, but All in all, most approaches that are used in the wild nowadays.",
                    "label": 0
                },
                {
                    "sent": "They usually encode domain knowledge in some lexicons, so that's kind of still knowledge from some existing source which assigns a unique, globally valid number, or in a class for each word that appear.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the word good is almost almost always somewhat positive.",
                    "label": 0
                },
                {
                    "sent": "This works OK. You don't get perfect performance with this big cause.",
                    "label": 0
                },
                {
                    "sent": "A lot of words are context sensitive, so if something is heavy, that might not be a good or bad thing.",
                    "label": 0
                },
                {
                    "sent": "Right, so if some for instance some phone is like that might be a good thing, but if some for instance.",
                    "label": 0
                },
                {
                    "sent": "Something with example that is inappropriate.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I had this the same one, but I forgot what was the other thing.",
                    "label": 0
                },
                {
                    "sent": "Anyway, things are not context insensitive.",
                    "label": 0
                },
                {
                    "sent": "There are quite some good lexical sources for this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just kind of a quick snapshot.",
                    "label": 0
                },
                {
                    "sent": "How is this problem actually formulated in general so it's an opinion.",
                    "label": 0
                },
                {
                    "sent": "Is a quintuplets 05 points or one is Holder who has the opinion?",
                    "label": 1
                },
                {
                    "sent": "Target is about what aspect about which property orientation is positive, negative mostly or also it could be gradient in gradient and also when was this opinion stated?",
                    "label": 0
                },
                {
                    "sent": "So this can be also interesting playgrounds for some other problems.",
                    "label": 1
                },
                {
                    "sent": "For instance holders and targets sometimes have to be extracted from the content itself, and aspect extraction is also a whole new area of research.",
                    "label": 0
                },
                {
                    "sent": "OK, for simplification here will mostly talk about orientation here, and we're also dealing with Holder in target extraction, But this we treat this as a preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "What we do is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We replace the Holder in the target with some like OPEC.",
                    "label": 0
                },
                {
                    "sent": "Kind of, for instance, we save up person, love that and we do.",
                    "label": 0
                },
                {
                    "sent": "Actually instead of having the persons name, there is a feature.",
                    "label": 0
                },
                {
                    "sent": "We actually said this is just a person or a Holder.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit cleaned up for further learning.",
                    "label": 0
                },
                {
                    "sent": "So yes, this is the actual problem.",
                    "label": 0
                },
                {
                    "sent": "We have objective, negative and positive.",
                    "label": 1
                },
                {
                    "sent": "This is the simplified version, so we don't at first consider doing this in the settings.",
                    "label": 0
                },
                {
                    "sent": "I'll explain later why.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a three class classification problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there we have some hypothesis here which would like to think about and evaluate.",
                    "label": 0
                },
                {
                    "sent": "So one is, can we use external sources of information to increase performance or various lexicons?",
                    "label": 1
                },
                {
                    "sent": "Various regular expressions, patterns and stuff like that?",
                    "label": 0
                },
                {
                    "sent": "Things that we know that should have some information and may not be explicitly present in the data.",
                    "label": 1
                },
                {
                    "sent": "So what's the best way to encode is so there's a different ways to model this as features and kind of how to actually incorporate it in a learning setting.",
                    "label": 0
                },
                {
                    "sent": "Then when we have lexicons which work best for what domain, and of course in the end kind of the descriptive thing, what are the differences across domains and languages?",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to go through the datasets here so we have two languages, Spanish, English and three domains.",
                    "label": 0
                },
                {
                    "sent": "So new social media and movie reviews.",
                    "label": 0
                },
                {
                    "sent": "So this was.",
                    "label": 0
                },
                {
                    "sent": "1st for instance, go through the news domain.",
                    "label": 0
                },
                {
                    "sent": "We use the JRC new sentiment corpus.",
                    "label": 0
                },
                {
                    "sent": "It's around 2000, something 1200 something examples.",
                    "label": 0
                },
                {
                    "sent": "This was originally done for English, but we have a translation done by Bing Translate for in Spanish and it's it's not a bad thing to do actually, at least for news it not not a lot is lost.",
                    "label": 0
                },
                {
                    "sent": "We have the classic reviews data set from both Pang and leading Lee.",
                    "label": 0
                },
                {
                    "sent": "This is the kernel data set, which is really often cited.",
                    "label": 0
                },
                {
                    "sent": "And we also constructed some of our own datasets for social media for specific domains of telecommunication providers and how the people expressed the sentiment about those.",
                    "label": 0
                },
                {
                    "sent": "So they are a bit different in size.",
                    "label": 0
                },
                {
                    "sent": "Some are smaller, so some of them only have two classes that actually don't have any neutral examples.",
                    "label": 0
                },
                {
                    "sent": "And the class distributions are also a bit varied.",
                    "label": 0
                },
                {
                    "sent": "For instance, the new datasets has mostly objective statements.",
                    "label": 0
                },
                {
                    "sent": "Whereas this one basically one has everything subjective and it's a bit different in others, but it's quite OK balanced.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's not doesn't have any strict, really rare occurrences.",
                    "label": 0
                },
                {
                    "sent": "So to represent all this data.",
                    "label": 0
                },
                {
                    "sent": "So how to?",
                    "label": 0
                },
                {
                    "sent": "What do we use to detect this?",
                    "label": 0
                },
                {
                    "sent": "We have three main sources, so first is the content.",
                    "label": 1
                },
                {
                    "sent": "So this is the baseline Model S. So if you have a.",
                    "label": 0
                },
                {
                    "sent": "Piece of text.",
                    "label": 0
                },
                {
                    "sent": "If you do this.",
                    "label": 0
                },
                {
                    "sent": "Tokenization, preprocessing, stemming, lemmatization, stopword removal, and the classic stuff so.",
                    "label": 0
                },
                {
                    "sent": "Here we actually Additionally perform removal of numbers.",
                    "label": 0
                },
                {
                    "sent": "Removal of.",
                    "label": 0
                },
                {
                    "sent": "Targets opinion holders plus removal of URLs.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is simply for cleaning up stuff for social media.",
                    "label": 0
                },
                {
                    "sent": "And people got also really good results by cleaning up misspellings, so this was actually.",
                    "label": 0
                },
                {
                    "sent": "Cleaning up data is brings a much bigger performance improvement than actually any other algorithm.",
                    "label": 1
                },
                {
                    "sent": "Secondly, we use the sentiment lexicons which I mentioned earlier.",
                    "label": 1
                },
                {
                    "sent": "This is a global list of mapping a word to some sentiment score.",
                    "label": 0
                },
                {
                    "sent": "And also some surface patterns which are basically just saying how are things expressed that people are repeating vowels screaming using caps lock, lots of punctuation's, emoticons, smileys and so on.",
                    "label": 0
                },
                {
                    "sent": "So we also have some pattern which are kind of non.",
                    "label": 0
                },
                {
                    "sent": "Non lexical, but we it's kind of how words are actually expressed then.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today for the new spelling corrections, is this something that we're particularly well in the social media setting?",
                    "label": 0
                },
                {
                    "sent": "Yes, we have a lot of them.",
                    "label": 0
                },
                {
                    "sent": "Or yes, it's actually.",
                    "label": 0
                },
                {
                    "sent": "I would also say that for Spanish you have more than than four English.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's the English that is from Ireland, so we'll see if that makes a difference.",
                    "label": 0
                },
                {
                    "sent": "But yeah, some some parts.",
                    "label": 0
                },
                {
                    "sent": "Some cultures actually have really well established slang for social media.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how deep this goes, but yes, for English and for some domains on social media, cleaning up with simple spelling corrections worked wonders.",
                    "label": 0
                },
                {
                    "sent": "No algorithm, language modeling and character modeling and so on.",
                    "label": 0
                },
                {
                    "sent": "So this and some simple dictionary rules.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK you can see the water the presence feature.",
                    "label": 0
                },
                {
                    "sent": "So mostly standard thing except for the masking and also ask normalization.",
                    "label": 0
                },
                {
                    "sent": "This was quite important for Spanish because Spanish has a bit extra characters, but they're not consistently used, so some people actually use the ASK equivalent.",
                    "label": 0
                },
                {
                    "sent": "So if you have a with this mark on top then just say in the end.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK force Lexicon features.",
                    "label": 0
                },
                {
                    "sent": "So this is the second part.",
                    "label": 0
                },
                {
                    "sent": "When we look at the sentiment lexicons, we take a look at every word.",
                    "label": 0
                },
                {
                    "sent": "If it's in there and then, we generate like a sum of all scores in the in the.",
                    "label": 0
                },
                {
                    "sent": "Example, some of the absolute scores, so that's kind of gives us this opinionated Ness.",
                    "label": 0
                },
                {
                    "sent": "Then the ratio of positive to negative and all of the above for every individual simplified part of speech.",
                    "label": 1
                },
                {
                    "sent": "For instance, what is the sum of scores for all nouns?",
                    "label": 0
                },
                {
                    "sent": "And that's one feature here.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a.",
                    "label": 0
                },
                {
                    "sent": "Way to to generate an aggregate score for the whole example, given that you only have individual words that are detected.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some words about the lexicons that we used, so there are several existing ones.",
                    "label": 0
                },
                {
                    "sent": "English, Spanish and we developed some new ones, so this is what inner contributed alot here.",
                    "label": 0
                },
                {
                    "sent": "So we call this render Lex and render Lex links and there is a project that is being worked for and the links part is interesting 'cause we also include them.",
                    "label": 0
                },
                {
                    "sent": "The links of that.",
                    "label": 0
                },
                {
                    "sent": "So the Co occurrence links between words and known sentiment words.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if a word Co occurs with some other work which we know that is positive, we say this is a positive link, and if it Co occurs with a negative word but in a contrast setting, so this is.",
                    "label": 1
                },
                {
                    "sent": "Good but slow and then we say OK.",
                    "label": 0
                },
                {
                    "sent": "If this is a bad slow that means that good is a good thing.",
                    "label": 0
                },
                {
                    "sent": "If so, is is a bad thing.",
                    "label": 0
                },
                {
                    "sent": "So basically we tried to generate features about how these things Co occur.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the surface features, or basically, how should I say this character patterns?",
                    "label": 0
                },
                {
                    "sent": "These are the ones that we can take a look at, so looking at capitalization of individual words in digital characters, proportions of how many things are repeated.",
                    "label": 0
                },
                {
                    "sent": "Also, if negation or contrast words are used emoticons, we actually have special lexicons for English and Spanish because they use totally different emoticons.",
                    "label": 0
                },
                {
                    "sent": "And of course curse words and also punctuation is also quite a big.",
                    "label": 0
                },
                {
                    "sent": "Indicator.",
                    "label": 0
                },
                {
                    "sent": "Some go, you see in the end, but just to profanity, is not used a lot.",
                    "label": 0
                },
                {
                    "sent": "It exists, but not enough to be picked up as a big feature.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Well, that it's in the models.",
                    "label": 0
                },
                {
                    "sent": "It didn't pick up with a big weight, so it's like one of the small signals, but it doesn't make a big difference.",
                    "label": 1
                },
                {
                    "sent": "Also in the score it didn't.",
                    "label": 0
                },
                {
                    "sent": "Kind of contribute much, but included because I saw some people cursing in the data so.",
                    "label": 0
                },
                {
                    "sent": "Let's so OK, this is 1 hypothesis here.",
                    "label": 0
                },
                {
                    "sent": "How should we model this?",
                    "label": 1
                },
                {
                    "sent": "So bag of words representation is usually really sparse, so we have an example which has like 50 words maximum and then.",
                    "label": 0
                },
                {
                    "sent": "More or less in a space of like 100,000 dimensions, and that usually works OK for most models.",
                    "label": 0
                },
                {
                    "sent": "But we also wanted to experiment whether these features, which are basically more like Co occurrence occurrence and TF IDF based, play well with these which are have a bit of a different scale.",
                    "label": 0
                },
                {
                    "sent": "So these are proportional and indicator features and so on.",
                    "label": 0
                },
                {
                    "sent": "And besides, trying just a simple way to concatenate all these things three we tried to do first intermediate model which first used only bag of words to predict what the sentiment and then use this prediction as an input feature for the final classifier along with lexical and surface features.",
                    "label": 0
                },
                {
                    "sent": "So this was kind of a two step like 2 level pipeline in away and the idea was that this might kind of clean up something that.",
                    "label": 0
                },
                {
                    "sent": "Maybe show up.",
                    "label": 0
                },
                {
                    "sent": "It works actually.",
                    "label": 0
                },
                {
                    "sent": "Separating these two further didn't really help a lot, maybe because I think it's more it has to do something with distributions and sparsity I guess.",
                    "label": 0
                },
                {
                    "sent": "I didn't get too deep into this, but I just report on what worked.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, just to say what the setup here.",
                    "label": 0
                },
                {
                    "sent": "So you have these three feature classes.",
                    "label": 0
                },
                {
                    "sent": "We have various combinations of this evaluated and we have, so we valid models on this word plus features 2 levels think to level combination and also using feature scaling.",
                    "label": 0
                },
                {
                    "sent": "So scaling the features to have a mean of zero and variance of 1.",
                    "label": 0
                },
                {
                    "sent": "This reportedly worked well for some SVM problems, especially when you do stochastic learning.",
                    "label": 0
                },
                {
                    "sent": "And we'll see if it works.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for news data, there's not much to see here.",
                    "label": 0
                },
                {
                    "sent": "Becausw the baseline back before test VM was the best Model S nothing else actually came better.",
                    "label": 0
                },
                {
                    "sent": "Things are kind of equivalent statistically, but not much is changed on a good note, this is better than what the people who released the datasets had.",
                    "label": 0
                },
                {
                    "sent": "Like for three points or so, so I think it was in the same was also for English, more or less.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe this one is actually more interesting.",
                    "label": 0
                },
                {
                    "sent": "Here we got better results by including this letter.",
                    "label": 0
                },
                {
                    "sent": "OK, sure.",
                    "label": 0
                },
                {
                    "sent": "You can look here.",
                    "label": 0
                },
                {
                    "sent": "And we hear there wasn't a big difference, but it sounded, including all features here gave us a bit of a boost.",
                    "label": 0
                },
                {
                    "sent": "Compared to other things, so one thing which is obvious not having a bag of words is a pretty bad idea because you actually lose like huge amount of information there.",
                    "label": 0
                },
                {
                    "sent": "Lexicons themselves give some information, but it's hard to tell which one would be better.",
                    "label": 0
                },
                {
                    "sent": "So we have some domain specific ones, and some general ones, and they all kind of contribute something.",
                    "label": 0
                },
                {
                    "sent": "The interesting part is that if we use all of them, we don't actually get worse results, so this was quite shocking.",
                    "label": 0
                },
                {
                    "sent": "London, so I guess, but it doesn't see I think the sparsity so, but this is social media.",
                    "label": 0
                },
                {
                    "sent": "Social media.",
                    "label": 0
                },
                {
                    "sent": "This is like it's also small data set is like eight 900 examples.",
                    "label": 0
                },
                {
                    "sent": "So so even I was surprised that bag of words gave me anything actually.",
                    "label": 0
                },
                {
                    "sent": "But it also shows that this combination model with.",
                    "label": 0
                },
                {
                    "sent": "Two level features actually gives like a slight edge above the SVM one and also in other examples.",
                    "label": 0
                },
                {
                    "sent": "So this is data set is on tweets on services and products from big telephone.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so my phone doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So I bought an iPhone and now it's cheaper so.",
                    "label": 0
                },
                {
                    "sent": "And stuff like that, so yes.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just quit.",
                    "label": 0
                },
                {
                    "sent": "This, uh, this one.",
                    "label": 0
                },
                {
                    "sent": "So there are things that on news we don't do any better on social media we do better.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we can go to the English part.",
                    "label": 0
                },
                {
                    "sent": "So here we we didn't really introduce bag of words.",
                    "label": 0
                },
                {
                    "sent": "Still basically the best thing, more or less.",
                    "label": 0
                },
                {
                    "sent": "So the difference here.",
                    "label": 0
                },
                {
                    "sent": "It wasn't that significant actually.",
                    "label": 0
                },
                {
                    "sent": "But it shows that this combination and feature scaling really was kind of in general a good thing to do here, although the best result still came from just having a bag of words.",
                    "label": 0
                },
                {
                    "sent": "And actually this.",
                    "label": 0
                },
                {
                    "sent": "This one here is not that much below what was actually state of the art.",
                    "label": 0
                },
                {
                    "sent": "Using really sophisticated parsers and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So I think above .91 is what people get with really heavy machinery, so doing something pretty much which is tokenization more or less and getting that Paris was quite OK I guess.",
                    "label": 0
                },
                {
                    "sent": "Come from this rule based.",
                    "label": 0
                },
                {
                    "sent": "No, but they they have a specific.",
                    "label": 0
                },
                {
                    "sent": "Something really complex involving parsers and not rule based.",
                    "label": 0
                },
                {
                    "sent": "It's learned based, still learning, so this is more less people do machine learning on this one more.",
                    "label": 0
                },
                {
                    "sent": "I actually don't remember what was the exact approach.",
                    "label": 0
                },
                {
                    "sent": "The deep learning one got also around 91, so that one is also pattern based.",
                    "label": 0
                },
                {
                    "sent": "And didn't use any of this parsing rules, but.",
                    "label": 0
                },
                {
                    "sent": "The point the point I'm trying to make the yeah so comes from parsing more or less so.",
                    "label": 0
                },
                {
                    "sent": "The data set is not that hard to maximize I guess, so it's easy task in the end and this one is 2002.",
                    "label": 0
                },
                {
                    "sent": "So lot of people actually try this one and most everyone got basically up there somewhere.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the news data.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "For the English part of the news, surprisingly, we actually got some benefit out of using lexicons, so Surface features didn't bring up anything.",
                    "label": 0
                },
                {
                    "sent": "Cause I mean certain features aren't used to express sentiment in use.",
                    "label": 0
                },
                {
                    "sent": "People use!",
                    "label": 0
                },
                {
                    "sent": "Marks really rarely.",
                    "label": 0
                },
                {
                    "sent": "There mostly dots dashes are used where grammar says they should be used and people don't really scream a lot there.",
                    "label": 0
                },
                {
                    "sent": "And I haven't seen any curse words here, so.",
                    "label": 0
                },
                {
                    "sent": "But yeah, lexicons do help a bit cause the data set is still not that big.",
                    "label": 0
                },
                {
                    "sent": "1300 examples and it shows.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the social media data set, this one was actually pretty small.",
                    "label": 0
                },
                {
                    "sent": "So here the the benefit of using lexical huge and actually using bag of words wasn't that good.",
                    "label": 0
                },
                {
                    "sent": "So I think it was that was in some other experiments so.",
                    "label": 0
                },
                {
                    "sent": "Here, because it's a small data set, we don't have a lot of to learn from, so we can actually benefit from having a generalized representation in the form of lexicons.",
                    "label": 0
                },
                {
                    "sent": "So actually here it also helps to have this step version, so just having concatenation really not that impressive.",
                    "label": 0
                },
                {
                    "sent": "And also just to clarify, so this is SVM, multinomial naive Bayes, SVM.",
                    "label": 0
                },
                {
                    "sent": "With these two levels and two level plus scaling.",
                    "label": 0
                },
                {
                    "sent": "So scaling helps a bit here, but not that I would be too happy about it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just to summarize this one and kind of conclude because I think I'm almost out of time.",
                    "label": 0
                },
                {
                    "sent": "Oh, so reviews are pretty hard.",
                    "label": 0
                },
                {
                    "sent": "I think this one was for Spanish, so this is wrong.",
                    "label": 0
                },
                {
                    "sent": "The lexicons in proof.",
                    "label": 0
                },
                {
                    "sent": "Two layers help for.",
                    "label": 0
                },
                {
                    "sent": "Especially for social media.",
                    "label": 0
                },
                {
                    "sent": "And the lexicons which.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So earlier here, so the links, parts and their other lexicons don't show a big difference, except for this one, which is domain specific, which works much better than the one which is general.",
                    "label": 0
                },
                {
                    "sent": "So this is also an interesting thing to notice.",
                    "label": 0
                },
                {
                    "sent": "So that only few few losing only lexicons which are domain specific on navbase you get 2.8, which is OK.",
                    "label": 0
                },
                {
                    "sent": "I guess I mean the best result is about .88.",
                    "label": 0
                },
                {
                    "sent": "But it's actually quite a good compromise, because sometimes you simply don't have training data, or you know for some other constraints.",
                    "label": 0
                },
                {
                    "sent": "So for the first shot, doing lexicons on social media is a good idea.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the other link features didn't help in any way.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe just looking at some models, I actually threw this into a decision tree learning so it's called predictive clustering tree.",
                    "label": 0
                },
                {
                    "sent": "So the software is called clues.",
                    "label": 0
                },
                {
                    "sent": "So The thing is, maybe it shows that what are the big features here so the.",
                    "label": 0
                },
                {
                    "sent": "This one is actually not using bag of words, because if you throw in that you just get a big line of single single branches, so it's useless.",
                    "label": 0
                },
                {
                    "sent": "But for lexicons an for surface you get quite a nice story.",
                    "label": 0
                },
                {
                    "sent": "So here you see that lexicons kind of go on top, then you get some.",
                    "label": 0
                },
                {
                    "sent": "Capitalization is the only surface feature which makes sense.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "If there are people being mentioned there that maybe has to do something with objectivity, because if capitalization.",
                    "label": 0
                },
                {
                    "sent": "Is not higher than some percentage, then its objective.",
                    "label": 0
                },
                {
                    "sent": "So if no one is being mentioned, then it's objective.",
                    "label": 0
                },
                {
                    "sent": "So that makes some sense.",
                    "label": 0
                },
                {
                    "sent": "And then there's basically if someone is asking a question.",
                    "label": 0
                },
                {
                    "sent": "That that can be.",
                    "label": 0
                },
                {
                    "sent": "Subject subjective because if someone is not then.",
                    "label": 0
                },
                {
                    "sent": "No questions are objective, so this is one thing.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK for social media.",
                    "label": 0
                },
                {
                    "sent": "So basically the the top layers are mostly number of rows, number of capitalizations, vows, repetitions of letters and there is also some lexicon sprinkled in, so it's.",
                    "label": 0
                },
                {
                    "sent": "Smiley yes sad face.",
                    "label": 0
                },
                {
                    "sent": "Of course, if sad face.",
                    "label": 0
                },
                {
                    "sent": "Yes, positive.",
                    "label": 0
                },
                {
                    "sent": "I have no idea how this happens, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's some cultural barrier here.",
                    "label": 0
                },
                {
                    "sent": "I didn't cross quite.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the reviews Mace basically just simply lexical feature for the simple reason that this data set is quite well cleaned up so it doesn't have anything.",
                    "label": 0
                },
                {
                    "sent": "Just like those configurations.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm I have a lexicon of negation words and I figure out if it's mentioned there or not.",
                    "label": 0
                },
                {
                    "sent": "I don't have any specific engineering for that.",
                    "label": 0
                },
                {
                    "sent": "It's just a feature actually, so I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't have any specific processing, I just say OK, this is a negation word one.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But without the context, just yeah, this one that's but bit simplification and also for contrast words.",
                    "label": 0
                },
                {
                    "sent": "But those don't pop up a lot.",
                    "label": 0
                },
                {
                    "sent": "But despite, however, this one didn't show much.",
                    "label": 0
                },
                {
                    "sent": "So basically ratio of positive to negative words was quite interesting, and some lexicons.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The newsletter set was also.",
                    "label": 0
                },
                {
                    "sent": "Quite so lexicon heavy so not many surface features except for capitalization, which is kind of a proxy for mentioning people.",
                    "label": 0
                },
                {
                    "sent": "And a bit more on adjectives then announce, so maybe that's a translation artifact.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, for English social media, it's mostly.",
                    "label": 0
                },
                {
                    "sent": "Lexicons, I was surprised, so apparent I think maybe in English.",
                    "label": 0
                },
                {
                    "sent": "And so this was actually Irish people use less like heavy exclamation uppercasing stuff like that, and they actually say things in words, not in how they write it.",
                    "label": 0
                },
                {
                    "sent": "So maybe that's some cultural thing here.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, just to conclude this.",
                    "label": 0
                },
                {
                    "sent": "We also tried hierarchical representation, so having this model into 2 levels like first objective and then positive negative and this didn't work really well.",
                    "label": 1
                },
                {
                    "sent": "It's same or lower and I would say feature scaling would still be recommended but not in all feature spaces.",
                    "label": 1
                },
                {
                    "sent": "So this would be kind of a thing to try, but not without consideration.",
                    "label": 0
                },
                {
                    "sent": "And of course, having a two layer model was almost always.",
                    "label": 0
                },
                {
                    "sent": "Either the same or better than having just a concatenated model.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, we go with below state of the art on the reviews above on the new data set, which is quite full surprise for me and.",
                    "label": 1
                },
                {
                    "sent": "We went through some of these feature differences in different domains and languages and kind of shed some light on some possibly cultural cultural differences.",
                    "label": 0
                },
                {
                    "sent": "Here I I'm not that expert in that area, so I'll kind of leave this a bit, but there are.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There so finished.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}