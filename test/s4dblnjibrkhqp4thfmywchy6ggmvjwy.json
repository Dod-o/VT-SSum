{
    "id": "s4dblnjibrkhqp4thfmywchy6ggmvjwy",
    "title": "Selective Sampling with Almost Optimal Guarantees for Learning to Rank from Pairwise Preferences",
    "info": {
        "author": [
            "Ron Begleiter, Technion - Israel Institute of Technology"
        ],
        "published": "Jan. 24, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Astronomy->Cosmology"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_begleiter_pairwise/",
    "segmentation": [
        [
            "Hi, I'll present a joint work with near and after an about selective sampling algorithm with guarantees.",
            "For learning to rank from pairwise preferences."
        ],
        [
            "So the agenda briefly will be.",
            "I'll present the problem and then I'll go through our specific solution, which, being a novel new structural property over hypothesis spaces."
        ],
        [
            "In a nutshell, what we want to do is to reduce the number of preference when we try to learn to rank from Paris preferences.",
            "An will use a structural property over hypothesis that will lead to a near optimal solution.",
            "An innocence of worst case information theory consideration like inserting.",
            "So if we have elements we assume a little above of N."
        ],
        [
            "Again, pairwise comparisons, so let's start by defining the.",
            "Where you going to program so we have a finite set of elements V and elements, and the pairwise preference function over pairs so.",
            "The preference of the pair is a binary function with the value of 1.",
            "If the first element is preferred over the 2nd element.",
            "Otherwise it is 0.",
            "And we require a pairwise consistency, which means that in each pair exactly one element will be preferred over the other OK.",
            "However, note that the preference function is not necessarily a ranking.",
            "It may define preference cycles like this one in this notation.",
            "This means that these elements is preferred over that element, so we can get.",
            "Such cycles, which semantically means that you is preferred over itself, OK?"
        ],
        [
            "And given this set of elements which which we also call alternatives and a pairwise preference function W, you want to find an order over these elements with minimal pair pairwise disagreement Miss W. So in the formal lecture you've seen this kind of loss.",
            "This is the pairwise disagreement loss their mind.",
            "This notation this is.",
            "In all there over the elements and now with semantically mean that.",
            "Assume this is a preference, so by you is preferred over V in pie.",
            "If the index of few if the index of you in this order is lower than the index of the order.",
            "So if you is preferred over V in Pi, however, V is preferred over U according to the target preference function.",
            "We have a disagreement and we count that as an error, so we sum over all such errors and we should normalize because of symmetry.",
            "So this is our loss function, which is an extension of the well known Kendall Tau distance.",
            "Because the kennel down distance is the distance between permutations and W is not necessarily permutation contains cycles."
        ],
        [
            "OK, let's play with the toy example.",
            "So this is a specific ranking over music titles where.",
            "The right title is where this title is the least preferred, and this is the most preferred.",
            "For example, here is our preference function.",
            "OK, so when this is we can view it as a binary Azam 2 dimensional matrix with binary values where a value of 1 here means that the role element is preferred over the column element.",
            "OK. And instead of the view is of W of the preference function is as a graph.",
            "Where is a directed graph where we have a directed edge between two nodes.",
            "If the source is preferred over the target.",
            "OK, so notice that here we have a preference cycle.",
            "OK, which is our noise.",
            "So the cost of this ranking, for example, the Kendall Tau like cost is exactly 2 because we have a flip of preferences here.",
            "Actually, this is preferred over this one.",
            "In the target function, and we also have a flip here.",
            "OK, and just as a test yourself question and we ask can we have ranking with 0 cost?",
            "In this.",
            "No, no, the the input is V and consider that we have a W in our hands.",
            "I let her define exactly.",
            "That noise, so the ranking of the dog.",
            "Do you get this sign?",
            "No, no, this this is a preference function and the preferences.",
            "We assume that we get him from nature from natural process like a human being and human beings are irrational and if you give them to a person.",
            "Yeah, no, no W is the ground truth.",
            "It's something we that we get from human beings so human beings are rationales and they can provide such cycles.",
            "And what we want to do is you need fair specific ranking over the elements.",
            "While we have such noises in our inputs.",
            "So this is kind of agnostic setting in which our hypothesis space is a permutations over elements.",
            "However, the target function is not a permutation.",
            "OK, any other question?",
            "So can we have a 0 value of the 0 cost ranking?",
            "The answer is no, because we have a preference cycle here an any ranking we have to eliminate at least one edge which isn't this cycle.",
            "So any ranking will occur at least a loss of one over this cycle.",
            "OK.",
            "This is the elementary example, so if you have questions please do ask OK.",
            "If you if you stand for giving you the number of the DW love you X, you will have the number of items which is preferred to date element.",
            "Yes.",
            "Nothing from the setup items.",
            "This would be fine, yes exactly.",
            "However, in the next slide that will explain why this problem is not trivial and why this kind of algorithm will not provide the optimal solution.",
            "OK, so this is a good question, so I should play.",
            "This seems like a very easy problem, however."
        ],
        [
            "This is just the fact that we need to for this talk OK.",
            "However, even when W is known when the target function when you have the targeted function at hand, this problem is not also known as the minimum feedback arc set in tournament, which is NP hard.",
            "This means that we can't assume to provide the polynomial time.",
            "An exact solution for this problem.",
            "OK.",
            "However, recently can ensure that they provided a polynomial time approximation scheme for this problem.",
            "OK, Iapetus, which means that they provided an algorithm and efficient, efficient computational algorithm which give a near optimal solution for this problem.",
            "They provide the they can provide the ranking which is very close to the optimal, where a permutation within the class of permutations.",
            "However, their solution might need to know the full full knowledge of the graph or the matrix, and this is what we actually want to eliminate.",
            "We want to use as least knowledge of preferences as list edges within the graph in order to learn very good or competitive ranking.",
            "So recently in NIPS in the current nips near alone who is also Co. Authoring this work, he presented the first query efficient method for this setting by query efficient method, I mean that he presented them an algorithm which provided a competitive solution using trying to while trying to minimize the number of edges or preferences.",
            "In order to do that.",
            "However, his work has several flows and this work is completely concept different in a conceptual way and it proves his work by giving a tighter bound and also as I'll show later, this work is applicable for rank SVM."
        ],
        [
            "OK, so now few notations before represent the selective sampling algorithm.",
            "So we consider an instant SpaceX of pairs and a set F of permutations or functions from the instant space 201.",
            "OK, where these conditions just enforce that these functions are really permutations?",
            "Or rankings, we also have a soda metric over the space of functions or the summer so dramatic is just the Kendall Tau.",
            "Like distance that I showed before.",
            "It's actually the cost.",
            "Again, our goal is essentially to find a permutation within this set of functions.",
            "And it gives a minimal cost where the cost is the distance between the permutation and the target function W the preferences.",
            "OK, this is actually very standard and you know machine learning setting.",
            "This is a transaction transactive testing.",
            "We have an instant space.",
            "We have a set of functions.",
            "We have a loss or cost.",
            "So dramatic over this function and the cost of a solution, and we want to find the solution that minimize this cost."
        ],
        [
            "So here's the algorithm, which is has a simple form.",
            "Actually, what we do, we start with any initial initial initial solution, any permutation, arbitrary permutation, and then we iteratively on or gradually improve this solution until we output to something that is as I will show, is a competitive solution with respect to the optimal minimal cost.",
            "So what we need actually to view in this algorithm is the marked step, which is actually we should provide two components in order to use this algorithm or utilize it.",
            "The first is.",
            "This estimator, which for now I'll change I'll expand, explain it later.",
            "For now, think about it as just an empirical risk of a biased sample, OK?",
            "So we should provide the specific instance for this estimator and the second component is a some way to actually minimize this functional over the set of functions.",
            "So you know a basic question would be we are just minimizing the functional of our set of functions.",
            "So why just not minimize the original cost original function, which is the Kendall Tau kind of distance, so I should convince you that we can do better by minimizing this kind of ERM, biased sample.",
            "So I should show 2 components in order to actually utilize this algorithm."
        ],
        [
            "So let's start with the selective sampling, which turned to be in a really simple form, which is easy to for example, implement in code.",
            "So let's assume we possess a certain permutation and let's call it a pivot.",
            "OK, this is where in the one iteration of this algorithm and all we do is get a random sample an from set of instances or elements of that size and the actual actual size.",
            "It just need to we need to use it in the analyzes the show later, so we get a sample of elements in.",
            "The trick is just assemble with certain bias and the bias is as follows.",
            "We just for each pair an given correspond.",
            "We define the corresponding coin.",
            "OK, according to this pivot permutation and the coin is just proportional to the distance between the index or the position of U.",
            "The first element in the second element, within despair in the pivot permutation.",
            "OK, for example, the distance between the 1st and the last element within the example permutation is just three 123.",
            "OK, so the intuition behind this kind of sampling is as follows.",
            "We assume that this iterative algorithm gradually improve our solution.",
            "So we start with any solution, any permutation, and we gradually improve and improve it.",
            "And let's assume that we're in a step that we have a fairly good permutation.",
            "You know it's not the best.",
            "But it's OK, permutation.",
            "It's OK ranking.",
            "So intuitively, intuitively, we assume that in a OK ranking we won't have mistakes between really far elements on index ranking.",
            "So that ranking we assume that it is distracting will not make me flip the first element and the last element, because it is good.",
            "OK, however it's not the best.",
            "Then it will probably flip a close elements within this ranking.",
            "OK, so we want to grab as many clothes elements and we make a slack as we as these elements go becomes far system files from each other.",
            "So this is actually the basic idea.",
            "So now we have a sample of.",
            "Let's attend and we need to."
        ],
        [
            "Find this estimator.",
            "This creature so.",
            "OK. Recall that the Kendall Tau distance just summed over.",
            "This was a sum over inconsistencies in pairs OK.",
            "So we define an the disagreement of single pair according to a permutation Sigma to be just if and what Sigma think the preferences on this pair on this edge is different than the one of the true target function.",
            "OK.",
            "In our estimator is just as follows.",
            "We just it is just the counts.",
            "The difference between the cost of this Sigma permutation and the cost of the pivot pie that we have attend.",
            "Currently, however, we have, we don't have the full knowledge of labels of all the labels at hand, we only have labels of the current sample that we sampled and then.",
            "We wait these mistakes and with course, in proportion to the inverse of the coin that was using to choose these mistakes.",
            "So what intuitively this means that we penalized if we have mistakes in farthest pairs and we give Slack if you have mistakes in closer pairs."
        ],
        [
            "Yeah.",
            "OK, so now I'll turn to the actual structural property that we use in the analyzes.",
            "So we define first the higher relative regret.",
            "So Pi is any arbitrary permutation and we define the pie relative regrets over any other permutation Sigma to be just the cost difference between the cost of the test set permutation Sigma and the pivotal permutation \u03c0.",
            "So this is the cost.",
            "This is the chemical distance of like distance between Sigma, the true as target function and this is the cost of the pivots with respect to the tourist ground truth.",
            "And now the epsilon smooth approximation of these by relative regret is just any function.",
            "And such it for all the permutation within the class or function within the class an.",
            "The difference between the estimator and the pie relative regret the actual pie.",
            "Relative regret is behaves like the distance between these two permutations and multiplied by a smooth parameter epsilon.",
            "OK, so we want this estimator to be good for closer and permutation to the current solution, and we give a certain slack to permutation to our farthest from our current pivot solution."
        ],
        [
            "OK, so now we can actually use this kind of property to prove a very powerful statement, which means recall that we in this is iterative algorithm and in each iteration we minimize over this kind of epsilon smooth creatures.",
            "So the ice the eye solution occur it cost that is bounded from above by epsilon regret in respect to the.",
            "Optimal minimal loss.",
            "Plus, a knapsack exponential decaying term.",
            "OK, so this.",
            "Converge quite fast to something that is competitive with the optimal loss that we can gain."
        ],
        [
            "So we provided this kind of estimator and we can prove that with high probability this estimator is actually an epsilon smooth estimator of the pie relative regret actually pirated regret and we call that we computed this estimator using only N polylog N pairs in each round.",
            "And because of the exponential decay in term, we can use only in order of log N iterations.",
            "In order to achieve.",
            "An active learning algorithm for learning to rank from pairwise preferences.",
            "That achieved one plus order of epsilon times the optimal minimal loss within our class of function using only an order magnitude of N polylog N epsilon minus nine pairs.",
            "OK.",
            "So this is a very strong state."
        ],
        [
            "So this solved the first component I had to show.",
            "I promise to show a real estimator for a real problem that I can plug into the algorithm.",
            "Now we should describe the second component or element they mean to minimize.",
            "This epsilon smooth approximator.",
            "So if we try to just to solve this problem directly, then it is not clear really clear how to do it.",
            "This is actually defined subset of the original problem, which is a sparser represent representation of the original problem, which is known to be even harder than original, which was NP hard.",
            "So it is not clear if there is an approximation there or not.",
            "So what instead we suggest to do is to use a relaxed cost.",
            "So we define a relaxation where the loss or the para Los just need the relaxed parallel pairwise loss.",
            "Just need to bound from above the original 1.",
            "This is all we can actually get the similar, all the similar theorems.",
            "So for example.",
            "An example for such a relaxation is the subsample drunk SVM.",
            "So now we described each alternative as a feature vector.",
            "We we give it give a corresponding feature vector in Rd.",
            "And we take their relaxed loss to beat the hinge loss.",
            "OK, So what we actually can get in this case is.",
            "So OK, if you want to activate the A rank is game over this problem, we should fit the SVM with a concentrated correspond the full matrix.",
            "The full preference matrix that would be from, so we can actually with this idea of epsilon smoothness we can actually fit a rank SVM awaited version of Frank SVM with the order fan.",
            "Polylog N constraints and still get an competitive or epsilon regret competitive solution with respect to the ranks when that was a trend over the full matrix.",
            "OK.",
            "So actually for minimisation we can use the mechanism or optimization problem like in SVM.",
            "Yes."
        ],
        [
            "OK, so I'll conclude.",
            "What we did we provided the near optimal query.",
            "Efficient and OP competitive solution for learning to rank from pairwise preferences.",
            "We use the worst case analysis and in diagnostic setting and we prove the recent result by a long that presented as a poster nips.",
            "The current NIPS and from future work or several possibilities among them are, it would be interesting to apply our idea to other settings.",
            "For example, in the NIPS conference and also an extended version of this work was presented here at work by Gemsona Nowak, which presented in average case analyzes also embedding in Rd and a different kind of hypothesis class.",
            "So it will be interesting to see if this idea can.",
            "Also work in such a setting.",
            "The algorithm is that I presented this.",
            "Actually, you know codable, you can code it quite easily.",
            "All you need is for the selective sampling to define these coins and to do to draw them and for the minimization you use SVM tools.",
            "So it will be interesting.",
            "To see whether this algorithm works, works well in practice because in the analyzes we have big hose that you know Heights constantly.",
            "We don't know and also our firm, Echizen done with high probability.",
            "So we want to check whether this works a good and finally the epsilon smoothness idea.",
            "It seems to be quite, you know, general, so it will be very interesting to actually see whether we can apply this idea on other classes of problems.",
            "So all we need to do there is actually to provide a different kind of epsilon, smooth approximator, and we are done.",
            "We can use these results to give.",
            "In active learning relating to a different problem with similar guarantees.",
            "OK, so I'll conclude here and if you have any questions.",
            "Yeah, I'll be gladly.",
            "How do you choose?",
            "Can you adapt this probability of PUB?",
            "Need for now OK?",
            "Yeah.",
            "Thank you here."
        ],
        [
            "Yes and see.",
            "So actually for analysis we use, you know.",
            "Concentration, concentration bounds and deviation bounds.",
            "So for simplicity we don't want the samples of different grounds to relate to each other.",
            "So we ignore this kind of improvement, but maybe with other kind of analytic tools for getting there.",
            "The analyze with algorithm, we can adapt this.",
            "By Estelle to this.",
            "Can you be OK?",
            "Modify it and make it adaptable because I guess that's OK.",
            "This this would be difficult to choose.",
            "What will be difficult to tune, Sir?",
            "Ah, and what you mean to buy to tune?",
            "We don't really too.",
            "We just set it like that.",
            "We have a fixed, you know.",
            "It's closed, it has a closed form and all we have to do is to we have a reference permutation pivot and we can calculate the distance between this specific pair within this reference permutation.",
            "This gives us this value and here we have, you know, just a number that is related to the epsilon is the perimeter of the algorithm.",
            "The smoothness parameter N is the number of elements.",
            "But maybe I didn't understand your question.",
            "ESPN.",
            "If we see this along the line of simulated annealing.",
            "Alright, you're changing your delivery and order to weaken your solution, and you're trying to find something which is close, yeah.",
            "Of course it makes sense as time goes on to to have to for a contribution to do more and more customers, and this would go to having stepsons.",
            "OK, so this is an interesting point and I will think about it.",
            "Thank you.",
            "We discussed this a little bit.",
            "I was wondering if you or anyone else in the room very interested in knowing the lower bounds with this problem in terms of query complexity present N and epsilon.",
            "If anyone knew of any results like that.",
            "Really great up about how low can we go now.",
            "There is a, you know, an information theoretic lower bound on sorting and this is a bit difficult, more difficult problem unless you have other assumptions how?",
            "How many periods must you ask to get to solve your problem?",
            "Yes, so so the lower bound is N log in, right?",
            "In order to sort the elements, yeah, I mean.",
            "There's though with arrows.",
            "OK, so it's much we must be higher than an login.",
            "OK dependence.",
            "Excellent, yeah, it seems to be very difficult problem, yeah?",
            "Speaker OK thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'll present a joint work with near and after an about selective sampling algorithm with guarantees.",
                    "label": 0
                },
                {
                    "sent": "For learning to rank from pairwise preferences.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the agenda briefly will be.",
                    "label": 0
                },
                {
                    "sent": "I'll present the problem and then I'll go through our specific solution, which, being a novel new structural property over hypothesis spaces.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a nutshell, what we want to do is to reduce the number of preference when we try to learn to rank from Paris preferences.",
                    "label": 1
                },
                {
                    "sent": "An will use a structural property over hypothesis that will lead to a near optimal solution.",
                    "label": 1
                },
                {
                    "sent": "An innocence of worst case information theory consideration like inserting.",
                    "label": 0
                },
                {
                    "sent": "So if we have elements we assume a little above of N.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, pairwise comparisons, so let's start by defining the.",
                    "label": 0
                },
                {
                    "sent": "Where you going to program so we have a finite set of elements V and elements, and the pairwise preference function over pairs so.",
                    "label": 1
                },
                {
                    "sent": "The preference of the pair is a binary function with the value of 1.",
                    "label": 0
                },
                {
                    "sent": "If the first element is preferred over the 2nd element.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it is 0.",
                    "label": 0
                },
                {
                    "sent": "And we require a pairwise consistency, which means that in each pair exactly one element will be preferred over the other OK.",
                    "label": 0
                },
                {
                    "sent": "However, note that the preference function is not necessarily a ranking.",
                    "label": 0
                },
                {
                    "sent": "It may define preference cycles like this one in this notation.",
                    "label": 0
                },
                {
                    "sent": "This means that these elements is preferred over that element, so we can get.",
                    "label": 0
                },
                {
                    "sent": "Such cycles, which semantically means that you is preferred over itself, OK?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And given this set of elements which which we also call alternatives and a pairwise preference function W, you want to find an order over these elements with minimal pair pairwise disagreement Miss W. So in the formal lecture you've seen this kind of loss.",
                    "label": 1
                },
                {
                    "sent": "This is the pairwise disagreement loss their mind.",
                    "label": 0
                },
                {
                    "sent": "This notation this is.",
                    "label": 0
                },
                {
                    "sent": "In all there over the elements and now with semantically mean that.",
                    "label": 0
                },
                {
                    "sent": "Assume this is a preference, so by you is preferred over V in pie.",
                    "label": 0
                },
                {
                    "sent": "If the index of few if the index of you in this order is lower than the index of the order.",
                    "label": 0
                },
                {
                    "sent": "So if you is preferred over V in Pi, however, V is preferred over U according to the target preference function.",
                    "label": 0
                },
                {
                    "sent": "We have a disagreement and we count that as an error, so we sum over all such errors and we should normalize because of symmetry.",
                    "label": 1
                },
                {
                    "sent": "So this is our loss function, which is an extension of the well known Kendall Tau distance.",
                    "label": 0
                },
                {
                    "sent": "Because the kennel down distance is the distance between permutations and W is not necessarily permutation contains cycles.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's play with the toy example.",
                    "label": 1
                },
                {
                    "sent": "So this is a specific ranking over music titles where.",
                    "label": 0
                },
                {
                    "sent": "The right title is where this title is the least preferred, and this is the most preferred.",
                    "label": 0
                },
                {
                    "sent": "For example, here is our preference function.",
                    "label": 0
                },
                {
                    "sent": "OK, so when this is we can view it as a binary Azam 2 dimensional matrix with binary values where a value of 1 here means that the role element is preferred over the column element.",
                    "label": 0
                },
                {
                    "sent": "OK. And instead of the view is of W of the preference function is as a graph.",
                    "label": 1
                },
                {
                    "sent": "Where is a directed graph where we have a directed edge between two nodes.",
                    "label": 0
                },
                {
                    "sent": "If the source is preferred over the target.",
                    "label": 0
                },
                {
                    "sent": "OK, so notice that here we have a preference cycle.",
                    "label": 1
                },
                {
                    "sent": "OK, which is our noise.",
                    "label": 0
                },
                {
                    "sent": "So the cost of this ranking, for example, the Kendall Tau like cost is exactly 2 because we have a flip of preferences here.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is preferred over this one.",
                    "label": 0
                },
                {
                    "sent": "In the target function, and we also have a flip here.",
                    "label": 0
                },
                {
                    "sent": "OK, and just as a test yourself question and we ask can we have ranking with 0 cost?",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                },
                {
                    "sent": "No, no, the the input is V and consider that we have a W in our hands.",
                    "label": 0
                },
                {
                    "sent": "I let her define exactly.",
                    "label": 0
                },
                {
                    "sent": "That noise, so the ranking of the dog.",
                    "label": 0
                },
                {
                    "sent": "Do you get this sign?",
                    "label": 1
                },
                {
                    "sent": "No, no, this this is a preference function and the preferences.",
                    "label": 0
                },
                {
                    "sent": "We assume that we get him from nature from natural process like a human being and human beings are irrational and if you give them to a person.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, no W is the ground truth.",
                    "label": 0
                },
                {
                    "sent": "It's something we that we get from human beings so human beings are rationales and they can provide such cycles.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is you need fair specific ranking over the elements.",
                    "label": 0
                },
                {
                    "sent": "While we have such noises in our inputs.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of agnostic setting in which our hypothesis space is a permutations over elements.",
                    "label": 0
                },
                {
                    "sent": "However, the target function is not a permutation.",
                    "label": 0
                },
                {
                    "sent": "OK, any other question?",
                    "label": 0
                },
                {
                    "sent": "So can we have a 0 value of the 0 cost ranking?",
                    "label": 0
                },
                {
                    "sent": "The answer is no, because we have a preference cycle here an any ranking we have to eliminate at least one edge which isn't this cycle.",
                    "label": 0
                },
                {
                    "sent": "So any ranking will occur at least a loss of one over this cycle.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is the elementary example, so if you have questions please do ask OK.",
                    "label": 0
                },
                {
                    "sent": "If you if you stand for giving you the number of the DW love you X, you will have the number of items which is preferred to date element.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Nothing from the setup items.",
                    "label": 0
                },
                {
                    "sent": "This would be fine, yes exactly.",
                    "label": 0
                },
                {
                    "sent": "However, in the next slide that will explain why this problem is not trivial and why this kind of algorithm will not provide the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a good question, so I should play.",
                    "label": 0
                },
                {
                    "sent": "This seems like a very easy problem, however.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just the fact that we need to for this talk OK.",
                    "label": 0
                },
                {
                    "sent": "However, even when W is known when the target function when you have the targeted function at hand, this problem is not also known as the minimum feedback arc set in tournament, which is NP hard.",
                    "label": 1
                },
                {
                    "sent": "This means that we can't assume to provide the polynomial time.",
                    "label": 0
                },
                {
                    "sent": "An exact solution for this problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "However, recently can ensure that they provided a polynomial time approximation scheme for this problem.",
                    "label": 1
                },
                {
                    "sent": "OK, Iapetus, which means that they provided an algorithm and efficient, efficient computational algorithm which give a near optimal solution for this problem.",
                    "label": 0
                },
                {
                    "sent": "They provide the they can provide the ranking which is very close to the optimal, where a permutation within the class of permutations.",
                    "label": 0
                },
                {
                    "sent": "However, their solution might need to know the full full knowledge of the graph or the matrix, and this is what we actually want to eliminate.",
                    "label": 0
                },
                {
                    "sent": "We want to use as least knowledge of preferences as list edges within the graph in order to learn very good or competitive ranking.",
                    "label": 0
                },
                {
                    "sent": "So recently in NIPS in the current nips near alone who is also Co. Authoring this work, he presented the first query efficient method for this setting by query efficient method, I mean that he presented them an algorithm which provided a competitive solution using trying to while trying to minimize the number of edges or preferences.",
                    "label": 0
                },
                {
                    "sent": "In order to do that.",
                    "label": 0
                },
                {
                    "sent": "However, his work has several flows and this work is completely concept different in a conceptual way and it proves his work by giving a tighter bound and also as I'll show later, this work is applicable for rank SVM.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now few notations before represent the selective sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we consider an instant SpaceX of pairs and a set F of permutations or functions from the instant space 201.",
                    "label": 0
                },
                {
                    "sent": "OK, where these conditions just enforce that these functions are really permutations?",
                    "label": 0
                },
                {
                    "sent": "Or rankings, we also have a soda metric over the space of functions or the summer so dramatic is just the Kendall Tau.",
                    "label": 0
                },
                {
                    "sent": "Like distance that I showed before.",
                    "label": 0
                },
                {
                    "sent": "It's actually the cost.",
                    "label": 0
                },
                {
                    "sent": "Again, our goal is essentially to find a permutation within this set of functions.",
                    "label": 0
                },
                {
                    "sent": "And it gives a minimal cost where the cost is the distance between the permutation and the target function W the preferences.",
                    "label": 0
                },
                {
                    "sent": "OK, this is actually very standard and you know machine learning setting.",
                    "label": 0
                },
                {
                    "sent": "This is a transaction transactive testing.",
                    "label": 0
                },
                {
                    "sent": "We have an instant space.",
                    "label": 0
                },
                {
                    "sent": "We have a set of functions.",
                    "label": 0
                },
                {
                    "sent": "We have a loss or cost.",
                    "label": 0
                },
                {
                    "sent": "So dramatic over this function and the cost of a solution, and we want to find the solution that minimize this cost.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the algorithm, which is has a simple form.",
                    "label": 0
                },
                {
                    "sent": "Actually, what we do, we start with any initial initial initial solution, any permutation, arbitrary permutation, and then we iteratively on or gradually improve this solution until we output to something that is as I will show, is a competitive solution with respect to the optimal minimal cost.",
                    "label": 0
                },
                {
                    "sent": "So what we need actually to view in this algorithm is the marked step, which is actually we should provide two components in order to use this algorithm or utilize it.",
                    "label": 0
                },
                {
                    "sent": "The first is.",
                    "label": 0
                },
                {
                    "sent": "This estimator, which for now I'll change I'll expand, explain it later.",
                    "label": 0
                },
                {
                    "sent": "For now, think about it as just an empirical risk of a biased sample, OK?",
                    "label": 1
                },
                {
                    "sent": "So we should provide the specific instance for this estimator and the second component is a some way to actually minimize this functional over the set of functions.",
                    "label": 0
                },
                {
                    "sent": "So you know a basic question would be we are just minimizing the functional of our set of functions.",
                    "label": 0
                },
                {
                    "sent": "So why just not minimize the original cost original function, which is the Kendall Tau kind of distance, so I should convince you that we can do better by minimizing this kind of ERM, biased sample.",
                    "label": 0
                },
                {
                    "sent": "So I should show 2 components in order to actually utilize this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with the selective sampling, which turned to be in a really simple form, which is easy to for example, implement in code.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we possess a certain permutation and let's call it a pivot.",
                    "label": 0
                },
                {
                    "sent": "OK, this is where in the one iteration of this algorithm and all we do is get a random sample an from set of instances or elements of that size and the actual actual size.",
                    "label": 0
                },
                {
                    "sent": "It just need to we need to use it in the analyzes the show later, so we get a sample of elements in.",
                    "label": 0
                },
                {
                    "sent": "The trick is just assemble with certain bias and the bias is as follows.",
                    "label": 0
                },
                {
                    "sent": "We just for each pair an given correspond.",
                    "label": 0
                },
                {
                    "sent": "We define the corresponding coin.",
                    "label": 0
                },
                {
                    "sent": "OK, according to this pivot permutation and the coin is just proportional to the distance between the index or the position of U.",
                    "label": 0
                },
                {
                    "sent": "The first element in the second element, within despair in the pivot permutation.",
                    "label": 0
                },
                {
                    "sent": "OK, for example, the distance between the 1st and the last element within the example permutation is just three 123.",
                    "label": 0
                },
                {
                    "sent": "OK, so the intuition behind this kind of sampling is as follows.",
                    "label": 0
                },
                {
                    "sent": "We assume that this iterative algorithm gradually improve our solution.",
                    "label": 0
                },
                {
                    "sent": "So we start with any solution, any permutation, and we gradually improve and improve it.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that we're in a step that we have a fairly good permutation.",
                    "label": 0
                },
                {
                    "sent": "You know it's not the best.",
                    "label": 0
                },
                {
                    "sent": "But it's OK, permutation.",
                    "label": 0
                },
                {
                    "sent": "It's OK ranking.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, intuitively, we assume that in a OK ranking we won't have mistakes between really far elements on index ranking.",
                    "label": 0
                },
                {
                    "sent": "So that ranking we assume that it is distracting will not make me flip the first element and the last element, because it is good.",
                    "label": 0
                },
                {
                    "sent": "OK, however it's not the best.",
                    "label": 0
                },
                {
                    "sent": "Then it will probably flip a close elements within this ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to grab as many clothes elements and we make a slack as we as these elements go becomes far system files from each other.",
                    "label": 0
                },
                {
                    "sent": "So this is actually the basic idea.",
                    "label": 0
                },
                {
                    "sent": "So now we have a sample of.",
                    "label": 0
                },
                {
                    "sent": "Let's attend and we need to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find this estimator.",
                    "label": 0
                },
                {
                    "sent": "This creature so.",
                    "label": 0
                },
                {
                    "sent": "OK. Recall that the Kendall Tau distance just summed over.",
                    "label": 0
                },
                {
                    "sent": "This was a sum over inconsistencies in pairs OK.",
                    "label": 0
                },
                {
                    "sent": "So we define an the disagreement of single pair according to a permutation Sigma to be just if and what Sigma think the preferences on this pair on this edge is different than the one of the true target function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In our estimator is just as follows.",
                    "label": 0
                },
                {
                    "sent": "We just it is just the counts.",
                    "label": 0
                },
                {
                    "sent": "The difference between the cost of this Sigma permutation and the cost of the pivot pie that we have attend.",
                    "label": 0
                },
                {
                    "sent": "Currently, however, we have, we don't have the full knowledge of labels of all the labels at hand, we only have labels of the current sample that we sampled and then.",
                    "label": 0
                },
                {
                    "sent": "We wait these mistakes and with course, in proportion to the inverse of the coin that was using to choose these mistakes.",
                    "label": 0
                },
                {
                    "sent": "So what intuitively this means that we penalized if we have mistakes in farthest pairs and we give Slack if you have mistakes in closer pairs.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'll turn to the actual structural property that we use in the analyzes.",
                    "label": 0
                },
                {
                    "sent": "So we define first the higher relative regret.",
                    "label": 0
                },
                {
                    "sent": "So Pi is any arbitrary permutation and we define the pie relative regrets over any other permutation Sigma to be just the cost difference between the cost of the test set permutation Sigma and the pivotal permutation \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So this is the cost.",
                    "label": 0
                },
                {
                    "sent": "This is the chemical distance of like distance between Sigma, the true as target function and this is the cost of the pivots with respect to the tourist ground truth.",
                    "label": 0
                },
                {
                    "sent": "And now the epsilon smooth approximation of these by relative regret is just any function.",
                    "label": 0
                },
                {
                    "sent": "And such it for all the permutation within the class or function within the class an.",
                    "label": 0
                },
                {
                    "sent": "The difference between the estimator and the pie relative regret the actual pie.",
                    "label": 0
                },
                {
                    "sent": "Relative regret is behaves like the distance between these two permutations and multiplied by a smooth parameter epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want this estimator to be good for closer and permutation to the current solution, and we give a certain slack to permutation to our farthest from our current pivot solution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we can actually use this kind of property to prove a very powerful statement, which means recall that we in this is iterative algorithm and in each iteration we minimize over this kind of epsilon smooth creatures.",
                    "label": 0
                },
                {
                    "sent": "So the ice the eye solution occur it cost that is bounded from above by epsilon regret in respect to the.",
                    "label": 0
                },
                {
                    "sent": "Optimal minimal loss.",
                    "label": 0
                },
                {
                    "sent": "Plus, a knapsack exponential decaying term.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Converge quite fast to something that is competitive with the optimal loss that we can gain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we provided this kind of estimator and we can prove that with high probability this estimator is actually an epsilon smooth estimator of the pie relative regret actually pirated regret and we call that we computed this estimator using only N polylog N pairs in each round.",
                    "label": 0
                },
                {
                    "sent": "And because of the exponential decay in term, we can use only in order of log N iterations.",
                    "label": 0
                },
                {
                    "sent": "In order to achieve.",
                    "label": 0
                },
                {
                    "sent": "An active learning algorithm for learning to rank from pairwise preferences.",
                    "label": 0
                },
                {
                    "sent": "That achieved one plus order of epsilon times the optimal minimal loss within our class of function using only an order magnitude of N polylog N epsilon minus nine pairs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a very strong state.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this solved the first component I had to show.",
                    "label": 0
                },
                {
                    "sent": "I promise to show a real estimator for a real problem that I can plug into the algorithm.",
                    "label": 1
                },
                {
                    "sent": "Now we should describe the second component or element they mean to minimize.",
                    "label": 0
                },
                {
                    "sent": "This epsilon smooth approximator.",
                    "label": 0
                },
                {
                    "sent": "So if we try to just to solve this problem directly, then it is not clear really clear how to do it.",
                    "label": 0
                },
                {
                    "sent": "This is actually defined subset of the original problem, which is a sparser represent representation of the original problem, which is known to be even harder than original, which was NP hard.",
                    "label": 1
                },
                {
                    "sent": "So it is not clear if there is an approximation there or not.",
                    "label": 0
                },
                {
                    "sent": "So what instead we suggest to do is to use a relaxed cost.",
                    "label": 0
                },
                {
                    "sent": "So we define a relaxation where the loss or the para Los just need the relaxed parallel pairwise loss.",
                    "label": 0
                },
                {
                    "sent": "Just need to bound from above the original 1.",
                    "label": 1
                },
                {
                    "sent": "This is all we can actually get the similar, all the similar theorems.",
                    "label": 1
                },
                {
                    "sent": "So for example.",
                    "label": 1
                },
                {
                    "sent": "An example for such a relaxation is the subsample drunk SVM.",
                    "label": 0
                },
                {
                    "sent": "So now we described each alternative as a feature vector.",
                    "label": 0
                },
                {
                    "sent": "We we give it give a corresponding feature vector in Rd.",
                    "label": 0
                },
                {
                    "sent": "And we take their relaxed loss to beat the hinge loss.",
                    "label": 1
                },
                {
                    "sent": "OK, So what we actually can get in this case is.",
                    "label": 0
                },
                {
                    "sent": "So OK, if you want to activate the A rank is game over this problem, we should fit the SVM with a concentrated correspond the full matrix.",
                    "label": 0
                },
                {
                    "sent": "The full preference matrix that would be from, so we can actually with this idea of epsilon smoothness we can actually fit a rank SVM awaited version of Frank SVM with the order fan.",
                    "label": 0
                },
                {
                    "sent": "Polylog N constraints and still get an competitive or epsilon regret competitive solution with respect to the ranks when that was a trend over the full matrix.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So actually for minimisation we can use the mechanism or optimization problem like in SVM.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll conclude.",
                    "label": 0
                },
                {
                    "sent": "What we did we provided the near optimal query.",
                    "label": 1
                },
                {
                    "sent": "Efficient and OP competitive solution for learning to rank from pairwise preferences.",
                    "label": 0
                },
                {
                    "sent": "We use the worst case analysis and in diagnostic setting and we prove the recent result by a long that presented as a poster nips.",
                    "label": 0
                },
                {
                    "sent": "The current NIPS and from future work or several possibilities among them are, it would be interesting to apply our idea to other settings.",
                    "label": 1
                },
                {
                    "sent": "For example, in the NIPS conference and also an extended version of this work was presented here at work by Gemsona Nowak, which presented in average case analyzes also embedding in Rd and a different kind of hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So it will be interesting to see if this idea can.",
                    "label": 0
                },
                {
                    "sent": "Also work in such a setting.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is that I presented this.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know codable, you can code it quite easily.",
                    "label": 0
                },
                {
                    "sent": "All you need is for the selective sampling to define these coins and to do to draw them and for the minimization you use SVM tools.",
                    "label": 0
                },
                {
                    "sent": "So it will be interesting.",
                    "label": 0
                },
                {
                    "sent": "To see whether this algorithm works, works well in practice because in the analyzes we have big hose that you know Heights constantly.",
                    "label": 0
                },
                {
                    "sent": "We don't know and also our firm, Echizen done with high probability.",
                    "label": 0
                },
                {
                    "sent": "So we want to check whether this works a good and finally the epsilon smoothness idea.",
                    "label": 1
                },
                {
                    "sent": "It seems to be quite, you know, general, so it will be very interesting to actually see whether we can apply this idea on other classes of problems.",
                    "label": 0
                },
                {
                    "sent": "So all we need to do there is actually to provide a different kind of epsilon, smooth approximator, and we are done.",
                    "label": 0
                },
                {
                    "sent": "We can use these results to give.",
                    "label": 0
                },
                {
                    "sent": "In active learning relating to a different problem with similar guarantees.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll conclude here and if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll be gladly.",
                    "label": 0
                },
                {
                    "sent": "How do you choose?",
                    "label": 0
                },
                {
                    "sent": "Can you adapt this probability of PUB?",
                    "label": 0
                },
                {
                    "sent": "Need for now OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes and see.",
                    "label": 0
                },
                {
                    "sent": "So actually for analysis we use, you know.",
                    "label": 0
                },
                {
                    "sent": "Concentration, concentration bounds and deviation bounds.",
                    "label": 0
                },
                {
                    "sent": "So for simplicity we don't want the samples of different grounds to relate to each other.",
                    "label": 0
                },
                {
                    "sent": "So we ignore this kind of improvement, but maybe with other kind of analytic tools for getting there.",
                    "label": 0
                },
                {
                    "sent": "The analyze with algorithm, we can adapt this.",
                    "label": 0
                },
                {
                    "sent": "By Estelle to this.",
                    "label": 0
                },
                {
                    "sent": "Can you be OK?",
                    "label": 0
                },
                {
                    "sent": "Modify it and make it adaptable because I guess that's OK.",
                    "label": 0
                },
                {
                    "sent": "This this would be difficult to choose.",
                    "label": 0
                },
                {
                    "sent": "What will be difficult to tune, Sir?",
                    "label": 0
                },
                {
                    "sent": "Ah, and what you mean to buy to tune?",
                    "label": 0
                },
                {
                    "sent": "We don't really too.",
                    "label": 0
                },
                {
                    "sent": "We just set it like that.",
                    "label": 0
                },
                {
                    "sent": "We have a fixed, you know.",
                    "label": 0
                },
                {
                    "sent": "It's closed, it has a closed form and all we have to do is to we have a reference permutation pivot and we can calculate the distance between this specific pair within this reference permutation.",
                    "label": 0
                },
                {
                    "sent": "This gives us this value and here we have, you know, just a number that is related to the epsilon is the perimeter of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The smoothness parameter N is the number of elements.",
                    "label": 0
                },
                {
                    "sent": "But maybe I didn't understand your question.",
                    "label": 0
                },
                {
                    "sent": "ESPN.",
                    "label": 0
                },
                {
                    "sent": "If we see this along the line of simulated annealing.",
                    "label": 0
                },
                {
                    "sent": "Alright, you're changing your delivery and order to weaken your solution, and you're trying to find something which is close, yeah.",
                    "label": 0
                },
                {
                    "sent": "Of course it makes sense as time goes on to to have to for a contribution to do more and more customers, and this would go to having stepsons.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an interesting point and I will think about it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "We discussed this a little bit.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you or anyone else in the room very interested in knowing the lower bounds with this problem in terms of query complexity present N and epsilon.",
                    "label": 0
                },
                {
                    "sent": "If anyone knew of any results like that.",
                    "label": 0
                },
                {
                    "sent": "Really great up about how low can we go now.",
                    "label": 0
                },
                {
                    "sent": "There is a, you know, an information theoretic lower bound on sorting and this is a bit difficult, more difficult problem unless you have other assumptions how?",
                    "label": 0
                },
                {
                    "sent": "How many periods must you ask to get to solve your problem?",
                    "label": 0
                },
                {
                    "sent": "Yes, so so the lower bound is N log in, right?",
                    "label": 0
                },
                {
                    "sent": "In order to sort the elements, yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "There's though with arrows.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's much we must be higher than an login.",
                    "label": 0
                },
                {
                    "sent": "OK dependence.",
                    "label": 0
                },
                {
                    "sent": "Excellent, yeah, it seems to be very difficult problem, yeah?",
                    "label": 0
                },
                {
                    "sent": "Speaker OK thank you.",
                    "label": 0
                }
            ]
        }
    }
}