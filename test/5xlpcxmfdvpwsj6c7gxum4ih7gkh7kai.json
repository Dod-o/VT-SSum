{
    "id": "5xlpcxmfdvpwsj6c7gxum4ih7gkh7kai",
    "title": "Efficient Indexing of Repeated n-Grams",
    "info": {
        "author": [
            "Samuel Huston, Department of Computer Science, University of Massachusetts Amherst"
        ],
        "published": "Aug. 9, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Web Search"
        ]
    },
    "url": "http://videolectures.net/wsdm2011_huston_eir/",
    "segmentation": [
        [
            "Hi, I'm so I'm Sam Houston.",
            "I'm with the University of Massachusetts in Amherst and this is a collaborative work between us and the University of Melbourne.",
            "We were looking into ways of efficiently indexing repeated engrams."
        ],
        [
            "So let's start first with an outline.",
            "I'll give some definitions, I'll.",
            "I'll look at ways of indexing on a single CPU, which is a simpler model and a bit easier to analyze.",
            "And then we'll look at what happens when we introduce massive data and how to parallelize these these algorithms.",
            "Will will end up with a discussion of some empirical experiments over some English text.",
            "Very large amount of English text."
        ],
        [
            "So.",
            "An ngram in the definition we're talking about is in adjacent words, as you would extract from a collection or a corpus of text.",
            "Where representing each word is a 32 bit integer.",
            "This enables a variety of.",
            "Ease of analysis and it also improves the speed of of.",
            "Reading and writing.",
            "We're interested in ngram indexes because they capture local term dependencies and this is useful for a wider range of tasks, linguistics, similar documents, search, all sorts of stuff."
        ],
        [
            "Um?",
            "But generally, we're not interested in things that only occur a few times within search.",
            "If something only occurs once, not having it in the index is only going to hurt one document.",
            "If you're looking for, say, repeated text or quote to say.",
            "Plagiarism detection, then things that only occur once and never useful.",
            "It's also worth worth noting that as N increases, the fraction of repeated text drops of increases, sorry.",
            "Which.",
            "Which increases the size of air intakes.",
            "So just to introduce some notation, I'm going to use M as my variable for the number of times it's being reused."
        ],
        [
            "So let's move on to the 1st."
        ],
        [
            "Ever.",
            "So we present a standard indexing algorithm.",
            "We read all of the index the N grams out of out of some collection.",
            "We sort them and we then as we write it out, discard posting lists that are not sufficiently log.",
            "This seems plausible enough.",
            "We've used it variety of times before.",
            "Especially over, say, single term indexes."
        ],
        [
            "But let's look at the space complexity.",
            "The problem is the intermediate data size.",
            "Each each term, each each symbol in our corpus is going to be repeated in NN grams.",
            "There's also a plus one for the location it's being held at, so we end up with N plus one times the size of the corpus, effectively being sorted in that central intermediate stage.",
            "And this means that it blows up the intermediate space massively.",
            "So for a four terabyte collection with N = 10, we end up with 44 terabytes of data to be sorted."
        ],
        [
            "It might be interesting to note that the time complexity doesn't change.",
            "We still sorting the same number of things.",
            "Taking N grams doesn't change the length of the collection, but as the data grows, we spend more and more time reading and writing."
        ],
        [
            "So.",
            "With any one pass algorithm, we have no way of telling whether an engram occurs once or more times at the first time we see it, so we can't easily discard it.",
            "In fact, we can't discount it at all.",
            "So.",
            "Given that, that's really what we want to do.",
            "If we can discard at that first pass.",
            "Would reduce that intermediate data size and we then be able to to index abit more space efficiently.",
            "So what about 2 passes?",
            "Can we pass through the corpus once, collect enough information to be able to pass through the second time, and only keep things that were actually going to index?"
        ],
        [
            "So we introduce a hash based filter.",
            "During the first pass, we click at a fixed width hash value for each engram.",
            "And we write it down to a file.",
            "We sort it.",
            "We collect engrams and we can then have a list of basically a Bloom filter of things that occur sufficient times using that bloom filter.",
            "We can then.",
            "Filter the corpus on the."
        ],
        [
            "So here's a diagram that will hopefully.",
            "Show my point.",
            "I've made this diagram so that the size of the files actually corresponds to the size of the files that are being generated.",
            "File one and file two of the same width that both be bit hash values.",
            "But file too is a subset.",
            "It only contains unique repeated hash values.",
            "File 3 is much wider.",
            "It contains N grams and positions, not hash values.",
            "But it's the same height as file 2 effectively.",
            "It should be noted that using hash values will introduce some collisions, but with a sufficiently large hash width the collisions should be relatively minimal."
        ],
        [
            "So given we can do 2 passes, why not more?",
            "There's some previous work called specs that looked at.",
            "Creating a filter for uni grams that they then used to build a filter for bigrams that they then use the trigrams and so on and then once they get up to N -- 1 where N is the desired number, they use that filter Gen generate the final index."
        ],
        [
            "In comparison to this hash approach, their algorithm doesn't produce file one.",
            "They produce file to directly in memory, and they use that file to to generate the next filter and the next filter, and eventually the final N -- 1 file 2 is used to generate file 3."
        ],
        [
            "So what about space usage?",
            "Which file is the big one?",
            "And this is an actual example.",
            "The various end though the F there is observed fractions of the corpus that are actually going to be indexed as estimated from a chunk of Trek Clue Web B.",
            "File one is always going to be a fixed width of the original collection size.",
            "File 2 is the subset of that that's that's repeated and file 3 is.",
            "The same size as file too, but much wider.",
            "Clearly the space usage implies that we don't need to do more than two passes in this case."
        ],
        [
            "So now I'm going to show you how much we're throwing away.",
            "For various end, this is the sequence the input corpus, the blue section at the bottom is the things that only occur once.",
            "The red section just above it is is the vocabulary of things that occur more than once in the green section above.",
            "That is the second or more or case instance.",
            "Of the engrams?",
            "So you can see we're actually discarding a very large section of our collection, and it's worth noting that if we were not to filter, if we were to keep everything, the blue and the red would would account for the vocabulary size that you have to index.",
            "And that would blow up your space proportions massively."
        ],
        [
            "So on a single CPU we performed a few simple experiments to see how long each of these algorithms take.",
            "You can see quite clearly that.",
            "The number of times we Passover the corpus is the dominating time factor.",
            "This may not be surprising at all.",
            "I should note that the specs one made me rather unpopular in the.",
            "The Department 'cause it used about two days per run and each data point was replicated 10 times."
        ],
        [
            "So let's move on.",
            "Let's see what happens when we introduce mass."
        ],
        [
            "Save data.",
            "We end up with a very similar diagram.",
            "It is what we had before.",
            "Only each stage is now replicated across several processes.",
            "The corpus is divided up into sections and passed to several nodes which extract file one in parallel.",
            "File one is then.",
            "Sorted and merged into file to this, discard some of the data.",
            "Then because we're using hash values, we have to merge all of those into a single Bloom filter, which we then copy on to every single node so that we can produce file 3 reliably if we choose not to pass a hash value to a node, and that node actually gets an engram that needs that hashes to that value, we end up discarding something we don't want to discard.",
            "We end up getting a false negative.",
            "Currently, our system would only produce false bogus.",
            "Positives.",
            "And it's that merge that makes this sis."
        ],
        [
            "I'm not scalable.",
            "So.",
            "As we increase the size of our collection, the number of things that we need to keep the number of hash values in our Bloom filter will increase.",
            "And if we go far enough, it doesn't matter how many processes we've got, each processor needs to have that entire set of data, and it needs to be in memory because it is randomly accessed.",
            "So hash based methods cannot be scalable.",
            "At a certain point, you simply need to buy more RAM for every single machine."
        ],
        [
            "Your cluster.",
            "So we fix this problem.",
            "Instead of using hash values, we can use locations in the corpus to determine which ones are repeated and which ones are not.",
            "So now file 2 is a list of locations.",
            "Because file one kept locations with hash values.",
            "They can then be sorted into location order.",
            "We can stream the filter off file in parallel with the section of the corpus we're reading.",
            "We can also distribute accurately the filter or appropriately, which means each processor doesn't have data it doesn't need, and this makes the algorithm much more scalable."
        ],
        [
            "This does affect space usage.",
            "File one now contains almost twice the data.",
            "File 2 now now can't use unique hash values.",
            "It has to keep every location.",
            "But we can still see that final three dominates the space usage.",
            "It's still not worth doing 2 passes."
        ],
        [
            "So now we move on to our expense."
        ],
        [
            "It's.",
            "We use this setup.",
            "It's outlined in the paper.",
            "I'm not going to really go over it.",
            "Key points are our hash tables are in memory.",
            "Hash tables were limited to 1 gig.",
            "That meant, at a certain point they said that they stopped being effective.",
            "We started getting more and more hash collisions."
        ],
        [
            "So this diagram, this graph shows how each algorithm scales as we increase processors and data across the bottom.",
            "We should see approximately horizontal horizontal lines for algorithms that scale.",
            "Each processor still gets approximately the same amount of it.",
            "It does get.",
            "It gets exactly the same amount of data.",
            "As we go across the graph, we can see that all of these algorithms approximately are scalable.",
            "The height difference is just show that we're using more passes over the data and that costs time."
        ],
        [
            "But on space usage, we see that displays one pass increases with with in a linear way with respect to the size of the corpus and the slope of the line is determined by N = 8.",
            "In comparison allocation method, the Pink pink line.",
            "Does not it hugs quite closely to the size of the final line index.",
            "The difference is the the number of collisions that we get using our hash values.",
            "You can see the hash value of the hash based one.",
            "The Green line just above.",
            "Has more collisions.",
            "This is because we were using a set sized hash.",
            "We couldn't increase the hash because we didn't have more memory to give it.",
            "So you can see it starts decaying and eventually if we continue up it should hit disk based.",
            "And it's a similar problem with respect.",
            "It's also using an in memory hash table."
        ],
        [
            "So then we scale up even further.",
            "We're now operating over half and whole of Clue, Abby.",
            "This is 1 1/2 terabytes of text.",
            "Uncompressed it contains 40 billion symbols.",
            "And our experiments are designed to keep the amount of data on each node constant so that we can compare the times and show that it's still scalable.",
            "We increased processes and the amount of data accordingly so that.",
            "We keep the same amount of data per."
        ],
        [
            "No.",
            "So these are our results.",
            "We can see how long it took the amount of space it used and final index size.",
            "We can see that location based users more time.",
            "This based is faster.",
            "This is unsurprising.",
            "We're still passing over the same amount of data twice or once."
        ],
        [
            "We can see that location based is more compact.",
            "It uses much less space.",
            "The reason we don't have disk based over the whole is I simply didn't have the space on my cluster to be able to do it.",
            "Other people using it at the time."
        ],
        [
            "We can also see that location based scales well even doubling the data.",
            "We don't get a large jump in time.",
            "Well, yeah."
        ],
        [
            "So we can draw a few conclusions we can.",
            "We can show that we've shown that displays 1 past is very efficient if you have the space.",
            "Very efficient in terms of speed.",
            "It is scalable.",
            "It can be parallelized very well.",
            "But the space usage grows within.",
            "And our new method provides a nice option.",
            "It allows us to.",
            "Only use the space we need to use.",
            "In a more limited system.",
            "It does cost more time, but it does parallel well, does parallelize well, and it does scale."
        ],
        [
            "And that's it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, I'm so I'm Sam Houston.",
                    "label": 0
                },
                {
                    "sent": "I'm with the University of Massachusetts in Amherst and this is a collaborative work between us and the University of Melbourne.",
                    "label": 1
                },
                {
                    "sent": "We were looking into ways of efficiently indexing repeated engrams.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start first with an outline.",
                    "label": 0
                },
                {
                    "sent": "I'll give some definitions, I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll look at ways of indexing on a single CPU, which is a simpler model and a bit easier to analyze.",
                    "label": 0
                },
                {
                    "sent": "And then we'll look at what happens when we introduce massive data and how to parallelize these these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Will will end up with a discussion of some empirical experiments over some English text.",
                    "label": 0
                },
                {
                    "sent": "Very large amount of English text.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An ngram in the definition we're talking about is in adjacent words, as you would extract from a collection or a corpus of text.",
                    "label": 0
                },
                {
                    "sent": "Where representing each word is a 32 bit integer.",
                    "label": 1
                },
                {
                    "sent": "This enables a variety of.",
                    "label": 0
                },
                {
                    "sent": "Ease of analysis and it also improves the speed of of.",
                    "label": 0
                },
                {
                    "sent": "Reading and writing.",
                    "label": 0
                },
                {
                    "sent": "We're interested in ngram indexes because they capture local term dependencies and this is useful for a wider range of tasks, linguistics, similar documents, search, all sorts of stuff.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But generally, we're not interested in things that only occur a few times within search.",
                    "label": 0
                },
                {
                    "sent": "If something only occurs once, not having it in the index is only going to hurt one document.",
                    "label": 0
                },
                {
                    "sent": "If you're looking for, say, repeated text or quote to say.",
                    "label": 0
                },
                {
                    "sent": "Plagiarism detection, then things that only occur once and never useful.",
                    "label": 0
                },
                {
                    "sent": "It's also worth worth noting that as N increases, the fraction of repeated text drops of increases, sorry.",
                    "label": 1
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Which increases the size of air intakes.",
                    "label": 1
                },
                {
                    "sent": "So just to introduce some notation, I'm going to use M as my variable for the number of times it's being reused.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's move on to the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ever.",
                    "label": 0
                },
                {
                    "sent": "So we present a standard indexing algorithm.",
                    "label": 0
                },
                {
                    "sent": "We read all of the index the N grams out of out of some collection.",
                    "label": 0
                },
                {
                    "sent": "We sort them and we then as we write it out, discard posting lists that are not sufficiently log.",
                    "label": 0
                },
                {
                    "sent": "This seems plausible enough.",
                    "label": 0
                },
                {
                    "sent": "We've used it variety of times before.",
                    "label": 0
                },
                {
                    "sent": "Especially over, say, single term indexes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But let's look at the space complexity.",
                    "label": 1
                },
                {
                    "sent": "The problem is the intermediate data size.",
                    "label": 1
                },
                {
                    "sent": "Each each term, each each symbol in our corpus is going to be repeated in NN grams.",
                    "label": 1
                },
                {
                    "sent": "There's also a plus one for the location it's being held at, so we end up with N plus one times the size of the corpus, effectively being sorted in that central intermediate stage.",
                    "label": 0
                },
                {
                    "sent": "And this means that it blows up the intermediate space massively.",
                    "label": 0
                },
                {
                    "sent": "So for a four terabyte collection with N = 10, we end up with 44 terabytes of data to be sorted.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It might be interesting to note that the time complexity doesn't change.",
                    "label": 1
                },
                {
                    "sent": "We still sorting the same number of things.",
                    "label": 1
                },
                {
                    "sent": "Taking N grams doesn't change the length of the collection, but as the data grows, we spend more and more time reading and writing.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "With any one pass algorithm, we have no way of telling whether an engram occurs once or more times at the first time we see it, so we can't easily discard it.",
                    "label": 1
                },
                {
                    "sent": "In fact, we can't discount it at all.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Given that, that's really what we want to do.",
                    "label": 0
                },
                {
                    "sent": "If we can discard at that first pass.",
                    "label": 0
                },
                {
                    "sent": "Would reduce that intermediate data size and we then be able to to index abit more space efficiently.",
                    "label": 0
                },
                {
                    "sent": "So what about 2 passes?",
                    "label": 0
                },
                {
                    "sent": "Can we pass through the corpus once, collect enough information to be able to pass through the second time, and only keep things that were actually going to index?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we introduce a hash based filter.",
                    "label": 1
                },
                {
                    "sent": "During the first pass, we click at a fixed width hash value for each engram.",
                    "label": 1
                },
                {
                    "sent": "And we write it down to a file.",
                    "label": 0
                },
                {
                    "sent": "We sort it.",
                    "label": 0
                },
                {
                    "sent": "We collect engrams and we can then have a list of basically a Bloom filter of things that occur sufficient times using that bloom filter.",
                    "label": 0
                },
                {
                    "sent": "We can then.",
                    "label": 0
                },
                {
                    "sent": "Filter the corpus on the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a diagram that will hopefully.",
                    "label": 0
                },
                {
                    "sent": "Show my point.",
                    "label": 0
                },
                {
                    "sent": "I've made this diagram so that the size of the files actually corresponds to the size of the files that are being generated.",
                    "label": 0
                },
                {
                    "sent": "File one and file two of the same width that both be bit hash values.",
                    "label": 0
                },
                {
                    "sent": "But file too is a subset.",
                    "label": 0
                },
                {
                    "sent": "It only contains unique repeated hash values.",
                    "label": 0
                },
                {
                    "sent": "File 3 is much wider.",
                    "label": 0
                },
                {
                    "sent": "It contains N grams and positions, not hash values.",
                    "label": 0
                },
                {
                    "sent": "But it's the same height as file 2 effectively.",
                    "label": 0
                },
                {
                    "sent": "It should be noted that using hash values will introduce some collisions, but with a sufficiently large hash width the collisions should be relatively minimal.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So given we can do 2 passes, why not more?",
                    "label": 0
                },
                {
                    "sent": "There's some previous work called specs that looked at.",
                    "label": 0
                },
                {
                    "sent": "Creating a filter for uni grams that they then used to build a filter for bigrams that they then use the trigrams and so on and then once they get up to N -- 1 where N is the desired number, they use that filter Gen generate the final index.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In comparison to this hash approach, their algorithm doesn't produce file one.",
                    "label": 0
                },
                {
                    "sent": "They produce file to directly in memory, and they use that file to to generate the next filter and the next filter, and eventually the final N -- 1 file 2 is used to generate file 3.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what about space usage?",
                    "label": 1
                },
                {
                    "sent": "Which file is the big one?",
                    "label": 0
                },
                {
                    "sent": "And this is an actual example.",
                    "label": 1
                },
                {
                    "sent": "The various end though the F there is observed fractions of the corpus that are actually going to be indexed as estimated from a chunk of Trek Clue Web B.",
                    "label": 1
                },
                {
                    "sent": "File one is always going to be a fixed width of the original collection size.",
                    "label": 0
                },
                {
                    "sent": "File 2 is the subset of that that's that's repeated and file 3 is.",
                    "label": 1
                },
                {
                    "sent": "The same size as file too, but much wider.",
                    "label": 0
                },
                {
                    "sent": "Clearly the space usage implies that we don't need to do more than two passes in this case.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to show you how much we're throwing away.",
                    "label": 0
                },
                {
                    "sent": "For various end, this is the sequence the input corpus, the blue section at the bottom is the things that only occur once.",
                    "label": 0
                },
                {
                    "sent": "The red section just above it is is the vocabulary of things that occur more than once in the green section above.",
                    "label": 0
                },
                {
                    "sent": "That is the second or more or case instance.",
                    "label": 0
                },
                {
                    "sent": "Of the engrams?",
                    "label": 0
                },
                {
                    "sent": "So you can see we're actually discarding a very large section of our collection, and it's worth noting that if we were not to filter, if we were to keep everything, the blue and the red would would account for the vocabulary size that you have to index.",
                    "label": 0
                },
                {
                    "sent": "And that would blow up your space proportions massively.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on a single CPU we performed a few simple experiments to see how long each of these algorithms take.",
                    "label": 0
                },
                {
                    "sent": "You can see quite clearly that.",
                    "label": 0
                },
                {
                    "sent": "The number of times we Passover the corpus is the dominating time factor.",
                    "label": 0
                },
                {
                    "sent": "This may not be surprising at all.",
                    "label": 0
                },
                {
                    "sent": "I should note that the specs one made me rather unpopular in the.",
                    "label": 0
                },
                {
                    "sent": "The Department 'cause it used about two days per run and each data point was replicated 10 times.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's move on.",
                    "label": 0
                },
                {
                    "sent": "Let's see what happens when we introduce mass.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Save data.",
                    "label": 0
                },
                {
                    "sent": "We end up with a very similar diagram.",
                    "label": 0
                },
                {
                    "sent": "It is what we had before.",
                    "label": 0
                },
                {
                    "sent": "Only each stage is now replicated across several processes.",
                    "label": 0
                },
                {
                    "sent": "The corpus is divided up into sections and passed to several nodes which extract file one in parallel.",
                    "label": 0
                },
                {
                    "sent": "File one is then.",
                    "label": 0
                },
                {
                    "sent": "Sorted and merged into file to this, discard some of the data.",
                    "label": 0
                },
                {
                    "sent": "Then because we're using hash values, we have to merge all of those into a single Bloom filter, which we then copy on to every single node so that we can produce file 3 reliably if we choose not to pass a hash value to a node, and that node actually gets an engram that needs that hashes to that value, we end up discarding something we don't want to discard.",
                    "label": 0
                },
                {
                    "sent": "We end up getting a false negative.",
                    "label": 0
                },
                {
                    "sent": "Currently, our system would only produce false bogus.",
                    "label": 0
                },
                {
                    "sent": "Positives.",
                    "label": 0
                },
                {
                    "sent": "And it's that merge that makes this sis.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not scalable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As we increase the size of our collection, the number of things that we need to keep the number of hash values in our Bloom filter will increase.",
                    "label": 1
                },
                {
                    "sent": "And if we go far enough, it doesn't matter how many processes we've got, each processor needs to have that entire set of data, and it needs to be in memory because it is randomly accessed.",
                    "label": 0
                },
                {
                    "sent": "So hash based methods cannot be scalable.",
                    "label": 1
                },
                {
                    "sent": "At a certain point, you simply need to buy more RAM for every single machine.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your cluster.",
                    "label": 0
                },
                {
                    "sent": "So we fix this problem.",
                    "label": 0
                },
                {
                    "sent": "Instead of using hash values, we can use locations in the corpus to determine which ones are repeated and which ones are not.",
                    "label": 1
                },
                {
                    "sent": "So now file 2 is a list of locations.",
                    "label": 0
                },
                {
                    "sent": "Because file one kept locations with hash values.",
                    "label": 0
                },
                {
                    "sent": "They can then be sorted into location order.",
                    "label": 1
                },
                {
                    "sent": "We can stream the filter off file in parallel with the section of the corpus we're reading.",
                    "label": 0
                },
                {
                    "sent": "We can also distribute accurately the filter or appropriately, which means each processor doesn't have data it doesn't need, and this makes the algorithm much more scalable.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This does affect space usage.",
                    "label": 0
                },
                {
                    "sent": "File one now contains almost twice the data.",
                    "label": 0
                },
                {
                    "sent": "File 2 now now can't use unique hash values.",
                    "label": 1
                },
                {
                    "sent": "It has to keep every location.",
                    "label": 0
                },
                {
                    "sent": "But we can still see that final three dominates the space usage.",
                    "label": 0
                },
                {
                    "sent": "It's still not worth doing 2 passes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we move on to our expense.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "We use this setup.",
                    "label": 0
                },
                {
                    "sent": "It's outlined in the paper.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to really go over it.",
                    "label": 0
                },
                {
                    "sent": "Key points are our hash tables are in memory.",
                    "label": 0
                },
                {
                    "sent": "Hash tables were limited to 1 gig.",
                    "label": 0
                },
                {
                    "sent": "That meant, at a certain point they said that they stopped being effective.",
                    "label": 0
                },
                {
                    "sent": "We started getting more and more hash collisions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this diagram, this graph shows how each algorithm scales as we increase processors and data across the bottom.",
                    "label": 0
                },
                {
                    "sent": "We should see approximately horizontal horizontal lines for algorithms that scale.",
                    "label": 0
                },
                {
                    "sent": "Each processor still gets approximately the same amount of it.",
                    "label": 0
                },
                {
                    "sent": "It does get.",
                    "label": 0
                },
                {
                    "sent": "It gets exactly the same amount of data.",
                    "label": 0
                },
                {
                    "sent": "As we go across the graph, we can see that all of these algorithms approximately are scalable.",
                    "label": 0
                },
                {
                    "sent": "The height difference is just show that we're using more passes over the data and that costs time.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But on space usage, we see that displays one pass increases with with in a linear way with respect to the size of the corpus and the slope of the line is determined by N = 8.",
                    "label": 1
                },
                {
                    "sent": "In comparison allocation method, the Pink pink line.",
                    "label": 0
                },
                {
                    "sent": "Does not it hugs quite closely to the size of the final line index.",
                    "label": 0
                },
                {
                    "sent": "The difference is the the number of collisions that we get using our hash values.",
                    "label": 0
                },
                {
                    "sent": "You can see the hash value of the hash based one.",
                    "label": 1
                },
                {
                    "sent": "The Green line just above.",
                    "label": 0
                },
                {
                    "sent": "Has more collisions.",
                    "label": 0
                },
                {
                    "sent": "This is because we were using a set sized hash.",
                    "label": 0
                },
                {
                    "sent": "We couldn't increase the hash because we didn't have more memory to give it.",
                    "label": 1
                },
                {
                    "sent": "So you can see it starts decaying and eventually if we continue up it should hit disk based.",
                    "label": 0
                },
                {
                    "sent": "And it's a similar problem with respect.",
                    "label": 0
                },
                {
                    "sent": "It's also using an in memory hash table.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we scale up even further.",
                    "label": 0
                },
                {
                    "sent": "We're now operating over half and whole of Clue, Abby.",
                    "label": 0
                },
                {
                    "sent": "This is 1 1/2 terabytes of text.",
                    "label": 0
                },
                {
                    "sent": "Uncompressed it contains 40 billion symbols.",
                    "label": 0
                },
                {
                    "sent": "And our experiments are designed to keep the amount of data on each node constant so that we can compare the times and show that it's still scalable.",
                    "label": 0
                },
                {
                    "sent": "We increased processes and the amount of data accordingly so that.",
                    "label": 0
                },
                {
                    "sent": "We keep the same amount of data per.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So these are our results.",
                    "label": 0
                },
                {
                    "sent": "We can see how long it took the amount of space it used and final index size.",
                    "label": 1
                },
                {
                    "sent": "We can see that location based users more time.",
                    "label": 0
                },
                {
                    "sent": "This based is faster.",
                    "label": 0
                },
                {
                    "sent": "This is unsurprising.",
                    "label": 0
                },
                {
                    "sent": "We're still passing over the same amount of data twice or once.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can see that location based is more compact.",
                    "label": 1
                },
                {
                    "sent": "It uses much less space.",
                    "label": 0
                },
                {
                    "sent": "The reason we don't have disk based over the whole is I simply didn't have the space on my cluster to be able to do it.",
                    "label": 0
                },
                {
                    "sent": "Other people using it at the time.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also see that location based scales well even doubling the data.",
                    "label": 1
                },
                {
                    "sent": "We don't get a large jump in time.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can draw a few conclusions we can.",
                    "label": 0
                },
                {
                    "sent": "We can show that we've shown that displays 1 past is very efficient if you have the space.",
                    "label": 0
                },
                {
                    "sent": "Very efficient in terms of speed.",
                    "label": 0
                },
                {
                    "sent": "It is scalable.",
                    "label": 0
                },
                {
                    "sent": "It can be parallelized very well.",
                    "label": 0
                },
                {
                    "sent": "But the space usage grows within.",
                    "label": 0
                },
                {
                    "sent": "And our new method provides a nice option.",
                    "label": 0
                },
                {
                    "sent": "It allows us to.",
                    "label": 0
                },
                {
                    "sent": "Only use the space we need to use.",
                    "label": 0
                },
                {
                    "sent": "In a more limited system.",
                    "label": 0
                },
                {
                    "sent": "It does cost more time, but it does parallel well, does parallelize well, and it does scale.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}