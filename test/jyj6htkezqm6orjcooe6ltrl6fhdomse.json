{
    "id": "jyj6htkezqm6orjcooe6ltrl6fhdomse",
    "title": "Fitting a Graph to Vector Data",
    "info": {
        "author": [
            "Daniel A. Spielman, Department of Computer Science, Yale University"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/mlss09us_spielman_fgvd/",
    "segmentation": [
        [
            "I think that works.",
            "So.",
            "To begin, I should tell you this is joint work with John Kellner, who's back at Yale furiously trying to finish writing his thesis.",
            "Sorry, not jungle let Sam Dite furiously, writing a thesis in John Kellner, whose at MIT.",
            "Let me begin by saying that none of us work in machine learning, so I don't know this audience very well.",
            "Please be prepared to stop me if I say anything that is really unfamiliar or doesn't make sense.",
            "Also, let me just give you a little background on me to explain this talk, so I grew up in the theoretical computer science community, which means that I spent a lot of my Graduate School years learning about graphs and graph algorithms, and in the last and so for me, the 1st order object you always go to is a graph.",
            "In the last few years I've realized there are a lot of people for whom inputs actually aren't graphs.",
            "There are sets of vectors.",
            "Maybe you know these vectors are different coordinates represent features or something like that.",
            "I'm OK this talk in this paper is part of my attempt to take those vectors and turn him into a graph, which I can then think about, and that's something I can deal with.",
            "Is this going to change pages?",
            "No, that's bad.",
            "Now."
        ],
        [
            "OK, so let me give concretely the question we're asking and I'm just going to propose one solution.",
            "There are probably other interesting solutions, but I think it's a start.",
            "So imagine I have a bunch of collection of vectors.",
            "Here I call him X, one through XN, living in D dimensions, D will always be the dimensions for me.",
            "I want to associate a natural graph with these vectors.",
            "So for me the vertex set will be the vectors, or you can identify it is just one to end 'cause there in vectors.",
            "Now what do I mean by natural?",
            "Well, first it should somehow be theoretically well motivated.",
            "There should be some reason why I'm choosing this graph.",
            "For this audience it should help you solve standard machine learning problems like classification or regression or things you might want to do with these vectors.",
            "For me it is specially if interesting combinatorial properties that for me I sort of want.",
            "You know the a natural graph?",
            "Maybe?",
            "Think of it as gods graph.",
            "It should be interesting somehow.",
            "But OK, maybe I have some different constraints then God.",
            "It should be efficiently computable.",
            "I need to be able to compute the darn things.",
            "It should be sparse.",
            "You know.",
            "I don't want to take the complete graph with some strange weights.",
            "I really want the graph notion to be important, which means that there shouldn't.",
            "You shouldn't be overwhelmed with edges.",
            "I have one other goal you'll see which is."
        ],
        [
            "So that helps solve theoretical computer science problems on the vectors.",
            "I'm not using it for that yet, but that's part of what's going on in the background here and why.",
            "I'll be very interested in combinatorial properties.",
            "OK, so I have a big out."
        ],
        [
            "Fine, and we'll see what we get through Main Liang to start out by telling you how people generally associate graphs with vectors.",
            "Probably this is familiar to a lot of you.",
            "Then try to explain our proposal for one natural way of doing this and then I will explain reinterpret that proposal in terms of laplacian's.",
            "It will simplify notation alot through the rest of the talk.",
            "I will tell you about some interesting combinatorial properties that these graphs have.",
            "I'll present some experiments showing what sort of results we get when we use them to solve classification, regression or clustering problems that will at least convince you.",
            "I hope that I'm not crazy.",
            "You'll see you do get some good answers to these problems using these graphs.",
            "I'll talk about how we can compute them when I say efficient computation that is going to be heuristic rather than theoretical meaning, we have an algorithm we can prove when it stops, it does give you the correct answer and it stops relatively quickly, but I can't bound the running time nearly as nicely as I'd like to.",
            "If there's time I will talk about the notion of call approximate sparsity, which I guess I should just define with one type of sparsity that's absolute earlier.",
            "Another type of sparsity later.",
            "I don't want to mention another approach to this problem that comes out of work of Lo Vas, which is really part of what motivated me towards doing this and it could provide another very natural approach to this problem, but we can't work it all out yet.",
            "And they're going to be lots of open questions, which I'll repeat all at the end because we barely understand what we're talking about so far.",
            "So why don't I start out with the staff?"
        ],
        [
            "Anyways, people usually make graphs when you're given a collection of vectors.",
            "I described it as you need to choose what are the edge is going to be an?",
            "What are the weights?",
            "'cause usually you want weighted graphs.",
            "So there are two natural ways to go about each of these things.",
            "For Edge is one of the standard things to do is fine for every vertex, it's K nearest neighbors for some K Anne, let those be the edges.",
            "Another very natural thing people have done is they choose some distance threshold, I'll call it Delta.",
            "And can put an edge between every two vertices if their distance from each other is less than Delta.",
            "I think we saw both of those at some point today already there.",
            "The questions of what you're going about weights some people like to be agnostic.",
            "They say that you know the distance really doesn't mean very much, so we're going to go with unweighted graphs, which is very reasonable choice.",
            "Another thing to do if you're going to use weights, I think the Canonical thing to do is to make them exponential.",
            "So usually the standard is exponential in the distance between the vertices squared.",
            "Well, inverse exponential divided by two Sigma squared, and then you've got to think about how you're going to choose Sigma.",
            "But what the heck, you already had to think about how you're going to choose K and how you're going to choose Delta, and you can often do some experiments to find what's a reasonable value.",
            "OK, so these are the standard ways of choosing graphs.",
            "They have some very nice features.",
            "One, they're really easy to compute.",
            "This is just a prescription for where to add edges and also at least we've learned if you have a discretization of a manifold and you just grew ties, it nicely enough.",
            "These give you something natural and through other graphs that give you other natural things.",
            "I'm going to try to do things without manifolds, so our whole motivation is going to be completely different."
        ],
        [
            "So first I'm just going to give you the formula for this proposal, not because I expect everyone to understand it right now, but some of you are impatient and probably just want to see it.",
            "Then I'll motivate it.",
            "Going to choose edge weights to minimize the following quantity.",
            "It is a sum over all vertices, so I goes from one to N. Let me start with the term over.",
            "Here I write the sum over J squiggle I.",
            "That's if J is a neighbor of node I.",
            "We take a weight of the edge AIJ times the vector for vertex J.",
            "All of that gets subtracted from vert vector.",
            "I times its weighted degree, so DI means the weighted degree of node I.",
            "That's just the sum of these weights.",
            "This is a symmetric graph, so the weight from node eyes to J is the same as the weight from node Jade I.",
            "And we want all of the weighted degrees to be at least one.",
            "OK, that's the proposal.",
            "I'm here is 1 example of a."
        ],
        [
            "Point set and this was the graph that we got out.",
            "I truncated the digits, they didn't actually hit exactly those numbers, but it's good enough.",
            "OK, so that's the proposal.",
            "So for those who are impatient, you now know what it is for the better.",
            "Let me motivate it, and then we'll come back to it.",
            "So my motivation?"
        ],
        [
            "And is this?",
            "Let's consider solving a regression problem on a graph.",
            "Meaning you have some function or a regression problem on the vectors using the graph.",
            "So you have some function, say F of the vertices.",
            "Let's say it gives for each vector XI.",
            "It gives a label why I which I'll treat is a real number now.",
            "Or you could make zero or one or something like that.",
            "Let's think of now just the simplest problem.",
            "I give you a weighted graph and I ask you to please try to figure out why I is and I'll give you the value of Y it every other vector.",
            "So I'll give you YJ for every other vector.",
            "And I'll give you a weighted graph.",
            "Maybe I no longer tell you what the vectors are.",
            "The question is, if I just give you the weighted graph, what is the most natural rule you could come up with to guess the wise?",
            "And I maintain that the most natural rule to guess why I given the value of YJ at every other vertex is take a look at its neighbors.",
            "And take a look at the weighted average of the value of Y at its neighbors.",
            "So that is literally some over the neighbors of Vertex I.",
            "The value YJ times the weight in 1 / D. I normalizes this so the you get a weighted average is the sum of the weights.",
            "So I think that to me is the most natural rule for given a weighted graph and given a bunch of real numbers associated with the vertices, guessing a number at the one vertex he didn't know.",
            "OK, I'm going to make that my interpolation rule.",
            "In this case, let's take a look at what the error is.",
            "Let's consider you know leaving out every single vertex intern.",
            "I believe when I started trying to read machine learning papers that called this leave one out cross validation at least want to measure the error of that.",
            "So I'll sum over every single vertex.",
            "If this is my error, this is what why I actually is.",
            "This is the guess I will get not using iy but using all the other data will square that and take their sum.",
            "OK, now how am I going to use this idea to form a graph?",
            "The question is what's my data?",
            "What I know is I know a vector for every single vertex.",
            "I want to choose my graph so that I at least do well with respect to."
        ],
        [
            "Those vectors.",
            "So meaning let me take every single vector XI and I'll write, superscripts indicate the coordinates, I'll call it XI soup one through excise soup D. I maintain that if we're going to be able to use these vectors to solve regression problems, then you know the function F had better be pretty nice in terms of the coordinates and I'm just going to start out thinking about the nicest functions.",
            "That's the coordinate vectors themselves.",
            "So if I try to take a look at this procedure again, let's say I choose a weighted graph, what will my expected error or what my error be?",
            "Summed over all vertices on the coordinate vectors well in the case coordinate vector, I'll get this term in here and then will sum the sum of squares over all K. So you can just, you know rewrite this.",
            "Putting K inside and write it as the norm squared of sum over every vector XI minus the weighted average of its neighbors.",
            "Take a look at the norm of that squared.",
            "Our goal is to choose the graph to minimize this quantity."
        ],
        [
            "So again, my idea is if I just wanted to solve the regression problem on the coordinate vectors, I want to choose the graph that would solve that best for me.",
            "So that is the start of our idea.",
            "This is a good point for where I should ask if I have any questions and if everyone understand if I'm speaking the right language really.",
            "OK, now.",
            "Evil excuse me.",
            "No, yeah, we can't necessarily find a graph for which this is 0.",
            "Even just to think about, there can be boundary effects.",
            "If I have a vertex which is on the convex Hull, you can't represent it as an average of its neighbors, and they'll always be some vertex on the convex Hull.",
            "OK, this is my motivation.",
            "You might recognize that the initial expression I gave you a slightly different 'cause there's a problem with doing this.",
            "Well, there are two problems."
        ],
        [
            "The first is if I try to optimize this, it leads to a non convex program.",
            "That's because the weights are appearing both in weighted degrees down here and here.",
            "And there's a worse problem.",
            "Let me just give you an example."
        ],
        [
            "So let's just consider the points arranged in a regular Pentagon.",
            "Let me ask you to find the weights that are going to be best with respect to this objective function.",
            "The answer is what you say."
        ],
        [
            "It should be you just hook him up in a regular Pentagon and you could put weight one on.",
            "And well, this doesn't match this invariant up to scaling, so this could be wait 1 two.",
            "You'd get the same answer, doesn't matter.",
            "And if you take a look at what the errors are or meaning in a where are we getting error here?"
        ],
        [
            "At this top node here, we're representing it as the average of these two, so you're sort of guessing it's here and you're getting the sum of the red line squared, but you get one for every single vertex.",
            "OK, that's a nice example.",
            "What if I put a point?"
        ],
        [
            "In the middle.",
            "If I put a point in the middle, it becomes a little messier.",
            "This is the solution you."
        ],
        [
            "Yet.",
            "You'd like to put ones on each of these edges, but these things can be epsilon and actually the smaller epsilon is.",
            "The lower this objective function.",
            "Because in some sense, for this node I mean, these nodes here don't want to use that node to represent them.",
            "They do better using just their two neighbors across here.",
            "This node wants to be the center of all the others, and the fact that the weighted degree is zero.",
            "Well, that expression doesn't care 'cause it's just taking weighted average of the neighbors.",
            "So annoying thing about this graph is these epsilons go to zero and actually it can get worse.",
            "You can make examples where there are many different epsilons going to zero at different rates.",
            "Some can want to be squares of each other and really when you look at this program you're not really getting weights.",
            "You're really figuring out how fast are things going to zero.",
            "And I that's not something I really think like to compute with.",
            "It's not a concrete graph, for me, it's just sort of how zero are they?",
            "OK, so we need to fix this a little bit."
        ],
        [
            "Here's how we're going to fix it.",
            "We will view the weighted degree of a node as its importance.",
            "And we will then instead of measuring how well the sort of regression instead of just measuring the regression error over all nodes, we waited by the weighted degrees of nodes squares.",
            "So I just get rid of this expression instead and put the weighted degree in here and multiply it through.",
            "So we get the weighted degree times a vertex minus the average of its neighbors, and that's essentially our proposal.",
            "Again, with one caveat, you'll notice that of course this can be minimized by just setting all the weights to 0.",
            "So we need to do something to prevent that.",
            "Well, we."
        ],
        [
            "Look at two different solutions.",
            "The first one is we say, OK, you've got to make the weighted degree of every node at least one.",
            "That says you have to treat every node as being important.",
            "If you make the weighted degree node of a node more than one, it tends to make this expression larger, so there's very little incentive to do it.",
            "But sometimes there are few nodes for which it happens in practice.",
            "The other idea is to say you can't have your degrees be too much less than one, but we allow some outliers.",
            "By the way, this constraint will call the hard constraint.",
            "I'll call this the hard graph.",
            "The other way we looked at it is take a look at the sum over all degrees that are less than one of 1 minus that degree squared, and we say that can't be too big to say it's less than Alpha N will call that the Alpha soft graph.",
            "So in this case you're allowed to give low degree to some nodes but not too low degree to too many.",
            "OK, now I've actually given you our proposal for how we form these graphs.",
            "Yes.",
            "No, not at this point.",
            "At this point I've got a quadratic objective function being minimized, subject to non negativity constraints, and either of these makes it sufficiently determined.",
            "Well, our weights always have to be positive.",
            "Oh wait, maybe I've gone wrong, so this means there always is an answer.",
            "I haven't guaranteed a unique answer yet.",
            "Oh in.",
            "Wasted.",
            "Actually no, let me answer.",
            "Mention that now there's a conjecture.",
            "In every experiment we've ever done on real data, it is always unique and by looking at the KKT conditions of the solution, you can certify that.",
            "However, there are examples where the solution is not unique.",
            "If you have highly symmetric point sets and I'll actually prove that to you later."
        ],
        [
            "So you're stuck with that.",
            "Had non uniqueness is sort of unavoidable, but it's our conjecture is that if you make an infinitesimally small perturbation to any point set, then if it's infinitesimally small random perturbation then this solution will be unique with probability.",
            "One is our conjecture, but we can't prove it yet.",
            "OK, so here's an example this I just took a random set of points in two dimensions and computed the .1 soft graph for them.",
            "So I want to point out two things here.",
            "Oh, and by the way, the thickness of the edges is proportional to the weights.",
            "So the first thing I want to point out is this is a relatively nice looking graph which we were not a priority expecting when you set out to, say, minimize regression error or something like that, but the graph looks OK. Let me make another observation.",
            "This graph is planar.",
            "No edges between vertices cross.",
            "That'll prove for you later in the talk.",
            "It's actually not too hard, so for points in two dimensions, it does turn out to be planner.",
            "And let me just also point out it's again, it looks like a relatively nice graph so that I'm sort of very happy with.",
            "A natural question is what if you have two clusters that are far away from each other?"
        ],
        [
            "Well, what we find is if I stick two clusters that are far away from each other, you might expect there would be no edge between them.",
            "That's not the case.",
            "There are going to be edges between them, they are just much thinner or lighter weight edges than the edges connecting the clusters themselves.",
            "That said, we also don't yet know how to prove this graph is connected.",
            "We think it is.",
            "It always is, but can't prove that yet.",
            "But I can prove the one statement I made to you a moment ago.",
            "If you have two different clusters that are very far apart, we can prove that there is an edge going from one to the other.",
            "That proof just breaks down as they start to get too close to each other.",
            "So there's a lot of open questions here, so I said we barely understand what we're doing, but we've seen some interesting things so far."
        ],
        [
            "Here's another fun example here.",
            "I just took 10 points at regular around the circle.",
            "Have them, you know, have their distance to the origin and then have it again.",
            "The one thing I want to end here, I've again drawn the weights on the edges.",
            "This was the .1 soft graph so you can see that these nodes on the outside actually have much lower.",
            "Weighted degree, then the nodes on the inside.",
            "But one thing I want to point out here is at least the structure you get here.",
            "You always get these rings even though the scales are changing greatly over the graph.",
            "Which is sort of interesting because if you go at choosing something like, well, K. Nearest neighbors, I guess would probably get something like that, not even necessarily depending on how you set K. And if you definitely built a graph based on a threshold on distances well, by the time you start connecting these nodes, you have all sorts of connections in here.",
            "So somehow this is doing something fairly natural, varying the graph at different scales when the points vary in different scales, yes.",
            "About why you don't.",
            "Um?",
            "I will actually say I cannot give intuition.",
            "I can give a proof which I will do.",
            "I will give two or three.",
            "I'll give two different proofs of that, but.",
            "Yeah, that's not.",
            "My intuition is not quick.",
            "I admit, you know we were sort of surprised at first, so.",
            "It."
        ],
        [
            "My understanding of this fact is more algebraic than intuitive so far.",
            "Actually, if anyone can come up with a really nice intuitive explanation, I'd be psyched.",
            "We don't have that yet.",
            "OK, so as I mentioned, the graph is not necessarily unique.",
            "An I will show you later examples of point sets will not in two dimensions, but in higher dimensions, in which it's not unique, and that sort of has to be.",
            "If you have a lot of symmetry, then you just can't expect uniqueness is I'll explain, but as I said, we conjecture that it's unique with probability one after an infinitesimally small perturbation.",
            "I would love it if someone could prove that for us, we don't know how to get there yet."
        ],
        [
            "OK, this is related to a lot of things that have been considered before.",
            "I want to point out the relation to locally linear linear embeddings.",
            "Locally, linear embeddings is another way of.",
            "Well, it's not really associating a graph with a set of vectors well the first they do associated graph, but they choose the neighbors and then they choose weights.",
            "But the weights are not chosen to be symmetric, but there's still minimizing the same objective function.",
            "So you can see this as being related 'cause it's the same objective function.",
            "They just sort of choose what the neighbors are and then minimize the weights and don't have any symmetry in it.",
            "This is also closely related to K means, so if I actually insisted that your graph was a collection of disjoint cliques all within each clique, having at least two vertices, and you look at the K means objective function, it's the exact same objective function that we have here.",
            "Which is actually part of how we got started on this.",
            "When Sam noticed that and pointed it out to me."
        ],
        [
            "OK, so now I need to describe this in terms of the graph.",
            "Laplacian is 1 because I spend a lot of time thinking about graph laplacians and two it will simplify notation.",
            "Out of curiosity, how many of you are familiar with the graph Laplacian?",
            "Terrific almost everybody.",
            "So just for those few others I will quickly say.",
            "The graph Laplacian, we take it, you take the weighted adjacency matrix.",
            "Say you subtract it from the diagonal matrix of degrees.",
            "So for this graph here, where the vertices I've numbered.",
            "This is the Laplacian matrix for it.",
            "So say for the edge between vertex one and three, having weight .67.",
            "That means that we have a negative .67 in row one, column three and symmetrically column one row 3.",
            "The only thing that's really bad about this example is the degree of every node is 1.",
            "So I have ones down the diagonal that does not happen in general.",
            "I'm sorry about that.",
            "That's the drawback of using the same example twice.",
            "Generally, the weighted degree appears here."
        ],
        [
            "OK, our objective function in terms of the weighted Laplacian is just the Frobenius norm squared of the Laplacian matrix times the data matrix or X.",
            "So think of X as being this N by D matrix of your vectors.",
            "You multiply that by the Laplacian on the squares of all the entries.",
            "That's our objective function.",
            "And what I should point out the start of being able to compute this or doing anything else with it is that the Laplacian itself is a matrix, is linear in the edge weights, so LX is linear in the edge weights, so this is a quadratic.",
            "In the edge weights 'cause you're taking this linear things and squaring it so our optimization problem is fundamentally going to be one of quadratic optimization.",
            "That said, I will explain later in the talk how you can make that quadratic optimization practical, but for now just keep in mind it's quadratic, which means it's convex and has all sorts of other nice properties.",
            "I want to point out that the objective function I rejected earlier was the same thing you get for the normalized Laplacian, that is, it had.",
            "I taken D inverse times L * X, That was the one that was non convex and at all these annoying zeros ideologically, that bothers me because I prefer the normalized Laplacian to the Laplacian but.",
            "Reality doesn't let me use it.",
            "Maybe someone can fix this somehow."
        ],
        [
            "OK, so now let me tell you why there is a sparse solution, at least when you're at least in terms of the dimension.",
            "So I will prove in a moment that you always have average degree at most 2 * D + 1, where D is the dimension.",
            "So I mean, there can be cases where it's dense.",
            "If I give you is your data set the corners of a regular simplex, then you're going to get the complete graph 'cause if the corners of the regular simplex or endpoints and dimensions there's nothing else you can do just by symmetry you've got to return the complete graph.",
            "But if your dimension is lower, you don't.",
            "OK, so here is one way of seeing that.",
            "Imagine fixing elex.",
            "That is, fixing all the entries of this end by D matrix that imposes end times D linear constraints.",
            "If I say fix it so I tell you what the values will be.",
            "Alternatively, you can think of that as fixing these difference vectors, which always come up in our analysis.",
            "For each node I we have this term we call Delta I, which is DIXI minus the weighted average of its neighbors.",
            "Imagine fixing all of those.",
            "You can also see that that's D * N linear constraints you're imposing.",
            "Also imagine fixing the weighted degrees, who I changed the order an I didn't define you yet, but if W is the vector of weights of edges, there's a matrix that you can multiply it by.",
            "That gets you the weighted degree of every node, so fixing the weighted degrees gives you another N linear constraints.",
            "OK, So what does that mean?",
            "If you have N * D + 1 linear constraints, that's all we're imposing on the weights and that they be non negative.",
            "So that means that there is actually a large vector space of solutions.",
            "If you weren't worried about Nonnegativity.",
            "Of the weights, but it turns out that if you have more than end times D + 1 non zero weights of edges.",
            "Then you can find some vector in this vector space that's not all positive.",
            "Well, actually doesn't matter because we can find some vector in this vector space.",
            "To add or subtract from our current vector on the weights of edges.",
            "And if you have two different solutions, one of 'em is all positive and the other one is different, you can subtract the different one off until some weight goes to 0.",
            "Or you add some multiple of the other vector until some weights goes to 0.",
            "So this is actually not a property.",
            "How to put it?",
            "If I do that an I keep LX fixed, I'm not changing the objective function and if I keep the weighted degrees fixed I'm not changing my constraint whether or not my constraint is satisfied.",
            "So it says that for any solution that you get, if you had more than N * D + 1 non zeros, there's a way to change it to make another entry zero.",
            "I mean this isn't really special to our program.",
            "This is a very general property in quadratic programming that just sort of comes from the form of it.",
            "But you can just think of it as you know, keeping the objective function fixed is at worst end times D constraints and keeping the weighted degrees fixed is another end.",
            "So as soon as you have too many nonzero entries that are all positive, you can make another 1 zero and.",
            "So that tells us there is always a sparse solution.",
            "At least in low dimensions, yes.",
            "Complementary.",
            "Um interest, is it so the question was, is this related with complementary slackness?",
            "And in some sense, pry think the answer is probably, but I'd have to think about it to give you a good answer.",
            "And I guess I said OK, so that is at least one way of saying that in low dimensions you get a sparse solution."
        ],
        [
            "If you take a look once I've given you end times D + 1 edges, that's actually I want to point out average degree 2 * D + 1.",
            "Because every edge touches two vertices, so we're going to average three 2 * D + 1 An.",
            "I will give a theorem if there's time later sort of saying there's always an approximately sparse solution in any dimension, I think there will be time for that, I'm not sure, but we don't know about a not.",
            "Well, there's not necessarily an optimal solution in a dimension, but there's always an approximate sparse one, but that's a theorem that needs to be improved."
        ],
        [
            "OK, so here are some examples on some real data.",
            "We grabbed a bunch of data from the UCI repository and from the web pages of the LIBSVM folks maintain and I've just listed all the datasets we grabbed here.",
            "The type of problem people were looking at the number of vectors, the dimension, and the average degree we get for the hard and soft graphs, so it's almost always less than two times.",
            "It's always less than 2 * T + 1.",
            "It's often closer to the dimension.",
            "Sometimes the average degree of the graph is a lot lower.",
            "So I want to point."
        ],
        [
            "At a few examples on these datasets, the average degree is really much much lower than the dimension.",
            "Um?",
            "That makes me think that maybe this data is in some way low dimensional, but I have no way of making that rigorous yet.",
            "But I would love to get a notion of the natural dimensionality of some data and try to show that that is related to the average degree of the graphs that we get, but that's an open problem.",
            "We don't know what to do with that yet."
        ],
        [
            "OK, let me just quickly take some profits here and show you how you can get graphs for which it can't possibly get datasets for which our graphs cannot possibly be unique.",
            "If you have enough symmetries, you're not going to get uniqueness.",
            "So for example, take a look at all the vectors in the hypercube, so 01 to the but not all of them.",
            "Take all of them of even parity.",
            "You take all the vectors in the hypercube of even parity.",
            "There's a whole lot of automorphisms of those you know, permute the dimensions of the hypercube and spin your hypercube around.",
            "Because we have a quadratic program and it's convex.",
            "If you have any two solutions and add them together, you get another solution.",
            "And so take all of these automorphisms of your point set onto itself and some over all of the solutions that you get.",
            "If you do that, you can show that your graph has to have degree at least D, choose two.",
            "Actually, in this case it will be exactly choose to, but you can prove it has to be at least D. Choose two.",
            "On the other hand, I proved you a few moments ago that there always is a solution in which the average degrees at most 2 * D + 1.",
            "So once D is sufficiently big, probably you get here five or six these cross you know there's not a unique solution because there's a symmetric solution and there's a sparse solution.",
            "So you can think of this as thinking that symmetry is in some sense the enemy of sparsity.",
            "If you have really highly symmetric point sets, if you want a symmetric solution, you can have it, but you can't, or you can have a sparse solution, but you can't necessarily get both.",
            "I've learned not to let this bother me, but.",
            "Part of me wants the symmetric solution, but computationally the sparse solution makes life a lot easier.",
            "OK, let me talk about."
        ],
        [
            "Parity, so as I mentioned before in 2D, the point sets are planar.",
            "This to me again, if the graph is planar, there's just a crazy thing to observe.",
            "So what I will prove you is actually there is always a hard and Alpha soft graph.",
            "This planner and I say there is one because again, I don't know that their unique.",
            "So again, planarity.",
            "If you don't think about graphs, just means that there are no edges crossing like this.",
            "I will tell you how the proof goes before I do that, let me just mention there are a few other generalizations of this."
        ],
        [
            "You can also prove that there is no triangle containing a point, and Gary Miller pointed out to me this morning that if you look at the proof in our paper, it also tells you that in higher dimensions the graph you get is a simplicial complex.",
            "So there's something interesting going on here.",
            "K here's how you wind up proving this."
        ],
        [
            "Basically, if you give me any situation like this.",
            "We're going to prove to you, at least for now, that if you take a look at a graph with a maximum, if there are many solutions, take a solution that maximizes the sum of the weighted degrees.",
            "We will prove that such a solution cannot be planned and cannot have crossings.",
            "And the way we'll do that is we will evidence another solution with a higher sum of weighted degrees in the same objective function value.",
            "And because we're increasing the sum of weighted degrees and keeping the objective function value the same, then that other thing wasn't a solution, at least of maximum sum of weighted degrees.",
            "An if it's unique, then we don't need to worry about that business.",
            "So here's how we do it.",
            "If I take a look at a crossing, consider drawing this 4 sided shape.",
            "I should remember the names of those.",
            "I know my daughter in second grade learned this.",
            "Again quadrilateral.",
            "I think around those points.",
            "What we're going to do is we're going to increase the weights and all these edges on the outside and decrease the weights on the edges in the inside.",
            "In such a way that not only does the objective function not change, but actually those difference vectors I showed you before won't change, or really LX won't change and it will increase all of the weights.",
            "Turns out there's a really easy way to do this."
        ],
        [
            "And the way you do it is you use barycentric coordinates.",
            "So you take a look at the point Z where they cross and you write it as a weighted average of these two point 10 points of 1 edge.",
            "So that means you, right?",
            "It is some coefficient times X0 plus some coefficient times X1, where the sum of those coefficients is 1.",
            "You do the same for the other point.",
            "And now those barycentric coordinates tell you how to modify the edge weights.",
            "It's a very simple.",
            "We will choose some parameter T, they just increase the weight on this.",
            "Say if I have an edge from X1 to Y1 it gets multiplied by the coefficient for X one times the coefficient for Y1 and if you look at the picture you'll see the pattern.",
            "Once you know the pattern, it's a fairly easy exercise in geometry to show that none of the difference vectors are changing.",
            "Or is it LXS staying the same?",
            "And this just comes from the magic of barycentric coordinates, and the same proof works in higher dimensions.",
            "If you give me any two simplices that intersect, you represent the point of intersection in the barycentric coordinates of both, and then you'll show that you didn't want those simple, you didn't want the intersection.",
            "OK, so that's plenty."
        ],
        [
            "Now let me go back to trying to prove I'm not crazy, which is telling you a little bit about what we found when we used experiments on these datasets to try to solve classification, regression and clustering problems.",
            "So let me tell you that we used again the simplest regression rule that we knew which was published.",
            "I think 1st paper view at all.",
            "So let's say I give you some subset of the vectors S and I tell you the labels on them, and you want to figure out labels everywhere else.",
            "Well, what we do is we look at minimizing this Laplacian energy or Z transpose, LZ, or that's sort of this measure of smoothness looking for very smooth function.",
            "Subject to it getting the right values at the known vertices.",
            "So there's no mean.",
            "So yeah, no regularization or anything, we're going to pin down these nodes and then optimize.",
            "Then make the Laplacian as small as possible elsewhere.",
            "This is the natural generalization of the take the average of your neighbors rule when you just don't know one vertex.",
            "This is the right way to generalize it to many.",
            "And OK, so that's how we did regression for clustering.",
            "We of course through if there were two clusters, we put in an indicator vector.",
            "If there were multiple clusters, we use K indicator vectors."
        ],
        [
            "And this is the part I'll just tell you.",
            "What we did was ten fold cross validation.",
            "We had no parameters to train, we just have a graph in a rule to compare to what happens with the other constructions of graphs.",
            "Basically, on the 9/10 we weren't trying to evaluate on, we just gritted overall reasonable choices of values and used leave one out cross validation to see how well they didn't chose the best.",
            "Anne repeated this 100 times and came back and reported some results."
        ],
        [
            "So here is what we get on those datasets we found.",
            "I hope one can see I tried to show in bold.",
            "OK, I didn't put down the hard graph, I just put down the .1 soft graph 'cause the hard graph is similar.",
            "Sometimes it's a percent or two off, but it wins whenever the soft graph wins compared it to two types of graphs, 01 choosing K nearest neighbors to choose the edges, the other using distance threshold nodes are close enough.",
            "Together we put down an edge.",
            "And you know, we tried all sorts of values of K and the distance threshold and all sorts of different parameters.",
            "Sigma for weighted graphs and I just tried to put in bold the cases where are with each line in bold, whichever came out best, yes.",
            "Oh, that is a great question.",
            "It's one of my open questions repeated.",
            "The question is roughly, what if another point comes in later?",
            "Because particularly poignant, because if you take a look for other ways of building graphs, or at least this one, there are some great ways of doing something reasonable when a new point comes in, we don't have a great way of doing that yet.",
            "I would love to come up with one, I mean other than adding it to the graph, which doesn't take as much time as you'd think, as we can do a warm start on solving.",
            "Our quadratic program, but there should be something better.",
            "We don't have it yet.",
            "So yeah, we've got we're not there yet.",
            "One thing I want to point out is, you know, we don't always win.",
            "That's not surprising.",
            "I actually thought that these graphs were going to do horribly when you choose the threshold compared to K nearest neighbors, I was wrong.",
            "They don't have a win in any column, but it is worth pointing out there are times when they really beat.",
            "Let's see if we can see that.",
            "There are times when they really beat like the K nearest neighbor graph.",
            "That sort of surprised me.",
            "I wasn't expecting that.",
            "So sometimes using a distance threshold helped.",
            "Of course you know we tried a lot of different parameter ranges, so you know when you keep going over parameters, it makes sense that something would win.",
            "I guess the other thing to point out is there are times when.",
            "I mean our constructions when they lose, they never lose by much when they win.",
            "Sometimes they win by a lot.",
            "Which is.",
            "Well, that I don't know if it's a lot OK Now, let's see if this thing will let me choose my.",
            "Page again OK for gresso."
        ],
        [
            "I also put in support vector machines of course, 'cause we wanted to compare against this thing that everyone tells me about.",
            "And yeah, we use Lib SVM that won a heck of a lot of the fights.",
            "I think that left us with two.",
            "And on a lot of these it.",
            "Still doing a lot better, and for that we just used the training procedure that they supplied with that again on all of the other data.",
            "I'm very curious if there is a way to tweak our construction.",
            "Maybe putting in a kernel to train somehow and see if we can somehow compete with support vector machines.",
            "I have no idea yet."
        ],
        [
            "OK, we also tried regression experiments on the regression datasets we had.",
            "You know these.",
            "I you can also see we did fairly well and again we used Lib SVM and even on one data set abalone after running it for a weekend and it didn't stop.",
            "We just gave up.",
            "I don't know why abalone seems like a messy data set, but also took a heck of a long time for us to compute our graphs on that data set, but certainly less time than running Lib SVM would have take to train it.",
            "Very."
        ],
        [
            "Also, as mentioned quickly, some clustering experiments for clustering data, so we put up K means, which again wins.",
            "Most of the fights.",
            "We tried clustering the same way in Jordan wasted, so they chose a variation of this K nearest neighbors construction, get a graph, take a look at a couple of its eigenvectors, project into these eigenspaces and run K means there instead of in the original space.",
            "And we tried the same thing.",
            "And there's this one question, which is, how many eigenvectors do you choose in their paper?",
            "They suggested choosing a number of eigenvectors, equal number of clusters you want, and there's some motivation for that.",
            "If your clusters are all really far apart, but we didn't like that idea in general, so we also said you know what if we choose the number of dimensions in the eigenspaces to project into a little bit more Intel."
        ],
        [
            "Elegantly and then it turns out you can wind up doing much better, and if there's.",
            "I mean, often these things are all comprable.",
            "The only points where I notice we seem to get something a lot lowers for this wine data set.",
            "It's an order of 10 lower than a lot of the others in factor of 10.",
            "There's some others that are sort of interesting.",
            "But in this at least, I think goes back to arguing.",
            "These ideas are not crazy.",
            "Which was the main goal here so far."
        ],
        [
            "And OK, I had a slide on how we choose those Spectra.",
            "OK, this is a little fuzzy.",
            "I'll just mention briefly.",
            "John Kelner is the master of this right now.",
            "We had some questions about.",
            "We take our how do we choose which eigenvectors to use?",
            "Well, we take the eigenvectors, order them by the eigenvalue, project our data set back onto the eigenvectors.",
            "Anyway, so the idea here was just we tried to take a look.",
            "If you take a look at projecting your vectors into the eigen space, you find that the first couple eigenvectors have almost all the projection and then it goes down and we just try to choose the point at which it really drops off.",
            "But we can argue about where it is, so this is not math yet.",
            "This is just an art, you know, like John and I can agree it's here, but I think it's there and he thinks it here.",
            "The answer is it doesn't matter very much.",
            "You get pretty much similar results.",
            "Whatever you choose.",
            "Actually, as long as it's a reasonable choice we found."
        ],
        [
            "But who knows?",
            "OK, so I want to quickly mention something about computing these.",
            "To do that, I want to point out that the Laplacian has a very nice form.",
            "We can represent the Laplacian as assigned vertex edge adjacency matrix times the diagonal matrix of the edge weights times that edge, vertex adjacency matrix transpose.",
            "So what is this matrix?",
            "The rows are indexed by vertices, the edges, the columns by edges, and for each edge of a one and a -- 1 in its column zeros everywhere else.",
            "It's sort of the sign in edge.",
            "It doesn't matter which way you put the ones in minus ones 'cause that all comes out when you multiply by the transpose on the other side.",
            "This form winds up being useful.",
            "What we can do with that is we."
        ],
        [
            "And rewrite this objective function.",
            "Maybe I will not go through the computation, but just point out that the quadratic objective function can be nicely turned into something, some matrix times our vector of edge weights, so you can write the quadratic very cleanly and see what it is, and that's useful if you want to compute anything with it.",
            "Also becomes useful because you see that this matrix is sparse.",
            "If your graph is sparse and that accelerates your computation, you try to preserve that during the computation OK."
        ],
        [
            "So here's the key points about computing this.",
            "Probably the useful thing to take away.",
            "We were trying to minimize a quadratic form, say for the soft graph, subject to two constraints, one of them being non negativity and the other this sort of quadratic constraint on the weighted degrees.",
            "So this is a little bit messy to make our lives easier, we threw the constraint into the object."
        ],
        [
            "Function.",
            "So instead we add on some factor times the term determining this constraint, we call that factor mu.",
            "There is a mu for which you will get the optimal solution.",
            "Well, first you can realize that the optimal solution that inequality is always inequality.",
            "And you can do a search overview to find the right value that gets you that.",
            "That sounds like you're solving many programs, but once you've solved one, solving the next one is much easier.",
            "So it's actually not so bad.",
            "All the work is solving the first one.",
            "While this makes our life easy, is now we've got a non negative least."
        ],
        [
            "Where's problem?",
            "So if you take a look at what this problem is, it is minimizing a quadratic objective function.",
            "Subject to some non negative terms.",
            "Oh well, I guess it's not quadratic yet.",
            "We have to add in some slack variables.",
            "OK so we add in some slack variables S here.",
            "Then you actually get a non negative least squares problem.",
            "So just WNS have to be non zero and you want to minimize this term squared an actually there's pretty good off the shelf codes for solving these breeders.",
            "Used matlab's not negatively squares routined an that works much better than their general quadratic programming solver.",
            "So that's how we saw."
        ],
        [
            "Of these things, there's one more detail.",
            "Again, if you have a graph with N nodes, there could be N. Choose two possible edges if an is 1000, that means you're dealing with about 500,000 possible edges, and then solving a quadratic programming that many variables sort of painful.",
            "But we know we're looking for sparse solutions.",
            "So what we do is we restrict our search to sparse solutions.",
            "We start out looking within a subset of the edges.",
            "Then when you get a solution, you can quickly check if it is optimal.",
            "Given a solution, I mean not negative least squares problems are really nice if when you if any variable is zero in the solution, that means that the gradient of the objective function has to be negative.",
            "So you just take a look after you come up with a solution on the subset of the edges that all the edges you didn't use.",
            "You say no is the objective function negative?",
            "If it is, we're done.",
            "If it's not, we put some of those edges back in and solve again.",
            "But that's how we actually managed to solve these things relatively quickly."
        ],
        [
            "Here I've put up a chart of our computation time.",
            "In seconds on an ordinary workstation, so you know none of these took too long though, at least for the soft graph.",
            "The hard graft takes a lot longer, and that's because we can't use quite as nice.",
            "It's not a non negative least squares problem.",
            "But at least now the worst of them that we tried abalones.",
            "Still, you know less than half an hour.",
            "So we could actually compute these things, and we've really barely scratched the surface of how to compute these quickly.",
            "We just tried to come up with code that would suffice to do some experiments to this set of experiments.",
            "If people decided they really wanted faster code, there are a lot of other things one can do.",
            "OK, hopefully I've."
        ],
        [
            "On fast enough to talk about, one of my favorite topics.",
            "Sparse solutions in higher dimensions.",
            "OK, so this is back to theory.",
            "One natural questions again in high dimensions.",
            "Can there be a sparse solution?",
            "Well, again, as I said, if I gave you the points, the corners of regular simplex, there is a solution that's not sparse, but is there an approximately optimal solution that sparse and in some sense, yes.",
            "So what we can prove right now?",
            "Is it for any graph G you give me?",
            "There exists a graph, I'll call it G~ The average degree is like 16 over epsilon squared so that the quadratic form in the Laplacian squared and in the approximate Laplacian squared is the same for all vectors approximately.",
            "When I say approximately the same.",
            "Oh darn, I'm sorry, this square root shouldn't be there.",
            "It's most epsilon times X transpose LX times.",
            "We've got this annoying term.",
            "Here comes in the form of al times the norm of X. I'll mention that in a moment what that means, at least if these things are not too small.",
            "The objective function value is the same if they're really small, our approximation falls off an using it to get any interpolation, solve any regression problem, at least in the leave one out.",
            "Since you get similar answers with this approximate Laplacian.",
            "And I don't."
        ],
        [
            "Point out this is not what we want.",
            "What we want is something really relative error.",
            "We would like this to be at most epsilon times X, transpose, Laplacian squared X.",
            "We don't know how to do that yet.",
            "Instead, we have this ugly term.",
            "Let me point out.",
            "Doing this is actually a very interesting thing.",
            "Gary Miller is going to talk tomorrow about Linear equation solvers and being able to do this would be very useful for coming up with a V cycle and linear equation solvers.",
            "Something comes out of multigrid would have a lot of other applications.",
            "We don't know how to do it.",
            "But there's some hope.",
            "I'll tell you why there's hope, yeah.",
            "Because that's what our objective function is in right now.",
            "Oh, I'm sorry.",
            "L Yeah, then this is.",
            "This is what we needed to optimize and for the V cycle.",
            "I don't know if Gary will tell us, but let me take that offline.",
            "Let me see I'm almost to the finish.",
            "Let me try to do it quickly.",
            "The reason we've got this ugly bound is because proving this theorem is like driving in a screw and we're using a sledgehammer.",
            "You know it gets it most of the way in, but it makes a mess.",
            "On the other hand, I made the sledgehammer, so I want to tell you a little bit about the sledgehammer.",
            "So the sledgehammer is the same right theorem, except not for Laplacian squared."
        ],
        [
            "Josh Batson and Nikhil Srivastava.",
            "I proved this.",
            "So we can say that for any graph G There exists a sparse graph G~ so that we neglect their laplacian's, they approximate each other in a relative sense on all vectors.",
            "So the quadratic form in the original Laplacian is within 1 minus epsilon, one plus epsilon of the quadratic form in the approximate Laplacian, no matter what vector you put in.",
            "And we can do this with four N over epsilon squared edges.",
            "And I should mention that four is within a factor of two of optimal.",
            "If you could get two over epsilon squared, that's the Ramanujan bound and we know you can't beat that.",
            "I actually think you can get there too, but we don't know it yet, but this tells you that you can get really good approximations and they exist.",
            "This is a polynomial time."
        ],
        [
            "Function, so I've been spending the last few years on this, so if you want faster constructions you can you gotta use more edges, essentially N Logn edges.",
            "We can do it with about.",
            "Loggins solutions of linear systems of linear equations in the Laplacian.",
            "Or if you really don't want to do anything like that, you can get these horrible but still nearly linear bounds that Shanghai Tang and I got a few years ago.",
            "OK, that was the quick thing.",
            "I wanted to say about sparsification.",
            "If I have a minute.",
            "When I just tell you quickly about these other graphs that Lovasz constructed, and then I'll finish, this is."
        ],
        [
            "Thing that I spent a long time thinking about is part of the motivation for this.",
            "Solo bus in this paper called Steinitz representation in the colon, divergent number.",
            "Please excuse my pronunciation.",
            "Considers for well.",
            "OK, he just considers sets of points that are on the convex Hull or their extreme points of a convex polytope, so they should all.",
            "None should be a linear combination of any others, and he considers making matrices that aren't exactly laplacian's.",
            "The Matrix does have the form of a diagonal minus an adjacency matrix, but the diagonal matrix doesn't.",
            "I'll call it P because it's not the matrix of degrees, but it is diagonal and non negative.",
            "But what he did is he looked at trying to make.",
            "M X = 0 so that means he tried to make this matrix so that actually the nullspace were exactly the coordinate vectors.",
            "So in some sense, rather than trying to just make something that small on the coordinate vectors is making the nullspace accordant factors and having only one negative eigenvalue exactly one negative eigenvalue.",
            "And he could prove this was possible.",
            "OK, so he proved it for 3 dimensional for points in 3D, but you can prove it in higher dimensions as well if the points all lie on the convex Hull of a polytope.",
            "And the interesting thing is you will only have edges in the graph for edges in the polytope.",
            "So a will really only be non 0 where there are edges in the polytope.",
            "And for me this is a very nice way of capturing.",
            "A set of points or a making another matrix that captures a set of points.",
            "The annoying thing is that construction really depends upon using the combinatorial structures of the polytope, so I don't really know how to go about it in general.",
            "I mean, it's easy to satisfy this condition, making you know matrix which the coordinate vectors in the null space getting just one negative eigenvalue is a little trickier, so I don't know if this can be generalized at all, but it was one of our motivations in coming up with this construction that we have here.",
            "OK, so let me start with open questions."
        ],
        [
            "First, the main one is.",
            "Are there other natural graphs or useful graphs to associate with the set of points?",
            "We've got one proposal, it's got some interesting properties, but it really feels like it's not the right one yet.",
            "And I say that because we've got two.",
            "You know, there's a hard grafana soft graph, and neither feels quite right.",
            "There's probably something better.",
            "And other questions.",
            "Other interesting ways to parameterise or modify our construction.",
            "Whenever I do these experiments and classification or regression, I'm jealous of these other algorithms where we can try all sorts of parameters and see what works best.",
            "OK, it's nice that our algorithm doesn't have any parameters, but if I wanted to just do better in those experiments, a parameter would probably help me.",
            "So simple things we don't know, as I say, are graphs connected or unique.",
            "We really don't know.",
            "So as I say, I really do.",
            "There exist sparse approximate solutions in all dimensions.",
            "I really want relative approximations, but which I mean getting the squad rat the Laplacian squared right now say that is actually a deep question that would have a lot of implications and other places.",
            "So really, can one sparsified the square of the Laplacian.",
            "And the other question was asked earlier is OK if someone comes along and gives me points that were I didn't have originally.",
            "How can I do regression for them?",
            "How can I interpolate for them?",
            "I'd like to do something better than just adding them to the graph, 'cause that could require changing the weights of every single edge.",
            "I'll stop there, thanks.",
            "Yeah, I'm from.",
            "Inverse of your problem.",
            "So, given given a graph, maybe give him spinning.",
            "Yes.",
            "Right or tots?",
            "How to draw a graph is a famous paper.",
            "Yes Sir, like stop.",
            "Yeah, so yeah.",
            "So what?",
            "Yeah CGL or doing is there like right you can view it as they're nailing down nodes certain places in the letting rubber bands be the edges of given the strength, proportional the weights and then just letting it relax and getting the solution.",
            "Yeah I guess The thing is if you start out with a graph all those things are great.",
            "But I said my goal was to get to the graph and the way I've got to do that is you know people in machine learning seem to keep giving me point sets so I've got to get a graph somehow and that's what we were trying to do.",
            "Yeah, naughty.",
            "It doesn't work.",
            "Yes.",
            "Oh OK, so can.",
            "OK.",
            "So constraining the degrees to be one is natural and but it also makes some ugliness.",
            "For example, I really like the fact that see if I can draw it for points in the line right now.",
            "We get a path graph, but if you constrain the degrees to be one, you just cannot get this graph out, because if I force the degree of this to be one in the degree of that to be one, the degree of that's going to be 2.",
            "So if I put points in the line, you know.",
            "You get things that I find sort of unnatural, like that is the graph instead, so that I don't know.",
            "But it is a reasonable idea and we certainly did a lot of experiments with solving for degrees, constrained to be near 1.",
            "Both upper and lower bounds.",
            "Before we figured out that we could actually just penalize degrees less than one, and you get pretty much similar results so.",
            "Yes.",
            "Oh, if you.",
            "OK, so let's see.",
            "So if we just go for linear combination rather than like affine combination or weighted combination, then strange things happen.",
            "We did try.",
            "I mean we've tried played around with both of these.",
            "The answer is, you know fixing all degrees to be one or near 1 is not so bad.",
            "I just don't like it.",
            "'cause I like it when my points are in the line I get back a path graph.",
            "The other one I remember produced all sorts of degenerate solutions.",
            "What if you try?",
            "If you don't sort of restrict to be a weighted average, but you let it be just a general linear combination of your neighbors, so it seems like that would make sense.",
            "'cause if your points, let's say in the Pentagon, then you could exactly get a point.",
            "But it does worse things in other graphs, so but I don't know.",
            "I do not remember exactly what went wrong, I just remembered gave some strange nastiness.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think that works.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To begin, I should tell you this is joint work with John Kellner, who's back at Yale furiously trying to finish writing his thesis.",
                    "label": 0
                },
                {
                    "sent": "Sorry, not jungle let Sam Dite furiously, writing a thesis in John Kellner, whose at MIT.",
                    "label": 0
                },
                {
                    "sent": "Let me begin by saying that none of us work in machine learning, so I don't know this audience very well.",
                    "label": 1
                },
                {
                    "sent": "Please be prepared to stop me if I say anything that is really unfamiliar or doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "Also, let me just give you a little background on me to explain this talk, so I grew up in the theoretical computer science community, which means that I spent a lot of my Graduate School years learning about graphs and graph algorithms, and in the last and so for me, the 1st order object you always go to is a graph.",
                    "label": 0
                },
                {
                    "sent": "In the last few years I've realized there are a lot of people for whom inputs actually aren't graphs.",
                    "label": 0
                },
                {
                    "sent": "There are sets of vectors.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know these vectors are different coordinates represent features or something like that.",
                    "label": 1
                },
                {
                    "sent": "I'm OK this talk in this paper is part of my attempt to take those vectors and turn him into a graph, which I can then think about, and that's something I can deal with.",
                    "label": 0
                },
                {
                    "sent": "Is this going to change pages?",
                    "label": 0
                },
                {
                    "sent": "No, that's bad.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me give concretely the question we're asking and I'm just going to propose one solution.",
                    "label": 0
                },
                {
                    "sent": "There are probably other interesting solutions, but I think it's a start.",
                    "label": 0
                },
                {
                    "sent": "So imagine I have a bunch of collection of vectors.",
                    "label": 1
                },
                {
                    "sent": "Here I call him X, one through XN, living in D dimensions, D will always be the dimensions for me.",
                    "label": 0
                },
                {
                    "sent": "I want to associate a natural graph with these vectors.",
                    "label": 0
                },
                {
                    "sent": "So for me the vertex set will be the vectors, or you can identify it is just one to end 'cause there in vectors.",
                    "label": 0
                },
                {
                    "sent": "Now what do I mean by natural?",
                    "label": 0
                },
                {
                    "sent": "Well, first it should somehow be theoretically well motivated.",
                    "label": 0
                },
                {
                    "sent": "There should be some reason why I'm choosing this graph.",
                    "label": 0
                },
                {
                    "sent": "For this audience it should help you solve standard machine learning problems like classification or regression or things you might want to do with these vectors.",
                    "label": 1
                },
                {
                    "sent": "For me it is specially if interesting combinatorial properties that for me I sort of want.",
                    "label": 0
                },
                {
                    "sent": "You know the a natural graph?",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Think of it as gods graph.",
                    "label": 0
                },
                {
                    "sent": "It should be interesting somehow.",
                    "label": 1
                },
                {
                    "sent": "But OK, maybe I have some different constraints then God.",
                    "label": 0
                },
                {
                    "sent": "It should be efficiently computable.",
                    "label": 0
                },
                {
                    "sent": "I need to be able to compute the darn things.",
                    "label": 0
                },
                {
                    "sent": "It should be sparse.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "I don't want to take the complete graph with some strange weights.",
                    "label": 0
                },
                {
                    "sent": "I really want the graph notion to be important, which means that there shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't be overwhelmed with edges.",
                    "label": 0
                },
                {
                    "sent": "I have one other goal you'll see which is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that helps solve theoretical computer science problems on the vectors.",
                    "label": 1
                },
                {
                    "sent": "I'm not using it for that yet, but that's part of what's going on in the background here and why.",
                    "label": 1
                },
                {
                    "sent": "I'll be very interested in combinatorial properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have a big out.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fine, and we'll see what we get through Main Liang to start out by telling you how people generally associate graphs with vectors.",
                    "label": 0
                },
                {
                    "sent": "Probably this is familiar to a lot of you.",
                    "label": 0
                },
                {
                    "sent": "Then try to explain our proposal for one natural way of doing this and then I will explain reinterpret that proposal in terms of laplacian's.",
                    "label": 0
                },
                {
                    "sent": "It will simplify notation alot through the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will tell you about some interesting combinatorial properties that these graphs have.",
                    "label": 1
                },
                {
                    "sent": "I'll present some experiments showing what sort of results we get when we use them to solve classification, regression or clustering problems that will at least convince you.",
                    "label": 0
                },
                {
                    "sent": "I hope that I'm not crazy.",
                    "label": 0
                },
                {
                    "sent": "You'll see you do get some good answers to these problems using these graphs.",
                    "label": 1
                },
                {
                    "sent": "I'll talk about how we can compute them when I say efficient computation that is going to be heuristic rather than theoretical meaning, we have an algorithm we can prove when it stops, it does give you the correct answer and it stops relatively quickly, but I can't bound the running time nearly as nicely as I'd like to.",
                    "label": 0
                },
                {
                    "sent": "If there's time I will talk about the notion of call approximate sparsity, which I guess I should just define with one type of sparsity that's absolute earlier.",
                    "label": 0
                },
                {
                    "sent": "Another type of sparsity later.",
                    "label": 0
                },
                {
                    "sent": "I don't want to mention another approach to this problem that comes out of work of Lo Vas, which is really part of what motivated me towards doing this and it could provide another very natural approach to this problem, but we can't work it all out yet.",
                    "label": 1
                },
                {
                    "sent": "And they're going to be lots of open questions, which I'll repeat all at the end because we barely understand what we're talking about so far.",
                    "label": 0
                },
                {
                    "sent": "So why don't I start out with the staff?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyways, people usually make graphs when you're given a collection of vectors.",
                    "label": 0
                },
                {
                    "sent": "I described it as you need to choose what are the edge is going to be an?",
                    "label": 0
                },
                {
                    "sent": "What are the weights?",
                    "label": 0
                },
                {
                    "sent": "'cause usually you want weighted graphs.",
                    "label": 0
                },
                {
                    "sent": "So there are two natural ways to go about each of these things.",
                    "label": 0
                },
                {
                    "sent": "For Edge is one of the standard things to do is fine for every vertex, it's K nearest neighbors for some K Anne, let those be the edges.",
                    "label": 1
                },
                {
                    "sent": "Another very natural thing people have done is they choose some distance threshold, I'll call it Delta.",
                    "label": 0
                },
                {
                    "sent": "And can put an edge between every two vertices if their distance from each other is less than Delta.",
                    "label": 1
                },
                {
                    "sent": "I think we saw both of those at some point today already there.",
                    "label": 0
                },
                {
                    "sent": "The questions of what you're going about weights some people like to be agnostic.",
                    "label": 0
                },
                {
                    "sent": "They say that you know the distance really doesn't mean very much, so we're going to go with unweighted graphs, which is very reasonable choice.",
                    "label": 0
                },
                {
                    "sent": "Another thing to do if you're going to use weights, I think the Canonical thing to do is to make them exponential.",
                    "label": 0
                },
                {
                    "sent": "So usually the standard is exponential in the distance between the vertices squared.",
                    "label": 0
                },
                {
                    "sent": "Well, inverse exponential divided by two Sigma squared, and then you've got to think about how you're going to choose Sigma.",
                    "label": 0
                },
                {
                    "sent": "But what the heck, you already had to think about how you're going to choose K and how you're going to choose Delta, and you can often do some experiments to find what's a reasonable value.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the standard ways of choosing graphs.",
                    "label": 0
                },
                {
                    "sent": "They have some very nice features.",
                    "label": 0
                },
                {
                    "sent": "One, they're really easy to compute.",
                    "label": 0
                },
                {
                    "sent": "This is just a prescription for where to add edges and also at least we've learned if you have a discretization of a manifold and you just grew ties, it nicely enough.",
                    "label": 0
                },
                {
                    "sent": "These give you something natural and through other graphs that give you other natural things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to do things without manifolds, so our whole motivation is going to be completely different.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'm just going to give you the formula for this proposal, not because I expect everyone to understand it right now, but some of you are impatient and probably just want to see it.",
                    "label": 0
                },
                {
                    "sent": "Then I'll motivate it.",
                    "label": 0
                },
                {
                    "sent": "Going to choose edge weights to minimize the following quantity.",
                    "label": 1
                },
                {
                    "sent": "It is a sum over all vertices, so I goes from one to N. Let me start with the term over.",
                    "label": 0
                },
                {
                    "sent": "Here I write the sum over J squiggle I.",
                    "label": 0
                },
                {
                    "sent": "That's if J is a neighbor of node I.",
                    "label": 0
                },
                {
                    "sent": "We take a weight of the edge AIJ times the vector for vertex J.",
                    "label": 0
                },
                {
                    "sent": "All of that gets subtracted from vert vector.",
                    "label": 0
                },
                {
                    "sent": "I times its weighted degree, so DI means the weighted degree of node I.",
                    "label": 0
                },
                {
                    "sent": "That's just the sum of these weights.",
                    "label": 0
                },
                {
                    "sent": "This is a symmetric graph, so the weight from node eyes to J is the same as the weight from node Jade I.",
                    "label": 1
                },
                {
                    "sent": "And we want all of the weighted degrees to be at least one.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the proposal.",
                    "label": 0
                },
                {
                    "sent": "I'm here is 1 example of a.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point set and this was the graph that we got out.",
                    "label": 0
                },
                {
                    "sent": "I truncated the digits, they didn't actually hit exactly those numbers, but it's good enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the proposal.",
                    "label": 0
                },
                {
                    "sent": "So for those who are impatient, you now know what it is for the better.",
                    "label": 0
                },
                {
                    "sent": "Let me motivate it, and then we'll come back to it.",
                    "label": 0
                },
                {
                    "sent": "So my motivation?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And is this?",
                    "label": 0
                },
                {
                    "sent": "Let's consider solving a regression problem on a graph.",
                    "label": 1
                },
                {
                    "sent": "Meaning you have some function or a regression problem on the vectors using the graph.",
                    "label": 0
                },
                {
                    "sent": "So you have some function, say F of the vertices.",
                    "label": 0
                },
                {
                    "sent": "Let's say it gives for each vector XI.",
                    "label": 0
                },
                {
                    "sent": "It gives a label why I which I'll treat is a real number now.",
                    "label": 0
                },
                {
                    "sent": "Or you could make zero or one or something like that.",
                    "label": 0
                },
                {
                    "sent": "Let's think of now just the simplest problem.",
                    "label": 0
                },
                {
                    "sent": "I give you a weighted graph and I ask you to please try to figure out why I is and I'll give you the value of Y it every other vector.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you YJ for every other vector.",
                    "label": 0
                },
                {
                    "sent": "And I'll give you a weighted graph.",
                    "label": 0
                },
                {
                    "sent": "Maybe I no longer tell you what the vectors are.",
                    "label": 0
                },
                {
                    "sent": "The question is, if I just give you the weighted graph, what is the most natural rule you could come up with to guess the wise?",
                    "label": 0
                },
                {
                    "sent": "And I maintain that the most natural rule to guess why I given the value of YJ at every other vertex is take a look at its neighbors.",
                    "label": 0
                },
                {
                    "sent": "And take a look at the weighted average of the value of Y at its neighbors.",
                    "label": 0
                },
                {
                    "sent": "So that is literally some over the neighbors of Vertex I.",
                    "label": 0
                },
                {
                    "sent": "The value YJ times the weight in 1 / D. I normalizes this so the you get a weighted average is the sum of the weights.",
                    "label": 0
                },
                {
                    "sent": "So I think that to me is the most natural rule for given a weighted graph and given a bunch of real numbers associated with the vertices, guessing a number at the one vertex he didn't know.",
                    "label": 1
                },
                {
                    "sent": "OK, I'm going to make that my interpolation rule.",
                    "label": 0
                },
                {
                    "sent": "In this case, let's take a look at what the error is.",
                    "label": 0
                },
                {
                    "sent": "Let's consider you know leaving out every single vertex intern.",
                    "label": 0
                },
                {
                    "sent": "I believe when I started trying to read machine learning papers that called this leave one out cross validation at least want to measure the error of that.",
                    "label": 0
                },
                {
                    "sent": "So I'll sum over every single vertex.",
                    "label": 0
                },
                {
                    "sent": "If this is my error, this is what why I actually is.",
                    "label": 0
                },
                {
                    "sent": "This is the guess I will get not using iy but using all the other data will square that and take their sum.",
                    "label": 0
                },
                {
                    "sent": "OK, now how am I going to use this idea to form a graph?",
                    "label": 0
                },
                {
                    "sent": "The question is what's my data?",
                    "label": 0
                },
                {
                    "sent": "What I know is I know a vector for every single vertex.",
                    "label": 0
                },
                {
                    "sent": "I want to choose my graph so that I at least do well with respect to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those vectors.",
                    "label": 0
                },
                {
                    "sent": "So meaning let me take every single vector XI and I'll write, superscripts indicate the coordinates, I'll call it XI soup one through excise soup D. I maintain that if we're going to be able to use these vectors to solve regression problems, then you know the function F had better be pretty nice in terms of the coordinates and I'm just going to start out thinking about the nicest functions.",
                    "label": 0
                },
                {
                    "sent": "That's the coordinate vectors themselves.",
                    "label": 1
                },
                {
                    "sent": "So if I try to take a look at this procedure again, let's say I choose a weighted graph, what will my expected error or what my error be?",
                    "label": 0
                },
                {
                    "sent": "Summed over all vertices on the coordinate vectors well in the case coordinate vector, I'll get this term in here and then will sum the sum of squares over all K. So you can just, you know rewrite this.",
                    "label": 1
                },
                {
                    "sent": "Putting K inside and write it as the norm squared of sum over every vector XI minus the weighted average of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "Take a look at the norm of that squared.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to choose the graph to minimize this quantity.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, my idea is if I just wanted to solve the regression problem on the coordinate vectors, I want to choose the graph that would solve that best for me.",
                    "label": 0
                },
                {
                    "sent": "So that is the start of our idea.",
                    "label": 0
                },
                {
                    "sent": "This is a good point for where I should ask if I have any questions and if everyone understand if I'm speaking the right language really.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "Evil excuse me.",
                    "label": 0
                },
                {
                    "sent": "No, yeah, we can't necessarily find a graph for which this is 0.",
                    "label": 0
                },
                {
                    "sent": "Even just to think about, there can be boundary effects.",
                    "label": 0
                },
                {
                    "sent": "If I have a vertex which is on the convex Hull, you can't represent it as an average of its neighbors, and they'll always be some vertex on the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "OK, this is my motivation.",
                    "label": 0
                },
                {
                    "sent": "You might recognize that the initial expression I gave you a slightly different 'cause there's a problem with doing this.",
                    "label": 0
                },
                {
                    "sent": "Well, there are two problems.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first is if I try to optimize this, it leads to a non convex program.",
                    "label": 1
                },
                {
                    "sent": "That's because the weights are appearing both in weighted degrees down here and here.",
                    "label": 0
                },
                {
                    "sent": "And there's a worse problem.",
                    "label": 0
                },
                {
                    "sent": "Let me just give you an example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just consider the points arranged in a regular Pentagon.",
                    "label": 0
                },
                {
                    "sent": "Let me ask you to find the weights that are going to be best with respect to this objective function.",
                    "label": 0
                },
                {
                    "sent": "The answer is what you say.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It should be you just hook him up in a regular Pentagon and you could put weight one on.",
                    "label": 0
                },
                {
                    "sent": "And well, this doesn't match this invariant up to scaling, so this could be wait 1 two.",
                    "label": 0
                },
                {
                    "sent": "You'd get the same answer, doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "And if you take a look at what the errors are or meaning in a where are we getting error here?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this top node here, we're representing it as the average of these two, so you're sort of guessing it's here and you're getting the sum of the red line squared, but you get one for every single vertex.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a nice example.",
                    "label": 0
                },
                {
                    "sent": "What if I put a point?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the middle.",
                    "label": 0
                },
                {
                    "sent": "If I put a point in the middle, it becomes a little messier.",
                    "label": 0
                },
                {
                    "sent": "This is the solution you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet.",
                    "label": 0
                },
                {
                    "sent": "You'd like to put ones on each of these edges, but these things can be epsilon and actually the smaller epsilon is.",
                    "label": 0
                },
                {
                    "sent": "The lower this objective function.",
                    "label": 0
                },
                {
                    "sent": "Because in some sense, for this node I mean, these nodes here don't want to use that node to represent them.",
                    "label": 0
                },
                {
                    "sent": "They do better using just their two neighbors across here.",
                    "label": 0
                },
                {
                    "sent": "This node wants to be the center of all the others, and the fact that the weighted degree is zero.",
                    "label": 0
                },
                {
                    "sent": "Well, that expression doesn't care 'cause it's just taking weighted average of the neighbors.",
                    "label": 0
                },
                {
                    "sent": "So annoying thing about this graph is these epsilons go to zero and actually it can get worse.",
                    "label": 0
                },
                {
                    "sent": "You can make examples where there are many different epsilons going to zero at different rates.",
                    "label": 0
                },
                {
                    "sent": "Some can want to be squares of each other and really when you look at this program you're not really getting weights.",
                    "label": 0
                },
                {
                    "sent": "You're really figuring out how fast are things going to zero.",
                    "label": 0
                },
                {
                    "sent": "And I that's not something I really think like to compute with.",
                    "label": 0
                },
                {
                    "sent": "It's not a concrete graph, for me, it's just sort of how zero are they?",
                    "label": 0
                },
                {
                    "sent": "OK, so we need to fix this a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's how we're going to fix it.",
                    "label": 0
                },
                {
                    "sent": "We will view the weighted degree of a node as its importance.",
                    "label": 0
                },
                {
                    "sent": "And we will then instead of measuring how well the sort of regression instead of just measuring the regression error over all nodes, we waited by the weighted degrees of nodes squares.",
                    "label": 0
                },
                {
                    "sent": "So I just get rid of this expression instead and put the weighted degree in here and multiply it through.",
                    "label": 0
                },
                {
                    "sent": "So we get the weighted degree times a vertex minus the average of its neighbors, and that's essentially our proposal.",
                    "label": 0
                },
                {
                    "sent": "Again, with one caveat, you'll notice that of course this can be minimized by just setting all the weights to 0.",
                    "label": 0
                },
                {
                    "sent": "So we need to do something to prevent that.",
                    "label": 0
                },
                {
                    "sent": "Well, we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at two different solutions.",
                    "label": 0
                },
                {
                    "sent": "The first one is we say, OK, you've got to make the weighted degree of every node at least one.",
                    "label": 0
                },
                {
                    "sent": "That says you have to treat every node as being important.",
                    "label": 0
                },
                {
                    "sent": "If you make the weighted degree node of a node more than one, it tends to make this expression larger, so there's very little incentive to do it.",
                    "label": 0
                },
                {
                    "sent": "But sometimes there are few nodes for which it happens in practice.",
                    "label": 0
                },
                {
                    "sent": "The other idea is to say you can't have your degrees be too much less than one, but we allow some outliers.",
                    "label": 0
                },
                {
                    "sent": "By the way, this constraint will call the hard constraint.",
                    "label": 0
                },
                {
                    "sent": "I'll call this the hard graph.",
                    "label": 0
                },
                {
                    "sent": "The other way we looked at it is take a look at the sum over all degrees that are less than one of 1 minus that degree squared, and we say that can't be too big to say it's less than Alpha N will call that the Alpha soft graph.",
                    "label": 0
                },
                {
                    "sent": "So in this case you're allowed to give low degree to some nodes but not too low degree to too many.",
                    "label": 0
                },
                {
                    "sent": "OK, now I've actually given you our proposal for how we form these graphs.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No, not at this point.",
                    "label": 0
                },
                {
                    "sent": "At this point I've got a quadratic objective function being minimized, subject to non negativity constraints, and either of these makes it sufficiently determined.",
                    "label": 0
                },
                {
                    "sent": "Well, our weights always have to be positive.",
                    "label": 0
                },
                {
                    "sent": "Oh wait, maybe I've gone wrong, so this means there always is an answer.",
                    "label": 0
                },
                {
                    "sent": "I haven't guaranteed a unique answer yet.",
                    "label": 0
                },
                {
                    "sent": "Oh in.",
                    "label": 0
                },
                {
                    "sent": "Wasted.",
                    "label": 0
                },
                {
                    "sent": "Actually no, let me answer.",
                    "label": 0
                },
                {
                    "sent": "Mention that now there's a conjecture.",
                    "label": 0
                },
                {
                    "sent": "In every experiment we've ever done on real data, it is always unique and by looking at the KKT conditions of the solution, you can certify that.",
                    "label": 0
                },
                {
                    "sent": "However, there are examples where the solution is not unique.",
                    "label": 0
                },
                {
                    "sent": "If you have highly symmetric point sets and I'll actually prove that to you later.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you're stuck with that.",
                    "label": 0
                },
                {
                    "sent": "Had non uniqueness is sort of unavoidable, but it's our conjecture is that if you make an infinitesimally small perturbation to any point set, then if it's infinitesimally small random perturbation then this solution will be unique with probability.",
                    "label": 0
                },
                {
                    "sent": "One is our conjecture, but we can't prove it yet.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an example this I just took a random set of points in two dimensions and computed the .1 soft graph for them.",
                    "label": 0
                },
                {
                    "sent": "So I want to point out two things here.",
                    "label": 0
                },
                {
                    "sent": "Oh, and by the way, the thickness of the edges is proportional to the weights.",
                    "label": 0
                },
                {
                    "sent": "So the first thing I want to point out is this is a relatively nice looking graph which we were not a priority expecting when you set out to, say, minimize regression error or something like that, but the graph looks OK. Let me make another observation.",
                    "label": 0
                },
                {
                    "sent": "This graph is planar.",
                    "label": 0
                },
                {
                    "sent": "No edges between vertices cross.",
                    "label": 0
                },
                {
                    "sent": "That'll prove for you later in the talk.",
                    "label": 0
                },
                {
                    "sent": "It's actually not too hard, so for points in two dimensions, it does turn out to be planner.",
                    "label": 1
                },
                {
                    "sent": "And let me just also point out it's again, it looks like a relatively nice graph so that I'm sort of very happy with.",
                    "label": 0
                },
                {
                    "sent": "A natural question is what if you have two clusters that are far away from each other?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what we find is if I stick two clusters that are far away from each other, you might expect there would be no edge between them.",
                    "label": 0
                },
                {
                    "sent": "That's not the case.",
                    "label": 0
                },
                {
                    "sent": "There are going to be edges between them, they are just much thinner or lighter weight edges than the edges connecting the clusters themselves.",
                    "label": 0
                },
                {
                    "sent": "That said, we also don't yet know how to prove this graph is connected.",
                    "label": 0
                },
                {
                    "sent": "We think it is.",
                    "label": 0
                },
                {
                    "sent": "It always is, but can't prove that yet.",
                    "label": 0
                },
                {
                    "sent": "But I can prove the one statement I made to you a moment ago.",
                    "label": 0
                },
                {
                    "sent": "If you have two different clusters that are very far apart, we can prove that there is an edge going from one to the other.",
                    "label": 0
                },
                {
                    "sent": "That proof just breaks down as they start to get too close to each other.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of open questions here, so I said we barely understand what we're doing, but we've seen some interesting things so far.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another fun example here.",
                    "label": 0
                },
                {
                    "sent": "I just took 10 points at regular around the circle.",
                    "label": 0
                },
                {
                    "sent": "Have them, you know, have their distance to the origin and then have it again.",
                    "label": 0
                },
                {
                    "sent": "The one thing I want to end here, I've again drawn the weights on the edges.",
                    "label": 0
                },
                {
                    "sent": "This was the .1 soft graph so you can see that these nodes on the outside actually have much lower.",
                    "label": 0
                },
                {
                    "sent": "Weighted degree, then the nodes on the inside.",
                    "label": 0
                },
                {
                    "sent": "But one thing I want to point out here is at least the structure you get here.",
                    "label": 0
                },
                {
                    "sent": "You always get these rings even though the scales are changing greatly over the graph.",
                    "label": 0
                },
                {
                    "sent": "Which is sort of interesting because if you go at choosing something like, well, K. Nearest neighbors, I guess would probably get something like that, not even necessarily depending on how you set K. And if you definitely built a graph based on a threshold on distances well, by the time you start connecting these nodes, you have all sorts of connections in here.",
                    "label": 0
                },
                {
                    "sent": "So somehow this is doing something fairly natural, varying the graph at different scales when the points vary in different scales, yes.",
                    "label": 0
                },
                {
                    "sent": "About why you don't.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I will actually say I cannot give intuition.",
                    "label": 0
                },
                {
                    "sent": "I can give a proof which I will do.",
                    "label": 0
                },
                {
                    "sent": "I will give two or three.",
                    "label": 0
                },
                {
                    "sent": "I'll give two different proofs of that, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's not.",
                    "label": 0
                },
                {
                    "sent": "My intuition is not quick.",
                    "label": 0
                },
                {
                    "sent": "I admit, you know we were sort of surprised at first, so.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My understanding of this fact is more algebraic than intuitive so far.",
                    "label": 0
                },
                {
                    "sent": "Actually, if anyone can come up with a really nice intuitive explanation, I'd be psyched.",
                    "label": 0
                },
                {
                    "sent": "We don't have that yet.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I mentioned, the graph is not necessarily unique.",
                    "label": 1
                },
                {
                    "sent": "An I will show you later examples of point sets will not in two dimensions, but in higher dimensions, in which it's not unique, and that sort of has to be.",
                    "label": 1
                },
                {
                    "sent": "If you have a lot of symmetry, then you just can't expect uniqueness is I'll explain, but as I said, we conjecture that it's unique with probability one after an infinitesimally small perturbation.",
                    "label": 0
                },
                {
                    "sent": "I would love it if someone could prove that for us, we don't know how to get there yet.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is related to a lot of things that have been considered before.",
                    "label": 0
                },
                {
                    "sent": "I want to point out the relation to locally linear linear embeddings.",
                    "label": 1
                },
                {
                    "sent": "Locally, linear embeddings is another way of.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not really associating a graph with a set of vectors well the first they do associated graph, but they choose the neighbors and then they choose weights.",
                    "label": 1
                },
                {
                    "sent": "But the weights are not chosen to be symmetric, but there's still minimizing the same objective function.",
                    "label": 1
                },
                {
                    "sent": "So you can see this as being related 'cause it's the same objective function.",
                    "label": 0
                },
                {
                    "sent": "They just sort of choose what the neighbors are and then minimize the weights and don't have any symmetry in it.",
                    "label": 0
                },
                {
                    "sent": "This is also closely related to K means, so if I actually insisted that your graph was a collection of disjoint cliques all within each clique, having at least two vertices, and you look at the K means objective function, it's the exact same objective function that we have here.",
                    "label": 0
                },
                {
                    "sent": "Which is actually part of how we got started on this.",
                    "label": 0
                },
                {
                    "sent": "When Sam noticed that and pointed it out to me.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I need to describe this in terms of the graph.",
                    "label": 1
                },
                {
                    "sent": "Laplacian is 1 because I spend a lot of time thinking about graph laplacians and two it will simplify notation.",
                    "label": 0
                },
                {
                    "sent": "Out of curiosity, how many of you are familiar with the graph Laplacian?",
                    "label": 0
                },
                {
                    "sent": "Terrific almost everybody.",
                    "label": 0
                },
                {
                    "sent": "So just for those few others I will quickly say.",
                    "label": 0
                },
                {
                    "sent": "The graph Laplacian, we take it, you take the weighted adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "Say you subtract it from the diagonal matrix of degrees.",
                    "label": 0
                },
                {
                    "sent": "So for this graph here, where the vertices I've numbered.",
                    "label": 0
                },
                {
                    "sent": "This is the Laplacian matrix for it.",
                    "label": 0
                },
                {
                    "sent": "So say for the edge between vertex one and three, having weight .67.",
                    "label": 0
                },
                {
                    "sent": "That means that we have a negative .67 in row one, column three and symmetrically column one row 3.",
                    "label": 0
                },
                {
                    "sent": "The only thing that's really bad about this example is the degree of every node is 1.",
                    "label": 0
                },
                {
                    "sent": "So I have ones down the diagonal that does not happen in general.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry about that.",
                    "label": 0
                },
                {
                    "sent": "That's the drawback of using the same example twice.",
                    "label": 0
                },
                {
                    "sent": "Generally, the weighted degree appears here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, our objective function in terms of the weighted Laplacian is just the Frobenius norm squared of the Laplacian matrix times the data matrix or X.",
                    "label": 1
                },
                {
                    "sent": "So think of X as being this N by D matrix of your vectors.",
                    "label": 0
                },
                {
                    "sent": "You multiply that by the Laplacian on the squares of all the entries.",
                    "label": 0
                },
                {
                    "sent": "That's our objective function.",
                    "label": 1
                },
                {
                    "sent": "And what I should point out the start of being able to compute this or doing anything else with it is that the Laplacian itself is a matrix, is linear in the edge weights, so LX is linear in the edge weights, so this is a quadratic.",
                    "label": 0
                },
                {
                    "sent": "In the edge weights 'cause you're taking this linear things and squaring it so our optimization problem is fundamentally going to be one of quadratic optimization.",
                    "label": 0
                },
                {
                    "sent": "That said, I will explain later in the talk how you can make that quadratic optimization practical, but for now just keep in mind it's quadratic, which means it's convex and has all sorts of other nice properties.",
                    "label": 0
                },
                {
                    "sent": "I want to point out that the objective function I rejected earlier was the same thing you get for the normalized Laplacian, that is, it had.",
                    "label": 0
                },
                {
                    "sent": "I taken D inverse times L * X, That was the one that was non convex and at all these annoying zeros ideologically, that bothers me because I prefer the normalized Laplacian to the Laplacian but.",
                    "label": 0
                },
                {
                    "sent": "Reality doesn't let me use it.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone can fix this somehow.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let me tell you why there is a sparse solution, at least when you're at least in terms of the dimension.",
                    "label": 0
                },
                {
                    "sent": "So I will prove in a moment that you always have average degree at most 2 * D + 1, where D is the dimension.",
                    "label": 0
                },
                {
                    "sent": "So I mean, there can be cases where it's dense.",
                    "label": 0
                },
                {
                    "sent": "If I give you is your data set the corners of a regular simplex, then you're going to get the complete graph 'cause if the corners of the regular simplex or endpoints and dimensions there's nothing else you can do just by symmetry you've got to return the complete graph.",
                    "label": 0
                },
                {
                    "sent": "But if your dimension is lower, you don't.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is one way of seeing that.",
                    "label": 0
                },
                {
                    "sent": "Imagine fixing elex.",
                    "label": 0
                },
                {
                    "sent": "That is, fixing all the entries of this end by D matrix that imposes end times D linear constraints.",
                    "label": 0
                },
                {
                    "sent": "If I say fix it so I tell you what the values will be.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, you can think of that as fixing these difference vectors, which always come up in our analysis.",
                    "label": 1
                },
                {
                    "sent": "For each node I we have this term we call Delta I, which is DIXI minus the weighted average of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "Imagine fixing all of those.",
                    "label": 0
                },
                {
                    "sent": "You can also see that that's D * N linear constraints you're imposing.",
                    "label": 0
                },
                {
                    "sent": "Also imagine fixing the weighted degrees, who I changed the order an I didn't define you yet, but if W is the vector of weights of edges, there's a matrix that you can multiply it by.",
                    "label": 1
                },
                {
                    "sent": "That gets you the weighted degree of every node, so fixing the weighted degrees gives you another N linear constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "If you have N * D + 1 linear constraints, that's all we're imposing on the weights and that they be non negative.",
                    "label": 0
                },
                {
                    "sent": "So that means that there is actually a large vector space of solutions.",
                    "label": 1
                },
                {
                    "sent": "If you weren't worried about Nonnegativity.",
                    "label": 0
                },
                {
                    "sent": "Of the weights, but it turns out that if you have more than end times D + 1 non zero weights of edges.",
                    "label": 0
                },
                {
                    "sent": "Then you can find some vector in this vector space that's not all positive.",
                    "label": 0
                },
                {
                    "sent": "Well, actually doesn't matter because we can find some vector in this vector space.",
                    "label": 0
                },
                {
                    "sent": "To add or subtract from our current vector on the weights of edges.",
                    "label": 0
                },
                {
                    "sent": "And if you have two different solutions, one of 'em is all positive and the other one is different, you can subtract the different one off until some weight goes to 0.",
                    "label": 0
                },
                {
                    "sent": "Or you add some multiple of the other vector until some weights goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is actually not a property.",
                    "label": 0
                },
                {
                    "sent": "How to put it?",
                    "label": 0
                },
                {
                    "sent": "If I do that an I keep LX fixed, I'm not changing the objective function and if I keep the weighted degrees fixed I'm not changing my constraint whether or not my constraint is satisfied.",
                    "label": 0
                },
                {
                    "sent": "So it says that for any solution that you get, if you had more than N * D + 1 non zeros, there's a way to change it to make another entry zero.",
                    "label": 0
                },
                {
                    "sent": "I mean this isn't really special to our program.",
                    "label": 0
                },
                {
                    "sent": "This is a very general property in quadratic programming that just sort of comes from the form of it.",
                    "label": 0
                },
                {
                    "sent": "But you can just think of it as you know, keeping the objective function fixed is at worst end times D constraints and keeping the weighted degrees fixed is another end.",
                    "label": 0
                },
                {
                    "sent": "So as soon as you have too many nonzero entries that are all positive, you can make another 1 zero and.",
                    "label": 1
                },
                {
                    "sent": "So that tells us there is always a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "At least in low dimensions, yes.",
                    "label": 0
                },
                {
                    "sent": "Complementary.",
                    "label": 0
                },
                {
                    "sent": "Um interest, is it so the question was, is this related with complementary slackness?",
                    "label": 0
                },
                {
                    "sent": "And in some sense, pry think the answer is probably, but I'd have to think about it to give you a good answer.",
                    "label": 0
                },
                {
                    "sent": "And I guess I said OK, so that is at least one way of saying that in low dimensions you get a sparse solution.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you take a look once I've given you end times D + 1 edges, that's actually I want to point out average degree 2 * D + 1.",
                    "label": 1
                },
                {
                    "sent": "Because every edge touches two vertices, so we're going to average three 2 * D + 1 An.",
                    "label": 0
                },
                {
                    "sent": "I will give a theorem if there's time later sort of saying there's always an approximately sparse solution in any dimension, I think there will be time for that, I'm not sure, but we don't know about a not.",
                    "label": 1
                },
                {
                    "sent": "Well, there's not necessarily an optimal solution in a dimension, but there's always an approximate sparse one, but that's a theorem that needs to be improved.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are some examples on some real data.",
                    "label": 0
                },
                {
                    "sent": "We grabbed a bunch of data from the UCI repository and from the web pages of the LIBSVM folks maintain and I've just listed all the datasets we grabbed here.",
                    "label": 0
                },
                {
                    "sent": "The type of problem people were looking at the number of vectors, the dimension, and the average degree we get for the hard and soft graphs, so it's almost always less than two times.",
                    "label": 0
                },
                {
                    "sent": "It's always less than 2 * T + 1.",
                    "label": 0
                },
                {
                    "sent": "It's often closer to the dimension.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the average degree of the graph is a lot lower.",
                    "label": 0
                },
                {
                    "sent": "So I want to point.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At a few examples on these datasets, the average degree is really much much lower than the dimension.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That makes me think that maybe this data is in some way low dimensional, but I have no way of making that rigorous yet.",
                    "label": 0
                },
                {
                    "sent": "But I would love to get a notion of the natural dimensionality of some data and try to show that that is related to the average degree of the graphs that we get, but that's an open problem.",
                    "label": 0
                },
                {
                    "sent": "We don't know what to do with that yet.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me just quickly take some profits here and show you how you can get graphs for which it can't possibly get datasets for which our graphs cannot possibly be unique.",
                    "label": 0
                },
                {
                    "sent": "If you have enough symmetries, you're not going to get uniqueness.",
                    "label": 0
                },
                {
                    "sent": "So for example, take a look at all the vectors in the hypercube, so 01 to the but not all of them.",
                    "label": 0
                },
                {
                    "sent": "Take all of them of even parity.",
                    "label": 0
                },
                {
                    "sent": "You take all the vectors in the hypercube of even parity.",
                    "label": 1
                },
                {
                    "sent": "There's a whole lot of automorphisms of those you know, permute the dimensions of the hypercube and spin your hypercube around.",
                    "label": 0
                },
                {
                    "sent": "Because we have a quadratic program and it's convex.",
                    "label": 1
                },
                {
                    "sent": "If you have any two solutions and add them together, you get another solution.",
                    "label": 1
                },
                {
                    "sent": "And so take all of these automorphisms of your point set onto itself and some over all of the solutions that you get.",
                    "label": 0
                },
                {
                    "sent": "If you do that, you can show that your graph has to have degree at least D, choose two.",
                    "label": 1
                },
                {
                    "sent": "Actually, in this case it will be exactly choose to, but you can prove it has to be at least D. Choose two.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I proved you a few moments ago that there always is a solution in which the average degrees at most 2 * D + 1.",
                    "label": 0
                },
                {
                    "sent": "So once D is sufficiently big, probably you get here five or six these cross you know there's not a unique solution because there's a symmetric solution and there's a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as thinking that symmetry is in some sense the enemy of sparsity.",
                    "label": 0
                },
                {
                    "sent": "If you have really highly symmetric point sets, if you want a symmetric solution, you can have it, but you can't, or you can have a sparse solution, but you can't necessarily get both.",
                    "label": 0
                },
                {
                    "sent": "I've learned not to let this bother me, but.",
                    "label": 0
                },
                {
                    "sent": "Part of me wants the symmetric solution, but computationally the sparse solution makes life a lot easier.",
                    "label": 0
                },
                {
                    "sent": "OK, let me talk about.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parity, so as I mentioned before in 2D, the point sets are planar.",
                    "label": 0
                },
                {
                    "sent": "This to me again, if the graph is planar, there's just a crazy thing to observe.",
                    "label": 0
                },
                {
                    "sent": "So what I will prove you is actually there is always a hard and Alpha soft graph.",
                    "label": 1
                },
                {
                    "sent": "This planner and I say there is one because again, I don't know that their unique.",
                    "label": 0
                },
                {
                    "sent": "So again, planarity.",
                    "label": 0
                },
                {
                    "sent": "If you don't think about graphs, just means that there are no edges crossing like this.",
                    "label": 0
                },
                {
                    "sent": "I will tell you how the proof goes before I do that, let me just mention there are a few other generalizations of this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also prove that there is no triangle containing a point, and Gary Miller pointed out to me this morning that if you look at the proof in our paper, it also tells you that in higher dimensions the graph you get is a simplicial complex.",
                    "label": 1
                },
                {
                    "sent": "So there's something interesting going on here.",
                    "label": 0
                },
                {
                    "sent": "K here's how you wind up proving this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, if you give me any situation like this.",
                    "label": 0
                },
                {
                    "sent": "We're going to prove to you, at least for now, that if you take a look at a graph with a maximum, if there are many solutions, take a solution that maximizes the sum of the weighted degrees.",
                    "label": 0
                },
                {
                    "sent": "We will prove that such a solution cannot be planned and cannot have crossings.",
                    "label": 0
                },
                {
                    "sent": "And the way we'll do that is we will evidence another solution with a higher sum of weighted degrees in the same objective function value.",
                    "label": 0
                },
                {
                    "sent": "And because we're increasing the sum of weighted degrees and keeping the objective function value the same, then that other thing wasn't a solution, at least of maximum sum of weighted degrees.",
                    "label": 1
                },
                {
                    "sent": "An if it's unique, then we don't need to worry about that business.",
                    "label": 0
                },
                {
                    "sent": "So here's how we do it.",
                    "label": 0
                },
                {
                    "sent": "If I take a look at a crossing, consider drawing this 4 sided shape.",
                    "label": 0
                },
                {
                    "sent": "I should remember the names of those.",
                    "label": 0
                },
                {
                    "sent": "I know my daughter in second grade learned this.",
                    "label": 0
                },
                {
                    "sent": "Again quadrilateral.",
                    "label": 0
                },
                {
                    "sent": "I think around those points.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to increase the weights and all these edges on the outside and decrease the weights on the edges in the inside.",
                    "label": 0
                },
                {
                    "sent": "In such a way that not only does the objective function not change, but actually those difference vectors I showed you before won't change, or really LX won't change and it will increase all of the weights.",
                    "label": 0
                },
                {
                    "sent": "Turns out there's a really easy way to do this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way you do it is you use barycentric coordinates.",
                    "label": 0
                },
                {
                    "sent": "So you take a look at the point Z where they cross and you write it as a weighted average of these two point 10 points of 1 edge.",
                    "label": 0
                },
                {
                    "sent": "So that means you, right?",
                    "label": 0
                },
                {
                    "sent": "It is some coefficient times X0 plus some coefficient times X1, where the sum of those coefficients is 1.",
                    "label": 0
                },
                {
                    "sent": "You do the same for the other point.",
                    "label": 0
                },
                {
                    "sent": "And now those barycentric coordinates tell you how to modify the edge weights.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple.",
                    "label": 0
                },
                {
                    "sent": "We will choose some parameter T, they just increase the weight on this.",
                    "label": 0
                },
                {
                    "sent": "Say if I have an edge from X1 to Y1 it gets multiplied by the coefficient for X one times the coefficient for Y1 and if you look at the picture you'll see the pattern.",
                    "label": 0
                },
                {
                    "sent": "Once you know the pattern, it's a fairly easy exercise in geometry to show that none of the difference vectors are changing.",
                    "label": 0
                },
                {
                    "sent": "Or is it LXS staying the same?",
                    "label": 0
                },
                {
                    "sent": "And this just comes from the magic of barycentric coordinates, and the same proof works in higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "If you give me any two simplices that intersect, you represent the point of intersection in the barycentric coordinates of both, and then you'll show that you didn't want those simple, you didn't want the intersection.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's plenty.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me go back to trying to prove I'm not crazy, which is telling you a little bit about what we found when we used experiments on these datasets to try to solve classification, regression and clustering problems.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you that we used again the simplest regression rule that we knew which was published.",
                    "label": 0
                },
                {
                    "sent": "I think 1st paper view at all.",
                    "label": 0
                },
                {
                    "sent": "So let's say I give you some subset of the vectors S and I tell you the labels on them, and you want to figure out labels everywhere else.",
                    "label": 0
                },
                {
                    "sent": "Well, what we do is we look at minimizing this Laplacian energy or Z transpose, LZ, or that's sort of this measure of smoothness looking for very smooth function.",
                    "label": 0
                },
                {
                    "sent": "Subject to it getting the right values at the known vertices.",
                    "label": 0
                },
                {
                    "sent": "So there's no mean.",
                    "label": 0
                },
                {
                    "sent": "So yeah, no regularization or anything, we're going to pin down these nodes and then optimize.",
                    "label": 0
                },
                {
                    "sent": "Then make the Laplacian as small as possible elsewhere.",
                    "label": 0
                },
                {
                    "sent": "This is the natural generalization of the take the average of your neighbors rule when you just don't know one vertex.",
                    "label": 0
                },
                {
                    "sent": "This is the right way to generalize it to many.",
                    "label": 0
                },
                {
                    "sent": "And OK, so that's how we did regression for clustering.",
                    "label": 0
                },
                {
                    "sent": "We of course through if there were two clusters, we put in an indicator vector.",
                    "label": 0
                },
                {
                    "sent": "If there were multiple clusters, we use K indicator vectors.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the part I'll just tell you.",
                    "label": 0
                },
                {
                    "sent": "What we did was ten fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "We had no parameters to train, we just have a graph in a rule to compare to what happens with the other constructions of graphs.",
                    "label": 1
                },
                {
                    "sent": "Basically, on the 9/10 we weren't trying to evaluate on, we just gritted overall reasonable choices of values and used leave one out cross validation to see how well they didn't chose the best.",
                    "label": 0
                },
                {
                    "sent": "Anne repeated this 100 times and came back and reported some results.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is what we get on those datasets we found.",
                    "label": 0
                },
                {
                    "sent": "I hope one can see I tried to show in bold.",
                    "label": 0
                },
                {
                    "sent": "OK, I didn't put down the hard graph, I just put down the .1 soft graph 'cause the hard graph is similar.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's a percent or two off, but it wins whenever the soft graph wins compared it to two types of graphs, 01 choosing K nearest neighbors to choose the edges, the other using distance threshold nodes are close enough.",
                    "label": 0
                },
                {
                    "sent": "Together we put down an edge.",
                    "label": 0
                },
                {
                    "sent": "And you know, we tried all sorts of values of K and the distance threshold and all sorts of different parameters.",
                    "label": 0
                },
                {
                    "sent": "Sigma for weighted graphs and I just tried to put in bold the cases where are with each line in bold, whichever came out best, yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, that is a great question.",
                    "label": 0
                },
                {
                    "sent": "It's one of my open questions repeated.",
                    "label": 0
                },
                {
                    "sent": "The question is roughly, what if another point comes in later?",
                    "label": 0
                },
                {
                    "sent": "Because particularly poignant, because if you take a look for other ways of building graphs, or at least this one, there are some great ways of doing something reasonable when a new point comes in, we don't have a great way of doing that yet.",
                    "label": 0
                },
                {
                    "sent": "I would love to come up with one, I mean other than adding it to the graph, which doesn't take as much time as you'd think, as we can do a warm start on solving.",
                    "label": 0
                },
                {
                    "sent": "Our quadratic program, but there should be something better.",
                    "label": 0
                },
                {
                    "sent": "We don't have it yet.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we've got we're not there yet.",
                    "label": 0
                },
                {
                    "sent": "One thing I want to point out is, you know, we don't always win.",
                    "label": 0
                },
                {
                    "sent": "That's not surprising.",
                    "label": 0
                },
                {
                    "sent": "I actually thought that these graphs were going to do horribly when you choose the threshold compared to K nearest neighbors, I was wrong.",
                    "label": 0
                },
                {
                    "sent": "They don't have a win in any column, but it is worth pointing out there are times when they really beat.",
                    "label": 0
                },
                {
                    "sent": "Let's see if we can see that.",
                    "label": 0
                },
                {
                    "sent": "There are times when they really beat like the K nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "That sort of surprised me.",
                    "label": 0
                },
                {
                    "sent": "I wasn't expecting that.",
                    "label": 0
                },
                {
                    "sent": "So sometimes using a distance threshold helped.",
                    "label": 0
                },
                {
                    "sent": "Of course you know we tried a lot of different parameter ranges, so you know when you keep going over parameters, it makes sense that something would win.",
                    "label": 0
                },
                {
                    "sent": "I guess the other thing to point out is there are times when.",
                    "label": 0
                },
                {
                    "sent": "I mean our constructions when they lose, they never lose by much when they win.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they win by a lot.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Well, that I don't know if it's a lot OK Now, let's see if this thing will let me choose my.",
                    "label": 0
                },
                {
                    "sent": "Page again OK for gresso.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I also put in support vector machines of course, 'cause we wanted to compare against this thing that everyone tells me about.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we use Lib SVM that won a heck of a lot of the fights.",
                    "label": 0
                },
                {
                    "sent": "I think that left us with two.",
                    "label": 0
                },
                {
                    "sent": "And on a lot of these it.",
                    "label": 0
                },
                {
                    "sent": "Still doing a lot better, and for that we just used the training procedure that they supplied with that again on all of the other data.",
                    "label": 0
                },
                {
                    "sent": "I'm very curious if there is a way to tweak our construction.",
                    "label": 0
                },
                {
                    "sent": "Maybe putting in a kernel to train somehow and see if we can somehow compete with support vector machines.",
                    "label": 0
                },
                {
                    "sent": "I have no idea yet.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we also tried regression experiments on the regression datasets we had.",
                    "label": 0
                },
                {
                    "sent": "You know these.",
                    "label": 0
                },
                {
                    "sent": "I you can also see we did fairly well and again we used Lib SVM and even on one data set abalone after running it for a weekend and it didn't stop.",
                    "label": 0
                },
                {
                    "sent": "We just gave up.",
                    "label": 0
                },
                {
                    "sent": "I don't know why abalone seems like a messy data set, but also took a heck of a long time for us to compute our graphs on that data set, but certainly less time than running Lib SVM would have take to train it.",
                    "label": 0
                },
                {
                    "sent": "Very.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, as mentioned quickly, some clustering experiments for clustering data, so we put up K means, which again wins.",
                    "label": 0
                },
                {
                    "sent": "Most of the fights.",
                    "label": 0
                },
                {
                    "sent": "We tried clustering the same way in Jordan wasted, so they chose a variation of this K nearest neighbors construction, get a graph, take a look at a couple of its eigenvectors, project into these eigenspaces and run K means there instead of in the original space.",
                    "label": 0
                },
                {
                    "sent": "And we tried the same thing.",
                    "label": 0
                },
                {
                    "sent": "And there's this one question, which is, how many eigenvectors do you choose in their paper?",
                    "label": 0
                },
                {
                    "sent": "They suggested choosing a number of eigenvectors, equal number of clusters you want, and there's some motivation for that.",
                    "label": 0
                },
                {
                    "sent": "If your clusters are all really far apart, but we didn't like that idea in general, so we also said you know what if we choose the number of dimensions in the eigenspaces to project into a little bit more Intel.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elegantly and then it turns out you can wind up doing much better, and if there's.",
                    "label": 0
                },
                {
                    "sent": "I mean, often these things are all comprable.",
                    "label": 0
                },
                {
                    "sent": "The only points where I notice we seem to get something a lot lowers for this wine data set.",
                    "label": 0
                },
                {
                    "sent": "It's an order of 10 lower than a lot of the others in factor of 10.",
                    "label": 0
                },
                {
                    "sent": "There's some others that are sort of interesting.",
                    "label": 0
                },
                {
                    "sent": "But in this at least, I think goes back to arguing.",
                    "label": 0
                },
                {
                    "sent": "These ideas are not crazy.",
                    "label": 0
                },
                {
                    "sent": "Which was the main goal here so far.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And OK, I had a slide on how we choose those Spectra.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a little fuzzy.",
                    "label": 0
                },
                {
                    "sent": "I'll just mention briefly.",
                    "label": 0
                },
                {
                    "sent": "John Kelner is the master of this right now.",
                    "label": 0
                },
                {
                    "sent": "We had some questions about.",
                    "label": 0
                },
                {
                    "sent": "We take our how do we choose which eigenvectors to use?",
                    "label": 0
                },
                {
                    "sent": "Well, we take the eigenvectors, order them by the eigenvalue, project our data set back onto the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so the idea here was just we tried to take a look.",
                    "label": 0
                },
                {
                    "sent": "If you take a look at projecting your vectors into the eigen space, you find that the first couple eigenvectors have almost all the projection and then it goes down and we just try to choose the point at which it really drops off.",
                    "label": 0
                },
                {
                    "sent": "But we can argue about where it is, so this is not math yet.",
                    "label": 0
                },
                {
                    "sent": "This is just an art, you know, like John and I can agree it's here, but I think it's there and he thinks it here.",
                    "label": 0
                },
                {
                    "sent": "The answer is it doesn't matter very much.",
                    "label": 0
                },
                {
                    "sent": "You get pretty much similar results.",
                    "label": 0
                },
                {
                    "sent": "Whatever you choose.",
                    "label": 0
                },
                {
                    "sent": "Actually, as long as it's a reasonable choice we found.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But who knows?",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to quickly mention something about computing these.",
                    "label": 0
                },
                {
                    "sent": "To do that, I want to point out that the Laplacian has a very nice form.",
                    "label": 0
                },
                {
                    "sent": "We can represent the Laplacian as assigned vertex edge adjacency matrix times the diagonal matrix of the edge weights times that edge, vertex adjacency matrix transpose.",
                    "label": 1
                },
                {
                    "sent": "So what is this matrix?",
                    "label": 0
                },
                {
                    "sent": "The rows are indexed by vertices, the edges, the columns by edges, and for each edge of a one and a -- 1 in its column zeros everywhere else.",
                    "label": 0
                },
                {
                    "sent": "It's sort of the sign in edge.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter which way you put the ones in minus ones 'cause that all comes out when you multiply by the transpose on the other side.",
                    "label": 0
                },
                {
                    "sent": "This form winds up being useful.",
                    "label": 0
                },
                {
                    "sent": "What we can do with that is we.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And rewrite this objective function.",
                    "label": 0
                },
                {
                    "sent": "Maybe I will not go through the computation, but just point out that the quadratic objective function can be nicely turned into something, some matrix times our vector of edge weights, so you can write the quadratic very cleanly and see what it is, and that's useful if you want to compute anything with it.",
                    "label": 1
                },
                {
                    "sent": "Also becomes useful because you see that this matrix is sparse.",
                    "label": 0
                },
                {
                    "sent": "If your graph is sparse and that accelerates your computation, you try to preserve that during the computation OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the key points about computing this.",
                    "label": 0
                },
                {
                    "sent": "Probably the useful thing to take away.",
                    "label": 0
                },
                {
                    "sent": "We were trying to minimize a quadratic form, say for the soft graph, subject to two constraints, one of them being non negativity and the other this sort of quadratic constraint on the weighted degrees.",
                    "label": 1
                },
                {
                    "sent": "So this is a little bit messy to make our lives easier, we threw the constraint into the object.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "So instead we add on some factor times the term determining this constraint, we call that factor mu.",
                    "label": 0
                },
                {
                    "sent": "There is a mu for which you will get the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Well, first you can realize that the optimal solution that inequality is always inequality.",
                    "label": 0
                },
                {
                    "sent": "And you can do a search overview to find the right value that gets you that.",
                    "label": 0
                },
                {
                    "sent": "That sounds like you're solving many programs, but once you've solved one, solving the next one is much easier.",
                    "label": 0
                },
                {
                    "sent": "So it's actually not so bad.",
                    "label": 0
                },
                {
                    "sent": "All the work is solving the first one.",
                    "label": 0
                },
                {
                    "sent": "While this makes our life easy, is now we've got a non negative least.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where's problem?",
                    "label": 0
                },
                {
                    "sent": "So if you take a look at what this problem is, it is minimizing a quadratic objective function.",
                    "label": 0
                },
                {
                    "sent": "Subject to some non negative terms.",
                    "label": 0
                },
                {
                    "sent": "Oh well, I guess it's not quadratic yet.",
                    "label": 0
                },
                {
                    "sent": "We have to add in some slack variables.",
                    "label": 0
                },
                {
                    "sent": "OK so we add in some slack variables S here.",
                    "label": 0
                },
                {
                    "sent": "Then you actually get a non negative least squares problem.",
                    "label": 0
                },
                {
                    "sent": "So just WNS have to be non zero and you want to minimize this term squared an actually there's pretty good off the shelf codes for solving these breeders.",
                    "label": 0
                },
                {
                    "sent": "Used matlab's not negatively squares routined an that works much better than their general quadratic programming solver.",
                    "label": 0
                },
                {
                    "sent": "So that's how we saw.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of these things, there's one more detail.",
                    "label": 0
                },
                {
                    "sent": "Again, if you have a graph with N nodes, there could be N. Choose two possible edges if an is 1000, that means you're dealing with about 500,000 possible edges, and then solving a quadratic programming that many variables sort of painful.",
                    "label": 0
                },
                {
                    "sent": "But we know we're looking for sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we restrict our search to sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "We start out looking within a subset of the edges.",
                    "label": 1
                },
                {
                    "sent": "Then when you get a solution, you can quickly check if it is optimal.",
                    "label": 0
                },
                {
                    "sent": "Given a solution, I mean not negative least squares problems are really nice if when you if any variable is zero in the solution, that means that the gradient of the objective function has to be negative.",
                    "label": 0
                },
                {
                    "sent": "So you just take a look after you come up with a solution on the subset of the edges that all the edges you didn't use.",
                    "label": 0
                },
                {
                    "sent": "You say no is the objective function negative?",
                    "label": 0
                },
                {
                    "sent": "If it is, we're done.",
                    "label": 1
                },
                {
                    "sent": "If it's not, we put some of those edges back in and solve again.",
                    "label": 0
                },
                {
                    "sent": "But that's how we actually managed to solve these things relatively quickly.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I've put up a chart of our computation time.",
                    "label": 0
                },
                {
                    "sent": "In seconds on an ordinary workstation, so you know none of these took too long though, at least for the soft graph.",
                    "label": 0
                },
                {
                    "sent": "The hard graft takes a lot longer, and that's because we can't use quite as nice.",
                    "label": 0
                },
                {
                    "sent": "It's not a non negative least squares problem.",
                    "label": 0
                },
                {
                    "sent": "But at least now the worst of them that we tried abalones.",
                    "label": 0
                },
                {
                    "sent": "Still, you know less than half an hour.",
                    "label": 0
                },
                {
                    "sent": "So we could actually compute these things, and we've really barely scratched the surface of how to compute these quickly.",
                    "label": 0
                },
                {
                    "sent": "We just tried to come up with code that would suffice to do some experiments to this set of experiments.",
                    "label": 0
                },
                {
                    "sent": "If people decided they really wanted faster code, there are a lot of other things one can do.",
                    "label": 0
                },
                {
                    "sent": "OK, hopefully I've.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On fast enough to talk about, one of my favorite topics.",
                    "label": 0
                },
                {
                    "sent": "Sparse solutions in higher dimensions.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is back to theory.",
                    "label": 0
                },
                {
                    "sent": "One natural questions again in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Can there be a sparse solution?",
                    "label": 0
                },
                {
                    "sent": "Well, again, as I said, if I gave you the points, the corners of regular simplex, there is a solution that's not sparse, but is there an approximately optimal solution that sparse and in some sense, yes.",
                    "label": 0
                },
                {
                    "sent": "So what we can prove right now?",
                    "label": 0
                },
                {
                    "sent": "Is it for any graph G you give me?",
                    "label": 0
                },
                {
                    "sent": "There exists a graph, I'll call it G~ The average degree is like 16 over epsilon squared so that the quadratic form in the Laplacian squared and in the approximate Laplacian squared is the same for all vectors approximately.",
                    "label": 1
                },
                {
                    "sent": "When I say approximately the same.",
                    "label": 0
                },
                {
                    "sent": "Oh darn, I'm sorry, this square root shouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "It's most epsilon times X transpose LX times.",
                    "label": 0
                },
                {
                    "sent": "We've got this annoying term.",
                    "label": 0
                },
                {
                    "sent": "Here comes in the form of al times the norm of X. I'll mention that in a moment what that means, at least if these things are not too small.",
                    "label": 0
                },
                {
                    "sent": "The objective function value is the same if they're really small, our approximation falls off an using it to get any interpolation, solve any regression problem, at least in the leave one out.",
                    "label": 0
                },
                {
                    "sent": "Since you get similar answers with this approximate Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And I don't.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point out this is not what we want.",
                    "label": 0
                },
                {
                    "sent": "What we want is something really relative error.",
                    "label": 0
                },
                {
                    "sent": "We would like this to be at most epsilon times X, transpose, Laplacian squared X.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to do that yet.",
                    "label": 0
                },
                {
                    "sent": "Instead, we have this ugly term.",
                    "label": 0
                },
                {
                    "sent": "Let me point out.",
                    "label": 0
                },
                {
                    "sent": "Doing this is actually a very interesting thing.",
                    "label": 0
                },
                {
                    "sent": "Gary Miller is going to talk tomorrow about Linear equation solvers and being able to do this would be very useful for coming up with a V cycle and linear equation solvers.",
                    "label": 0
                },
                {
                    "sent": "Something comes out of multigrid would have a lot of other applications.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to do it.",
                    "label": 0
                },
                {
                    "sent": "But there's some hope.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you why there's hope, yeah.",
                    "label": 0
                },
                {
                    "sent": "Because that's what our objective function is in right now.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "L Yeah, then this is.",
                    "label": 0
                },
                {
                    "sent": "This is what we needed to optimize and for the V cycle.",
                    "label": 0
                },
                {
                    "sent": "I don't know if Gary will tell us, but let me take that offline.",
                    "label": 0
                },
                {
                    "sent": "Let me see I'm almost to the finish.",
                    "label": 0
                },
                {
                    "sent": "Let me try to do it quickly.",
                    "label": 0
                },
                {
                    "sent": "The reason we've got this ugly bound is because proving this theorem is like driving in a screw and we're using a sledgehammer.",
                    "label": 0
                },
                {
                    "sent": "You know it gets it most of the way in, but it makes a mess.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I made the sledgehammer, so I want to tell you a little bit about the sledgehammer.",
                    "label": 0
                },
                {
                    "sent": "So the sledgehammer is the same right theorem, except not for Laplacian squared.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Josh Batson and Nikhil Srivastava.",
                    "label": 0
                },
                {
                    "sent": "I proved this.",
                    "label": 0
                },
                {
                    "sent": "So we can say that for any graph G There exists a sparse graph G~ so that we neglect their laplacian's, they approximate each other in a relative sense on all vectors.",
                    "label": 1
                },
                {
                    "sent": "So the quadratic form in the original Laplacian is within 1 minus epsilon, one plus epsilon of the quadratic form in the approximate Laplacian, no matter what vector you put in.",
                    "label": 0
                },
                {
                    "sent": "And we can do this with four N over epsilon squared edges.",
                    "label": 0
                },
                {
                    "sent": "And I should mention that four is within a factor of two of optimal.",
                    "label": 0
                },
                {
                    "sent": "If you could get two over epsilon squared, that's the Ramanujan bound and we know you can't beat that.",
                    "label": 0
                },
                {
                    "sent": "I actually think you can get there too, but we don't know it yet, but this tells you that you can get really good approximations and they exist.",
                    "label": 0
                },
                {
                    "sent": "This is a polynomial time.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function, so I've been spending the last few years on this, so if you want faster constructions you can you gotta use more edges, essentially N Logn edges.",
                    "label": 0
                },
                {
                    "sent": "We can do it with about.",
                    "label": 0
                },
                {
                    "sent": "Loggins solutions of linear systems of linear equations in the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Or if you really don't want to do anything like that, you can get these horrible but still nearly linear bounds that Shanghai Tang and I got a few years ago.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the quick thing.",
                    "label": 0
                },
                {
                    "sent": "I wanted to say about sparsification.",
                    "label": 0
                },
                {
                    "sent": "If I have a minute.",
                    "label": 0
                },
                {
                    "sent": "When I just tell you quickly about these other graphs that Lovasz constructed, and then I'll finish, this is.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing that I spent a long time thinking about is part of the motivation for this.",
                    "label": 0
                },
                {
                    "sent": "Solo bus in this paper called Steinitz representation in the colon, divergent number.",
                    "label": 0
                },
                {
                    "sent": "Please excuse my pronunciation.",
                    "label": 0
                },
                {
                    "sent": "Considers for well.",
                    "label": 0
                },
                {
                    "sent": "OK, he just considers sets of points that are on the convex Hull or their extreme points of a convex polytope, so they should all.",
                    "label": 1
                },
                {
                    "sent": "None should be a linear combination of any others, and he considers making matrices that aren't exactly laplacian's.",
                    "label": 0
                },
                {
                    "sent": "The Matrix does have the form of a diagonal minus an adjacency matrix, but the diagonal matrix doesn't.",
                    "label": 0
                },
                {
                    "sent": "I'll call it P because it's not the matrix of degrees, but it is diagonal and non negative.",
                    "label": 0
                },
                {
                    "sent": "But what he did is he looked at trying to make.",
                    "label": 0
                },
                {
                    "sent": "M X = 0 so that means he tried to make this matrix so that actually the nullspace were exactly the coordinate vectors.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, rather than trying to just make something that small on the coordinate vectors is making the nullspace accordant factors and having only one negative eigenvalue exactly one negative eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And he could prove this was possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so he proved it for 3 dimensional for points in 3D, but you can prove it in higher dimensions as well if the points all lie on the convex Hull of a polytope.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is you will only have edges in the graph for edges in the polytope.",
                    "label": 1
                },
                {
                    "sent": "So a will really only be non 0 where there are edges in the polytope.",
                    "label": 0
                },
                {
                    "sent": "And for me this is a very nice way of capturing.",
                    "label": 0
                },
                {
                    "sent": "A set of points or a making another matrix that captures a set of points.",
                    "label": 0
                },
                {
                    "sent": "The annoying thing is that construction really depends upon using the combinatorial structures of the polytope, so I don't really know how to go about it in general.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's easy to satisfy this condition, making you know matrix which the coordinate vectors in the null space getting just one negative eigenvalue is a little trickier, so I don't know if this can be generalized at all, but it was one of our motivations in coming up with this construction that we have here.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start with open questions.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, the main one is.",
                    "label": 0
                },
                {
                    "sent": "Are there other natural graphs or useful graphs to associate with the set of points?",
                    "label": 1
                },
                {
                    "sent": "We've got one proposal, it's got some interesting properties, but it really feels like it's not the right one yet.",
                    "label": 0
                },
                {
                    "sent": "And I say that because we've got two.",
                    "label": 0
                },
                {
                    "sent": "You know, there's a hard grafana soft graph, and neither feels quite right.",
                    "label": 0
                },
                {
                    "sent": "There's probably something better.",
                    "label": 0
                },
                {
                    "sent": "And other questions.",
                    "label": 1
                },
                {
                    "sent": "Other interesting ways to parameterise or modify our construction.",
                    "label": 0
                },
                {
                    "sent": "Whenever I do these experiments and classification or regression, I'm jealous of these other algorithms where we can try all sorts of parameters and see what works best.",
                    "label": 0
                },
                {
                    "sent": "OK, it's nice that our algorithm doesn't have any parameters, but if I wanted to just do better in those experiments, a parameter would probably help me.",
                    "label": 0
                },
                {
                    "sent": "So simple things we don't know, as I say, are graphs connected or unique.",
                    "label": 0
                },
                {
                    "sent": "We really don't know.",
                    "label": 0
                },
                {
                    "sent": "So as I say, I really do.",
                    "label": 0
                },
                {
                    "sent": "There exist sparse approximate solutions in all dimensions.",
                    "label": 1
                },
                {
                    "sent": "I really want relative approximations, but which I mean getting the squad rat the Laplacian squared right now say that is actually a deep question that would have a lot of implications and other places.",
                    "label": 1
                },
                {
                    "sent": "So really, can one sparsified the square of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And the other question was asked earlier is OK if someone comes along and gives me points that were I didn't have originally.",
                    "label": 0
                },
                {
                    "sent": "How can I do regression for them?",
                    "label": 0
                },
                {
                    "sent": "How can I interpolate for them?",
                    "label": 0
                },
                {
                    "sent": "I'd like to do something better than just adding them to the graph, 'cause that could require changing the weights of every single edge.",
                    "label": 0
                },
                {
                    "sent": "I'll stop there, thanks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm from.",
                    "label": 0
                },
                {
                    "sent": "Inverse of your problem.",
                    "label": 0
                },
                {
                    "sent": "So, given given a graph, maybe give him spinning.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right or tots?",
                    "label": 0
                },
                {
                    "sent": "How to draw a graph is a famous paper.",
                    "label": 0
                },
                {
                    "sent": "Yes Sir, like stop.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "Yeah CGL or doing is there like right you can view it as they're nailing down nodes certain places in the letting rubber bands be the edges of given the strength, proportional the weights and then just letting it relax and getting the solution.",
                    "label": 0
                },
                {
                    "sent": "Yeah I guess The thing is if you start out with a graph all those things are great.",
                    "label": 0
                },
                {
                    "sent": "But I said my goal was to get to the graph and the way I've got to do that is you know people in machine learning seem to keep giving me point sets so I've got to get a graph somehow and that's what we were trying to do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, naughty.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so can.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So constraining the degrees to be one is natural and but it also makes some ugliness.",
                    "label": 0
                },
                {
                    "sent": "For example, I really like the fact that see if I can draw it for points in the line right now.",
                    "label": 0
                },
                {
                    "sent": "We get a path graph, but if you constrain the degrees to be one, you just cannot get this graph out, because if I force the degree of this to be one in the degree of that to be one, the degree of that's going to be 2.",
                    "label": 0
                },
                {
                    "sent": "So if I put points in the line, you know.",
                    "label": 0
                },
                {
                    "sent": "You get things that I find sort of unnatural, like that is the graph instead, so that I don't know.",
                    "label": 0
                },
                {
                    "sent": "But it is a reasonable idea and we certainly did a lot of experiments with solving for degrees, constrained to be near 1.",
                    "label": 0
                },
                {
                    "sent": "Both upper and lower bounds.",
                    "label": 0
                },
                {
                    "sent": "Before we figured out that we could actually just penalize degrees less than one, and you get pretty much similar results so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, if you.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see.",
                    "label": 0
                },
                {
                    "sent": "So if we just go for linear combination rather than like affine combination or weighted combination, then strange things happen.",
                    "label": 0
                },
                {
                    "sent": "We did try.",
                    "label": 0
                },
                {
                    "sent": "I mean we've tried played around with both of these.",
                    "label": 0
                },
                {
                    "sent": "The answer is, you know fixing all degrees to be one or near 1 is not so bad.",
                    "label": 0
                },
                {
                    "sent": "I just don't like it.",
                    "label": 0
                },
                {
                    "sent": "'cause I like it when my points are in the line I get back a path graph.",
                    "label": 0
                },
                {
                    "sent": "The other one I remember produced all sorts of degenerate solutions.",
                    "label": 0
                },
                {
                    "sent": "What if you try?",
                    "label": 0
                },
                {
                    "sent": "If you don't sort of restrict to be a weighted average, but you let it be just a general linear combination of your neighbors, so it seems like that would make sense.",
                    "label": 0
                },
                {
                    "sent": "'cause if your points, let's say in the Pentagon, then you could exactly get a point.",
                    "label": 0
                },
                {
                    "sent": "But it does worse things in other graphs, so but I don't know.",
                    "label": 0
                },
                {
                    "sent": "I do not remember exactly what went wrong, I just remembered gave some strange nastiness.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}