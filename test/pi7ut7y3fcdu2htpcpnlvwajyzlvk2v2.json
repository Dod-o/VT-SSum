{
    "id": "pi7ut7y3fcdu2htpcpnlvwajyzlvk2v2",
    "title": "Channel Coding with LDPC Codes",
    "info": {
        "author": [
            "Fernando Perez-Cruz, Carlos III University of Madrid"
        ],
        "published": "Jan. 25, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_perez_cruz_ldpc_codes/",
    "segmentation": [
        [
            "So as Neil said, when he started his lecture, he has a point of view how he wants to do things and how he wants to do the.",
            "The dimensionality reduction.",
            "I also have some kind of a message that I want to convey in the in the talks at all.",
            "I want to do so.",
            "Initially I thought of just giving up your information theoretic course and then I thought for the type of people that we have.",
            "Maybe it's better to leave overview of why I think machine learning might be good for information theory.",
            "OK, what kind of problems they are solving nowadays in information theory?",
            "In which machine learning can bring?",
            "And a fresh look into into those into those problems.",
            "And also what kind of difficulties we can find on their way in order to solve those problems.",
            "So I think there are going to be talking.",
            "I'm going to be talking mainly about a simple example which is channel coding, but there are two things in information theory that are relevant now that nowadays which is network information theory and known as synthetic information theory, and in both of those problems.",
            "I think machine learning solutions.",
            "Can really bring benefit to the type of problems that they are.",
            "They are looking into it and I'll try to explain as I go alone.",
            "I'll do a summary at the end to see if that if I can explain that that better.",
            "So my idea here is just to show how to solve the channel problem with LBC codes, which is roughly using belief propagation which is an we could say so much in learning algorithm, although it has many faces and you can see it also in.",
            "In communications you can see in transportation there are many other ways that you can derive information, but the way it was used to solve the channel coding problem, it came from a machine learning point of view, and I think that's that's important too.",
            "To tell you about, so I'm going to start showing a few."
        ],
        [
            "Things in information theory and which type of problem I'm going to be working on an I'm not going to give a lot of information about communications or information theory, but just enough for you to know exactly what we want to do.",
            "So the main problem that we have is we have a source of information that we want to transmit to another end through a channel.",
            "That's the whole point.",
            "OK, so we have a source that gives us symbols that can be text.",
            "Can be boys can be video can be anything that you want.",
            "OK and it has a certain probability measure.",
            "OK, and we want to convey that information to another end.",
            "OK. And we want to do before we do that, we want to do two things.",
            "The first thing is we want to compress the source.",
            "OK, we want to take out all the redundancy that there is in the source.",
            "So if we have less to transmit, it will cost us less.",
            "OK, so you can know an image or video voice.",
            "Music.",
            "It is very redundant sources, so we want to compress as much as we can.",
            "OK.",
            "But then we're going to transmit that through a faulty channel.",
            "That is going to introduce errors into it.",
            "So once we.",
            "Taking out all the windows in the race in the in the in the source, we're going to add control the random redundancy.",
            "And we're going to add control redundancy to get rid of those errors that we're going to get into the channel.",
            "So that those are the two main points that we want to do.",
            "First, remove redundancy so we transmit less information.",
            "Let's bet.",
            "And then add redundancy in order to control the errors that we that we have in the into the channel.",
            "OK. And the thing that we have and the results are we get is that.",
            "There's also encoder has to compress the source to sum right above the entropy of the of the source.",
            "OK, so if the entropy of the source is 5 bits per second.",
            "You have to use at least 5 bits per second.",
            "If you use less, the source is going to be recoverable.",
            "OK, and actually the minimum.",
            "I mean the quality sign is what we is what we call the minimum rate that we can we consume it.",
            "And then the channel encoder has the opposite, the opposite thing.",
            "Is you have to transmit information at less rate that the channel can take.",
            "OK, so if the channel is introducing error at a certain rate.",
            "You have to transmit less information at that rate in order to be able to recover information.",
            "OK, so roughly is telling you OK if you have one bit per second that you want to transmit.",
            "But the mutual information between the input on the output is 0.5.",
            "It means that for each information bit that you're putting in, at least you have to put one redundancy in it.",
            "So your total rate is only one bit of information every 2 seconds.",
            "OK, that is what is is what is telling you that lower bound.",
            "Of course these two right they have to be equal, so if they're right that you're encoding the source encoding is higher than the other one.",
            "There is nothing you can do.",
            "OK.",
            "When we achieve when we do are equal to the mutual information between X&Y is what we call channel capacity.",
            "OK, and actually we have to maximize with respect to your input distribution.",
            "OK.",
            "But the thing that we are that we are thinking in here is that we cannot optimize over the input distribution the X, because that's going to be given by our source.",
            "OK, So what actually it happens in most cases is if there's also the words correctly.",
            "It will give you an IID source of the probable signals.",
            "That's the best thing that you can do.",
            "And then most of the channels that will be the optimal distribution for most channels.",
            "And then we can achieve the channel capacity.",
            "OK, so now I'm only going to focus on this problem and then there is one thing that is called the separation principle that it tells you that you can do both things separately.",
            "OK. That is.",
            "It doesn't always OK, for example, for network information doesn't hold, but when you do point to point communication it holds and roughly that is the way we've been designing things.",
            "So you can see source encoders you can think of many different things, so you have JPEG.",
            "You have impact, you have MP threes.",
            "All these standards are roughly doing source encoding, trying to reduce the redundancy of the source to independent and equally probable bits.",
            "OK, lemme see.",
            "Logarithm is the same thing.",
            "And then in the transmission rate we have also a set of algorithms trying to get R as close to the mutual information and reduce the quality of.",
            "OK, I'm going to talk about those in a little bit, but we have always.",
            "Design then.",
            "Separately, so we're going to have this also encoder.",
            "You're going to have somebody that is an expert in this source.",
            "Trying to compress a source as much as you can, and then you're going to have a channel encoder expert that is going to try to put around and see in the smartest possible way.",
            "OK, in order to get to the.",
            "So that you have to put in the less amount the least amount of redundancy into the bits that you want to transmit.",
            "OK.",
            "I'm not going to get into the into the proof of any of these things.",
            "But I'm sure you remember the example that Niels put the first day about.",
            "They they widen the yellow and that when you tend to Infinity the green part, the one that is in the middle.",
            "Took all the probability mass.",
            "That's the that's roughly the proof.",
            "To get those those values.",
            "OK, the thing that even say is that in order to get to those values, you are going to get zero.",
            "You're going to have.",
            "Probability of error of making a mistake.",
            "On the other end, as small as you want.",
            "OK, if you transmit below that rate or if you compress up of the entropy.",
            "OK, and that's the dust.",
            "Those are the main results that I'm going to be talking about.",
            "So right now what I'm going to be concentrating on.",
            "Is in this spot?",
            "How can we introduce redundancy in order?",
            "To protect the information that we that we want to transmit."
        ],
        [
            "OK, so we're going to assume that the source or the source encoder give us a set of symbols.",
            "OK, all the way one to the end and the channel the channel encoder is going to give each one of these symbols.",
            "A string of bits.",
            "Up to N bits.",
            "OK, typically we will use C for those.",
            "But in order to make sense of all the machine learning that we're doing, I'm going to be using X during that, and if the message that comes from the source encoder is also binary, we're going to assume that we have.",
            "Kabit an all over the talk K. It's going to be less than.",
            "OK, so we're going to use.",
            "We're going to use.",
            "K information bits, and we're going to add.",
            "N -- K. Read and submit.",
            "That additional redundancy bits should allows us.",
            "To find errors and correct the errors, OK. And if these are is less than the mutual information, as we make NNK tend to Infinity.",
            "We should be able to correct every error that appears in the code so we can get an error less communication if we make NK large enough.",
            "OK, those are the results of channel coding that if you make the right low enough you can get an errorless communication through any communication channel.",
            "OK. Those results are due to Shannon in the 40s.",
            "An they were remarkable because in the 40s they thought that the only way to get errors communication was to get error zero rate.",
            "So you have to put more and more redundancy in order not to get any errors and what he proved.",
            "Was basically that you can get errors communication at a finite rate.",
            "And since we knew that this limit exists, we've been trying to design encoders and decoders to get to that right?",
            "OK, and that's roughly what LDC codes achieve, we're going to be talking about curing the election problem."
        ],
        [
            "OK, so the first thing that we're going to do is talk about linear encoding and the thing about talking about linear encoding is if you think about the problem coding in general.",
            "If you're going from K bits.",
            "To ambit.",
            "That's an exponential problem, either for the foreign cover.",
            "OK, because we have to have a table that says.",
            "Each of the inputs how they go through each one of the outputs, so we'll have to have a two to the K table saying what is the mapping from one to the other.",
            "If we use a linear encoding.",
            "That's only quadratic in the number of bits that we know we want to transmit, so we're going to use and this is something.",
            "It's weird, but I'm going to be using bro vectors.",
            "I mean when we did, is this type of things in channel encoding everything is raw vectors so.",
            "Is completely the opposite that everything else.",
            "But is the standard way of doing it and I don't want to change it in case you look at other things.",
            "So M is a row vector of K components and G is a K by N matrix.",
            "That converts they K initial bits into a bit OK, and all the additions or multiplications.",
            "Animal model is 2 OK, so we're only going to be working over a bit.",
            "OK, I'm not going to put it not to cloud the information and there are several results.",
            "I'm not going to talk about them, but linear channel encoding is good enough to get to capacity, so we're not losing anything for not using linear and nonlinear mapping.",
            "And the other thing is.",
            "You can always prepare the geometrics to have an identity matrix and AP matrix.",
            "So the first N bits.",
            "It's going to be a copy of your message.",
            "The last N -- K bit is going to be that redundancy that you that you had today.",
            "And to the information.",
            "OK, so X in that if we use G in that sense is going to be.",
            "I'm done some.",
            "Redundance events Yep.",
            "Yep.",
            "Better.",
            "OK. On the other thing that we typically assume is that the channel worked in independent realizations, so we expect that every time that we transmit a symbol.",
            "A noise is added to that bit, independent from any other of the of the of the of the noise to the other on the other bids.",
            "If that doesn't happen well, we talking about a completely different problem and we don't want to get into that, but this is a good assumption in most communications system of of interest.",
            "OK, when I say is iid noise.",
            "So depending of if P of Y given X is is is a Gaussian channel.",
            "CI is going to be Gaussian.",
            "If it's a binary symmetric channel, CI is going to be over newly random variable.",
            "Depending on which type of channels we think CI is going to be one distribution or another, OK, but it's typically known when you decide this system OK. And the maximum in the channel decoder the object is to find the maximum likelihood solution.",
            "So if you look from the point of view of the maximum likelihood, the function is pretty simple because it.",
            "An is a product of the marginals and it should be easy to optimize.",
            "The problem comes from the restriction.",
            "Because we need X to be a code word.",
            "OK, that's the that's the main.",
            "That's the main issue, and that is what in general makes the problem.",
            "Exponential, so we need to try every possible solution in order to get to that.",
            "OK, so I then tell you about this, but the full objective of solving this problem is that we want at least linear complexity in the encoder and we want at least linear complexity in the decoder.",
            "OK, if we have the possibility of exponential complexity, the problem is very simple.",
            "Just choose the world at random and it works.",
            "OK. That's what Shannon that's what Shannon proved us OK, but that it gives you exponential complexity and I'm sure you all remember Robin Roberts lies with this potential complexity.",
            "That it really blows in your face very, very soon.",
            "OK, So what we want to do is we want everything to be linear complexity or as close to linear as we as we can, and that's under the Latino want."
        ],
        [
            "OK.",
            "So the next thing that we need for the in order to get what we want is what we call the dual space of G. OK, and that all spaces and orthogonal.",
            "So G defines a linear linear subspace of dimension K, so H. Is a linear subspace of dimension N -- K that is orthogonal to G OK an?",
            "Age is just to compute the syndrome.",
            "OK, so when we transmit there were X and we got the Y.",
            "In the decoder, the first thing that we can do is multiply that by the matrix H, transpose OK, and the matrix agents pose.",
            "Under syndrome, it only depends on the noise.",
            "Doesn't tell you anything about X OK, and that is pretty easy to see which is.",
            "Extends extra suppose.",
            "Is N * G. And we have.",
            "And that's equal to 0.",
            "OK. You're going to see that in all the examples that I'm going to use, minus P is going to become P. Oh, minus P transpose is going to become P transpose, and this is only because I'm working with binary symbols.",
            "OK, so minus one is 1.",
            "Right, but if we were working with other type of symbols, you need the minus on on top of that.",
            "OK, again, this syndrome unique uniquely identifies their pattern that we're looking for.",
            "But again, it's an exponential table.",
            "OK, it's going to be we'll have two to the K entries and FSC and grows with a fixed rate, maybe conservation.",
            "So he's not really going to give us a solution.",
            "OK."
        ],
        [
            "OK, so which type of solutions have been given to these problems?",
            "OK, so this is.",
            "A very short summary of what we have and then we have the.",
            "So the first thing that we came up with with algebraic codes, so I'm sure most of you have heard of Hamming codes.",
            "Anyone who has heard about Hamming codes?",
            "Most people go like oats.",
            "OK, very few red Muller codes.",
            "Read Solomon codes.",
            "OK, so those are the type of codes that where they sign in the in the from the faulty something in 70s.",
            "And the objective there was always to use something that is called a cyclic code.",
            "Which instead of using this matrix.",
            "It could be.",
            "It could be encoded with a linear time machine with a linear in linear complexity, and there were design in order to maximize the distance between the words.",
            "OK, the idea was to put all the words as far as possible from each other.",
            "OK, and then the recorders.",
            "They only corrected up to the half of that distance, so the number of errors that they have.",
            "And that seems to work fine, but as you get to high dimension there are many areas of the space that you're going to explore, so you cannot get to capacity with it.",
            "OK, so they are very nice.",
            "I'll give reconstructions.",
            "But they are unable to get to capacity.",
            "The good thing of all these codes is that the encoding is linear.",
            "You can do in linear time.",
            "Under recording you can do it in linear time and for example the Red Solomon codes.",
            "You can find them in your CD's and DVD's.",
            "That's the algorithm of choice.",
            "In order to decode those.",
            "Those standards OK.",
            "In the 60s we came thing was 15.",
            "Actually we came up with what we call the convolutional codes.",
            "And then they also have a linear encoding and linear decoding.",
            "But in order to.",
            "For a fixed memory, but in order to get to capacity you have to make the memory go to Infinity.",
            "Under recording this potential in the memory so you also couldn't get to.",
            "A to capacity and then in the 90s from the 90s we have Turbo codes and NDC codes and the idea there is going back to what Shannon told us which is try to use one.",
            "OK, and try to get always the best answer by week.",
            "OK, so algebraic coding convolutional codes are very good if you're only thinking about a few bits 5100 bits, but if you get above that.",
            "The ideas that we have in low dimension that work well.",
            "Don't carry over to high dimensions, that's the main limitation of algebraic coding and convolutional codes.",
            "OK.",
            "When you go to Olympus E Codes and Turbo codes.",
            "They were well when you go to very high dimensions, 10,100 dimensions, 100,000 dimension.",
            "That's when they start working working properly because random do you want.",
            "Anne.",
            "That's when they when they started working properly OK and when I say the coding and decoding is almost linear and we'll see that the encoding you have to use the geometrics, which is will be quadratic.",
            "But there are algorithms that you can simplify to make it almost linear.",
            "And the recording will see an algorithm that is linear.",
            "But as we get to capacity, we need more and more iterations.",
            "So it's not going to be exactly linear, it's going to be a little bit, but for a fix, right?",
            "Is going to be linear OK and they're almost achieved capacity?",
            "For example, for the Gaussian channel, I think they are.",
            "Is Zero point 4.5 to 10 to the minus 3D B?",
            "Decibels to the to their capacity, so they are as good as as they get OK. What did I put 60 three over there?",
            "So actually in 1963 and Rob Gallagher proposed these LDC codes in his PhD thesis.",
            "OK, he proposed the code.",
            "He proposed some algorithms to decode them, but he wasn't able to prove that they were good enough and they were completely forgotten afterwards.",
            "OK, it took until nineteen 1995 that baby Mike, I recovered idea of LDC codes and was able to prove that they were very good and very close to.",
            "To capacity, so we lost 30 years.",
            "We lost 30 years there."
        ],
        [
            "OK, so now I want to get to the to the Howell DC codes works and there is a tool that has been used in for for a long time in information theory which is eternal graph which is trying to represent the variables.",
            "On the G matrix on the H matrix into a graph OK.",
            "It is just a factor graph.",
            "OK, I don't want to tell a lot about graphical models because you will have.",
            "So been telling you about them next week and I'm sure he's going to do a much better work that I do.",
            "The only thing that I want to do is just give you enough information to follow what we want.",
            "OK, and I'm sure the way he's going to tell them is probably much different than what I'm going to be doing, so I guess it's going to be complementary any anyway, OK, so let's say that we have a parity check matrix that H over there.",
            "OK, you can see the I and the PS structure that we talk about.",
            "I interchange them because I usually do it that way, but you don't care because you can.",
            "You can flip the columns around so you have an identity matrix.",
            "An RP transpose matrix on the others OK.",
            "So what do we know?",
            "What is the restriction that we have over X1X5 six and seven?",
            "Anyone?"
        ],
        [
            "And over X2X, four X = 7.",
            "Or the other.",
            "So yes, remember."
        ],
        [
            "That X for each transpose has to be 0.",
            "OK, so it means that if you look X One X 5867.",
            "Are the four ones.",
            "That I have in the first row of H. So I have a one in position 156 and seven.",
            "OK, so when I multiply X by the 1st row of H. I should get a civil.",
            "OK, so that's a pretty constraint.",
            "Those four bits.",
            "They had up to add up to 0.",
            "OK, so we have 4 zeros.",
            "For once or two ones and two zeros.",
            "Any other possibility is not allowed.",
            "OK.",
            "So we're going to represent that in a graph the following way.",
            "OK. Stop whoops.",
            "This.",
            "These circles over here represent the variables that we are transmitting the X1.",
            "All the way to 8 seven.",
            "OK, and the lines connect to a.",
            "We call a factor.",
            "Which is the row.",
            "Of the matrix.",
            "OK, so the first row is connected to variable one.",
            "567 and we call.",
            "This is a check node because you have to check that the four variables add up to 0.",
            "OK.",
            "The next one.",
            "It's connected to 24.",
            "Six and seven.",
            "OK."
        ],
        [
            "And the last one is connected to 34517 and that's what we call a Telegraph.",
            "It's also called a factor graph and also called bipartite graph.",
            "The byproduct bipartite comes from the thing that we have variables on one side, factors on the others and the variable can Only Connect to the factors and.",
            "The factor can Only Connect to the variables.",
            "OK. And for this one there is a nice rearrangement which is the following."
        ],
        [
            "OK, here's the same graph.",
            "I just put it in a way so the lines don't cross and it's easier to see.",
            "OK, I laid out so we have the four bits that I have over here.",
            "Will be the four information bits and one and two and three and four, and the access that I left out.",
            "Are there one that other redundancy bits?",
            "So then it's very easy to use this graph in order to do the encoding.",
            "OK, so for example, if I put out 100 and a zero here.",
            "Like we know that this has to be a parity constraint.",
            "If I have a wanna 00, that guy has to be a one.",
            "This guy has to be a one as well.",
            "And this guy has to be assumed.",
            "OK, so we can also use the graph.",
            "For encoding purposes.",
            "OK, that will be X one X2 and X3.",
            "In case you were wondering, but.",
            "Actually, we don't.",
            "We don't care because we can put it any other that worked.",
            "OK.",
            "It's clear how I constructed the graph from the H metrics.",
            "Yes."
        ],
        [
            "OK, so now we're going to try to use this graph.",
            "OK, in order to get the maximum likelihood they cover OK.",
            "So or actually we're going to do something different.",
            "OK, so the maximum likelihood they called as we said before it was the argument of the maximum of the quality of Y given X. OK. And instead of putting extra via code work, we can do this with a different operation.",
            "So we can say that we're looking for the maximum of all possible axes in which we have to maximize the likelihood.",
            "And make sure that all the parity constraints are fulfilled.",
            "OK, so that will be a Delta function.",
            "So if XH transpose.",
            "Is equal to 0.",
            "We get one, otherwise we get us here.",
            "OK, so we need to fulfill all those constraints and then maximize the likelihood.",
            "OK. Again, the solution in this case is exponentially now in the general case OK, and then there is also a different problem that we sometimes want to solve, which is what we call the bitwise map.",
            "OK, so the bitwise math solution, but it does is.",
            "It uses base equation to flip the solution and then we use for the maximum for each one of the bits individually.",
            "OK, so the map solution is identical to them.",
            "L solution in this case.",
            "OK, be 'cause all the all the words are equally likely.",
            "But the bitwise map solution is not identical to the email solution.",
            "OK. And the way to see it is in the email solution.",
            "We enforce that they were that we get is a code word.",
            "OK, in the in the bitwise map solution we try to get the best solution for every bit.",
            "We don't care about anything else.",
            "So given all the information, all the wise that I got for the first wait, what is the best value?",
            "For the second bit, what is the best value?",
            "And so on and so forth.",
            "And that doesn't have to give you the optimal and I real code can give you something that is different from the real code.",
            "OK, of course, both solutions are pretty close and in most cases they coincide, but there are cases that they don't.",
            "OK. Yep.",
            "Can you speak up?",
            "There is a new device.",
            "Hey Yep, we will go into that in a minute so.",
            "There are two arguments to those, so if we do them L solution where you make sure is that you get the minimum probability of getting are working correctly.",
            "OK, so.",
            "You will get the least error rate in works in the number of total words that you're transmitting.",
            "If you if you do the meat wise map solution, you get the minimum probability of error in the bits that you are receiving.",
            "OK.",
            "So those are the two arguments for the for that order.",
            "OK, so the second one is optimal.",
            "At bit level, the first one is optimal at world level, but the thing that we're going to use the bitwise map solution is still exponential.",
            "The complexity for getting the optimal solution is the same one.",
            "The only thing that we've got.",
            "Is that there are tricks in order to try to get the solution in a linear time?",
            "OK. And that's what we're going to go and use the graph.",
            "OK, so the idea is, let's try to use the graph in order to get a linear solution and approximate linear solution to the bitwise math solution in linear time.",
            "OK. And that's something that we cannot do for the email solution, so idea is.",
            "Even if we wanted to get the email solution, we're going to use as proxy.",
            "The bitwise map solution, because we can compute it in nearly linear time.",
            "OK."
        ],
        [
            "OK.",
            "This is just coming again to the same thing, so.",
            "We first compute the maximum posteriori solution for the P of X. OK, and again P of X given Y if P of X is uniform.",
            "The maximum of this will be the same as the maximum of that.",
            "But we want to get the bitwise map solution, so we're going to Add all the bits.",
            "S at X equal to VI and then once we get this thing.",
            "We have to decide which one of these larger if V equal to 0 or V equal to 1.",
            "OK, The thing is, this sum is in general.",
            "An exponential complexity.",
            "We will have to have to compute Tour 2 to the power of N elements and we have two to the power of minus.",
            "One is actually is minus two sums.",
            "OK.",
            "But that is if we ignore the graphic structure.",
            "OK, let's try to use the graphic structure in our."
        ],
        [
            "In at an in our favor, and if we do, we can really simplify this solution that we can.",
            "OK that thing is something that I prepare in 2009 where the joke was kind of funny.",
            "Not anymore, I guess."
        ],
        [
            "OK, so I draw a graph.",
            "OK.",
            "I drew a graph where we have 7 variables and four factors.",
            "And if we want to compute the solution, we will have to do the same for X2 all the way to X3.",
            "I'm going to compute P of X1.",
            "For all those factors that we have OK. And to simplify things, because I don't want to be carrying all the wise over here.",
            "I'm assuming that I incorporated the information from the likelihoods in each one of the factors.",
            "OK, like this is a function of X, one X2 and X3.",
            "The priority of Y1 given X one.",
            "I can put it in there.",
            "OK, they say with two and three.",
            "Therefore I can put it in D day, five under 6.",
            "I can put it in B and then I can put it in C. OK ask why is something that we observe that we know.",
            "Something that is fixed so we're not going to care about it, so I don't want to carry a lot of variables that we that we cloud everything that we're doing.",
            "OK, so we just have the four factors, and if we do this some we have to compute 128 elements.",
            "We have to add up 64 of them, 4X1 equal to 0.",
            "64 of them, 4X1 equal to 1 and then compare them in order to make a decision.",
            "OK."
        ],
        [
            "What I just did in here is I rearrange the sums.",
            "OK, I rearranged the sums in a way that is going to be convenient for me.",
            "He's going to make all these doing this seems pretty easy.",
            "I'm going to start from the end from the right hand side and I'm going to be computing the sums 1 by 1.",
            "OK, so the first time that I'm going to do is 6 seven.",
            "And I'm going to assume that X5 is fixed.",
            "OK, so if I do that I only have to compute four terms.",
            "FC4X5 equal to 0 seven C = 0 and 01 and for that I get a function that is only a function of X5.",
            "OK, if I'm adding X7 I only get a function that depends on X5.",
            "Everybody sees that.",
            "OK.",
            "If I put this in a general way, so factors could be anything.",
            "If the factors are parity check factors.",
            "Actually, the one on the.",
            "This one and this one will have to be 0.",
            "OK, but you can do this for any factor.",
            "They don't have to be parity checks, so that's what I put in the whole thing for computing this time I need to compute four components and performance sounds."
        ],
        [
            "OK. And now I'm going to do some's at the same time, so I'm going to do this one of X4 that if you look is the same one that we did for examine before.",
            "And I'm going to do this one which is X5 and X6, and I'm going to get a function.",
            "Affect OK, forget about this.",
            "Be going to X2 they will.",
            "They will mean something afterwards.",
            "OK, the important thing here is that I get a function of X2 when I sum X5 and X6 and I get a function of X3 when I sum X4.",
            "For this one I need to compute 8 components and performance exams.",
            "For this, one is again the same thing for and."
        ],
        [
            "And for the last one, I'm going to add X 2X3.",
            "On the thing that I'm going to do is again, I have to compute components under forcing something six times.",
            "OK, and now we know that P of X one is proportional to.",
            "These are a of X1.",
            "This function of X1 and we just normalize.",
            "And when we normalize, we get there.",
            "Effects one if the project one is greater than 0.5 then will say that the map solution is X1 equal to 1.",
            "If it's less than 0.5, will say that the map solution.",
            "He said so on equal to 0.",
            "OK.",
            "I'm sure all of you have been counting.",
            "But you can see."
        ],
        [
            "We only do 26 computation and $0.17, so it's an order of magnitude less.",
            "But if we have done everything altogether.",
            "OK. That's the beauty of rearranging systems that once we do it that way.",
            "We can do everything in a much fewer computation, and we've been using the graph structure in order to do that.",
            "OK, the good thing is, as we've been doing this thing.",
            "We also recover the partition function.",
            "This C value which is going to be the same one for all of them.",
            "We can get it from the sum of this of these two values OK, and for the other marginals we have to do a little bit more of work.",
            "Actually, if we wanted to get X2, we have to arrange and rearrange the sums in a different way.",
            "If you want to get X7, we have to rearrange the samples in a different way.",
            "OK, so that seems like a little.",
            "I mean, even if I did this very fast now you tell me OK, if I have to repeat these things for each one of the variables.",
            "At the end I'm going to get the same complexity.",
            "OK so I'm not making anything there.",
            "Good thing is."
        ],
        [
            "We don't.",
            "We don't need to sort the variables.",
            "We can do it without sorting the variables.",
            "OK."
        ],
        [
            "And this is what I'm.",
            "The message passing algorithm and what the notation that I was using before is.",
            "Is helpful, so if you remember there are of the name of the factor to a variable, what it was telling.",
            "Is the information that each one of the factors.",
            "Knows about that variable.",
            "OK, and that's a local computation.",
            "So in this case, each one of the factors is going to send a message to each one of the bit about the information that they know about the bits.",
            "OK. And then the variables is going to send to the factors.",
            "The information about what they know about themselves.",
            "And you're going to iterate.",
            "Until that algorithm converges.",
            "OK. And when it does.",
            "You get the bitwise map solution for each one of the base value.",
            "OK.",
            "So the idea is very simple is I'm a variable X one and I want to know what my value is.",
            "OK, so you spent all the facts that are connected to me to send information that they know about me, that I didn't tell them before.",
            "And I'm going to tell each one of the facts of the information that I know about myself, that we didn't know about me.",
            "OK.",
            "This algorithm works and it works in a finite number of iteration if.",
            "And this is a big if.",
            "Guardian has no loops.",
            "OK.",
            "I plot the graph in a way.",
            "That you don't know if he had looks or not, but it's easy to see.",
            "This is the graph that I was using.",
            "There are no loops OK.",
            "There are loops.",
            "There are two things that you can do.",
            "OK, there is something called a junction tree algorithms that tried to collapse variables.",
            "Together until you get out.",
            "A loopless graph.",
            "And then you can apply this algorithm.",
            "And then you can do the engineering solution.",
            "Which is all the message that I'm using our local.",
            "So from a local perspective I don't know.",
            "There are looks, so I still use it.",
            "Let's see what happens.",
            "OK. And if it's work, why should I worry?",
            "OK. And that's actually what we do.",
            "Then we worry when it's not working, but initially we don't worry about it.",
            "OK, so let me remind you about the the message."
        ],
        [
            "So we can see it.",
            "OK, and what I'm going to do is I'm going to add a factor now for each one of them, incorporating the information about why OK. OK, because this is the best way that we can incorporate information about why in each one of them.",
            "So just assume that I have.",
            "OK, and assume there is a Y at the end of each one of them.",
            "OK, so the factor the variable is going to send to each factor.",
            "The information it knows about himself that the factor ignores."
        ],
        [
            "OK so X2 is connected to a B and each.",
            "OK, so has to send information about 2X22A2E2.",
            "And to be and it has to send all the information that we know about itself.",
            "OK."
        ],
        [
            "And then the factors has to send information to all the bits or all the variables that is close by.",
            "So a has to send information to X1.",
            "XX3 and X2.",
            "An important thing here is that you don't need to replicate information."
        ],
        [
            "OK, so we called the message from the variables to.",
            "The factors is all the information that X2 knows about itself.",
            "That doesn't know the factor already.",
            "OK so X2 sends the information that it goes from B.",
            "Two way and he says the information that he got from E22A.",
            "But we don't multiply by the information that a already has about us, so we don't multiply by air of a 2X2.",
            "OK, and the factors do the same thing."
        ],
        [
            "They get the information from the different variables so they get the information for next 2 from X1 and X3.",
            "And they also have the factor function.",
            "They have to add for everything that you have in the factor.",
            "OK, and then they send that information back to the to the node.",
            "So in a way.",
            "Is what I what I just said is X2 tells the factor everything that it knows about itself, that the factor doesn't know already.",
            "And the factor says you have to fulfill several conditions.",
            "That's a factor, and I have this information from these other variables that you don't know, so I'm going to send it to you.",
            "OK, and here is what you can see the problem when you have looks when you don't have loops.",
            "The factor knows is sending always new information.",
            "That the variable doesn't know.",
            "But when you have loops, all this information propagates through the loops and you can get a feedback of the same information that you already saw through the loop.",
            "OK. What we always hope is that that information is distorted.",
            "So when it gets to you.",
            "Is not really going to change what you know about yourself.",
            "OK, again, sometimes those things don't happen.",
            "OK, but we were worried that in a second.",
            "OK, so this is the algorithm and then it's easy to prove that algorithm.",
            "The complexity of algorithm is linear because if the if you don't have.",
            "The number of iterations that you need to converge is the maximum path in your graph.",
            "On each one of the operations, you expect that in each one of the factors you have a finite number of very low number of variables.",
            "OK.",
            "So it will be exponential in those variables, But if those variables remain fixed.",
            "He's not exponential.",
            "As you increase the graph.",
            "OK. And I think this is what I want to break."
        ],
        [
            "OK, this is the next one and this is just I don't want to put this equation initially because every time people see.",
            "The general equation that you say, hey, this doesn't make any sense.",
            "OK so the only thing that I'm saying in the first equation is you have to multiply all the information that you get from the from the other factors except the ones that you know about yourself.",
            "So XM send information to Jay of all the other factors affect what it knows from J.",
            "And M the cardinalities there set M has all the factors that are connected to XN.",
            "OK. And they were able to factor in the same thing.",
            "You have to add for all the variables at the one year in transmitting on.",
            "And you have to multiply by all the message that you got is that the one that you got from that value?",
            "OK, those are where these two equations these two equations.",
            "OK. And this is the algorithm that we're going to try to analyze."
        ],
        [
            "OK.",
            "So the important thing about the message passing is first thing is we don't need to sort the sums.",
            "OK, that sounds are already.",
            "We can do this message passing without sorting the sums OK and we don't need to know the structure of the whole graph.",
            "Each factor it only needs to know who is neighbor 2.",
            "And is valuable only knows to know Mr know who is neighboring.",
            "Even you need to care about the whole structure.",
            "And this is what we can apply the algorithm either, even if you have looks in it because locally you don't see any loops.",
            "OK, we yes no that if you had loops it might not work.",
            "OK. And that's the last thing, OK?",
            "As I said, for general graph is not applicable, you can use the junction tree algorithm and I'm sure something will talk about it.",
            "But in many cases it works well in practice and for channel coding is one of those cases.",
            "Because we're going to build the graph so we have few loops.",
            "Sexually, that's the whole point.",
            "OK.",
            "So.",
            "And that is the thing that I will see in an image.",
            "OK."
        ],
        [
            "So we're going to stop here for 10 minutes, and then we continue."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as Neil said, when he started his lecture, he has a point of view how he wants to do things and how he wants to do the.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "I also have some kind of a message that I want to convey in the in the talks at all.",
                    "label": 0
                },
                {
                    "sent": "I want to do so.",
                    "label": 0
                },
                {
                    "sent": "Initially I thought of just giving up your information theoretic course and then I thought for the type of people that we have.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's better to leave overview of why I think machine learning might be good for information theory.",
                    "label": 0
                },
                {
                    "sent": "OK, what kind of problems they are solving nowadays in information theory?",
                    "label": 0
                },
                {
                    "sent": "In which machine learning can bring?",
                    "label": 0
                },
                {
                    "sent": "And a fresh look into into those into those problems.",
                    "label": 0
                },
                {
                    "sent": "And also what kind of difficulties we can find on their way in order to solve those problems.",
                    "label": 0
                },
                {
                    "sent": "So I think there are going to be talking.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking mainly about a simple example which is channel coding, but there are two things in information theory that are relevant now that nowadays which is network information theory and known as synthetic information theory, and in both of those problems.",
                    "label": 0
                },
                {
                    "sent": "I think machine learning solutions.",
                    "label": 0
                },
                {
                    "sent": "Can really bring benefit to the type of problems that they are.",
                    "label": 0
                },
                {
                    "sent": "They are looking into it and I'll try to explain as I go alone.",
                    "label": 0
                },
                {
                    "sent": "I'll do a summary at the end to see if that if I can explain that that better.",
                    "label": 0
                },
                {
                    "sent": "So my idea here is just to show how to solve the channel problem with LBC codes, which is roughly using belief propagation which is an we could say so much in learning algorithm, although it has many faces and you can see it also in.",
                    "label": 0
                },
                {
                    "sent": "In communications you can see in transportation there are many other ways that you can derive information, but the way it was used to solve the channel coding problem, it came from a machine learning point of view, and I think that's that's important too.",
                    "label": 0
                },
                {
                    "sent": "To tell you about, so I'm going to start showing a few.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things in information theory and which type of problem I'm going to be working on an I'm not going to give a lot of information about communications or information theory, but just enough for you to know exactly what we want to do.",
                    "label": 1
                },
                {
                    "sent": "So the main problem that we have is we have a source of information that we want to transmit to another end through a channel.",
                    "label": 0
                },
                {
                    "sent": "That's the whole point.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a source that gives us symbols that can be text.",
                    "label": 0
                },
                {
                    "sent": "Can be boys can be video can be anything that you want.",
                    "label": 0
                },
                {
                    "sent": "OK and it has a certain probability measure.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want to convey that information to another end.",
                    "label": 0
                },
                {
                    "sent": "OK. And we want to do before we do that, we want to do two things.",
                    "label": 0
                },
                {
                    "sent": "The first thing is we want to compress the source.",
                    "label": 0
                },
                {
                    "sent": "OK, we want to take out all the redundancy that there is in the source.",
                    "label": 0
                },
                {
                    "sent": "So if we have less to transmit, it will cost us less.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can know an image or video voice.",
                    "label": 0
                },
                {
                    "sent": "Music.",
                    "label": 0
                },
                {
                    "sent": "It is very redundant sources, so we want to compress as much as we can.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But then we're going to transmit that through a faulty channel.",
                    "label": 0
                },
                {
                    "sent": "That is going to introduce errors into it.",
                    "label": 0
                },
                {
                    "sent": "So once we.",
                    "label": 0
                },
                {
                    "sent": "Taking out all the windows in the race in the in the in the source, we're going to add control the random redundancy.",
                    "label": 0
                },
                {
                    "sent": "And we're going to add control redundancy to get rid of those errors that we're going to get into the channel.",
                    "label": 0
                },
                {
                    "sent": "So that those are the two main points that we want to do.",
                    "label": 0
                },
                {
                    "sent": "First, remove redundancy so we transmit less information.",
                    "label": 0
                },
                {
                    "sent": "Let's bet.",
                    "label": 0
                },
                {
                    "sent": "And then add redundancy in order to control the errors that we that we have in the into the channel.",
                    "label": 0
                },
                {
                    "sent": "OK. And the thing that we have and the results are we get is that.",
                    "label": 0
                },
                {
                    "sent": "There's also encoder has to compress the source to sum right above the entropy of the of the source.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the entropy of the source is 5 bits per second.",
                    "label": 0
                },
                {
                    "sent": "You have to use at least 5 bits per second.",
                    "label": 0
                },
                {
                    "sent": "If you use less, the source is going to be recoverable.",
                    "label": 0
                },
                {
                    "sent": "OK, and actually the minimum.",
                    "label": 0
                },
                {
                    "sent": "I mean the quality sign is what we is what we call the minimum rate that we can we consume it.",
                    "label": 0
                },
                {
                    "sent": "And then the channel encoder has the opposite, the opposite thing.",
                    "label": 0
                },
                {
                    "sent": "Is you have to transmit information at less rate that the channel can take.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the channel is introducing error at a certain rate.",
                    "label": 0
                },
                {
                    "sent": "You have to transmit less information at that rate in order to be able to recover information.",
                    "label": 0
                },
                {
                    "sent": "OK, so roughly is telling you OK if you have one bit per second that you want to transmit.",
                    "label": 0
                },
                {
                    "sent": "But the mutual information between the input on the output is 0.5.",
                    "label": 0
                },
                {
                    "sent": "It means that for each information bit that you're putting in, at least you have to put one redundancy in it.",
                    "label": 0
                },
                {
                    "sent": "So your total rate is only one bit of information every 2 seconds.",
                    "label": 0
                },
                {
                    "sent": "OK, that is what is is what is telling you that lower bound.",
                    "label": 0
                },
                {
                    "sent": "Of course these two right they have to be equal, so if they're right that you're encoding the source encoding is higher than the other one.",
                    "label": 0
                },
                {
                    "sent": "There is nothing you can do.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "When we achieve when we do are equal to the mutual information between X&Y is what we call channel capacity.",
                    "label": 0
                },
                {
                    "sent": "OK, and actually we have to maximize with respect to your input distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But the thing that we are that we are thinking in here is that we cannot optimize over the input distribution the X, because that's going to be given by our source.",
                    "label": 0
                },
                {
                    "sent": "OK, So what actually it happens in most cases is if there's also the words correctly.",
                    "label": 0
                },
                {
                    "sent": "It will give you an IID source of the probable signals.",
                    "label": 0
                },
                {
                    "sent": "That's the best thing that you can do.",
                    "label": 0
                },
                {
                    "sent": "And then most of the channels that will be the optimal distribution for most channels.",
                    "label": 0
                },
                {
                    "sent": "And then we can achieve the channel capacity.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm only going to focus on this problem and then there is one thing that is called the separation principle that it tells you that you can do both things separately.",
                    "label": 0
                },
                {
                    "sent": "OK. That is.",
                    "label": 0
                },
                {
                    "sent": "It doesn't always OK, for example, for network information doesn't hold, but when you do point to point communication it holds and roughly that is the way we've been designing things.",
                    "label": 0
                },
                {
                    "sent": "So you can see source encoders you can think of many different things, so you have JPEG.",
                    "label": 0
                },
                {
                    "sent": "You have impact, you have MP threes.",
                    "label": 1
                },
                {
                    "sent": "All these standards are roughly doing source encoding, trying to reduce the redundancy of the source to independent and equally probable bits.",
                    "label": 0
                },
                {
                    "sent": "OK, lemme see.",
                    "label": 0
                },
                {
                    "sent": "Logarithm is the same thing.",
                    "label": 0
                },
                {
                    "sent": "And then in the transmission rate we have also a set of algorithms trying to get R as close to the mutual information and reduce the quality of.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to talk about those in a little bit, but we have always.",
                    "label": 0
                },
                {
                    "sent": "Design then.",
                    "label": 0
                },
                {
                    "sent": "Separately, so we're going to have this also encoder.",
                    "label": 0
                },
                {
                    "sent": "You're going to have somebody that is an expert in this source.",
                    "label": 0
                },
                {
                    "sent": "Trying to compress a source as much as you can, and then you're going to have a channel encoder expert that is going to try to put around and see in the smartest possible way.",
                    "label": 0
                },
                {
                    "sent": "OK, in order to get to the.",
                    "label": 0
                },
                {
                    "sent": "So that you have to put in the less amount the least amount of redundancy into the bits that you want to transmit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into the into the proof of any of these things.",
                    "label": 0
                },
                {
                    "sent": "But I'm sure you remember the example that Niels put the first day about.",
                    "label": 0
                },
                {
                    "sent": "They they widen the yellow and that when you tend to Infinity the green part, the one that is in the middle.",
                    "label": 0
                },
                {
                    "sent": "Took all the probability mass.",
                    "label": 0
                },
                {
                    "sent": "That's the that's roughly the proof.",
                    "label": 0
                },
                {
                    "sent": "To get those those values.",
                    "label": 0
                },
                {
                    "sent": "OK, the thing that even say is that in order to get to those values, you are going to get zero.",
                    "label": 0
                },
                {
                    "sent": "You're going to have.",
                    "label": 0
                },
                {
                    "sent": "Probability of error of making a mistake.",
                    "label": 0
                },
                {
                    "sent": "On the other end, as small as you want.",
                    "label": 0
                },
                {
                    "sent": "OK, if you transmit below that rate or if you compress up of the entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's the dust.",
                    "label": 1
                },
                {
                    "sent": "Those are the main results that I'm going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "So right now what I'm going to be concentrating on.",
                    "label": 0
                },
                {
                    "sent": "Is in this spot?",
                    "label": 0
                },
                {
                    "sent": "How can we introduce redundancy in order?",
                    "label": 0
                },
                {
                    "sent": "To protect the information that we that we want to transmit.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to assume that the source or the source encoder give us a set of symbols.",
                    "label": 0
                },
                {
                    "sent": "OK, all the way one to the end and the channel the channel encoder is going to give each one of these symbols.",
                    "label": 1
                },
                {
                    "sent": "A string of bits.",
                    "label": 0
                },
                {
                    "sent": "Up to N bits.",
                    "label": 0
                },
                {
                    "sent": "OK, typically we will use C for those.",
                    "label": 0
                },
                {
                    "sent": "But in order to make sense of all the machine learning that we're doing, I'm going to be using X during that, and if the message that comes from the source encoder is also binary, we're going to assume that we have.",
                    "label": 1
                },
                {
                    "sent": "Kabit an all over the talk K. It's going to be less than.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to use.",
                    "label": 0
                },
                {
                    "sent": "We're going to use.",
                    "label": 1
                },
                {
                    "sent": "K information bits, and we're going to add.",
                    "label": 0
                },
                {
                    "sent": "N -- K. Read and submit.",
                    "label": 0
                },
                {
                    "sent": "That additional redundancy bits should allows us.",
                    "label": 0
                },
                {
                    "sent": "To find errors and correct the errors, OK. And if these are is less than the mutual information, as we make NNK tend to Infinity.",
                    "label": 0
                },
                {
                    "sent": "We should be able to correct every error that appears in the code so we can get an error less communication if we make NK large enough.",
                    "label": 0
                },
                {
                    "sent": "OK, those are the results of channel coding that if you make the right low enough you can get an errorless communication through any communication channel.",
                    "label": 0
                },
                {
                    "sent": "OK. Those results are due to Shannon in the 40s.",
                    "label": 0
                },
                {
                    "sent": "An they were remarkable because in the 40s they thought that the only way to get errors communication was to get error zero rate.",
                    "label": 0
                },
                {
                    "sent": "So you have to put more and more redundancy in order not to get any errors and what he proved.",
                    "label": 0
                },
                {
                    "sent": "Was basically that you can get errors communication at a finite rate.",
                    "label": 0
                },
                {
                    "sent": "And since we knew that this limit exists, we've been trying to design encoders and decoders to get to that right?",
                    "label": 0
                },
                {
                    "sent": "OK, and that's roughly what LDC codes achieve, we're going to be talking about curing the election problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first thing that we're going to do is talk about linear encoding and the thing about talking about linear encoding is if you think about the problem coding in general.",
                    "label": 0
                },
                {
                    "sent": "If you're going from K bits.",
                    "label": 0
                },
                {
                    "sent": "To ambit.",
                    "label": 0
                },
                {
                    "sent": "That's an exponential problem, either for the foreign cover.",
                    "label": 0
                },
                {
                    "sent": "OK, because we have to have a table that says.",
                    "label": 0
                },
                {
                    "sent": "Each of the inputs how they go through each one of the outputs, so we'll have to have a two to the K table saying what is the mapping from one to the other.",
                    "label": 0
                },
                {
                    "sent": "If we use a linear encoding.",
                    "label": 0
                },
                {
                    "sent": "That's only quadratic in the number of bits that we know we want to transmit, so we're going to use and this is something.",
                    "label": 0
                },
                {
                    "sent": "It's weird, but I'm going to be using bro vectors.",
                    "label": 0
                },
                {
                    "sent": "I mean when we did, is this type of things in channel encoding everything is raw vectors so.",
                    "label": 0
                },
                {
                    "sent": "Is completely the opposite that everything else.",
                    "label": 0
                },
                {
                    "sent": "But is the standard way of doing it and I don't want to change it in case you look at other things.",
                    "label": 0
                },
                {
                    "sent": "So M is a row vector of K components and G is a K by N matrix.",
                    "label": 1
                },
                {
                    "sent": "That converts they K initial bits into a bit OK, and all the additions or multiplications.",
                    "label": 0
                },
                {
                    "sent": "Animal model is 2 OK, so we're only going to be working over a bit.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not going to put it not to cloud the information and there are several results.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about them, but linear channel encoding is good enough to get to capacity, so we're not losing anything for not using linear and nonlinear mapping.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is.",
                    "label": 0
                },
                {
                    "sent": "You can always prepare the geometrics to have an identity matrix and AP matrix.",
                    "label": 0
                },
                {
                    "sent": "So the first N bits.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a copy of your message.",
                    "label": 0
                },
                {
                    "sent": "The last N -- K bit is going to be that redundancy that you that you had today.",
                    "label": 0
                },
                {
                    "sent": "And to the information.",
                    "label": 0
                },
                {
                    "sent": "OK, so X in that if we use G in that sense is going to be.",
                    "label": 0
                },
                {
                    "sent": "I'm done some.",
                    "label": 0
                },
                {
                    "sent": "Redundance events Yep.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "OK. On the other thing that we typically assume is that the channel worked in independent realizations, so we expect that every time that we transmit a symbol.",
                    "label": 0
                },
                {
                    "sent": "A noise is added to that bit, independent from any other of the of the of the of the noise to the other on the other bids.",
                    "label": 0
                },
                {
                    "sent": "If that doesn't happen well, we talking about a completely different problem and we don't want to get into that, but this is a good assumption in most communications system of of interest.",
                    "label": 0
                },
                {
                    "sent": "OK, when I say is iid noise.",
                    "label": 0
                },
                {
                    "sent": "So depending of if P of Y given X is is is a Gaussian channel.",
                    "label": 0
                },
                {
                    "sent": "CI is going to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If it's a binary symmetric channel, CI is going to be over newly random variable.",
                    "label": 0
                },
                {
                    "sent": "Depending on which type of channels we think CI is going to be one distribution or another, OK, but it's typically known when you decide this system OK. And the maximum in the channel decoder the object is to find the maximum likelihood solution.",
                    "label": 0
                },
                {
                    "sent": "So if you look from the point of view of the maximum likelihood, the function is pretty simple because it.",
                    "label": 0
                },
                {
                    "sent": "An is a product of the marginals and it should be easy to optimize.",
                    "label": 0
                },
                {
                    "sent": "The problem comes from the restriction.",
                    "label": 0
                },
                {
                    "sent": "Because we need X to be a code word.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the that's the main.",
                    "label": 0
                },
                {
                    "sent": "That's the main issue, and that is what in general makes the problem.",
                    "label": 0
                },
                {
                    "sent": "Exponential, so we need to try every possible solution in order to get to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so I then tell you about this, but the full objective of solving this problem is that we want at least linear complexity in the encoder and we want at least linear complexity in the decoder.",
                    "label": 0
                },
                {
                    "sent": "OK, if we have the possibility of exponential complexity, the problem is very simple.",
                    "label": 0
                },
                {
                    "sent": "Just choose the world at random and it works.",
                    "label": 0
                },
                {
                    "sent": "OK. That's what Shannon that's what Shannon proved us OK, but that it gives you exponential complexity and I'm sure you all remember Robin Roberts lies with this potential complexity.",
                    "label": 0
                },
                {
                    "sent": "That it really blows in your face very, very soon.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we want to do is we want everything to be linear complexity or as close to linear as we as we can, and that's under the Latino want.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the next thing that we need for the in order to get what we want is what we call the dual space of G. OK, and that all spaces and orthogonal.",
                    "label": 1
                },
                {
                    "sent": "So G defines a linear linear subspace of dimension K, so H. Is a linear subspace of dimension N -- K that is orthogonal to G OK an?",
                    "label": 0
                },
                {
                    "sent": "Age is just to compute the syndrome.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we transmit there were X and we got the Y.",
                    "label": 0
                },
                {
                    "sent": "In the decoder, the first thing that we can do is multiply that by the matrix H, transpose OK, and the matrix agents pose.",
                    "label": 0
                },
                {
                    "sent": "Under syndrome, it only depends on the noise.",
                    "label": 0
                },
                {
                    "sent": "Doesn't tell you anything about X OK, and that is pretty easy to see which is.",
                    "label": 0
                },
                {
                    "sent": "Extends extra suppose.",
                    "label": 0
                },
                {
                    "sent": "Is N * G. And we have.",
                    "label": 0
                },
                {
                    "sent": "And that's equal to 0.",
                    "label": 0
                },
                {
                    "sent": "OK. You're going to see that in all the examples that I'm going to use, minus P is going to become P. Oh, minus P transpose is going to become P transpose, and this is only because I'm working with binary symbols.",
                    "label": 0
                },
                {
                    "sent": "OK, so minus one is 1.",
                    "label": 0
                },
                {
                    "sent": "Right, but if we were working with other type of symbols, you need the minus on on top of that.",
                    "label": 1
                },
                {
                    "sent": "OK, again, this syndrome unique uniquely identifies their pattern that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "But again, it's an exponential table.",
                    "label": 0
                },
                {
                    "sent": "OK, it's going to be we'll have two to the K entries and FSC and grows with a fixed rate, maybe conservation.",
                    "label": 0
                },
                {
                    "sent": "So he's not really going to give us a solution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so which type of solutions have been given to these problems?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "A very short summary of what we have and then we have the.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we came up with with algebraic codes, so I'm sure most of you have heard of Hamming codes.",
                    "label": 0
                },
                {
                    "sent": "Anyone who has heard about Hamming codes?",
                    "label": 0
                },
                {
                    "sent": "Most people go like oats.",
                    "label": 0
                },
                {
                    "sent": "OK, very few red Muller codes.",
                    "label": 0
                },
                {
                    "sent": "Read Solomon codes.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are the type of codes that where they sign in the in the from the faulty something in 70s.",
                    "label": 0
                },
                {
                    "sent": "And the objective there was always to use something that is called a cyclic code.",
                    "label": 0
                },
                {
                    "sent": "Which instead of using this matrix.",
                    "label": 0
                },
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "It could be encoded with a linear time machine with a linear in linear complexity, and there were design in order to maximize the distance between the words.",
                    "label": 0
                },
                {
                    "sent": "OK, the idea was to put all the words as far as possible from each other.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the recorders.",
                    "label": 0
                },
                {
                    "sent": "They only corrected up to the half of that distance, so the number of errors that they have.",
                    "label": 0
                },
                {
                    "sent": "And that seems to work fine, but as you get to high dimension there are many areas of the space that you're going to explore, so you cannot get to capacity with it.",
                    "label": 0
                },
                {
                    "sent": "OK, so they are very nice.",
                    "label": 0
                },
                {
                    "sent": "I'll give reconstructions.",
                    "label": 0
                },
                {
                    "sent": "But they are unable to get to capacity.",
                    "label": 0
                },
                {
                    "sent": "The good thing of all these codes is that the encoding is linear.",
                    "label": 0
                },
                {
                    "sent": "You can do in linear time.",
                    "label": 0
                },
                {
                    "sent": "Under recording you can do it in linear time and for example the Red Solomon codes.",
                    "label": 0
                },
                {
                    "sent": "You can find them in your CD's and DVD's.",
                    "label": 0
                },
                {
                    "sent": "That's the algorithm of choice.",
                    "label": 0
                },
                {
                    "sent": "In order to decode those.",
                    "label": 0
                },
                {
                    "sent": "Those standards OK.",
                    "label": 0
                },
                {
                    "sent": "In the 60s we came thing was 15.",
                    "label": 0
                },
                {
                    "sent": "Actually we came up with what we call the convolutional codes.",
                    "label": 0
                },
                {
                    "sent": "And then they also have a linear encoding and linear decoding.",
                    "label": 0
                },
                {
                    "sent": "But in order to.",
                    "label": 0
                },
                {
                    "sent": "For a fixed memory, but in order to get to capacity you have to make the memory go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Under recording this potential in the memory so you also couldn't get to.",
                    "label": 0
                },
                {
                    "sent": "A to capacity and then in the 90s from the 90s we have Turbo codes and NDC codes and the idea there is going back to what Shannon told us which is try to use one.",
                    "label": 0
                },
                {
                    "sent": "OK, and try to get always the best answer by week.",
                    "label": 0
                },
                {
                    "sent": "OK, so algebraic coding convolutional codes are very good if you're only thinking about a few bits 5100 bits, but if you get above that.",
                    "label": 0
                },
                {
                    "sent": "The ideas that we have in low dimension that work well.",
                    "label": 0
                },
                {
                    "sent": "Don't carry over to high dimensions, that's the main limitation of algebraic coding and convolutional codes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "When you go to Olympus E Codes and Turbo codes.",
                    "label": 1
                },
                {
                    "sent": "They were well when you go to very high dimensions, 10,100 dimensions, 100,000 dimension.",
                    "label": 0
                },
                {
                    "sent": "That's when they start working working properly because random do you want.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That's when they when they started working properly OK and when I say the coding and decoding is almost linear and we'll see that the encoding you have to use the geometrics, which is will be quadratic.",
                    "label": 0
                },
                {
                    "sent": "But there are algorithms that you can simplify to make it almost linear.",
                    "label": 0
                },
                {
                    "sent": "And the recording will see an algorithm that is linear.",
                    "label": 0
                },
                {
                    "sent": "But as we get to capacity, we need more and more iterations.",
                    "label": 0
                },
                {
                    "sent": "So it's not going to be exactly linear, it's going to be a little bit, but for a fix, right?",
                    "label": 0
                },
                {
                    "sent": "Is going to be linear OK and they're almost achieved capacity?",
                    "label": 0
                },
                {
                    "sent": "For example, for the Gaussian channel, I think they are.",
                    "label": 0
                },
                {
                    "sent": "Is Zero point 4.5 to 10 to the minus 3D B?",
                    "label": 0
                },
                {
                    "sent": "Decibels to the to their capacity, so they are as good as as they get OK. What did I put 60 three over there?",
                    "label": 0
                },
                {
                    "sent": "So actually in 1963 and Rob Gallagher proposed these LDC codes in his PhD thesis.",
                    "label": 0
                },
                {
                    "sent": "OK, he proposed the code.",
                    "label": 0
                },
                {
                    "sent": "He proposed some algorithms to decode them, but he wasn't able to prove that they were good enough and they were completely forgotten afterwards.",
                    "label": 0
                },
                {
                    "sent": "OK, it took until nineteen 1995 that baby Mike, I recovered idea of LDC codes and was able to prove that they were very good and very close to.",
                    "label": 0
                },
                {
                    "sent": "To capacity, so we lost 30 years.",
                    "label": 0
                },
                {
                    "sent": "We lost 30 years there.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I want to get to the to the Howell DC codes works and there is a tool that has been used in for for a long time in information theory which is eternal graph which is trying to represent the variables.",
                    "label": 0
                },
                {
                    "sent": "On the G matrix on the H matrix into a graph OK.",
                    "label": 0
                },
                {
                    "sent": "It is just a factor graph.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't want to tell a lot about graphical models because you will have.",
                    "label": 0
                },
                {
                    "sent": "So been telling you about them next week and I'm sure he's going to do a much better work that I do.",
                    "label": 0
                },
                {
                    "sent": "The only thing that I want to do is just give you enough information to follow what we want.",
                    "label": 0
                },
                {
                    "sent": "OK, and I'm sure the way he's going to tell them is probably much different than what I'm going to be doing, so I guess it's going to be complementary any anyway, OK, so let's say that we have a parity check matrix that H over there.",
                    "label": 0
                },
                {
                    "sent": "OK, you can see the I and the PS structure that we talk about.",
                    "label": 0
                },
                {
                    "sent": "I interchange them because I usually do it that way, but you don't care because you can.",
                    "label": 0
                },
                {
                    "sent": "You can flip the columns around so you have an identity matrix.",
                    "label": 0
                },
                {
                    "sent": "An RP transpose matrix on the others OK.",
                    "label": 0
                },
                {
                    "sent": "So what do we know?",
                    "label": 1
                },
                {
                    "sent": "What is the restriction that we have over X1X5 six and seven?",
                    "label": 0
                },
                {
                    "sent": "Anyone?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And over X2X, four X = 7.",
                    "label": 0
                },
                {
                    "sent": "Or the other.",
                    "label": 0
                },
                {
                    "sent": "So yes, remember.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That X for each transpose has to be 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that if you look X One X 5867.",
                    "label": 0
                },
                {
                    "sent": "Are the four ones.",
                    "label": 0
                },
                {
                    "sent": "That I have in the first row of H. So I have a one in position 156 and seven.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I multiply X by the 1st row of H. I should get a civil.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a pretty constraint.",
                    "label": 0
                },
                {
                    "sent": "Those four bits.",
                    "label": 0
                },
                {
                    "sent": "They had up to add up to 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have 4 zeros.",
                    "label": 0
                },
                {
                    "sent": "For once or two ones and two zeros.",
                    "label": 0
                },
                {
                    "sent": "Any other possibility is not allowed.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we're going to represent that in a graph the following way.",
                    "label": 0
                },
                {
                    "sent": "OK. Stop whoops.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "These circles over here represent the variables that we are transmitting the X1.",
                    "label": 0
                },
                {
                    "sent": "All the way to 8 seven.",
                    "label": 0
                },
                {
                    "sent": "OK, and the lines connect to a.",
                    "label": 0
                },
                {
                    "sent": "We call a factor.",
                    "label": 0
                },
                {
                    "sent": "Which is the row.",
                    "label": 0
                },
                {
                    "sent": "Of the matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first row is connected to variable one.",
                    "label": 0
                },
                {
                    "sent": "567 and we call.",
                    "label": 0
                },
                {
                    "sent": "This is a check node because you have to check that the four variables add up to 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The next one.",
                    "label": 0
                },
                {
                    "sent": "It's connected to 24.",
                    "label": 0
                },
                {
                    "sent": "Six and seven.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last one is connected to 34517 and that's what we call a Telegraph.",
                    "label": 0
                },
                {
                    "sent": "It's also called a factor graph and also called bipartite graph.",
                    "label": 1
                },
                {
                    "sent": "The byproduct bipartite comes from the thing that we have variables on one side, factors on the others and the variable can Only Connect to the factors and.",
                    "label": 0
                },
                {
                    "sent": "The factor can Only Connect to the variables.",
                    "label": 0
                },
                {
                    "sent": "OK. And for this one there is a nice rearrangement which is the following.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's the same graph.",
                    "label": 0
                },
                {
                    "sent": "I just put it in a way so the lines don't cross and it's easier to see.",
                    "label": 0
                },
                {
                    "sent": "OK, I laid out so we have the four bits that I have over here.",
                    "label": 0
                },
                {
                    "sent": "Will be the four information bits and one and two and three and four, and the access that I left out.",
                    "label": 0
                },
                {
                    "sent": "Are there one that other redundancy bits?",
                    "label": 0
                },
                {
                    "sent": "So then it's very easy to use this graph in order to do the encoding.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, if I put out 100 and a zero here.",
                    "label": 0
                },
                {
                    "sent": "Like we know that this has to be a parity constraint.",
                    "label": 0
                },
                {
                    "sent": "If I have a wanna 00, that guy has to be a one.",
                    "label": 0
                },
                {
                    "sent": "This guy has to be a one as well.",
                    "label": 0
                },
                {
                    "sent": "And this guy has to be assumed.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can also use the graph.",
                    "label": 0
                },
                {
                    "sent": "For encoding purposes.",
                    "label": 0
                },
                {
                    "sent": "OK, that will be X one X2 and X3.",
                    "label": 0
                },
                {
                    "sent": "In case you were wondering, but.",
                    "label": 0
                },
                {
                    "sent": "Actually, we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't care because we can put it any other that worked.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's clear how I constructed the graph from the H metrics.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we're going to try to use this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, in order to get the maximum likelihood they cover OK.",
                    "label": 1
                },
                {
                    "sent": "So or actually we're going to do something different.",
                    "label": 0
                },
                {
                    "sent": "OK, so the maximum likelihood they called as we said before it was the argument of the maximum of the quality of Y given X. OK. And instead of putting extra via code work, we can do this with a different operation.",
                    "label": 0
                },
                {
                    "sent": "So we can say that we're looking for the maximum of all possible axes in which we have to maximize the likelihood.",
                    "label": 0
                },
                {
                    "sent": "And make sure that all the parity constraints are fulfilled.",
                    "label": 0
                },
                {
                    "sent": "OK, so that will be a Delta function.",
                    "label": 0
                },
                {
                    "sent": "So if XH transpose.",
                    "label": 0
                },
                {
                    "sent": "Is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "We get one, otherwise we get us here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need to fulfill all those constraints and then maximize the likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK. Again, the solution in this case is exponentially now in the general case OK, and then there is also a different problem that we sometimes want to solve, which is what we call the bitwise map.",
                    "label": 0
                },
                {
                    "sent": "OK, so the bitwise math solution, but it does is.",
                    "label": 0
                },
                {
                    "sent": "It uses base equation to flip the solution and then we use for the maximum for each one of the bits individually.",
                    "label": 0
                },
                {
                    "sent": "OK, so the map solution is identical to them.",
                    "label": 0
                },
                {
                    "sent": "L solution in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, be 'cause all the all the words are equally likely.",
                    "label": 0
                },
                {
                    "sent": "But the bitwise map solution is not identical to the email solution.",
                    "label": 0
                },
                {
                    "sent": "OK. And the way to see it is in the email solution.",
                    "label": 0
                },
                {
                    "sent": "We enforce that they were that we get is a code word.",
                    "label": 0
                },
                {
                    "sent": "OK, in the in the bitwise map solution we try to get the best solution for every bit.",
                    "label": 0
                },
                {
                    "sent": "We don't care about anything else.",
                    "label": 0
                },
                {
                    "sent": "So given all the information, all the wise that I got for the first wait, what is the best value?",
                    "label": 0
                },
                {
                    "sent": "For the second bit, what is the best value?",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And that doesn't have to give you the optimal and I real code can give you something that is different from the real code.",
                    "label": 0
                },
                {
                    "sent": "OK, of course, both solutions are pretty close and in most cases they coincide, but there are cases that they don't.",
                    "label": 0
                },
                {
                    "sent": "OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "Can you speak up?",
                    "label": 0
                },
                {
                    "sent": "There is a new device.",
                    "label": 0
                },
                {
                    "sent": "Hey Yep, we will go into that in a minute so.",
                    "label": 0
                },
                {
                    "sent": "There are two arguments to those, so if we do them L solution where you make sure is that you get the minimum probability of getting are working correctly.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You will get the least error rate in works in the number of total words that you're transmitting.",
                    "label": 0
                },
                {
                    "sent": "If you if you do the meat wise map solution, you get the minimum probability of error in the bits that you are receiving.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So those are the two arguments for the for that order.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second one is optimal.",
                    "label": 0
                },
                {
                    "sent": "At bit level, the first one is optimal at world level, but the thing that we're going to use the bitwise map solution is still exponential.",
                    "label": 1
                },
                {
                    "sent": "The complexity for getting the optimal solution is the same one.",
                    "label": 0
                },
                {
                    "sent": "The only thing that we've got.",
                    "label": 0
                },
                {
                    "sent": "Is that there are tricks in order to try to get the solution in a linear time?",
                    "label": 0
                },
                {
                    "sent": "OK. And that's what we're going to go and use the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is, let's try to use the graph in order to get a linear solution and approximate linear solution to the bitwise math solution in linear time.",
                    "label": 0
                },
                {
                    "sent": "OK. And that's something that we cannot do for the email solution, so idea is.",
                    "label": 0
                },
                {
                    "sent": "Even if we wanted to get the email solution, we're going to use as proxy.",
                    "label": 0
                },
                {
                    "sent": "The bitwise map solution, because we can compute it in nearly linear time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is just coming again to the same thing, so.",
                    "label": 0
                },
                {
                    "sent": "We first compute the maximum posteriori solution for the P of X. OK, and again P of X given Y if P of X is uniform.",
                    "label": 0
                },
                {
                    "sent": "The maximum of this will be the same as the maximum of that.",
                    "label": 0
                },
                {
                    "sent": "But we want to get the bitwise map solution, so we're going to Add all the bits.",
                    "label": 0
                },
                {
                    "sent": "S at X equal to VI and then once we get this thing.",
                    "label": 0
                },
                {
                    "sent": "We have to decide which one of these larger if V equal to 0 or V equal to 1.",
                    "label": 0
                },
                {
                    "sent": "OK, The thing is, this sum is in general.",
                    "label": 0
                },
                {
                    "sent": "An exponential complexity.",
                    "label": 0
                },
                {
                    "sent": "We will have to have to compute Tour 2 to the power of N elements and we have two to the power of minus.",
                    "label": 1
                },
                {
                    "sent": "One is actually is minus two sums.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "But that is if we ignore the graphic structure.",
                    "label": 1
                },
                {
                    "sent": "OK, let's try to use the graphic structure in our.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In at an in our favor, and if we do, we can really simplify this solution that we can.",
                    "label": 0
                },
                {
                    "sent": "OK that thing is something that I prepare in 2009 where the joke was kind of funny.",
                    "label": 0
                },
                {
                    "sent": "Not anymore, I guess.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I draw a graph.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I drew a graph where we have 7 variables and four factors.",
                    "label": 0
                },
                {
                    "sent": "And if we want to compute the solution, we will have to do the same for X2 all the way to X3.",
                    "label": 0
                },
                {
                    "sent": "I'm going to compute P of X1.",
                    "label": 0
                },
                {
                    "sent": "For all those factors that we have OK. And to simplify things, because I don't want to be carrying all the wise over here.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that I incorporated the information from the likelihoods in each one of the factors.",
                    "label": 0
                },
                {
                    "sent": "OK, like this is a function of X, one X2 and X3.",
                    "label": 0
                },
                {
                    "sent": "The priority of Y1 given X one.",
                    "label": 0
                },
                {
                    "sent": "I can put it in there.",
                    "label": 0
                },
                {
                    "sent": "OK, they say with two and three.",
                    "label": 0
                },
                {
                    "sent": "Therefore I can put it in D day, five under 6.",
                    "label": 0
                },
                {
                    "sent": "I can put it in B and then I can put it in C. OK ask why is something that we observe that we know.",
                    "label": 0
                },
                {
                    "sent": "Something that is fixed so we're not going to care about it, so I don't want to carry a lot of variables that we that we cloud everything that we're doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just have the four factors, and if we do this some we have to compute 128 elements.",
                    "label": 0
                },
                {
                    "sent": "We have to add up 64 of them, 4X1 equal to 0.",
                    "label": 0
                },
                {
                    "sent": "64 of them, 4X1 equal to 1 and then compare them in order to make a decision.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I just did in here is I rearrange the sums.",
                    "label": 0
                },
                {
                    "sent": "OK, I rearranged the sums in a way that is going to be convenient for me.",
                    "label": 0
                },
                {
                    "sent": "He's going to make all these doing this seems pretty easy.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start from the end from the right hand side and I'm going to be computing the sums 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first time that I'm going to do is 6 seven.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to assume that X5 is fixed.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I do that I only have to compute four terms.",
                    "label": 0
                },
                {
                    "sent": "FC4X5 equal to 0 seven C = 0 and 01 and for that I get a function that is only a function of X5.",
                    "label": 0
                },
                {
                    "sent": "OK, if I'm adding X7 I only get a function that depends on X5.",
                    "label": 0
                },
                {
                    "sent": "Everybody sees that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If I put this in a general way, so factors could be anything.",
                    "label": 0
                },
                {
                    "sent": "If the factors are parity check factors.",
                    "label": 0
                },
                {
                    "sent": "Actually, the one on the.",
                    "label": 0
                },
                {
                    "sent": "This one and this one will have to be 0.",
                    "label": 0
                },
                {
                    "sent": "OK, but you can do this for any factor.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be parity checks, so that's what I put in the whole thing for computing this time I need to compute four components and performance sounds.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And now I'm going to do some's at the same time, so I'm going to do this one of X4 that if you look is the same one that we did for examine before.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to do this one which is X5 and X6, and I'm going to get a function.",
                    "label": 0
                },
                {
                    "sent": "Affect OK, forget about this.",
                    "label": 0
                },
                {
                    "sent": "Be going to X2 they will.",
                    "label": 0
                },
                {
                    "sent": "They will mean something afterwards.",
                    "label": 0
                },
                {
                    "sent": "OK, the important thing here is that I get a function of X2 when I sum X5 and X6 and I get a function of X3 when I sum X4.",
                    "label": 0
                },
                {
                    "sent": "For this one I need to compute 8 components and performance exams.",
                    "label": 1
                },
                {
                    "sent": "For this, one is again the same thing for and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the last one, I'm going to add X 2X3.",
                    "label": 0
                },
                {
                    "sent": "On the thing that I'm going to do is again, I have to compute components under forcing something six times.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we know that P of X one is proportional to.",
                    "label": 0
                },
                {
                    "sent": "These are a of X1.",
                    "label": 0
                },
                {
                    "sent": "This function of X1 and we just normalize.",
                    "label": 0
                },
                {
                    "sent": "And when we normalize, we get there.",
                    "label": 0
                },
                {
                    "sent": "Effects one if the project one is greater than 0.5 then will say that the map solution is X1 equal to 1.",
                    "label": 0
                },
                {
                    "sent": "If it's less than 0.5, will say that the map solution.",
                    "label": 0
                },
                {
                    "sent": "He said so on equal to 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm sure all of you have been counting.",
                    "label": 0
                },
                {
                    "sent": "But you can see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We only do 26 computation and $0.17, so it's an order of magnitude less.",
                    "label": 0
                },
                {
                    "sent": "But if we have done everything altogether.",
                    "label": 0
                },
                {
                    "sent": "OK. That's the beauty of rearranging systems that once we do it that way.",
                    "label": 0
                },
                {
                    "sent": "We can do everything in a much fewer computation, and we've been using the graph structure in order to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, the good thing is, as we've been doing this thing.",
                    "label": 0
                },
                {
                    "sent": "We also recover the partition function.",
                    "label": 1
                },
                {
                    "sent": "This C value which is going to be the same one for all of them.",
                    "label": 0
                },
                {
                    "sent": "We can get it from the sum of this of these two values OK, and for the other marginals we have to do a little bit more of work.",
                    "label": 1
                },
                {
                    "sent": "Actually, if we wanted to get X2, we have to arrange and rearrange the sums in a different way.",
                    "label": 0
                },
                {
                    "sent": "If you want to get X7, we have to rearrange the samples in a different way.",
                    "label": 0
                },
                {
                    "sent": "OK, so that seems like a little.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if I did this very fast now you tell me OK, if I have to repeat these things for each one of the variables.",
                    "label": 0
                },
                {
                    "sent": "At the end I'm going to get the same complexity.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm not making anything there.",
                    "label": 0
                },
                {
                    "sent": "Good thing is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We don't need to sort the variables.",
                    "label": 1
                },
                {
                    "sent": "We can do it without sorting the variables.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what I'm.",
                    "label": 0
                },
                {
                    "sent": "The message passing algorithm and what the notation that I was using before is.",
                    "label": 1
                },
                {
                    "sent": "Is helpful, so if you remember there are of the name of the factor to a variable, what it was telling.",
                    "label": 0
                },
                {
                    "sent": "Is the information that each one of the factors.",
                    "label": 0
                },
                {
                    "sent": "Knows about that variable.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's a local computation.",
                    "label": 0
                },
                {
                    "sent": "So in this case, each one of the factors is going to send a message to each one of the bit about the information that they know about the bits.",
                    "label": 0
                },
                {
                    "sent": "OK. And then the variables is going to send to the factors.",
                    "label": 1
                },
                {
                    "sent": "The information about what they know about themselves.",
                    "label": 0
                },
                {
                    "sent": "And you're going to iterate.",
                    "label": 0
                },
                {
                    "sent": "Until that algorithm converges.",
                    "label": 0
                },
                {
                    "sent": "OK. And when it does.",
                    "label": 0
                },
                {
                    "sent": "You get the bitwise map solution for each one of the base value.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea is very simple is I'm a variable X one and I want to know what my value is.",
                    "label": 0
                },
                {
                    "sent": "OK, so you spent all the facts that are connected to me to send information that they know about me, that I didn't tell them before.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to tell each one of the facts of the information that I know about myself, that we didn't know about me.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "This algorithm works and it works in a finite number of iteration if.",
                    "label": 0
                },
                {
                    "sent": "And this is a big if.",
                    "label": 0
                },
                {
                    "sent": "Guardian has no loops.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I plot the graph in a way.",
                    "label": 0
                },
                {
                    "sent": "That you don't know if he had looks or not, but it's easy to see.",
                    "label": 0
                },
                {
                    "sent": "This is the graph that I was using.",
                    "label": 0
                },
                {
                    "sent": "There are no loops OK.",
                    "label": 0
                },
                {
                    "sent": "There are loops.",
                    "label": 0
                },
                {
                    "sent": "There are two things that you can do.",
                    "label": 0
                },
                {
                    "sent": "OK, there is something called a junction tree algorithms that tried to collapse variables.",
                    "label": 0
                },
                {
                    "sent": "Together until you get out.",
                    "label": 0
                },
                {
                    "sent": "A loopless graph.",
                    "label": 0
                },
                {
                    "sent": "And then you can apply this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then you can do the engineering solution.",
                    "label": 0
                },
                {
                    "sent": "Which is all the message that I'm using our local.",
                    "label": 0
                },
                {
                    "sent": "So from a local perspective I don't know.",
                    "label": 0
                },
                {
                    "sent": "There are looks, so I still use it.",
                    "label": 1
                },
                {
                    "sent": "Let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "OK. And if it's work, why should I worry?",
                    "label": 0
                },
                {
                    "sent": "OK. And that's actually what we do.",
                    "label": 0
                },
                {
                    "sent": "Then we worry when it's not working, but initially we don't worry about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me remind you about the the message.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can see it.",
                    "label": 0
                },
                {
                    "sent": "OK, and what I'm going to do is I'm going to add a factor now for each one of them, incorporating the information about why OK. OK, because this is the best way that we can incorporate information about why in each one of them.",
                    "label": 1
                },
                {
                    "sent": "So just assume that I have.",
                    "label": 0
                },
                {
                    "sent": "OK, and assume there is a Y at the end of each one of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so the factor the variable is going to send to each factor.",
                    "label": 1
                },
                {
                    "sent": "The information it knows about himself that the factor ignores.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so X2 is connected to a B and each.",
                    "label": 1
                },
                {
                    "sent": "OK, so has to send information about 2X22A2E2.",
                    "label": 0
                },
                {
                    "sent": "And to be and it has to send all the information that we know about itself.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the factors has to send information to all the bits or all the variables that is close by.",
                    "label": 1
                },
                {
                    "sent": "So a has to send information to X1.",
                    "label": 1
                },
                {
                    "sent": "XX3 and X2.",
                    "label": 0
                },
                {
                    "sent": "An important thing here is that you don't need to replicate information.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we called the message from the variables to.",
                    "label": 0
                },
                {
                    "sent": "The factors is all the information that X2 knows about itself.",
                    "label": 0
                },
                {
                    "sent": "That doesn't know the factor already.",
                    "label": 0
                },
                {
                    "sent": "OK so X2 sends the information that it goes from B.",
                    "label": 0
                },
                {
                    "sent": "Two way and he says the information that he got from E22A.",
                    "label": 0
                },
                {
                    "sent": "But we don't multiply by the information that a already has about us, so we don't multiply by air of a 2X2.",
                    "label": 0
                },
                {
                    "sent": "OK, and the factors do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They get the information from the different variables so they get the information for next 2 from X1 and X3.",
                    "label": 1
                },
                {
                    "sent": "And they also have the factor function.",
                    "label": 1
                },
                {
                    "sent": "They have to add for everything that you have in the factor.",
                    "label": 0
                },
                {
                    "sent": "OK, and then they send that information back to the to the node.",
                    "label": 0
                },
                {
                    "sent": "So in a way.",
                    "label": 0
                },
                {
                    "sent": "Is what I what I just said is X2 tells the factor everything that it knows about itself, that the factor doesn't know already.",
                    "label": 0
                },
                {
                    "sent": "And the factor says you have to fulfill several conditions.",
                    "label": 0
                },
                {
                    "sent": "That's a factor, and I have this information from these other variables that you don't know, so I'm going to send it to you.",
                    "label": 0
                },
                {
                    "sent": "OK, and here is what you can see the problem when you have looks when you don't have loops.",
                    "label": 0
                },
                {
                    "sent": "The factor knows is sending always new information.",
                    "label": 0
                },
                {
                    "sent": "That the variable doesn't know.",
                    "label": 1
                },
                {
                    "sent": "But when you have loops, all this information propagates through the loops and you can get a feedback of the same information that you already saw through the loop.",
                    "label": 0
                },
                {
                    "sent": "OK. What we always hope is that that information is distorted.",
                    "label": 0
                },
                {
                    "sent": "So when it gets to you.",
                    "label": 0
                },
                {
                    "sent": "Is not really going to change what you know about yourself.",
                    "label": 0
                },
                {
                    "sent": "OK, again, sometimes those things don't happen.",
                    "label": 0
                },
                {
                    "sent": "OK, but we were worried that in a second.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the algorithm and then it's easy to prove that algorithm.",
                    "label": 0
                },
                {
                    "sent": "The complexity of algorithm is linear because if the if you don't have.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations that you need to converge is the maximum path in your graph.",
                    "label": 0
                },
                {
                    "sent": "On each one of the operations, you expect that in each one of the factors you have a finite number of very low number of variables.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it will be exponential in those variables, But if those variables remain fixed.",
                    "label": 0
                },
                {
                    "sent": "He's not exponential.",
                    "label": 0
                },
                {
                    "sent": "As you increase the graph.",
                    "label": 0
                },
                {
                    "sent": "OK. And I think this is what I want to break.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is the next one and this is just I don't want to put this equation initially because every time people see.",
                    "label": 0
                },
                {
                    "sent": "The general equation that you say, hey, this doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "OK so the only thing that I'm saying in the first equation is you have to multiply all the information that you get from the from the other factors except the ones that you know about yourself.",
                    "label": 0
                },
                {
                    "sent": "So XM send information to Jay of all the other factors affect what it knows from J.",
                    "label": 0
                },
                {
                    "sent": "And M the cardinalities there set M has all the factors that are connected to XN.",
                    "label": 1
                },
                {
                    "sent": "OK. And they were able to factor in the same thing.",
                    "label": 1
                },
                {
                    "sent": "You have to add for all the variables at the one year in transmitting on.",
                    "label": 0
                },
                {
                    "sent": "And you have to multiply by all the message that you got is that the one that you got from that value?",
                    "label": 0
                },
                {
                    "sent": "OK, those are where these two equations these two equations.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is the algorithm that we're going to try to analyze.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the important thing about the message passing is first thing is we don't need to sort the sums.",
                    "label": 1
                },
                {
                    "sent": "OK, that sounds are already.",
                    "label": 0
                },
                {
                    "sent": "We can do this message passing without sorting the sums OK and we don't need to know the structure of the whole graph.",
                    "label": 1
                },
                {
                    "sent": "Each factor it only needs to know who is neighbor 2.",
                    "label": 0
                },
                {
                    "sent": "And is valuable only knows to know Mr know who is neighboring.",
                    "label": 0
                },
                {
                    "sent": "Even you need to care about the whole structure.",
                    "label": 0
                },
                {
                    "sent": "And this is what we can apply the algorithm either, even if you have looks in it because locally you don't see any loops.",
                    "label": 0
                },
                {
                    "sent": "OK, we yes no that if you had loops it might not work.",
                    "label": 1
                },
                {
                    "sent": "OK. And that's the last thing, OK?",
                    "label": 0
                },
                {
                    "sent": "As I said, for general graph is not applicable, you can use the junction tree algorithm and I'm sure something will talk about it.",
                    "label": 0
                },
                {
                    "sent": "But in many cases it works well in practice and for channel coding is one of those cases.",
                    "label": 0
                },
                {
                    "sent": "Because we're going to build the graph so we have few loops.",
                    "label": 0
                },
                {
                    "sent": "Sexually, that's the whole point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And that is the thing that I will see in an image.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to stop here for 10 minutes, and then we continue.",
                    "label": 0
                }
            ]
        }
    }
}