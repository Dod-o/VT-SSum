{
    "id": "l2fungk5urm5axsh2k5mczvjqaklkmba",
    "title": "Depth Extraction from Video Using Non-parametric Sampling",
    "info": {
        "author": [
            "Kevin Karsch, University of Illinois at Urbana-Champaign"
        ],
        "chairman": [
            "Aude Oliva, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT",
            "Silvio Savarese, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2012_karsch_sampling/",
    "segmentation": [
        [
            "This is joint work with Microsoft Research and I'm from the University of."
        ],
        [
            "So the problem we're trying to solve is, given an image or a video, we want to estimate the depth per pixel, so the distance from the camera and we want to do this without imposing any restrictions on the type of input.",
            "What I mean by that is we don't want to impose that the camera has any parallax."
        ],
        [
            "So we can handle things like static stationary cameras and static images."
        ],
        [
            "And we also want to allow for camera motion in the form of translation, rotation, or even a change in focal length.",
            "And we also want to."
        ],
        [
            "Be able to handle things like moving or motion in the scene, an overall dynamic scenes.",
            "In fact, any combination of dynamic scene, static scene or dynamic camera static camera."
        ],
        [
            "And so in the past, we've seen kind of two separate classes of depth estimation.",
            "One takes more of a geometric approach like structure from motion, but it requires parallax and also a static scene, and the other class is kind of more of a learning approach, which works for single images, but it's not quite clear how to extend these to video."
        ],
        [
            "So again, what we're proposing is a technique that doesn't require any parallax or static scene, and there's a very easy extension to video.",
            "And our method tends towards past parametric approaches, but it uses a data driven non parametric model instead.",
            "It's also fundamentally different than these past learning approaches."
        ],
        [
            "The scale at which we're doing the training, ad and so for example, in some of the first work on single image depth estimation, sex scene at all trained A1 depth model over their entire training set.",
            "And then we saw a few years back that you could improve your results by actually training a different model of depth for each semantic class in your data set.",
            "So something like a different model for Tree, Sky and so on.",
            "And now what we're proposing is that if you actually take one step back and train a different model only on scenes that are similar, so it seems with similar semantics, you can actually improve your results even further."
        ],
        [
            "Insert techniques.",
            "Data driven.",
            "We require an RGB data set and we actually use 21 for outdoors and one for indoors.",
            "The outdoor one comes from a 3D data set and the indoors we actually collected ourselves, which we've titled the MSR V 3D data set which contains indoor videos containing depth and this was captured with the Connect."
        ],
        [
            "And our algorithm hinges on densin alignment, and for this we use sift flow and so let me quickly go over the SIFT flow algorithm.",
            "It's essentially optical flow using dense sift features with the difference from optical flow that it uses the largest search window and modified smoothness constraints.",
            "So the idea is that if we have two images here, image A&B.",
            "We want to rearrange the semantics of image A to match those in image B.",
            "So the first thing that happens is we compute dense sift on each image and then match sift features according to the Sift Flow algorithm."
        ],
        [
            "This results in a warping operator, which we can then apply to."
        ],
        [
            "Image A just semantically align it with image B and this is the result for these two images."
        ],
        [
            "And notice that the appearance doesn't match.",
            "Even so, in slightly and these two, but the semantics are actually matched pretty well.",
            "So if I flip back and forth, you can see that the ground has been matched to the ground Sky to Sky tree to tree, and so on."
        ],
        [
            "So I'm getting into algorithm.",
            "I'll start with the single Image case 1st and then show you the extension of video in a few slides.",
            "So we start with an input image and also an RGB database.",
            "And the first thing we do is compute just features over the for each RGB image in our data set and also on the input image.",
            "And we do a simple nearest."
        ],
        [
            "Your query to find images that match well in terms of just features."
        ],
        [
            "This results in a set of candidates that we can then warp using Sift flow to hopefully semantically align these images with our input and so sift flow results in an operator that we can then use to warp the depth as well.",
            "So we're not just warping the RGB values."
        ],
        [
            "And that's actually it for the training phase of our algorithm.",
            "Or you can think of that as a training phase of our algorithm, because so once we have this.",
            "What's left is to use a few pieces of data to infer the depth for a given image, so the inference uses the input the word candidates and then also a prior term, which we compute as the average per pixel of our depth data set.",
            "And so this is 1 result."
        ],
        [
            "Tickle more a little bit, so the inference is actually phrased as a continuous optimization an the inferred depth can be thought of roughly as a linear combination of our candidate depth with the warp candidate depths, and so there's three terms up here in our objective function, the first term is meant to enforce the infer depth to match the candidate depths, and it's done at the absolute and relative scale.",
            "That's what the two terms correspond to in this part of the equation, and this is also weighted by a measure of confidence which comes from the SIFT reprojection error.",
            "Obtained during shift flow.",
            "And then we also regularize the solution with the smoothest term and a prior term, and so one thing you might notice is we're using an L1 distance metric, and this is for a few reasons which are detailed in the paper.",
            "But the main reason is that we want to enforce sparsity in the number of match candidates that we're actually using in the first term.",
            "And so one thing."
        ],
        [
            "Point out is that we're not solving a labeling problem here.",
            "This is actually due to the fact that we want this linear combination of candidates, and we also want to use this relative depth term that was in the the first initial term that enforces depth to match the candidates.",
            "And so to do that, we can actually treat this as a labeling problem."
        ],
        [
            "So now here a few of our single image results.",
            "On the left is the input image, the middle is the ground truth depth from the make 3D data set and on the right is our inferred depth."
        ],
        [
            "And here's the same result, but a different visualization with the fly through an one thing to notice is that even though there are repetitive structures and repetitive patches of appearance, like in the tree bark, we can still localize the depth with relative accuracy."
        ],
        [
            "And here's one more result.",
            "Again, the middle depth comes from the make 3D data set and then on the right is our result."
        ],
        [
            "And one thing you might notice is that we're losing thin structures like the lamppost, but this is due to smoothing, but overall the field of depth is about right."
        ],
        [
            "So let me go back to the objective function just for a second and on the last result.",
            "This is an example of what our inference does with and without transferring the relative depth, only transferring the absolute depth results in a scale ambiguity in a few places.",
            "That's in this example, it's noticeable near the top of the tree and around the middle of the tree."
        ],
        [
            "We've also done some quantitative evaluation."
        ],
        [
            "On the mic through the data set and we're scoring about as well and even a little bit better in some of the metrics then the state of the art approaches for single image depth estimation.",
            "And one thing I'll point out is that."
        ],
        [
            "Several these methods actually use RGB plus labeled data for training, and we only require so the labels are semantic labels and we only require RGB data, so this can be just captured with hardware rather than having a user segments."
        ],
        [
            "So now moving to the video extension, so the input here is a sequence of frames and one thing we could do is use our single image solution to compute depth at each of these frames and then concatenate this into one video.",
            "An Kolesar solution.",
            "But this isn't the best idea."
        ],
        [
            "Because we're not incorporating any of the rich temporal information that's available in videos, and so our idea is if we incorporate optical flow and motion segmentation, we should be able to get a much better result."
        ],
        [
            "And so the only thing that really changes about our algorithm is in the objective function.",
            "So the initial phase of finding candidates and warping them stays exactly the same on a per frame basis, and the only thing we have to do is add in two additional terms into our objective.",
            "So the first term stays exactly the same as the one I just showed a few slides ago, and now the next to come from a few observations.",
            "So one observation is that depth changes are typically gradual from frame to frame, and what we do is we compute first the optical flow.",
            "From frame."
        ],
        [
            "Frame and ensure that the depth along this optical flow direction is the change in depth is small."
        ],
        [
            "The other term comes from the fact that gravity forces moving objects to usually be on the ground.",
            "So if we can get an estimate of motion then we can enforce everything in motion to have a similar depth value as where it Contacts the ground and so.",
            "This term is just encoded in.",
            "By forcing the moving objects to be actually at this point in the ground."
        ],
        [
            "And here's a concrete example of with and without the temporal information.",
            "So the middle result is the one I described where we just compute are single image solution for each frame in the video and then on the right is once we incorporate these two additional terms and the things to note are the depth is much smoother temporally in the right example and also the moving object in person has been localized much better."
        ],
        [
            "I'm moving on to a few of our."
        ],
        [
            "Video results.",
            "So again, we're able to do this from just a stationary camera and for dynamic scene objects and it works outdoors."
        ],
        [
            "As well as indoors, as in this example, and this example was trained using our indoor data set."
        ],
        [
            "And it also works in cases with dynamic scene and a dynamic camera."
        ],
        [
            "And using our indoor data set, we've also done some thorough evaluation which is available on our project web page and on the paper or in the paper, and so the results that I'm showing here are on the each triplet of images, the input, the groundtruth Connect depth, and then our estimate of depth.",
            "So I'll refer you to the project web page and the paper for more details on this.",
            "So our failures largely fall into two categories.",
            "The first is due to our data set not being representative enough of the input, and this is a limitation of pretty much any data driven approach and the 2nd."
        ],
        [
            "Case usually happens for or when we have incorrect motion estimation or if one of our motion assumptions doesn't hold.",
            "For example, the basketball here is floating, and we're assuming that it's actually contacting the ground."
        ],
        [
            "One application we've investigated is converting moscovic 2D videos into stereoscopic 3D.",
            "And we can do this by taking our video sequence.",
            "We can do automatically by taking our video sequence and converting it into depth using our algorithm and then automatically using depth image based rendering to render stereoviews.",
            "And then creating any 3D viewing format, for example anaglyph and these are a couple frames that we that we computed using our algorithm on the film charade.",
            "And we actually do."
        ],
        [
            "In a comparison to, YouTube has a 2D to 3D automatic conversion system as well, and we have a comparison on our web page and I guess it's hard to see the difference without 3D glasses.",
            "But one thing to notice is the difference in the amount of disparity between our result and the YouTube result.",
            "And if you're interested, you can take a closer look on our project website and also our coding data set are now available as well.",
            "Thanks for your time.",
            "Anybody have any questions?",
            "Were there humans in the training data?",
            "Yes so.",
            "At least in the.",
            "See.",
            "So for the outdoor, for the most part, no.",
            "I believe these were just static scenes though for the outdoor."
        ],
        [
            "Images and then for the indoor we had.",
            "It was videos of primarily static camera with a dynamic scene.",
            "So in the dynamic parts were usually people and or cars.",
            "And so these are examples of from the indoor data set.",
            "Can you give us any insight on the computational complexity?",
            "So how fast is sure sure so?",
            "Zyflo I guess on a 5 by 5 fold image on a reasonable computer takes maybe 30 seconds per image, easily parallelizable, but I guess for the generator result like the ones I'm showing here.",
            "Which is 3260 frames can take about a minute to frame so it is still pretty slow, but there is room for optimization.",
            "How to estimate the depth of moving objects?",
            "Sorry, moving objects.",
            "How do you submit the depth?",
            "Oh, how do you segment the?",
            "How do you actually obtain the motion estimation?",
            "Know how to use to make the depth so oh, so if an object is in motion then we get a binary motion mask for that object and then we assume that that moving object Contacts the floor and so the yes the then we have a term that enforces the depth of that mask to take the.",
            "The depth at where they contact the ground.",
            "OK, let's fix the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Microsoft Research and I'm from the University of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem we're trying to solve is, given an image or a video, we want to estimate the depth per pixel, so the distance from the camera and we want to do this without imposing any restrictions on the type of input.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that is we don't want to impose that the camera has any parallax.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can handle things like static stationary cameras and static images.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also want to allow for camera motion in the form of translation, rotation, or even a change in focal length.",
                    "label": 0
                },
                {
                    "sent": "And we also want to.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be able to handle things like moving or motion in the scene, an overall dynamic scenes.",
                    "label": 0
                },
                {
                    "sent": "In fact, any combination of dynamic scene, static scene or dynamic camera static camera.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in the past, we've seen kind of two separate classes of depth estimation.",
                    "label": 0
                },
                {
                    "sent": "One takes more of a geometric approach like structure from motion, but it requires parallax and also a static scene, and the other class is kind of more of a learning approach, which works for single images, but it's not quite clear how to extend these to video.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, what we're proposing is a technique that doesn't require any parallax or static scene, and there's a very easy extension to video.",
                    "label": 0
                },
                {
                    "sent": "And our method tends towards past parametric approaches, but it uses a data driven non parametric model instead.",
                    "label": 0
                },
                {
                    "sent": "It's also fundamentally different than these past learning approaches.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The scale at which we're doing the training, ad and so for example, in some of the first work on single image depth estimation, sex scene at all trained A1 depth model over their entire training set.",
                    "label": 0
                },
                {
                    "sent": "And then we saw a few years back that you could improve your results by actually training a different model of depth for each semantic class in your data set.",
                    "label": 0
                },
                {
                    "sent": "So something like a different model for Tree, Sky and so on.",
                    "label": 1
                },
                {
                    "sent": "And now what we're proposing is that if you actually take one step back and train a different model only on scenes that are similar, so it seems with similar semantics, you can actually improve your results even further.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Insert techniques.",
                    "label": 0
                },
                {
                    "sent": "Data driven.",
                    "label": 0
                },
                {
                    "sent": "We require an RGB data set and we actually use 21 for outdoors and one for indoors.",
                    "label": 0
                },
                {
                    "sent": "The outdoor one comes from a 3D data set and the indoors we actually collected ourselves, which we've titled the MSR V 3D data set which contains indoor videos containing depth and this was captured with the Connect.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our algorithm hinges on densin alignment, and for this we use sift flow and so let me quickly go over the SIFT flow algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's essentially optical flow using dense sift features with the difference from optical flow that it uses the largest search window and modified smoothness constraints.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that if we have two images here, image A&B.",
                    "label": 0
                },
                {
                    "sent": "We want to rearrange the semantics of image A to match those in image B.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that happens is we compute dense sift on each image and then match sift features according to the Sift Flow algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This results in a warping operator, which we can then apply to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image A just semantically align it with image B and this is the result for these two images.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And notice that the appearance doesn't match.",
                    "label": 0
                },
                {
                    "sent": "Even so, in slightly and these two, but the semantics are actually matched pretty well.",
                    "label": 0
                },
                {
                    "sent": "So if I flip back and forth, you can see that the ground has been matched to the ground Sky to Sky tree to tree, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm getting into algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'll start with the single Image case 1st and then show you the extension of video in a few slides.",
                    "label": 0
                },
                {
                    "sent": "So we start with an input image and also an RGB database.",
                    "label": 1
                },
                {
                    "sent": "And the first thing we do is compute just features over the for each RGB image in our data set and also on the input image.",
                    "label": 0
                },
                {
                    "sent": "And we do a simple nearest.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your query to find images that match well in terms of just features.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This results in a set of candidates that we can then warp using Sift flow to hopefully semantically align these images with our input and so sift flow results in an operator that we can then use to warp the depth as well.",
                    "label": 0
                },
                {
                    "sent": "So we're not just warping the RGB values.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's actually it for the training phase of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or you can think of that as a training phase of our algorithm, because so once we have this.",
                    "label": 0
                },
                {
                    "sent": "What's left is to use a few pieces of data to infer the depth for a given image, so the inference uses the input the word candidates and then also a prior term, which we compute as the average per pixel of our depth data set.",
                    "label": 0
                },
                {
                    "sent": "And so this is 1 result.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tickle more a little bit, so the inference is actually phrased as a continuous optimization an the inferred depth can be thought of roughly as a linear combination of our candidate depth with the warp candidate depths, and so there's three terms up here in our objective function, the first term is meant to enforce the infer depth to match the candidate depths, and it's done at the absolute and relative scale.",
                    "label": 1
                },
                {
                    "sent": "That's what the two terms correspond to in this part of the equation, and this is also weighted by a measure of confidence which comes from the SIFT reprojection error.",
                    "label": 0
                },
                {
                    "sent": "Obtained during shift flow.",
                    "label": 0
                },
                {
                    "sent": "And then we also regularize the solution with the smoothest term and a prior term, and so one thing you might notice is we're using an L1 distance metric, and this is for a few reasons which are detailed in the paper.",
                    "label": 0
                },
                {
                    "sent": "But the main reason is that we want to enforce sparsity in the number of match candidates that we're actually using in the first term.",
                    "label": 0
                },
                {
                    "sent": "And so one thing.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point out is that we're not solving a labeling problem here.",
                    "label": 0
                },
                {
                    "sent": "This is actually due to the fact that we want this linear combination of candidates, and we also want to use this relative depth term that was in the the first initial term that enforces depth to match the candidates.",
                    "label": 1
                },
                {
                    "sent": "And so to do that, we can actually treat this as a labeling problem.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now here a few of our single image results.",
                    "label": 0
                },
                {
                    "sent": "On the left is the input image, the middle is the ground truth depth from the make 3D data set and on the right is our inferred depth.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the same result, but a different visualization with the fly through an one thing to notice is that even though there are repetitive structures and repetitive patches of appearance, like in the tree bark, we can still localize the depth with relative accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's one more result.",
                    "label": 0
                },
                {
                    "sent": "Again, the middle depth comes from the make 3D data set and then on the right is our result.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one thing you might notice is that we're losing thin structures like the lamppost, but this is due to smoothing, but overall the field of depth is about right.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me go back to the objective function just for a second and on the last result.",
                    "label": 0
                },
                {
                    "sent": "This is an example of what our inference does with and without transferring the relative depth, only transferring the absolute depth results in a scale ambiguity in a few places.",
                    "label": 0
                },
                {
                    "sent": "That's in this example, it's noticeable near the top of the tree and around the middle of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've also done some quantitative evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the mic through the data set and we're scoring about as well and even a little bit better in some of the metrics then the state of the art approaches for single image depth estimation.",
                    "label": 0
                },
                {
                    "sent": "And one thing I'll point out is that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Several these methods actually use RGB plus labeled data for training, and we only require so the labels are semantic labels and we only require RGB data, so this can be just captured with hardware rather than having a user segments.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now moving to the video extension, so the input here is a sequence of frames and one thing we could do is use our single image solution to compute depth at each of these frames and then concatenate this into one video.",
                    "label": 0
                },
                {
                    "sent": "An Kolesar solution.",
                    "label": 0
                },
                {
                    "sent": "But this isn't the best idea.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we're not incorporating any of the rich temporal information that's available in videos, and so our idea is if we incorporate optical flow and motion segmentation, we should be able to get a much better result.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the only thing that really changes about our algorithm is in the objective function.",
                    "label": 0
                },
                {
                    "sent": "So the initial phase of finding candidates and warping them stays exactly the same on a per frame basis, and the only thing we have to do is add in two additional terms into our objective.",
                    "label": 0
                },
                {
                    "sent": "So the first term stays exactly the same as the one I just showed a few slides ago, and now the next to come from a few observations.",
                    "label": 0
                },
                {
                    "sent": "So one observation is that depth changes are typically gradual from frame to frame, and what we do is we compute first the optical flow.",
                    "label": 1
                },
                {
                    "sent": "From frame.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frame and ensure that the depth along this optical flow direction is the change in depth is small.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other term comes from the fact that gravity forces moving objects to usually be on the ground.",
                    "label": 1
                },
                {
                    "sent": "So if we can get an estimate of motion then we can enforce everything in motion to have a similar depth value as where it Contacts the ground and so.",
                    "label": 0
                },
                {
                    "sent": "This term is just encoded in.",
                    "label": 0
                },
                {
                    "sent": "By forcing the moving objects to be actually at this point in the ground.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's a concrete example of with and without the temporal information.",
                    "label": 0
                },
                {
                    "sent": "So the middle result is the one I described where we just compute are single image solution for each frame in the video and then on the right is once we incorporate these two additional terms and the things to note are the depth is much smoother temporally in the right example and also the moving object in person has been localized much better.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm moving on to a few of our.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Video results.",
                    "label": 0
                },
                {
                    "sent": "So again, we're able to do this from just a stationary camera and for dynamic scene objects and it works outdoors.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well as indoors, as in this example, and this example was trained using our indoor data set.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it also works in cases with dynamic scene and a dynamic camera.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And using our indoor data set, we've also done some thorough evaluation which is available on our project web page and on the paper or in the paper, and so the results that I'm showing here are on the each triplet of images, the input, the groundtruth Connect depth, and then our estimate of depth.",
                    "label": 0
                },
                {
                    "sent": "So I'll refer you to the project web page and the paper for more details on this.",
                    "label": 0
                },
                {
                    "sent": "So our failures largely fall into two categories.",
                    "label": 0
                },
                {
                    "sent": "The first is due to our data set not being representative enough of the input, and this is a limitation of pretty much any data driven approach and the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case usually happens for or when we have incorrect motion estimation or if one of our motion assumptions doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "For example, the basketball here is floating, and we're assuming that it's actually contacting the ground.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One application we've investigated is converting moscovic 2D videos into stereoscopic 3D.",
                    "label": 0
                },
                {
                    "sent": "And we can do this by taking our video sequence.",
                    "label": 0
                },
                {
                    "sent": "We can do automatically by taking our video sequence and converting it into depth using our algorithm and then automatically using depth image based rendering to render stereoviews.",
                    "label": 0
                },
                {
                    "sent": "And then creating any 3D viewing format, for example anaglyph and these are a couple frames that we that we computed using our algorithm on the film charade.",
                    "label": 0
                },
                {
                    "sent": "And we actually do.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a comparison to, YouTube has a 2D to 3D automatic conversion system as well, and we have a comparison on our web page and I guess it's hard to see the difference without 3D glasses.",
                    "label": 0
                },
                {
                    "sent": "But one thing to notice is the difference in the amount of disparity between our result and the YouTube result.",
                    "label": 0
                },
                {
                    "sent": "And if you're interested, you can take a closer look on our project website and also our coding data set are now available as well.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your time.",
                    "label": 0
                },
                {
                    "sent": "Anybody have any questions?",
                    "label": 0
                },
                {
                    "sent": "Were there humans in the training data?",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "At least in the.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "So for the outdoor, for the most part, no.",
                    "label": 0
                },
                {
                    "sent": "I believe these were just static scenes though for the outdoor.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images and then for the indoor we had.",
                    "label": 0
                },
                {
                    "sent": "It was videos of primarily static camera with a dynamic scene.",
                    "label": 0
                },
                {
                    "sent": "So in the dynamic parts were usually people and or cars.",
                    "label": 0
                },
                {
                    "sent": "And so these are examples of from the indoor data set.",
                    "label": 0
                },
                {
                    "sent": "Can you give us any insight on the computational complexity?",
                    "label": 0
                },
                {
                    "sent": "So how fast is sure sure so?",
                    "label": 0
                },
                {
                    "sent": "Zyflo I guess on a 5 by 5 fold image on a reasonable computer takes maybe 30 seconds per image, easily parallelizable, but I guess for the generator result like the ones I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "Which is 3260 frames can take about a minute to frame so it is still pretty slow, but there is room for optimization.",
                    "label": 0
                },
                {
                    "sent": "How to estimate the depth of moving objects?",
                    "label": 0
                },
                {
                    "sent": "Sorry, moving objects.",
                    "label": 0
                },
                {
                    "sent": "How do you submit the depth?",
                    "label": 0
                },
                {
                    "sent": "Oh, how do you segment the?",
                    "label": 0
                },
                {
                    "sent": "How do you actually obtain the motion estimation?",
                    "label": 0
                },
                {
                    "sent": "Know how to use to make the depth so oh, so if an object is in motion then we get a binary motion mask for that object and then we assume that that moving object Contacts the floor and so the yes the then we have a term that enforces the depth of that mask to take the.",
                    "label": 0
                },
                {
                    "sent": "The depth at where they contact the ground.",
                    "label": 0
                },
                {
                    "sent": "OK, let's fix the speaker.",
                    "label": 0
                }
            ]
        }
    }
}