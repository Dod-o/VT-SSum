{
    "id": "xnylifqy5c6objyrrheohl7rs53gupqt",
    "title": "Multi-Task Learning with Gaussian Processes with Applications to Robot Inverse Dynamics",
    "info": {
        "author": [
            "Chris Williams, University of Edinburgh"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes",
            "Top->Computer Science->Machine Learning->Multi-Task Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_williams_mlgp/",
    "segmentation": [
        [
            "I hope you all had a nice lunch break or skybreaker whatever you decided to do.",
            "So I'm Yorkville today and I'll be the chair for this afternoon session.",
            "So we have.",
            "5 interesting talks.",
            "So will have first be invited talking to regular talks, then a coffee break.",
            "After that, the last two talks and then some discussion.",
            "So first I'd like to welcome our second invited speaker, Professor Chris Williams, from the University of Edinburgh.",
            "So Chris is.",
            "Well, Chris is done.",
            "A lot of things, but one of the things he has especially been active in Goshen process is yes, within a large number of papers on that, and I think he has even written a book about that with call Rasmussen.",
            "And in addition he has also studied how these.",
            "Very interesting probabilistic models can be used in multitask learning and I believe this is what we will.",
            "Talk about now so.",
            "OK, yes, thank you.",
            "Thank you for the invitation and hopefully the doors are.",
            "Still working right?",
            "So what I wanted to talk about was multitask, learning, GPS and I should say this is work.",
            "Well, no report on a couple of pieces of work one by my student Kambing Chai.",
            "And also by colleague Stefan Plank in situ.",
            "Vijay Kumar at Edinburgh."
        ],
        [
            "So really, just to motivate multitask learning, we can get and they can be multi task problems that occur in all sorts of places.",
            "So in geostatistics, we have things like the Co occurrence of different oils and so on.",
            "Of course, in machine learning problems we might have multiple tasks, such as like multiple object categories.",
            "We want to recognize, and maybe correlations between these different classes.",
            "You might think about personalization, where there's basically a whole bunch of different related problems.",
            "We might be interested in.",
            "This is also work that my group is done on.",
            "If we have essentially optimization over space of transformations with the compilers, then there may be multiple problem programs that we're trying to optimize over and we want to learn about what commonality's there are between the different problems.",
            "So fundamentally and also this robot inverse dynamics, which will actually talk about later.",
            "So basically the key idea, which is very familiar idea in statistical modeling.",
            "Is the idea to gain strength across different tasks in some sense the question is, well, really.",
            "How should we do this?",
            "OK, so that's the kind of goal here.",
            "The other thing to say is that I'm really going to be talking about multi task learning and of course it is possible to take a rather broader view of things and talk about.",
            "For example, you know ideas about rather more high level ideas about meta learning and so on.",
            "Learning about learning.",
            "So we might have for example lots of different learning algorithms we can apply to different problems we might be interested in trying to understand when different learning algorithms.",
            "Good for different situations.",
            "I think those are interesting things to do, but they are not the reader focus of my talk this afternoon.",
            "So."
        ],
        [
            "So as an outline, will talk about first about Gaussian process prediction.",
            "Hope that's familiar to many, but maybe it's useful to have a revision and then talk about Coke reading, which is basically the sort of geostatistics term in this idea of multi task learning with Gaussian processes.",
            "I'll talk about one particular model that we can use for Coke region called the intrinsic correlation model.",
            "This is language coming out of geostatistics, and we talk about then about how different kinds of notions of task relation this can actually be.",
            "Put into the Gaussian process framework through kind of higher hierarchical modeling ideas, but also transformations of the input ideas and also some other ideas about shared feature extraction.",
            "And also say that I'm kind of interested in the case as well.",
            "If there are things that don't fit into the GP framework as well, and I don't wish to shoehorn everything into that framework and.",
            "Used to think about that.",
            "Those are some nice theory which my student camming has done on about theory for this intrinsic correlation model we talk about briefly and I also want to talk about the example of multi task learning in robot inverse dynamics which I think was meant to be a good fit between this workshop was it actually has robotics and multi task.",
            "OK."
        ],
        [
            "That's where I want to go.",
            "So what's a GP basically going to infinite infinite dimensional Gaussian?",
            "And certainly this wouldn't keep mathematicians happy, but we think of a function is essentially an infinitely long vector, and then you want to define a Gaussian distribution with being collection of random variables and any finite subset will have a consistent Gaussian distribution.",
            "This will define the Gaussian process.",
            "And OK, and just in the same way that a Gaussian is, is fully specified by a mean and covariance matrix.",
            "Then I guess in process is fully specified by amine function you have X or M of X and the covariance function K of XX point.",
            "OK, specifying the covariance between the two locations X&X prime.",
            "So that's that."
        ],
        [
            "P. And basically the key thing that saves us.",
            "Dealing with infinitely long vectors that all functions the whole time is the marginalization property of a Gaussian.",
            "So basically they have a huge great Gaussian and I'm only interested in some of those variables.",
            "All I have to do is to take that block of the matrix which I'm actually interested in.",
            "I can marginalized everything else.",
            "OK, it's important to say.",
            "We use the covariance matrix.",
            "We can do this right if we actually were working in terms of inverse covariance matrices, then that actually wouldn't work.",
            "OK.",
            "So basically we can marginalized on the covariance and that's why working with the covariance function is a good idea.",
            "And there's also, by the way, why it can be difficult when dealing with say models that are defined in terms of parameterising the inverse covariance, such as Gaussian MRF's.",
            "And this is more complex than to make that marginalization.",
            "Anyway, so in the end it means that we basically consider, say we're in a standard machine living set up.",
            "We've got N training examples we can be interested in the function values at those corresponding to those axes.",
            "And of course if we have a test point, was just the N + 1 point.",
            "So we can take this huge vector and really just consider the N + 1 dimensional distribution.",
            "That's the good news.",
            "Just to give some intuitions about what sampling in Gaussian Pro."
        ],
        [
            "Look like, well, we we can define some function F of X with some mean and some covariance.",
            "So this mean is 0 here and this is a coherence.",
            "Of course we have a lot of choices over covariance functions and it's very important to emphasize that this thing that looks perhaps something like a Gaussian isn't the Gaussian here.",
            "OK this, I'll tend to call this A squared exponential kernel, basically 'cause it's an exponential function, it's a square in here and.",
            "We could have lots of other forms of covariance function here.",
            "For example, if I just replace this X -- X prime squared, we just absolute value of X.",
            "The value of that argument I'll end up with the Ornstein Uhlenbeck covariance function, which actually has very different properties to the.",
            "To this squared exponential kernel, it's about as different as you could actually have, while still being a Gaussian process, for example the.",
            "Stoneback function is no more mean square differentiable, whereas this is infinitely mean square differentiable.",
            "So basically what I understand what GPS look like, we can just we can choose some function locations and then just sample from this thing and that's kind of nice and."
        ],
        [
            "To do.",
            "Of course, there really.",
            "It's really looks like this and I just happened to join these dots to plot, but really what we're actually generating is those finite dimension functions."
        ],
        [
            "Another thing, OK, we can do that in 1D and 2D and."
        ],
        [
            "Any D that we want.",
            "The other thing, of course, that's really nice about gassing processes, is that we can actually do inference with them, so we can define some prize over functions.",
            "That's nice.",
            "And for Gaussian distributions, if we assume that we have for example.",
            "Boys with variance Sigma squared N on the observations.",
            "Then we can actually analytically get this posterior posterior function.",
            "So this is a sort of why stars I think of why star evaluated X star.",
            "So this is the posterior.",
            "And we get this thing that looks like that basically.",
            "So here's the prior, you know.",
            "Observe these data points here.",
            "And that basically narrows down the function that those locations is, specially if the noise level is small and I'm proving here predicted variance.",
            "OK, basically where we have data, the variance is small and it kind of increases like this.",
            "And so these are actually these red, green, and blue things actually samples drawn from the posterior.",
            "And that's actually quite easy to do 'cause we can actually get the full joint posterior.",
            "OK, well I'm showing here is just the marginal posterior to certain location.",
            "Next are given the data, but we can actually do is workout the full covariance posterior covariance or any set of data points.",
            "Given the data, so it's kind of the nice things you can do with Gaussian process.",
            "Hopefully that's maybe that's boringly Famille."
        ],
        [
            "To many of you.",
            "Just to say one other thing, the marginal likelihood is basically by working at the probability of the log problem.",
            "The data under this model, and of course this is a Gaussian.",
            "This is just the log of a multivariate Gaussian.",
            "And so we can workout this in closed form.",
            "And what's more important is that if there are parameters that define this covariance function K, and basically one can adjust the.",
            "Parameters in case so as to maximize the marginal likelihood, or if we're being Bayesian kersley examples sample over this posterior.",
            "So the nice thing is that this this thing is in closed form.",
            "That's the good news.",
            "It's also worth saying, of course, that.",
            "If we think about this as a function of the parameters that define the covariance function, then really there can be different things that go on.",
            "We can essentially give different interpretations of the data and some of the most.",
            "Obvious examples of that.",
            "For example, if we have.",
            "Go back if we have date."
        ],
        [
            "Like this, we could believe that there actually there's some function with a reasonably short length scale and this would be interpolating these data points.",
            "We could believe that we actually had a very long length scale, so essentially with some high noise level.",
            "So basically these two things can be encoded in the model, and it's quite easy to actually see as well what the different relative probabilities of those configurations are.",
            "Fundamentally, what what that will be driven by is essentially looking at you.",
            "If this really was.",
            "So this long length scale process with noise.",
            "These were essentially this sequence would be white.",
            "OK, there being a noise.",
            "So there are things that actually we can tell about what's going on in the data.",
            "Anyway.",
            "That's just a recap about."
        ],
        [
            "Gaussian process is.",
            "Let's talk about multitask learning.",
            "So really now we have.",
            "Say.",
            "The EM tasks and if we consider the elf task will have FL, which is the function value of XI on task L. Really all we have to do to extend the previous framework before we had to say for any X&X prime, what's the covariance function for those two?",
            "Those two locations?",
            "So this effort effort.",
            "XFX prime that the covariance was K of XX prime.",
            "Now all we have to do is to say.",
            "We have to introduce extra arguments here that specify the two different tasks we're interested in.",
            "And what we really need to do is just a great one.",
            "Big Gaussian OK, it's just what we're going to do is create some joint Gaussian that specifies the correlations within tasks and also between tasks.",
            "I'm really.",
            "The whole question hold pretty much the rest of the talk is about, well.",
            "What are reasonable structures for doing that?",
            "And then how can we compute with it and at the moment I'm not gonna worry too much about the computational issues.",
            "Let's assume we given this big Gaussian, we can actually compute with it.",
            "Of course, sometimes we have to make approximations as a lot of different approximations.",
            "We could talk about, but I'll just keep it conceptual for now.",
            "And."
        ],
        [
            "Pictorially.",
            "This is going on if we think of this X dimension.",
            "Here we consider one function.",
            "We can trace it out like this.",
            "Fundamentally, what we've got is we've got a set of correlated functions.",
            "So if this we think of these are the different functions we could draw.",
            "Somehow these are correlated.",
            "As we can imagine.",
            "Alternatively, drawing independent samples like we had.",
            "Way back here, right these?"
        ],
        [
            "So essentially, independent samples drawn from the same covariance, what we're now doing is we're saying we got multiple tasks."
        ],
        [
            "And those are actually somehow correlated, so these ones, things that are nearby along this axis are quite correlated.",
            "That's the basic idea behind multi task Gaussian processes.",
            "So."
        ],
        [
            "As I was saying really, the question is well.",
            "What are appropriate cross covariance structures that fit into the ideas of multitask learning and also other things that don't fit so well with that?"
        ],
        [
            "So this is perhaps a simple building block of.",
            "Multitell Scouting Process is the so called intrinsic correlation model.",
            "So what we want to do is to if we have FLXN FM X prime, then we're going to set and have a circle separable covariance function that separates the Inter task similarity.",
            "In this matrix CLM.",
            "And this covariance function here, which specifies the similarity next space it was separated out into task similarity and the sort of Inter X similarity.",
            "I will assume we have observation lines on on our wise which we observe.",
            "So in general that mean that this KF.",
            "This is an M by N matrix.",
            "If we got EM tasks is.",
            "Basically valid, all it needs to be as positive semidefinite.",
            "And specifying the inverse in Intel similarities.",
            "Now it's also worth saying one way to get this matrix is that if we might have things which are kind of like tasks descriptors OK, we might have some features that describe a task, and we can actually compute this.",
            "In to tell similarity based on say, some covariance function operating on those task descriptors.",
            "In fact, in the 1st way we got into multi task learning and actually we started off in that way.",
            "But what then we realized that in fact you know there's no reason why we can't just make essentially this this matrix of free parameter and actually just.",
            "Optimize or do inference over this problem over this matrix in terms of multi task learning.",
            "So it's also worth saying that we can have sums of the intrinsic correlation model.",
            "An in geostatistics terms, that's called the linear model of Co regionalization and this place is worth saying that these these kind of these kind of models at least as far as I know, one can trace them back in the Geo statistics in the future to say around the 70s."
        ],
        [
            "There's one quite nice interpretation of what's going on in the intrinsic correlation model, so we just saw it there.",
            "You thought about it as decomposing into this separable part between to task and.",
            "So X based similarity, but we can actually think about it in a different way.",
            "Just kind of nice when we worked it out we could have some latent functions zed one up to them we can take linear combinations of those latent functions and these are the weights.",
            "So for the end task I have row one up tomorrow, M each of the weights that we multiply these functions by to get FM OK, M Hill Little Lamb will run over from one to N. So basically it's saying that the.",
            "The different tasks essentially different linear combinations of the underlying.",
            "These of these latent processes, and if these latent processes have covariance, KXX primed, and then if this, basically the entry K FLM is just ro m.roloksothesedifferent.",
            "Mixing factors that we get here can't contribute to make the.",
            "This KFC in total similarity.",
            "So it's kind of nice to see that you can think about it like that as well.",
            "This kind of model is actually."
        ],
        [
            "Very similar to the semiparametric latent factor model of tea towel.",
            "The only difference is that in that case there actually a little bit more general where they allowed each of these latent processes to have different covariance functions, whereas we are actually imposing them that they have the same underlying X space covariance.",
            "And it turns out that as we see later, this kind of setup is actually very nicely tuned.",
            "Some of the things that we want to do.",
            "OK, by the way, if you want to ask questions on the way feel, feel free to do so."
        ],
        [
            "OK, so.",
            "Limits think about.",
            "Different structures how we might get to multitask.",
            "Learning in these different situations.",
            "So one set up for multi task learning is this kind of hierarchical modeling.",
            "Set up some latent parameter theater here and basically are the different tasks.",
            "So we got three tasks here.",
            "You draw these apps.",
            "These different realizations of the functions based on this this underlying theater.",
            "And then we observe the data the wide which is generated from these functions.",
            "And this is.",
            "And of course, we'll get sharing between the tasks, essentially through this latent structure.",
            "So we observe stuff here, you know, transmit information there influencing to say into here.",
            "Question is really well, what kinds of structure should we actually think about here for?"
        ],
        [
            "Leader so it could be, you know, some sort of genuine.",
            "If these were linear regression models, we might have some sort of generic kind of shrinkage prior on the coefficients of regression is like a nice topic, Gaussian.",
            "There might be some kind of other model we might have had taught me might have clusters we might believe, essentially that these different tasks fall into clusters and what that latent variable Len is essentially is telling us which cluster we're in and basically those things those most clusters should share things and really not share things across between different.",
            "Clusters and that actually can be interesting implemented in the ICM model, basically by.",
            "Imagine that those different tasks that kind of ordered so they occur in blocks matrix and basically have a block diagonal KF that specifies into task correlations within those set of tasks and.",
            "Has zero correlation between different tasks.",
            "And of course that that ordering into a block diagonal matrix just conceptual.",
            "If we learned the matrix.",
            "Actually find those tasks, even if they're not in that order.",
            "Another thing we might have, for example, is some kind of low dimensional."
        ],
        [
            "Subspace on theater and then that will actually give us kind of low rank structure in KF and we can see that really from this latent process for you here that in maybe if we have low rank structure essentially what's going to happen is actually their aunt M different process is here, but maybe some smaller number.",
            "OK so essentially the zeros that come in here in this setting.",
            "These loadings that then give rise to a reduced rank matrix.",
            "Is the kind of structure you might get."
        ],
        [
            "And of course, if that old nips operator effect is good, then mixture of X is good as well.",
            "OK, so we can combine these two ideas and have low rank structure within the blocks and also.",
            "Task clustering as well.",
            "And I think I mentioned this already.",
            "We can actually think about.",
            "If they're all task descriptors, we might parameterize this covariance function rather than just have it as a free form matrix.",
            "Of course, it's also worth saying that I'm describing this from a Bayesian hierarchical model, but these these kind of ideas can be expressed within a regularization correct framework.",
            "So, for example, this paper by again, you ET al in 2005.",
            "Does that kind of set up there.",
            "There are slight differences in the regularization permit.",
            "You may tend to think about the inverse covariance rather than the covariance, but.",
            "Certainly the high level.",
            "These kind of ideas are rather similar."
        ],
        [
            "OK, and so.",
            "One thing that you can think about happening in the we had that diagram with the latent theater up above.",
            "Really in some sense we could integrate that out to introduce correlations between these different FS, right?",
            "That's essentially what this structure that we're using.",
            "Specifying.",
            "Into task.",
            "Correlations are actually doing cross.",
            "Another way that we might."
        ],
        [
            "Think about.",
            "Getting different into task similarities and this is a motivating example, is essentially by.",
            "You know, we might think that somehow the second task is kind of a translated version of the first task that we have first task is this red curve and somehow the second task is a shifted version of that.",
            "Of course, perhaps potentially in multi dimensions.",
            "And this kind of idea, for example, was talked about by Ben David and Chill in 2003.",
            "So of course, we just think about this in the Gaussian process framework.",
            "If we essentially draw one function F1 and then kind of shift it to get F2, then basically this correlation between F1 X 2X prime basically can be rewritten like this costs X. F2 is just a shifted version of F1 shifted by this vector A and so we get this covariance.",
            "So basically will see that there are covariances arise between the task and cross covariances.",
            "Due to the.",
            "This shifting that goes on this is the kind of motivational example, but one can actually be."
        ],
        [
            "The.",
            "We can generalize this.",
            "OK, so there's some underlying latent function G. We take some kernel, which we convolve it with reduced FI.",
            "And we can imagine having a bunch of different.",
            "Kernels here, so for example, this Delta function that we've just seen we're H. Here is actually just the Delta function.",
            "This would give us the example.",
            "We just see where we can have kind of generic kernels here.",
            "So and these kind of ideas we talked about by number of different people.",
            "And one nice generalization of this is actually to have a number of different latent processes.",
            "Have these convolutions and then basically sum them up.",
            "So we take the art process involve it.",
            "There's some kernel for the ice observation, and then some of these things will end up with some basic set of.",
            "Convolutions that give rise to the underlying to these observable FIS so we can think about these F's being generated from these underlying latent geez by this kind of operation and in fact we've already seen the example of this kind of thing already and the only difference was that here basically this HI are the Delta function on submission at X = X prime.",
            "So basically just.",
            "Copying the function value at X, primed to here and maybe with some weight so that basically that idea linearly combining those those functions with some different weights is exactly what we've got here.",
            "But now we generalized it to introduce cut.",
            "Convolutional aspects as well, so it's another way of actually getting at the kind of cross covariances.",
            "OK."
        ],
        [
            "One other idea that we might have for this kind of task.",
            "And this also is something that can be traced back.",
            "Some way is the idea that well.",
            "And here we actually have something very unusual.",
            "For Maps, we have a neural network.",
            "Maybe the younger people have almost never seen such a thing.",
            "OK, so what's going on?",
            "We have some inputs.",
            "We transform this in some number of hidden layers and then here there are multiple outputs.",
            "OK, so we have N outputs.",
            "Basically, the idea is that we have some kind of feature extraction we can think about this hidden layer having done feature extraction on the inputs and then based on those features, these outputs are basically combining them in different amounts.",
            "So similar to that kind of idea of these.",
            "These are combining those latent functions, but one thing we might want to get is really the idea that you might want to kind of optimize.",
            "Want to tune this transformation.",
            "Ull stuff here in order to actually extract features that are relevant to the task.",
            "And this again is actually some of the ideas that you know for example in.",
            "Our so called deep learning we see essentially the idea is to do some feature extraction.",
            "Of course in this case this is a supervised feature extraction remote where motivating it by the fact we want to actually extract features such that they're good, helping us predict the outputs.",
            "So that's the kind of idea behind this, and this example is actually taken from 2003 worker back in Huskies where they actually did this thing in neural network setup.",
            "But"
        ],
        [
            "One thing that in fact so.",
            "Let me just go back to this slide if we think about having some nonlinear.",
            "Transformation here or the input?"
        ],
        [
            "Can we run a Gaussian process based on that on these on these?",
            "Values we obtain here.",
            "We think of them as inputs to the Gaussian process, and we have the function values as well.",
            "Basically what we've got is some nonlinear transformation of the input into these into these into a new space, and then we actually run the Gaussian process, for example on that new space.",
            "And that's the kind of idea, and so we can really.",
            "What you might want to do is to actually be cost.",
            "These tasks are sharing this hidden representation.",
            "That means that we really need to optimize the hyperparameters, which are essentially now the parameters in this transformation jointly across all tasks and that actually.",
            "It's fairly easy to do if we assume that these tasks are conditionally independent given the.",
            "The transformation here, then basically what's happening is that marginal likelihood that I wrote down earlier on just for one task, we just add that up over different tasks.",
            "But we remember that we're sharing the parameters of this.",
            "Processing across the different tasks.",
            "OK, so that's the idea.",
            "And then again, one can find."
        ],
        [
            "Some examples in the Gaussian process literature during this.",
            "In fact, men can pick our dinner in the paper that I think was rejected from NIPS.",
            "They published on their website actually.",
            "Pointed out that basically if we have multiple tasks, then we actually can jointly optimize over these hyperparameters and we want to do that so as to extract these features.",
            "So there are, you know, there can be fairly simple examples of feature extraction.",
            "I've drawn there a fairly complex example with nonlinear transformations, but basically we could also have something simpler with, so just linear transformations of the input into some, maybe some reduced space, and that pregnant or something I worked on with my student Francesco Vivarelli back in that.",
            "1999 there are other extensions of this kind of idea where there kind of some similarities going on, but maybe some allowing some flexibility.",
            "So not all processors are exactly the same kernel that somehow related kernel, so of course you know kind of hierarchical modeling framework.",
            "You can set up these kind of things.",
            "And there are some quite fun things that you could do which you could basically imagine.",
            "Thinking of the different tasks having different spaces, but kind of having a common hidden representation which we could use, so there's some fun things you could try to do."
        ],
        [
            "Switching gears a little bit if we come back to the intrinsic correlation model.",
            "We can ask well how much can a secondary task help us.",
            "So let's consider the simplest.",
            "Say that we can have this insertions acknowledged work done by my student coming Chai actually was presented in the main conference just earlier in the week.",
            "We have a primary task T and the secondary task S and basically in that situation this KF this matrix of correlations will reduce to something that has ones on the diagonal and some correlation rho in the off diagonal.",
            "Position.",
            "So that that basically defines the correlation between the tasks and then the other thing that we can have is basically the amount of data that we have in the primary task and in the secondary task.",
            "So here I'm going to assume that \u03c0 S is the amount of data in the second group task.",
            "OK. Then basically what we can do for given task if we have given X locations for the primary and the secondary task, we can actually compute the generalization error in this.",
            "Sorry, I actually find it quite annoying to hear whispering.",
            "Um?",
            "We can have these forgiven X locations in the primary and secondary secondary data and some correlation.",
            "We can workout essentially the predicted variance, which under assuming that the model is correct, we actually that's the the generalization error.",
            "What we then want to do, of course, is the average over that over the density.",
            "To get some average generalization error.",
            "But this depends on the particular locations of XMS.",
            "The kind of quotes that we might be interested in in terms of more theoretical characterizations is the learning curve.",
            "To do that, we really want to average over the possible locations of xtian access.",
            "That's basically what we like to get hold of is to understand how much and this is going to be a function in our terms of row and pious.",
            "So any is the number of data points of which \u03c0 S times enter in the secondary task and 1 -- \u03c0 S times N in the primary task is correlation rho?",
            "OK. OK, I'm only considering here 2 tasks.",
            "OK yeah, it's just a two by two matrix and row is the the off diagonal entry in that, yeah?",
            "XPS.",
            "So that is.",
            "OK, so in this kind of setup, what we're imagining is that we draw some locations.",
            "For the primary task and the secondary task from some underlying dense DP of X. OK, so that's the game.",
            "It's kind of theoretical current calculation.",
            "We play that game.",
            "Yeah, we're drawing the ex locations from identity.",
            "Yeah, OK, thanks and summary what we really want to understand is how much is it going to help us to have this secondary task right?",
            "We've got some data on the second detail.",
            "Is that gonna actually be a help?"
        ],
        [
            "And the.",
            "There's some fairly complicated results in camping thesis, but one fairly simple result.",
            "OK, we might have some.",
            "Here.",
            "We have a lower bound on the generalization error with covariance row.",
            "Basically, here's the case where we have zero correlation.",
            "So basically here we actually only have N * 1 -- \u03c0 S. These are the observations on the primary task and really the secondary task is telling us nothing here.",
            "And as we'd expect, basically.",
            "As we increase row.",
            "Basically, what's going to happen is that this the average generalization is forgiven N or pull down because we're getting information from the secondary task on the primary task, and it also it depends on on \u03c0 S as well.",
            "Basically, how much of the data is there.",
            "So fairly clean result you can get out of this and actually there the.",
            "Underlying relationship inequality that actually gives rise.",
            "This is actually is actually quite tight and so coming is done.",
            "Experiments on both 1D problems and on actually, uh, which in some of them to toy, but also in a rather more interesting problem.",
            "So the cycle stator for.",
            "Robot inverse dynamics.",
            "Is a kind of 21D data and there we have the distribution of X.",
            "Is that somehow natural for this task?",
            "He then created some Gaussian process is primary and secondary tasks using that kind of data and then?",
            "Tested the effectiveness of his.",
            "For this kind of relationship there and actually found also performed quite well there.",
            "So in other words, this kind of.",
            "Information with this kind of characterization is actually quite accurate about what's going on.",
            "So that was the kind of theoretical bit.",
            "Let me now."
        ],
        [
            "And perhaps I can just.",
            "In terms of that, let me just say that.",
            "I think that we've talked about various."
        ],
        [
            "Kinds of setup of how to induce correlations between these tasks, and I think it's very interesting to ask if there up.",
            "The other kinds of technique and other kinds of structures that we can actually exploit here.",
            "OK, so then to relate this to robot inverse dynamics.",
            "We might have some.",
            "Robot with a number of joint angles Q and basically what we want to do is we want to in this setup.",
            "You want to specify a trajectory, so we want to specify a sequence of cues.",
            "And then basically figure out how to predict the talks that we need to drive that.",
            "So actually we want to specify queues, Q dots, the angular velocities and QDC lots in angular acceleration.",
            "Given that we want to actually specify the talks we need to do that.",
            "That's the kind of control problem."
        ],
        [
            "And.",
            "OK, you could say, well, you know this is just physics, why don't you just do the right thing?",
            "OK, there are a few reasons why that's problematic.",
            "OK, so one is that yes, there is some kind of physics involving the kinetics, the potentials and maybe some discus frictions and so on.",
            "The other thing, of course, is it's really difficult to actually get get hold of these parameters.",
            "The other thing is also that you know.",
            "The frictions there are joint elasticity.",
            "This thing may not be the full on the full story and even.",
            "And it's particularly the case that we might have, you know, you might change the circumstances under which were operating.",
            "For example, have different loads on the end of the end effector, and this changes all these equations.",
            "OK, so it's not entirely.",
            "It's not like we this is just a irrelevant problem.",
            "We actually really do need to have these kind of inverse dynamics problems.",
            "Particularly in things like compliant, lightweight, humanoid robots.",
            "So I'd say particularly that.",
            "This is the work where my colleague Sir Stephen Clanking so two.",
            "Vijay Kumar the robot assists and that's what this is.",
            "They know their input on this and understanding this is very important."
        ],
        [
            "So yeah, there's just saying the functions change if we have different loads on the end that that that happens and.",
            "Bad news is of course yes.",
            "We need a different inverse dynamics model for different loads, and the other thing is of course if I pick up a heavy load, wave it around it may actually.",
            "Maybe I'll have different kinds of trajectory's different kinds of this X SpaceX member is QQQ double dot.",
            "OK."
        ],
        [
            "And then.",
            "But there is also some good news and basically certainly in the case we just varying the mass on the end effector.",
            "Then changes are basically happening there.",
            "And this is also a very nice result that OK, the talks are basically comprised of a set of nonlinear.",
            "Functions of X these yiz.",
            "But they're also depending linearly on these parameters, Pi OK?",
            "And these characterize the mass and the moments of inertia and so on.",
            "OK, So what of the object on the load?",
            "So that should start to look familiar, engaged and start to say OK.",
            "I've got some functions here and I've got different linear combinations of those functions.",
            "Need we saw something earlier about that so just a little bit of notation.",
            "Of course that we can do some.",
            "Essentially there's some freedom here about if we re parameterized in here so we might think of these latent functions as being called and said, and these weights for the different tasks.",
            "Being called an OK.",
            "So essentially that."
        ],
        [
            "Set up that we had four applies again.",
            "We've got these different latent.",
            "Functions here and actually.",
            "This will apply.",
            "Actually, this applies separately for all different joints.",
            "That's why there's a plate outside here.",
            "OK, but then here we have a plate over tasks M. So what we're saying is that for this joint, the ice joint we draw, we draw these latent functions and have linear combinations of them and those linear combinations of those rows.",
            "Exactly what we saw before.",
            "So we again we have that Inter task similarity derived from this this.",
            "From this linear combination of these latent functions.",
            "Yes, OK, that's what I mean by different tasks.",
            "Thanks for the clarification."
        ],
        [
            "OK. We we need to specify actually do experiments.",
            "We need to express it fast.",
            "Specify our prior Care XX, primed, and basically we're going to use a fairly standard thing, something like a bias, which just basically as a prior variance and shifting things up and down linear in the.",
            "So look at the linear kernel here.",
            "Squared exponential kernel.",
            "On this ARD is automatic relevance determination, so it means basically there's different dimensions could have different weights or different importances, and also this Coulomb friction term which depends on the sign of the angular velocity Q dot.",
            "That's the kind of kernel and it was actually important for this data to use this kernel function to.",
            "OK, so."
        ],
        [
            "Are we getting data from we're actually using robot ball using realistic simulator?",
            "The Puma 560 robot arm?",
            "And there's also a bunch of setups that we can have here.",
            "We can drive this arm along different trajectories using figure of eight trajectories at different speeds, so therefore different paths.",
            "Different locations the origin of the workspace and we've got different speeds so that we can drive this thing along and so that basic and we've also got different loads, so we've got different masses in range .2 to three KG various shapes and sizes.",
            "So basically we can collect data by putting these different masses on, driving them along the different trajectories, and so on.",
            "What how we actually did that is that there is one sort of reference trajectory on which.",
            "Let me just get them."
        ],
        [
            "Invitation OK yeah I will.",
            "I'll refer to these as paths and the combination of path and speed is a trajectory so OK there full pads and full speed giving us 16 different trajectories overall.",
            "So basically this comes out of reference trajectory.",
            "So all we have this load and for all different loans we drive along some common trajectory and then we'll also use unique trajectory's one for each of the different.",
            "Modes.",
            "And then what's happening is that we actually have one trajectory in which we have no data, and that's always novel.",
            "And then really two ways we might test what's going on and how much information we get sharing across tasks.",
            "And so the whole goal of this is really that we would hope that by having these different trajectories.",
            "And these different loads which are.",
            "Data from those should actually help us to predict better to get the inverse dynamics better for a given task.",
            "OK, that's the that's the goal, how he tested that is in two ways.",
            "One is a kind of interpolating task.",
            "So you saw here that there's one reference trajectory and 14 unique trajectories.",
            "OK, and then what?",
            "What we can do is basically we gotta do a kind of interpolation where we test only on that.",
            "Comment on the trajectory is where we have actually training data.",
            "Or we could do a kind of extrapolate re thing where we actually look at all different trajectories.",
            "OK, so those are the two setups.",
            "And OK, what are the different ways we can do this?",
            "Or one way is we can just say OK, I've got a single load, I'll just train a Gaussian process predictor for doing this.",
            "To prediction.",
            "Do inverse dynamics.",
            "Another thing we could do is we could say."
        ],
        [
            "OK, we got a bunch of independent Gaussian processes, but they can actually share hyperparameters so.",
            "In here there were a bunch of hyperparameters involved in this covariance function, and we want to show."
        ],
        [
            "Are those."
        ],
        [
            "You can also do kind of pull Gaussian process where what we actually have is basically to put all the data, just stick it all in to say this is all data from the same task.",
            "Let's just train on that.",
            "And then the thing that we hope will do better.",
            "Is there multitask Gaussian process where we have basically this kind of structure we talked about?",
            "Here is KFKX decomposition.",
            "It is worth saying once a technical note is that we actually of course can determine we can have some choice over the rank of KF.",
            "OK, and we actually determine that using a BICS criterion.",
            "So we tested different ranks and these be icy.",
            "OK, so he."
        ],
        [
            "The results basically OK. What's going on?",
            "First of all, this is for a particular joint joint five.",
            "They're actually 6 joints.",
            "This is the interpolation task.",
            "This is an extrapolation task.",
            "This is the size of the amount of training data we have.",
            "That's actually the total number of points across all tasks, and this is the mean average normalized mean squared error.",
            "So basically it's quite a lot of averaging that can go on here because we have a number of different tasks over which you want to to actually average.",
            "So what we can see.",
            "So this is the color coding for.",
            "Single GP, The independent GP.",
            "The pool GP and the multi tenant GP basically see this kind of structure so the multi task one is the line black.",
            "It's doing better here but maybe there are some error bars.",
            "Here is perhaps a bit hard to see.",
            "How well it's doing?",
            "We'll see that on the next figure.",
            "Basically, you're seeing input, so here on this joint.",
            "Basically the independent Gaussian process is the kind of best competitor here.",
            "And similarly on this task.",
            "Basically these are just kind of nominal values that would otherwise go off the scale.",
            "And.",
            "Just to look at so obviously one."
        ],
        [
            "You can do, yeah.",
            "So."
        ],
        [
            "Right?",
            "Well, OK, OK, this is just a nominal level.",
            "OK, these are actually really off somewhere up here.",
            "Let me just plotting them here.",
            "Yeah, so yes, yeah I would agree that probably adding more data should help.",
            "Yes, OK, that's a fair question, but.",
            "Yeah, but there's someone out there somewhere off the scale here anyway, OK.",
            "So really, just to get a bit more insight."
        ],
        [
            "What's going on?",
            "We can, of course do paired comparisons, 'cause when we do the experiments you might, you know, choose some sample size and will be actually draw different data.",
            "Points.",
            "Of course.",
            "Will you repair that across the different experiment so we can look at the differences of the average MSE and basically what you're seeing here is so actually there are five replications, so we've plotted all those data.",
            "Really there isn't much difference on the interpolatory task between these model, but on the extrapolate re task.",
            "So we're actually comparing here.",
            "The paired difference between the best.",
            "Out of the pool, the Independent or the single, and this is the multi task.",
            "So this difference here actually shows.",
            "Basically there is a win on the extrapolator task here.",
            "Almost done just."
        ],
        [
            "Basically what I've tried to tell you about is the multitask formulation of Gaussian processes and the kind of cokriging framework.",
            "And we've seen how we can try to construct things that have cross covariances and how to use those.",
            "And this also, hopefully I've convinced you, at least in the multi context, inverse dynamics problems are quite natural fit that actually remember is actually to the ICM.",
            "The intrinsic correlation model that simple one factorizes between task similarity and X space similarity.",
            "And I think one of the most interesting questions really is to say what kinds of.",
            "Structure are there that there might be between between tasks, either in the GP framework.",
            "Maybe they're different kinds of covariance functions, or cross covariance functions, normal use.",
            "Or maybe there are things that you know.",
            "Really good models to do multi task learning, but actually it's very hard to get those into GP framework.",
            "I'd like I'd be interested to hear if there are thoughts on that.",
            "Thank you."
        ],
        [
            "Short motorcycle questions.",
            "Who is noise free or is there?",
            "Good question.",
            "OK.",
            "I wouldn't.",
            "OK, so this is this is the kind of place where the student advisor should come clean and say kambing really knows the details of that.",
            "I'm not going to.",
            "I'm not going to guess my guess.",
            "Well, OK, I'm I'm not going to assert that it's one thing or the other.",
            "My guess is that would be noise in there.",
            "If anything, this noise will have to be the same in this class.",
            "In no, I don't think so.",
            "It's certainly.",
            "We can certainly set this up to have slight noise.",
            "Variances are different on different tasks.",
            "There's no difficulty in doing that, and you can kind of put that into the hyperparameter optimization.",
            "That would certainly be a reasonable thing to do.",
            "Very good thing.",
            "I ready when I say.",
            "Classification label regression.",
            "OK, certainly we could imagine having you know there are correlated latent process is and we can have kind of different readout mechanisms, almost latent processes, as you'll be familiar currently easiest way to do GP stuff is is with the regression situation.",
            "But yeah, that that's actually a nice point that we could use essentially any sort of generalized linear model readout for those different types.",
            "Maybe you can think yeah.",
            "Other questions.",
            "So you say you you can talk about the essential question relation between the middle part.",
            "Minus.",
            "That refers to the role.",
            "There is no.",
            "Maybe I missed it.",
            "How do you decide?",
            "Hopefully OK, so basically we can just optimize over that matrix.",
            "And it's OK, so perhaps I must have avoided saying that.",
            "Basically.",
            "There are various ways of doing it.",
            "You can actually do kind of VM things, or you can just do kind of gradient descent.",
            "Things on on echo.",
            "So we represent their matrix bytes to ASCII factorization and then optimize over that.",
            "So yeah, thank you for clarifying that, yeah.",
            "Do gradient ascent.",
            "Weather Christian server.",
            "Motion distributions there are these conspiratorial results, like, for example, the Goshen is either knowledge about the 2nd order.",
            "This is the distribution implies there so.",
            "Some similar kinds of results for Goshen Process or in the more general monitors.",
            "Motion process is.",
            "Um?",
            "OK, so if we just simplify to their sort of 1 task single task Gaussian process.",
            "I imagine that a similar result would specify go through if you ask about this stochastic process that has.",
            "Say, for example, you know some variance or some.",
            "Mean functions and covariance function.",
            "And you say what's the stochastic process that has maximum entropy I?",
            "Imagine that the Gaussian process the answer that, but they cannot hesitate to sort of assert that too strongly, because they often can be mathematical niceties that one has to take care of.",
            "And I I'm not familiar enough with that.",
            "The reason of course I'm asking is that you know then.",
            "Single golfers, you can save it if you don't know anything other than the 2nd order statistics.",
            "That working is is with some some sense the most non informative way to specify a moral.",
            "So then if there was some kind of result like this then maybe it would.",
            "It would tell why grocery process will be the way to go other than of course these these nice abilities to to have analytical results.",
            "Yes.",
            "Yeah, I think that's a.",
            "That's a reasonable idea, although there's something that's kind of.",
            "Thanks you slightly nervous about the Gaussian from the sort of robustness points of views right then.",
            "The Gaussian is notoriously non robust, right?",
            "So it's it's a maximum entropy conditional on that meaning covariance structure of course.",
            "Yeah, maybe maybe I shouldn't speculate any further, OK?",
            "I'm just wondering whether it would be possible to deal with this problem of music acts as a structured prediction problem.",
            "Cowboy.",
            "Presently we have one case.",
            "Reply.",
            "At outlook.",
            "What's essentially what?",
            "Essentially what we actually do have here, right?",
            "You know that already, right?",
            "Yes.",
            "Is going to put more fires is the order would be structured output problem?",
            "OK, fair question.",
            "I think I'll have to remember exactly how the Tasker L stuff works well enough in order to actually to answer that question.",
            "I mean.",
            "OK, so there's just wondering if it's good idea to do this now, or whether we should take this offline.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hope you all had a nice lunch break or skybreaker whatever you decided to do.",
                    "label": 0
                },
                {
                    "sent": "So I'm Yorkville today and I'll be the chair for this afternoon session.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "5 interesting talks.",
                    "label": 0
                },
                {
                    "sent": "So will have first be invited talking to regular talks, then a coffee break.",
                    "label": 0
                },
                {
                    "sent": "After that, the last two talks and then some discussion.",
                    "label": 0
                },
                {
                    "sent": "So first I'd like to welcome our second invited speaker, Professor Chris Williams, from the University of Edinburgh.",
                    "label": 0
                },
                {
                    "sent": "So Chris is.",
                    "label": 0
                },
                {
                    "sent": "Well, Chris is done.",
                    "label": 0
                },
                {
                    "sent": "A lot of things, but one of the things he has especially been active in Goshen process is yes, within a large number of papers on that, and I think he has even written a book about that with call Rasmussen.",
                    "label": 0
                },
                {
                    "sent": "And in addition he has also studied how these.",
                    "label": 0
                },
                {
                    "sent": "Very interesting probabilistic models can be used in multitask learning and I believe this is what we will.",
                    "label": 0
                },
                {
                    "sent": "Talk about now so.",
                    "label": 0
                },
                {
                    "sent": "OK, yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the invitation and hopefully the doors are.",
                    "label": 0
                },
                {
                    "sent": "Still working right?",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to talk about was multitask, learning, GPS and I should say this is work.",
                    "label": 0
                },
                {
                    "sent": "Well, no report on a couple of pieces of work one by my student Kambing Chai.",
                    "label": 0
                },
                {
                    "sent": "And also by colleague Stefan Plank in situ.",
                    "label": 0
                },
                {
                    "sent": "Vijay Kumar at Edinburgh.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So really, just to motivate multitask learning, we can get and they can be multi task problems that occur in all sorts of places.",
                    "label": 1
                },
                {
                    "sent": "So in geostatistics, we have things like the Co occurrence of different oils and so on.",
                    "label": 0
                },
                {
                    "sent": "Of course, in machine learning problems we might have multiple tasks, such as like multiple object categories.",
                    "label": 1
                },
                {
                    "sent": "We want to recognize, and maybe correlations between these different classes.",
                    "label": 0
                },
                {
                    "sent": "You might think about personalization, where there's basically a whole bunch of different related problems.",
                    "label": 0
                },
                {
                    "sent": "We might be interested in.",
                    "label": 0
                },
                {
                    "sent": "This is also work that my group is done on.",
                    "label": 0
                },
                {
                    "sent": "If we have essentially optimization over space of transformations with the compilers, then there may be multiple problem programs that we're trying to optimize over and we want to learn about what commonality's there are between the different problems.",
                    "label": 0
                },
                {
                    "sent": "So fundamentally and also this robot inverse dynamics, which will actually talk about later.",
                    "label": 1
                },
                {
                    "sent": "So basically the key idea, which is very familiar idea in statistical modeling.",
                    "label": 0
                },
                {
                    "sent": "Is the idea to gain strength across different tasks in some sense the question is, well, really.",
                    "label": 0
                },
                {
                    "sent": "How should we do this?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the kind of goal here.",
                    "label": 0
                },
                {
                    "sent": "The other thing to say is that I'm really going to be talking about multi task learning and of course it is possible to take a rather broader view of things and talk about.",
                    "label": 0
                },
                {
                    "sent": "For example, you know ideas about rather more high level ideas about meta learning and so on.",
                    "label": 0
                },
                {
                    "sent": "Learning about learning.",
                    "label": 0
                },
                {
                    "sent": "So we might have for example lots of different learning algorithms we can apply to different problems we might be interested in trying to understand when different learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Good for different situations.",
                    "label": 0
                },
                {
                    "sent": "I think those are interesting things to do, but they are not the reader focus of my talk this afternoon.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as an outline, will talk about first about Gaussian process prediction.",
                    "label": 1
                },
                {
                    "sent": "Hope that's familiar to many, but maybe it's useful to have a revision and then talk about Coke reading, which is basically the sort of geostatistics term in this idea of multi task learning with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about one particular model that we can use for Coke region called the intrinsic correlation model.",
                    "label": 1
                },
                {
                    "sent": "This is language coming out of geostatistics, and we talk about then about how different kinds of notions of task relation this can actually be.",
                    "label": 0
                },
                {
                    "sent": "Put into the Gaussian process framework through kind of higher hierarchical modeling ideas, but also transformations of the input ideas and also some other ideas about shared feature extraction.",
                    "label": 0
                },
                {
                    "sent": "And also say that I'm kind of interested in the case as well.",
                    "label": 0
                },
                {
                    "sent": "If there are things that don't fit into the GP framework as well, and I don't wish to shoehorn everything into that framework and.",
                    "label": 0
                },
                {
                    "sent": "Used to think about that.",
                    "label": 0
                },
                {
                    "sent": "Those are some nice theory which my student camming has done on about theory for this intrinsic correlation model we talk about briefly and I also want to talk about the example of multi task learning in robot inverse dynamics which I think was meant to be a good fit between this workshop was it actually has robotics and multi task.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's where I want to go.",
                    "label": 0
                },
                {
                    "sent": "So what's a GP basically going to infinite infinite dimensional Gaussian?",
                    "label": 0
                },
                {
                    "sent": "And certainly this wouldn't keep mathematicians happy, but we think of a function is essentially an infinitely long vector, and then you want to define a Gaussian distribution with being collection of random variables and any finite subset will have a consistent Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "This will define the Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "And OK, and just in the same way that a Gaussian is, is fully specified by a mean and covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Then I guess in process is fully specified by amine function you have X or M of X and the covariance function K of XX point.",
                    "label": 0
                },
                {
                    "sent": "OK, specifying the covariance between the two locations X&X prime.",
                    "label": 0
                },
                {
                    "sent": "So that's that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "P. And basically the key thing that saves us.",
                    "label": 0
                },
                {
                    "sent": "Dealing with infinitely long vectors that all functions the whole time is the marginalization property of a Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So basically they have a huge great Gaussian and I'm only interested in some of those variables.",
                    "label": 0
                },
                {
                    "sent": "All I have to do is to take that block of the matrix which I'm actually interested in.",
                    "label": 0
                },
                {
                    "sent": "I can marginalized everything else.",
                    "label": 0
                },
                {
                    "sent": "OK, it's important to say.",
                    "label": 0
                },
                {
                    "sent": "We use the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "We can do this right if we actually were working in terms of inverse covariance matrices, then that actually wouldn't work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically we can marginalized on the covariance and that's why working with the covariance function is a good idea.",
                    "label": 0
                },
                {
                    "sent": "And there's also, by the way, why it can be difficult when dealing with say models that are defined in terms of parameterising the inverse covariance, such as Gaussian MRF's.",
                    "label": 0
                },
                {
                    "sent": "And this is more complex than to make that marginalization.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so in the end it means that we basically consider, say we're in a standard machine living set up.",
                    "label": 0
                },
                {
                    "sent": "We've got N training examples we can be interested in the function values at those corresponding to those axes.",
                    "label": 1
                },
                {
                    "sent": "And of course if we have a test point, was just the N + 1 point.",
                    "label": 0
                },
                {
                    "sent": "So we can take this huge vector and really just consider the N + 1 dimensional distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the good news.",
                    "label": 0
                },
                {
                    "sent": "Just to give some intuitions about what sampling in Gaussian Pro.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look like, well, we we can define some function F of X with some mean and some covariance.",
                    "label": 0
                },
                {
                    "sent": "So this mean is 0 here and this is a coherence.",
                    "label": 0
                },
                {
                    "sent": "Of course we have a lot of choices over covariance functions and it's very important to emphasize that this thing that looks perhaps something like a Gaussian isn't the Gaussian here.",
                    "label": 0
                },
                {
                    "sent": "OK this, I'll tend to call this A squared exponential kernel, basically 'cause it's an exponential function, it's a square in here and.",
                    "label": 0
                },
                {
                    "sent": "We could have lots of other forms of covariance function here.",
                    "label": 0
                },
                {
                    "sent": "For example, if I just replace this X -- X prime squared, we just absolute value of X.",
                    "label": 0
                },
                {
                    "sent": "The value of that argument I'll end up with the Ornstein Uhlenbeck covariance function, which actually has very different properties to the.",
                    "label": 0
                },
                {
                    "sent": "To this squared exponential kernel, it's about as different as you could actually have, while still being a Gaussian process, for example the.",
                    "label": 1
                },
                {
                    "sent": "Stoneback function is no more mean square differentiable, whereas this is infinitely mean square differentiable.",
                    "label": 0
                },
                {
                    "sent": "So basically what I understand what GPS look like, we can just we can choose some function locations and then just sample from this thing and that's kind of nice and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do.",
                    "label": 0
                },
                {
                    "sent": "Of course, there really.",
                    "label": 0
                },
                {
                    "sent": "It's really looks like this and I just happened to join these dots to plot, but really what we're actually generating is those finite dimension functions.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing, OK, we can do that in 1D and 2D and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any D that we want.",
                    "label": 0
                },
                {
                    "sent": "The other thing, of course, that's really nice about gassing processes, is that we can actually do inference with them, so we can define some prize over functions.",
                    "label": 0
                },
                {
                    "sent": "That's nice.",
                    "label": 0
                },
                {
                    "sent": "And for Gaussian distributions, if we assume that we have for example.",
                    "label": 0
                },
                {
                    "sent": "Boys with variance Sigma squared N on the observations.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually analytically get this posterior posterior function.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of why stars I think of why star evaluated X star.",
                    "label": 0
                },
                {
                    "sent": "So this is the posterior.",
                    "label": 0
                },
                {
                    "sent": "And we get this thing that looks like that basically.",
                    "label": 0
                },
                {
                    "sent": "So here's the prior, you know.",
                    "label": 0
                },
                {
                    "sent": "Observe these data points here.",
                    "label": 0
                },
                {
                    "sent": "And that basically narrows down the function that those locations is, specially if the noise level is small and I'm proving here predicted variance.",
                    "label": 0
                },
                {
                    "sent": "OK, basically where we have data, the variance is small and it kind of increases like this.",
                    "label": 0
                },
                {
                    "sent": "And so these are actually these red, green, and blue things actually samples drawn from the posterior.",
                    "label": 0
                },
                {
                    "sent": "And that's actually quite easy to do 'cause we can actually get the full joint posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, well I'm showing here is just the marginal posterior to certain location.",
                    "label": 0
                },
                {
                    "sent": "Next are given the data, but we can actually do is workout the full covariance posterior covariance or any set of data points.",
                    "label": 0
                },
                {
                    "sent": "Given the data, so it's kind of the nice things you can do with Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that's maybe that's boringly Famille.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To many of you.",
                    "label": 0
                },
                {
                    "sent": "Just to say one other thing, the marginal likelihood is basically by working at the probability of the log problem.",
                    "label": 1
                },
                {
                    "sent": "The data under this model, and of course this is a Gaussian.",
                    "label": 1
                },
                {
                    "sent": "This is just the log of a multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And so we can workout this in closed form.",
                    "label": 0
                },
                {
                    "sent": "And what's more important is that if there are parameters that define this covariance function K, and basically one can adjust the.",
                    "label": 0
                },
                {
                    "sent": "Parameters in case so as to maximize the marginal likelihood, or if we're being Bayesian kersley examples sample over this posterior.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing is that this this thing is in closed form.",
                    "label": 0
                },
                {
                    "sent": "That's the good news.",
                    "label": 0
                },
                {
                    "sent": "It's also worth saying, of course, that.",
                    "label": 0
                },
                {
                    "sent": "If we think about this as a function of the parameters that define the covariance function, then really there can be different things that go on.",
                    "label": 0
                },
                {
                    "sent": "We can essentially give different interpretations of the data and some of the most.",
                    "label": 1
                },
                {
                    "sent": "Obvious examples of that.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have.",
                    "label": 0
                },
                {
                    "sent": "Go back if we have date.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, we could believe that there actually there's some function with a reasonably short length scale and this would be interpolating these data points.",
                    "label": 0
                },
                {
                    "sent": "We could believe that we actually had a very long length scale, so essentially with some high noise level.",
                    "label": 0
                },
                {
                    "sent": "So basically these two things can be encoded in the model, and it's quite easy to actually see as well what the different relative probabilities of those configurations are.",
                    "label": 0
                },
                {
                    "sent": "Fundamentally, what what that will be driven by is essentially looking at you.",
                    "label": 0
                },
                {
                    "sent": "If this really was.",
                    "label": 0
                },
                {
                    "sent": "So this long length scale process with noise.",
                    "label": 0
                },
                {
                    "sent": "These were essentially this sequence would be white.",
                    "label": 0
                },
                {
                    "sent": "OK, there being a noise.",
                    "label": 0
                },
                {
                    "sent": "So there are things that actually we can tell about what's going on in the data.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "That's just a recap about.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gaussian process is.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about multitask learning.",
                    "label": 0
                },
                {
                    "sent": "So really now we have.",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "The EM tasks and if we consider the elf task will have FL, which is the function value of XI on task L. Really all we have to do to extend the previous framework before we had to say for any X&X prime, what's the covariance function for those two?",
                    "label": 1
                },
                {
                    "sent": "Those two locations?",
                    "label": 0
                },
                {
                    "sent": "So this effort effort.",
                    "label": 0
                },
                {
                    "sent": "XFX prime that the covariance was K of XX prime.",
                    "label": 0
                },
                {
                    "sent": "Now all we have to do is to say.",
                    "label": 0
                },
                {
                    "sent": "We have to introduce extra arguments here that specify the two different tasks we're interested in.",
                    "label": 0
                },
                {
                    "sent": "And what we really need to do is just a great one.",
                    "label": 0
                },
                {
                    "sent": "Big Gaussian OK, it's just what we're going to do is create some joint Gaussian that specifies the correlations within tasks and also between tasks.",
                    "label": 0
                },
                {
                    "sent": "I'm really.",
                    "label": 0
                },
                {
                    "sent": "The whole question hold pretty much the rest of the talk is about, well.",
                    "label": 0
                },
                {
                    "sent": "What are reasonable structures for doing that?",
                    "label": 0
                },
                {
                    "sent": "And then how can we compute with it and at the moment I'm not gonna worry too much about the computational issues.",
                    "label": 0
                },
                {
                    "sent": "Let's assume we given this big Gaussian, we can actually compute with it.",
                    "label": 0
                },
                {
                    "sent": "Of course, sometimes we have to make approximations as a lot of different approximations.",
                    "label": 0
                },
                {
                    "sent": "We could talk about, but I'll just keep it conceptual for now.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pictorially.",
                    "label": 0
                },
                {
                    "sent": "This is going on if we think of this X dimension.",
                    "label": 0
                },
                {
                    "sent": "Here we consider one function.",
                    "label": 0
                },
                {
                    "sent": "We can trace it out like this.",
                    "label": 0
                },
                {
                    "sent": "Fundamentally, what we've got is we've got a set of correlated functions.",
                    "label": 0
                },
                {
                    "sent": "So if this we think of these are the different functions we could draw.",
                    "label": 0
                },
                {
                    "sent": "Somehow these are correlated.",
                    "label": 0
                },
                {
                    "sent": "As we can imagine.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, drawing independent samples like we had.",
                    "label": 0
                },
                {
                    "sent": "Way back here, right these?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially, independent samples drawn from the same covariance, what we're now doing is we're saying we got multiple tasks.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And those are actually somehow correlated, so these ones, things that are nearby along this axis are quite correlated.",
                    "label": 0
                },
                {
                    "sent": "That's the basic idea behind multi task Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I was saying really, the question is well.",
                    "label": 0
                },
                {
                    "sent": "What are appropriate cross covariance structures that fit into the ideas of multitask learning and also other things that don't fit so well with that?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is perhaps a simple building block of.",
                    "label": 0
                },
                {
                    "sent": "Multitell Scouting Process is the so called intrinsic correlation model.",
                    "label": 1
                },
                {
                    "sent": "So what we want to do is to if we have FLXN FM X prime, then we're going to set and have a circle separable covariance function that separates the Inter task similarity.",
                    "label": 0
                },
                {
                    "sent": "In this matrix CLM.",
                    "label": 0
                },
                {
                    "sent": "And this covariance function here, which specifies the similarity next space it was separated out into task similarity and the sort of Inter X similarity.",
                    "label": 0
                },
                {
                    "sent": "I will assume we have observation lines on on our wise which we observe.",
                    "label": 0
                },
                {
                    "sent": "So in general that mean that this KF.",
                    "label": 0
                },
                {
                    "sent": "This is an M by N matrix.",
                    "label": 1
                },
                {
                    "sent": "If we got EM tasks is.",
                    "label": 0
                },
                {
                    "sent": "Basically valid, all it needs to be as positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "And specifying the inverse in Intel similarities.",
                    "label": 0
                },
                {
                    "sent": "Now it's also worth saying one way to get this matrix is that if we might have things which are kind of like tasks descriptors OK, we might have some features that describe a task, and we can actually compute this.",
                    "label": 1
                },
                {
                    "sent": "In to tell similarity based on say, some covariance function operating on those task descriptors.",
                    "label": 0
                },
                {
                    "sent": "In fact, in the 1st way we got into multi task learning and actually we started off in that way.",
                    "label": 0
                },
                {
                    "sent": "But what then we realized that in fact you know there's no reason why we can't just make essentially this this matrix of free parameter and actually just.",
                    "label": 0
                },
                {
                    "sent": "Optimize or do inference over this problem over this matrix in terms of multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So it's also worth saying that we can have sums of the intrinsic correlation model.",
                    "label": 0
                },
                {
                    "sent": "An in geostatistics terms, that's called the linear model of Co regionalization and this place is worth saying that these these kind of these kind of models at least as far as I know, one can trace them back in the Geo statistics in the future to say around the 70s.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's one quite nice interpretation of what's going on in the intrinsic correlation model, so we just saw it there.",
                    "label": 0
                },
                {
                    "sent": "You thought about it as decomposing into this separable part between to task and.",
                    "label": 0
                },
                {
                    "sent": "So X based similarity, but we can actually think about it in a different way.",
                    "label": 0
                },
                {
                    "sent": "Just kind of nice when we worked it out we could have some latent functions zed one up to them we can take linear combinations of those latent functions and these are the weights.",
                    "label": 0
                },
                {
                    "sent": "So for the end task I have row one up tomorrow, M each of the weights that we multiply these functions by to get FM OK, M Hill Little Lamb will run over from one to N. So basically it's saying that the.",
                    "label": 0
                },
                {
                    "sent": "The different tasks essentially different linear combinations of the underlying.",
                    "label": 0
                },
                {
                    "sent": "These of these latent processes, and if these latent processes have covariance, KXX primed, and then if this, basically the entry K FLM is just ro m.roloksothesedifferent.",
                    "label": 0
                },
                {
                    "sent": "Mixing factors that we get here can't contribute to make the.",
                    "label": 0
                },
                {
                    "sent": "This KFC in total similarity.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of nice to see that you can think about it like that as well.",
                    "label": 0
                },
                {
                    "sent": "This kind of model is actually.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very similar to the semiparametric latent factor model of tea towel.",
                    "label": 1
                },
                {
                    "sent": "The only difference is that in that case there actually a little bit more general where they allowed each of these latent processes to have different covariance functions, whereas we are actually imposing them that they have the same underlying X space covariance.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that as we see later, this kind of setup is actually very nicely tuned.",
                    "label": 0
                },
                {
                    "sent": "Some of the things that we want to do.",
                    "label": 0
                },
                {
                    "sent": "OK, by the way, if you want to ask questions on the way feel, feel free to do so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Limits think about.",
                    "label": 0
                },
                {
                    "sent": "Different structures how we might get to multitask.",
                    "label": 1
                },
                {
                    "sent": "Learning in these different situations.",
                    "label": 0
                },
                {
                    "sent": "So one set up for multi task learning is this kind of hierarchical modeling.",
                    "label": 0
                },
                {
                    "sent": "Set up some latent parameter theater here and basically are the different tasks.",
                    "label": 0
                },
                {
                    "sent": "So we got three tasks here.",
                    "label": 0
                },
                {
                    "sent": "You draw these apps.",
                    "label": 0
                },
                {
                    "sent": "These different realizations of the functions based on this this underlying theater.",
                    "label": 0
                },
                {
                    "sent": "And then we observe the data the wide which is generated from these functions.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "And of course, we'll get sharing between the tasks, essentially through this latent structure.",
                    "label": 0
                },
                {
                    "sent": "So we observe stuff here, you know, transmit information there influencing to say into here.",
                    "label": 0
                },
                {
                    "sent": "Question is really well, what kinds of structure should we actually think about here for?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leader so it could be, you know, some sort of genuine.",
                    "label": 0
                },
                {
                    "sent": "If these were linear regression models, we might have some sort of generic kind of shrinkage prior on the coefficients of regression is like a nice topic, Gaussian.",
                    "label": 0
                },
                {
                    "sent": "There might be some kind of other model we might have had taught me might have clusters we might believe, essentially that these different tasks fall into clusters and what that latent variable Len is essentially is telling us which cluster we're in and basically those things those most clusters should share things and really not share things across between different.",
                    "label": 0
                },
                {
                    "sent": "Clusters and that actually can be interesting implemented in the ICM model, basically by.",
                    "label": 1
                },
                {
                    "sent": "Imagine that those different tasks that kind of ordered so they occur in blocks matrix and basically have a block diagonal KF that specifies into task correlations within those set of tasks and.",
                    "label": 0
                },
                {
                    "sent": "Has zero correlation between different tasks.",
                    "label": 0
                },
                {
                    "sent": "And of course that that ordering into a block diagonal matrix just conceptual.",
                    "label": 0
                },
                {
                    "sent": "If we learned the matrix.",
                    "label": 0
                },
                {
                    "sent": "Actually find those tasks, even if they're not in that order.",
                    "label": 0
                },
                {
                    "sent": "Another thing we might have, for example, is some kind of low dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subspace on theater and then that will actually give us kind of low rank structure in KF and we can see that really from this latent process for you here that in maybe if we have low rank structure essentially what's going to happen is actually their aunt M different process is here, but maybe some smaller number.",
                    "label": 0
                },
                {
                    "sent": "OK so essentially the zeros that come in here in this setting.",
                    "label": 0
                },
                {
                    "sent": "These loadings that then give rise to a reduced rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Is the kind of structure you might get.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, if that old nips operator effect is good, then mixture of X is good as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can combine these two ideas and have low rank structure within the blocks and also.",
                    "label": 0
                },
                {
                    "sent": "Task clustering as well.",
                    "label": 0
                },
                {
                    "sent": "And I think I mentioned this already.",
                    "label": 0
                },
                {
                    "sent": "We can actually think about.",
                    "label": 0
                },
                {
                    "sent": "If they're all task descriptors, we might parameterize this covariance function rather than just have it as a free form matrix.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's also worth saying that I'm describing this from a Bayesian hierarchical model, but these these kind of ideas can be expressed within a regularization correct framework.",
                    "label": 0
                },
                {
                    "sent": "So, for example, this paper by again, you ET al in 2005.",
                    "label": 0
                },
                {
                    "sent": "Does that kind of set up there.",
                    "label": 0
                },
                {
                    "sent": "There are slight differences in the regularization permit.",
                    "label": 0
                },
                {
                    "sent": "You may tend to think about the inverse covariance rather than the covariance, but.",
                    "label": 0
                },
                {
                    "sent": "Certainly the high level.",
                    "label": 0
                },
                {
                    "sent": "These kind of ideas are rather similar.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "One thing that you can think about happening in the we had that diagram with the latent theater up above.",
                    "label": 0
                },
                {
                    "sent": "Really in some sense we could integrate that out to introduce correlations between these different FS, right?",
                    "label": 0
                },
                {
                    "sent": "That's essentially what this structure that we're using.",
                    "label": 0
                },
                {
                    "sent": "Specifying.",
                    "label": 0
                },
                {
                    "sent": "Into task.",
                    "label": 0
                },
                {
                    "sent": "Correlations are actually doing cross.",
                    "label": 0
                },
                {
                    "sent": "Another way that we might.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think about.",
                    "label": 0
                },
                {
                    "sent": "Getting different into task similarities and this is a motivating example, is essentially by.",
                    "label": 0
                },
                {
                    "sent": "You know, we might think that somehow the second task is kind of a translated version of the first task that we have first task is this red curve and somehow the second task is a shifted version of that.",
                    "label": 0
                },
                {
                    "sent": "Of course, perhaps potentially in multi dimensions.",
                    "label": 0
                },
                {
                    "sent": "And this kind of idea, for example, was talked about by Ben David and Chill in 2003.",
                    "label": 0
                },
                {
                    "sent": "So of course, we just think about this in the Gaussian process framework.",
                    "label": 0
                },
                {
                    "sent": "If we essentially draw one function F1 and then kind of shift it to get F2, then basically this correlation between F1 X 2X prime basically can be rewritten like this costs X. F2 is just a shifted version of F1 shifted by this vector A and so we get this covariance.",
                    "label": 0
                },
                {
                    "sent": "So basically will see that there are covariances arise between the task and cross covariances.",
                    "label": 0
                },
                {
                    "sent": "Due to the.",
                    "label": 0
                },
                {
                    "sent": "This shifting that goes on this is the kind of motivational example, but one can actually be.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "We can generalize this.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's some underlying latent function G. We take some kernel, which we convolve it with reduced FI.",
                    "label": 0
                },
                {
                    "sent": "And we can imagine having a bunch of different.",
                    "label": 0
                },
                {
                    "sent": "Kernels here, so for example, this Delta function that we've just seen we're H. Here is actually just the Delta function.",
                    "label": 0
                },
                {
                    "sent": "This would give us the example.",
                    "label": 0
                },
                {
                    "sent": "We just see where we can have kind of generic kernels here.",
                    "label": 0
                },
                {
                    "sent": "So and these kind of ideas we talked about by number of different people.",
                    "label": 0
                },
                {
                    "sent": "And one nice generalization of this is actually to have a number of different latent processes.",
                    "label": 0
                },
                {
                    "sent": "Have these convolutions and then basically sum them up.",
                    "label": 0
                },
                {
                    "sent": "So we take the art process involve it.",
                    "label": 0
                },
                {
                    "sent": "There's some kernel for the ice observation, and then some of these things will end up with some basic set of.",
                    "label": 0
                },
                {
                    "sent": "Convolutions that give rise to the underlying to these observable FIS so we can think about these F's being generated from these underlying latent geez by this kind of operation and in fact we've already seen the example of this kind of thing already and the only difference was that here basically this HI are the Delta function on submission at X = X prime.",
                    "label": 0
                },
                {
                    "sent": "So basically just.",
                    "label": 0
                },
                {
                    "sent": "Copying the function value at X, primed to here and maybe with some weight so that basically that idea linearly combining those those functions with some different weights is exactly what we've got here.",
                    "label": 0
                },
                {
                    "sent": "But now we generalized it to introduce cut.",
                    "label": 0
                },
                {
                    "sent": "Convolutional aspects as well, so it's another way of actually getting at the kind of cross covariances.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One other idea that we might have for this kind of task.",
                    "label": 0
                },
                {
                    "sent": "And this also is something that can be traced back.",
                    "label": 1
                },
                {
                    "sent": "Some way is the idea that well.",
                    "label": 0
                },
                {
                    "sent": "And here we actually have something very unusual.",
                    "label": 0
                },
                {
                    "sent": "For Maps, we have a neural network.",
                    "label": 0
                },
                {
                    "sent": "Maybe the younger people have almost never seen such a thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's going on?",
                    "label": 0
                },
                {
                    "sent": "We have some inputs.",
                    "label": 0
                },
                {
                    "sent": "We transform this in some number of hidden layers and then here there are multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have N outputs.",
                    "label": 0
                },
                {
                    "sent": "Basically, the idea is that we have some kind of feature extraction we can think about this hidden layer having done feature extraction on the inputs and then based on those features, these outputs are basically combining them in different amounts.",
                    "label": 1
                },
                {
                    "sent": "So similar to that kind of idea of these.",
                    "label": 0
                },
                {
                    "sent": "These are combining those latent functions, but one thing we might want to get is really the idea that you might want to kind of optimize.",
                    "label": 0
                },
                {
                    "sent": "Want to tune this transformation.",
                    "label": 0
                },
                {
                    "sent": "Ull stuff here in order to actually extract features that are relevant to the task.",
                    "label": 0
                },
                {
                    "sent": "And this again is actually some of the ideas that you know for example in.",
                    "label": 0
                },
                {
                    "sent": "Our so called deep learning we see essentially the idea is to do some feature extraction.",
                    "label": 0
                },
                {
                    "sent": "Of course in this case this is a supervised feature extraction remote where motivating it by the fact we want to actually extract features such that they're good, helping us predict the outputs.",
                    "label": 1
                },
                {
                    "sent": "So that's the kind of idea behind this, and this example is actually taken from 2003 worker back in Huskies where they actually did this thing in neural network setup.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing that in fact so.",
                    "label": 0
                },
                {
                    "sent": "Let me just go back to this slide if we think about having some nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Transformation here or the input?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we run a Gaussian process based on that on these on these?",
                    "label": 0
                },
                {
                    "sent": "Values we obtain here.",
                    "label": 0
                },
                {
                    "sent": "We think of them as inputs to the Gaussian process, and we have the function values as well.",
                    "label": 0
                },
                {
                    "sent": "Basically what we've got is some nonlinear transformation of the input into these into these into a new space, and then we actually run the Gaussian process, for example on that new space.",
                    "label": 0
                },
                {
                    "sent": "And that's the kind of idea, and so we can really.",
                    "label": 0
                },
                {
                    "sent": "What you might want to do is to actually be cost.",
                    "label": 0
                },
                {
                    "sent": "These tasks are sharing this hidden representation.",
                    "label": 0
                },
                {
                    "sent": "That means that we really need to optimize the hyperparameters, which are essentially now the parameters in this transformation jointly across all tasks and that actually.",
                    "label": 0
                },
                {
                    "sent": "It's fairly easy to do if we assume that these tasks are conditionally independent given the.",
                    "label": 0
                },
                {
                    "sent": "The transformation here, then basically what's happening is that marginal likelihood that I wrote down earlier on just for one task, we just add that up over different tasks.",
                    "label": 0
                },
                {
                    "sent": "But we remember that we're sharing the parameters of this.",
                    "label": 0
                },
                {
                    "sent": "Processing across the different tasks.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the idea.",
                    "label": 0
                },
                {
                    "sent": "And then again, one can find.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some examples in the Gaussian process literature during this.",
                    "label": 0
                },
                {
                    "sent": "In fact, men can pick our dinner in the paper that I think was rejected from NIPS.",
                    "label": 0
                },
                {
                    "sent": "They published on their website actually.",
                    "label": 0
                },
                {
                    "sent": "Pointed out that basically if we have multiple tasks, then we actually can jointly optimize over these hyperparameters and we want to do that so as to extract these features.",
                    "label": 1
                },
                {
                    "sent": "So there are, you know, there can be fairly simple examples of feature extraction.",
                    "label": 0
                },
                {
                    "sent": "I've drawn there a fairly complex example with nonlinear transformations, but basically we could also have something simpler with, so just linear transformations of the input into some, maybe some reduced space, and that pregnant or something I worked on with my student Francesco Vivarelli back in that.",
                    "label": 1
                },
                {
                    "sent": "1999 there are other extensions of this kind of idea where there kind of some similarities going on, but maybe some allowing some flexibility.",
                    "label": 0
                },
                {
                    "sent": "So not all processors are exactly the same kernel that somehow related kernel, so of course you know kind of hierarchical modeling framework.",
                    "label": 1
                },
                {
                    "sent": "You can set up these kind of things.",
                    "label": 0
                },
                {
                    "sent": "And there are some quite fun things that you could do which you could basically imagine.",
                    "label": 0
                },
                {
                    "sent": "Thinking of the different tasks having different spaces, but kind of having a common hidden representation which we could use, so there's some fun things you could try to do.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Switching gears a little bit if we come back to the intrinsic correlation model.",
                    "label": 0
                },
                {
                    "sent": "We can ask well how much can a secondary task help us.",
                    "label": 0
                },
                {
                    "sent": "So let's consider the simplest.",
                    "label": 0
                },
                {
                    "sent": "Say that we can have this insertions acknowledged work done by my student coming Chai actually was presented in the main conference just earlier in the week.",
                    "label": 0
                },
                {
                    "sent": "We have a primary task T and the secondary task S and basically in that situation this KF this matrix of correlations will reduce to something that has ones on the diagonal and some correlation rho in the off diagonal.",
                    "label": 0
                },
                {
                    "sent": "Position.",
                    "label": 0
                },
                {
                    "sent": "So that that basically defines the correlation between the tasks and then the other thing that we can have is basically the amount of data that we have in the primary task and in the secondary task.",
                    "label": 1
                },
                {
                    "sent": "So here I'm going to assume that \u03c0 S is the amount of data in the second group task.",
                    "label": 0
                },
                {
                    "sent": "OK. Then basically what we can do for given task if we have given X locations for the primary and the secondary task, we can actually compute the generalization error in this.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I actually find it quite annoying to hear whispering.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We can have these forgiven X locations in the primary and secondary secondary data and some correlation.",
                    "label": 0
                },
                {
                    "sent": "We can workout essentially the predicted variance, which under assuming that the model is correct, we actually that's the the generalization error.",
                    "label": 0
                },
                {
                    "sent": "What we then want to do, of course, is the average over that over the density.",
                    "label": 1
                },
                {
                    "sent": "To get some average generalization error.",
                    "label": 0
                },
                {
                    "sent": "But this depends on the particular locations of XMS.",
                    "label": 0
                },
                {
                    "sent": "The kind of quotes that we might be interested in in terms of more theoretical characterizations is the learning curve.",
                    "label": 1
                },
                {
                    "sent": "To do that, we really want to average over the possible locations of xtian access.",
                    "label": 0
                },
                {
                    "sent": "That's basically what we like to get hold of is to understand how much and this is going to be a function in our terms of row and pious.",
                    "label": 0
                },
                {
                    "sent": "So any is the number of data points of which \u03c0 S times enter in the secondary task and 1 -- \u03c0 S times N in the primary task is correlation rho?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, I'm only considering here 2 tasks.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, it's just a two by two matrix and row is the the off diagonal entry in that, yeah?",
                    "label": 0
                },
                {
                    "sent": "XPS.",
                    "label": 0
                },
                {
                    "sent": "So that is.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this kind of setup, what we're imagining is that we draw some locations.",
                    "label": 1
                },
                {
                    "sent": "For the primary task and the secondary task from some underlying dense DP of X. OK, so that's the game.",
                    "label": 0
                },
                {
                    "sent": "It's kind of theoretical current calculation.",
                    "label": 0
                },
                {
                    "sent": "We play that game.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we're drawing the ex locations from identity.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, thanks and summary what we really want to understand is how much is it going to help us to have this secondary task right?",
                    "label": 0
                },
                {
                    "sent": "We've got some data on the second detail.",
                    "label": 0
                },
                {
                    "sent": "Is that gonna actually be a help?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "There's some fairly complicated results in camping thesis, but one fairly simple result.",
                    "label": 0
                },
                {
                    "sent": "OK, we might have some.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We have a lower bound on the generalization error with covariance row.",
                    "label": 1
                },
                {
                    "sent": "Basically, here's the case where we have zero correlation.",
                    "label": 0
                },
                {
                    "sent": "So basically here we actually only have N * 1 -- \u03c0 S. These are the observations on the primary task and really the secondary task is telling us nothing here.",
                    "label": 0
                },
                {
                    "sent": "And as we'd expect, basically.",
                    "label": 0
                },
                {
                    "sent": "As we increase row.",
                    "label": 0
                },
                {
                    "sent": "Basically, what's going to happen is that this the average generalization is forgiven N or pull down because we're getting information from the secondary task on the primary task, and it also it depends on on \u03c0 S as well.",
                    "label": 0
                },
                {
                    "sent": "Basically, how much of the data is there.",
                    "label": 0
                },
                {
                    "sent": "So fairly clean result you can get out of this and actually there the.",
                    "label": 0
                },
                {
                    "sent": "Underlying relationship inequality that actually gives rise.",
                    "label": 0
                },
                {
                    "sent": "This is actually is actually quite tight and so coming is done.",
                    "label": 1
                },
                {
                    "sent": "Experiments on both 1D problems and on actually, uh, which in some of them to toy, but also in a rather more interesting problem.",
                    "label": 0
                },
                {
                    "sent": "So the cycle stator for.",
                    "label": 0
                },
                {
                    "sent": "Robot inverse dynamics.",
                    "label": 0
                },
                {
                    "sent": "Is a kind of 21D data and there we have the distribution of X.",
                    "label": 0
                },
                {
                    "sent": "Is that somehow natural for this task?",
                    "label": 0
                },
                {
                    "sent": "He then created some Gaussian process is primary and secondary tasks using that kind of data and then?",
                    "label": 0
                },
                {
                    "sent": "Tested the effectiveness of his.",
                    "label": 0
                },
                {
                    "sent": "For this kind of relationship there and actually found also performed quite well there.",
                    "label": 0
                },
                {
                    "sent": "So in other words, this kind of.",
                    "label": 0
                },
                {
                    "sent": "Information with this kind of characterization is actually quite accurate about what's going on.",
                    "label": 0
                },
                {
                    "sent": "So that was the kind of theoretical bit.",
                    "label": 0
                },
                {
                    "sent": "Let me now.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And perhaps I can just.",
                    "label": 0
                },
                {
                    "sent": "In terms of that, let me just say that.",
                    "label": 0
                },
                {
                    "sent": "I think that we've talked about various.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kinds of setup of how to induce correlations between these tasks, and I think it's very interesting to ask if there up.",
                    "label": 0
                },
                {
                    "sent": "The other kinds of technique and other kinds of structures that we can actually exploit here.",
                    "label": 0
                },
                {
                    "sent": "OK, so then to relate this to robot inverse dynamics.",
                    "label": 1
                },
                {
                    "sent": "We might have some.",
                    "label": 0
                },
                {
                    "sent": "Robot with a number of joint angles Q and basically what we want to do is we want to in this setup.",
                    "label": 1
                },
                {
                    "sent": "You want to specify a trajectory, so we want to specify a sequence of cues.",
                    "label": 0
                },
                {
                    "sent": "And then basically figure out how to predict the talks that we need to drive that.",
                    "label": 0
                },
                {
                    "sent": "So actually we want to specify queues, Q dots, the angular velocities and QDC lots in angular acceleration.",
                    "label": 0
                },
                {
                    "sent": "Given that we want to actually specify the talks we need to do that.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of control problem.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, you could say, well, you know this is just physics, why don't you just do the right thing?",
                    "label": 0
                },
                {
                    "sent": "OK, there are a few reasons why that's problematic.",
                    "label": 0
                },
                {
                    "sent": "OK, so one is that yes, there is some kind of physics involving the kinetics, the potentials and maybe some discus frictions and so on.",
                    "label": 0
                },
                {
                    "sent": "The other thing, of course, is it's really difficult to actually get get hold of these parameters.",
                    "label": 0
                },
                {
                    "sent": "The other thing is also that you know.",
                    "label": 0
                },
                {
                    "sent": "The frictions there are joint elasticity.",
                    "label": 1
                },
                {
                    "sent": "This thing may not be the full on the full story and even.",
                    "label": 0
                },
                {
                    "sent": "And it's particularly the case that we might have, you know, you might change the circumstances under which were operating.",
                    "label": 0
                },
                {
                    "sent": "For example, have different loads on the end of the end effector, and this changes all these equations.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not entirely.",
                    "label": 1
                },
                {
                    "sent": "It's not like we this is just a irrelevant problem.",
                    "label": 0
                },
                {
                    "sent": "We actually really do need to have these kind of inverse dynamics problems.",
                    "label": 0
                },
                {
                    "sent": "Particularly in things like compliant, lightweight, humanoid robots.",
                    "label": 1
                },
                {
                    "sent": "So I'd say particularly that.",
                    "label": 0
                },
                {
                    "sent": "This is the work where my colleague Sir Stephen Clanking so two.",
                    "label": 0
                },
                {
                    "sent": "Vijay Kumar the robot assists and that's what this is.",
                    "label": 0
                },
                {
                    "sent": "They know their input on this and understanding this is very important.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, there's just saying the functions change if we have different loads on the end that that that happens and.",
                    "label": 1
                },
                {
                    "sent": "Bad news is of course yes.",
                    "label": 0
                },
                {
                    "sent": "We need a different inverse dynamics model for different loads, and the other thing is of course if I pick up a heavy load, wave it around it may actually.",
                    "label": 1
                },
                {
                    "sent": "Maybe I'll have different kinds of trajectory's different kinds of this X SpaceX member is QQQ double dot.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "But there is also some good news and basically certainly in the case we just varying the mass on the end effector.",
                    "label": 1
                },
                {
                    "sent": "Then changes are basically happening there.",
                    "label": 0
                },
                {
                    "sent": "And this is also a very nice result that OK, the talks are basically comprised of a set of nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Functions of X these yiz.",
                    "label": 0
                },
                {
                    "sent": "But they're also depending linearly on these parameters, Pi OK?",
                    "label": 1
                },
                {
                    "sent": "And these characterize the mass and the moments of inertia and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, So what of the object on the load?",
                    "label": 0
                },
                {
                    "sent": "So that should start to look familiar, engaged and start to say OK.",
                    "label": 0
                },
                {
                    "sent": "I've got some functions here and I've got different linear combinations of those functions.",
                    "label": 0
                },
                {
                    "sent": "Need we saw something earlier about that so just a little bit of notation.",
                    "label": 0
                },
                {
                    "sent": "Of course that we can do some.",
                    "label": 0
                },
                {
                    "sent": "Essentially there's some freedom here about if we re parameterized in here so we might think of these latent functions as being called and said, and these weights for the different tasks.",
                    "label": 0
                },
                {
                    "sent": "Being called an OK.",
                    "label": 0
                },
                {
                    "sent": "So essentially that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set up that we had four applies again.",
                    "label": 0
                },
                {
                    "sent": "We've got these different latent.",
                    "label": 0
                },
                {
                    "sent": "Functions here and actually.",
                    "label": 0
                },
                {
                    "sent": "This will apply.",
                    "label": 0
                },
                {
                    "sent": "Actually, this applies separately for all different joints.",
                    "label": 0
                },
                {
                    "sent": "That's why there's a plate outside here.",
                    "label": 0
                },
                {
                    "sent": "OK, but then here we have a plate over tasks M. So what we're saying is that for this joint, the ice joint we draw, we draw these latent functions and have linear combinations of them and those linear combinations of those rows.",
                    "label": 0
                },
                {
                    "sent": "Exactly what we saw before.",
                    "label": 0
                },
                {
                    "sent": "So we again we have that Inter task similarity derived from this this.",
                    "label": 0
                },
                {
                    "sent": "From this linear combination of these latent functions.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, that's what I mean by different tasks.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the clarification.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. We we need to specify actually do experiments.",
                    "label": 0
                },
                {
                    "sent": "We need to express it fast.",
                    "label": 0
                },
                {
                    "sent": "Specify our prior Care XX, primed, and basically we're going to use a fairly standard thing, something like a bias, which just basically as a prior variance and shifting things up and down linear in the.",
                    "label": 0
                },
                {
                    "sent": "So look at the linear kernel here.",
                    "label": 0
                },
                {
                    "sent": "Squared exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "On this ARD is automatic relevance determination, so it means basically there's different dimensions could have different weights or different importances, and also this Coulomb friction term which depends on the sign of the angular velocity Q dot.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of kernel and it was actually important for this data to use this kernel function to.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are we getting data from we're actually using robot ball using realistic simulator?",
                    "label": 1
                },
                {
                    "sent": "The Puma 560 robot arm?",
                    "label": 1
                },
                {
                    "sent": "And there's also a bunch of setups that we can have here.",
                    "label": 0
                },
                {
                    "sent": "We can drive this arm along different trajectories using figure of eight trajectories at different speeds, so therefore different paths.",
                    "label": 1
                },
                {
                    "sent": "Different locations the origin of the workspace and we've got different speeds so that we can drive this thing along and so that basic and we've also got different loads, so we've got different masses in range .2 to three KG various shapes and sizes.",
                    "label": 0
                },
                {
                    "sent": "So basically we can collect data by putting these different masses on, driving them along the different trajectories, and so on.",
                    "label": 0
                },
                {
                    "sent": "What how we actually did that is that there is one sort of reference trajectory on which.",
                    "label": 0
                },
                {
                    "sent": "Let me just get them.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Invitation OK yeah I will.",
                    "label": 0
                },
                {
                    "sent": "I'll refer to these as paths and the combination of path and speed is a trajectory so OK there full pads and full speed giving us 16 different trajectories overall.",
                    "label": 0
                },
                {
                    "sent": "So basically this comes out of reference trajectory.",
                    "label": 1
                },
                {
                    "sent": "So all we have this load and for all different loans we drive along some common trajectory and then we'll also use unique trajectory's one for each of the different.",
                    "label": 0
                },
                {
                    "sent": "Modes.",
                    "label": 0
                },
                {
                    "sent": "And then what's happening is that we actually have one trajectory in which we have no data, and that's always novel.",
                    "label": 1
                },
                {
                    "sent": "And then really two ways we might test what's going on and how much information we get sharing across tasks.",
                    "label": 0
                },
                {
                    "sent": "And so the whole goal of this is really that we would hope that by having these different trajectories.",
                    "label": 0
                },
                {
                    "sent": "And these different loads which are.",
                    "label": 0
                },
                {
                    "sent": "Data from those should actually help us to predict better to get the inverse dynamics better for a given task.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the that's the goal, how he tested that is in two ways.",
                    "label": 0
                },
                {
                    "sent": "One is a kind of interpolating task.",
                    "label": 0
                },
                {
                    "sent": "So you saw here that there's one reference trajectory and 14 unique trajectories.",
                    "label": 1
                },
                {
                    "sent": "OK, and then what?",
                    "label": 0
                },
                {
                    "sent": "What we can do is basically we gotta do a kind of interpolation where we test only on that.",
                    "label": 0
                },
                {
                    "sent": "Comment on the trajectory is where we have actually training data.",
                    "label": 0
                },
                {
                    "sent": "Or we could do a kind of extrapolate re thing where we actually look at all different trajectories.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are the two setups.",
                    "label": 0
                },
                {
                    "sent": "And OK, what are the different ways we can do this?",
                    "label": 0
                },
                {
                    "sent": "Or one way is we can just say OK, I've got a single load, I'll just train a Gaussian process predictor for doing this.",
                    "label": 0
                },
                {
                    "sent": "To prediction.",
                    "label": 0
                },
                {
                    "sent": "Do inverse dynamics.",
                    "label": 0
                },
                {
                    "sent": "Another thing we could do is we could say.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we got a bunch of independent Gaussian processes, but they can actually share hyperparameters so.",
                    "label": 0
                },
                {
                    "sent": "In here there were a bunch of hyperparameters involved in this covariance function, and we want to show.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are those.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do kind of pull Gaussian process where what we actually have is basically to put all the data, just stick it all in to say this is all data from the same task.",
                    "label": 0
                },
                {
                    "sent": "Let's just train on that.",
                    "label": 0
                },
                {
                    "sent": "And then the thing that we hope will do better.",
                    "label": 0
                },
                {
                    "sent": "Is there multitask Gaussian process where we have basically this kind of structure we talked about?",
                    "label": 0
                },
                {
                    "sent": "Here is KFKX decomposition.",
                    "label": 0
                },
                {
                    "sent": "It is worth saying once a technical note is that we actually of course can determine we can have some choice over the rank of KF.",
                    "label": 0
                },
                {
                    "sent": "OK, and we actually determine that using a BICS criterion.",
                    "label": 0
                },
                {
                    "sent": "So we tested different ranks and these be icy.",
                    "label": 0
                },
                {
                    "sent": "OK, so he.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results basically OK. What's going on?",
                    "label": 0
                },
                {
                    "sent": "First of all, this is for a particular joint joint five.",
                    "label": 0
                },
                {
                    "sent": "They're actually 6 joints.",
                    "label": 0
                },
                {
                    "sent": "This is the interpolation task.",
                    "label": 0
                },
                {
                    "sent": "This is an extrapolation task.",
                    "label": 0
                },
                {
                    "sent": "This is the size of the amount of training data we have.",
                    "label": 1
                },
                {
                    "sent": "That's actually the total number of points across all tasks, and this is the mean average normalized mean squared error.",
                    "label": 1
                },
                {
                    "sent": "So basically it's quite a lot of averaging that can go on here because we have a number of different tasks over which you want to to actually average.",
                    "label": 0
                },
                {
                    "sent": "So what we can see.",
                    "label": 0
                },
                {
                    "sent": "So this is the color coding for.",
                    "label": 0
                },
                {
                    "sent": "Single GP, The independent GP.",
                    "label": 0
                },
                {
                    "sent": "The pool GP and the multi tenant GP basically see this kind of structure so the multi task one is the line black.",
                    "label": 1
                },
                {
                    "sent": "It's doing better here but maybe there are some error bars.",
                    "label": 0
                },
                {
                    "sent": "Here is perhaps a bit hard to see.",
                    "label": 0
                },
                {
                    "sent": "How well it's doing?",
                    "label": 0
                },
                {
                    "sent": "We'll see that on the next figure.",
                    "label": 0
                },
                {
                    "sent": "Basically, you're seeing input, so here on this joint.",
                    "label": 0
                },
                {
                    "sent": "Basically the independent Gaussian process is the kind of best competitor here.",
                    "label": 0
                },
                {
                    "sent": "And similarly on this task.",
                    "label": 0
                },
                {
                    "sent": "Basically these are just kind of nominal values that would otherwise go off the scale.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Just to look at so obviously one.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do, yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Well, OK, OK, this is just a nominal level.",
                    "label": 0
                },
                {
                    "sent": "OK, these are actually really off somewhere up here.",
                    "label": 0
                },
                {
                    "sent": "Let me just plotting them here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yes, yeah I would agree that probably adding more data should help.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, that's a fair question, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but there's someone out there somewhere off the scale here anyway, OK.",
                    "label": 0
                },
                {
                    "sent": "So really, just to get a bit more insight.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "We can, of course do paired comparisons, 'cause when we do the experiments you might, you know, choose some sample size and will be actually draw different data.",
                    "label": 0
                },
                {
                    "sent": "Points.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Will you repair that across the different experiment so we can look at the differences of the average MSE and basically what you're seeing here is so actually there are five replications, so we've plotted all those data.",
                    "label": 0
                },
                {
                    "sent": "Really there isn't much difference on the interpolatory task between these model, but on the extrapolate re task.",
                    "label": 0
                },
                {
                    "sent": "So we're actually comparing here.",
                    "label": 0
                },
                {
                    "sent": "The paired difference between the best.",
                    "label": 1
                },
                {
                    "sent": "Out of the pool, the Independent or the single, and this is the multi task.",
                    "label": 0
                },
                {
                    "sent": "So this difference here actually shows.",
                    "label": 0
                },
                {
                    "sent": "Basically there is a win on the extrapolator task here.",
                    "label": 0
                },
                {
                    "sent": "Almost done just.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically what I've tried to tell you about is the multitask formulation of Gaussian processes and the kind of cokriging framework.",
                    "label": 1
                },
                {
                    "sent": "And we've seen how we can try to construct things that have cross covariances and how to use those.",
                    "label": 1
                },
                {
                    "sent": "And this also, hopefully I've convinced you, at least in the multi context, inverse dynamics problems are quite natural fit that actually remember is actually to the ICM.",
                    "label": 0
                },
                {
                    "sent": "The intrinsic correlation model that simple one factorizes between task similarity and X space similarity.",
                    "label": 1
                },
                {
                    "sent": "And I think one of the most interesting questions really is to say what kinds of.",
                    "label": 0
                },
                {
                    "sent": "Structure are there that there might be between between tasks, either in the GP framework.",
                    "label": 0
                },
                {
                    "sent": "Maybe they're different kinds of covariance functions, or cross covariance functions, normal use.",
                    "label": 0
                },
                {
                    "sent": "Or maybe there are things that you know.",
                    "label": 0
                },
                {
                    "sent": "Really good models to do multi task learning, but actually it's very hard to get those into GP framework.",
                    "label": 0
                },
                {
                    "sent": "I'd like I'd be interested to hear if there are thoughts on that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Short motorcycle questions.",
                    "label": 0
                },
                {
                    "sent": "Who is noise free or is there?",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the kind of place where the student advisor should come clean and say kambing really knows the details of that.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to guess my guess.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, I'm I'm not going to assert that it's one thing or the other.",
                    "label": 0
                },
                {
                    "sent": "My guess is that would be noise in there.",
                    "label": 0
                },
                {
                    "sent": "If anything, this noise will have to be the same in this class.",
                    "label": 0
                },
                {
                    "sent": "In no, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "It's certainly.",
                    "label": 0
                },
                {
                    "sent": "We can certainly set this up to have slight noise.",
                    "label": 0
                },
                {
                    "sent": "Variances are different on different tasks.",
                    "label": 0
                },
                {
                    "sent": "There's no difficulty in doing that, and you can kind of put that into the hyperparameter optimization.",
                    "label": 0
                },
                {
                    "sent": "That would certainly be a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "Very good thing.",
                    "label": 0
                },
                {
                    "sent": "I ready when I say.",
                    "label": 0
                },
                {
                    "sent": "Classification label regression.",
                    "label": 0
                },
                {
                    "sent": "OK, certainly we could imagine having you know there are correlated latent process is and we can have kind of different readout mechanisms, almost latent processes, as you'll be familiar currently easiest way to do GP stuff is is with the regression situation.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that that's actually a nice point that we could use essentially any sort of generalized linear model readout for those different types.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can think yeah.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "So you say you you can talk about the essential question relation between the middle part.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "That refers to the role.",
                    "label": 0
                },
                {
                    "sent": "There is no.",
                    "label": 0
                },
                {
                    "sent": "Maybe I missed it.",
                    "label": 0
                },
                {
                    "sent": "How do you decide?",
                    "label": 0
                },
                {
                    "sent": "Hopefully OK, so basically we can just optimize over that matrix.",
                    "label": 0
                },
                {
                    "sent": "And it's OK, so perhaps I must have avoided saying that.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "There are various ways of doing it.",
                    "label": 0
                },
                {
                    "sent": "You can actually do kind of VM things, or you can just do kind of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Things on on echo.",
                    "label": 0
                },
                {
                    "sent": "So we represent their matrix bytes to ASCII factorization and then optimize over that.",
                    "label": 0
                },
                {
                    "sent": "So yeah, thank you for clarifying that, yeah.",
                    "label": 0
                },
                {
                    "sent": "Do gradient ascent.",
                    "label": 0
                },
                {
                    "sent": "Weather Christian server.",
                    "label": 0
                },
                {
                    "sent": "Motion distributions there are these conspiratorial results, like, for example, the Goshen is either knowledge about the 2nd order.",
                    "label": 0
                },
                {
                    "sent": "This is the distribution implies there so.",
                    "label": 0
                },
                {
                    "sent": "Some similar kinds of results for Goshen Process or in the more general monitors.",
                    "label": 0
                },
                {
                    "sent": "Motion process is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we just simplify to their sort of 1 task single task Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "I imagine that a similar result would specify go through if you ask about this stochastic process that has.",
                    "label": 0
                },
                {
                    "sent": "Say, for example, you know some variance or some.",
                    "label": 0
                },
                {
                    "sent": "Mean functions and covariance function.",
                    "label": 0
                },
                {
                    "sent": "And you say what's the stochastic process that has maximum entropy I?",
                    "label": 0
                },
                {
                    "sent": "Imagine that the Gaussian process the answer that, but they cannot hesitate to sort of assert that too strongly, because they often can be mathematical niceties that one has to take care of.",
                    "label": 0
                },
                {
                    "sent": "And I I'm not familiar enough with that.",
                    "label": 0
                },
                {
                    "sent": "The reason of course I'm asking is that you know then.",
                    "label": 0
                },
                {
                    "sent": "Single golfers, you can save it if you don't know anything other than the 2nd order statistics.",
                    "label": 0
                },
                {
                    "sent": "That working is is with some some sense the most non informative way to specify a moral.",
                    "label": 0
                },
                {
                    "sent": "So then if there was some kind of result like this then maybe it would.",
                    "label": 0
                },
                {
                    "sent": "It would tell why grocery process will be the way to go other than of course these these nice abilities to to have analytical results.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's a.",
                    "label": 0
                },
                {
                    "sent": "That's a reasonable idea, although there's something that's kind of.",
                    "label": 0
                },
                {
                    "sent": "Thanks you slightly nervous about the Gaussian from the sort of robustness points of views right then.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian is notoriously non robust, right?",
                    "label": 0
                },
                {
                    "sent": "So it's it's a maximum entropy conditional on that meaning covariance structure of course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe maybe I shouldn't speculate any further, OK?",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering whether it would be possible to deal with this problem of music acts as a structured prediction problem.",
                    "label": 0
                },
                {
                    "sent": "Cowboy.",
                    "label": 0
                },
                {
                    "sent": "Presently we have one case.",
                    "label": 0
                },
                {
                    "sent": "Reply.",
                    "label": 0
                },
                {
                    "sent": "At outlook.",
                    "label": 0
                },
                {
                    "sent": "What's essentially what?",
                    "label": 0
                },
                {
                    "sent": "Essentially what we actually do have here, right?",
                    "label": 0
                },
                {
                    "sent": "You know that already, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is going to put more fires is the order would be structured output problem?",
                    "label": 0
                },
                {
                    "sent": "OK, fair question.",
                    "label": 0
                },
                {
                    "sent": "I think I'll have to remember exactly how the Tasker L stuff works well enough in order to actually to answer that question.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's just wondering if it's good idea to do this now, or whether we should take this offline.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}