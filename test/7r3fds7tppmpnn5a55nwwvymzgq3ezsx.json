{
    "id": "7r3fds7tppmpnn5a55nwwvymzgq3ezsx",
    "title": "Sparse-Coded Features for Image Retrieval",
    "info": {
        "author": [
            "Tiezheng Ge, University of Science and Technology of China"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_ge_image_retrieval/",
    "segmentation": [
        [
            "Hi everyone, I'm teaching from University of Science and Technology of China and this paper title is sparse coded feature for image retrieval and the coauthors, our Chief Uncle and Jensen from Microsoft."
        ],
        [
            "In this paper focused on the topic of large scale image search, the topical topical target to building a system, the system help restore database which contains large number of images.",
            "When some user uploads a query image and the system shows select images which represent the same object with the input one and then return to the user."
        ],
        [
            "There are many methods proposed for this method for this topic.",
            "A large group of these methods rely on local feature.",
            "In general, their workflows are giving an image in the system first, extract hundreds of local features such as shift and then transfer local features into image representation in order to gather representation.",
            "One popular way is a bag of virtual world.",
            "It first quantized local features.",
            "Into which words and then applies the inverted file system.",
            "An alternative method is the so called future aggregation.",
            "It tries to merge or local feature which belong to one image into a Ninja level Ninja level representation representation vector.",
            "Such vector can be further quantized using standard vector conversation.",
            "They represent representative work very D and facial kernel."
        ],
        [
            "On our work follows the 2nd way we find that these two representative aggregator aggregators, vid and official kernel can be reformulated into two speak two steps step one coding in Tomb map, each vector X into a high dimensional vector GX.",
            "Step 2 pulling into March or old map vector belonging to 1MG into a representation vector.",
            "Big GX.",
            "Since the leading says search access, since leading search search distant uniformly follow such two step strategy, we are encouraged to try more coding method under the coding pulling formulation.",
            "Therefore we turn to sparse coding."
        ],
        [
            "Sparse coding is a widely accepted technique in many areas of computer vision, such as face recognition, image classification and image burn.",
            "For image classification is proposed to use sparse coding, adding spatial pyramid pooling.",
            "We apply it to image search.",
            "Is the standard two step structure encoding step where you stand are L1 regularised, pulling one regularised.",
            "Coding improving step.",
            "We choose simple Max pooling."
        ],
        [
            "We know two differences between our image retrieval and image classification in sparse coding.",
            "First, the input local descriptors are computed from interest points instead of from instead of from dense sampling grids.",
            "Second, we don't use the visual spatial pyramid for their translations and rotations in image search task, while SPM is sensitive to them."
        ],
        [
            "And then we explore how or why sparse coding works for image search.",
            "Here let's define two terms.",
            "The first one is active."
        ],
        [
            "And let's take take a image for example.",
            "Assuming it has three features X, One X2 and X3, and their code vectors RGX.",
            "One checks two and GX3 like this.",
            "Then according to the rule of Max pooling their final reputation, Big GX is like this."
        ],
        [
            "You see X1 and X2 contributes to the final representation, but X Ray doesn't.",
            "In another world."
        ],
        [
            "The final reputation speak GX, depends solely on X1X2, but doesn't depend on X3.",
            "Then we see X, one X2 are active.",
            "An X Ray is inactive.",
            "Since the final representation only depends on the active state, we remove the inactive one."
        ],
        [
            "The next one is Coactive.",
            "If 2 features from 2 images are active and they happen to contribute to the same dimension, we say they are coactive.",
            "We go on with the example just law.",
            "X1 is active and it and it contributes to the third dimension."
        ],
        [
            "Then here comes another image along with this feature Y one if Y one is active and it still contributes to the third dimension.",
            "And then we say X1 and Y1R coactive.",
            "For illumination will."
        ],
        [
            "Link coactive pairs bireli."
        ],
        [
            "If two images are related, we hope they share many coactive pairs and these pairs are indeed true match pair.",
            "This is similar to that for Bag World.",
            "We hope the matched pair match points are quantized to the same thing."
        ],
        [
            "Then what cereal scenario?",
            "This is a real case of the two related images.",
            "Hello that is the active feature and the rail line links the coactive pair.",
            "For clarity, we only show a pair of the part of the coactive pairs.",
            "Our observations are first."
        ],
        [
            "Most descriptors are active only a few are removed 2nd."
        ],
        [
            "There are many coactive pairs.",
            "For most of them are indeed true matched ones."
        ],
        [
            "So our conclusion is for interest point based local features.",
            "Sparse coding works like a future mature feature matcher.",
            "It connects matched pairs."
        ],
        [
            "Here's another example, the same observation."
        ],
        [
            "Until now, we focus on the single feature one feature generator."
        ],
        [
            "And kind of local feature which is, uh, aggregated into one representative representation vector."
        ],
        [
            "The high damage the high dimensional vector is.",
            "Is quantized to the compact vector by PC.",
            "This pipeline is natural to extend to multiple features, for example."
        ],
        [
            "We extract local visual two and aggregate two vector 2."
        ],
        [
            "Then we calculated vector one and vector 2."
        ],
        [
            "And still applying PC upon the joint venture."
        ],
        [
            "The fellow compact vector contains information, or both feature one and feature 2.",
            "Note that the final compact vector has the same dimension with that for single feature.",
            "This is desirable for image search for it."
        ],
        [
            "And here is the online search efficiency."
        ],
        [
            "The next problem is to choose local features we explore.",
            "We explored different local feature detectors such as Harris Corner, Laplacian, Gaussian and different feature descriptors such as Swift and dancing.",
            "And we found that the best combination is harvest with Daisy and Laplacian got lost in Goshen with safety.",
            "We delete them as HD and LS."
        ],
        [
            "Since the feature combination is effective, we are encouraged to combine more local feature.",
            "We design a local local feature called Micro Feature.",
            "It's A kind of color feature and inspired by the work of bag colors.",
            "To extract micro feature we don't lease Tampa the local Patch.",
            "Each Patch is owned by an color Patch.",
            "We transfer it to LAB color space and use it 3 channel values as descriptor.",
            "So the descriptor is ambion by three dimension.",
            "We found this simple feature works pretty well for sparse coding framework."
        ],
        [
            "Since Michael Fisher is densely sampled, it's hard to build Point point correspondence like that for interest points.",
            "Then how does best coding work for micro feature?",
            "We still explore the active points.",
            "Here are four images along with their active micro features.",
            "The yellow dot are active features.",
            "We observed that in."
        ],
        [
            "Discriminative region in discriminative region.",
            "The active points distribute density while in smooth region."
        ],
        [
            "The active points are sparse.",
            "As most repeating patterns are removed.",
            "So our conclusion is, for densely sampled features, sparse coding works like a filter.",
            "It focuses on representative patterns."
        ],
        [
            "We test our method on 2 standard datasets Holidays and UK B.",
            "We first compare sparse coding with very D and facial kernel for fair comparison.",
            "For fair comparison we use single local feature.",
            "The highest rating the highest AC.",
            "We compare the performance in Origonal dimension and that after PC to 128 and 60 four dimension.",
            "We observe that sparse coding outperforms in most scenarios."
        ],
        [
            "Then we test the multiple feature combination.",
            "We report the result of using only single feature, combining double features and triple feature respectively.",
            "We observed that the performance the performance significantly increase with more features even after reducing tools that."
        ],
        [
            "Even after reducing to the same dimension, the multiple feature wins."
        ],
        [
            "We find this study the scalability, using Holidays, adding formatting images from Flickr.",
            "We test our performance of reality for single feature sparse coding for single feature double features and triple features, our observation is.",
            "Our observation is consistent with that instead of the Holidays.",
            "For single feet."
        ],
        [
            "Person, sparse coding or performance varied by three points."
        ],
        [
            "Using double and triple features the system."
        ],
        [
            "Maybe will greatly increase the increment is about sorting and 45 points respectively."
        ],
        [
            "To summarize.",
            "In this paper we describe a system for large scale image search.",
            "Our work is based on the framework of local feature aggregation.",
            "It applies sparse coding, Max pooling aggregator.",
            "Uses feature combination and proposes a kind of local local feature called Micro Feature.",
            "By combining these technicals, we reached a system that is promising and competitive."
        ],
        [
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, I'm teaching from University of Science and Technology of China and this paper title is sparse coded feature for image retrieval and the coauthors, our Chief Uncle and Jensen from Microsoft.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this paper focused on the topic of large scale image search, the topical topical target to building a system, the system help restore database which contains large number of images.",
                    "label": 0
                },
                {
                    "sent": "When some user uploads a query image and the system shows select images which represent the same object with the input one and then return to the user.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many methods proposed for this method for this topic.",
                    "label": 0
                },
                {
                    "sent": "A large group of these methods rely on local feature.",
                    "label": 1
                },
                {
                    "sent": "In general, their workflows are giving an image in the system first, extract hundreds of local features such as shift and then transfer local features into image representation in order to gather representation.",
                    "label": 0
                },
                {
                    "sent": "One popular way is a bag of virtual world.",
                    "label": 1
                },
                {
                    "sent": "It first quantized local features.",
                    "label": 1
                },
                {
                    "sent": "Into which words and then applies the inverted file system.",
                    "label": 0
                },
                {
                    "sent": "An alternative method is the so called future aggregation.",
                    "label": 0
                },
                {
                    "sent": "It tries to merge or local feature which belong to one image into a Ninja level Ninja level representation representation vector.",
                    "label": 0
                },
                {
                    "sent": "Such vector can be further quantized using standard vector conversation.",
                    "label": 0
                },
                {
                    "sent": "They represent representative work very D and facial kernel.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On our work follows the 2nd way we find that these two representative aggregator aggregators, vid and official kernel can be reformulated into two speak two steps step one coding in Tomb map, each vector X into a high dimensional vector GX.",
                    "label": 0
                },
                {
                    "sent": "Step 2 pulling into March or old map vector belonging to 1MG into a representation vector.",
                    "label": 1
                },
                {
                    "sent": "Big GX.",
                    "label": 0
                },
                {
                    "sent": "Since the leading says search access, since leading search search distant uniformly follow such two step strategy, we are encouraged to try more coding method under the coding pulling formulation.",
                    "label": 1
                },
                {
                    "sent": "Therefore we turn to sparse coding.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sparse coding is a widely accepted technique in many areas of computer vision, such as face recognition, image classification and image burn.",
                    "label": 0
                },
                {
                    "sent": "For image classification is proposed to use sparse coding, adding spatial pyramid pooling.",
                    "label": 1
                },
                {
                    "sent": "We apply it to image search.",
                    "label": 0
                },
                {
                    "sent": "Is the standard two step structure encoding step where you stand are L1 regularised, pulling one regularised.",
                    "label": 0
                },
                {
                    "sent": "Coding improving step.",
                    "label": 0
                },
                {
                    "sent": "We choose simple Max pooling.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We know two differences between our image retrieval and image classification in sparse coding.",
                    "label": 1
                },
                {
                    "sent": "First, the input local descriptors are computed from interest points instead of from instead of from dense sampling grids.",
                    "label": 1
                },
                {
                    "sent": "Second, we don't use the visual spatial pyramid for their translations and rotations in image search task, while SPM is sensitive to them.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we explore how or why sparse coding works for image search.",
                    "label": 1
                },
                {
                    "sent": "Here let's define two terms.",
                    "label": 0
                },
                {
                    "sent": "The first one is active.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's take take a image for example.",
                    "label": 0
                },
                {
                    "sent": "Assuming it has three features X, One X2 and X3, and their code vectors RGX.",
                    "label": 0
                },
                {
                    "sent": "One checks two and GX3 like this.",
                    "label": 0
                },
                {
                    "sent": "Then according to the rule of Max pooling their final reputation, Big GX is like this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see X1 and X2 contributes to the final representation, but X Ray doesn't.",
                    "label": 0
                },
                {
                    "sent": "In another world.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The final reputation speak GX, depends solely on X1X2, but doesn't depend on X3.",
                    "label": 1
                },
                {
                    "sent": "Then we see X, one X2 are active.",
                    "label": 0
                },
                {
                    "sent": "An X Ray is inactive.",
                    "label": 0
                },
                {
                    "sent": "Since the final representation only depends on the active state, we remove the inactive one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next one is Coactive.",
                    "label": 0
                },
                {
                    "sent": "If 2 features from 2 images are active and they happen to contribute to the same dimension, we say they are coactive.",
                    "label": 0
                },
                {
                    "sent": "We go on with the example just law.",
                    "label": 0
                },
                {
                    "sent": "X1 is active and it and it contributes to the third dimension.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then here comes another image along with this feature Y one if Y one is active and it still contributes to the third dimension.",
                    "label": 0
                },
                {
                    "sent": "And then we say X1 and Y1R coactive.",
                    "label": 0
                },
                {
                    "sent": "For illumination will.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Link coactive pairs bireli.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If two images are related, we hope they share many coactive pairs and these pairs are indeed true match pair.",
                    "label": 0
                },
                {
                    "sent": "This is similar to that for Bag World.",
                    "label": 0
                },
                {
                    "sent": "We hope the matched pair match points are quantized to the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then what cereal scenario?",
                    "label": 0
                },
                {
                    "sent": "This is a real case of the two related images.",
                    "label": 0
                },
                {
                    "sent": "Hello that is the active feature and the rail line links the coactive pair.",
                    "label": 0
                },
                {
                    "sent": "For clarity, we only show a pair of the part of the coactive pairs.",
                    "label": 0
                },
                {
                    "sent": "Our observations are first.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most descriptors are active only a few are removed 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are many coactive pairs.",
                    "label": 0
                },
                {
                    "sent": "For most of them are indeed true matched ones.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our conclusion is for interest point based local features.",
                    "label": 0
                },
                {
                    "sent": "Sparse coding works like a future mature feature matcher.",
                    "label": 1
                },
                {
                    "sent": "It connects matched pairs.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example, the same observation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until now, we focus on the single feature one feature generator.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And kind of local feature which is, uh, aggregated into one representative representation vector.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The high damage the high dimensional vector is.",
                    "label": 0
                },
                {
                    "sent": "Is quantized to the compact vector by PC.",
                    "label": 1
                },
                {
                    "sent": "This pipeline is natural to extend to multiple features, for example.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We extract local visual two and aggregate two vector 2.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we calculated vector one and vector 2.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And still applying PC upon the joint venture.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The fellow compact vector contains information, or both feature one and feature 2.",
                    "label": 1
                },
                {
                    "sent": "Note that the final compact vector has the same dimension with that for single feature.",
                    "label": 0
                },
                {
                    "sent": "This is desirable for image search for it.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the online search efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next problem is to choose local features we explore.",
                    "label": 0
                },
                {
                    "sent": "We explored different local feature detectors such as Harris Corner, Laplacian, Gaussian and different feature descriptors such as Swift and dancing.",
                    "label": 0
                },
                {
                    "sent": "And we found that the best combination is harvest with Daisy and Laplacian got lost in Goshen with safety.",
                    "label": 0
                },
                {
                    "sent": "We delete them as HD and LS.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since the feature combination is effective, we are encouraged to combine more local feature.",
                    "label": 0
                },
                {
                    "sent": "We design a local local feature called Micro Feature.",
                    "label": 0
                },
                {
                    "sent": "It's A kind of color feature and inspired by the work of bag colors.",
                    "label": 0
                },
                {
                    "sent": "To extract micro feature we don't lease Tampa the local Patch.",
                    "label": 0
                },
                {
                    "sent": "Each Patch is owned by an color Patch.",
                    "label": 1
                },
                {
                    "sent": "We transfer it to LAB color space and use it 3 channel values as descriptor.",
                    "label": 0
                },
                {
                    "sent": "So the descriptor is ambion by three dimension.",
                    "label": 0
                },
                {
                    "sent": "We found this simple feature works pretty well for sparse coding framework.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since Michael Fisher is densely sampled, it's hard to build Point point correspondence like that for interest points.",
                    "label": 0
                },
                {
                    "sent": "Then how does best coding work for micro feature?",
                    "label": 1
                },
                {
                    "sent": "We still explore the active points.",
                    "label": 1
                },
                {
                    "sent": "Here are four images along with their active micro features.",
                    "label": 0
                },
                {
                    "sent": "The yellow dot are active features.",
                    "label": 0
                },
                {
                    "sent": "We observed that in.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discriminative region in discriminative region.",
                    "label": 0
                },
                {
                    "sent": "The active points distribute density while in smooth region.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The active points are sparse.",
                    "label": 1
                },
                {
                    "sent": "As most repeating patterns are removed.",
                    "label": 0
                },
                {
                    "sent": "So our conclusion is, for densely sampled features, sparse coding works like a filter.",
                    "label": 0
                },
                {
                    "sent": "It focuses on representative patterns.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We test our method on 2 standard datasets Holidays and UK B.",
                    "label": 0
                },
                {
                    "sent": "We first compare sparse coding with very D and facial kernel for fair comparison.",
                    "label": 0
                },
                {
                    "sent": "For fair comparison we use single local feature.",
                    "label": 0
                },
                {
                    "sent": "The highest rating the highest AC.",
                    "label": 0
                },
                {
                    "sent": "We compare the performance in Origonal dimension and that after PC to 128 and 60 four dimension.",
                    "label": 0
                },
                {
                    "sent": "We observe that sparse coding outperforms in most scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we test the multiple feature combination.",
                    "label": 0
                },
                {
                    "sent": "We report the result of using only single feature, combining double features and triple feature respectively.",
                    "label": 0
                },
                {
                    "sent": "We observed that the performance the performance significantly increase with more features even after reducing tools that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even after reducing to the same dimension, the multiple feature wins.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We find this study the scalability, using Holidays, adding formatting images from Flickr.",
                    "label": 1
                },
                {
                    "sent": "We test our performance of reality for single feature sparse coding for single feature double features and triple features, our observation is.",
                    "label": 0
                },
                {
                    "sent": "Our observation is consistent with that instead of the Holidays.",
                    "label": 0
                },
                {
                    "sent": "For single feet.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Person, sparse coding or performance varied by three points.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using double and triple features the system.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe will greatly increase the increment is about sorting and 45 points respectively.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize.",
                    "label": 0
                },
                {
                    "sent": "In this paper we describe a system for large scale image search.",
                    "label": 0
                },
                {
                    "sent": "Our work is based on the framework of local feature aggregation.",
                    "label": 1
                },
                {
                    "sent": "It applies sparse coding, Max pooling aggregator.",
                    "label": 0
                },
                {
                    "sent": "Uses feature combination and proposes a kind of local local feature called Micro Feature.",
                    "label": 0
                },
                {
                    "sent": "By combining these technicals, we reached a system that is promising and competitive.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}