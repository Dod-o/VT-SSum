{
    "id": "omlfwazzccktb2o22tdwqapiolzfzrln",
    "title": "A Hilbert-Schmidt Dependence Maximization Approach to Unsupervised Structure Discovery",
    "info": {
        "author": [
            "Arthur Gretton, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Aug. 25, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/mlg08_gretton_ahsdma/",
    "segmentation": [
        [
            "OK, so this is work by myself and Matthew Blaschko.",
            "Who is this guy here and who is soon looking for a job so."
        ],
        [
            "Need to mention this.",
            "So the purpose of this talk is to find taxonomies in data.",
            "So this problem, what we're going to do is."
        ],
        [
            "Well, it's it's split into sort of two parts, so one is to assign the data to clusters and then to fit a tree structure to these clusters, and we want to do this simultaneously.",
            "And so we set the window.",
            "Yep, Yep, the way that we do this is basically by maximizing the dependence between the original data and the clustering and tree fitting of the data.",
            "And our dependence measure is going to be something called the Hilbert Schmidt independence criterion, which I'm going to introduce.",
            "So this is a kernel dependence measure."
        ],
        [
            "So, just briefly, what we're going to be able to achieve with this procedure.",
            "One hand in visualization because if the data is sort of nicely sorted into a tree structure, you can visualize it easily.",
            "Also, if the underlying data really does have a cluster structure, you can improve the clustering results by forcing it to fit that structure.",
            "So if the underlying data really looks like a tree, and you force it to fit a tree, then your clustering becomes more accurate.",
            "So this is what we found."
        ],
        [
            "Alright, so this is the talk structure.",
            "I'm going to begin with an introduction of the dependence criterion that we're going to use.",
            "And then going to talk about basically how this applies in the clustering and fitting of some.",
            "Dependent structure to the data.",
            "Then I'm going to talk about numerical taxonomy and what it means to fit data trees and how that fits in.",
            "With the above sort of part of the talk and then finally I have some results both on text and on images to present."
        ],
        [
            "OK, so let's start with the Hill which made independence criterion.",
            "So this is going to be partly a kernel talk.",
            "So we have two.",
            "Reproducing kernel Hilbert spaces F&G so F is on our input space, so our original data SpaceX.",
            "G is on our clustering space way, which I'll define later.",
            "And well, F has kernel K&G has kernel L. So these are basically inner products between feature mappings of the."
        ],
        [
            "Later, so yeah.",
            "Now to define dependence between the domain X in the domain, why we're going to use a covariance operator?",
            "So this is basically a generalization of a covariance matrix, but two infinite dimensional spaces, potentially that they don't have to be.",
            "So what does this operator do?",
            "So it's a mapping from one space to another space, such that when you apply it to a function in G and take the inner product with F, you just get the covariance between F&G.",
            "So this is basically the death."
        ],
        [
            "Mission of the operator.",
            "So this operator might be infinite dimensional, in which case it's kind of hard to sort of see what it's doing.",
            "But you can compute statistics on it, and in particular will be concerned with it till which met Norm.",
            "So the sum of the squared singular values of this operator, and this is a way of determining how much dependence there is between variables.",
            "So it's exactly by analogy with a covariance matrix."
        ],
        [
            "So even though this is a quite complicated nonparametric statistic of dependence, it turns out that it's very easy to compute all that you need to do is take the trace of a product of matrices.",
            "So this K matrix is the matrix between data points, the gram matrix.",
            "So the kernel matrix between data points in the input space.",
            "The L is the kernel matrix between the clusters.",
            "And then this H is just a centering matrix in which need not concern us, it's just a.",
            "It's not dependent on the data."
        ],
        [
            "Alright, so again, it's kind of hard to know exactly what the covariance operator is doing, so I'm going to try and illustrate what it means using just a very simple toy example to give people some idea of what it is.",
            "So here you have two variables X&Y.",
            "Which are dependent, but in this case I'm correlated to make the problem more interesting.",
            "And basically, of course we can't visualize the covariance operator, but we can visualize the functions that correspond to its largest singular value.",
            "So what these are, these are the functions of unit norm in F&G which maximize.",
            "This expression here OK, and these functions mean that this is equal to the maximum singular value for those particular functions.",
            "So what these are smooth functions that take these original variables and give them a strong linear dependence as they can subject to being smooth?",
            "And this is what you've got here.",
            "So you can think of a whole series of such functions where each pair is orthogonal to the previous pair, and each of them tries to maximize the dependants object to this orthogonal orthogonality constraint.",
            "So if you take the sum of the squared functions, this is sort of a summary of all of the dependence that can exist between X&Y and this is a sort of way to think about this dependence criterion.",
            "So this is a bit of a cartoon of this, so for more detail you should look at the papers on the topic, but that just gives an idea."
        ],
        [
            "Alright, so now we're going to use this criterion in our setting of clustering and taxonomy."
        ],
        [
            "Fitting.",
            "So here is our main objective function.",
            "What we ignore for the moment, we can ignore the denominator and just look at the numerator.",
            "What this is, is the Hilbert Schmidt independence criterion between a centered.",
            "Matrix KA gram matrix K. On the inputs and this cluster of matrices here, which Instagram matrix."
        ],
        [
            "For the clustering.",
            "So what we've done is we've decomposed this gram matrix into two components.",
            "When is a cluster assignment matrix?",
            "So that's a matrix of K columns, where K is the number of clusters and each row has a single one in it, which means that you're assigning each point to a as an individual cluster.",
            "OK, and then this matrix.",
            "Here is a gram matrix between the clusters, so this is an inner product between the clusters and says how similar the clusters are.",
            "So our goal overall is to learn both of these, and I'm going to sort of talk about how to do that.",
            "Now, the reason for the denominator is because of this way here.",
            "So if it wasn't there, you could just increase this function here by making the way arbitrarily large.",
            "So this is just to make sure that that's not possible."
        ],
        [
            "OK.",
            "So before talking about how we optimize that, just a note on related work, the most closely related work is this algorithm here called classic.",
            "Which basically gives you your way in advance and then optimizes the numerator with respect to pay.",
            "So this is what sort of started us on this topic of research.",
            "You can also recover normalized cuts if you set Y to be I, and then you set the input gram matrix in accordance with this expression here.",
            "So this a matrix is basically the distance between points in a graph and then the matrix is the total wage flowing out of each node in the graph, so that's another link.",
            "There's also kernel target alignment, which looks like this, but.",
            "Also contained in the denominator, a normal of M and doesn't have centering done in the same way that Alice is, so that's the third link."
        ],
        [
            "Alright, so that's the problem setting, so before trying to solve it in its generality, we're going to look at some special cases, and these cases are going to provide us with ingredients which we're going to use in our final algorithm, even though there any towards specific sort of subproblems.",
            "So in the first case, let's just say that Pi is a vector.",
            "And why is the identity matrix?",
            "So then the problem simplifies into just solving this ratio here."
        ],
        [
            "And we can rearrange this to get this eigenvalue problem.",
            "So basically now you're just looking for the eigenvectors and eigenvalues of your input gram matrix, and so up to rotation.",
            "What this looks like is basically spectral clustering, where this is your input matrix."
        ],
        [
            "So here is another subproblem.",
            "Imagine that you're given.",
            "A cluster assignment and you want to find the best possible gram matrix on the clusters?",
            "Well, then it's basically straightforward.",
            "You have a constrained optimization problem.",
            "You solve it in the usual way and you get this expression here.",
            "So if you plug this back into the original criteria."
        ],
        [
            "You get this quantity here is being the optimal cluster assignment.",
            "So you might say then well, the problems kind of solved.",
            "You say this is my cluster assignment, then you substitute that you get your way, then you're finished.",
            "So why might you not want to do that?",
            "The reason is because all that we've said about why is that it should be positive definite, and what we're interested in is actually imposing more structure on the way, and in particular, we want our cluster our clusters to have a tree structure, and we hope that this is going to improve our clustering as well as."
        ],
        [
            "Visualization."
        ],
        [
            "So this is going to be then the topic of the the next part."
        ],
        [
            "OK, so now I'm going to just introduce some concepts from numerical taxonomy and then this will lead to the actual clustering algo."
        ],
        [
            "Them.",
            "So what we have is a matrix way, which is the inner product, like the gram matrix for the clusters.",
            "From this we can compute distances between the clusters.",
            "And what we want to do is to be able to take those distances and to fit a tree to the clusters.",
            "Now for this to be possible and for the tree to be unique, these distances have to satisfy a four point condition.",
            "Which is written here.",
            "So I've just done a small drawing here to illustrate what this means.",
            "So what we want is that the distance from A to B plus the distance from C to D is less than or equal to the larger of the sums of this distances and these distances.",
            "So the way I've set it up now, this one and this one are the same, so you hit the upper bound.",
            "And likewise, if you add this distance in this distance, it should be smaller than the larger of these two sums, and in this case that satisfied.",
            "So for this toy example, the four point condition is satisfied.",
            "So in the course of our optimization, obviously the four point condition is probably not going to be satisfied."
        ],
        [
            "At some point, and So what you're going to do is that at some stage in the algorithm, we get a distance matrix.",
            "And we want to find the closest distance matrix in some sense to which you can fit a tree.",
            "And there's an algorithm to do that by Jabhat Al.",
            "It's the one we use.",
            "And finally, once we finish running our algorithm, we can fit a tree.",
            "Using this DT using very old algorithm by Waterman ET al.",
            "OK, so these are now all the ingredients that we need.",
            "So let's go to."
        ],
        [
            "The actual algorithm itself.",
            "So one thing to emphasize, of course, there's not a unique solution for this algorithm, it's just going to converge to something local, but nonetheless what we found is that it tends not to need many re initializations.",
            "So our input is a gram matrix on the initial data and we output a cluster assignment and a gram matrix which satisfies the four point conditions when you turned it into a distance matrix.",
            "We initialize just using spectral clustering.",
            "Then we iterate.",
            "We solve for Y given the cluster assignments.",
            "We turn our gram matrix into a distance matrix.",
            "We find the closest.",
            "Matrix that satisfies the four point conditions and then we go backwards to a gram matrix.",
            "So this is the iteration to get way and then we update our cluster assignment as well.",
            "So just a brief note on what goes on here.",
            "So this is again a greedy search.",
            "What you do is that for each of your data points you look to re assign it to another cluster such that you maximize the overall quality of clustering criterion and you iterate through your data points.",
            "We assigning points to clusters until basically you reach some point where you get no further improvement.",
            "And so that's how that works."
        ],
        [
            "OK, so that's the algorithm.",
            "Now we have some results."
        ],
        [
            "So this is a data set which was in the original paper of some metal.",
            "There's a number of very attractive researchers who are showing different facial expressions.",
            "And what we can see is that, well, hopefully the researchers are grouped by identity of by their identity first.",
            "So the same researcher with different facial expressions is close.",
            "And then these guys are further from these guys or these guys, for instance.",
            "And you can see that this clustering here or this tree fitting works very well.",
            "So it sort of gives you an answer that you would expect."
        ],
        [
            "So now let's compare to what was done earlier.",
            "So what you can see here is a number of possible tree structures that you could impose on the data, and so if you remember from the original algorithm of Song ET al.",
            "What they did is imposed a structure on the data and then tried to cluster accordingly.",
            "So.",
            "On the table below you can see the results, so this 1st result here is spectral clustering where you don't impose any structure on the data.",
            "And this number here is the relative entropy of the cluster centers compared to the true cluster identity.",
            "So basically smaller is better.",
            "And corresponds to a more accurate clustering.",
            "So spectral clustering is pretty good of all of these possible structures that one could impose.",
            "The best clustering is obtained with Structure B and that's exactly the structure that first groups people by their identity, and then by their facial expressions.",
            "So this seems to work.",
            "But the trouble with imposing these structures is that imposes a distance matrix or a gram matrix on the data, which might not be a reflection of the true distance matrix because some.",
            "Peoples faces, for instance, are further away than others, so this one's kind of an outlier for some reason.",
            "Um?",
            "So if you allow yourself to learn the gram matrix as well, then you get a higher quality clustering.",
            "So this is a way to improve the cluster."
        ],
        [
            "Help.",
            "OK, so we also ran the algorithm on the NIPS data set and we got the following tree here.",
            "So you can see that it's actually grouping things very sensibly, so reinforcement learning is out there.",
            "Bayesian learning, discriminative learning, neuroscience hardware said the VLSI things and so on.",
            "And you'll also notice.",
            "So what we have to impose in advance here is the number of clusters, and here you have this cluster sort of stuck in the middle somewhere called miscellaneous."
        ],
        [
            "And if you look, I mean, these are basically so this table.",
            "Here are the words that occur most often in the particular cluster here, which don't occur in the other clusters.",
            "So these are kind of the informative words.",
            "But of course we don't use these in our clustering, we're just using these for display purposes.",
            "And you can see that the miscellaneous is kind of.",
            "It's a bit everywhere, so there's a lot of.",
            "There's a bit of control theory in there.",
            "Actually, maybe it's more of a control theory one.",
            "Anyway, yeah.",
            "But all of the other ones behaving quite reasonably, so yeah.",
            "Actually, I think these labels are wrong.",
            "Yeah.",
            "No, I think the labels are wrong.",
            "OK so.",
            "Oh dear yeah.",
            "Anyway, so you can see that this is a neural one.",
            "This one is a reinforcement learning one.",
            "This one is neural networks and so on, so unfortunately.",
            "These labels are correct, but the table these headings are incorrect.",
            "Anyway.",
            "So let's see if we can find miscellaneous.",
            "Probably not, yeah."
        ],
        [
            "So one more thing is that you can perturb the spectrum of your input gram matrix by basically forcing your kernel to look more like a will to have a similar spectral structure to.",
            "So to have all that singular values more close to unity.",
            "And you can sort of see the effect that this has on the clustering.",
            "So.",
            "What this is doing now is basically as you're increasing.",
            "The amount of I hear.",
            "All of these structure is being lost progressively and eventually you get something that effectively just looks like a star where every set of faces has its own cluster, but the relation between the cluster is lost.",
            "So this is a way of illustrating.",
            "Basically like the importance of the gram matrix on the quality of the clustering and how it sort of captures this structure."
        ],
        [
            "OK, so that's basically the conclusion of the talk, so we have a dependence measure that we optimize both well to create a clustering.",
            "But in optimizing this we also fit a tree structure to our clusters and this results both in good visualization and also in improved clustering performance.",
            "So at the moment I guess the biggest shortcoming of our work is that.",
            "The method for finding the cluster assignments given the distance matrix is a bit well, inefficient and also probably not optimal, so we'd like to improve that.",
            "OK, so thank you.",
            "There's a question.",
            "Yeah, I'm not sure, so you want to impose a graph structure on your way.",
            "I mean, what graph structure would you have in mind?",
            "Yeah, I mean, OK, I think it would be for sure.",
            "It would be interesting."
        ],
        [
            "So.",
            "If you look, I mean if you know the original submittal paper.",
            "They look at a whole lot of different structures besides trees, so they have, for instance, a rotating teapot, and in that case obviously the structure is circular, and so on.",
            "And at the moment of course we're not able to learn that.",
            "So basically the points for extending it would be these two points.",
            "So what you'd need to be able to do is to find at this point here a distance structure that corresponds to whatever graph constraint that you want to impose, which is close to the distance which you currently have in your algorithm.",
            "And then you need a way to take that distance matrix and turn it into a graph, which I guess is the easier problem of the two.",
            "But this step is the one that is probably the main thing that you'd need to look at here.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is work by myself and Matthew Blaschko.",
                    "label": 0
                },
                {
                    "sent": "Who is this guy here and who is soon looking for a job so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need to mention this.",
                    "label": 0
                },
                {
                    "sent": "So the purpose of this talk is to find taxonomies in data.",
                    "label": 1
                },
                {
                    "sent": "So this problem, what we're going to do is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it's it's split into sort of two parts, so one is to assign the data to clusters and then to fit a tree structure to these clusters, and we want to do this simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And so we set the window.",
                    "label": 0
                },
                {
                    "sent": "Yep, Yep, the way that we do this is basically by maximizing the dependence between the original data and the clustering and tree fitting of the data.",
                    "label": 1
                },
                {
                    "sent": "And our dependence measure is going to be something called the Hilbert Schmidt independence criterion, which I'm going to introduce.",
                    "label": 0
                },
                {
                    "sent": "So this is a kernel dependence measure.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just briefly, what we're going to be able to achieve with this procedure.",
                    "label": 0
                },
                {
                    "sent": "One hand in visualization because if the data is sort of nicely sorted into a tree structure, you can visualize it easily.",
                    "label": 0
                },
                {
                    "sent": "Also, if the underlying data really does have a cluster structure, you can improve the clustering results by forcing it to fit that structure.",
                    "label": 0
                },
                {
                    "sent": "So if the underlying data really looks like a tree, and you force it to fit a tree, then your clustering becomes more accurate.",
                    "label": 0
                },
                {
                    "sent": "So this is what we found.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is the talk structure.",
                    "label": 0
                },
                {
                    "sent": "I'm going to begin with an introduction of the dependence criterion that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "And then going to talk about basically how this applies in the clustering and fitting of some.",
                    "label": 0
                },
                {
                    "sent": "Dependent structure to the data.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about numerical taxonomy and what it means to fit data trees and how that fits in.",
                    "label": 0
                },
                {
                    "sent": "With the above sort of part of the talk and then finally I have some results both on text and on images to present.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with the Hill which made independence criterion.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be partly a kernel talk.",
                    "label": 0
                },
                {
                    "sent": "So we have two.",
                    "label": 0
                },
                {
                    "sent": "Reproducing kernel Hilbert spaces F&G so F is on our input space, so our original data SpaceX.",
                    "label": 0
                },
                {
                    "sent": "G is on our clustering space way, which I'll define later.",
                    "label": 0
                },
                {
                    "sent": "And well, F has kernel K&G has kernel L. So these are basically inner products between feature mappings of the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Now to define dependence between the domain X in the domain, why we're going to use a covariance operator?",
                    "label": 0
                },
                {
                    "sent": "So this is basically a generalization of a covariance matrix, but two infinite dimensional spaces, potentially that they don't have to be.",
                    "label": 0
                },
                {
                    "sent": "So what does this operator do?",
                    "label": 0
                },
                {
                    "sent": "So it's a mapping from one space to another space, such that when you apply it to a function in G and take the inner product with F, you just get the covariance between F&G.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the death.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission of the operator.",
                    "label": 0
                },
                {
                    "sent": "So this operator might be infinite dimensional, in which case it's kind of hard to sort of see what it's doing.",
                    "label": 0
                },
                {
                    "sent": "But you can compute statistics on it, and in particular will be concerned with it till which met Norm.",
                    "label": 0
                },
                {
                    "sent": "So the sum of the squared singular values of this operator, and this is a way of determining how much dependence there is between variables.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly by analogy with a covariance matrix.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So even though this is a quite complicated nonparametric statistic of dependence, it turns out that it's very easy to compute all that you need to do is take the trace of a product of matrices.",
                    "label": 0
                },
                {
                    "sent": "So this K matrix is the matrix between data points, the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "So the kernel matrix between data points in the input space.",
                    "label": 0
                },
                {
                    "sent": "The L is the kernel matrix between the clusters.",
                    "label": 0
                },
                {
                    "sent": "And then this H is just a centering matrix in which need not concern us, it's just a.",
                    "label": 0
                },
                {
                    "sent": "It's not dependent on the data.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so again, it's kind of hard to know exactly what the covariance operator is doing, so I'm going to try and illustrate what it means using just a very simple toy example to give people some idea of what it is.",
                    "label": 0
                },
                {
                    "sent": "So here you have two variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "Which are dependent, but in this case I'm correlated to make the problem more interesting.",
                    "label": 0
                },
                {
                    "sent": "And basically, of course we can't visualize the covariance operator, but we can visualize the functions that correspond to its largest singular value.",
                    "label": 0
                },
                {
                    "sent": "So what these are, these are the functions of unit norm in F&G which maximize.",
                    "label": 0
                },
                {
                    "sent": "This expression here OK, and these functions mean that this is equal to the maximum singular value for those particular functions.",
                    "label": 0
                },
                {
                    "sent": "So what these are smooth functions that take these original variables and give them a strong linear dependence as they can subject to being smooth?",
                    "label": 0
                },
                {
                    "sent": "And this is what you've got here.",
                    "label": 0
                },
                {
                    "sent": "So you can think of a whole series of such functions where each pair is orthogonal to the previous pair, and each of them tries to maximize the dependants object to this orthogonal orthogonality constraint.",
                    "label": 0
                },
                {
                    "sent": "So if you take the sum of the squared functions, this is sort of a summary of all of the dependence that can exist between X&Y and this is a sort of way to think about this dependence criterion.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit of a cartoon of this, so for more detail you should look at the papers on the topic, but that just gives an idea.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now we're going to use this criterion in our setting of clustering and taxonomy.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fitting.",
                    "label": 0
                },
                {
                    "sent": "So here is our main objective function.",
                    "label": 1
                },
                {
                    "sent": "What we ignore for the moment, we can ignore the denominator and just look at the numerator.",
                    "label": 0
                },
                {
                    "sent": "What this is, is the Hilbert Schmidt independence criterion between a centered.",
                    "label": 1
                },
                {
                    "sent": "Matrix KA gram matrix K. On the inputs and this cluster of matrices here, which Instagram matrix.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the clustering.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is we've decomposed this gram matrix into two components.",
                    "label": 0
                },
                {
                    "sent": "When is a cluster assignment matrix?",
                    "label": 1
                },
                {
                    "sent": "So that's a matrix of K columns, where K is the number of clusters and each row has a single one in it, which means that you're assigning each point to a as an individual cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, and then this matrix.",
                    "label": 1
                },
                {
                    "sent": "Here is a gram matrix between the clusters, so this is an inner product between the clusters and says how similar the clusters are.",
                    "label": 0
                },
                {
                    "sent": "So our goal overall is to learn both of these, and I'm going to sort of talk about how to do that.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason for the denominator is because of this way here.",
                    "label": 0
                },
                {
                    "sent": "So if it wasn't there, you could just increase this function here by making the way arbitrarily large.",
                    "label": 0
                },
                {
                    "sent": "So this is just to make sure that that's not possible.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So before talking about how we optimize that, just a note on related work, the most closely related work is this algorithm here called classic.",
                    "label": 0
                },
                {
                    "sent": "Which basically gives you your way in advance and then optimizes the numerator with respect to pay.",
                    "label": 0
                },
                {
                    "sent": "So this is what sort of started us on this topic of research.",
                    "label": 0
                },
                {
                    "sent": "You can also recover normalized cuts if you set Y to be I, and then you set the input gram matrix in accordance with this expression here.",
                    "label": 1
                },
                {
                    "sent": "So this a matrix is basically the distance between points in a graph and then the matrix is the total wage flowing out of each node in the graph, so that's another link.",
                    "label": 1
                },
                {
                    "sent": "There's also kernel target alignment, which looks like this, but.",
                    "label": 0
                },
                {
                    "sent": "Also contained in the denominator, a normal of M and doesn't have centering done in the same way that Alice is, so that's the third link.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so that's the problem setting, so before trying to solve it in its generality, we're going to look at some special cases, and these cases are going to provide us with ingredients which we're going to use in our final algorithm, even though there any towards specific sort of subproblems.",
                    "label": 1
                },
                {
                    "sent": "So in the first case, let's just say that Pi is a vector.",
                    "label": 1
                },
                {
                    "sent": "And why is the identity matrix?",
                    "label": 0
                },
                {
                    "sent": "So then the problem simplifies into just solving this ratio here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can rearrange this to get this eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "So basically now you're just looking for the eigenvectors and eigenvalues of your input gram matrix, and so up to rotation.",
                    "label": 0
                },
                {
                    "sent": "What this looks like is basically spectral clustering, where this is your input matrix.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is another subproblem.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you're given.",
                    "label": 0
                },
                {
                    "sent": "A cluster assignment and you want to find the best possible gram matrix on the clusters?",
                    "label": 0
                },
                {
                    "sent": "Well, then it's basically straightforward.",
                    "label": 0
                },
                {
                    "sent": "You have a constrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You solve it in the usual way and you get this expression here.",
                    "label": 0
                },
                {
                    "sent": "So if you plug this back into the original criteria.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get this quantity here is being the optimal cluster assignment.",
                    "label": 0
                },
                {
                    "sent": "So you might say then well, the problems kind of solved.",
                    "label": 0
                },
                {
                    "sent": "You say this is my cluster assignment, then you substitute that you get your way, then you're finished.",
                    "label": 0
                },
                {
                    "sent": "So why might you not want to do that?",
                    "label": 0
                },
                {
                    "sent": "The reason is because all that we've said about why is that it should be positive definite, and what we're interested in is actually imposing more structure on the way, and in particular, we want our cluster our clusters to have a tree structure, and we hope that this is going to improve our clustering as well as.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visualization.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is going to be then the topic of the the next part.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to just introduce some concepts from numerical taxonomy and then this will lead to the actual clustering algo.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Them.",
                    "label": 0
                },
                {
                    "sent": "So what we have is a matrix way, which is the inner product, like the gram matrix for the clusters.",
                    "label": 0
                },
                {
                    "sent": "From this we can compute distances between the clusters.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is to be able to take those distances and to fit a tree to the clusters.",
                    "label": 0
                },
                {
                    "sent": "Now for this to be possible and for the tree to be unique, these distances have to satisfy a four point condition.",
                    "label": 0
                },
                {
                    "sent": "Which is written here.",
                    "label": 0
                },
                {
                    "sent": "So I've just done a small drawing here to illustrate what this means.",
                    "label": 0
                },
                {
                    "sent": "So what we want is that the distance from A to B plus the distance from C to D is less than or equal to the larger of the sums of this distances and these distances.",
                    "label": 0
                },
                {
                    "sent": "So the way I've set it up now, this one and this one are the same, so you hit the upper bound.",
                    "label": 0
                },
                {
                    "sent": "And likewise, if you add this distance in this distance, it should be smaller than the larger of these two sums, and in this case that satisfied.",
                    "label": 0
                },
                {
                    "sent": "So for this toy example, the four point condition is satisfied.",
                    "label": 1
                },
                {
                    "sent": "So in the course of our optimization, obviously the four point condition is probably not going to be satisfied.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At some point, and So what you're going to do is that at some stage in the algorithm, we get a distance matrix.",
                    "label": 0
                },
                {
                    "sent": "And we want to find the closest distance matrix in some sense to which you can fit a tree.",
                    "label": 0
                },
                {
                    "sent": "And there's an algorithm to do that by Jabhat Al.",
                    "label": 0
                },
                {
                    "sent": "It's the one we use.",
                    "label": 0
                },
                {
                    "sent": "And finally, once we finish running our algorithm, we can fit a tree.",
                    "label": 0
                },
                {
                    "sent": "Using this DT using very old algorithm by Waterman ET al.",
                    "label": 1
                },
                {
                    "sent": "OK, so these are now all the ingredients that we need.",
                    "label": 0
                },
                {
                    "sent": "So let's go to.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The actual algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "So one thing to emphasize, of course, there's not a unique solution for this algorithm, it's just going to converge to something local, but nonetheless what we found is that it tends not to need many re initializations.",
                    "label": 0
                },
                {
                    "sent": "So our input is a gram matrix on the initial data and we output a cluster assignment and a gram matrix which satisfies the four point conditions when you turned it into a distance matrix.",
                    "label": 0
                },
                {
                    "sent": "We initialize just using spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Then we iterate.",
                    "label": 0
                },
                {
                    "sent": "We solve for Y given the cluster assignments.",
                    "label": 1
                },
                {
                    "sent": "We turn our gram matrix into a distance matrix.",
                    "label": 0
                },
                {
                    "sent": "We find the closest.",
                    "label": 0
                },
                {
                    "sent": "Matrix that satisfies the four point conditions and then we go backwards to a gram matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is the iteration to get way and then we update our cluster assignment as well.",
                    "label": 0
                },
                {
                    "sent": "So just a brief note on what goes on here.",
                    "label": 0
                },
                {
                    "sent": "So this is again a greedy search.",
                    "label": 0
                },
                {
                    "sent": "What you do is that for each of your data points you look to re assign it to another cluster such that you maximize the overall quality of clustering criterion and you iterate through your data points.",
                    "label": 0
                },
                {
                    "sent": "We assigning points to clusters until basically you reach some point where you get no further improvement.",
                    "label": 0
                },
                {
                    "sent": "And so that's how that works.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we have some results.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a data set which was in the original paper of some metal.",
                    "label": 0
                },
                {
                    "sent": "There's a number of very attractive researchers who are showing different facial expressions.",
                    "label": 0
                },
                {
                    "sent": "And what we can see is that, well, hopefully the researchers are grouped by identity of by their identity first.",
                    "label": 0
                },
                {
                    "sent": "So the same researcher with different facial expressions is close.",
                    "label": 0
                },
                {
                    "sent": "And then these guys are further from these guys or these guys, for instance.",
                    "label": 0
                },
                {
                    "sent": "And you can see that this clustering here or this tree fitting works very well.",
                    "label": 0
                },
                {
                    "sent": "So it sort of gives you an answer that you would expect.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's compare to what was done earlier.",
                    "label": 0
                },
                {
                    "sent": "So what you can see here is a number of possible tree structures that you could impose on the data, and so if you remember from the original algorithm of Song ET al.",
                    "label": 1
                },
                {
                    "sent": "What they did is imposed a structure on the data and then tried to cluster accordingly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "On the table below you can see the results, so this 1st result here is spectral clustering where you don't impose any structure on the data.",
                    "label": 0
                },
                {
                    "sent": "And this number here is the relative entropy of the cluster centers compared to the true cluster identity.",
                    "label": 0
                },
                {
                    "sent": "So basically smaller is better.",
                    "label": 0
                },
                {
                    "sent": "And corresponds to a more accurate clustering.",
                    "label": 0
                },
                {
                    "sent": "So spectral clustering is pretty good of all of these possible structures that one could impose.",
                    "label": 0
                },
                {
                    "sent": "The best clustering is obtained with Structure B and that's exactly the structure that first groups people by their identity, and then by their facial expressions.",
                    "label": 0
                },
                {
                    "sent": "So this seems to work.",
                    "label": 0
                },
                {
                    "sent": "But the trouble with imposing these structures is that imposes a distance matrix or a gram matrix on the data, which might not be a reflection of the true distance matrix because some.",
                    "label": 0
                },
                {
                    "sent": "Peoples faces, for instance, are further away than others, so this one's kind of an outlier for some reason.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So if you allow yourself to learn the gram matrix as well, then you get a higher quality clustering.",
                    "label": 0
                },
                {
                    "sent": "So this is a way to improve the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Help.",
                    "label": 0
                },
                {
                    "sent": "OK, so we also ran the algorithm on the NIPS data set and we got the following tree here.",
                    "label": 0
                },
                {
                    "sent": "So you can see that it's actually grouping things very sensibly, so reinforcement learning is out there.",
                    "label": 0
                },
                {
                    "sent": "Bayesian learning, discriminative learning, neuroscience hardware said the VLSI things and so on.",
                    "label": 0
                },
                {
                    "sent": "And you'll also notice.",
                    "label": 0
                },
                {
                    "sent": "So what we have to impose in advance here is the number of clusters, and here you have this cluster sort of stuck in the middle somewhere called miscellaneous.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you look, I mean, these are basically so this table.",
                    "label": 0
                },
                {
                    "sent": "Here are the words that occur most often in the particular cluster here, which don't occur in the other clusters.",
                    "label": 0
                },
                {
                    "sent": "So these are kind of the informative words.",
                    "label": 0
                },
                {
                    "sent": "But of course we don't use these in our clustering, we're just using these for display purposes.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the miscellaneous is kind of.",
                    "label": 0
                },
                {
                    "sent": "It's a bit everywhere, so there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of control theory in there.",
                    "label": 0
                },
                {
                    "sent": "Actually, maybe it's more of a control theory one.",
                    "label": 0
                },
                {
                    "sent": "Anyway, yeah.",
                    "label": 0
                },
                {
                    "sent": "But all of the other ones behaving quite reasonably, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think these labels are wrong.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, I think the labels are wrong.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Oh dear yeah.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so you can see that this is a neural one.",
                    "label": 0
                },
                {
                    "sent": "This one is a reinforcement learning one.",
                    "label": 0
                },
                {
                    "sent": "This one is neural networks and so on, so unfortunately.",
                    "label": 0
                },
                {
                    "sent": "These labels are correct, but the table these headings are incorrect.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "So let's see if we can find miscellaneous.",
                    "label": 0
                },
                {
                    "sent": "Probably not, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one more thing is that you can perturb the spectrum of your input gram matrix by basically forcing your kernel to look more like a will to have a similar spectral structure to.",
                    "label": 0
                },
                {
                    "sent": "So to have all that singular values more close to unity.",
                    "label": 0
                },
                {
                    "sent": "And you can sort of see the effect that this has on the clustering.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What this is doing now is basically as you're increasing.",
                    "label": 0
                },
                {
                    "sent": "The amount of I hear.",
                    "label": 0
                },
                {
                    "sent": "All of these structure is being lost progressively and eventually you get something that effectively just looks like a star where every set of faces has its own cluster, but the relation between the cluster is lost.",
                    "label": 0
                },
                {
                    "sent": "So this is a way of illustrating.",
                    "label": 0
                },
                {
                    "sent": "Basically like the importance of the gram matrix on the quality of the clustering and how it sort of captures this structure.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's basically the conclusion of the talk, so we have a dependence measure that we optimize both well to create a clustering.",
                    "label": 0
                },
                {
                    "sent": "But in optimizing this we also fit a tree structure to our clusters and this results both in good visualization and also in improved clustering performance.",
                    "label": 0
                },
                {
                    "sent": "So at the moment I guess the biggest shortcoming of our work is that.",
                    "label": 0
                },
                {
                    "sent": "The method for finding the cluster assignments given the distance matrix is a bit well, inefficient and also probably not optimal, so we'd like to improve that.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                },
                {
                    "sent": "There's a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure, so you want to impose a graph structure on your way.",
                    "label": 0
                },
                {
                    "sent": "I mean, what graph structure would you have in mind?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, OK, I think it would be for sure.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you look, I mean if you know the original submittal paper.",
                    "label": 0
                },
                {
                    "sent": "They look at a whole lot of different structures besides trees, so they have, for instance, a rotating teapot, and in that case obviously the structure is circular, and so on.",
                    "label": 0
                },
                {
                    "sent": "And at the moment of course we're not able to learn that.",
                    "label": 0
                },
                {
                    "sent": "So basically the points for extending it would be these two points.",
                    "label": 0
                },
                {
                    "sent": "So what you'd need to be able to do is to find at this point here a distance structure that corresponds to whatever graph constraint that you want to impose, which is close to the distance which you currently have in your algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then you need a way to take that distance matrix and turn it into a graph, which I guess is the easier problem of the two.",
                    "label": 0
                },
                {
                    "sent": "But this step is the one that is probably the main thing that you'd need to look at here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}