{
    "id": "2yj5hnvrvdnmptej2o76kvmq2jiglc4c",
    "title": "Old and New algorithm for Blind Deconvolution",
    "info": {
        "author": [
            "Yair Weiss, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "Jan. 23, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Computer Vision->Computational Photography"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_weiss_deconvolution/",
    "segmentation": [
        [
            "Thank you.",
            "This is work.",
            "Steven was kind enough to mention a lot of the work I've done recently in blind convolution on solve that work is done by a not as is this work.",
            "But one more person I'd like to thank here is a mayor Feder who is a professor of information.",
            "Tell him University and in some sense this talk is an argument for coffee breaks.",
            "I think it's better to leave the door open because people were complaining about oxygen.",
            "So basically I was on a several committees with the.",
            "With mayor and during the coffee breaks we started talking about what each of us were doing, and.",
            "He pointed out to me that there's a lot of interesting work on blind deconvolution done in the communication theory world and that we should really take a look at it and essentially that's what the bottom line of this talk is.",
            "We should really take a look at it so it's more as the previous speaker said, the computer vision people should like the image processing people, but there's even a broader community out there that we should all be familiar with."
        ],
        [
            "OK, so why do?",
            "Why do I think it's great to work on blind convolution?",
            "It's because it's like all my favorite problems.",
            "It's Anil post problem.",
            "You're given the single image why I need to reconstruct you 2 images.",
            "So there's an infinite number of possible solutions and that's what makes it an interesting problem to work on."
        ],
        [
            "Despite the fact that this is a little post problem, in recent years we've been able to solve it, which is really quite impressive.",
            "And again, if you look sort of at the history, it's true that people in all sorts of communities have looked at blind deconvolution over the years.",
            "I think for me, really, starting with Rob's work in 2006, the amazing thing is to actually get this to work on real images.",
            "Real camera shakes even though you know there's no problem.",
            "We're just making up these priors that are may or may not be correct, but you know, lots of different methods Now can take an image like this.",
            "And come out with the Blur kernel, which is what's shown here in the insets, and with the sharp image over here.",
            "Really.",
            "Just many people sitting in this room, but I think it's it's good to sort of sit back and reflect on the fact that we can actually solve this.",
            "I think it's quite an achievement."
        ],
        [
            "Almost all of these methods, and also I guess the methods mentioned the previous talk are basically Bayesian message with the sparse prior, so we know that if we look at the derivatives of natural image and we plotted histogram, it's going to have this non Gaussian distribution.",
            "Heavy tail distribution, often modeled by penalising for the absolute value of derivative race to the Alpha, where Alpha is a number between zero and one.",
            "Is a 70 norms that were mentioned earlier?",
            "And that gives us the joint distribution of both the image unknown image X and the unknown burchenal K given the observed image Y.",
            "And given this joint distribution, there are two directions 1 could go.",
            "You could try to do global map, find the latent image X and the unknown filter K which maximizes this posterior probability of there should be a - here, or minimizes this energy function over here.",
            "Night and I and billing for Adobe Reader paper for years ago, pointing out that this is guaranteed to fail.",
            "That is that if you actually.",
            "Find the global optimum of this energy function.",
            "In blind image deconvolution you always get the wrong blur kernel.",
            "Anything is, we thought by writing this paper, we would stop people using it, but it hasn't all happen.",
            "In fact, it often works well in practice if you don't keep iterating it.",
            "If you stop the iterations and you don't find the global optimum, this is a surprisingly sort of an embarrassingly successful method.",
            "And I'll say a few words.",
            "I think our current understanding of why that is, but the algorithm that is guaranteed to succeed is when you integrate over the latent image and just maximize over P of XP of K. So you find the most likely derivative most likely filter blur kernel while integrating over X, and that it's a very simple proof to show if your images are sampled from a prior and everything is all you're basing assumptions are correct.",
            "This will succeed this, on the other hand.",
            "Even if all your assumptions are correct is guaranteed to fail if you globally optimize it.",
            "OK, so that was a very."
        ],
        [
            "Introduction For where we are in computational photography.",
            "And now I want to talk about the consequences of our coffee break.",
            "We're following the suggestion, familiar Feder.",
            "We went and read a little bit about what people in communication systems do.",
            "I think the most interesting thing is that the vast majority of their approaches are not map approaches.",
            "They're not Bayesian approaches.",
            "These different approach.",
            "And they have us in some sense stronger correctness guarantees than we have.",
            "They can prove things or will find correct blur kernel under stronger conditions, weaker conditions, more general conditions.",
            "So I'll talk about the my main message, the.",
            "A corollary of this is that actually some of these math XK algorithms, the ones that, as I mentioned earlier, are guaranteed to fail, can be thought of as doing an approximation to these old algorithms, and that can explain some of their success."
        ],
        [
            "OK, so I don't know how many people have seen this.",
            "This is a 56 K modem.",
            "You know when you turned it on in the old days and having people ever had dial-up modems, there would be this.",
            "I since we're being filmed, I won't try to reproduce the signal here, but there do this very pleasant sound where the modem would emit a synchronization tone.",
            "And why is it doing that?",
            "It's the solver deconvolution problem, right?",
            "So when the modem transmits information over a phone line, or this is of course true for any communication system.",
            "If you transmit X of T, what you receive on the other side is.",
            "Linear combination of X at different delays where you know the signal reverberates through the medium and basically the received signal.",
            "Y is a convolution of X with an unknown blur kernel, so it's exactly the same problem that we face in camera shake.",
            "So how do you solve it again in the one version is again this synchronization sequence basically is a known X, so the modem sends out a particular tone that the receiver knows, and then the receiver just has to solve, and this is the LMS rule that many of us learned in our neural network class.",
            "Is an iterative algorithm for finding the blur kernel when X&Y are both known.",
            "And that's good for short communications, but if you have a very long communication.",
            "Setting the Blur kernel will change overtime, so sending the communication the synchronization signal in the beginning is not going to give you the right blur kernel because it keeps changing overtime.",
            "And so here and so there's been a lot of work in this field on blind deconvolution because you want to re estimate this blur kernel automatically during the long communication protocol.",
            "Here these are two papers that I think everyone all of us should read.",
            "One thing you can't see the font, but this is 1980 IEEE Trans.",
            "Transaction communications in 1980, which does blind deconvolution.",
            "And this is actually, I think this is from 1998 by shelving Weinstein, which is mostly what I'm going to talk about.",
            "This paper reinterprets this old algorithm in the way that I found it easier to understand and has more generalizations.",
            "OK, so how do these classical algorithms work?"
        ],
        [
            "And really, the mathematics behind it is very simple.",
            "It's based on the central limit theorem.",
            "And So what they?",
            "What they point out?",
            "This is again the Shelvy and Weinstein.",
            "Exposition if you take here, I'm showing communication signals.",
            "Of course, for 1D signals, but since I'm so used to looking at images, I'm going to show them as an image.",
            "So think of this as time goes like this as scanlines.",
            "So here's our original signal states a bit.",
            "It's either A plus one or minus one.",
            "Histogram will look like this when we blur it.",
            "So what is blur?",
            "Do really right blur?",
            "Does linear combination every pixel here or every received symbol is a linear combination of these IID signals.",
            "So what does that mean about the distribution of Y?",
            "It should be Gaussian, right?",
            "Again, these are independent random variables.",
            "Every bit is independent.",
            "That's the crucial assumption.",
            "So every bit here is independent, so it has this histogram of plus one or minus one.",
            "Here every every received.",
            "Vector is a linear combination of these independent random variables, so it's going to look like a Gaussian.",
            "It's not, of course, it will look like a Gaussian if the bertran were infinitely big.",
            "And that's not really the case that we're interested in, but it will always be more Gaussian than this.",
            "And you can measure gaussianity as many of us in this room know by looking at the crew ptosis.",
            "So here the kurtosis was one.",
            "This was a + 1 -- 1 signal and the crisis of the Gaussian is 3 and this signal is not exactly Gaussian, but it's kurtosis approaches three.",
            "This is the standard definition proptosis.",
            "We divide the signal by standard deviation and measure the 4th moment, and if it's a Gaussian, we get three out.",
            "And if these are sort of sub Gaussian signals, their kurtosis was less than one to begin with.",
            "So after blur they become, their kurtosis increases and it becomes more Gaussian."
        ],
        [
            "Here is a different kind of communication signal which has this super Gaussian behavior, so it's a sparse signal everyone most of the time the input is close to zero and every once in awhile we get a sharp peak around 0, so it's Christos is 26.",
            "When we blur it again, what happens?",
            "We're averaging independent random variables.",
            "That makes things more Gaussian, and we don't go all the way to three.",
            "We go to five here, but it's always the case that the Blur signal will be more Gaussian.",
            "Then the original signal I'm really that, all that is, is the central limit theorem."
        ],
        [
            "OK, so this gives us a very simple algorithm for blind deconvolution.",
            "This should be 1990.",
            "And then here's the algorithm.",
            "I can just really write it down very simply.",
            "So one thing is, we're not going to solve for K. The Unblocker will suffer the inverse filter.",
            "The equalizer they call it, so E is a filter that when we it's the inverse of can we apply it to our blurred image or alert signal we get back the original thing.",
            "And the only thing again compared to what we were talking about in the previous talk, and almost all the work in computational photography, we need to know a lot about the prior distribution over our signals in blindly in computational photography.",
            "Here we don't need to know that all we just need to know a sign we need to know is the original kurtosis less than three or greater than three.",
            "If it's less than three, like in the plus 1 -- 1 signal that I showed you earlier, what you need to do is just.",
            "Find equalizer such that when you convolve Y with that equalizer, the kurtosis is maximal.",
            "And if it were Super Gaussian, sorry here.",
            "Should be minimal here.",
            "We want to minimize the kurtosis and he want to maximize the kurtosis.",
            "And I'll show this in picture why that is.",
            "But if the original signal had a kurtosis that was less than the Galaxy, and then what we want to do is minimize the kurtosis as much as possible.",
            "If the original signal had a crisis that was greater than that of a Gaussian, wanna maximize the proptosis?",
            "And this very simple algorithm comes with a very strong guarantee that it's guaranteed to find the correct.",
            "As long as the X is where ID this is guaranteed to find the correct inverse filter and there are no local minima.",
            "Again, this is much stronger than what we can usually show for our blind convolution algorithms, where before lucky we find the criterion with global optimum gives the right lower kernel.",
            "Here it's even not only that is correct, but there are no local Optima either.",
            "Any stationary point of the kurtosis.",
            "Is going to give you the correct one and the proof is really 3 lines."
        ],
        [
            "We will do it.",
            "Remember, the main thing to remember is a central limit theorem that says that blur makes signals more Gaussian.",
            "So let's start with the Super Gaussian case.",
            "So if our original signal had a kurtosis greater than three, the algorithm just says find an equalizer so that when we apply to our blurred signal, the proptosis is maximum and what we want to show is that that will only will be maximal if and only if K is the correct East is the inverse of K. OK, so how do we show it?",
            "Here?",
            "Is our D blurred image.",
            "I keep saying image, but it's a signal.",
            "Here is our deep alert signal X star.",
            "It's a convolution of E with with Y&Y is K * X.",
            "So basically our hypothesized X is the convolution of something with the original signal.",
            "This original signal is IID, so any convolution of it is always going to be more Gaussian than the original X.",
            "The only way you could maximize this quantity is if Y times if he is the inverse of K and you get a Delta function.",
            "So X equal to X and the other thing you'll always have kurtosis less than X, yes?",
            "Support this evening system.",
            "Let me repeat the question so much.",
            "Yes is asking.",
            "What happens if he does not exist?",
            "If the inverse kernel does not exist, and that's a good question.",
            "The simple proof assumes that this that the inverse kernel exists there are there are generalizations where you don't need to assume that the filter is invertible, but we're going to assume for now that filter is invertible.",
            "So this is clearly with.",
            "Multiple kernels so they all satisfy.",
            "Basically IDK equals Delta or a shift of Delta, so you can get different shifts.",
            "Of course, if you don't have Delta, if you have data center, Dizzy Wright at one or two, you'll get the exact same kurtosis, right?",
            "But those are the only stationary points of this algorithm.",
            "That is, you're guaranteed to find not even the global optimum.",
            "Any local optimum will either give you the correct T or a shift of E. Which is really getting much stronger than what we can prove using our algorithm."
        ],
        [
            "So here's a proof by pictures.",
            "Here's the original signal.",
            "Here it's histogram, Christos is 26, we blur.",
            "It is what I showed earlier.",
            "We get things that are more Gaussian.",
            "Kurtosis is 5.",
            "The point is that we apply a wrong E to this, so we get something here and that gives us a clue to 614 which not coincidentally, is between 5 and 26, right?",
            "No matter what we do, when we apply a blur function to, this will get a number between 14 and 26.",
            "And the maximum will be when we have the right E will get 26.",
            "Any other inverse filter will give us a number that's less than 26.",
            "So by maximizing the kurtosis, we're guaranteed to find the right thing.",
            "Yeah.",
            "Are you giving us happy with averaging a low pass filters and blurs in this sort of thing?",
            "Does approval for all convolution filters.",
            "Yes, it doesn't matter.",
            "Actually this is a very confusing thing, but if you do a 1 -- 1 filter.",
            "In terms of the central limit theorem, it's exactly the same thing, right?",
            "You don't have to average it's long.",
            "Take a linear combination of independent random variables.",
            "You end up getting.",
            "Gaussian things coming up.",
            "The crypto SIS approaches to criticism.",
            "Yeah, so it has nothing to do with low pass and high pass."
        ],
        [
            "OK.",
            "So let me just summarize.",
            "What we learned by looking at this literature from communication theory.",
            "So basically the algorithm is very very simple.",
            "You don't need to know whether the original signal, sub Gaussian or Super Gaussian.",
            "It has a universal proof of correctness.",
            "If you ever read a paper in IEEE Transactions on information theory, I think the most common word is universality depends, I guess, on the years in which you read information theorists are very keen on universality, which means that these are algorithms that work regardless of the distribution over X.",
            "You don't need to know the foreign P of X as opposed to sort of Bayesian approach where we try to model very well.",
            "Although prior distributions, universal algorithms are supposed to work for any.",
            "Prior distribution and this is an example of such an algorithm.",
            "Of course you don't need to know P of X you do.",
            "You do need to assume IID.",
            "And you need to know just a single bit is a sub Gaussian super Gaussian.",
            "But given that you know these things, you don't need to know the prior or you don't need to know prior, even approximately.",
            "You just need to know that it's IID.",
            "I mentioned this earlier.",
            "It has this global convergence property, which is much stronger than what we can prove even though they use very simple iterative algorithms, they can show that any stationary pointer algorithm is correct.",
            "There are no no local Optima, and you know I talked a lot about math and proofs, but I think the main thing to notice is that this algorithm is used in millions of devices, so you know what this is.",
            "This is a cable set top box.",
            "So they all cable set top boxes apparently use Goddards algorithm from the 70.",
            "So it's been used in radio transmissions in the 80s.",
            "Cable set top box in the 90s, wireless communication, so the Wi-Fi standards, the Wimax standards.",
            "All of them have blind deconvolution inside the standards and some of them use this algorithms and for me this is so we can ask questions about it.",
            "Non variability, numerical stability, noise.",
            "But the fact that it works in millions of devices means we should pay attention to it and try to see can we use this for blind image deblurring?"
        ],
        [
            "OK, so here's an insight that actually appears in the Ferguson all paper from 2006.",
            "But if you look at the derivatives, so again the strong assumption in communication theory is that the individual X is are independent, and that's certainly not true for pixels.",
            "Pixels are not IID.",
            "But if you look at derivatives and they're not exactly, I'd either, but they're more independent, but it is true that when you blur an image, the kurtosis approaches that of a Gaussian.",
            "So here's the image we saw earlier.",
            "It's histogram looks like this.",
            "It's heavy tailed that produces 20.",
            "Here's the histogram after blurring where the green is the original histogram.",
            "So basically blur it again, this is camera shake.",
            "It's not necessarily uniform blur.",
            "ORENCIA form, it ends up decreasing.",
            "The tails and making things more Gaussian so the kurtosis went from 20 to 17.",
            "So indeed things become more Gaussian."
        ],
        [
            "So this suggests a new class of blind deblurring algorithms.",
            "Where now we're going to get this blurred images input.",
            "And we're going to search for X&K such that Y is when we blur X with K, we get Y and we make X is non Gaussian as possible.",
            "And again, the assumption is blur makes the derivatives more Gaussian.",
            "If we search for the K so that the universe is non Gaussian as possible.",
            "This should if the derivatives are IID, this algorithm will probably find the blur kernel, and this Matias goes.",
            "It goes back to your question.",
            "This particular definition of the algorithm doesn't require the inverse anymore.",
            "We solve specifically for X&K and not for the inverse.",
            "So how?"
        ],
        [
            "We measure non gaussianity mentioning the crypto sis since this is nips we have a long history in this community of different measures of non gaussianity.",
            "Very nice papers.",
            "By varnon.",
            "All these people who worked on ICA.",
            "Could you basically kurtosis just one form of a normalized moment?",
            "You take the signal divided by standard deviation and measure the 4th moment.",
            "That's the kurtosis.",
            "But actually, what environment makes the points?",
            "That's not really very good measure of non gaussianity, because the 4th moment is very sensitive to outliers.",
            "You're better off actually using an Alpha that's less than one.",
            "So you take the normalized moment with an Alpha less than one.",
            "And that also acts as a measure of non gaussianity.",
            "But it has a different.",
            "Different directionality, so the crypto SIS is very large for these sparse distributions, and decreases.",
            "For Gaussian distribution.",
            "These are log probabilities, so this is the Gaussian quadratic.",
            "This is the Laplacian.",
            "This if there is more sparse distributions and the purchase goes from 25 down to three if you use Alpha less than one.",
            "I think here used Alpha equal to 1/2 it goes in the opposite direction, so the sparsest solutions have low normalized moments, and it increases as you approach the Gaussian.",
            "So we can either minimize this normalized moment, or maximize this normalized moment.",
            "So I'm going to choose to minimize this normalized moment and you'll show you'll see that that gives an algorithm that's very interesting to look at, so we minimize the normalized moment of X and want to search, because that will make X as non Gaussian as possible."
        ],
        [
            "OK, so here's the algorithm we're given Y, which is a blurred image.",
            "We look at assistant.",
            "We look at the histograms at the derivatives, not not the original image.",
            "Why is our derivative the derivative?",
            "Are blurred image here that histogram and we search for an X in the K such that when we convolve X with K we get Y and we also want to make X is non Gaussian as possible?",
            "I make exit non Gaussian as possible so we minimize absolute.",
            "The 1/2 moment of X but after it's been normalized normalized moment and this algorithm is guaranteed to find the correct blur kernel if the derivatives were IID.",
            "Now this looks very familiar."
        ],
        [
            "Sure.",
            "If you look at this algorithm where you minimize the sparsity, the 1/2 moment of X.",
            "Plus this term, these are these map global map algorithms that do map both an X on K. So this algorithm is guaranteed to fail.",
            "It's."
        ],
        [
            "This algorithm is guaranteed to succeed."
        ],
        [
            "Guaranteed to fail, guaranteed."
        ],
        [
            "Succeed.",
            "What's the difference this line over here?",
            "If you to minimize to maximize the non gaussianity, you want to minimize the normalized moments.",
            "If you just, I'm sorry.",
            "I'm too many buttons here to press."
        ],
        [
            "If you just if you just minimize the moments, that's not a good idea.",
            "That doesn't minimize non gaussianity because it doesn't have a standard deviation here.",
            "You can always minimize this one half moment, the unnormalized one, by sending the distribution to 0, having all points go to zero.",
            "But if you."
        ],
        [
            "Optimize the normalized moment and the variance is always.",
            "Fix this minimize.",
            "This actually looks at the shape of the distribution.",
            "Makes it as long as possible.",
            "Yes, go ahead.",
            "For all my problems when you actually try to solve it in both X&K, you're guaranteed to fail.",
            "Yes, we can always take this and think of it as a certain prior in a map formulation.",
            "Yes, I think this is.",
            "This is true when I say guaranteed you cannot."
        ],
        [
            "Always.",
            "This does not have another prior on K. This, particularly the one that is guaranteed to fail, is the one where you have no prior on K or noninformative prior on K or sparsity prior on K. There are ways to put very complicated priors on K that will make less basically with the prospect approximation.",
            "Does that make this guaranteed to succeed after all?",
            "But those are not.",
            "I wouldn't call them priors over K because they don't really correspond to what we think happens with real world relationals.",
            "It's just a hack to get this thing to.",
            "To succeed, but this algorithm, as is basically what we showed in our 2009 paper, the global minimum of this is always going to be the Delta kernel.",
            "That's the main point.",
            "Pencil, the other one exactly which is highly difficult.",
            "Then you're guaranteed to succeed.",
            "It's a very very."
        ],
        [
            "Wait this one.",
            "Yes, this is guaranteed to succeed.",
            "If you find the global optimal, that's correct."
        ],
        [
            "Where is this going?"
        ],
        [
            "Fail.",
            "OK, so is this a new algorithm?",
            "Well, it's not that knew it.",
            "Of course it's not you because it's we're just importing an algorithm that's been around from the 80s, but is it a new algorithm in image processing?",
            "Well, no.",
            "So if you look at this very nice paper by Dilip and Rob from this year, they proposed minimizing what they call normalized sparsity, which is the minimum over X&K the blur constraint plus the L1 over the L2 norm of the signal.",
            "Plus some prior over K and this is exact.",
            "Can be seen as minimizing non gaussianity.",
            "That's not the way they derived it, but we can think of it as if you think of this over this as X bar.",
            "This is the minimizing non gauss maximizing non gaussianity of X.",
            "So that can be seen as a special case of this class of algorithm."
        ],
        [
            "I think the more interesting thing are these very mysterious algorithms.",
            "Witcher map XC with bilateral filtering algorithms?",
            "So this is a graph Asia Paper from 2009 and you know, I just refuse to believe that this algorithm would work.",
            "Then I visited tubing, game and Highland Steven was saying they were using this algorithm actually works, so let me explain how it works.",
            "It's a lot like the map excal rhythms we solve for X offer case.",
            "Offer Excel for K and so if you just iterated these two steps, the algorithm guaranteed to fail.",
            "But they have this extra step where after solving for the hidden image, so here's your predicted image.",
            "Here you solve in this case.",
            "This is for new software.",
            "The blur you get a sharp image.",
            "You go back here and if you were to go to here, that would be the standard algorithm.",
            "Sharp image blurring will sharp image blur Trimble here what they have is another step where you take your predicted sharp image, applied bilateral filter to it.",
            "An by zeroing out the low gradients, increasing the high gradients, applying shock filters, all sorts of things that are very difficult to understand where they come from, and that's crucial.",
            "This is what they told me when I visited.",
            "Again, if you take away this step, it doesn't work anymore.",
            "But still, you know nobody really knows what this algorithm is doing when you iterate it.",
            "Many times will happen and there is a way to see these algorithms that actually Max maximizing the non gaussianity of X, because again, this step of taking X and going to X bar is similar to requiring that.",
            "That we keep the variance of X constant.",
            "It's not exactly the same, but there was a way to write algorithms very similar to this one that is guaranteed to succeed, and this is an insight we get by looking at these old algorithm."
        ],
        [
            "OK."
        ],
        [
            "So what is this bias?",
            "The point of this talk is trying to link what's already known for the people who make modems to what we can do with room camera shake.",
            "So obviously that allows us to understand some recent algorithms.",
            "So for example, the Krishnan Fergus algorithm didn't have, I think, a proof.",
            "Of correctness or maybe did have under some conditions, this generalizes the conditions under which we can prove.",
            "That is, if this, if the derivatives are really IID, then we can show that that algorithm will succeed.",
            "But that's a big if when I just said it.",
            "Filters are not really IID and suggest ways to improve them.",
            "So for example, if we replace the one simple derivative filters with filters that look more like ICA, this predicts that will do a better job because the filters will be more independent.",
            "And it also suggests looking at iterative algorithms that people in communication theory or familiar with that have this global convergence.",
            "I don't know if any computer vision or image processing algorithm that has these global convergence guarantees would be great if we could have."
        ],
        [
            "OK, so I'm done at the title of the talk is old and new algorithms for blind deconvolution.",
            "So the new algorithms are typically math algorithms.",
            "I repeated what we showed a few years ago.",
            "Some of these algorithms are guaranteed to fail, but they still work even when they're not supposed to.",
            "The old algorithms and communication theory usually are not math algorithms.",
            "They have universal guarantees, global convergence, and I think the most important thing they used in millions of devices, probably as in this room we probably have many devices that use some variant of this algorithm.",
            "And I think this connection can help us understand and improve image blurring algorithms.",
            "Thank you.",
            "So Matthias was mentioning the fact that a lot of people don't use the variational methods because they think they're very complicated to optimize, and I think that's one reason people use these Matic scale rhythms are guaranteed to fail because they're very simple to implement and to generalize, and Matthias was referring to papers that we had last year where we try to really do a very tutorial version of the variational method.",
            "And show that it's really not very difficult, doesn't quite a lot of computation, but I don't think that's been a great success that most people I talked to will still rather use.",
            "Unfortunately the mapics cases so simple.",
            "Also, I see Joseph is here.",
            "If you look at the amount of work that Oliver and Joseph had to do to derive the variational approximation in.",
            "It's not just programming work, I'm saying it's also a lot of mathematics often to get the variational things to work.",
            "There's lots of equations.",
            "And unfortunately people don't like to use it.",
            "These algorithms don't have any of these integration.",
            "I mean system in global optimization.",
            "Sorry this one.",
            "So I think these are much more likely to be used.",
            "By other people, because there's no integrals here, it's just optimization.",
            "There is a normalization step which makes the optimization a little bit more difficult, but again, there's so much work that people have figured out how to optimize these things.",
            "Like that I think.",
            "I think I'm much more optimistic people use this than the variational methods.",
            "On the left is might work better.",
            "This is a different question.",
            "I mean this has guarantees of success under IID assumptions.",
            "Innovational methods don't.",
            "Necessarily those ID assumptions.",
            "So I repeat the question if we literally Stephen was saying if we literally translate the.",
            "The communication theory algorithms.",
            "Where are they?",
            "They often software the equalizer."
        ],
        [
            "And there's a question, whether in blind convolution for images, whether this equalizer will be will make sense.",
            "So we've done a little bit of experience on this.",
            "You can often find very good equalizers that are compact.",
            "Even for realistic blurs.",
            "And there has been a lot of work in the communication theory world saying what happens if you place the true E with E. That's guarantee that's a compact.",
            "And you can still prove correctness under certain cases there, so I'm less worried about that.",
            "I'm more worried about what Matthias question is sometimes.",
            "You know, there are frequencies that were zeroed out, and he doesn't really exist, and so you have to replace it, maybe with the leader filter, and then whether the math the math is more tricky.",
            "I'm not totally sure that that algorithm we can prove the same things about, but we did try.",
            "If you take just a blur in this database that are not as online, she has a number of.",
            "Camera shake blur trails only found you can do a very good job of finding a compact equalizer for those.",
            "OK, let me just repeat the question.",
            "So again the mystery is of this map.",
            "Thing is, even though we wrote this paper saying it can't possibly work.",
            "It's used a lot so many people run it where is."
        ],
        [
            "Yeah, this algorithm.",
            "So if you globally optimize this, we show that's guaranteed to fail, and yet there's been a number of papers who use it, and the question was why does it work?",
            "Well, one option is people stop it before they find the global optimum, and the other option is local minima and both have.",
            "But both of these answers are correct.",
            "That is, there is often a local minimum or a saddle point around the correct X.",
            "So if you optimize only in certain directions and you're lucky not to go in the other directions.",
            "That will work.",
            "And it's also true that people always stop this early.",
            "Yes, I'm wondering what.",
            "Noise in the snow.",
            "Gosh, entity things.",
            "Is that an issue?",
            "Yeah, so actually if the noise is Gaussian, which is the typical assumption that just makes things even more Gaussian.",
            "So in the original shelving Weinstein paper, they prove that will also work under gas additive Gaussian noise.",
            "But again, I think the most impressive thing is it works in modems that have noise.",
            "It's not a mathematical, so it must be work with noise.",
            "Yes, no, no, this is a very long repeat the question.",
            "That's a very interesting question.",
            "He tried to make the image is non Gaussian as possible and we felt was asking well, can't you make it too sharp can't you over sharpen it?",
            "And there's a proof here that you can do it with a linear.",
            "That's the nice thing with a linear equalizer.",
            "As long as you restrict yourself to linear equalizers, you can."
        ],
        [
            "Would be much more non Gaussian in the original image.",
            "So that's very."
        ],
        [
            "You can keep cranking it up and you'll never overshoot, unlike many other algorithms.",
            "And again, it's the central limit."
        ],
        [
            "Here on the.",
            "Any other thing as long as you play linear filter, any other thing even it's very high pass, it will be more Gaussian in the original thing, so maximizing non gaussianity will always find you the correct thing.",
            "It's a good question since it's based on the Central Limit theorem.",
            "How many samples do you need in order to?",
            "Prove that it moves, yeah, so let me repeat the question.",
            "I keep mentioning the central Limit Theorem, which says that things become Gaussian and the natural questions.",
            "Well, maybe this needs very big blur kernels for the central Limit theorem can.",
            "I was just using Central Limit Theorem as shorthand.",
            "The proof is actually for any blur.",
            "Even of size 2.",
            "So blur monotonically sort of makes you more Gaussian.",
            "Infinite Blur will make you Gaussian, but the kurtosis becomes closer to the process of a Gaussian.",
            "Even with just a tap filter.",
            "That's what they actually showed in the paper.",
            "I didn't want to go into that proof, but even with the two tap filter, you already guaranteed to be more Gaussian than the original 1.",
            "If you have a very large number of taps will become Gaussian, but with two taps already become organizing.",
            "The question was whether.",
            "Whether there's a connection so that these new algorithms."
        ],
        [
            "The old new algorithms, whatever.",
            "Basically add another term or another constraint.",
            "You instead of measuring the moment of actually measure the moment of X divided by standard deviation, and so they look a lot like this and the question was whether this extra term maybe could be understood as some some approximation that gets stored map Ki.",
            "Think that's a great question, I don't know, but I would like to emphasize that the guarantees of correctness are different.",
            "The map K are proof that will succeed.",
            "Assumes that X samples from the prior.",
            "Like all Bayesian methods, if X if you don't have the prior correct, you're not guaranteed to succeed anymore.",
            "These newer algorithms, which are the old algorithms, don't require any knowledge of their universal, they just require the IID assumption.",
            "So even if we made that connection, there's something stronger from these other way of thinking.",
            "OK, great thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "This is work.",
                    "label": 0
                },
                {
                    "sent": "Steven was kind enough to mention a lot of the work I've done recently in blind convolution on solve that work is done by a not as is this work.",
                    "label": 0
                },
                {
                    "sent": "But one more person I'd like to thank here is a mayor Feder who is a professor of information.",
                    "label": 0
                },
                {
                    "sent": "Tell him University and in some sense this talk is an argument for coffee breaks.",
                    "label": 0
                },
                {
                    "sent": "I think it's better to leave the door open because people were complaining about oxygen.",
                    "label": 0
                },
                {
                    "sent": "So basically I was on a several committees with the.",
                    "label": 0
                },
                {
                    "sent": "With mayor and during the coffee breaks we started talking about what each of us were doing, and.",
                    "label": 0
                },
                {
                    "sent": "He pointed out to me that there's a lot of interesting work on blind deconvolution done in the communication theory world and that we should really take a look at it and essentially that's what the bottom line of this talk is.",
                    "label": 0
                },
                {
                    "sent": "We should really take a look at it so it's more as the previous speaker said, the computer vision people should like the image processing people, but there's even a broader community out there that we should all be familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so why do?",
                    "label": 0
                },
                {
                    "sent": "Why do I think it's great to work on blind convolution?",
                    "label": 0
                },
                {
                    "sent": "It's because it's like all my favorite problems.",
                    "label": 0
                },
                {
                    "sent": "It's Anil post problem.",
                    "label": 0
                },
                {
                    "sent": "You're given the single image why I need to reconstruct you 2 images.",
                    "label": 0
                },
                {
                    "sent": "So there's an infinite number of possible solutions and that's what makes it an interesting problem to work on.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Despite the fact that this is a little post problem, in recent years we've been able to solve it, which is really quite impressive.",
                    "label": 0
                },
                {
                    "sent": "And again, if you look sort of at the history, it's true that people in all sorts of communities have looked at blind deconvolution over the years.",
                    "label": 0
                },
                {
                    "sent": "I think for me, really, starting with Rob's work in 2006, the amazing thing is to actually get this to work on real images.",
                    "label": 0
                },
                {
                    "sent": "Real camera shakes even though you know there's no problem.",
                    "label": 0
                },
                {
                    "sent": "We're just making up these priors that are may or may not be correct, but you know, lots of different methods Now can take an image like this.",
                    "label": 0
                },
                {
                    "sent": "And come out with the Blur kernel, which is what's shown here in the insets, and with the sharp image over here.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Just many people sitting in this room, but I think it's it's good to sort of sit back and reflect on the fact that we can actually solve this.",
                    "label": 0
                },
                {
                    "sent": "I think it's quite an achievement.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Almost all of these methods, and also I guess the methods mentioned the previous talk are basically Bayesian message with the sparse prior, so we know that if we look at the derivatives of natural image and we plotted histogram, it's going to have this non Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Heavy tail distribution, often modeled by penalising for the absolute value of derivative race to the Alpha, where Alpha is a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Is a 70 norms that were mentioned earlier?",
                    "label": 0
                },
                {
                    "sent": "And that gives us the joint distribution of both the image unknown image X and the unknown burchenal K given the observed image Y.",
                    "label": 0
                },
                {
                    "sent": "And given this joint distribution, there are two directions 1 could go.",
                    "label": 0
                },
                {
                    "sent": "You could try to do global map, find the latent image X and the unknown filter K which maximizes this posterior probability of there should be a - here, or minimizes this energy function over here.",
                    "label": 0
                },
                {
                    "sent": "Night and I and billing for Adobe Reader paper for years ago, pointing out that this is guaranteed to fail.",
                    "label": 0
                },
                {
                    "sent": "That is that if you actually.",
                    "label": 0
                },
                {
                    "sent": "Find the global optimum of this energy function.",
                    "label": 0
                },
                {
                    "sent": "In blind image deconvolution you always get the wrong blur kernel.",
                    "label": 0
                },
                {
                    "sent": "Anything is, we thought by writing this paper, we would stop people using it, but it hasn't all happen.",
                    "label": 0
                },
                {
                    "sent": "In fact, it often works well in practice if you don't keep iterating it.",
                    "label": 1
                },
                {
                    "sent": "If you stop the iterations and you don't find the global optimum, this is a surprisingly sort of an embarrassingly successful method.",
                    "label": 0
                },
                {
                    "sent": "And I'll say a few words.",
                    "label": 0
                },
                {
                    "sent": "I think our current understanding of why that is, but the algorithm that is guaranteed to succeed is when you integrate over the latent image and just maximize over P of XP of K. So you find the most likely derivative most likely filter blur kernel while integrating over X, and that it's a very simple proof to show if your images are sampled from a prior and everything is all you're basing assumptions are correct.",
                    "label": 0
                },
                {
                    "sent": "This will succeed this, on the other hand.",
                    "label": 1
                },
                {
                    "sent": "Even if all your assumptions are correct is guaranteed to fail if you globally optimize it.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was a very.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduction For where we are in computational photography.",
                    "label": 0
                },
                {
                    "sent": "And now I want to talk about the consequences of our coffee break.",
                    "label": 0
                },
                {
                    "sent": "We're following the suggestion, familiar Feder.",
                    "label": 0
                },
                {
                    "sent": "We went and read a little bit about what people in communication systems do.",
                    "label": 1
                },
                {
                    "sent": "I think the most interesting thing is that the vast majority of their approaches are not map approaches.",
                    "label": 0
                },
                {
                    "sent": "They're not Bayesian approaches.",
                    "label": 0
                },
                {
                    "sent": "These different approach.",
                    "label": 0
                },
                {
                    "sent": "And they have us in some sense stronger correctness guarantees than we have.",
                    "label": 0
                },
                {
                    "sent": "They can prove things or will find correct blur kernel under stronger conditions, weaker conditions, more general conditions.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about the my main message, the.",
                    "label": 0
                },
                {
                    "sent": "A corollary of this is that actually some of these math XK algorithms, the ones that, as I mentioned earlier, are guaranteed to fail, can be thought of as doing an approximation to these old algorithms, and that can explain some of their success.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I don't know how many people have seen this.",
                    "label": 0
                },
                {
                    "sent": "This is a 56 K modem.",
                    "label": 0
                },
                {
                    "sent": "You know when you turned it on in the old days and having people ever had dial-up modems, there would be this.",
                    "label": 0
                },
                {
                    "sent": "I since we're being filmed, I won't try to reproduce the signal here, but there do this very pleasant sound where the modem would emit a synchronization tone.",
                    "label": 0
                },
                {
                    "sent": "And why is it doing that?",
                    "label": 0
                },
                {
                    "sent": "It's the solver deconvolution problem, right?",
                    "label": 0
                },
                {
                    "sent": "So when the modem transmits information over a phone line, or this is of course true for any communication system.",
                    "label": 0
                },
                {
                    "sent": "If you transmit X of T, what you receive on the other side is.",
                    "label": 0
                },
                {
                    "sent": "Linear combination of X at different delays where you know the signal reverberates through the medium and basically the received signal.",
                    "label": 1
                },
                {
                    "sent": "Y is a convolution of X with an unknown blur kernel, so it's exactly the same problem that we face in camera shake.",
                    "label": 0
                },
                {
                    "sent": "So how do you solve it again in the one version is again this synchronization sequence basically is a known X, so the modem sends out a particular tone that the receiver knows, and then the receiver just has to solve, and this is the LMS rule that many of us learned in our neural network class.",
                    "label": 0
                },
                {
                    "sent": "Is an iterative algorithm for finding the blur kernel when X&Y are both known.",
                    "label": 0
                },
                {
                    "sent": "And that's good for short communications, but if you have a very long communication.",
                    "label": 0
                },
                {
                    "sent": "Setting the Blur kernel will change overtime, so sending the communication the synchronization signal in the beginning is not going to give you the right blur kernel because it keeps changing overtime.",
                    "label": 0
                },
                {
                    "sent": "And so here and so there's been a lot of work in this field on blind deconvolution because you want to re estimate this blur kernel automatically during the long communication protocol.",
                    "label": 0
                },
                {
                    "sent": "Here these are two papers that I think everyone all of us should read.",
                    "label": 0
                },
                {
                    "sent": "One thing you can't see the font, but this is 1980 IEEE Trans.",
                    "label": 0
                },
                {
                    "sent": "Transaction communications in 1980, which does blind deconvolution.",
                    "label": 1
                },
                {
                    "sent": "And this is actually, I think this is from 1998 by shelving Weinstein, which is mostly what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "This paper reinterprets this old algorithm in the way that I found it easier to understand and has more generalizations.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do these classical algorithms work?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And really, the mathematics behind it is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's based on the central limit theorem.",
                    "label": 1
                },
                {
                    "sent": "And So what they?",
                    "label": 0
                },
                {
                    "sent": "What they point out?",
                    "label": 1
                },
                {
                    "sent": "This is again the Shelvy and Weinstein.",
                    "label": 0
                },
                {
                    "sent": "Exposition if you take here, I'm showing communication signals.",
                    "label": 0
                },
                {
                    "sent": "Of course, for 1D signals, but since I'm so used to looking at images, I'm going to show them as an image.",
                    "label": 0
                },
                {
                    "sent": "So think of this as time goes like this as scanlines.",
                    "label": 0
                },
                {
                    "sent": "So here's our original signal states a bit.",
                    "label": 0
                },
                {
                    "sent": "It's either A plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "Histogram will look like this when we blur it.",
                    "label": 0
                },
                {
                    "sent": "So what is blur?",
                    "label": 0
                },
                {
                    "sent": "Do really right blur?",
                    "label": 0
                },
                {
                    "sent": "Does linear combination every pixel here or every received symbol is a linear combination of these IID signals.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean about the distribution of Y?",
                    "label": 0
                },
                {
                    "sent": "It should be Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "Again, these are independent random variables.",
                    "label": 0
                },
                {
                    "sent": "Every bit is independent.",
                    "label": 0
                },
                {
                    "sent": "That's the crucial assumption.",
                    "label": 0
                },
                {
                    "sent": "So every bit here is independent, so it has this histogram of plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "Here every every received.",
                    "label": 0
                },
                {
                    "sent": "Vector is a linear combination of these independent random variables, so it's going to look like a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It's not, of course, it will look like a Gaussian if the bertran were infinitely big.",
                    "label": 0
                },
                {
                    "sent": "And that's not really the case that we're interested in, but it will always be more Gaussian than this.",
                    "label": 0
                },
                {
                    "sent": "And you can measure gaussianity as many of us in this room know by looking at the crew ptosis.",
                    "label": 0
                },
                {
                    "sent": "So here the kurtosis was one.",
                    "label": 0
                },
                {
                    "sent": "This was a + 1 -- 1 signal and the crisis of the Gaussian is 3 and this signal is not exactly Gaussian, but it's kurtosis approaches three.",
                    "label": 0
                },
                {
                    "sent": "This is the standard definition proptosis.",
                    "label": 0
                },
                {
                    "sent": "We divide the signal by standard deviation and measure the 4th moment, and if it's a Gaussian, we get three out.",
                    "label": 0
                },
                {
                    "sent": "And if these are sort of sub Gaussian signals, their kurtosis was less than one to begin with.",
                    "label": 0
                },
                {
                    "sent": "So after blur they become, their kurtosis increases and it becomes more Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a different kind of communication signal which has this super Gaussian behavior, so it's a sparse signal everyone most of the time the input is close to zero and every once in awhile we get a sharp peak around 0, so it's Christos is 26.",
                    "label": 0
                },
                {
                    "sent": "When we blur it again, what happens?",
                    "label": 0
                },
                {
                    "sent": "We're averaging independent random variables.",
                    "label": 0
                },
                {
                    "sent": "That makes things more Gaussian, and we don't go all the way to three.",
                    "label": 0
                },
                {
                    "sent": "We go to five here, but it's always the case that the Blur signal will be more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then the original signal I'm really that, all that is, is the central limit theorem.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this gives us a very simple algorithm for blind deconvolution.",
                    "label": 0
                },
                {
                    "sent": "This should be 1990.",
                    "label": 0
                },
                {
                    "sent": "And then here's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I can just really write it down very simply.",
                    "label": 0
                },
                {
                    "sent": "So one thing is, we're not going to solve for K. The Unblocker will suffer the inverse filter.",
                    "label": 1
                },
                {
                    "sent": "The equalizer they call it, so E is a filter that when we it's the inverse of can we apply it to our blurred image or alert signal we get back the original thing.",
                    "label": 0
                },
                {
                    "sent": "And the only thing again compared to what we were talking about in the previous talk, and almost all the work in computational photography, we need to know a lot about the prior distribution over our signals in blindly in computational photography.",
                    "label": 0
                },
                {
                    "sent": "Here we don't need to know that all we just need to know a sign we need to know is the original kurtosis less than three or greater than three.",
                    "label": 0
                },
                {
                    "sent": "If it's less than three, like in the plus 1 -- 1 signal that I showed you earlier, what you need to do is just.",
                    "label": 0
                },
                {
                    "sent": "Find equalizer such that when you convolve Y with that equalizer, the kurtosis is maximal.",
                    "label": 0
                },
                {
                    "sent": "And if it were Super Gaussian, sorry here.",
                    "label": 0
                },
                {
                    "sent": "Should be minimal here.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the kurtosis and he want to maximize the kurtosis.",
                    "label": 0
                },
                {
                    "sent": "And I'll show this in picture why that is.",
                    "label": 0
                },
                {
                    "sent": "But if the original signal had a kurtosis that was less than the Galaxy, and then what we want to do is minimize the kurtosis as much as possible.",
                    "label": 0
                },
                {
                    "sent": "If the original signal had a crisis that was greater than that of a Gaussian, wanna maximize the proptosis?",
                    "label": 0
                },
                {
                    "sent": "And this very simple algorithm comes with a very strong guarantee that it's guaranteed to find the correct.",
                    "label": 0
                },
                {
                    "sent": "As long as the X is where ID this is guaranteed to find the correct inverse filter and there are no local minima.",
                    "label": 1
                },
                {
                    "sent": "Again, this is much stronger than what we can usually show for our blind convolution algorithms, where before lucky we find the criterion with global optimum gives the right lower kernel.",
                    "label": 0
                },
                {
                    "sent": "Here it's even not only that is correct, but there are no local Optima either.",
                    "label": 0
                },
                {
                    "sent": "Any stationary point of the kurtosis.",
                    "label": 0
                },
                {
                    "sent": "Is going to give you the correct one and the proof is really 3 lines.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will do it.",
                    "label": 0
                },
                {
                    "sent": "Remember, the main thing to remember is a central limit theorem that says that blur makes signals more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the Super Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "So if our original signal had a kurtosis greater than three, the algorithm just says find an equalizer so that when we apply to our blurred signal, the proptosis is maximum and what we want to show is that that will only will be maximal if and only if K is the correct East is the inverse of K. OK, so how do we show it?",
                    "label": 0
                },
                {
                    "sent": "Here?",
                    "label": 0
                },
                {
                    "sent": "Is our D blurred image.",
                    "label": 0
                },
                {
                    "sent": "I keep saying image, but it's a signal.",
                    "label": 0
                },
                {
                    "sent": "Here is our deep alert signal X star.",
                    "label": 0
                },
                {
                    "sent": "It's a convolution of E with with Y&Y is K * X.",
                    "label": 0
                },
                {
                    "sent": "So basically our hypothesized X is the convolution of something with the original signal.",
                    "label": 0
                },
                {
                    "sent": "This original signal is IID, so any convolution of it is always going to be more Gaussian than the original X.",
                    "label": 1
                },
                {
                    "sent": "The only way you could maximize this quantity is if Y times if he is the inverse of K and you get a Delta function.",
                    "label": 0
                },
                {
                    "sent": "So X equal to X and the other thing you'll always have kurtosis less than X, yes?",
                    "label": 0
                },
                {
                    "sent": "Support this evening system.",
                    "label": 0
                },
                {
                    "sent": "Let me repeat the question so much.",
                    "label": 0
                },
                {
                    "sent": "Yes is asking.",
                    "label": 0
                },
                {
                    "sent": "What happens if he does not exist?",
                    "label": 0
                },
                {
                    "sent": "If the inverse kernel does not exist, and that's a good question.",
                    "label": 0
                },
                {
                    "sent": "The simple proof assumes that this that the inverse kernel exists there are there are generalizations where you don't need to assume that the filter is invertible, but we're going to assume for now that filter is invertible.",
                    "label": 0
                },
                {
                    "sent": "So this is clearly with.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernels so they all satisfy.",
                    "label": 0
                },
                {
                    "sent": "Basically IDK equals Delta or a shift of Delta, so you can get different shifts.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you don't have Delta, if you have data center, Dizzy Wright at one or two, you'll get the exact same kurtosis, right?",
                    "label": 0
                },
                {
                    "sent": "But those are the only stationary points of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is, you're guaranteed to find not even the global optimum.",
                    "label": 0
                },
                {
                    "sent": "Any local optimum will either give you the correct T or a shift of E. Which is really getting much stronger than what we can prove using our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a proof by pictures.",
                    "label": 1
                },
                {
                    "sent": "Here's the original signal.",
                    "label": 0
                },
                {
                    "sent": "Here it's histogram, Christos is 26, we blur.",
                    "label": 0
                },
                {
                    "sent": "It is what I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "We get things that are more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Kurtosis is 5.",
                    "label": 0
                },
                {
                    "sent": "The point is that we apply a wrong E to this, so we get something here and that gives us a clue to 614 which not coincidentally, is between 5 and 26, right?",
                    "label": 0
                },
                {
                    "sent": "No matter what we do, when we apply a blur function to, this will get a number between 14 and 26.",
                    "label": 0
                },
                {
                    "sent": "And the maximum will be when we have the right E will get 26.",
                    "label": 0
                },
                {
                    "sent": "Any other inverse filter will give us a number that's less than 26.",
                    "label": 0
                },
                {
                    "sent": "So by maximizing the kurtosis, we're guaranteed to find the right thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Are you giving us happy with averaging a low pass filters and blurs in this sort of thing?",
                    "label": 0
                },
                {
                    "sent": "Does approval for all convolution filters.",
                    "label": 0
                },
                {
                    "sent": "Yes, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Actually this is a very confusing thing, but if you do a 1 -- 1 filter.",
                    "label": 0
                },
                {
                    "sent": "In terms of the central limit theorem, it's exactly the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "You don't have to average it's long.",
                    "label": 0
                },
                {
                    "sent": "Take a linear combination of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "You end up getting.",
                    "label": 0
                },
                {
                    "sent": "Gaussian things coming up.",
                    "label": 0
                },
                {
                    "sent": "The crypto SIS approaches to criticism.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it has nothing to do with low pass and high pass.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me just summarize.",
                    "label": 0
                },
                {
                    "sent": "What we learned by looking at this literature from communication theory.",
                    "label": 0
                },
                {
                    "sent": "So basically the algorithm is very very simple.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know whether the original signal, sub Gaussian or Super Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It has a universal proof of correctness.",
                    "label": 1
                },
                {
                    "sent": "If you ever read a paper in IEEE Transactions on information theory, I think the most common word is universality depends, I guess, on the years in which you read information theorists are very keen on universality, which means that these are algorithms that work regardless of the distribution over X.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know the foreign P of X as opposed to sort of Bayesian approach where we try to model very well.",
                    "label": 0
                },
                {
                    "sent": "Although prior distributions, universal algorithms are supposed to work for any.",
                    "label": 0
                },
                {
                    "sent": "Prior distribution and this is an example of such an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Of course you don't need to know P of X you do.",
                    "label": 1
                },
                {
                    "sent": "You do need to assume IID.",
                    "label": 0
                },
                {
                    "sent": "And you need to know just a single bit is a sub Gaussian super Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But given that you know these things, you don't need to know the prior or you don't need to know prior, even approximately.",
                    "label": 0
                },
                {
                    "sent": "You just need to know that it's IID.",
                    "label": 0
                },
                {
                    "sent": "I mentioned this earlier.",
                    "label": 0
                },
                {
                    "sent": "It has this global convergence property, which is much stronger than what we can prove even though they use very simple iterative algorithms, they can show that any stationary pointer algorithm is correct.",
                    "label": 0
                },
                {
                    "sent": "There are no no local Optima, and you know I talked a lot about math and proofs, but I think the main thing to notice is that this algorithm is used in millions of devices, so you know what this is.",
                    "label": 0
                },
                {
                    "sent": "This is a cable set top box.",
                    "label": 1
                },
                {
                    "sent": "So they all cable set top boxes apparently use Goddards algorithm from the 70.",
                    "label": 0
                },
                {
                    "sent": "So it's been used in radio transmissions in the 80s.",
                    "label": 0
                },
                {
                    "sent": "Cable set top box in the 90s, wireless communication, so the Wi-Fi standards, the Wimax standards.",
                    "label": 0
                },
                {
                    "sent": "All of them have blind deconvolution inside the standards and some of them use this algorithms and for me this is so we can ask questions about it.",
                    "label": 0
                },
                {
                    "sent": "Non variability, numerical stability, noise.",
                    "label": 0
                },
                {
                    "sent": "But the fact that it works in millions of devices means we should pay attention to it and try to see can we use this for blind image deblurring?",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's an insight that actually appears in the Ferguson all paper from 2006.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the derivatives, so again the strong assumption in communication theory is that the individual X is are independent, and that's certainly not true for pixels.",
                    "label": 0
                },
                {
                    "sent": "Pixels are not IID.",
                    "label": 0
                },
                {
                    "sent": "But if you look at derivatives and they're not exactly, I'd either, but they're more independent, but it is true that when you blur an image, the kurtosis approaches that of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So here's the image we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "It's histogram looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's heavy tailed that produces 20.",
                    "label": 0
                },
                {
                    "sent": "Here's the histogram after blurring where the green is the original histogram.",
                    "label": 0
                },
                {
                    "sent": "So basically blur it again, this is camera shake.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily uniform blur.",
                    "label": 0
                },
                {
                    "sent": "ORENCIA form, it ends up decreasing.",
                    "label": 0
                },
                {
                    "sent": "The tails and making things more Gaussian so the kurtosis went from 20 to 17.",
                    "label": 0
                },
                {
                    "sent": "So indeed things become more Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this suggests a new class of blind deblurring algorithms.",
                    "label": 0
                },
                {
                    "sent": "Where now we're going to get this blurred images input.",
                    "label": 0
                },
                {
                    "sent": "And we're going to search for X&K such that Y is when we blur X with K, we get Y and we make X is non Gaussian as possible.",
                    "label": 1
                },
                {
                    "sent": "And again, the assumption is blur makes the derivatives more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If we search for the K so that the universe is non Gaussian as possible.",
                    "label": 0
                },
                {
                    "sent": "This should if the derivatives are IID, this algorithm will probably find the blur kernel, and this Matias goes.",
                    "label": 1
                },
                {
                    "sent": "It goes back to your question.",
                    "label": 0
                },
                {
                    "sent": "This particular definition of the algorithm doesn't require the inverse anymore.",
                    "label": 0
                },
                {
                    "sent": "We solve specifically for X&K and not for the inverse.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We measure non gaussianity mentioning the crypto sis since this is nips we have a long history in this community of different measures of non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "Very nice papers.",
                    "label": 0
                },
                {
                    "sent": "By varnon.",
                    "label": 0
                },
                {
                    "sent": "All these people who worked on ICA.",
                    "label": 0
                },
                {
                    "sent": "Could you basically kurtosis just one form of a normalized moment?",
                    "label": 0
                },
                {
                    "sent": "You take the signal divided by standard deviation and measure the 4th moment.",
                    "label": 0
                },
                {
                    "sent": "That's the kurtosis.",
                    "label": 0
                },
                {
                    "sent": "But actually, what environment makes the points?",
                    "label": 0
                },
                {
                    "sent": "That's not really very good measure of non gaussianity, because the 4th moment is very sensitive to outliers.",
                    "label": 0
                },
                {
                    "sent": "You're better off actually using an Alpha that's less than one.",
                    "label": 0
                },
                {
                    "sent": "So you take the normalized moment with an Alpha less than one.",
                    "label": 0
                },
                {
                    "sent": "And that also acts as a measure of non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "But it has a different.",
                    "label": 0
                },
                {
                    "sent": "Different directionality, so the crypto SIS is very large for these sparse distributions, and decreases.",
                    "label": 0
                },
                {
                    "sent": "For Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "These are log probabilities, so this is the Gaussian quadratic.",
                    "label": 0
                },
                {
                    "sent": "This is the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "This if there is more sparse distributions and the purchase goes from 25 down to three if you use Alpha less than one.",
                    "label": 0
                },
                {
                    "sent": "I think here used Alpha equal to 1/2 it goes in the opposite direction, so the sparsest solutions have low normalized moments, and it increases as you approach the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So we can either minimize this normalized moment, or maximize this normalized moment.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to choose to minimize this normalized moment and you'll show you'll see that that gives an algorithm that's very interesting to look at, so we minimize the normalized moment of X and want to search, because that will make X as non Gaussian as possible.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the algorithm we're given Y, which is a blurred image.",
                    "label": 1
                },
                {
                    "sent": "We look at assistant.",
                    "label": 0
                },
                {
                    "sent": "We look at the histograms at the derivatives, not not the original image.",
                    "label": 0
                },
                {
                    "sent": "Why is our derivative the derivative?",
                    "label": 0
                },
                {
                    "sent": "Are blurred image here that histogram and we search for an X in the K such that when we convolve X with K we get Y and we also want to make X is non Gaussian as possible?",
                    "label": 0
                },
                {
                    "sent": "I make exit non Gaussian as possible so we minimize absolute.",
                    "label": 1
                },
                {
                    "sent": "The 1/2 moment of X but after it's been normalized normalized moment and this algorithm is guaranteed to find the correct blur kernel if the derivatives were IID.",
                    "label": 0
                },
                {
                    "sent": "Now this looks very familiar.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "If you look at this algorithm where you minimize the sparsity, the 1/2 moment of X.",
                    "label": 0
                },
                {
                    "sent": "Plus this term, these are these map global map algorithms that do map both an X on K. So this algorithm is guaranteed to fail.",
                    "label": 1
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This algorithm is guaranteed to succeed.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guaranteed to fail, guaranteed.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Succeed.",
                    "label": 0
                },
                {
                    "sent": "What's the difference this line over here?",
                    "label": 0
                },
                {
                    "sent": "If you to minimize to maximize the non gaussianity, you want to minimize the normalized moments.",
                    "label": 0
                },
                {
                    "sent": "If you just, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I'm too many buttons here to press.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you just if you just minimize the moments, that's not a good idea.",
                    "label": 0
                },
                {
                    "sent": "That doesn't minimize non gaussianity because it doesn't have a standard deviation here.",
                    "label": 0
                },
                {
                    "sent": "You can always minimize this one half moment, the unnormalized one, by sending the distribution to 0, having all points go to zero.",
                    "label": 0
                },
                {
                    "sent": "But if you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimize the normalized moment and the variance is always.",
                    "label": 0
                },
                {
                    "sent": "Fix this minimize.",
                    "label": 0
                },
                {
                    "sent": "This actually looks at the shape of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Makes it as long as possible.",
                    "label": 0
                },
                {
                    "sent": "Yes, go ahead.",
                    "label": 0
                },
                {
                    "sent": "For all my problems when you actually try to solve it in both X&K, you're guaranteed to fail.",
                    "label": 0
                },
                {
                    "sent": "Yes, we can always take this and think of it as a certain prior in a map formulation.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think this is.",
                    "label": 0
                },
                {
                    "sent": "This is true when I say guaranteed you cannot.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Always.",
                    "label": 0
                },
                {
                    "sent": "This does not have another prior on K. This, particularly the one that is guaranteed to fail, is the one where you have no prior on K or noninformative prior on K or sparsity prior on K. There are ways to put very complicated priors on K that will make less basically with the prospect approximation.",
                    "label": 0
                },
                {
                    "sent": "Does that make this guaranteed to succeed after all?",
                    "label": 0
                },
                {
                    "sent": "But those are not.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't call them priors over K because they don't really correspond to what we think happens with real world relationals.",
                    "label": 0
                },
                {
                    "sent": "It's just a hack to get this thing to.",
                    "label": 0
                },
                {
                    "sent": "To succeed, but this algorithm, as is basically what we showed in our 2009 paper, the global minimum of this is always going to be the Delta kernel.",
                    "label": 0
                },
                {
                    "sent": "That's the main point.",
                    "label": 0
                },
                {
                    "sent": "Pencil, the other one exactly which is highly difficult.",
                    "label": 0
                },
                {
                    "sent": "Then you're guaranteed to succeed.",
                    "label": 1
                },
                {
                    "sent": "It's a very very.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wait this one.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is guaranteed to succeed.",
                    "label": 1
                },
                {
                    "sent": "If you find the global optimal, that's correct.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where is this going?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fail.",
                    "label": 0
                },
                {
                    "sent": "OK, so is this a new algorithm?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not that knew it.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not you because it's we're just importing an algorithm that's been around from the 80s, but is it a new algorithm in image processing?",
                    "label": 0
                },
                {
                    "sent": "Well, no.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this very nice paper by Dilip and Rob from this year, they proposed minimizing what they call normalized sparsity, which is the minimum over X&K the blur constraint plus the L1 over the L2 norm of the signal.",
                    "label": 0
                },
                {
                    "sent": "Plus some prior over K and this is exact.",
                    "label": 0
                },
                {
                    "sent": "Can be seen as minimizing non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "That's not the way they derived it, but we can think of it as if you think of this over this as X bar.",
                    "label": 0
                },
                {
                    "sent": "This is the minimizing non gauss maximizing non gaussianity of X.",
                    "label": 0
                },
                {
                    "sent": "So that can be seen as a special case of this class of algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think the more interesting thing are these very mysterious algorithms.",
                    "label": 0
                },
                {
                    "sent": "Witcher map XC with bilateral filtering algorithms?",
                    "label": 1
                },
                {
                    "sent": "So this is a graph Asia Paper from 2009 and you know, I just refuse to believe that this algorithm would work.",
                    "label": 0
                },
                {
                    "sent": "Then I visited tubing, game and Highland Steven was saying they were using this algorithm actually works, so let me explain how it works.",
                    "label": 0
                },
                {
                    "sent": "It's a lot like the map excal rhythms we solve for X offer case.",
                    "label": 0
                },
                {
                    "sent": "Offer Excel for K and so if you just iterated these two steps, the algorithm guaranteed to fail.",
                    "label": 0
                },
                {
                    "sent": "But they have this extra step where after solving for the hidden image, so here's your predicted image.",
                    "label": 0
                },
                {
                    "sent": "Here you solve in this case.",
                    "label": 0
                },
                {
                    "sent": "This is for new software.",
                    "label": 0
                },
                {
                    "sent": "The blur you get a sharp image.",
                    "label": 0
                },
                {
                    "sent": "You go back here and if you were to go to here, that would be the standard algorithm.",
                    "label": 0
                },
                {
                    "sent": "Sharp image blurring will sharp image blur Trimble here what they have is another step where you take your predicted sharp image, applied bilateral filter to it.",
                    "label": 0
                },
                {
                    "sent": "An by zeroing out the low gradients, increasing the high gradients, applying shock filters, all sorts of things that are very difficult to understand where they come from, and that's crucial.",
                    "label": 0
                },
                {
                    "sent": "This is what they told me when I visited.",
                    "label": 0
                },
                {
                    "sent": "Again, if you take away this step, it doesn't work anymore.",
                    "label": 0
                },
                {
                    "sent": "But still, you know nobody really knows what this algorithm is doing when you iterate it.",
                    "label": 0
                },
                {
                    "sent": "Many times will happen and there is a way to see these algorithms that actually Max maximizing the non gaussianity of X, because again, this step of taking X and going to X bar is similar to requiring that.",
                    "label": 0
                },
                {
                    "sent": "That we keep the variance of X constant.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly the same, but there was a way to write algorithms very similar to this one that is guaranteed to succeed, and this is an insight we get by looking at these old algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is this bias?",
                    "label": 1
                },
                {
                    "sent": "The point of this talk is trying to link what's already known for the people who make modems to what we can do with room camera shake.",
                    "label": 0
                },
                {
                    "sent": "So obviously that allows us to understand some recent algorithms.",
                    "label": 0
                },
                {
                    "sent": "So for example, the Krishnan Fergus algorithm didn't have, I think, a proof.",
                    "label": 0
                },
                {
                    "sent": "Of correctness or maybe did have under some conditions, this generalizes the conditions under which we can prove.",
                    "label": 0
                },
                {
                    "sent": "That is, if this, if the derivatives are really IID, then we can show that that algorithm will succeed.",
                    "label": 0
                },
                {
                    "sent": "But that's a big if when I just said it.",
                    "label": 0
                },
                {
                    "sent": "Filters are not really IID and suggest ways to improve them.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we replace the one simple derivative filters with filters that look more like ICA, this predicts that will do a better job because the filters will be more independent.",
                    "label": 0
                },
                {
                    "sent": "And it also suggests looking at iterative algorithms that people in communication theory or familiar with that have this global convergence.",
                    "label": 1
                },
                {
                    "sent": "I don't know if any computer vision or image processing algorithm that has these global convergence guarantees would be great if we could have.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm done at the title of the talk is old and new algorithms for blind deconvolution.",
                    "label": 1
                },
                {
                    "sent": "So the new algorithms are typically math algorithms.",
                    "label": 0
                },
                {
                    "sent": "I repeated what we showed a few years ago.",
                    "label": 0
                },
                {
                    "sent": "Some of these algorithms are guaranteed to fail, but they still work even when they're not supposed to.",
                    "label": 1
                },
                {
                    "sent": "The old algorithms and communication theory usually are not math algorithms.",
                    "label": 0
                },
                {
                    "sent": "They have universal guarantees, global convergence, and I think the most important thing they used in millions of devices, probably as in this room we probably have many devices that use some variant of this algorithm.",
                    "label": 1
                },
                {
                    "sent": "And I think this connection can help us understand and improve image blurring algorithms.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So Matthias was mentioning the fact that a lot of people don't use the variational methods because they think they're very complicated to optimize, and I think that's one reason people use these Matic scale rhythms are guaranteed to fail because they're very simple to implement and to generalize, and Matthias was referring to papers that we had last year where we try to really do a very tutorial version of the variational method.",
                    "label": 0
                },
                {
                    "sent": "And show that it's really not very difficult, doesn't quite a lot of computation, but I don't think that's been a great success that most people I talked to will still rather use.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately the mapics cases so simple.",
                    "label": 0
                },
                {
                    "sent": "Also, I see Joseph is here.",
                    "label": 0
                },
                {
                    "sent": "If you look at the amount of work that Oliver and Joseph had to do to derive the variational approximation in.",
                    "label": 0
                },
                {
                    "sent": "It's not just programming work, I'm saying it's also a lot of mathematics often to get the variational things to work.",
                    "label": 0
                },
                {
                    "sent": "There's lots of equations.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately people don't like to use it.",
                    "label": 0
                },
                {
                    "sent": "These algorithms don't have any of these integration.",
                    "label": 0
                },
                {
                    "sent": "I mean system in global optimization.",
                    "label": 0
                },
                {
                    "sent": "Sorry this one.",
                    "label": 0
                },
                {
                    "sent": "So I think these are much more likely to be used.",
                    "label": 0
                },
                {
                    "sent": "By other people, because there's no integrals here, it's just optimization.",
                    "label": 0
                },
                {
                    "sent": "There is a normalization step which makes the optimization a little bit more difficult, but again, there's so much work that people have figured out how to optimize these things.",
                    "label": 0
                },
                {
                    "sent": "Like that I think.",
                    "label": 0
                },
                {
                    "sent": "I think I'm much more optimistic people use this than the variational methods.",
                    "label": 0
                },
                {
                    "sent": "On the left is might work better.",
                    "label": 0
                },
                {
                    "sent": "This is a different question.",
                    "label": 0
                },
                {
                    "sent": "I mean this has guarantees of success under IID assumptions.",
                    "label": 0
                },
                {
                    "sent": "Innovational methods don't.",
                    "label": 0
                },
                {
                    "sent": "Necessarily those ID assumptions.",
                    "label": 0
                },
                {
                    "sent": "So I repeat the question if we literally Stephen was saying if we literally translate the.",
                    "label": 0
                },
                {
                    "sent": "The communication theory algorithms.",
                    "label": 0
                },
                {
                    "sent": "Where are they?",
                    "label": 0
                },
                {
                    "sent": "They often software the equalizer.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a question, whether in blind convolution for images, whether this equalizer will be will make sense.",
                    "label": 0
                },
                {
                    "sent": "So we've done a little bit of experience on this.",
                    "label": 0
                },
                {
                    "sent": "You can often find very good equalizers that are compact.",
                    "label": 0
                },
                {
                    "sent": "Even for realistic blurs.",
                    "label": 0
                },
                {
                    "sent": "And there has been a lot of work in the communication theory world saying what happens if you place the true E with E. That's guarantee that's a compact.",
                    "label": 0
                },
                {
                    "sent": "And you can still prove correctness under certain cases there, so I'm less worried about that.",
                    "label": 0
                },
                {
                    "sent": "I'm more worried about what Matthias question is sometimes.",
                    "label": 0
                },
                {
                    "sent": "You know, there are frequencies that were zeroed out, and he doesn't really exist, and so you have to replace it, maybe with the leader filter, and then whether the math the math is more tricky.",
                    "label": 0
                },
                {
                    "sent": "I'm not totally sure that that algorithm we can prove the same things about, but we did try.",
                    "label": 0
                },
                {
                    "sent": "If you take just a blur in this database that are not as online, she has a number of.",
                    "label": 0
                },
                {
                    "sent": "Camera shake blur trails only found you can do a very good job of finding a compact equalizer for those.",
                    "label": 0
                },
                {
                    "sent": "OK, let me just repeat the question.",
                    "label": 0
                },
                {
                    "sent": "So again the mystery is of this map.",
                    "label": 0
                },
                {
                    "sent": "Thing is, even though we wrote this paper saying it can't possibly work.",
                    "label": 0
                },
                {
                    "sent": "It's used a lot so many people run it where is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if you globally optimize this, we show that's guaranteed to fail, and yet there's been a number of papers who use it, and the question was why does it work?",
                    "label": 1
                },
                {
                    "sent": "Well, one option is people stop it before they find the global optimum, and the other option is local minima and both have.",
                    "label": 0
                },
                {
                    "sent": "But both of these answers are correct.",
                    "label": 0
                },
                {
                    "sent": "That is, there is often a local minimum or a saddle point around the correct X.",
                    "label": 0
                },
                {
                    "sent": "So if you optimize only in certain directions and you're lucky not to go in the other directions.",
                    "label": 0
                },
                {
                    "sent": "That will work.",
                    "label": 0
                },
                {
                    "sent": "And it's also true that people always stop this early.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm wondering what.",
                    "label": 0
                },
                {
                    "sent": "Noise in the snow.",
                    "label": 0
                },
                {
                    "sent": "Gosh, entity things.",
                    "label": 0
                },
                {
                    "sent": "Is that an issue?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so actually if the noise is Gaussian, which is the typical assumption that just makes things even more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So in the original shelving Weinstein paper, they prove that will also work under gas additive Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "But again, I think the most impressive thing is it works in modems that have noise.",
                    "label": 0
                },
                {
                    "sent": "It's not a mathematical, so it must be work with noise.",
                    "label": 0
                },
                {
                    "sent": "Yes, no, no, this is a very long repeat the question.",
                    "label": 0
                },
                {
                    "sent": "That's a very interesting question.",
                    "label": 0
                },
                {
                    "sent": "He tried to make the image is non Gaussian as possible and we felt was asking well, can't you make it too sharp can't you over sharpen it?",
                    "label": 0
                },
                {
                    "sent": "And there's a proof here that you can do it with a linear.",
                    "label": 0
                },
                {
                    "sent": "That's the nice thing with a linear equalizer.",
                    "label": 0
                },
                {
                    "sent": "As long as you restrict yourself to linear equalizers, you can.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be much more non Gaussian in the original image.",
                    "label": 0
                },
                {
                    "sent": "So that's very.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can keep cranking it up and you'll never overshoot, unlike many other algorithms.",
                    "label": 0
                },
                {
                    "sent": "And again, it's the central limit.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here on the.",
                    "label": 0
                },
                {
                    "sent": "Any other thing as long as you play linear filter, any other thing even it's very high pass, it will be more Gaussian in the original thing, so maximizing non gaussianity will always find you the correct thing.",
                    "label": 0
                },
                {
                    "sent": "It's a good question since it's based on the Central Limit theorem.",
                    "label": 0
                },
                {
                    "sent": "How many samples do you need in order to?",
                    "label": 0
                },
                {
                    "sent": "Prove that it moves, yeah, so let me repeat the question.",
                    "label": 0
                },
                {
                    "sent": "I keep mentioning the central Limit Theorem, which says that things become Gaussian and the natural questions.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe this needs very big blur kernels for the central Limit theorem can.",
                    "label": 0
                },
                {
                    "sent": "I was just using Central Limit Theorem as shorthand.",
                    "label": 0
                },
                {
                    "sent": "The proof is actually for any blur.",
                    "label": 0
                },
                {
                    "sent": "Even of size 2.",
                    "label": 0
                },
                {
                    "sent": "So blur monotonically sort of makes you more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Infinite Blur will make you Gaussian, but the kurtosis becomes closer to the process of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Even with just a tap filter.",
                    "label": 0
                },
                {
                    "sent": "That's what they actually showed in the paper.",
                    "label": 0
                },
                {
                    "sent": "I didn't want to go into that proof, but even with the two tap filter, you already guaranteed to be more Gaussian than the original 1.",
                    "label": 1
                },
                {
                    "sent": "If you have a very large number of taps will become Gaussian, but with two taps already become organizing.",
                    "label": 0
                },
                {
                    "sent": "The question was whether.",
                    "label": 0
                },
                {
                    "sent": "Whether there's a connection so that these new algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The old new algorithms, whatever.",
                    "label": 0
                },
                {
                    "sent": "Basically add another term or another constraint.",
                    "label": 0
                },
                {
                    "sent": "You instead of measuring the moment of actually measure the moment of X divided by standard deviation, and so they look a lot like this and the question was whether this extra term maybe could be understood as some some approximation that gets stored map Ki.",
                    "label": 0
                },
                {
                    "sent": "Think that's a great question, I don't know, but I would like to emphasize that the guarantees of correctness are different.",
                    "label": 0
                },
                {
                    "sent": "The map K are proof that will succeed.",
                    "label": 0
                },
                {
                    "sent": "Assumes that X samples from the prior.",
                    "label": 0
                },
                {
                    "sent": "Like all Bayesian methods, if X if you don't have the prior correct, you're not guaranteed to succeed anymore.",
                    "label": 0
                },
                {
                    "sent": "These newer algorithms, which are the old algorithms, don't require any knowledge of their universal, they just require the IID assumption.",
                    "label": 0
                },
                {
                    "sent": "So even if we made that connection, there's something stronger from these other way of thinking.",
                    "label": 0
                },
                {
                    "sent": "OK, great thank you again.",
                    "label": 0
                }
            ]
        }
    }
}