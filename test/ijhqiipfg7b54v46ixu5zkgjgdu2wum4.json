{
    "id": "ijhqiipfg7b54v46ixu5zkgjgdu2wum4",
    "title": "Data Spectroscopy: Learning Mixture Models using Eigenspaces of Convolution Operators",
    "info": {
        "author": [
            "Mikhail Belkin, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes",
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/icml08_belkin_dslmm/",
    "segmentation": [
        [
            "Thanks so yeah, so this work is basically how to use a certain class of spectral methods to learn mixture models and.",
            "This is joint work with Josh from Ohio State Department of Statistics and Venue.",
            "From UC."
        ],
        [
            "Berkeley statistics and let me start with actually kind of a classical questions for spectral geometry, and I'll explain.",
            "I'll just spend one minute on this.",
            "I'll explain why this is kind of irrelevant question and the question is the following.",
            "It's whether you can hear the shape of a drum.",
            "An OK here is a drum and you hit the drop and it makes a sound right when you look at the.",
            "Harmonical the composition of that sound a little bit and you know from can you deduce the shape of the drum from how it's out.",
            "Well, So what?",
            "What does it mean to hear the shape of a gram?",
            "And you know the drum can be something like this.",
            "Also imagine this is a surface of a Taurus and basically you are solving some equation in the harmonics associated to the drum eigenfunctions of this equation and you don't have to know what this equation is.",
            "The Laplace operator in this eigenfunction.",
            "So the Laplace operator.",
            "But the fact is the harmonics that you're here, or if you just pluck a string.",
            "So take a string and plug it and you'll hear a certain.",
            "Well, tone and that is actually an eigenfunction of this operator with some boundary conditions, and it turns out that from this harmonic decomposition, you cannot actually completely here.",
            "So there is this kind of the main harmonic of a drum Lambda zero.",
            "Then there is the next one, the next one, and there are infinitely many of them, and you cannot actually hear the shape, but you can hear a lot of things like the area of the drum for example, or the dimension of the drama fits.",
            "A high dimensional drum OK."
        ],
        [
            "So here is actually something to.",
            "I see from here it turns out that if you look at eigenfunctions of that operator again, I will ignore the question of what it is, but I can."
        ],
        [
            "Functions are fixed.",
            "You apply the second function, it actually.",
            "Yeah, you'll get the same thing up to some constant.",
            "OK, so you apply this eigenfunction, so if you think about the string, I plug this string and if I plug the string initially as a second function, then well occurs.",
            "The string decays to zero.",
            "Of course it wants to become constant, but as it's decaying it will actually keep the same shape.",
            "And that's what happens.",
            "You know you pull the string and you hear talk that corresponds to that.",
            "I mean functions and there is pointing frequencies eigen."
        ],
        [
            "Add.",
            "OK, it turns out that this harmonic actually form a basis of functions.",
            "On this on the drum, and this basis gives you everything.",
            "So in a sense this is a complete characterization of the drop of the space of functions on the drop.",
            "So for every function defined on the gram can be represented as a sum of this eigenfunctions.",
            "OK, and now here is a question.",
            "Here is a question I'm really going to."
        ],
        [
            "Address here, can you hear the shape of a probability distribution and particularly mixture of Gaussian?",
            "What does it?",
            "Mixture of Gaussian sound like?",
            "Well, here is a mixture of Gaussian.",
            "Well, here is actual mixture into some data sample from it and the question is can you hear it?",
            "And well how can you hear something like that?",
            "OK, we're not actually going to hear."
        ],
        [
            "Right, but what we can do?",
            "We can construct a certain operator associated to that probability distribution.",
            "And look at the spectrum of that probability distribution, and the spectrum is going to be defined exactly like in hearing the shape of the drop.",
            "So here is the operator which we will consider.",
            "The operator is the following.",
            "So no, this is I'm giving the probability."
        ],
        [
            "Distribution, yes, imagine this is my density there.",
            "The."
        ],
        [
            "Red line.",
            "And here is pointing to this density I'm coming.",
            "I'm constructing an operator, so this is an operator KP its density independent.",
            "Now KP of F is the following.",
            "I convolve this function F with some kernel K. With respect to this density.",
            "OK.",
            "So this is a convolution of the function F with a certain kernel.",
            "You can think of this big house in kernel that actually the case we will consider and with respect to this density.",
            "OK, so that's what this KP of FS and it turns out that this operator has a condition that this operator has a discrete spectrum and its decaying.",
            "It goes from the top eigenvalue to zero and eigenfunctions form an orthogonal basis for L2 of P for integrable functions with respect to this probability distribution.",
            "So kind of a nice fact that's exactly like the situation with the drop.",
            "So you can imagine I'm kind of hitting the probability distribution with this.",
            "With this kernel and listening for the sound of it.",
            "Looking at the spectrum, whatever metaphor you prefer, OK. Now this."
        ],
        [
            "So very nice, but let's me do.",
            "Let's do an example.",
            "So how do I hear single Gaussian?",
            "OK, so remember my probability distribution now.",
            "Is this gaussian?",
            "It has a parameters, mu is just one dimensional, let's say mu and variance Sigma.",
            "So this is mean invariant and I'm convolving with this Gaussian, that's that is a probability distribution that is just a kernel, it's not, it's just a Gaussian car.",
            "So KP of F is this thing.",
            "OK. Now, OK, this is a little bit ugly, but not that bad so.",
            "OK, so ignore the lamb dies for a second.",
            "Some complicated expression.",
            "OK, whatever.",
            "The nice thing.",
            "OK, there are two nice things about that.",
            "Well, there are more than two, but I'll concentrate on 2.",
            "First, it cannot be written explicitly, if a bit clumsily.",
            "Second, what is the top?",
            "Yeah so hi, is some polynomial.",
            "The H Zero is actually a constant function.",
            "So H 0 is a constant function, so that means that the top eigen function of this guy.",
            "Is actually nothing but also Gaussian.",
            "And where is this Gaussian?",
            "This hermit available?",
            "And where is this Gaussian located?",
            "It's located actually at the same mean as original Gaussian.",
            "OK, so it's a different Gaussian and it's centered at the original Gaussian, so that's nice.",
            "Another thing actually, which is also good."
        ],
        [
            "If I look at the expression and if I put R to be Lambda 0 divided by Lambda one, and you have to work it out, but it says elementary computation Sigma.",
            "The variance of my probability distribution is actually equal to.",
            "Omega Squared, which is the width of the kernel time R, which is this ratio divided by 1 -- R ^2.",
            "So this is again a very simple.",
            "Explicit algebraic expression for the for the variance.",
            "So the variance is actually expressed explicitly here."
        ],
        [
            "Now, OK, this is all good.",
            "But this is kind of an abstract mathematical object.",
            "What do I do from the data?",
            "From the data, if you think about what this convolution operator means, it's nothing but well, this operator becomes a matrix right when you have data, we discretize an integral becomes a matrix.",
            "What is this magic?",
            "Well, this is nothing else but."
        ],
        [
            "Just the kernel metric.",
            "OK, now this lamb doesn't this.",
            "If for data becomes the top and the second from the top eigenvalues of this kernel matrix, this is the same metrics we use in kernel PCA, for example.",
            "Other things basically any kernel method.",
            "So now how do we estimate the Sigma from the data?",
            "We construct this matrix by choosing some widths of the kernel.",
            "We take the top eigenvalue of this.",
            "We divided by the next one.",
            "Plug it in here.",
            "This is R. Plug it in here.",
            "This is an estimate for the various so, OK, we've got variants."
        ],
        [
            "OK, what about the mean?",
            "Well, you can kind of see if this is my data.",
            "This is a single Gaussian, the mean the empirical eigenvector.",
            "The eigenvector computed from the matrix actually looks like this.",
            "So the mean is actually where the top value is.",
            "So by taking the top value of this we can estimate the map or actually not the top."
        ],
        [
            "Alright, now what can we say about the top eigenfunction and again remember in that case it was a Gaussian and this is actually more general than just for the Gaussian, but here is important part, it is the only eigenfunction.",
            "With no sign change, it has multiplicity one that is easy to check and it decays quickly away from the mean.",
            "Well, it's a Gaussian.",
            "OK, no big surprise.",
            "But again, this is."
        ],
        [
            "More general.",
            "Now, what about the mixture of Gaussians?",
            "So mixture of Gaussians are things like dressin."
        ],
        [
            "In the.",
            "What do crabs have to do with mixtures of Gaussians?",
            "Actually, Pearson first introduced the method of moment by analyzing it."
        ],
        [
            "Population in any case.",
            "And well, how do people use them?",
            "A lot of methods you've actually am an probably fair to say the most popular one.",
            "However, it is sensitive to initialization and doesn't detect the number of components automatically.",
            "And again, this is."
        ],
        [
            "Something which can be discussed in some detail.",
            "So how do I use this method?",
            "This kind of spectral analysis to detect the mixture.",
            "So here is the idea.",
            "Well this is a mixture of two Gaussians.",
            "Very simple one.",
            "Now let me look at top two eigenvectors of this guy.",
            "So remember, I construct that matrix.",
            "I take top two eigenvectors of this matrix and this is what they look like.",
            "And this approximate the top two eigen functions of the convolution.",
            "They actually look like this.",
            "Now, if I take each one of this, this is a mixture of two.",
            "This is what this actually this components look like.",
            "If I take this component to take you taking vacation separately this and if I take this component intact, each eigenvector separately, it looks like this.",
            "Well, we see this is almost the same as this one, and this is almost the same as that.",
            "So when I have a mixture of two Gaussians, it's actually the eigenvectors are unions.",
            "The first one is like the first one of the share point components and the second one OK.",
            "It's not always a second one, but I probably don't have.",
            "This covers the next one in some sense I can.",
            "I can describe how to get it.",
            "Is the eigenvector of this one?"
        ],
        [
            "OK, so how to hear mixture of Gaussians?",
            "I mean this is a mixture of Gaussian specific.",
            "So single components we can.",
            "Actually it's beautiful.",
            "We can analytically express everything you know.",
            "We have closed form solutions for single Gaussians for everything.",
            "Mixture of component.",
            "Assuming there is enough separation for mixture of component, it basically the same as kind of the sum of those components separately and again you can prove theoretical result.",
            "So I'll flash a slide which one?",
            "And.",
            "Assuming enough separation, you can prove certain things about the mixture, but basically the idea is if they separated well, then you just look at them separately and the eigenvectors of those components are kind of like of the mixture, eigenvectors of individual components.",
            "An estimate from data construct kernel matrices plus here is a bonus.",
            "You can estimate the number of components and basically you estimate the number of components by looking at eigenvectors which are almost positive big cause for each component the top eigenvector is the only positive one.",
            "Remember it's only positive one that is that was the Gaussian an if you have several components, you look for eigenvectors, not necessarily consecutive.",
            "Which are almost positive they are not going to be exactly positive because you have this mixture effect, but if there's enough separation, it's going to be almost."
        ],
        [
            "OK, so here is the result.",
            "Basically this is saying that if there is enough separation between the component, then by some perturbation theory you can show that mixture of components and taking each mixture component separately is."
        ],
        [
            "Almost the same thing.",
            "OK, so here is an example and.",
            "Let me discuss this briefly, so this is what we call the spectroscopic estimator.",
            "This is actually very simple example, just one dimension.",
            "There are two mixer.",
            "There are two Gaussians, one is 9 times as big as the other one, so this one is 10% of the mass.",
            "Very simple example.",
            "However, here is a surprising thing here.",
            "I mean am which came into initialization arguably is the most popular kind of thing people do.",
            "In any case, if you do, K means.",
            "You cannot estimate this at all.",
            "For example, the second component they knew the means would be 0.",
            "If you just do, K means it's completely wrong.",
            "Em helps a little bit, but it's still wrong.",
            "You initialize it because you have bad initialization, AM doesn't kind of do magic.",
            "You have bad initialization.",
            "You're going to bed get bad results.",
            "It's a it's a local optimization procedure, so if you start here, well, you get slightly better, but it's still very bad.",
            "It's mu equals zero and well, you get minus one, so it's actually placing the mean of the second component here.",
            "Very bad.",
            "Now, if you do spectroscopic estimator that.",
            "Oh estimator, you're actually get quite good result.",
            ".0 two.",
            "It's quite good.",
            "But Interestingly if you do EM based on that in the you use the spectroscopic estimators initialization for EM, you do even better.",
            "I mean, you pretty much know perfectly here 'cause we know if Em was started.",
            "Well, even works well.",
            "OK, so.",
            "So here is what we see.",
            "So even actually kind of thing.",
            "This is a striking example, to me at least.",
            "I think to us it was very striking.",
            "'cause here's an example.",
            "In one dimension it's unbalanced, but not unreasonably unbalanced.",
            "You know, this is just nine times as big as that one, and suddenly expectation maximization just doesn't work right.",
            "I mean the fact that K means doesn't work is not so surprising, because came in when the components aren't balanced.",
            "Actually, the minimum of K means would tend to cut.",
            "This component is half.",
            "That's how you minimize energy and ignoring the second component.",
            "So you would get something.",
            "One mean here and another menu would get here.",
            "And that's roughly what happens.",
            "But the fact is that I am actually cannot overcome that.",
            "The deficiency of its initialization.",
            "But if you speak traffic estimate, it works pretty well and Moreover you can actually find the number of components automatically here.",
            "Of course you have to feed the number of components K means doesn't know.",
            "OK."
        ],
        [
            "So.",
            "OK, so you say what's kind of interesting here so it's a new method to estimate Gaussian mixture distributions.",
            "Ann, maybe I'll say one slide about clustering.",
            "It allows us to estimate number of components and in some sense it actually can be viewed as a way to understand.",
            "Kernel PCA.",
            "And it can work as initialization for am.",
            "Actually, that works best seemingly."
        ],
        [
            "So why does it work best?",
            "Because that actually it's not a perfect separation, as you can see.",
            "In fact, it's quite far from perfect, so there is some interference between the components, so you don't actually get the perfect.",
            "So if they are very far you would have perfect results, but here.",
            "They're not that far, so there is some interference an EM can."
        ],
        [
            "Correct that.",
            "OK, can work in serialization and there are some interesting theoretical questions and particularly interesting extensions to clustering.",
            "And OK, let me.",
            "How much time do I have?",
            "One minute, 2 minutes.",
            "OK, I oh, OK I'll I'll just discuss clustering a little bit so.",
            "So actually you can use.",
            "You can use it as a method for spectral clustering is kind of a slide.",
            "So instead of looking for.",
            "Instead, what do people do in spectral clustering?",
            "So, for example, there is a popular algorithm by N. Jordan, wise Ann.",
            "You projected into some space and you do K means in that space some spectral space and your projected using the top few eigenvectors of the graph Laplacian matrix.",
            "And for those who know how it works it, if you don't know, I apologize.",
            "It will be incomprehensible, but.",
            "What you can do actually instead of doing that, you can look for this almost positive eigenvectors of the matrix and you can project using this almost positive eigenvectors.",
            "And then there are several advantages to that.",
            "You don't actually need.",
            "To do any came into that space, you just use those eigenvectors to cluster, so it's kind of more like original spectral clustering by by partitioning when you just partition your data based on to and it works quite well actually, or at least with work.",
            "OK, you can construct or examples on which it works better than it seems to work well on real data, but on real data I can show sometimes, but it's harder to compare.",
            "Certainly it can contract or examples.",
            "So for example here if you look at and it learns a number of clusters automatically.",
            "For example here if you look at something like this, so this is probably three.",
            "Well, I don't know it's four groups or three groups, depending how you count an.",
            "Actually here Flaunden perfectly an if you do for example and then Jordans methods with four groups.",
            "It actually kind of screws it up.",
            "You you cut this loop in free?",
            "Our three part and the reason is if in your projection space you use K means as the projection space.",
            "If your data is not well balanced in that projection space, K means is not going to work well.",
            "Convince only works well for balanced data.",
            "So in the projective space you may have difficulty clustering.",
            "In any case, I would be happy to talk about this.",
            "I think some very interesting connection to spectral clustering OK?",
            "So in any case, I think I'll finish here.",
            "Thank you.",
            "Questions.",
            "OK.",
            "So you mentioned that you have some capability assumptions.",
            "Could you say something about how these capability assumptions relate to this capability?",
            "Assumptions that people in the spectral clustering community used, like Dasgupta and Canam, and all these other papers?",
            "So yeah, that's quite different.",
            "Well, OK, OK, that's good.",
            "Yeah, it's not completely different, but it's different how it is different.",
            "The sort of tradition in.",
            "Theoretical computer science is to look at the separation as a function of dimension, so you can also play this game here and you'll get actually something like the original does group.",
            "This paper that he feels is actually when it square root of N. It doesn't seem to be as good as some of the more recent results when they get.",
            "I think it's almost constant.",
            "I forget the exact the state of that, however it's constant, but with a huge constant in France, like on the order of 10 to the 8th or something.",
            "So so I mean.",
            "It's constant, but the separation is like 1,000,000 standard deviation or 1 billion standard deviation.",
            "So it's more of a theoretical result.",
            "Also very nice one.",
            "Yeah, but really the separation kind of it actually kind of intuitive.",
            "What is the separation assumption here?",
            "You're using some curve."
        ],
        [
            "You're using some kernel, so you have this.",
            "Which of the kernel?",
            "So when your kernel is centered?",
            "Here it shouldn't go over to the other side too much if that happened as bad.",
            "So roughly speaking, the mass of the kernel, which is centered here on the other mixture component should be relatively small, and then you can write that out.",
            "Thank you.",
            "Good question.",
            "I have a question in speech recognition.",
            "I mean gosh and mixtures are used a lot.",
            "So my question is first usually we we experience improvements.",
            "If we have quite sparse samples per per component and we also have the alignment problem.",
            "So you have some interdependence between this alignment and the Goshen.",
            "The Goshen mixture estimation.",
            "What do you think is it?",
            "Is this something which would help in speech recognition?",
            "You know we're actually trying now to.",
            "Very good question.",
            "We're actually thinking about trying to do it with speech recognition, because, as you say, the mixture.",
            "It's very standard kind of tool box from speech recognition.",
            "The thing is.",
            "Yeah, we've been actually sort of trying to work a little bit on everything is in speech recognition.",
            "There's so many layers to the problem that it's not easy to.",
            "I mean, it's not always that you can just sort of plug something in at one level and it will propagate even if you do better at that level.",
            "It's not my feeling is it would help, but it may need work to to get it working because there are many levels to to the problem.",
            "And but my feeling is it should it should at least as initialization of 2:00 AM.",
            "It should work.",
            "Will definitely get a bit better mixture.",
            "So this is a bit related to the previous previous question.",
            "It seems like if the width of your kernel is too big, you'll maybe miss, and if the width is too small, you'll maybe find too many.",
            "Is that true?",
            "And do you know how sensitive it is to the width?",
            "It's not extremely sensitive, but you cannot get.",
            "You know you cannot get it wildly wrong either, so it's kind of like the situation with the kernel method in general, you know you should.",
            "It's not terribly sensitive, but if you get it really wrong, you kind of.",
            "So a good good heuristic for that you know, take take as your scale takes the distance to the nearest neighbour, and then maybe take five times battle.",
            "That's kind of a good scale heuristic, so if you're 3 times or five times distance to the nearest neighbors are probably will be doing OK.",
            "But if you have counted times, so forget about it, right?",
            "Once the kernel kind of goes over both of them, it looks like a single cluster to the kernel.",
            "And if it's a kernel is very well in there alright?",
            "Each point looks like a cluster, so.",
            "But it leaves at the intuition.",
            "OK, can you say something about how sensitive is the method to when the data is not quite Gaussian?",
            "Yeah, actually I didn't have, so it's.",
            "What you can you can actually show something quite a bit more general.",
            "You can show that if you have kind of high density components then the method will still work.",
            "So basically you need separation between high density, high density regions and then in fact.",
            "In fact, you can see you can use it as a digit of 4, three and five, and it does a decent job, but you see, you need to take this positive eigenvectors which I mentioned.",
            "They're not actually one, and two 123 up to 15 all correspond to the digits 4, then suddenly digit three is 16 and it's quite good.",
            "I mean, it's quite.",
            "I mean this is not terribly difficult to separate this data set, but it's clearly not exactly Gaussian.",
            "So."
        ],
        [
            "Type in your screen.",
            "It's possible, yeah.",
            "Funny how that works out that much.",
            "Well, um.",
            "That's a good question, so if you're thinking you're just smooth it out, you see when you actually.",
            "If you just do smoothing, you'll probably get.",
            "You may get other local Maxima, so it's not completely clear that you would.",
            "I mean, if you have very good width of the kernel with smoothing, I think you will do fine, but my feeling is that the kernel, which will be kind of sensitive, right?",
            "'cause this can very well look is a local maximum if you don't smooth well.",
            "So yeah, my feeling is for smoothing you have to be very careful with the width of the kernel.",
            "OK, let's thank all the speakers again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks so yeah, so this work is basically how to use a certain class of spectral methods to learn mixture models and.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Josh from Ohio State Department of Statistics and Venue.",
                    "label": 1
                },
                {
                    "sent": "From UC.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Berkeley statistics and let me start with actually kind of a classical questions for spectral geometry, and I'll explain.",
                    "label": 0
                },
                {
                    "sent": "I'll just spend one minute on this.",
                    "label": 0
                },
                {
                    "sent": "I'll explain why this is kind of irrelevant question and the question is the following.",
                    "label": 0
                },
                {
                    "sent": "It's whether you can hear the shape of a drum.",
                    "label": 1
                },
                {
                    "sent": "An OK here is a drum and you hit the drop and it makes a sound right when you look at the.",
                    "label": 0
                },
                {
                    "sent": "Harmonical the composition of that sound a little bit and you know from can you deduce the shape of the drum from how it's out.",
                    "label": 0
                },
                {
                    "sent": "Well, So what?",
                    "label": 0
                },
                {
                    "sent": "What does it mean to hear the shape of a gram?",
                    "label": 0
                },
                {
                    "sent": "And you know the drum can be something like this.",
                    "label": 0
                },
                {
                    "sent": "Also imagine this is a surface of a Taurus and basically you are solving some equation in the harmonics associated to the drum eigenfunctions of this equation and you don't have to know what this equation is.",
                    "label": 0
                },
                {
                    "sent": "The Laplace operator in this eigenfunction.",
                    "label": 0
                },
                {
                    "sent": "So the Laplace operator.",
                    "label": 0
                },
                {
                    "sent": "But the fact is the harmonics that you're here, or if you just pluck a string.",
                    "label": 0
                },
                {
                    "sent": "So take a string and plug it and you'll hear a certain.",
                    "label": 0
                },
                {
                    "sent": "Well, tone and that is actually an eigenfunction of this operator with some boundary conditions, and it turns out that from this harmonic decomposition, you cannot actually completely here.",
                    "label": 0
                },
                {
                    "sent": "So there is this kind of the main harmonic of a drum Lambda zero.",
                    "label": 0
                },
                {
                    "sent": "Then there is the next one, the next one, and there are infinitely many of them, and you cannot actually hear the shape, but you can hear a lot of things like the area of the drum for example, or the dimension of the drama fits.",
                    "label": 0
                },
                {
                    "sent": "A high dimensional drum OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is actually something to.",
                    "label": 0
                },
                {
                    "sent": "I see from here it turns out that if you look at eigenfunctions of that operator again, I will ignore the question of what it is, but I can.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions are fixed.",
                    "label": 0
                },
                {
                    "sent": "You apply the second function, it actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you'll get the same thing up to some constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so you apply this eigenfunction, so if you think about the string, I plug this string and if I plug the string initially as a second function, then well occurs.",
                    "label": 0
                },
                {
                    "sent": "The string decays to zero.",
                    "label": 0
                },
                {
                    "sent": "Of course it wants to become constant, but as it's decaying it will actually keep the same shape.",
                    "label": 0
                },
                {
                    "sent": "And that's what happens.",
                    "label": 0
                },
                {
                    "sent": "You know you pull the string and you hear talk that corresponds to that.",
                    "label": 0
                },
                {
                    "sent": "I mean functions and there is pointing frequencies eigen.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Add.",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out that this harmonic actually form a basis of functions.",
                    "label": 0
                },
                {
                    "sent": "On this on the drum, and this basis gives you everything.",
                    "label": 0
                },
                {
                    "sent": "So in a sense this is a complete characterization of the drop of the space of functions on the drop.",
                    "label": 0
                },
                {
                    "sent": "So for every function defined on the gram can be represented as a sum of this eigenfunctions.",
                    "label": 1
                },
                {
                    "sent": "OK, and now here is a question.",
                    "label": 0
                },
                {
                    "sent": "Here is a question I'm really going to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Address here, can you hear the shape of a probability distribution and particularly mixture of Gaussian?",
                    "label": 1
                },
                {
                    "sent": "What does it?",
                    "label": 0
                },
                {
                    "sent": "Mixture of Gaussian sound like?",
                    "label": 0
                },
                {
                    "sent": "Well, here is a mixture of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Well, here is actual mixture into some data sample from it and the question is can you hear it?",
                    "label": 0
                },
                {
                    "sent": "And well how can you hear something like that?",
                    "label": 0
                },
                {
                    "sent": "OK, we're not actually going to hear.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, but what we can do?",
                    "label": 0
                },
                {
                    "sent": "We can construct a certain operator associated to that probability distribution.",
                    "label": 1
                },
                {
                    "sent": "And look at the spectrum of that probability distribution, and the spectrum is going to be defined exactly like in hearing the shape of the drop.",
                    "label": 0
                },
                {
                    "sent": "So here is the operator which we will consider.",
                    "label": 0
                },
                {
                    "sent": "The operator is the following.",
                    "label": 0
                },
                {
                    "sent": "So no, this is I'm giving the probability.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution, yes, imagine this is my density there.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Red line.",
                    "label": 0
                },
                {
                    "sent": "And here is pointing to this density I'm coming.",
                    "label": 0
                },
                {
                    "sent": "I'm constructing an operator, so this is an operator KP its density independent.",
                    "label": 0
                },
                {
                    "sent": "Now KP of F is the following.",
                    "label": 0
                },
                {
                    "sent": "I convolve this function F with some kernel K. With respect to this density.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a convolution of the function F with a certain kernel.",
                    "label": 0
                },
                {
                    "sent": "You can think of this big house in kernel that actually the case we will consider and with respect to this density.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what this KP of FS and it turns out that this operator has a condition that this operator has a discrete spectrum and its decaying.",
                    "label": 0
                },
                {
                    "sent": "It goes from the top eigenvalue to zero and eigenfunctions form an orthogonal basis for L2 of P for integrable functions with respect to this probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So kind of a nice fact that's exactly like the situation with the drop.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine I'm kind of hitting the probability distribution with this.",
                    "label": 0
                },
                {
                    "sent": "With this kernel and listening for the sound of it.",
                    "label": 0
                },
                {
                    "sent": "Looking at the spectrum, whatever metaphor you prefer, OK. Now this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very nice, but let's me do.",
                    "label": 0
                },
                {
                    "sent": "Let's do an example.",
                    "label": 0
                },
                {
                    "sent": "So how do I hear single Gaussian?",
                    "label": 0
                },
                {
                    "sent": "OK, so remember my probability distribution now.",
                    "label": 0
                },
                {
                    "sent": "Is this gaussian?",
                    "label": 0
                },
                {
                    "sent": "It has a parameters, mu is just one dimensional, let's say mu and variance Sigma.",
                    "label": 1
                },
                {
                    "sent": "So this is mean invariant and I'm convolving with this Gaussian, that's that is a probability distribution that is just a kernel, it's not, it's just a Gaussian car.",
                    "label": 0
                },
                {
                    "sent": "So KP of F is this thing.",
                    "label": 0
                },
                {
                    "sent": "OK. Now, OK, this is a little bit ugly, but not that bad so.",
                    "label": 0
                },
                {
                    "sent": "OK, so ignore the lamb dies for a second.",
                    "label": 0
                },
                {
                    "sent": "Some complicated expression.",
                    "label": 0
                },
                {
                    "sent": "OK, whatever.",
                    "label": 0
                },
                {
                    "sent": "The nice thing.",
                    "label": 0
                },
                {
                    "sent": "OK, there are two nice things about that.",
                    "label": 0
                },
                {
                    "sent": "Well, there are more than two, but I'll concentrate on 2.",
                    "label": 0
                },
                {
                    "sent": "First, it cannot be written explicitly, if a bit clumsily.",
                    "label": 0
                },
                {
                    "sent": "Second, what is the top?",
                    "label": 0
                },
                {
                    "sent": "Yeah so hi, is some polynomial.",
                    "label": 0
                },
                {
                    "sent": "The H Zero is actually a constant function.",
                    "label": 0
                },
                {
                    "sent": "So H 0 is a constant function, so that means that the top eigen function of this guy.",
                    "label": 0
                },
                {
                    "sent": "Is actually nothing but also Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And where is this Gaussian?",
                    "label": 0
                },
                {
                    "sent": "This hermit available?",
                    "label": 0
                },
                {
                    "sent": "And where is this Gaussian located?",
                    "label": 0
                },
                {
                    "sent": "It's located actually at the same mean as original Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a different Gaussian and it's centered at the original Gaussian, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "Another thing actually, which is also good.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I look at the expression and if I put R to be Lambda 0 divided by Lambda one, and you have to work it out, but it says elementary computation Sigma.",
                    "label": 0
                },
                {
                    "sent": "The variance of my probability distribution is actually equal to.",
                    "label": 0
                },
                {
                    "sent": "Omega Squared, which is the width of the kernel time R, which is this ratio divided by 1 -- R ^2.",
                    "label": 0
                },
                {
                    "sent": "So this is again a very simple.",
                    "label": 0
                },
                {
                    "sent": "Explicit algebraic expression for the for the variance.",
                    "label": 0
                },
                {
                    "sent": "So the variance is actually expressed explicitly here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, OK, this is all good.",
                    "label": 0
                },
                {
                    "sent": "But this is kind of an abstract mathematical object.",
                    "label": 0
                },
                {
                    "sent": "What do I do from the data?",
                    "label": 0
                },
                {
                    "sent": "From the data, if you think about what this convolution operator means, it's nothing but well, this operator becomes a matrix right when you have data, we discretize an integral becomes a matrix.",
                    "label": 0
                },
                {
                    "sent": "What is this magic?",
                    "label": 0
                },
                {
                    "sent": "Well, this is nothing else but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just the kernel metric.",
                    "label": 0
                },
                {
                    "sent": "OK, now this lamb doesn't this.",
                    "label": 0
                },
                {
                    "sent": "If for data becomes the top and the second from the top eigenvalues of this kernel matrix, this is the same metrics we use in kernel PCA, for example.",
                    "label": 0
                },
                {
                    "sent": "Other things basically any kernel method.",
                    "label": 0
                },
                {
                    "sent": "So now how do we estimate the Sigma from the data?",
                    "label": 0
                },
                {
                    "sent": "We construct this matrix by choosing some widths of the kernel.",
                    "label": 0
                },
                {
                    "sent": "We take the top eigenvalue of this.",
                    "label": 0
                },
                {
                    "sent": "We divided by the next one.",
                    "label": 0
                },
                {
                    "sent": "Plug it in here.",
                    "label": 0
                },
                {
                    "sent": "This is R. Plug it in here.",
                    "label": 0
                },
                {
                    "sent": "This is an estimate for the various so, OK, we've got variants.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, what about the mean?",
                    "label": 0
                },
                {
                    "sent": "Well, you can kind of see if this is my data.",
                    "label": 0
                },
                {
                    "sent": "This is a single Gaussian, the mean the empirical eigenvector.",
                    "label": 0
                },
                {
                    "sent": "The eigenvector computed from the matrix actually looks like this.",
                    "label": 0
                },
                {
                    "sent": "So the mean is actually where the top value is.",
                    "label": 0
                },
                {
                    "sent": "So by taking the top value of this we can estimate the map or actually not the top.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, now what can we say about the top eigenfunction and again remember in that case it was a Gaussian and this is actually more general than just for the Gaussian, but here is important part, it is the only eigenfunction.",
                    "label": 0
                },
                {
                    "sent": "With no sign change, it has multiplicity one that is easy to check and it decays quickly away from the mean.",
                    "label": 1
                },
                {
                    "sent": "Well, it's a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, no big surprise.",
                    "label": 0
                },
                {
                    "sent": "But again, this is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More general.",
                    "label": 0
                },
                {
                    "sent": "Now, what about the mixture of Gaussians?",
                    "label": 0
                },
                {
                    "sent": "So mixture of Gaussians are things like dressin.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "What do crabs have to do with mixtures of Gaussians?",
                    "label": 0
                },
                {
                    "sent": "Actually, Pearson first introduced the method of moment by analyzing it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Population in any case.",
                    "label": 0
                },
                {
                    "sent": "And well, how do people use them?",
                    "label": 0
                },
                {
                    "sent": "A lot of methods you've actually am an probably fair to say the most popular one.",
                    "label": 0
                },
                {
                    "sent": "However, it is sensitive to initialization and doesn't detect the number of components automatically.",
                    "label": 1
                },
                {
                    "sent": "And again, this is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something which can be discussed in some detail.",
                    "label": 0
                },
                {
                    "sent": "So how do I use this method?",
                    "label": 0
                },
                {
                    "sent": "This kind of spectral analysis to detect the mixture.",
                    "label": 0
                },
                {
                    "sent": "So here is the idea.",
                    "label": 0
                },
                {
                    "sent": "Well this is a mixture of two Gaussians.",
                    "label": 1
                },
                {
                    "sent": "Very simple one.",
                    "label": 0
                },
                {
                    "sent": "Now let me look at top two eigenvectors of this guy.",
                    "label": 0
                },
                {
                    "sent": "So remember, I construct that matrix.",
                    "label": 0
                },
                {
                    "sent": "I take top two eigenvectors of this matrix and this is what they look like.",
                    "label": 0
                },
                {
                    "sent": "And this approximate the top two eigen functions of the convolution.",
                    "label": 0
                },
                {
                    "sent": "They actually look like this.",
                    "label": 0
                },
                {
                    "sent": "Now, if I take each one of this, this is a mixture of two.",
                    "label": 0
                },
                {
                    "sent": "This is what this actually this components look like.",
                    "label": 0
                },
                {
                    "sent": "If I take this component to take you taking vacation separately this and if I take this component intact, each eigenvector separately, it looks like this.",
                    "label": 0
                },
                {
                    "sent": "Well, we see this is almost the same as this one, and this is almost the same as that.",
                    "label": 0
                },
                {
                    "sent": "So when I have a mixture of two Gaussians, it's actually the eigenvectors are unions.",
                    "label": 0
                },
                {
                    "sent": "The first one is like the first one of the share point components and the second one OK.",
                    "label": 0
                },
                {
                    "sent": "It's not always a second one, but I probably don't have.",
                    "label": 0
                },
                {
                    "sent": "This covers the next one in some sense I can.",
                    "label": 0
                },
                {
                    "sent": "I can describe how to get it.",
                    "label": 1
                },
                {
                    "sent": "Is the eigenvector of this one?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how to hear mixture of Gaussians?",
                    "label": 1
                },
                {
                    "sent": "I mean this is a mixture of Gaussian specific.",
                    "label": 0
                },
                {
                    "sent": "So single components we can.",
                    "label": 0
                },
                {
                    "sent": "Actually it's beautiful.",
                    "label": 1
                },
                {
                    "sent": "We can analytically express everything you know.",
                    "label": 0
                },
                {
                    "sent": "We have closed form solutions for single Gaussians for everything.",
                    "label": 0
                },
                {
                    "sent": "Mixture of component.",
                    "label": 0
                },
                {
                    "sent": "Assuming there is enough separation for mixture of component, it basically the same as kind of the sum of those components separately and again you can prove theoretical result.",
                    "label": 0
                },
                {
                    "sent": "So I'll flash a slide which one?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Assuming enough separation, you can prove certain things about the mixture, but basically the idea is if they separated well, then you just look at them separately and the eigenvectors of those components are kind of like of the mixture, eigenvectors of individual components.",
                    "label": 0
                },
                {
                    "sent": "An estimate from data construct kernel matrices plus here is a bonus.",
                    "label": 0
                },
                {
                    "sent": "You can estimate the number of components and basically you estimate the number of components by looking at eigenvectors which are almost positive big cause for each component the top eigenvector is the only positive one.",
                    "label": 0
                },
                {
                    "sent": "Remember it's only positive one that is that was the Gaussian an if you have several components, you look for eigenvectors, not necessarily consecutive.",
                    "label": 0
                },
                {
                    "sent": "Which are almost positive they are not going to be exactly positive because you have this mixture effect, but if there's enough separation, it's going to be almost.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the result.",
                    "label": 0
                },
                {
                    "sent": "Basically this is saying that if there is enough separation between the component, then by some perturbation theory you can show that mixture of components and taking each mixture component separately is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Almost the same thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is an example and.",
                    "label": 0
                },
                {
                    "sent": "Let me discuss this briefly, so this is what we call the spectroscopic estimator.",
                    "label": 0
                },
                {
                    "sent": "This is actually very simple example, just one dimension.",
                    "label": 0
                },
                {
                    "sent": "There are two mixer.",
                    "label": 0
                },
                {
                    "sent": "There are two Gaussians, one is 9 times as big as the other one, so this one is 10% of the mass.",
                    "label": 0
                },
                {
                    "sent": "Very simple example.",
                    "label": 0
                },
                {
                    "sent": "However, here is a surprising thing here.",
                    "label": 0
                },
                {
                    "sent": "I mean am which came into initialization arguably is the most popular kind of thing people do.",
                    "label": 0
                },
                {
                    "sent": "In any case, if you do, K means.",
                    "label": 0
                },
                {
                    "sent": "You cannot estimate this at all.",
                    "label": 0
                },
                {
                    "sent": "For example, the second component they knew the means would be 0.",
                    "label": 0
                },
                {
                    "sent": "If you just do, K means it's completely wrong.",
                    "label": 0
                },
                {
                    "sent": "Em helps a little bit, but it's still wrong.",
                    "label": 0
                },
                {
                    "sent": "You initialize it because you have bad initialization, AM doesn't kind of do magic.",
                    "label": 0
                },
                {
                    "sent": "You have bad initialization.",
                    "label": 0
                },
                {
                    "sent": "You're going to bed get bad results.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a local optimization procedure, so if you start here, well, you get slightly better, but it's still very bad.",
                    "label": 0
                },
                {
                    "sent": "It's mu equals zero and well, you get minus one, so it's actually placing the mean of the second component here.",
                    "label": 0
                },
                {
                    "sent": "Very bad.",
                    "label": 0
                },
                {
                    "sent": "Now, if you do spectroscopic estimator that.",
                    "label": 0
                },
                {
                    "sent": "Oh estimator, you're actually get quite good result.",
                    "label": 0
                },
                {
                    "sent": ".0 two.",
                    "label": 0
                },
                {
                    "sent": "It's quite good.",
                    "label": 0
                },
                {
                    "sent": "But Interestingly if you do EM based on that in the you use the spectroscopic estimators initialization for EM, you do even better.",
                    "label": 0
                },
                {
                    "sent": "I mean, you pretty much know perfectly here 'cause we know if Em was started.",
                    "label": 0
                },
                {
                    "sent": "Well, even works well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So here is what we see.",
                    "label": 0
                },
                {
                    "sent": "So even actually kind of thing.",
                    "label": 0
                },
                {
                    "sent": "This is a striking example, to me at least.",
                    "label": 0
                },
                {
                    "sent": "I think to us it was very striking.",
                    "label": 0
                },
                {
                    "sent": "'cause here's an example.",
                    "label": 0
                },
                {
                    "sent": "In one dimension it's unbalanced, but not unreasonably unbalanced.",
                    "label": 0
                },
                {
                    "sent": "You know, this is just nine times as big as that one, and suddenly expectation maximization just doesn't work right.",
                    "label": 0
                },
                {
                    "sent": "I mean the fact that K means doesn't work is not so surprising, because came in when the components aren't balanced.",
                    "label": 0
                },
                {
                    "sent": "Actually, the minimum of K means would tend to cut.",
                    "label": 0
                },
                {
                    "sent": "This component is half.",
                    "label": 0
                },
                {
                    "sent": "That's how you minimize energy and ignoring the second component.",
                    "label": 0
                },
                {
                    "sent": "So you would get something.",
                    "label": 0
                },
                {
                    "sent": "One mean here and another menu would get here.",
                    "label": 0
                },
                {
                    "sent": "And that's roughly what happens.",
                    "label": 0
                },
                {
                    "sent": "But the fact is that I am actually cannot overcome that.",
                    "label": 0
                },
                {
                    "sent": "The deficiency of its initialization.",
                    "label": 0
                },
                {
                    "sent": "But if you speak traffic estimate, it works pretty well and Moreover you can actually find the number of components automatically here.",
                    "label": 0
                },
                {
                    "sent": "Of course you have to feed the number of components K means doesn't know.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so you say what's kind of interesting here so it's a new method to estimate Gaussian mixture distributions.",
                    "label": 1
                },
                {
                    "sent": "Ann, maybe I'll say one slide about clustering.",
                    "label": 0
                },
                {
                    "sent": "It allows us to estimate number of components and in some sense it actually can be viewed as a way to understand.",
                    "label": 0
                },
                {
                    "sent": "Kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "And it can work as initialization for am.",
                    "label": 1
                },
                {
                    "sent": "Actually, that works best seemingly.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why does it work best?",
                    "label": 0
                },
                {
                    "sent": "Because that actually it's not a perfect separation, as you can see.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's quite far from perfect, so there is some interference between the components, so you don't actually get the perfect.",
                    "label": 0
                },
                {
                    "sent": "So if they are very far you would have perfect results, but here.",
                    "label": 0
                },
                {
                    "sent": "They're not that far, so there is some interference an EM can.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct that.",
                    "label": 0
                },
                {
                    "sent": "OK, can work in serialization and there are some interesting theoretical questions and particularly interesting extensions to clustering.",
                    "label": 1
                },
                {
                    "sent": "And OK, let me.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "One minute, 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, I oh, OK I'll I'll just discuss clustering a little bit so.",
                    "label": 0
                },
                {
                    "sent": "So actually you can use.",
                    "label": 0
                },
                {
                    "sent": "You can use it as a method for spectral clustering is kind of a slide.",
                    "label": 0
                },
                {
                    "sent": "So instead of looking for.",
                    "label": 0
                },
                {
                    "sent": "Instead, what do people do in spectral clustering?",
                    "label": 0
                },
                {
                    "sent": "So, for example, there is a popular algorithm by N. Jordan, wise Ann.",
                    "label": 0
                },
                {
                    "sent": "You projected into some space and you do K means in that space some spectral space and your projected using the top few eigenvectors of the graph Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "And for those who know how it works it, if you don't know, I apologize.",
                    "label": 0
                },
                {
                    "sent": "It will be incomprehensible, but.",
                    "label": 0
                },
                {
                    "sent": "What you can do actually instead of doing that, you can look for this almost positive eigenvectors of the matrix and you can project using this almost positive eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And then there are several advantages to that.",
                    "label": 0
                },
                {
                    "sent": "You don't actually need.",
                    "label": 0
                },
                {
                    "sent": "To do any came into that space, you just use those eigenvectors to cluster, so it's kind of more like original spectral clustering by by partitioning when you just partition your data based on to and it works quite well actually, or at least with work.",
                    "label": 0
                },
                {
                    "sent": "OK, you can construct or examples on which it works better than it seems to work well on real data, but on real data I can show sometimes, but it's harder to compare.",
                    "label": 0
                },
                {
                    "sent": "Certainly it can contract or examples.",
                    "label": 0
                },
                {
                    "sent": "So for example here if you look at and it learns a number of clusters automatically.",
                    "label": 0
                },
                {
                    "sent": "For example here if you look at something like this, so this is probably three.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know it's four groups or three groups, depending how you count an.",
                    "label": 0
                },
                {
                    "sent": "Actually here Flaunden perfectly an if you do for example and then Jordans methods with four groups.",
                    "label": 0
                },
                {
                    "sent": "It actually kind of screws it up.",
                    "label": 0
                },
                {
                    "sent": "You you cut this loop in free?",
                    "label": 0
                },
                {
                    "sent": "Our three part and the reason is if in your projection space you use K means as the projection space.",
                    "label": 0
                },
                {
                    "sent": "If your data is not well balanced in that projection space, K means is not going to work well.",
                    "label": 0
                },
                {
                    "sent": "Convince only works well for balanced data.",
                    "label": 0
                },
                {
                    "sent": "So in the projective space you may have difficulty clustering.",
                    "label": 0
                },
                {
                    "sent": "In any case, I would be happy to talk about this.",
                    "label": 0
                },
                {
                    "sent": "I think some very interesting connection to spectral clustering OK?",
                    "label": 0
                },
                {
                    "sent": "So in any case, I think I'll finish here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned that you have some capability assumptions.",
                    "label": 0
                },
                {
                    "sent": "Could you say something about how these capability assumptions relate to this capability?",
                    "label": 0
                },
                {
                    "sent": "Assumptions that people in the spectral clustering community used, like Dasgupta and Canam, and all these other papers?",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's quite different.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not completely different, but it's different how it is different.",
                    "label": 0
                },
                {
                    "sent": "The sort of tradition in.",
                    "label": 0
                },
                {
                    "sent": "Theoretical computer science is to look at the separation as a function of dimension, so you can also play this game here and you'll get actually something like the original does group.",
                    "label": 0
                },
                {
                    "sent": "This paper that he feels is actually when it square root of N. It doesn't seem to be as good as some of the more recent results when they get.",
                    "label": 0
                },
                {
                    "sent": "I think it's almost constant.",
                    "label": 0
                },
                {
                    "sent": "I forget the exact the state of that, however it's constant, but with a huge constant in France, like on the order of 10 to the 8th or something.",
                    "label": 0
                },
                {
                    "sent": "So so I mean.",
                    "label": 0
                },
                {
                    "sent": "It's constant, but the separation is like 1,000,000 standard deviation or 1 billion standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So it's more of a theoretical result.",
                    "label": 0
                },
                {
                    "sent": "Also very nice one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but really the separation kind of it actually kind of intuitive.",
                    "label": 0
                },
                {
                    "sent": "What is the separation assumption here?",
                    "label": 0
                },
                {
                    "sent": "You're using some curve.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're using some kernel, so you have this.",
                    "label": 0
                },
                {
                    "sent": "Which of the kernel?",
                    "label": 0
                },
                {
                    "sent": "So when your kernel is centered?",
                    "label": 0
                },
                {
                    "sent": "Here it shouldn't go over to the other side too much if that happened as bad.",
                    "label": 0
                },
                {
                    "sent": "So roughly speaking, the mass of the kernel, which is centered here on the other mixture component should be relatively small, and then you can write that out.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "I have a question in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "I mean gosh and mixtures are used a lot.",
                    "label": 0
                },
                {
                    "sent": "So my question is first usually we we experience improvements.",
                    "label": 0
                },
                {
                    "sent": "If we have quite sparse samples per per component and we also have the alignment problem.",
                    "label": 0
                },
                {
                    "sent": "So you have some interdependence between this alignment and the Goshen.",
                    "label": 0
                },
                {
                    "sent": "The Goshen mixture estimation.",
                    "label": 0
                },
                {
                    "sent": "What do you think is it?",
                    "label": 0
                },
                {
                    "sent": "Is this something which would help in speech recognition?",
                    "label": 0
                },
                {
                    "sent": "You know we're actually trying now to.",
                    "label": 0
                },
                {
                    "sent": "Very good question.",
                    "label": 0
                },
                {
                    "sent": "We're actually thinking about trying to do it with speech recognition, because, as you say, the mixture.",
                    "label": 0
                },
                {
                    "sent": "It's very standard kind of tool box from speech recognition.",
                    "label": 0
                },
                {
                    "sent": "The thing is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we've been actually sort of trying to work a little bit on everything is in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "There's so many layers to the problem that it's not easy to.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not always that you can just sort of plug something in at one level and it will propagate even if you do better at that level.",
                    "label": 0
                },
                {
                    "sent": "It's not my feeling is it would help, but it may need work to to get it working because there are many levels to to the problem.",
                    "label": 0
                },
                {
                    "sent": "And but my feeling is it should it should at least as initialization of 2:00 AM.",
                    "label": 0
                },
                {
                    "sent": "It should work.",
                    "label": 0
                },
                {
                    "sent": "Will definitely get a bit better mixture.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit related to the previous previous question.",
                    "label": 0
                },
                {
                    "sent": "It seems like if the width of your kernel is too big, you'll maybe miss, and if the width is too small, you'll maybe find too many.",
                    "label": 0
                },
                {
                    "sent": "Is that true?",
                    "label": 0
                },
                {
                    "sent": "And do you know how sensitive it is to the width?",
                    "label": 0
                },
                {
                    "sent": "It's not extremely sensitive, but you cannot get.",
                    "label": 0
                },
                {
                    "sent": "You know you cannot get it wildly wrong either, so it's kind of like the situation with the kernel method in general, you know you should.",
                    "label": 0
                },
                {
                    "sent": "It's not terribly sensitive, but if you get it really wrong, you kind of.",
                    "label": 0
                },
                {
                    "sent": "So a good good heuristic for that you know, take take as your scale takes the distance to the nearest neighbour, and then maybe take five times battle.",
                    "label": 0
                },
                {
                    "sent": "That's kind of a good scale heuristic, so if you're 3 times or five times distance to the nearest neighbors are probably will be doing OK.",
                    "label": 0
                },
                {
                    "sent": "But if you have counted times, so forget about it, right?",
                    "label": 0
                },
                {
                    "sent": "Once the kernel kind of goes over both of them, it looks like a single cluster to the kernel.",
                    "label": 0
                },
                {
                    "sent": "And if it's a kernel is very well in there alright?",
                    "label": 0
                },
                {
                    "sent": "Each point looks like a cluster, so.",
                    "label": 0
                },
                {
                    "sent": "But it leaves at the intuition.",
                    "label": 0
                },
                {
                    "sent": "OK, can you say something about how sensitive is the method to when the data is not quite Gaussian?",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually I didn't have, so it's.",
                    "label": 0
                },
                {
                    "sent": "What you can you can actually show something quite a bit more general.",
                    "label": 0
                },
                {
                    "sent": "You can show that if you have kind of high density components then the method will still work.",
                    "label": 0
                },
                {
                    "sent": "So basically you need separation between high density, high density regions and then in fact.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can see you can use it as a digit of 4, three and five, and it does a decent job, but you see, you need to take this positive eigenvectors which I mentioned.",
                    "label": 0
                },
                {
                    "sent": "They're not actually one, and two 123 up to 15 all correspond to the digits 4, then suddenly digit three is 16 and it's quite good.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's quite.",
                    "label": 0
                },
                {
                    "sent": "I mean this is not terribly difficult to separate this data set, but it's clearly not exactly Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type in your screen.",
                    "label": 0
                },
                {
                    "sent": "It's possible, yeah.",
                    "label": 0
                },
                {
                    "sent": "Funny how that works out that much.",
                    "label": 0
                },
                {
                    "sent": "Well, um.",
                    "label": 0
                },
                {
                    "sent": "That's a good question, so if you're thinking you're just smooth it out, you see when you actually.",
                    "label": 0
                },
                {
                    "sent": "If you just do smoothing, you'll probably get.",
                    "label": 0
                },
                {
                    "sent": "You may get other local Maxima, so it's not completely clear that you would.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have very good width of the kernel with smoothing, I think you will do fine, but my feeling is that the kernel, which will be kind of sensitive, right?",
                    "label": 0
                },
                {
                    "sent": "'cause this can very well look is a local maximum if you don't smooth well.",
                    "label": 0
                },
                {
                    "sent": "So yeah, my feeling is for smoothing you have to be very careful with the width of the kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank all the speakers again.",
                    "label": 0
                }
            ]
        }
    }
}