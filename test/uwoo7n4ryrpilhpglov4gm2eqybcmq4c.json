{
    "id": "uwoo7n4ryrpilhpglov4gm2eqybcmq4c",
    "title": "On the Stratification of Multi-Label Data",
    "info": {
        "author": [
            "Grigorios Tsoumakas, Department of Informatics, Aristotle University of Thessaloniki"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_tsoumakas_stratification/",
    "segmentation": [
        [
            "This is a joint work with Costadinos here.",
            "This now appears this student at University of Manchester and with young black olives."
        ],
        [
            "So in this work we look at the sampling, which as we know, plays a key role in several aspects in machine learning data mining.",
            "For example, we can use sampling to explore and to efficiently process vast amounts of data.",
            "And also we can use some.",
            "We typically use sampling to generate training, validation and test sets that again help us in various tasks like for accuracy estimation, model selection, hyperparameter selection meant overfitting avoidance too.",
            "To name issue.",
            "So in particularly when we're talking about classification tasks, then the stratified version of sampling is used.",
            "Stratifying, we mean that we try to maintain the proportion of examples of which class and the full data set within the subsets that we distribute the examples.",
            "So this has been found to improve.",
            "For example, the standard cross validation in terms of bias and variance of estimator."
        ],
        [
            "So what we are what we wanted to investigate in this work is how how bad?",
            "By applying stratified sampling to multi label data?",
            "So we're talking about data that are associated with a subset of a fixed set of labels.",
            "And here's an example of an image from Flickr that has been manually annotated.",
            "This part of the mask left 2010 talents, and you can see a number of labels that have been assigned to this.",
            "Here to this image.",
            "So."
        ],
        [
            "How can we stratify such data so the typical approach used in the literature is just to do random sampling, so in typical in experimental settings of papers received just a random cross validation, here we consider two different approaches to how to deal with this.",
            "One is to apply normal certification but try to maintain the distribution of label sets off label combinations and the bad thing with that is that they often in multi label data.",
            "We have a very large number of such label combinations and many of them are associated with very few examples, so that then becomes then stratification becomes impractical.",
            "And another view another way to approach the stratification for multi label data is to look independent labels and try to maintain the distribution of positive and negative labels within this subset.",
            "Of course I get this we cannot do it completely independently because we need to have a common examples in the produce subsets.",
            "So we propose an iterative way to deal with that."
        ],
        [
            "So just an example about the first, the first approach, so we have here.",
            "And data set the sampling data set with 9 examples and three labels.",
            "And let's say that we want to distribute them into three folds.",
            "So we.",
            "We will do it.",
            "We will do this based on the label sets the different combinations and in this data set we notice that we have six different labels labels that we denote them with decimal number of this of the decimal representation of this binary number.",
            "So we see that labels at 5 appears three times.",
            "Label set.",
            "One appears to Wise and the rest of the labels exhaust appear once, so we we normally stratified Now this data set based on the label sets."
        ],
        [
            "We will get something like this.",
            "Where labels at five will be distributed evenly to their faults.",
            "And of course, label labels at one will not appear in the same folder and the rest will be ratably distributed.",
            "So this sounds OK, but as I told you, the problem is that.",
            "In real world data we see many many combinations and these are fairly infrequent."
        ],
        [
            "So here, then, here are some that datasets and you can see value statistics like like the number of labels, the number of examples, the number of label combinations that in the results.",
            "And that these are all sorted according to this column and also this one this increasing it is the ratio of label sets over examples.",
            "And basically this shows us that as we go down this list, we have many many label sets and its label has very few examples.",
            "So this column shows us the average number of examples for labels.",
            "So what we notice here is that.",
            "We have a label sets that just have one.",
            "In all the annual datasets we have labels.",
            "It just occur once OK and in many of these datasets the average number of times that the label set occurs.",
            "It's actually very small.",
            "So it's going to be difficult to stratify then.",
            "This label sets in more than, let's say, then the ordinary.",
            "For example, 10 filters validation to stratify them correctly in the test subjects.",
            "And we also have situations where we have a rarity of labels themselves.",
            "So you see that here there are labels that just.",
            "Not label sets, but labels themselves that appears only once in some cases, so this this creates some problems in what we want to do."
        ],
        [
            "So here I will briefly sketch the iterative stratification algorithms that follows the second approach where we look in dependently the labels.",
            "So this in its iteration we select the label with the fewest remaining examples to be distributed.",
            "And the rationale behind this is that if we don't examine their labels in priority, then they may be distributed in an undesired way and we cannot correct that.",
            "Why for frequent labels we have the opportunity in subsequent preparations to correct any mismatches that have been made.",
            "So after we select the label with the fewest remaining graphics for its example of this label, we select the subset of distributed.",
            "First, in first priority based on the largest design with a number of examples for this label.",
            "So we select the subset that is missing.",
            "More examples for this label and in case of ties we put the example in that subset.",
            "That generally requires the largest number of examples and further price are randomly broken.",
            "And of course we updated the statistics like the desired number of examples that label that it's upset after we distribute its example.",
            "So just to give you an example of this process.",
            "Also note.",
            "That this algorithm does not apply a hard constraint on the desired number of examples, so it focuses.",
            "It tries to maintain the distribution of the labels and without actually producing equally equally sized subsets.",
            "In the case of, for example, validation."
        ],
        [
            "So here's an example where we have the same data set.",
            "In the last row we see the number of layers of appearances of its label, and again we want to stratify them, but we want to distribute them into three faults.",
            "And you see needs for the desired number of examples.",
            "So it's just the in this case as we have adjusted freefall providing base and we just divide the number of appearances of each label with the number of fault.",
            "So we will start with a label that is more rare, so label lab .2."
        ],
        [
            "And these are the examples of this label.",
            "The positive examples of this label.",
            "In the beginning we can just randomly distribute one exam."
        ],
        [
            "Of this label.",
            "But then the algorithm will select.",
            "One of these two faults, because this have the number with the largest number of desired examples, so we're OK here with this label, so we select."
        ],
        [
            "Probably one of those.",
            "And then they'll go."
        ],
        [
            "Continuous and put the library.",
            "Last example of the 2nd.",
            "So now we have 4 examples.",
            "Positive examples of label, Lambda one and five from label under 3.",
            "So we continue with label under one."
        ],
        [
            "Which has four examples.",
            "OK so here.",
            "We will select randomly between the first fault and the cell phone 'cause these are the two phones that have a largest desired number of this label, so this is OK.",
            "It has one example of this label, so we randomly select between the other two because they are missing 2 examples.",
            "It's so then we just randomly break this type."
        ],
        [
            "And so with this."
        ],
        [
            "Check the algorithm."
        ],
        [
            "Continuous."
        ],
        [
            "Until we distribute all examples, in this case, all examples have been distributed evenly, so there is no no problem with this constraint."
        ],
        [
            "So just to mention, where with triggering event for this work, we have considered the stratification of multi label data while developing our open source library.",
            "But what actually made us look more emotive?",
            "This problem was when we participated that last year Cymatic left silence and there was a problem 'cause we used an internal cross validation process to evaluate our annotation and probabilistic indexing.",
            "Label approaches, but for some labels it we had the issue that there were subsets in the cross validation with non positive example for some of the labels and then the main evaluation measure of that silence which was mean average precision.",
            "Macrobid failed to give a result so we didn't.",
            "We couldn't evaluate for that label in that fold.",
            "OK, so maybe we could just drop this label, but you know we wanted to achieve the best results possible."
        ],
        [
            "So this can happen when there are related labels in a data set and this causes apart from the mean average precision.",
            "It causes problems to other metrics like for example recall can become undefined and also in some cases precision can be can become an defined."
        ],
        [
            "So just a comparison of the approaches in terms of the examples we have seen.",
            "So in the random case I just distribute the examples in order so 123456789 and what we see and the rest are from the examples that you have already seen.",
            "And what we see is that the random approach.",
            "Produces false with non positive examples for label Lambda two and also does not correctly distribute the labels at 5 labels at 5.",
            "The second approach that was based on label sets distributes correctly the label set, but again falls into this problem of MP subjects, and that approach, on the other hand.",
            "Does not does not have the problem of subsets with non positive example for a label, but does not distribute well the labels it.",
            "So we can say from a statistical point of view that the label sets approach.",
            "What it does is it intends to maintain the joint distribution of the labels while the iterative approach tries to maintain the marginal distribution."
        ],
        [
            "So.",
            "We did some experiments with this data.",
            "We compared with these three approaches that I mentioned on the 15 multi label data set that you have seen briefly some statistics.",
            "And then we perform tenfold cross value based on experiments on lads on the ladder on the small datasets from from this collection and hold out.",
            "Elements on their loved ones that we show that different kinds of sampling and experiments were repeated five times and you will see the average of the results of these experiments."
        ],
        [
            "So in our first evaluation we wanted to look at how the labels are distributed and how the examples are distributed.",
            "'cause as I told you, I have iterative approach does not respect the desired number of examples.",
            "So for this reason we have these two metrics.",
            "This is basically counting the for its label and for its upset.",
            "This is the imbalance Redfield ratio of positive and negative examples in the subset and this is the positive to negative examples.",
            "Rescuing the full data set.",
            "So we see the deviation that so the smaller this value is the better.",
            "A label distribution is respected in the subset.",
            "And about the examples we compare the desired number of examples with actual number of examples in this Subs and we also measure how many subsets, how many false contain at least one label with zero positive examples and how?",
            "How many label pairs have this problem with zero positive examples?"
        ],
        [
            "So here we see the first metric, the label distribution.",
            "Actually, in this metric their worst results where there were notice for the random or the blue random approach.",
            "So we normalize.",
            "That is also you see the scaled with value according to the results of the random approach.",
            "And we have ordered the datasets as you have seen that in the table in increasing order of the number of label sets over the number of examples.",
            "So what do we see here?",
            "We see first that the second approach, the label based approach.",
            "Performs best.",
            "Has a lower label distribution for these three datasets that have a smaller number of the many examples for labels it.",
            "They perative approach produces a better label distribution for the rest of the data sets, and in general we see what we expected an increasing trend.",
            "Pair of words.",
            "Performance for the label set approach.",
            "As the number of.",
            "Examples better labels, it becomes smaller."
        ],
        [
            "For the example distribution, of course random entry level set approach.",
            "They distributed as evenly as possible the examples.",
            "While our approach did not observe this constraint as we said, and here we store, we sort the datasets in decreasing order of number of examples.",
            "So this is the data set with the most examples and what we notice is that.",
            "Anne.",
            "Of course, imperative certification does not perform well in this BIH cause it actually trade offs the example distribution to improve the label distribution and we especially see that we had before missing the larger datasets.",
            "Apart from this one media million own video data set.",
            "And this is becausw.",
            "Actually, this data set contains 1730 examples without any annotation, so this gave a lot of flexibility to the iterative stratification approach.",
            "To distribute these examples as better as it could.",
            "To based on their second attack criterium, maintaining the number of examples perception."
        ],
        [
            "As far as their subsets without label examples are concerned.",
            "We notice for this for this kind of values we can only obtain them in the cross validation experiments.",
            "OK, so in the holdout experiments it doesn't make much sense.",
            "So we first notice here that iterative stratification produces the lowest values.",
            "For these metrics, the best values for these metrics it avoids producing as much as possible subsets with without positive examples for label.",
            "Of course there are some data set that is, as I told you the beginning.",
            "They have absolutely rare labels with only one example, so there all methods fail.",
            "All methods do well in these two insane and emotions data sets that where there is a lot of examples per label set, so they don't produce any fall without without positive examples.",
            "And we see that only tentative certification manages to avoid this problem in the beta hand, image Clef 2010 data set rules, which was the inspiration for this work."
        ],
        [
            "So we did another set of experiments looking at the variance of the 10 fold cross validation estimates.",
            "We compared we use two algorithms for this by the binary relevance, one versus rest approach and the calibrated labor ranking approach.",
            "And the calibrate label ranking approach combines pairwise at one versus rest models and considers label dependencies.",
            "And we look at several metrics."
        ],
        [
            "So these are the various ranking results in all the datasets and we notice that the iterative approach.",
            "That's well in most of the metrics.",
            "An map.",
            "In the mapping them, in average precision, the label set approach performed better, but we must not forget that this.",
            "Actually this measure was only we managed to calculate it only based on synonym oceans because in the other data sets we have failures in producing subsets with positive examples and we couldn't calculate this method this method."
        ],
        [
            "So if we look at the data set where.",
            "The rest of labels prepared examples is small.",
            "Then we see that actually label set approach is expected as well.",
            "OK, because there we have a lot of examples."
        ],
        [
            "Are labeled.",
            "If we look at the other for example of other data sets where the ratio is large, then again iterative approach is better.",
            "As expected.",
            "We don't see map here be cause we have failures in all datasets for some methods."
        ],
        [
            "For Clr we only have results for the smaller datasets.",
            "For those with a lot of labels, its exams for labels it and here we see that the label set up roads does better.",
            "And OK again, this was expected based on there."
        ],
        [
            "Belt.",
            "And the comparison between the results for the two algorithms.",
            "So there's that.",
            "Iterative stratification, so if you look at the purple light and dark coins you see that the terrorist rectification shoots better than binary relevance approach.",
            "Instead of the Clr approach again, binary relevance looks independently at the labels, while Clr tries to take advantage of combinations and the inverse happens with calibrated label ranking algorithm, which in this case it's better with the label sets approach.",
            "If we compare the light and dark green columns."
        ],
        [
            "So to conclude.",
            "Label set based certification works well when this ratio of the number of labels and over examples is small.",
            "It works well with calibrated Lab label ranking, perhaps with other algorithms that try to exploit dependencies.",
            "Parody stratification works well in there in the other way when we have this ratio to be large and it worked well with binary relevance, perhaps with other label independent algorithms it works well for estimating the ranking loss and.",
            "It can't rain labels in a better way than the other approach that maintains the imbalance.",
            "Trash random stublic was worse in all these experiments and should be avoided.",
            "Contrary to what happens in the typical label experimental setup of the elite."
        ],
        [
            "As a future work.",
            "We want to investigate if we if we change the algorithm just it's very simple change to respect the desired number of examples.",
            "What will happen?",
            "We will probably lose on the labels distribution but we will have equally equally distributed examples, so this is usually what we want in sampling.",
            "We are working on a hybrid approach that will stratify based on label sets.",
            "The examples of frequent label sets and then continue with the iterative certification for the rest of the examples and of course we now in this work.",
            "We assume that the 10 fold cross validation estimate is a proper estimate of the generalization error, but we intend to conduct more statistically valid experiments.",
            "Track to us is also the bias and the variance of these two three schemes.",
            "So thank you very much for going to attention.",
            "OK, thank you very much and we have time for couple of questions.",
            "Any questions I nice problem.",
            "I think in your example you actually had a case which might be the reason why the Precalibrated label ranking because.",
            "You do not consider the correlation between the labels and in your example you had three examples of labels at 5, and your approach only put.",
            "Two of them in one set, one in this, the second set, and in the third one there was no example of labels at 5, so shouldn't you do something like considered pairs of labels or triples of labels and try to distribute that evenly?",
            "That's a very interesting idea, actually.",
            "Yes, this is a nice direction to improve their results.",
            "OK so I have a related question.",
            "This is actually before I saw your future work slide.",
            "Of course I mean I was told I was thinking about hybrid approaches and what would be the best strategies for hybrid approaches.",
            "And of course there is increasing complexity if you try to combine different parameters it becomes an optimization problem.",
            "So have you thought about it as an optimization problem with, you know, just select dynamically and iteratively.",
            "One of the strategies depending on the.",
            "Characteristics of the data would remain, to be honest, no, I haven't.",
            "I haven't thought this kind of approach, but again, could.",
            "It could also be an interesting idea to explore, OK?",
            "Alright, thank you very much and let's move on next talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a joint work with Costadinos here.",
                    "label": 0
                },
                {
                    "sent": "This now appears this student at University of Manchester and with young black olives.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we look at the sampling, which as we know, plays a key role in several aspects in machine learning data mining.",
                    "label": 1
                },
                {
                    "sent": "For example, we can use sampling to explore and to efficiently process vast amounts of data.",
                    "label": 0
                },
                {
                    "sent": "And also we can use some.",
                    "label": 0
                },
                {
                    "sent": "We typically use sampling to generate training, validation and test sets that again help us in various tasks like for accuracy estimation, model selection, hyperparameter selection meant overfitting avoidance too.",
                    "label": 1
                },
                {
                    "sent": "To name issue.",
                    "label": 1
                },
                {
                    "sent": "So in particularly when we're talking about classification tasks, then the stratified version of sampling is used.",
                    "label": 1
                },
                {
                    "sent": "Stratifying, we mean that we try to maintain the proportion of examples of which class and the full data set within the subsets that we distribute the examples.",
                    "label": 0
                },
                {
                    "sent": "So this has been found to improve.",
                    "label": 1
                },
                {
                    "sent": "For example, the standard cross validation in terms of bias and variance of estimator.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we are what we wanted to investigate in this work is how how bad?",
                    "label": 0
                },
                {
                    "sent": "By applying stratified sampling to multi label data?",
                    "label": 0
                },
                {
                    "sent": "So we're talking about data that are associated with a subset of a fixed set of labels.",
                    "label": 1
                },
                {
                    "sent": "And here's an example of an image from Flickr that has been manually annotated.",
                    "label": 0
                },
                {
                    "sent": "This part of the mask left 2010 talents, and you can see a number of labels that have been assigned to this.",
                    "label": 0
                },
                {
                    "sent": "Here to this image.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How can we stratify such data so the typical approach used in the literature is just to do random sampling, so in typical in experimental settings of papers received just a random cross validation, here we consider two different approaches to how to deal with this.",
                    "label": 1
                },
                {
                    "sent": "One is to apply normal certification but try to maintain the distribution of label sets off label combinations and the bad thing with that is that they often in multi label data.",
                    "label": 0
                },
                {
                    "sent": "We have a very large number of such label combinations and many of them are associated with very few examples, so that then becomes then stratification becomes impractical.",
                    "label": 1
                },
                {
                    "sent": "And another view another way to approach the stratification for multi label data is to look independent labels and try to maintain the distribution of positive and negative labels within this subset.",
                    "label": 0
                },
                {
                    "sent": "Of course I get this we cannot do it completely independently because we need to have a common examples in the produce subsets.",
                    "label": 0
                },
                {
                    "sent": "So we propose an iterative way to deal with that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just an example about the first, the first approach, so we have here.",
                    "label": 0
                },
                {
                    "sent": "And data set the sampling data set with 9 examples and three labels.",
                    "label": 0
                },
                {
                    "sent": "And let's say that we want to distribute them into three folds.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We will do it.",
                    "label": 0
                },
                {
                    "sent": "We will do this based on the label sets the different combinations and in this data set we notice that we have six different labels labels that we denote them with decimal number of this of the decimal representation of this binary number.",
                    "label": 0
                },
                {
                    "sent": "So we see that labels at 5 appears three times.",
                    "label": 0
                },
                {
                    "sent": "Label set.",
                    "label": 0
                },
                {
                    "sent": "One appears to Wise and the rest of the labels exhaust appear once, so we we normally stratified Now this data set based on the label sets.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will get something like this.",
                    "label": 0
                },
                {
                    "sent": "Where labels at five will be distributed evenly to their faults.",
                    "label": 0
                },
                {
                    "sent": "And of course, label labels at one will not appear in the same folder and the rest will be ratably distributed.",
                    "label": 0
                },
                {
                    "sent": "So this sounds OK, but as I told you, the problem is that.",
                    "label": 0
                },
                {
                    "sent": "In real world data we see many many combinations and these are fairly infrequent.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, then, here are some that datasets and you can see value statistics like like the number of labels, the number of examples, the number of label combinations that in the results.",
                    "label": 0
                },
                {
                    "sent": "And that these are all sorted according to this column and also this one this increasing it is the ratio of label sets over examples.",
                    "label": 0
                },
                {
                    "sent": "And basically this shows us that as we go down this list, we have many many label sets and its label has very few examples.",
                    "label": 0
                },
                {
                    "sent": "So this column shows us the average number of examples for labels.",
                    "label": 0
                },
                {
                    "sent": "So what we notice here is that.",
                    "label": 0
                },
                {
                    "sent": "We have a label sets that just have one.",
                    "label": 0
                },
                {
                    "sent": "In all the annual datasets we have labels.",
                    "label": 0
                },
                {
                    "sent": "It just occur once OK and in many of these datasets the average number of times that the label set occurs.",
                    "label": 0
                },
                {
                    "sent": "It's actually very small.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be difficult to stratify then.",
                    "label": 0
                },
                {
                    "sent": "This label sets in more than, let's say, then the ordinary.",
                    "label": 0
                },
                {
                    "sent": "For example, 10 filters validation to stratify them correctly in the test subjects.",
                    "label": 0
                },
                {
                    "sent": "And we also have situations where we have a rarity of labels themselves.",
                    "label": 0
                },
                {
                    "sent": "So you see that here there are labels that just.",
                    "label": 0
                },
                {
                    "sent": "Not label sets, but labels themselves that appears only once in some cases, so this this creates some problems in what we want to do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I will briefly sketch the iterative stratification algorithms that follows the second approach where we look in dependently the labels.",
                    "label": 0
                },
                {
                    "sent": "So this in its iteration we select the label with the fewest remaining examples to be distributed.",
                    "label": 0
                },
                {
                    "sent": "And the rationale behind this is that if we don't examine their labels in priority, then they may be distributed in an undesired way and we cannot correct that.",
                    "label": 1
                },
                {
                    "sent": "Why for frequent labels we have the opportunity in subsequent preparations to correct any mismatches that have been made.",
                    "label": 1
                },
                {
                    "sent": "So after we select the label with the fewest remaining graphics for its example of this label, we select the subset of distributed.",
                    "label": 1
                },
                {
                    "sent": "First, in first priority based on the largest design with a number of examples for this label.",
                    "label": 0
                },
                {
                    "sent": "So we select the subset that is missing.",
                    "label": 0
                },
                {
                    "sent": "More examples for this label and in case of ties we put the example in that subset.",
                    "label": 1
                },
                {
                    "sent": "That generally requires the largest number of examples and further price are randomly broken.",
                    "label": 0
                },
                {
                    "sent": "And of course we updated the statistics like the desired number of examples that label that it's upset after we distribute its example.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an example of this process.",
                    "label": 0
                },
                {
                    "sent": "Also note.",
                    "label": 0
                },
                {
                    "sent": "That this algorithm does not apply a hard constraint on the desired number of examples, so it focuses.",
                    "label": 0
                },
                {
                    "sent": "It tries to maintain the distribution of the labels and without actually producing equally equally sized subsets.",
                    "label": 0
                },
                {
                    "sent": "In the case of, for example, validation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example where we have the same data set.",
                    "label": 0
                },
                {
                    "sent": "In the last row we see the number of layers of appearances of its label, and again we want to stratify them, but we want to distribute them into three faults.",
                    "label": 0
                },
                {
                    "sent": "And you see needs for the desired number of examples.",
                    "label": 0
                },
                {
                    "sent": "So it's just the in this case as we have adjusted freefall providing base and we just divide the number of appearances of each label with the number of fault.",
                    "label": 0
                },
                {
                    "sent": "So we will start with a label that is more rare, so label lab .2.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are the examples of this label.",
                    "label": 0
                },
                {
                    "sent": "The positive examples of this label.",
                    "label": 1
                },
                {
                    "sent": "In the beginning we can just randomly distribute one exam.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this label.",
                    "label": 0
                },
                {
                    "sent": "But then the algorithm will select.",
                    "label": 0
                },
                {
                    "sent": "One of these two faults, because this have the number with the largest number of desired examples, so we're OK here with this label, so we select.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probably one of those.",
                    "label": 0
                },
                {
                    "sent": "And then they'll go.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Continuous and put the library.",
                    "label": 0
                },
                {
                    "sent": "Last example of the 2nd.",
                    "label": 0
                },
                {
                    "sent": "So now we have 4 examples.",
                    "label": 0
                },
                {
                    "sent": "Positive examples of label, Lambda one and five from label under 3.",
                    "label": 1
                },
                {
                    "sent": "So we continue with label under one.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which has four examples.",
                    "label": 0
                },
                {
                    "sent": "OK so here.",
                    "label": 0
                },
                {
                    "sent": "We will select randomly between the first fault and the cell phone 'cause these are the two phones that have a largest desired number of this label, so this is OK.",
                    "label": 0
                },
                {
                    "sent": "It has one example of this label, so we randomly select between the other two because they are missing 2 examples.",
                    "label": 0
                },
                {
                    "sent": "It's so then we just randomly break this type.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so with this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continuous.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until we distribute all examples, in this case, all examples have been distributed evenly, so there is no no problem with this constraint.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to mention, where with triggering event for this work, we have considered the stratification of multi label data while developing our open source library.",
                    "label": 0
                },
                {
                    "sent": "But what actually made us look more emotive?",
                    "label": 0
                },
                {
                    "sent": "This problem was when we participated that last year Cymatic left silence and there was a problem 'cause we used an internal cross validation process to evaluate our annotation and probabilistic indexing.",
                    "label": 0
                },
                {
                    "sent": "Label approaches, but for some labels it we had the issue that there were subsets in the cross validation with non positive example for some of the labels and then the main evaluation measure of that silence which was mean average precision.",
                    "label": 0
                },
                {
                    "sent": "Macrobid failed to give a result so we didn't.",
                    "label": 0
                },
                {
                    "sent": "We couldn't evaluate for that label in that fold.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe we could just drop this label, but you know we wanted to achieve the best results possible.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this can happen when there are related labels in a data set and this causes apart from the mean average precision.",
                    "label": 0
                },
                {
                    "sent": "It causes problems to other metrics like for example recall can become undefined and also in some cases precision can be can become an defined.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just a comparison of the approaches in terms of the examples we have seen.",
                    "label": 0
                },
                {
                    "sent": "So in the random case I just distribute the examples in order so 123456789 and what we see and the rest are from the examples that you have already seen.",
                    "label": 0
                },
                {
                    "sent": "And what we see is that the random approach.",
                    "label": 0
                },
                {
                    "sent": "Produces false with non positive examples for label Lambda two and also does not correctly distribute the labels at 5 labels at 5.",
                    "label": 0
                },
                {
                    "sent": "The second approach that was based on label sets distributes correctly the label set, but again falls into this problem of MP subjects, and that approach, on the other hand.",
                    "label": 0
                },
                {
                    "sent": "Does not does not have the problem of subsets with non positive example for a label, but does not distribute well the labels it.",
                    "label": 0
                },
                {
                    "sent": "So we can say from a statistical point of view that the label sets approach.",
                    "label": 0
                },
                {
                    "sent": "What it does is it intends to maintain the joint distribution of the labels while the iterative approach tries to maintain the marginal distribution.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We did some experiments with this data.",
                    "label": 0
                },
                {
                    "sent": "We compared with these three approaches that I mentioned on the 15 multi label data set that you have seen briefly some statistics.",
                    "label": 0
                },
                {
                    "sent": "And then we perform tenfold cross value based on experiments on lads on the ladder on the small datasets from from this collection and hold out.",
                    "label": 0
                },
                {
                    "sent": "Elements on their loved ones that we show that different kinds of sampling and experiments were repeated five times and you will see the average of the results of these experiments.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our first evaluation we wanted to look at how the labels are distributed and how the examples are distributed.",
                    "label": 0
                },
                {
                    "sent": "'cause as I told you, I have iterative approach does not respect the desired number of examples.",
                    "label": 0
                },
                {
                    "sent": "So for this reason we have these two metrics.",
                    "label": 0
                },
                {
                    "sent": "This is basically counting the for its label and for its upset.",
                    "label": 0
                },
                {
                    "sent": "This is the imbalance Redfield ratio of positive and negative examples in the subset and this is the positive to negative examples.",
                    "label": 0
                },
                {
                    "sent": "Rescuing the full data set.",
                    "label": 0
                },
                {
                    "sent": "So we see the deviation that so the smaller this value is the better.",
                    "label": 0
                },
                {
                    "sent": "A label distribution is respected in the subset.",
                    "label": 0
                },
                {
                    "sent": "And about the examples we compare the desired number of examples with actual number of examples in this Subs and we also measure how many subsets, how many false contain at least one label with zero positive examples and how?",
                    "label": 0
                },
                {
                    "sent": "How many label pairs have this problem with zero positive examples?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see the first metric, the label distribution.",
                    "label": 0
                },
                {
                    "sent": "Actually, in this metric their worst results where there were notice for the random or the blue random approach.",
                    "label": 0
                },
                {
                    "sent": "So we normalize.",
                    "label": 0
                },
                {
                    "sent": "That is also you see the scaled with value according to the results of the random approach.",
                    "label": 0
                },
                {
                    "sent": "And we have ordered the datasets as you have seen that in the table in increasing order of the number of label sets over the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So what do we see here?",
                    "label": 0
                },
                {
                    "sent": "We see first that the second approach, the label based approach.",
                    "label": 0
                },
                {
                    "sent": "Performs best.",
                    "label": 0
                },
                {
                    "sent": "Has a lower label distribution for these three datasets that have a smaller number of the many examples for labels it.",
                    "label": 0
                },
                {
                    "sent": "They perative approach produces a better label distribution for the rest of the data sets, and in general we see what we expected an increasing trend.",
                    "label": 0
                },
                {
                    "sent": "Pair of words.",
                    "label": 0
                },
                {
                    "sent": "Performance for the label set approach.",
                    "label": 0
                },
                {
                    "sent": "As the number of.",
                    "label": 0
                },
                {
                    "sent": "Examples better labels, it becomes smaller.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the example distribution, of course random entry level set approach.",
                    "label": 0
                },
                {
                    "sent": "They distributed as evenly as possible the examples.",
                    "label": 0
                },
                {
                    "sent": "While our approach did not observe this constraint as we said, and here we store, we sort the datasets in decreasing order of number of examples.",
                    "label": 0
                },
                {
                    "sent": "So this is the data set with the most examples and what we notice is that.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Of course, imperative certification does not perform well in this BIH cause it actually trade offs the example distribution to improve the label distribution and we especially see that we had before missing the larger datasets.",
                    "label": 0
                },
                {
                    "sent": "Apart from this one media million own video data set.",
                    "label": 0
                },
                {
                    "sent": "And this is becausw.",
                    "label": 1
                },
                {
                    "sent": "Actually, this data set contains 1730 examples without any annotation, so this gave a lot of flexibility to the iterative stratification approach.",
                    "label": 0
                },
                {
                    "sent": "To distribute these examples as better as it could.",
                    "label": 0
                },
                {
                    "sent": "To based on their second attack criterium, maintaining the number of examples perception.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As far as their subsets without label examples are concerned.",
                    "label": 1
                },
                {
                    "sent": "We notice for this for this kind of values we can only obtain them in the cross validation experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the holdout experiments it doesn't make much sense.",
                    "label": 0
                },
                {
                    "sent": "So we first notice here that iterative stratification produces the lowest values.",
                    "label": 1
                },
                {
                    "sent": "For these metrics, the best values for these metrics it avoids producing as much as possible subsets with without positive examples for label.",
                    "label": 0
                },
                {
                    "sent": "Of course there are some data set that is, as I told you the beginning.",
                    "label": 0
                },
                {
                    "sent": "They have absolutely rare labels with only one example, so there all methods fail.",
                    "label": 0
                },
                {
                    "sent": "All methods do well in these two insane and emotions data sets that where there is a lot of examples per label set, so they don't produce any fall without without positive examples.",
                    "label": 1
                },
                {
                    "sent": "And we see that only tentative certification manages to avoid this problem in the beta hand, image Clef 2010 data set rules, which was the inspiration for this work.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did another set of experiments looking at the variance of the 10 fold cross validation estimates.",
                    "label": 0
                },
                {
                    "sent": "We compared we use two algorithms for this by the binary relevance, one versus rest approach and the calibrated labor ranking approach.",
                    "label": 0
                },
                {
                    "sent": "And the calibrate label ranking approach combines pairwise at one versus rest models and considers label dependencies.",
                    "label": 0
                },
                {
                    "sent": "And we look at several metrics.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the various ranking results in all the datasets and we notice that the iterative approach.",
                    "label": 0
                },
                {
                    "sent": "That's well in most of the metrics.",
                    "label": 0
                },
                {
                    "sent": "An map.",
                    "label": 0
                },
                {
                    "sent": "In the mapping them, in average precision, the label set approach performed better, but we must not forget that this.",
                    "label": 0
                },
                {
                    "sent": "Actually this measure was only we managed to calculate it only based on synonym oceans because in the other data sets we have failures in producing subsets with positive examples and we couldn't calculate this method this method.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we look at the data set where.",
                    "label": 0
                },
                {
                    "sent": "The rest of labels prepared examples is small.",
                    "label": 0
                },
                {
                    "sent": "Then we see that actually label set approach is expected as well.",
                    "label": 0
                },
                {
                    "sent": "OK, because there we have a lot of examples.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are labeled.",
                    "label": 0
                },
                {
                    "sent": "If we look at the other for example of other data sets where the ratio is large, then again iterative approach is better.",
                    "label": 0
                },
                {
                    "sent": "As expected.",
                    "label": 0
                },
                {
                    "sent": "We don't see map here be cause we have failures in all datasets for some methods.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For Clr we only have results for the smaller datasets.",
                    "label": 0
                },
                {
                    "sent": "For those with a lot of labels, its exams for labels it and here we see that the label set up roads does better.",
                    "label": 0
                },
                {
                    "sent": "And OK again, this was expected based on there.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Belt.",
                    "label": 0
                },
                {
                    "sent": "And the comparison between the results for the two algorithms.",
                    "label": 0
                },
                {
                    "sent": "So there's that.",
                    "label": 0
                },
                {
                    "sent": "Iterative stratification, so if you look at the purple light and dark coins you see that the terrorist rectification shoots better than binary relevance approach.",
                    "label": 0
                },
                {
                    "sent": "Instead of the Clr approach again, binary relevance looks independently at the labels, while Clr tries to take advantage of combinations and the inverse happens with calibrated label ranking algorithm, which in this case it's better with the label sets approach.",
                    "label": 0
                },
                {
                    "sent": "If we compare the light and dark green columns.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "Label set based certification works well when this ratio of the number of labels and over examples is small.",
                    "label": 1
                },
                {
                    "sent": "It works well with calibrated Lab label ranking, perhaps with other algorithms that try to exploit dependencies.",
                    "label": 0
                },
                {
                    "sent": "Parody stratification works well in there in the other way when we have this ratio to be large and it worked well with binary relevance, perhaps with other label independent algorithms it works well for estimating the ranking loss and.",
                    "label": 0
                },
                {
                    "sent": "It can't rain labels in a better way than the other approach that maintains the imbalance.",
                    "label": 0
                },
                {
                    "sent": "Trash random stublic was worse in all these experiments and should be avoided.",
                    "label": 0
                },
                {
                    "sent": "Contrary to what happens in the typical label experimental setup of the elite.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a future work.",
                    "label": 0
                },
                {
                    "sent": "We want to investigate if we if we change the algorithm just it's very simple change to respect the desired number of examples.",
                    "label": 0
                },
                {
                    "sent": "What will happen?",
                    "label": 0
                },
                {
                    "sent": "We will probably lose on the labels distribution but we will have equally equally distributed examples, so this is usually what we want in sampling.",
                    "label": 0
                },
                {
                    "sent": "We are working on a hybrid approach that will stratify based on label sets.",
                    "label": 0
                },
                {
                    "sent": "The examples of frequent label sets and then continue with the iterative certification for the rest of the examples and of course we now in this work.",
                    "label": 0
                },
                {
                    "sent": "We assume that the 10 fold cross validation estimate is a proper estimate of the generalization error, but we intend to conduct more statistically valid experiments.",
                    "label": 0
                },
                {
                    "sent": "Track to us is also the bias and the variance of these two three schemes.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much for going to attention.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much and we have time for couple of questions.",
                    "label": 0
                },
                {
                    "sent": "Any questions I nice problem.",
                    "label": 0
                },
                {
                    "sent": "I think in your example you actually had a case which might be the reason why the Precalibrated label ranking because.",
                    "label": 0
                },
                {
                    "sent": "You do not consider the correlation between the labels and in your example you had three examples of labels at 5, and your approach only put.",
                    "label": 0
                },
                {
                    "sent": "Two of them in one set, one in this, the second set, and in the third one there was no example of labels at 5, so shouldn't you do something like considered pairs of labels or triples of labels and try to distribute that evenly?",
                    "label": 0
                },
                {
                    "sent": "That's a very interesting idea, actually.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is a nice direction to improve their results.",
                    "label": 0
                },
                {
                    "sent": "OK so I have a related question.",
                    "label": 0
                },
                {
                    "sent": "This is actually before I saw your future work slide.",
                    "label": 0
                },
                {
                    "sent": "Of course I mean I was told I was thinking about hybrid approaches and what would be the best strategies for hybrid approaches.",
                    "label": 0
                },
                {
                    "sent": "And of course there is increasing complexity if you try to combine different parameters it becomes an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So have you thought about it as an optimization problem with, you know, just select dynamically and iteratively.",
                    "label": 0
                },
                {
                    "sent": "One of the strategies depending on the.",
                    "label": 0
                },
                {
                    "sent": "Characteristics of the data would remain, to be honest, no, I haven't.",
                    "label": 0
                },
                {
                    "sent": "I haven't thought this kind of approach, but again, could.",
                    "label": 0
                },
                {
                    "sent": "It could also be an interesting idea to explore, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you very much and let's move on next talk.",
                    "label": 0
                }
            ]
        }
    }
}