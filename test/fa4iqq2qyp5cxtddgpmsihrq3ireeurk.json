{
    "id": "fa4iqq2qyp5cxtddgpmsihrq3ireeurk",
    "title": "Learning and Solving Many-Player Games through a Cluster-Based Representation",
    "info": {
        "author": [
            "Sevan G. Ficici, Harvard School of Engineering and Applied Sciences, Harvard University"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Mathematics->Game Theory",
            "Top->Computer Science->Machine Learning->Clustering",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/uai08_ficici_lsmpg/",
    "segmentation": [
        [
            "So we're interested in situations with a large number of interacting agents is actually a link to the talk.",
            "We're interested in situations like electronic markets or online gaming.",
            "There are many agents that have varied interests interacting with each other, and we want to use the standard tools of game theory to model."
        ],
        [
            "These kinds of situations.",
            "Unfortunately, game theory is usually limited to situations with a small number of agents, and the problem is that the size of the gain increases exponentially with the number of agents, so we can't even represent gains with large number of agents letter."
        ],
        [
            "Can solve them?",
            "One approach that has been used in EU AI community and elsewhere is to assume that there is some underlying structure to the game that makes the representation and solution of the game tractable.",
            "An example of this approach is graphical games.",
            "Another example is action graph games, and there are others."
        ],
        [
            "Our approach is a little bit different.",
            "We don't assume that the game itself has structure.",
            "Instead what we do is we try to find a compact approximation of the game.",
            "The approximation does have structure and hopefully our approximation will capture enough of the detail of the actual game that the strategies that we obtained from the approximation would be good strategies for the actual game and the other thing we do is since the game itself is too large to represent, we don't have access to the full representation of the game.",
            "So we learn our compact approximation die."
        ],
        [
            "Actually, from data.",
            "The idea behind our approximation is simple.",
            "In many games we have situations where different agents have what we call a similar strategic view of the game.",
            "Similar strategic view means they have similar pay offs and they have similar effects on other agents pay offs.",
            "So for example in the market companies in the same sector may have a similar strategic view or in online games similar players with similar properties may have a similar strategic view and then what we do is we cluster together agents that have a similar strategic view to obtain.",
            "A club."
        ],
        [
            "The based representation.",
            "An example of a game that has these characteristics is a vendor game.",
            "There are a number of vendors, a large number, each of which has to choose to sell goods in a certain location in a stadium, and there are different categories of vendors, so there may be sandwiches, vendors and drinks vendors, and within categories.",
            "There are also individual vendors who are further differentiated.",
            "So within the drinks category there may be a lemonade vendor and a beer vendor.",
            "Now there are substitute and complement relationships between different.",
            "Vendors and these relationships hold in a general way across categories, but there is variation between individual vendors within categories.",
            "So drinks vendors may in general be substitutes for each other, but lemonade may be a more severe substitute for orange juice than it is for beer."
        ],
        [
            "So here's an overview of our approach.",
            "We start with some data.",
            "From that we learn a clustered representation.",
            "From that clustered representation, we construct what we call the Twins game.",
            "We solve the Twins game to obtain strategies and then play them in the world and get some performance."
        ],
        [
            "So let's start with the data."
        ],
        [
            "First of all, the definition of the game.",
            "There's a number N of agents.",
            "There is a set of pure strategies.",
            "We assume that every agent has the same set of pure strategies and every agent has a payoff function that Maps strategy profiles of all the agents to utility's for that agent.",
            "And of course this payoff function is not explicitly represented."
        ],
        [
            "Our data consists of some rounds of interaction of of the game, so there's some number of agents in this case 10 agents, and we have some number of rounds.",
            "In each round we have the actions of all the agents and the payoff that every."
        ],
        [
            "The agent receives.",
            "Where does the data come from?",
            "We have two answers to this.",
            "One possible answer is that this is a game that's actually being played in the world and we get to observe what the agents in the world are doing in the playoffs they receive, and from that we obtain the data."
        ],
        [
            "TA the other option is that we have access to a simulator that, given a strategy profile, can tell us what the pay offs are, and the simulator may be represented in a compact way, whereas the full game matrix is not."
        ],
        [
            "So that's the data.",
            "Now I'll talk about our clustered representation and how we learn it.",
            "So we're going to take the N agents and cluster them into K clusters, where K is much smaller than N and the assumptions that our approximation makes our that agents in the same cluster receive the same pay offs when they play the same strategies.",
            "Agents in the same cluster have the same influences on other agents, and the payoff that an agent in the cluster gets depends only on the proportion of agents in each cluster playing in each strategy.",
            "And Furthermore, the pay offs are linear in those propose."
        ],
        [
            "Oceans.",
            "Under those assumptions, we construct a set of regression equations.",
            "We have one equation for every cluster and every pure strategy which specifies the payoff that an agent in that cluster receives.",
            "For playing that pure strategy.",
            "And it depends on the number of agents in each cluster, or the proportion of agents in each cluster.",
            "Playing in each strategy, and each of these regression equations has a term for every strategy profile of the clusters.",
            "Plus a con."
        ],
        [
            "Instant term.",
            "So, for example, suppose there are two clusters we want to estimate the payoff for an agent in cluster A for playing left.",
            "So first of all, this will have a term for the cluster strategy profile left left.",
            "There's a ton of beta parameter corresponding to that cluster strategy profile for an agent in cluster A.",
            "Playing left that is multiplied by the proportion of agents in cluster A who play left times the proportion of agents in cluster B who play left."
        ],
        [
            "And then we have another term for each of the different cluster strategy profiles and then a constant term at the end."
        ],
        [
            "So we have these equations.",
            "We have data, so naturally we learn them by linear regression.",
            "In particular, let's assume we were given a clustering.",
            "Then each agents payoff in around that we observe gives us festival of the cluster of the agent.",
            "The strategy that the agent played, the proportion of agents in each cluster playing in each strategy, and the payoff that that agent received.",
            "So that gives US1 estimate and a round of play gives us an estimate for.",
            "Every agent, so N estimates."
        ],
        [
            "Up to now, I've assumed the clustering was given, but in fact we don't assume that the clusters are known in advance.",
            "We learn the clusters from data and we use a very simple approach.",
            "We use K means clustering, in which the features are the average pay offs that the agents receive for different strategies."
        ],
        [
            "That they can play.",
            "So that's a clustered representation."
        ],
        [
            "What do we do with that?",
            "A natural thing we can do is to construct AK Player game where there's one player for every cluster.",
            "We estimate the payoff of that game using the regression equations.",
            "We solve that game to obtain a Nash equilibrium, and we instruct all agents in that in a cluster to play that clusters equally."
        ],
        [
            "And strategy.",
            "Unfortunately, that idea does not work very well.",
            "The reason is as follows.",
            "Suppose we solve this cluster game and we find that a clustered optimal strategy is to play left.",
            "So we're going to tell all the agents in the cluster to play left, but it may be the case that if all the agents are playing left, then a particular agent want to play right.",
            "For example, if all the drinks vendors are told to go to one location in the stadium, then a lemonade vendor might say I want to go to a different location, so.",
            "In this setting, individual agents have an incentive to deviate from their prescribed cluster strategies.",
            "We say that the representation is not individually responsive, meaning it doesn't respond to the interests of India."
        ],
        [
            "Actual agents.",
            "So what we do instead is we construct a 2K player game where every player where every cluster is represented by two identical twin players.",
            "So a cluster C is represented by Twins C&C Prime and these Twins are really identical.",
            "They have symmetric payoff functions, and they have the same."
        ],
        [
            "Affect on others pay offs.",
            "There are various ways to interpret these Twins from the point of view of 1 twin, let's say CC itself represents a single agent in the cluster C, Whereas its twin C prime represents cluster see in aggregate.",
            "From the point of view of C prime the situate the view is symmetric.",
            "See primed itself represents a single agent in cluster C, whereas it's Twins see represents the clustering aggregate and from the view of a player corresponding to a different cluster CNC prime between them Rep."
        ],
        [
            "Present cluster C. So let's talk about how we specify the pay offs in the Twins game.",
            "Let's say there are two clusters and be so four players a a prime B&B prime, and they're playing this strategy.",
            "Profile a plays left a prime, plays right, B plays left and be prime plays."
        ],
        [
            "Left so the payoff to play A is as follows from the point of view of player AA represents an individual agent in cluster A, so this is the payoff to an agent in cluster A for playing left, which is what player a plays.",
            "A prime represents the cluster as a whole, so this is the payoff when the cluster A as a whole plays right, which is what a prime is playing, and cluster B here is playing left."
        ],
        [
            "The payoff to player a prime.",
            "This symmetric is the payoff to an agent in cluster A for playing right, which is what a prime is playing when cluster A as a whole is playing left, which is what a is playing, and cluster B is playing."
        ],
        [
            "Left now for the payoff to play a be.",
            "Here A is playing left and a prime is playing right from the point of view of player.",
            "Be with you A and a prying as between them representing cluster A.",
            "So we interpret this as saying that half of cluster A is playing left and half of cluster A is playing right.",
            "So the payoff to Player B is the payoff to an agent in cluster before playing left when cluster B is a whole plays left and half of cluster A plays left and the other half plays right."
        ],
        [
            "And in this case, the payoff to player B prime is exactly the same."
        ],
        [
            "So we've constructed."
        ],
        [
            "This Twins game.",
            "How do we solve it?",
            "So we want a three step process.",
            "We solve the Twins game to obtain Nash equilibrium.",
            "We assign the strategies of the Twins to the corresponding clusters and then we tell the agents in a cluster to play their cluster strategies.",
            "Now when we assign the strategies of Twins to corresponding clusters, what we want is Nash equilibrium of the Twins game in which the twin players are playing identical mixed strategies and we call such equilibrium twin symmetric Nash equilibrium there Gary."
        ],
        [
            "Party to exist.",
            "Earlier I argued that the K player cluster gained the naive K player cluster gain is not individually responsive.",
            "Now I will argue to the Twins game is individually responsive.",
            "This is because, in a Nash equilibrium of the Twins game, player C is playing the best response to the other players, including its twin C prime by construction of the Twins game, the payoff that see gets is the payoff to an individual agent in cluster.",
            "See when the cluster is a whole players.",
            "C prime does.",
            "That means that an agent in Cluster C strategy is a best response to the strategy of player C prime.",
            "But in a twin symmetric Nash equilibrium, the strategy that cluster see is playing is the same as that of players.",
            "See prime.",
            "That means that an individual agent in cluster C is playing a best response to the strategy of cluster C and therefore no wins.",
            "Agent has incentive to deviate from its cluster strategy, and the representation is indivi."
        ],
        [
            "Truly responsive.",
            "So now we have stre."
        ],
        [
            "Geez, what is the performance?",
            "So we solve the game.",
            "We assign strategies to agents.",
            "We have them play a number of rounds, and then we compute the average regret of the agents, the regret of an agent is how much better the agent could have done if it had chosen optimally up your strategy and stuck to it.",
            "Given what the other agents did.",
            "And for Nash equilibrium, the expected regret is 0.",
            "So we want to get a regret as close to zero as possible.",
            "If we have a small regret, that means we're getting behavior that's similar to matching."
        ],
        [
            "Librium we compared the number of methods.",
            "We compared our 2K player Twins game.",
            "The K player game that I described to you.",
            "We also looked at a model free approach in which an agent simply chooses a strategy that would have maximized this rewards in the past.",
            "And we also compared our methods to a method of Wellmann ET al, which is also a clustering method, but a little bit different from ours.",
            "It only applies to symmetric gains and also their approach does not learn a representation from day."
        ],
        [
            "After and the results for the vendor game.",
            "What we see is that the regret of the Twins gain is much better than that of the K player game in the model free game.",
            "The model free approach.",
            "It's it's about an order of magnitude better."
        ],
        [
            "And we found similar results to this graph under a variety of different conditions, and one of the things we observed is that our learning actually works quite well even with a small number of observations.",
            "Even with us few is 3 rounds of the game.",
            "We can get a good regression fit and low regret.",
            "And also we found that our method scales up well to many agents.",
            "We scaled it up to 200 agents, but there were no inherent limitations there.",
            "I think we could schedule a top beyond that without much problem."
        ],
        [
            "The other domain we looked at in our experiments is the Santa Fe bar problem.",
            "In this domain there is a number N of agents who have to decide whether or not to go to a bar.",
            "The bar has a certain capacity and if the bar is not full, agents prefer to go to the bar.",
            "If the bar is full, agents prefer to stay home.",
            "Now this is a symmetric game, so we can apply well metals method from our point of view it means that all the agents have exactly the stress.",
            "Same strategic view of the game, so there's only one cluster.",
            "And also another property of this game is that the payoff is nonlinear in the proportion of agents who go to the bar.",
            "So it's an interesting test to see whether our method in which the approximation makes the assumption of linearity can work well in this setting."
        ],
        [
            "So since that's one cluster for our approach, we constructed a 2A2 player game.",
            "Our method produces only symmetric Nash equilibrium.",
            "We compared to Wellman ET al with both two and five clusters.",
            "Two clusters is a direct comparison because that also requires solving the two player game.",
            "Five clusters requires more computational resources, but maybe is able to do better, and their approach produces both symmetric and asymmetric Nash equilibrium.",
            "So we compared to both.",
            "And we varied the bark."
        ],
        [
            "Passati in our experiments.",
            "And what we found is the following when the bar capacity is an exact multiple of the cluster size then.",
            "Well, well Minitel's approach is able to find exactly an asymmetric Nash equilibrium, but in other cases we found that our method gets lower regrets than their approach with two clusters.",
            "It gets slightly higher regrets than their approach with five clusters, but is using using fewer computational resources.",
            "And it's also our approach is not sensitive to the bar capacity, so our approach gets good regrets across different."
        ],
        [
            "Bar capacities so to conclude we're interested in gangs with many players which are hard to tackle using standard game theoretic approaches.",
            "We presented a method that learns an approximate representation of a many players gain from data of play of the game.",
            "Our method works by clustering the agents into K clusters and then estimating the payoff spy linear regression.",
            "We then construct and solve a 2K player game called the Twins Game that ensures individual responsiveness that individual agents aren't have incentive to stick to their cluster strategies.",
            "Our results showed that our methods work well with few observations and scaling up to many agents an for future work.",
            "We are limited so far with our algorithms for computing twin symmetric Nash equilibrium.",
            "They're not very sophisticated, so we want better algorithms for doing that, and we're also looking at more sophisticated clustering algorithms to discover the different clusters.",
            "Thank you very much.",
            "This question there.",
            "The Twins game seems like a nice way of preserving the strategic properties that can be distraction for the approximation.",
            "You see using that independent of the learning, so that you should say stuff price and broad parameters in ancient Greece and that benifet Twins game.",
            "Yes, that's a good question.",
            "We haven't we should have, but we haven't tried gains where there is actually an exact clustering and.",
            "The games that satisfy all the assumptions of the approximation where we would not have to learn, we would just produce the clustering and seeing whether our Twins game actually captures the strategic nature of the original game and whether we get good strategies.",
            "So that's a that's a good question.",
            "I mean one.",
            "One difference is that the Twins game does if, let's say we have one cluster, the Twins game will find symmetric Nash equilibrium for the game, whereas you may be interested in a symmetric equilibrium.",
            "I have a question.",
            "Can you prove any kind of asymptotically theorems about?",
            "When your approximation gives you the right answer.",
            "Many agents or anything like that.",
            "I don't know.",
            "I mean, we have thought about proving properties we haven't looked at the limit of.",
            "Agents and we don't have any good theoretical results at the moment.",
            "This?",
            "One more.",
            "So it seems like the game will have in general have different values for players in different clusters, and it seems like in many real real world problems the number of players in the cluster responds to the value of the game for that cluster.",
            "And is there any way to deal with that in future work with this too intractable?",
            "So you mean that people or agents will choose clusters that have good values?",
            "So for example, if you're coming to the stadium and deciding what to sell, if there are a lot of drinks vendors and you decide not to be a drinks vendor, that kind of thing.",
            "We could do that with more sophisticated clustering methods by putting priors over the cluster sizes.",
            "That's how I would envision doing it.",
            "Seems like a natural thing to do.",
            "I mean for this particular application, it seems like you would want the opposite of a Chinese restaurant process, where people try to spread out.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're interested in situations with a large number of interacting agents is actually a link to the talk.",
                    "label": 0
                },
                {
                    "sent": "We're interested in situations like electronic markets or online gaming.",
                    "label": 0
                },
                {
                    "sent": "There are many agents that have varied interests interacting with each other, and we want to use the standard tools of game theory to model.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These kinds of situations.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, game theory is usually limited to situations with a small number of agents, and the problem is that the size of the gain increases exponentially with the number of agents, so we can't even represent gains with large number of agents letter.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can solve them?",
                    "label": 0
                },
                {
                    "sent": "One approach that has been used in EU AI community and elsewhere is to assume that there is some underlying structure to the game that makes the representation and solution of the game tractable.",
                    "label": 1
                },
                {
                    "sent": "An example of this approach is graphical games.",
                    "label": 1
                },
                {
                    "sent": "Another example is action graph games, and there are others.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our approach is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "We don't assume that the game itself has structure.",
                    "label": 1
                },
                {
                    "sent": "Instead what we do is we try to find a compact approximation of the game.",
                    "label": 0
                },
                {
                    "sent": "The approximation does have structure and hopefully our approximation will capture enough of the detail of the actual game that the strategies that we obtained from the approximation would be good strategies for the actual game and the other thing we do is since the game itself is too large to represent, we don't have access to the full representation of the game.",
                    "label": 1
                },
                {
                    "sent": "So we learn our compact approximation die.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, from data.",
                    "label": 0
                },
                {
                    "sent": "The idea behind our approximation is simple.",
                    "label": 1
                },
                {
                    "sent": "In many games we have situations where different agents have what we call a similar strategic view of the game.",
                    "label": 1
                },
                {
                    "sent": "Similar strategic view means they have similar pay offs and they have similar effects on other agents pay offs.",
                    "label": 0
                },
                {
                    "sent": "So for example in the market companies in the same sector may have a similar strategic view or in online games similar players with similar properties may have a similar strategic view and then what we do is we cluster together agents that have a similar strategic view to obtain.",
                    "label": 0
                },
                {
                    "sent": "A club.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The based representation.",
                    "label": 0
                },
                {
                    "sent": "An example of a game that has these characteristics is a vendor game.",
                    "label": 0
                },
                {
                    "sent": "There are a number of vendors, a large number, each of which has to choose to sell goods in a certain location in a stadium, and there are different categories of vendors, so there may be sandwiches, vendors and drinks vendors, and within categories.",
                    "label": 0
                },
                {
                    "sent": "There are also individual vendors who are further differentiated.",
                    "label": 1
                },
                {
                    "sent": "So within the drinks category there may be a lemonade vendor and a beer vendor.",
                    "label": 0
                },
                {
                    "sent": "Now there are substitute and complement relationships between different.",
                    "label": 0
                },
                {
                    "sent": "Vendors and these relationships hold in a general way across categories, but there is variation between individual vendors within categories.",
                    "label": 1
                },
                {
                    "sent": "So drinks vendors may in general be substitutes for each other, but lemonade may be a more severe substitute for orange juice than it is for beer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an overview of our approach.",
                    "label": 0
                },
                {
                    "sent": "We start with some data.",
                    "label": 0
                },
                {
                    "sent": "From that we learn a clustered representation.",
                    "label": 0
                },
                {
                    "sent": "From that clustered representation, we construct what we call the Twins game.",
                    "label": 1
                },
                {
                    "sent": "We solve the Twins game to obtain strategies and then play them in the world and get some performance.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with the data.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, the definition of the game.",
                    "label": 0
                },
                {
                    "sent": "There's a number N of agents.",
                    "label": 0
                },
                {
                    "sent": "There is a set of pure strategies.",
                    "label": 0
                },
                {
                    "sent": "We assume that every agent has the same set of pure strategies and every agent has a payoff function that Maps strategy profiles of all the agents to utility's for that agent.",
                    "label": 1
                },
                {
                    "sent": "And of course this payoff function is not explicitly represented.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our data consists of some rounds of interaction of of the game, so there's some number of agents in this case 10 agents, and we have some number of rounds.",
                    "label": 0
                },
                {
                    "sent": "In each round we have the actions of all the agents and the payoff that every.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The agent receives.",
                    "label": 0
                },
                {
                    "sent": "Where does the data come from?",
                    "label": 1
                },
                {
                    "sent": "We have two answers to this.",
                    "label": 0
                },
                {
                    "sent": "One possible answer is that this is a game that's actually being played in the world and we get to observe what the agents in the world are doing in the playoffs they receive, and from that we obtain the data.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "TA the other option is that we have access to a simulator that, given a strategy profile, can tell us what the pay offs are, and the simulator may be represented in a compact way, whereas the full game matrix is not.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the data.",
                    "label": 0
                },
                {
                    "sent": "Now I'll talk about our clustered representation and how we learn it.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take the N agents and cluster them into K clusters, where K is much smaller than N and the assumptions that our approximation makes our that agents in the same cluster receive the same pay offs when they play the same strategies.",
                    "label": 1
                },
                {
                    "sent": "Agents in the same cluster have the same influences on other agents, and the payoff that an agent in the cluster gets depends only on the proportion of agents in each cluster playing in each strategy.",
                    "label": 1
                },
                {
                    "sent": "And Furthermore, the pay offs are linear in those propose.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oceans.",
                    "label": 0
                },
                {
                    "sent": "Under those assumptions, we construct a set of regression equations.",
                    "label": 0
                },
                {
                    "sent": "We have one equation for every cluster and every pure strategy which specifies the payoff that an agent in that cluster receives.",
                    "label": 1
                },
                {
                    "sent": "For playing that pure strategy.",
                    "label": 0
                },
                {
                    "sent": "And it depends on the number of agents in each cluster, or the proportion of agents in each cluster.",
                    "label": 1
                },
                {
                    "sent": "Playing in each strategy, and each of these regression equations has a term for every strategy profile of the clusters.",
                    "label": 0
                },
                {
                    "sent": "Plus a con.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instant term.",
                    "label": 0
                },
                {
                    "sent": "So, for example, suppose there are two clusters we want to estimate the payoff for an agent in cluster A for playing left.",
                    "label": 0
                },
                {
                    "sent": "So first of all, this will have a term for the cluster strategy profile left left.",
                    "label": 0
                },
                {
                    "sent": "There's a ton of beta parameter corresponding to that cluster strategy profile for an agent in cluster A.",
                    "label": 0
                },
                {
                    "sent": "Playing left that is multiplied by the proportion of agents in cluster A who play left times the proportion of agents in cluster B who play left.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have another term for each of the different cluster strategy profiles and then a constant term at the end.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have these equations.",
                    "label": 0
                },
                {
                    "sent": "We have data, so naturally we learn them by linear regression.",
                    "label": 0
                },
                {
                    "sent": "In particular, let's assume we were given a clustering.",
                    "label": 1
                },
                {
                    "sent": "Then each agents payoff in around that we observe gives us festival of the cluster of the agent.",
                    "label": 1
                },
                {
                    "sent": "The strategy that the agent played, the proportion of agents in each cluster playing in each strategy, and the payoff that that agent received.",
                    "label": 1
                },
                {
                    "sent": "So that gives US1 estimate and a round of play gives us an estimate for.",
                    "label": 0
                },
                {
                    "sent": "Every agent, so N estimates.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Up to now, I've assumed the clustering was given, but in fact we don't assume that the clusters are known in advance.",
                    "label": 1
                },
                {
                    "sent": "We learn the clusters from data and we use a very simple approach.",
                    "label": 1
                },
                {
                    "sent": "We use K means clustering, in which the features are the average pay offs that the agents receive for different strategies.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That they can play.",
                    "label": 0
                },
                {
                    "sent": "So that's a clustered representation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do we do with that?",
                    "label": 0
                },
                {
                    "sent": "A natural thing we can do is to construct AK Player game where there's one player for every cluster.",
                    "label": 0
                },
                {
                    "sent": "We estimate the payoff of that game using the regression equations.",
                    "label": 1
                },
                {
                    "sent": "We solve that game to obtain a Nash equilibrium, and we instruct all agents in that in a cluster to play that clusters equally.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And strategy.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that idea does not work very well.",
                    "label": 0
                },
                {
                    "sent": "The reason is as follows.",
                    "label": 0
                },
                {
                    "sent": "Suppose we solve this cluster game and we find that a clustered optimal strategy is to play left.",
                    "label": 0
                },
                {
                    "sent": "So we're going to tell all the agents in the cluster to play left, but it may be the case that if all the agents are playing left, then a particular agent want to play right.",
                    "label": 1
                },
                {
                    "sent": "For example, if all the drinks vendors are told to go to one location in the stadium, then a lemonade vendor might say I want to go to a different location, so.",
                    "label": 1
                },
                {
                    "sent": "In this setting, individual agents have an incentive to deviate from their prescribed cluster strategies.",
                    "label": 0
                },
                {
                    "sent": "We say that the representation is not individually responsive, meaning it doesn't respond to the interests of India.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actual agents.",
                    "label": 0
                },
                {
                    "sent": "So what we do instead is we construct a 2K player game where every player where every cluster is represented by two identical twin players.",
                    "label": 1
                },
                {
                    "sent": "So a cluster C is represented by Twins C&C Prime and these Twins are really identical.",
                    "label": 0
                },
                {
                    "sent": "They have symmetric payoff functions, and they have the same.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Affect on others pay offs.",
                    "label": 0
                },
                {
                    "sent": "There are various ways to interpret these Twins from the point of view of 1 twin, let's say CC itself represents a single agent in the cluster C, Whereas its twin C prime represents cluster see in aggregate.",
                    "label": 1
                },
                {
                    "sent": "From the point of view of C prime the situate the view is symmetric.",
                    "label": 0
                },
                {
                    "sent": "See primed itself represents a single agent in cluster C, whereas it's Twins see represents the clustering aggregate and from the view of a player corresponding to a different cluster CNC prime between them Rep.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Present cluster C. So let's talk about how we specify the pay offs in the Twins game.",
                    "label": 0
                },
                {
                    "sent": "Let's say there are two clusters and be so four players a a prime B&B prime, and they're playing this strategy.",
                    "label": 0
                },
                {
                    "sent": "Profile a plays left a prime, plays right, B plays left and be prime plays.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Left so the payoff to play A is as follows from the point of view of player AA represents an individual agent in cluster A, so this is the payoff to an agent in cluster A for playing left, which is what player a plays.",
                    "label": 0
                },
                {
                    "sent": "A prime represents the cluster as a whole, so this is the payoff when the cluster A as a whole plays right, which is what a prime is playing, and cluster B here is playing left.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The payoff to player a prime.",
                    "label": 0
                },
                {
                    "sent": "This symmetric is the payoff to an agent in cluster A for playing right, which is what a prime is playing when cluster A as a whole is playing left, which is what a is playing, and cluster B is playing.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Left now for the payoff to play a be.",
                    "label": 0
                },
                {
                    "sent": "Here A is playing left and a prime is playing right from the point of view of player.",
                    "label": 0
                },
                {
                    "sent": "Be with you A and a prying as between them representing cluster A.",
                    "label": 0
                },
                {
                    "sent": "So we interpret this as saying that half of cluster A is playing left and half of cluster A is playing right.",
                    "label": 0
                },
                {
                    "sent": "So the payoff to Player B is the payoff to an agent in cluster before playing left when cluster B is a whole plays left and half of cluster A plays left and the other half plays right.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this case, the payoff to player B prime is exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've constructed.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This Twins game.",
                    "label": 0
                },
                {
                    "sent": "How do we solve it?",
                    "label": 0
                },
                {
                    "sent": "So we want a three step process.",
                    "label": 0
                },
                {
                    "sent": "We solve the Twins game to obtain Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "We assign the strategies of the Twins to the corresponding clusters and then we tell the agents in a cluster to play their cluster strategies.",
                    "label": 0
                },
                {
                    "sent": "Now when we assign the strategies of Twins to corresponding clusters, what we want is Nash equilibrium of the Twins game in which the twin players are playing identical mixed strategies and we call such equilibrium twin symmetric Nash equilibrium there Gary.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Party to exist.",
                    "label": 0
                },
                {
                    "sent": "Earlier I argued that the K player cluster gained the naive K player cluster gain is not individually responsive.",
                    "label": 0
                },
                {
                    "sent": "Now I will argue to the Twins game is individually responsive.",
                    "label": 0
                },
                {
                    "sent": "This is because, in a Nash equilibrium of the Twins game, player C is playing the best response to the other players, including its twin C prime by construction of the Twins game, the payoff that see gets is the payoff to an individual agent in cluster.",
                    "label": 1
                },
                {
                    "sent": "See when the cluster is a whole players.",
                    "label": 0
                },
                {
                    "sent": "C prime does.",
                    "label": 1
                },
                {
                    "sent": "That means that an agent in Cluster C strategy is a best response to the strategy of player C prime.",
                    "label": 0
                },
                {
                    "sent": "But in a twin symmetric Nash equilibrium, the strategy that cluster see is playing is the same as that of players.",
                    "label": 0
                },
                {
                    "sent": "See prime.",
                    "label": 1
                },
                {
                    "sent": "That means that an individual agent in cluster C is playing a best response to the strategy of cluster C and therefore no wins.",
                    "label": 0
                },
                {
                    "sent": "Agent has incentive to deviate from its cluster strategy, and the representation is indivi.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Truly responsive.",
                    "label": 0
                },
                {
                    "sent": "So now we have stre.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Geez, what is the performance?",
                    "label": 0
                },
                {
                    "sent": "So we solve the game.",
                    "label": 0
                },
                {
                    "sent": "We assign strategies to agents.",
                    "label": 1
                },
                {
                    "sent": "We have them play a number of rounds, and then we compute the average regret of the agents, the regret of an agent is how much better the agent could have done if it had chosen optimally up your strategy and stuck to it.",
                    "label": 1
                },
                {
                    "sent": "Given what the other agents did.",
                    "label": 0
                },
                {
                    "sent": "And for Nash equilibrium, the expected regret is 0.",
                    "label": 0
                },
                {
                    "sent": "So we want to get a regret as close to zero as possible.",
                    "label": 0
                },
                {
                    "sent": "If we have a small regret, that means we're getting behavior that's similar to matching.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Librium we compared the number of methods.",
                    "label": 0
                },
                {
                    "sent": "We compared our 2K player Twins game.",
                    "label": 1
                },
                {
                    "sent": "The K player game that I described to you.",
                    "label": 0
                },
                {
                    "sent": "We also looked at a model free approach in which an agent simply chooses a strategy that would have maximized this rewards in the past.",
                    "label": 1
                },
                {
                    "sent": "And we also compared our methods to a method of Wellmann ET al, which is also a clustering method, but a little bit different from ours.",
                    "label": 0
                },
                {
                    "sent": "It only applies to symmetric gains and also their approach does not learn a representation from day.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After and the results for the vendor game.",
                    "label": 1
                },
                {
                    "sent": "What we see is that the regret of the Twins gain is much better than that of the K player game in the model free game.",
                    "label": 0
                },
                {
                    "sent": "The model free approach.",
                    "label": 0
                },
                {
                    "sent": "It's it's about an order of magnitude better.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we found similar results to this graph under a variety of different conditions, and one of the things we observed is that our learning actually works quite well even with a small number of observations.",
                    "label": 1
                },
                {
                    "sent": "Even with us few is 3 rounds of the game.",
                    "label": 0
                },
                {
                    "sent": "We can get a good regression fit and low regret.",
                    "label": 0
                },
                {
                    "sent": "And also we found that our method scales up well to many agents.",
                    "label": 1
                },
                {
                    "sent": "We scaled it up to 200 agents, but there were no inherent limitations there.",
                    "label": 0
                },
                {
                    "sent": "I think we could schedule a top beyond that without much problem.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other domain we looked at in our experiments is the Santa Fe bar problem.",
                    "label": 0
                },
                {
                    "sent": "In this domain there is a number N of agents who have to decide whether or not to go to a bar.",
                    "label": 1
                },
                {
                    "sent": "The bar has a certain capacity and if the bar is not full, agents prefer to go to the bar.",
                    "label": 1
                },
                {
                    "sent": "If the bar is full, agents prefer to stay home.",
                    "label": 1
                },
                {
                    "sent": "Now this is a symmetric game, so we can apply well metals method from our point of view it means that all the agents have exactly the stress.",
                    "label": 0
                },
                {
                    "sent": "Same strategic view of the game, so there's only one cluster.",
                    "label": 0
                },
                {
                    "sent": "And also another property of this game is that the payoff is nonlinear in the proportion of agents who go to the bar.",
                    "label": 0
                },
                {
                    "sent": "So it's an interesting test to see whether our method in which the approximation makes the assumption of linearity can work well in this setting.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So since that's one cluster for our approach, we constructed a 2A2 player game.",
                    "label": 0
                },
                {
                    "sent": "Our method produces only symmetric Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "We compared to Wellman ET al with both two and five clusters.",
                    "label": 0
                },
                {
                    "sent": "Two clusters is a direct comparison because that also requires solving the two player game.",
                    "label": 1
                },
                {
                    "sent": "Five clusters requires more computational resources, but maybe is able to do better, and their approach produces both symmetric and asymmetric Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So we compared to both.",
                    "label": 0
                },
                {
                    "sent": "And we varied the bark.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Passati in our experiments.",
                    "label": 0
                },
                {
                    "sent": "And what we found is the following when the bar capacity is an exact multiple of the cluster size then.",
                    "label": 1
                },
                {
                    "sent": "Well, well Minitel's approach is able to find exactly an asymmetric Nash equilibrium, but in other cases we found that our method gets lower regrets than their approach with two clusters.",
                    "label": 0
                },
                {
                    "sent": "It gets slightly higher regrets than their approach with five clusters, but is using using fewer computational resources.",
                    "label": 0
                },
                {
                    "sent": "And it's also our approach is not sensitive to the bar capacity, so our approach gets good regrets across different.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bar capacities so to conclude we're interested in gangs with many players which are hard to tackle using standard game theoretic approaches.",
                    "label": 0
                },
                {
                    "sent": "We presented a method that learns an approximate representation of a many players gain from data of play of the game.",
                    "label": 1
                },
                {
                    "sent": "Our method works by clustering the agents into K clusters and then estimating the payoff spy linear regression.",
                    "label": 1
                },
                {
                    "sent": "We then construct and solve a 2K player game called the Twins Game that ensures individual responsiveness that individual agents aren't have incentive to stick to their cluster strategies.",
                    "label": 0
                },
                {
                    "sent": "Our results showed that our methods work well with few observations and scaling up to many agents an for future work.",
                    "label": 0
                },
                {
                    "sent": "We are limited so far with our algorithms for computing twin symmetric Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "They're not very sophisticated, so we want better algorithms for doing that, and we're also looking at more sophisticated clustering algorithms to discover the different clusters.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "This question there.",
                    "label": 0
                },
                {
                    "sent": "The Twins game seems like a nice way of preserving the strategic properties that can be distraction for the approximation.",
                    "label": 0
                },
                {
                    "sent": "You see using that independent of the learning, so that you should say stuff price and broad parameters in ancient Greece and that benifet Twins game.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "We haven't we should have, but we haven't tried gains where there is actually an exact clustering and.",
                    "label": 0
                },
                {
                    "sent": "The games that satisfy all the assumptions of the approximation where we would not have to learn, we would just produce the clustering and seeing whether our Twins game actually captures the strategic nature of the original game and whether we get good strategies.",
                    "label": 0
                },
                {
                    "sent": "So that's a that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I mean one.",
                    "label": 0
                },
                {
                    "sent": "One difference is that the Twins game does if, let's say we have one cluster, the Twins game will find symmetric Nash equilibrium for the game, whereas you may be interested in a symmetric equilibrium.",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "Can you prove any kind of asymptotically theorems about?",
                    "label": 0
                },
                {
                    "sent": "When your approximation gives you the right answer.",
                    "label": 0
                },
                {
                    "sent": "Many agents or anything like that.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have thought about proving properties we haven't looked at the limit of.",
                    "label": 0
                },
                {
                    "sent": "Agents and we don't have any good theoretical results at the moment.",
                    "label": 0
                },
                {
                    "sent": "This?",
                    "label": 0
                },
                {
                    "sent": "One more.",
                    "label": 0
                },
                {
                    "sent": "So it seems like the game will have in general have different values for players in different clusters, and it seems like in many real real world problems the number of players in the cluster responds to the value of the game for that cluster.",
                    "label": 0
                },
                {
                    "sent": "And is there any way to deal with that in future work with this too intractable?",
                    "label": 0
                },
                {
                    "sent": "So you mean that people or agents will choose clusters that have good values?",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're coming to the stadium and deciding what to sell, if there are a lot of drinks vendors and you decide not to be a drinks vendor, that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "We could do that with more sophisticated clustering methods by putting priors over the cluster sizes.",
                    "label": 0
                },
                {
                    "sent": "That's how I would envision doing it.",
                    "label": 0
                },
                {
                    "sent": "Seems like a natural thing to do.",
                    "label": 0
                },
                {
                    "sent": "I mean for this particular application, it seems like you would want the opposite of a Chinese restaurant process, where people try to spread out.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}