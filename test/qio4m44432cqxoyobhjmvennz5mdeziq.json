{
    "id": "qio4m44432cqxoyobhjmvennz5mdeziq",
    "title": "A Least Squares Formulation for Canonical Correlation Analysis",
    "info": {
        "author": [
            "Shuiwang Ji, School of Electrical Engineering and Computer Science, Washington State University"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_ji_alsf/",
    "segmentation": [
        [
            "So CCA is a well known technique, flow statistics.",
            "We try to find a correlation among two view of the same data.",
            "So for example, if you do a one meaning you have web page and within web page you may have text, so we can view text as one view of the object.",
            "Then we can view maybe image.",
            "A social text as another view of the object.",
            "So in this talk it will be 1 view as X.",
            "And otherwise why so both of them have the same number of Collins Lake correspond to the same set of object, and here, but the features are different.",
            "So for X we have the features in awhile with key features.",
            "So one popular use of CCS for supervised learning.",
            "We can view the input data acts as one view, and another view is a class label.",
            "So in this case it's a try to find maximum correlation among input data X and labor, why?",
            "So it's known since it involved.",
            "Eigenvalues public actually generalized eigenvalues problem.",
            "The science of the matrix depends on the number of objects, and it's no.",
            "If we deal with a large set of objects, is adding value.",
            "Problem can be very expensive to solve.",
            "So let's one imitation.",
            "When we try to scale CC to logistical problems.",
            "So in addition, there are a lot of study.",
            "Try to try to derive sparse model.",
            "Anesthesia is difficult, challenging to derive spots.",
            "Easy model due to aging value problem in CCA."
        ],
        [
            "Sorry, I've been some previous work.",
            "We try to address this issue by connecting CCA to linear regression.",
            "For example, if one of you why it's University is only one dimensional.",
            "Dimensions one.",
            "Then in this case, we can show CCA is equivalent to University linear regression.",
            "So there are several advantage of using linear regression.",
            "First, it can be solved efficiently using CG algorithm.",
            "It's a least square problem, so another advantage is linear regression.",
            "We can derive sparse models by using, for example, one organization following idea from Alaska.",
            "So this is true for the simple univariate case.",
            "So the question we try to address what happens for the more general case, one dimension of DY is larger than one.",
            "And in this case, when the dimension is larger than one, we typically apply multivariate linear regression.",
            "So in this work we try to address the connection between CCA and multivariate linear regression.",
            "So that's the model."
        ],
        [
            "And behind this work.",
            "So here's the main contribution of this work.",
            "We establish the equivalence connection between CCA and multivariate linear regression in a general case.",
            "Under mild condition.",
            "Land based on Lisa equivalent connection.",
            "We can derive several CC extension based on the square formulation.",
            "For example, sparse easier if we apply lasso penalty.",
            "We also derive Empire solution parts for sparse easier by using this last algorithm.",
            "Learn finally we showed our results which confirmed this equivalence."
        ],
        [
            "Connection.",
            "So here is loud enough for the rest of this talk.",
            "First, I will give a brief background or CC and and linear regression.",
            "Now will show a connection between these two models.",
            "Learn based on this connection will derive several extensions.",
            "Then finally we have excellent and."
        ],
        [
            "Discussion.",
            "So instead we deal with the two different representation of the same set of objects XY.",
            "Dimension can be different, and here the column of X or the corner of Y is denotes the number of objects.",
            "So that's an objects.",
            "These are two different view for the same set of objects.",
            "And in CSE we try to do projection on both XY the same number of projection vectors an in CC we try to compute projection so that their correlation is maximized.",
            "So here double X is the projection vector on X&Y, is the projection of Y, and we try to maximize the correlation coefficient in CCA."
        ],
        [
            "So equivalently, Lisa maximization can be solved by solving the following.",
            "Maximizing problem essentially makes my we fix the size of both XY of the projection and we try to maximize their inner product.",
            "Anne, it's known if white Times Y transpose this part the covariance for why is nonsingular learn projection in X can be computed by solving the following value problem.",
            "We call this generalized anger problem because we have matrix in the left side and we have matrix on the right side.",
            "And the optimal vector is given by the topic vector.",
            "So that's the case if we only deal with a 1 dimensional projection, we can also derive multi dimensional projection by essentially computing the top L. So if you want to derive L dimensional projection, you just compute Pop L Egan vector of this Journal."
        ],
        [
            "Say goodbye to problem.",
            "And in practice, we often apply regularization to this CC model we call RCA.",
            "Essentially, we add rich tend to the covariance matrix in X.",
            "And again we solve a problem.",
            "The difference here is now we have regularised covalency eggs."
        ],
        [
            "So in multivariate linear regression will give a training set XITI will edit Point XI, import TSV output and here T is dimension.",
            "In case one is University and here we deal with the case K is larger than one.",
            "So it's multivariate linear regression and here we assume both XY centered 0 mean.",
            "So in NLR we compute await Max W. Which minimize the following solved squared error functions.",
            "So double transpose excise prediction based on linear regression model and T is the true target and we try to minimize the approximation error.",
            "We can write this as a matrix form, double transpose X -- T so T shirt target which combine all the output and access the which combine all the input and the solution is given by this form.",
            "Covariance axle inverse X20 transports so T is low target output so again in a linear regression with also apply regulations.",
            "For example, in regression we apply to non regularization.",
            "If you do one or less you get lost so."
        ],
        [
            "Type of algorithm.",
            "So if we if we deal with the mountain label or multiclass classification, my taxes are import or feature vector and then we need to define the target matrix T. There are many ways to derive the target matrix T. You can see the linear regression if X fixed line really depends on how you define T, right?",
            "So there are several ways to define T1.",
            "Simple way is just Lisa 01 matrix.",
            "So he's talking to matrix Eva.",
            "Datapoint XJ contain the ice class label that is 1 otherwise 0.",
            "So we call this a class indicator matrix.",
            "So forgiven training data set in multiclass or multi level case.",
            "We have X and we can derive T. Then we can apply linear regression to do multi label or multiclass classification.",
            "So from our previous slide, the solution to this square really depends on the choice of class in econometrics."
        ],
        [
            "So in summary.",
            "The opium projection matrix WCCC consists of a topic vector of this matrix, here, soon inverse.",
            "So let's see model.",
            "While in multivariate linear regression the optimal weight vector W is given by this form.",
            "Tis the target matrix.",
            "We need to design for specific problems.",
            "So in the following a of this in the next few slides we show the equivalence connection between these two solutions for a particular choice of party."
        ],
        [
            "So before I show this connection, I will give some notation and definitions.",
            "Four, why is the 2nd view object will define edge matrix?",
            "Here we assume the covariance wise is none singular.",
            "So this is commonly true for multi label classification.",
            "And we denote the covariance access.",
            "CXX and this matrix RCH&CD and all the key fact we use actual here is this edge matrix is also normal over the colors.",
            "They also each other and lenses one and this is essential for the following proof.",
            "For example if H * H transpose it if edge of normal learn CD is also positive semidefinite.",
            "So X is our data matrix.",
            "We can compute SVD, so I will skip all the details here.",
            "U Sigma V transpose is full is busy and we can get compact SVD by removing all the zero eigenvalues.",
            "And then based on our previous discussion, optimal projection, since it consists of the top vector of this matrix SXL inverse.",
            "Let's see a charge so that slower."
        ],
        [
            "CCS solution.",
            "And we can define a matrix compute SVD and I will skip all the details here.",
            "You can revert to the paper by me and we get a compact agent decomposition for the matrix we derive for CCA and the top vector.",
            "Essentially spot U times Sigma inverse times B."
        ],
        [
            "So I will skip all this detail here and then in the end we get a solution for CCA.",
            "You wanna slice video X Sigma izla nonzero envelope art and PLP's Lola's video of a matrix.",
            "Here we assume with table top oil.",
            "Singular vectors for CCA.",
            "So now we go slow.",
            "Linear regression.",
            "We need to design AT the target matrix T and we do a simple choice T equal to H inverse.",
            "So at least matrix edge."
        ],
        [
            "Is defined here.",
            "Totally depends on, uh, why the second view, right?",
            "So we define."
        ],
        [
            "TSH in both then we'll get this solution for least square and plugging over the over SVD formula will get this form.",
            "So for detail, again virtual paper.",
            "So now we we get WCC NND errors.",
            "We tried to find the connection between these two solutions.",
            "We can see the first part you are.",
            "You want the same Sigma inverses thing and Pierre we can is also similar an we have Sigma and Q transpose.",
            "So here kill is orthogonal.",
            "So for a given projection, if we do any ugly transformation, does not change the pairwise distance, so Q is is OK.",
            "So the trick part is Sigma a matrix which comes from this video A.",
            "And if we can somehow show Sigma is identity learning, these two will be equivalent lonely.",
            "The only difference is the orthogonal transmission an commonly.",
            "After we do PCA, projection will apply as being honest labor to do classification.",
            "So if you apply random projection Q, this will not change the the second classification result because Q is ordinal.",
            "So we only need to take care of this Sigma matrix.",
            "So it turns out under mild condition.",
            "So if ranks equal to minus One X is the data after we remove the mean centered.",
            "If N -- 1 then we can actually show all the diagonal elements of Sigma.",
            "A Lisa diagonal matrix is 1.",
            "Essentially Sigma is identity matrix.",
            "At least with this condition, essentially claim to say all the data points are linearly independent.",
            "Right, so this commonly hold if the dimension of the data is larger than the sample size.",
            "But we are showing our results even if this condition is valid.",
            "There song list dependence among data least two formulation still very close, so we'll see in the in the."
        ],
        [
            "Last part of his talk.",
            "So we should, uh, equivalence connection between CC and linear regression.",
            "Now we try to develop several season extension based on this square formulation.",
            "And we know in a in a linear regression we commonly apply to non regulation.",
            "That's rich regression.",
            "Essentially we had a two norm penalty for overweight vector language alive Lisa, two regulars least glasses here.",
            "So one key advantage of this formulation, compared with our original CC formulations.",
            "Now we can use efficient solver for the square to solve CCA.",
            "For example, conjugal gradient.",
            "OK, this is much, much more efficient, especially when the data is sparse, such as text."
        ],
        [
            "A document.",
            "And in addition, we can derive the sparse easy model by adding a L1 norm penalty over with vector.",
            "So we add one on penalty to listen regression and we can derive the sparse CC model."
        ],
        [
            "So initially we can compute law so unlucky.",
            "So here's how to choose.",
            "Wonder if we can do an efficient cloth validation by computing the entire solution path of CCA using this angle regression algorithm loss."
        ],
        [
            "So for a family follow evaluation.",
            "We use two types of data.",
            "Both did our multi label data set.",
            "The first one is a biological image and the second one is the same data set commonly used benchmark data set for multi label classification and we compare this five method CCS original CC formulation without regularization Lambda zero and we have a regular CCA by adding a rich tend to the excellence.",
            "Then we have the square formation of CCA based on risk equivalent result, and we have at least 291 extension based on this list wax formulation.",
            "So after CC projection will do a classification.",
            "In this case we apply linear SVM for all the class labels and we use our C for the value."
        ],
        [
            "Measure.",
            "So first way.",
            "Uh.",
            "2000 something on the data set and we get a bunch of data set where the dimension of the data set is larger than the sample size, and in this case we are observing in all the cases.",
            "The rank condition.",
            "This condition is satisfied.",
            "As I mentioned, even dimension is large.",
            "Learn is commonly is likely over.",
            "The samples will be linearly independent then our submission will hold and we check.",
            "In this case, when somebody holds all the diagonal elements equal 1.",
            "And they achieve the same performance.",
            "So this shows the least concerns are."
        ],
        [
            "Theoretical results.",
            "So any stable wave compare all these five approach on a biological data set.",
            "We with different number of labels.",
            "So here's the sample size.",
            "And case number of labels and with all this approach, these are all the measures.",
            "So there are several observations, first one.",
            "Is the reckless version.",
            "Compared to original version typically outperform their original formulation.",
            "For example, RCA will achieve much higher hours value compared to original formulation 5260.",
            "Similar for least square formulation one onto Norm perform much better.",
            "So by the way, all the parameter in a list of regression, we do this cross validation to choose optimal value for realisation.",
            "And the interesting for this case, the sparsity model achieve the best performance in this experiment.",
            "But this is not the case in the other data set.",
            "They are very similar in another data set."
        ],
        [
            "So we see from this this figure.",
            "So in this case we want to see.",
            "Oh well, equivalence connection really based on assumption like were all the data points linearly independent and lease may not hold for general problems.",
            "OK so in this case we start the sensitivity for both the surging and same data set the dimension fixed so follow agenda set, dimension IS384 and 4C is 294.",
            "We fixed dimension and we value the number of data points from small 100 two 1000.",
            "And we try to observe the difference among all five algorithms.",
            "So we can see if for the first three columns.",
            "The sample size smaller dimension.",
            "We can see the first 2 columns.",
            "Sincerely, squeezy leads to achieve learning performance.",
            "This released while pregnant, so this somehow verify our theoretical results.",
            "The same thing for simpler said, when someone says small learn Lily store identical, exactly technical.",
            "So next we increase the size.",
            "400 until 900.",
            "The sample size is much larger than dimension, and there's a dependence in a data set.",
            "But still we can see this tool.",
            "The first 2 columns CC an extension are still fairly close, not valid.",
            "For all data set.",
            "So let's not follow case without organization.",
            "Then let's take a look at the case with regularization.",
            "The last three columns is one or two normalization using the square and the last one is regular.",
            "CCA is still a good value problem, so we can see.",
            "The last two column they correspond to the CC model with two reorganization.",
            "And we expect they may achieve similar performance.",
            "So if we take a careful look, last two columns.",
            "So in most cases they're not far away, not far away.",
            "And the one difference case follow Jim data set.",
            "The middle one is sparse model, perform much better.",
            "Learn the other model for the biological image data set.",
            "But for some data set they are compatible.",
            "Sparse model in some case achieve the best performance, but in this case they are compatible."
        ],
        [
            "So one of the advantage of this formulation for CC.",
            "We also derive entire CC solution path.",
            "OK for all the possible values of regularization parameter.",
            "So here we normalize the coefficient of organization from zero to 1 and we draw we hear withdraw the coefficient for the first week of FW.",
            "One is vector withdrawal occur for all entries.",
            "We can see Wannacry fish, regular impairment is 1.",
            "Then all entries non zero and we decrease the value.",
            "Decrease liquid resin parameter.",
            "Learn is more and more sparse.",
            "So after if very lesson 3.2 only one or two entries non 0 so we can vary the sparseness of the model by changing the parameter.",
            "Of this organization, and in practice we need to."
        ],
        [
            "Play close validation.",
            "So in conclusion is walkway stab Lish?",
            "Are equivalent connection between CC and linear regression an under mild condition which require all the samples are linearly independent?",
            "So based on this equivalence connection we can derive several CC model based on the regression model and our results show.",
            "Well, assumption is hold leads to equipment and our results also show on these two assumptions valid.",
            "These two models still very close, so this equivalence connection provides new insight on the CC model and also opens the way for further extension.",
            "For example, we can potentially do semisupervised see non ECC based on the list regression model.",
            "Thank you.",
            "Come up well.",
            "At one point.",
            "Yeah, so that's I think could be a possible extension, right?",
            "Some trace norm will be another extension.",
            "So essentially whatever we can do for linear regression, we can apply to this model based on the equivalence connection.",
            "One on piano and two unknown.",
            "Treatment.",
            "Square.",
            "To other settings.",
            "OK.",
            "So as I mentioned earlier, one of the key actually a result we use for this equivalence is the edge matrix.",
            "The target match T orthonormal.",
            "OK, so that's the key assumption as your key.",
            "Property we use here in this case, so all the proof will not hold on a T matrix is not as normal.",
            "And power recently we did some further study.",
            "It turns out this assumption can be actually relaxed.",
            "So this opens the way to actually to find the connection among.",
            "Many, I think even value based algorithm, machine learning and the square."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So CCA is a well known technique, flow statistics.",
                    "label": 1
                },
                {
                    "sent": "We try to find a correlation among two view of the same data.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you do a one meaning you have web page and within web page you may have text, so we can view text as one view of the object.",
                    "label": 0
                },
                {
                    "sent": "Then we can view maybe image.",
                    "label": 0
                },
                {
                    "sent": "A social text as another view of the object.",
                    "label": 0
                },
                {
                    "sent": "So in this talk it will be 1 view as X.",
                    "label": 0
                },
                {
                    "sent": "And otherwise why so both of them have the same number of Collins Lake correspond to the same set of object, and here, but the features are different.",
                    "label": 0
                },
                {
                    "sent": "So for X we have the features in awhile with key features.",
                    "label": 0
                },
                {
                    "sent": "So one popular use of CCS for supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We can view the input data acts as one view, and another view is a class label.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's a try to find maximum correlation among input data X and labor, why?",
                    "label": 0
                },
                {
                    "sent": "So it's known since it involved.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues public actually generalized eigenvalues problem.",
                    "label": 0
                },
                {
                    "sent": "The science of the matrix depends on the number of objects, and it's no.",
                    "label": 1
                },
                {
                    "sent": "If we deal with a large set of objects, is adding value.",
                    "label": 0
                },
                {
                    "sent": "Problem can be very expensive to solve.",
                    "label": 0
                },
                {
                    "sent": "So let's one imitation.",
                    "label": 1
                },
                {
                    "sent": "When we try to scale CC to logistical problems.",
                    "label": 0
                },
                {
                    "sent": "So in addition, there are a lot of study.",
                    "label": 0
                },
                {
                    "sent": "Try to try to derive sparse model.",
                    "label": 0
                },
                {
                    "sent": "Anesthesia is difficult, challenging to derive spots.",
                    "label": 0
                },
                {
                    "sent": "Easy model due to aging value problem in CCA.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, I've been some previous work.",
                    "label": 0
                },
                {
                    "sent": "We try to address this issue by connecting CCA to linear regression.",
                    "label": 1
                },
                {
                    "sent": "For example, if one of you why it's University is only one dimensional.",
                    "label": 0
                },
                {
                    "sent": "Dimensions one.",
                    "label": 0
                },
                {
                    "sent": "Then in this case, we can show CCA is equivalent to University linear regression.",
                    "label": 1
                },
                {
                    "sent": "So there are several advantage of using linear regression.",
                    "label": 1
                },
                {
                    "sent": "First, it can be solved efficiently using CG algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's a least square problem, so another advantage is linear regression.",
                    "label": 0
                },
                {
                    "sent": "We can derive sparse models by using, for example, one organization following idea from Alaska.",
                    "label": 0
                },
                {
                    "sent": "So this is true for the simple univariate case.",
                    "label": 0
                },
                {
                    "sent": "So the question we try to address what happens for the more general case, one dimension of DY is larger than one.",
                    "label": 0
                },
                {
                    "sent": "And in this case, when the dimension is larger than one, we typically apply multivariate linear regression.",
                    "label": 0
                },
                {
                    "sent": "So in this work we try to address the connection between CCA and multivariate linear regression.",
                    "label": 0
                },
                {
                    "sent": "So that's the model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And behind this work.",
                    "label": 0
                },
                {
                    "sent": "So here's the main contribution of this work.",
                    "label": 0
                },
                {
                    "sent": "We establish the equivalence connection between CCA and multivariate linear regression in a general case.",
                    "label": 1
                },
                {
                    "sent": "Under mild condition.",
                    "label": 0
                },
                {
                    "sent": "Land based on Lisa equivalent connection.",
                    "label": 0
                },
                {
                    "sent": "We can derive several CC extension based on the square formulation.",
                    "label": 0
                },
                {
                    "sent": "For example, sparse easier if we apply lasso penalty.",
                    "label": 0
                },
                {
                    "sent": "We also derive Empire solution parts for sparse easier by using this last algorithm.",
                    "label": 0
                },
                {
                    "sent": "Learn finally we showed our results which confirmed this equivalence.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Connection.",
                    "label": 0
                },
                {
                    "sent": "So here is loud enough for the rest of this talk.",
                    "label": 0
                },
                {
                    "sent": "First, I will give a brief background or CC and and linear regression.",
                    "label": 0
                },
                {
                    "sent": "Now will show a connection between these two models.",
                    "label": 0
                },
                {
                    "sent": "Learn based on this connection will derive several extensions.",
                    "label": 0
                },
                {
                    "sent": "Then finally we have excellent and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discussion.",
                    "label": 0
                },
                {
                    "sent": "So instead we deal with the two different representation of the same set of objects XY.",
                    "label": 1
                },
                {
                    "sent": "Dimension can be different, and here the column of X or the corner of Y is denotes the number of objects.",
                    "label": 0
                },
                {
                    "sent": "So that's an objects.",
                    "label": 0
                },
                {
                    "sent": "These are two different view for the same set of objects.",
                    "label": 0
                },
                {
                    "sent": "And in CSE we try to do projection on both XY the same number of projection vectors an in CC we try to compute projection so that their correlation is maximized.",
                    "label": 1
                },
                {
                    "sent": "So here double X is the projection vector on X&Y, is the projection of Y, and we try to maximize the correlation coefficient in CCA.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So equivalently, Lisa maximization can be solved by solving the following.",
                    "label": 1
                },
                {
                    "sent": "Maximizing problem essentially makes my we fix the size of both XY of the projection and we try to maximize their inner product.",
                    "label": 0
                },
                {
                    "sent": "Anne, it's known if white Times Y transpose this part the covariance for why is nonsingular learn projection in X can be computed by solving the following value problem.",
                    "label": 1
                },
                {
                    "sent": "We call this generalized anger problem because we have matrix in the left side and we have matrix on the right side.",
                    "label": 0
                },
                {
                    "sent": "And the optimal vector is given by the topic vector.",
                    "label": 0
                },
                {
                    "sent": "So that's the case if we only deal with a 1 dimensional projection, we can also derive multi dimensional projection by essentially computing the top L. So if you want to derive L dimensional projection, you just compute Pop L Egan vector of this Journal.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say goodbye to problem.",
                    "label": 0
                },
                {
                    "sent": "And in practice, we often apply regularization to this CC model we call RCA.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we add rich tend to the covariance matrix in X.",
                    "label": 0
                },
                {
                    "sent": "And again we solve a problem.",
                    "label": 0
                },
                {
                    "sent": "The difference here is now we have regularised covalency eggs.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in multivariate linear regression will give a training set XITI will edit Point XI, import TSV output and here T is dimension.",
                    "label": 1
                },
                {
                    "sent": "In case one is University and here we deal with the case K is larger than one.",
                    "label": 1
                },
                {
                    "sent": "So it's multivariate linear regression and here we assume both XY centered 0 mean.",
                    "label": 1
                },
                {
                    "sent": "So in NLR we compute await Max W. Which minimize the following solved squared error functions.",
                    "label": 0
                },
                {
                    "sent": "So double transpose excise prediction based on linear regression model and T is the true target and we try to minimize the approximation error.",
                    "label": 1
                },
                {
                    "sent": "We can write this as a matrix form, double transpose X -- T so T shirt target which combine all the output and access the which combine all the input and the solution is given by this form.",
                    "label": 0
                },
                {
                    "sent": "Covariance axle inverse X20 transports so T is low target output so again in a linear regression with also apply regulations.",
                    "label": 0
                },
                {
                    "sent": "For example, in regression we apply to non regularization.",
                    "label": 0
                },
                {
                    "sent": "If you do one or less you get lost so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Type of algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if we if we deal with the mountain label or multiclass classification, my taxes are import or feature vector and then we need to define the target matrix T. There are many ways to derive the target matrix T. You can see the linear regression if X fixed line really depends on how you define T, right?",
                    "label": 0
                },
                {
                    "sent": "So there are several ways to define T1.",
                    "label": 1
                },
                {
                    "sent": "Simple way is just Lisa 01 matrix.",
                    "label": 0
                },
                {
                    "sent": "So he's talking to matrix Eva.",
                    "label": 1
                },
                {
                    "sent": "Datapoint XJ contain the ice class label that is 1 otherwise 0.",
                    "label": 1
                },
                {
                    "sent": "So we call this a class indicator matrix.",
                    "label": 1
                },
                {
                    "sent": "So forgiven training data set in multiclass or multi level case.",
                    "label": 0
                },
                {
                    "sent": "We have X and we can derive T. Then we can apply linear regression to do multi label or multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "So from our previous slide, the solution to this square really depends on the choice of class in econometrics.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "The opium projection matrix WCCC consists of a topic vector of this matrix, here, soon inverse.",
                    "label": 1
                },
                {
                    "sent": "So let's see model.",
                    "label": 0
                },
                {
                    "sent": "While in multivariate linear regression the optimal weight vector W is given by this form.",
                    "label": 1
                },
                {
                    "sent": "Tis the target matrix.",
                    "label": 1
                },
                {
                    "sent": "We need to design for specific problems.",
                    "label": 0
                },
                {
                    "sent": "So in the following a of this in the next few slides we show the equivalence connection between these two solutions for a particular choice of party.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I show this connection, I will give some notation and definitions.",
                    "label": 1
                },
                {
                    "sent": "Four, why is the 2nd view object will define edge matrix?",
                    "label": 0
                },
                {
                    "sent": "Here we assume the covariance wise is none singular.",
                    "label": 0
                },
                {
                    "sent": "So this is commonly true for multi label classification.",
                    "label": 0
                },
                {
                    "sent": "And we denote the covariance access.",
                    "label": 0
                },
                {
                    "sent": "CXX and this matrix RCH&CD and all the key fact we use actual here is this edge matrix is also normal over the colors.",
                    "label": 1
                },
                {
                    "sent": "They also each other and lenses one and this is essential for the following proof.",
                    "label": 0
                },
                {
                    "sent": "For example if H * H transpose it if edge of normal learn CD is also positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "So X is our data matrix.",
                    "label": 0
                },
                {
                    "sent": "We can compute SVD, so I will skip all the details here.",
                    "label": 0
                },
                {
                    "sent": "U Sigma V transpose is full is busy and we can get compact SVD by removing all the zero eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And then based on our previous discussion, optimal projection, since it consists of the top vector of this matrix SXL inverse.",
                    "label": 1
                },
                {
                    "sent": "Let's see a charge so that slower.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "CCS solution.",
                    "label": 0
                },
                {
                    "sent": "And we can define a matrix compute SVD and I will skip all the details here.",
                    "label": 0
                },
                {
                    "sent": "You can revert to the paper by me and we get a compact agent decomposition for the matrix we derive for CCA and the top vector.",
                    "label": 0
                },
                {
                    "sent": "Essentially spot U times Sigma inverse times B.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will skip all this detail here and then in the end we get a solution for CCA.",
                    "label": 0
                },
                {
                    "sent": "You wanna slice video X Sigma izla nonzero envelope art and PLP's Lola's video of a matrix.",
                    "label": 0
                },
                {
                    "sent": "Here we assume with table top oil.",
                    "label": 0
                },
                {
                    "sent": "Singular vectors for CCA.",
                    "label": 0
                },
                {
                    "sent": "So now we go slow.",
                    "label": 0
                },
                {
                    "sent": "Linear regression.",
                    "label": 0
                },
                {
                    "sent": "We need to design AT the target matrix T and we do a simple choice T equal to H inverse.",
                    "label": 0
                },
                {
                    "sent": "So at least matrix edge.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is defined here.",
                    "label": 0
                },
                {
                    "sent": "Totally depends on, uh, why the second view, right?",
                    "label": 0
                },
                {
                    "sent": "So we define.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "TSH in both then we'll get this solution for least square and plugging over the over SVD formula will get this form.",
                    "label": 0
                },
                {
                    "sent": "So for detail, again virtual paper.",
                    "label": 0
                },
                {
                    "sent": "So now we we get WCC NND errors.",
                    "label": 0
                },
                {
                    "sent": "We tried to find the connection between these two solutions.",
                    "label": 0
                },
                {
                    "sent": "We can see the first part you are.",
                    "label": 0
                },
                {
                    "sent": "You want the same Sigma inverses thing and Pierre we can is also similar an we have Sigma and Q transpose.",
                    "label": 0
                },
                {
                    "sent": "So here kill is orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So for a given projection, if we do any ugly transformation, does not change the pairwise distance, so Q is is OK.",
                    "label": 0
                },
                {
                    "sent": "So the trick part is Sigma a matrix which comes from this video A.",
                    "label": 0
                },
                {
                    "sent": "And if we can somehow show Sigma is identity learning, these two will be equivalent lonely.",
                    "label": 0
                },
                {
                    "sent": "The only difference is the orthogonal transmission an commonly.",
                    "label": 0
                },
                {
                    "sent": "After we do PCA, projection will apply as being honest labor to do classification.",
                    "label": 0
                },
                {
                    "sent": "So if you apply random projection Q, this will not change the the second classification result because Q is ordinal.",
                    "label": 0
                },
                {
                    "sent": "So we only need to take care of this Sigma matrix.",
                    "label": 0
                },
                {
                    "sent": "So it turns out under mild condition.",
                    "label": 0
                },
                {
                    "sent": "So if ranks equal to minus One X is the data after we remove the mean centered.",
                    "label": 0
                },
                {
                    "sent": "If N -- 1 then we can actually show all the diagonal elements of Sigma.",
                    "label": 1
                },
                {
                    "sent": "A Lisa diagonal matrix is 1.",
                    "label": 0
                },
                {
                    "sent": "Essentially Sigma is identity matrix.",
                    "label": 0
                },
                {
                    "sent": "At least with this condition, essentially claim to say all the data points are linearly independent.",
                    "label": 0
                },
                {
                    "sent": "Right, so this commonly hold if the dimension of the data is larger than the sample size.",
                    "label": 0
                },
                {
                    "sent": "But we are showing our results even if this condition is valid.",
                    "label": 0
                },
                {
                    "sent": "There song list dependence among data least two formulation still very close, so we'll see in the in the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last part of his talk.",
                    "label": 0
                },
                {
                    "sent": "So we should, uh, equivalence connection between CC and linear regression.",
                    "label": 0
                },
                {
                    "sent": "Now we try to develop several season extension based on this square formulation.",
                    "label": 0
                },
                {
                    "sent": "And we know in a in a linear regression we commonly apply to non regulation.",
                    "label": 0
                },
                {
                    "sent": "That's rich regression.",
                    "label": 0
                },
                {
                    "sent": "Essentially we had a two norm penalty for overweight vector language alive Lisa, two regulars least glasses here.",
                    "label": 0
                },
                {
                    "sent": "So one key advantage of this formulation, compared with our original CC formulations.",
                    "label": 0
                },
                {
                    "sent": "Now we can use efficient solver for the square to solve CCA.",
                    "label": 0
                },
                {
                    "sent": "For example, conjugal gradient.",
                    "label": 0
                },
                {
                    "sent": "OK, this is much, much more efficient, especially when the data is sparse, such as text.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A document.",
                    "label": 0
                },
                {
                    "sent": "And in addition, we can derive the sparse easy model by adding a L1 norm penalty over with vector.",
                    "label": 0
                },
                {
                    "sent": "So we add one on penalty to listen regression and we can derive the sparse CC model.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So initially we can compute law so unlucky.",
                    "label": 0
                },
                {
                    "sent": "So here's how to choose.",
                    "label": 0
                },
                {
                    "sent": "Wonder if we can do an efficient cloth validation by computing the entire solution path of CCA using this angle regression algorithm loss.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for a family follow evaluation.",
                    "label": 0
                },
                {
                    "sent": "We use two types of data.",
                    "label": 1
                },
                {
                    "sent": "Both did our multi label data set.",
                    "label": 0
                },
                {
                    "sent": "The first one is a biological image and the second one is the same data set commonly used benchmark data set for multi label classification and we compare this five method CCS original CC formulation without regularization Lambda zero and we have a regular CCA by adding a rich tend to the excellence.",
                    "label": 0
                },
                {
                    "sent": "Then we have the square formation of CCA based on risk equivalent result, and we have at least 291 extension based on this list wax formulation.",
                    "label": 1
                },
                {
                    "sent": "So after CC projection will do a classification.",
                    "label": 0
                },
                {
                    "sent": "In this case we apply linear SVM for all the class labels and we use our C for the value.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Measure.",
                    "label": 0
                },
                {
                    "sent": "So first way.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "2000 something on the data set and we get a bunch of data set where the dimension of the data set is larger than the sample size, and in this case we are observing in all the cases.",
                    "label": 1
                },
                {
                    "sent": "The rank condition.",
                    "label": 0
                },
                {
                    "sent": "This condition is satisfied.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, even dimension is large.",
                    "label": 0
                },
                {
                    "sent": "Learn is commonly is likely over.",
                    "label": 0
                },
                {
                    "sent": "The samples will be linearly independent then our submission will hold and we check.",
                    "label": 0
                },
                {
                    "sent": "In this case, when somebody holds all the diagonal elements equal 1.",
                    "label": 1
                },
                {
                    "sent": "And they achieve the same performance.",
                    "label": 0
                },
                {
                    "sent": "So this shows the least concerns are.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theoretical results.",
                    "label": 0
                },
                {
                    "sent": "So any stable wave compare all these five approach on a biological data set.",
                    "label": 0
                },
                {
                    "sent": "We with different number of labels.",
                    "label": 0
                },
                {
                    "sent": "So here's the sample size.",
                    "label": 0
                },
                {
                    "sent": "And case number of labels and with all this approach, these are all the measures.",
                    "label": 0
                },
                {
                    "sent": "So there are several observations, first one.",
                    "label": 0
                },
                {
                    "sent": "Is the reckless version.",
                    "label": 0
                },
                {
                    "sent": "Compared to original version typically outperform their original formulation.",
                    "label": 0
                },
                {
                    "sent": "For example, RCA will achieve much higher hours value compared to original formulation 5260.",
                    "label": 0
                },
                {
                    "sent": "Similar for least square formulation one onto Norm perform much better.",
                    "label": 0
                },
                {
                    "sent": "So by the way, all the parameter in a list of regression, we do this cross validation to choose optimal value for realisation.",
                    "label": 0
                },
                {
                    "sent": "And the interesting for this case, the sparsity model achieve the best performance in this experiment.",
                    "label": 1
                },
                {
                    "sent": "But this is not the case in the other data set.",
                    "label": 1
                },
                {
                    "sent": "They are very similar in another data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we see from this this figure.",
                    "label": 0
                },
                {
                    "sent": "So in this case we want to see.",
                    "label": 0
                },
                {
                    "sent": "Oh well, equivalence connection really based on assumption like were all the data points linearly independent and lease may not hold for general problems.",
                    "label": 0
                },
                {
                    "sent": "OK so in this case we start the sensitivity for both the surging and same data set the dimension fixed so follow agenda set, dimension IS384 and 4C is 294.",
                    "label": 0
                },
                {
                    "sent": "We fixed dimension and we value the number of data points from small 100 two 1000.",
                    "label": 0
                },
                {
                    "sent": "And we try to observe the difference among all five algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we can see if for the first three columns.",
                    "label": 0
                },
                {
                    "sent": "The sample size smaller dimension.",
                    "label": 0
                },
                {
                    "sent": "We can see the first 2 columns.",
                    "label": 0
                },
                {
                    "sent": "Sincerely, squeezy leads to achieve learning performance.",
                    "label": 0
                },
                {
                    "sent": "This released while pregnant, so this somehow verify our theoretical results.",
                    "label": 0
                },
                {
                    "sent": "The same thing for simpler said, when someone says small learn Lily store identical, exactly technical.",
                    "label": 0
                },
                {
                    "sent": "So next we increase the size.",
                    "label": 0
                },
                {
                    "sent": "400 until 900.",
                    "label": 0
                },
                {
                    "sent": "The sample size is much larger than dimension, and there's a dependence in a data set.",
                    "label": 1
                },
                {
                    "sent": "But still we can see this tool.",
                    "label": 0
                },
                {
                    "sent": "The first 2 columns CC an extension are still fairly close, not valid.",
                    "label": 0
                },
                {
                    "sent": "For all data set.",
                    "label": 0
                },
                {
                    "sent": "So let's not follow case without organization.",
                    "label": 0
                },
                {
                    "sent": "Then let's take a look at the case with regularization.",
                    "label": 0
                },
                {
                    "sent": "The last three columns is one or two normalization using the square and the last one is regular.",
                    "label": 0
                },
                {
                    "sent": "CCA is still a good value problem, so we can see.",
                    "label": 0
                },
                {
                    "sent": "The last two column they correspond to the CC model with two reorganization.",
                    "label": 1
                },
                {
                    "sent": "And we expect they may achieve similar performance.",
                    "label": 0
                },
                {
                    "sent": "So if we take a careful look, last two columns.",
                    "label": 0
                },
                {
                    "sent": "So in most cases they're not far away, not far away.",
                    "label": 0
                },
                {
                    "sent": "And the one difference case follow Jim data set.",
                    "label": 1
                },
                {
                    "sent": "The middle one is sparse model, perform much better.",
                    "label": 0
                },
                {
                    "sent": "Learn the other model for the biological image data set.",
                    "label": 1
                },
                {
                    "sent": "But for some data set they are compatible.",
                    "label": 0
                },
                {
                    "sent": "Sparse model in some case achieve the best performance, but in this case they are compatible.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the advantage of this formulation for CC.",
                    "label": 1
                },
                {
                    "sent": "We also derive entire CC solution path.",
                    "label": 1
                },
                {
                    "sent": "OK for all the possible values of regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "So here we normalize the coefficient of organization from zero to 1 and we draw we hear withdraw the coefficient for the first week of FW.",
                    "label": 0
                },
                {
                    "sent": "One is vector withdrawal occur for all entries.",
                    "label": 1
                },
                {
                    "sent": "We can see Wannacry fish, regular impairment is 1.",
                    "label": 0
                },
                {
                    "sent": "Then all entries non zero and we decrease the value.",
                    "label": 1
                },
                {
                    "sent": "Decrease liquid resin parameter.",
                    "label": 0
                },
                {
                    "sent": "Learn is more and more sparse.",
                    "label": 0
                },
                {
                    "sent": "So after if very lesson 3.2 only one or two entries non 0 so we can vary the sparseness of the model by changing the parameter.",
                    "label": 1
                },
                {
                    "sent": "Of this organization, and in practice we need to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Play close validation.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion is walkway stab Lish?",
                    "label": 0
                },
                {
                    "sent": "Are equivalent connection between CC and linear regression an under mild condition which require all the samples are linearly independent?",
                    "label": 1
                },
                {
                    "sent": "So based on this equivalence connection we can derive several CC model based on the regression model and our results show.",
                    "label": 0
                },
                {
                    "sent": "Well, assumption is hold leads to equipment and our results also show on these two assumptions valid.",
                    "label": 1
                },
                {
                    "sent": "These two models still very close, so this equivalence connection provides new insight on the CC model and also opens the way for further extension.",
                    "label": 0
                },
                {
                    "sent": "For example, we can potentially do semisupervised see non ECC based on the list regression model.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Come up well.",
                    "label": 0
                },
                {
                    "sent": "At one point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's I think could be a possible extension, right?",
                    "label": 0
                },
                {
                    "sent": "Some trace norm will be another extension.",
                    "label": 0
                },
                {
                    "sent": "So essentially whatever we can do for linear regression, we can apply to this model based on the equivalence connection.",
                    "label": 1
                },
                {
                    "sent": "One on piano and two unknown.",
                    "label": 0
                },
                {
                    "sent": "Treatment.",
                    "label": 0
                },
                {
                    "sent": "Square.",
                    "label": 1
                },
                {
                    "sent": "To other settings.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned earlier, one of the key actually a result we use for this equivalence is the edge matrix.",
                    "label": 0
                },
                {
                    "sent": "The target match T orthonormal.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the key assumption as your key.",
                    "label": 0
                },
                {
                    "sent": "Property we use here in this case, so all the proof will not hold on a T matrix is not as normal.",
                    "label": 0
                },
                {
                    "sent": "And power recently we did some further study.",
                    "label": 0
                },
                {
                    "sent": "It turns out this assumption can be actually relaxed.",
                    "label": 0
                },
                {
                    "sent": "So this opens the way to actually to find the connection among.",
                    "label": 0
                },
                {
                    "sent": "Many, I think even value based algorithm, machine learning and the square.",
                    "label": 0
                }
            ]
        }
    }
}