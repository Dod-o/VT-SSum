{
    "id": "tk22rihirko6mk4gq5pmxt2mylmo5w73",
    "title": "From Bandits to Experts : On the Value of More Information",
    "info": {
        "author": [
            "Ohad Shamir, Faculty of Mathematics and Computer Science, Weizmann Institute of Science"
        ],
        "published": "July 25, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning->Exploration vs. Exploitation"
        ]
    },
    "url": "http://videolectures.net/explorationexploitation2011_shamir_bandits/",
    "segmentation": [
        [
            "OK, so this is joint work with shamineau from."
        ],
        [
            "The technion so we have bandits, and we have experts in the title, so just a quick reminder what those two settings are.",
            "So basically there are pretty similar settings, so it's a game with proceeds in rounds in each round.",
            "Some adversary picks a some bounded reward between zero and one for a set of K actions, or Alternatively, experts and the learner then needs to choose one of these actions and gets the reward of the action it shows.",
            "Now, in an expert setting, we usually assume that the learner then gets to see a all the rewards of all the experts, including ones that he didn't choose in a multi armed bandit setting, the learner only gets to see the reward of the action.",
            "He actually picked an.",
            "In both cases, our goal is to minimize the regret with respect to the best single action, or the best single expert in high."
        ],
        [
            "On site.",
            "Now, in terms of attainable guarantees.",
            "So in an expert setting, we know that it's possible to get regret with scales like square root of T and logarithmically in the number of experts in abandoned setting.",
            "We get worse regret in terms of K, the number of actions, so you have this square root of K factor here.",
            "And the kind of intuition why we get this worse regret for the bandit setting is that we get less information, sort of 1 / K as much information in a certain sense.",
            "And what we discuss in this work is a setting which basically interpolates between the expert setting on one hand and the multi armed bandit setting on the other.",
            "An.",
            "In a nutshell, the idea is that it's a setting where when you choose an action you observe not just reward of your own action, but some site observation.",
            "Some side information on some of the other actions rewards."
        ],
        [
            "And the saying it a bit more formally.",
            "So we assume that the game is defined by a sequence of possibly directed graphs, one for each round, and each time when the learner picks a certain action it's get it gets to see his own reward plus the reward he would have got a for any of the neighbors of this action in the graph.",
            "OK, so we can always get a regret with skills like a scroll of K because we can apply a standard multi armed bandits algorithm which ignores this sort of additional site information.",
            "And the main question is can we do a better by using this?"
        ],
        [
            "Information.",
            "So just to give two very simple examples.",
            "So if we talk about GT, which is the same for every round and it's the complete graph, then this is just the expert setting because it means that no matter which action you pick, you get to observe the rewards of all the other actions on the other."
        ],
        [
            "Extreme for bandit setting.",
            "G is going to be an empty graph, so no matter what you pick, you only get to see your own reward."
        ],
        [
            "But in between you can talk about all kinds of other graphs with structure, which might also change with time.",
            "And in fact one can even we can even deal with settings where your reward observations are not exact, where can only get an estimate.",
            "But in this talk I'll focus about exact observations for simplicity."
        ],
        [
            "And one can think on several practical motivations for this.",
            "So one for instance can be to capture structure between the action.",
            "So taking the Canonical application of web advertising so you know, suppose you have a two ads both for for a Lamborghini cars.",
            "So suppose you display one of these ads to user and the user clicked on it.",
            "So even though you didn't display the other ad, it's quite likely that.",
            "If you would have displayed that, add the user would have clicked on it as well, so this is an example of correlation between the ads, but you can also talk about anti correlation.",
            "So if you had an ad for Lamborghini, an ad for an old used clunker, then someone who clicks and this is unlikely to click on this ad and vice versa.",
            "Or maybe giving a stronger example if you have an ad for wheelchair accessories and an ad for running shoes, it's unlikely that.",
            "The user clicked in, one would have clicked on the other one can give also motivations from other areas such as sensor networks where getting information from one sensor also tells you something about what happens with neighboring sensors and so on."
        ],
        [
            "OK, so how do we deal with this setting?",
            "So here is the first attempt which basically tries to combine existing methods in order to do it.",
            "Eh, So suppose for now that our graph is fixed, so the information feedback structure is fixed for all."
        ],
        [
            "Ounce.",
            "And we can do the following.",
            "So we take our graph and we split it.",
            "2 clicks now."
        ],
        [
            "For each click it, we can apply a standard experts algorithm and we know that with respect to the actions in that click, this algorithm would give us low regret with respect to any action in this click.",
            "OK, so we can apply such an algorithm, assign such an algorithm for each click separately and."
        ],
        [
            "Then we can combine these algorithms.",
            "We can think of each such algorithm as a meta action and combine them with an experts algorithm.",
            "Sorry with the multi armed bandits algorithm to get a low regret with respect to each of these meta actions, but each meta action is actually an experts algorithm which gives us low regret with respect to any individual action, and that's how we can get an algorithm for this."
        ],
        [
            "Setting.",
            "In terms of the guarantee, so we guarantee basically looks as follows.",
            "So if you can split your graph to see clicks then you regret would scale like square root of C&C is always at least what's known as the click partition number kaybar of your graph, so that's the fewest possible number of clicks in which you can partition your graph.",
            "So this is a simple algorithm.",
            "It uses existing methods, but it also has some disadvantages.",
            "So one thing is that it only works for fixed graphs.",
            "The graph cannot change with time, as we'll see shortly, this is not the optimal regret.",
            "Even if you can make see the clique partition number and maybe the biggest disadvantage is that you cannot really get good regret here, at least in terms of computational hardness because.",
            "Finding a good click partition of the graph is NP hard.",
            "Even NP hard to approximate it.",
            "So in the worst case, you can't make this number.",
            "See here significantly smaller than K and then you get the same regret as a standard multi arm bandits algorithm."
        ],
        [
            "A in terms of a lower bound like what's the best we can possibly hope for so we can show that the best thing would basically depend on Alpha G, the independence number of the graph, which is just the size of the largest set of actions which have no edges between them, and it's important to emphasize that the gap between Alpha G and the click partition and number kaybar of G, which is.",
            "Quantify the regret of the algorithm.",
            "We talked about earlier the gap between the two can be very large, so the simplest example is maybe a random graph where the difference between the two is basically log K and almost K."
        ],
        [
            "And the basic intuition of the proof.",
            "I won't go into the details is that an adversary can make this game as hard as a standard multi armed bandits over Alpha G action.",
            "So we sort of forces the learner to play over this independent set in which on which you get no side information.",
            "So no matter which action you choose from this independent set, you don't get any information about the other actions, and then we utilize a known lower bound of square root NT for an actions.",
            "For standard material."
        ],
        [
            "And it's.",
            "OK, so here is a better algorithm actually denoted as LP short for exponentially weighted algorithm with linear programming.",
            "Yeah, I won't go again through all the technical details, but in a nutshell, it's a variant of the X Ray bandits algorithm with the key differences being that we use a different kind of reward estimate and estimate which takes the side observations that we have into account and also wears in X3 three.",
            "The exploration component is uniform.",
            "You explore uniformly over all arms.",
            "Here you need to a way it.",
            "According to a specific distribution, which is actually a solution of a linear program induced by the graph."
        ],
        [
            "Fare structure.",
            "And the guarantee that you can get for this algorithm is a it looks as follows.",
            "Assuming that you pick parameters appropriately according to the theory.",
            "And say for fixed graph G. So the regret is really the same as the lower bound I told you about earlier up to log factors.",
            "So this is the result for undirected graphs.",
            "Will speak about directed graphs shortly.",
            "And they actually there is a bit of a magic going on here because they regret.",
            "Involves this Alpha G term, the independence number of the graph, which is actually NP hard to compute.",
            "So in the worst case, you cannot even compute this thing, but somehow there's this magic going on that in terms of the algorithm is something that you can actually apply efficiently.",
            "You don't need to compute these hard graph theoretic numbers in order to make this algorithm work, just run.",
            "A inefficient linear."
        ],
        [
            "A program.",
            "So again, I want to go over all the details.",
            "You can find it in our a paper and so at a very basic level it's not very different than the standard approach 1 uses to develop bandit algorithms, but you have all kinds of different quantities that you need to handle.",
            "So for instance you have this term that you need to optimize, which is basically this linear program so.",
            "The intuition is that you sort of have to distribute your exploration in such a way that no matter which action you pick, the probability mass over its neighbors wouldn't be too small.",
            "And there are also various inequality's there which might be of independent interest, but I won't go into the deep."
        ],
        [
            "So what I showed previously was for undirected graphs.",
            "For directed graphs, things are a bit different, and the best thing that we can prove is this bound, which is similar for the undirected case.",
            "But instead of the independence number it's replaced by the larger click partition number.",
            "And as I said previously, the gap between these two things can be large.",
            "Even something like log K versus K. We don't know if this is the optimal thing that you can do for directed graphs, but we do suspect that you really need so for directed graphs.",
            "You cannot get something which depends on the independence number.",
            "You really need something which is a bit larger and we don't know if we can improve our lower bounds to account for."
        ],
        [
            "Yes.",
            "Just to give a very simple illustrative experiment of how these things behave.",
            "So we ran three algorithms.",
            "This standard X free algorithm A.",
            "The X band algorithm will just combines experts in bandits algorithm and our LP algorithm.",
            "So this is for a random graph over a 300 actions.",
            "Each action gets a Bernoulli reward with probability a half, and there is a single action whose reward is one of probability 3/4, and these different graphs correspond to different graph structures.",
            "So this is basically the connectivity parameter.",
            "The probability that any two actions would be linked.",
            "So the act three algorithm which doesn't use any set information gets the same kind of average payoff for.",
            "All these versions, but the other two algorithms do improve as they get more side information, But the LP algorithm seems to work quite a lot better."
        ],
        [
            "So to conclude, so we studied this setting which interpolates between the expert setting on one hand and multi armed bandits on the other hand, despite the fact that the regret involves these NP hard to compute quantities, the algorithm is computationally efficient.",
            "It's provably optimal for fixed undirected graphs, but there are still lots of open questions remaining.",
            "For instance, what happens if the graph is not fixed, or if it's?",
            "Corrected then we still don't know what's the optimal thing you can do in there.",
            "Also more general open open questions.",
            "For instance, what happens if you don't know the information feedback structure?",
            "You don't know in advance how the graphs look like.",
            "Eh, and so there are still several things here that we still don't understand."
        ],
        [
            "Um, so if you're interested in this work, we have a tech report which we uploaded to archive.",
            "Or you can also look at the slides or talk to me later on.",
            "That's it.",
            "Thank you very much.",
            "So there's a starting point you would go for.",
            "Analysis If you started with 64 and he said the rewards of all the actions which are nearby are the same, then you can identify the graph structure with agreement structure amongst the experts in use before.",
            "So you're talking about dealing with the setting where the graph structure is unknown.",
            "OK, so I think I'm just gonna repeat it.",
            "The simplification of your settings OK, where the rewards of the of the other arms you learn about identical to the words of the arms you you pull.",
            "OK, identify the graph structure with the agreement structure of the set of experts in these before.",
            "OK, but this is assuming that the rewards are the same.",
            "OK, so now so so so then it gives you a way to kind of mapping announces over into your setting and now if you want it weeks before to deal with different rewards on these other actions.",
            "There will be straightforward way to do that, because you could just be you.",
            "Yeah, it's like you have the reward and this other actions and function of the reward in the execute actually observe.",
            "And the probability of this other action isn't going to be just the probability of of the.",
            "The rather than probably this induced by by the projection of the distribution of experts on distribution over over your actions.",
            "Um so.",
            "So I think I'm understood what you meant, although OK, maybe we should take it offline.",
            "Bye.",
            "And.",
            "Yeah.",
            "Therein lies the XP for.",
            "From this perspective, trying to reduce this cost of factors and they actually did something very similar like linear programming, saying to use the exploration so their observation was the expiration could be really too high.",
            "If you have money expert that degree.",
            "So I'm wondering about like what exactly the relationship between the two girls is, so I'm not sure.",
            "Actually I talked with Brandon about that work and it didn't ring a Bell for him.",
            "So.",
            "So there might be something different in the setting, but I'm not familiar with that work, but.",
            "But if it's kind of the same idea that you bring, and then maybe there is even more general, you know underlying principle that we are not really exploiting yet, so there is a general underlying principle, which is that you need to impose a minimum probability on on the on the action and the linear programming is one way to do that.",
            "Missing with uniform.",
            "OK, so I mean, at least in our setting, just mixing it with uniform distribution really doesn't seem to be good enough, but.",
            "And.",
            "OK, yeah, I guess I need to check out that work that you mention.",
            "Next"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is joint work with shamineau from.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The technion so we have bandits, and we have experts in the title, so just a quick reminder what those two settings are.",
                    "label": 0
                },
                {
                    "sent": "So basically there are pretty similar settings, so it's a game with proceeds in rounds in each round.",
                    "label": 0
                },
                {
                    "sent": "Some adversary picks a some bounded reward between zero and one for a set of K actions, or Alternatively, experts and the learner then needs to choose one of these actions and gets the reward of the action it shows.",
                    "label": 0
                },
                {
                    "sent": "Now, in an expert setting, we usually assume that the learner then gets to see a all the rewards of all the experts, including ones that he didn't choose in a multi armed bandit setting, the learner only gets to see the reward of the action.",
                    "label": 0
                },
                {
                    "sent": "He actually picked an.",
                    "label": 0
                },
                {
                    "sent": "In both cases, our goal is to minimize the regret with respect to the best single action, or the best single expert in high.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On site.",
                    "label": 0
                },
                {
                    "sent": "Now, in terms of attainable guarantees.",
                    "label": 0
                },
                {
                    "sent": "So in an expert setting, we know that it's possible to get regret with scales like square root of T and logarithmically in the number of experts in abandoned setting.",
                    "label": 0
                },
                {
                    "sent": "We get worse regret in terms of K, the number of actions, so you have this square root of K factor here.",
                    "label": 0
                },
                {
                    "sent": "And the kind of intuition why we get this worse regret for the bandit setting is that we get less information, sort of 1 / K as much information in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "And what we discuss in this work is a setting which basically interpolates between the expert setting on one hand and the multi armed bandit setting on the other.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "In a nutshell, the idea is that it's a setting where when you choose an action you observe not just reward of your own action, but some site observation.",
                    "label": 0
                },
                {
                    "sent": "Some side information on some of the other actions rewards.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the saying it a bit more formally.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the game is defined by a sequence of possibly directed graphs, one for each round, and each time when the learner picks a certain action it's get it gets to see his own reward plus the reward he would have got a for any of the neighbors of this action in the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can always get a regret with skills like a scroll of K because we can apply a standard multi armed bandits algorithm which ignores this sort of additional site information.",
                    "label": 0
                },
                {
                    "sent": "And the main question is can we do a better by using this?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "So just to give two very simple examples.",
                    "label": 0
                },
                {
                    "sent": "So if we talk about GT, which is the same for every round and it's the complete graph, then this is just the expert setting because it means that no matter which action you pick, you get to observe the rewards of all the other actions on the other.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extreme for bandit setting.",
                    "label": 0
                },
                {
                    "sent": "G is going to be an empty graph, so no matter what you pick, you only get to see your own reward.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in between you can talk about all kinds of other graphs with structure, which might also change with time.",
                    "label": 0
                },
                {
                    "sent": "And in fact one can even we can even deal with settings where your reward observations are not exact, where can only get an estimate.",
                    "label": 0
                },
                {
                    "sent": "But in this talk I'll focus about exact observations for simplicity.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one can think on several practical motivations for this.",
                    "label": 0
                },
                {
                    "sent": "So one for instance can be to capture structure between the action.",
                    "label": 1
                },
                {
                    "sent": "So taking the Canonical application of web advertising so you know, suppose you have a two ads both for for a Lamborghini cars.",
                    "label": 0
                },
                {
                    "sent": "So suppose you display one of these ads to user and the user clicked on it.",
                    "label": 0
                },
                {
                    "sent": "So even though you didn't display the other ad, it's quite likely that.",
                    "label": 0
                },
                {
                    "sent": "If you would have displayed that, add the user would have clicked on it as well, so this is an example of correlation between the ads, but you can also talk about anti correlation.",
                    "label": 0
                },
                {
                    "sent": "So if you had an ad for Lamborghini, an ad for an old used clunker, then someone who clicks and this is unlikely to click on this ad and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Or maybe giving a stronger example if you have an ad for wheelchair accessories and an ad for running shoes, it's unlikely that.",
                    "label": 0
                },
                {
                    "sent": "The user clicked in, one would have clicked on the other one can give also motivations from other areas such as sensor networks where getting information from one sensor also tells you something about what happens with neighboring sensors and so on.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we deal with this setting?",
                    "label": 0
                },
                {
                    "sent": "So here is the first attempt which basically tries to combine existing methods in order to do it.",
                    "label": 0
                },
                {
                    "sent": "Eh, So suppose for now that our graph is fixed, so the information feedback structure is fixed for all.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ounce.",
                    "label": 0
                },
                {
                    "sent": "And we can do the following.",
                    "label": 0
                },
                {
                    "sent": "So we take our graph and we split it.",
                    "label": 0
                },
                {
                    "sent": "2 clicks now.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each click it, we can apply a standard experts algorithm and we know that with respect to the actions in that click, this algorithm would give us low regret with respect to any action in this click.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can apply such an algorithm, assign such an algorithm for each click separately and.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can combine these algorithms.",
                    "label": 0
                },
                {
                    "sent": "We can think of each such algorithm as a meta action and combine them with an experts algorithm.",
                    "label": 0
                },
                {
                    "sent": "Sorry with the multi armed bandits algorithm to get a low regret with respect to each of these meta actions, but each meta action is actually an experts algorithm which gives us low regret with respect to any individual action, and that's how we can get an algorithm for this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting.",
                    "label": 0
                },
                {
                    "sent": "In terms of the guarantee, so we guarantee basically looks as follows.",
                    "label": 0
                },
                {
                    "sent": "So if you can split your graph to see clicks then you regret would scale like square root of C&C is always at least what's known as the click partition number kaybar of your graph, so that's the fewest possible number of clicks in which you can partition your graph.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "It uses existing methods, but it also has some disadvantages.",
                    "label": 0
                },
                {
                    "sent": "So one thing is that it only works for fixed graphs.",
                    "label": 1
                },
                {
                    "sent": "The graph cannot change with time, as we'll see shortly, this is not the optimal regret.",
                    "label": 0
                },
                {
                    "sent": "Even if you can make see the clique partition number and maybe the biggest disadvantage is that you cannot really get good regret here, at least in terms of computational hardness because.",
                    "label": 0
                },
                {
                    "sent": "Finding a good click partition of the graph is NP hard.",
                    "label": 0
                },
                {
                    "sent": "Even NP hard to approximate it.",
                    "label": 0
                },
                {
                    "sent": "So in the worst case, you can't make this number.",
                    "label": 0
                },
                {
                    "sent": "See here significantly smaller than K and then you get the same regret as a standard multi arm bandits algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A in terms of a lower bound like what's the best we can possibly hope for so we can show that the best thing would basically depend on Alpha G, the independence number of the graph, which is just the size of the largest set of actions which have no edges between them, and it's important to emphasize that the gap between Alpha G and the click partition and number kaybar of G, which is.",
                    "label": 0
                },
                {
                    "sent": "Quantify the regret of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "We talked about earlier the gap between the two can be very large, so the simplest example is maybe a random graph where the difference between the two is basically log K and almost K.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the basic intuition of the proof.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the details is that an adversary can make this game as hard as a standard multi armed bandits over Alpha G action.",
                    "label": 0
                },
                {
                    "sent": "So we sort of forces the learner to play over this independent set in which on which you get no side information.",
                    "label": 0
                },
                {
                    "sent": "So no matter which action you choose from this independent set, you don't get any information about the other actions, and then we utilize a known lower bound of square root NT for an actions.",
                    "label": 0
                },
                {
                    "sent": "For standard material.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is a better algorithm actually denoted as LP short for exponentially weighted algorithm with linear programming.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I won't go again through all the technical details, but in a nutshell, it's a variant of the X Ray bandits algorithm with the key differences being that we use a different kind of reward estimate and estimate which takes the side observations that we have into account and also wears in X3 three.",
                    "label": 0
                },
                {
                    "sent": "The exploration component is uniform.",
                    "label": 0
                },
                {
                    "sent": "You explore uniformly over all arms.",
                    "label": 0
                },
                {
                    "sent": "Here you need to a way it.",
                    "label": 0
                },
                {
                    "sent": "According to a specific distribution, which is actually a solution of a linear program induced by the graph.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fare structure.",
                    "label": 0
                },
                {
                    "sent": "And the guarantee that you can get for this algorithm is a it looks as follows.",
                    "label": 0
                },
                {
                    "sent": "Assuming that you pick parameters appropriately according to the theory.",
                    "label": 0
                },
                {
                    "sent": "And say for fixed graph G. So the regret is really the same as the lower bound I told you about earlier up to log factors.",
                    "label": 0
                },
                {
                    "sent": "So this is the result for undirected graphs.",
                    "label": 1
                },
                {
                    "sent": "Will speak about directed graphs shortly.",
                    "label": 0
                },
                {
                    "sent": "And they actually there is a bit of a magic going on here because they regret.",
                    "label": 0
                },
                {
                    "sent": "Involves this Alpha G term, the independence number of the graph, which is actually NP hard to compute.",
                    "label": 0
                },
                {
                    "sent": "So in the worst case, you cannot even compute this thing, but somehow there's this magic going on that in terms of the algorithm is something that you can actually apply efficiently.",
                    "label": 0
                },
                {
                    "sent": "You don't need to compute these hard graph theoretic numbers in order to make this algorithm work, just run.",
                    "label": 0
                },
                {
                    "sent": "A inefficient linear.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A program.",
                    "label": 0
                },
                {
                    "sent": "So again, I want to go over all the details.",
                    "label": 0
                },
                {
                    "sent": "You can find it in our a paper and so at a very basic level it's not very different than the standard approach 1 uses to develop bandit algorithms, but you have all kinds of different quantities that you need to handle.",
                    "label": 0
                },
                {
                    "sent": "So for instance you have this term that you need to optimize, which is basically this linear program so.",
                    "label": 0
                },
                {
                    "sent": "The intuition is that you sort of have to distribute your exploration in such a way that no matter which action you pick, the probability mass over its neighbors wouldn't be too small.",
                    "label": 0
                },
                {
                    "sent": "And there are also various inequality's there which might be of independent interest, but I won't go into the deep.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I showed previously was for undirected graphs.",
                    "label": 0
                },
                {
                    "sent": "For directed graphs, things are a bit different, and the best thing that we can prove is this bound, which is similar for the undirected case.",
                    "label": 1
                },
                {
                    "sent": "But instead of the independence number it's replaced by the larger click partition number.",
                    "label": 0
                },
                {
                    "sent": "And as I said previously, the gap between these two things can be large.",
                    "label": 0
                },
                {
                    "sent": "Even something like log K versus K. We don't know if this is the optimal thing that you can do for directed graphs, but we do suspect that you really need so for directed graphs.",
                    "label": 0
                },
                {
                    "sent": "You cannot get something which depends on the independence number.",
                    "label": 0
                },
                {
                    "sent": "You really need something which is a bit larger and we don't know if we can improve our lower bounds to account for.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Just to give a very simple illustrative experiment of how these things behave.",
                    "label": 0
                },
                {
                    "sent": "So we ran three algorithms.",
                    "label": 0
                },
                {
                    "sent": "This standard X free algorithm A.",
                    "label": 0
                },
                {
                    "sent": "The X band algorithm will just combines experts in bandits algorithm and our LP algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is for a random graph over a 300 actions.",
                    "label": 0
                },
                {
                    "sent": "Each action gets a Bernoulli reward with probability a half, and there is a single action whose reward is one of probability 3/4, and these different graphs correspond to different graph structures.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the connectivity parameter.",
                    "label": 0
                },
                {
                    "sent": "The probability that any two actions would be linked.",
                    "label": 0
                },
                {
                    "sent": "So the act three algorithm which doesn't use any set information gets the same kind of average payoff for.",
                    "label": 0
                },
                {
                    "sent": "All these versions, but the other two algorithms do improve as they get more side information, But the LP algorithm seems to work quite a lot better.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, so we studied this setting which interpolates between the expert setting on one hand and multi armed bandits on the other hand, despite the fact that the regret involves these NP hard to compute quantities, the algorithm is computationally efficient.",
                    "label": 1
                },
                {
                    "sent": "It's provably optimal for fixed undirected graphs, but there are still lots of open questions remaining.",
                    "label": 1
                },
                {
                    "sent": "For instance, what happens if the graph is not fixed, or if it's?",
                    "label": 0
                },
                {
                    "sent": "Corrected then we still don't know what's the optimal thing you can do in there.",
                    "label": 0
                },
                {
                    "sent": "Also more general open open questions.",
                    "label": 1
                },
                {
                    "sent": "For instance, what happens if you don't know the information feedback structure?",
                    "label": 0
                },
                {
                    "sent": "You don't know in advance how the graphs look like.",
                    "label": 0
                },
                {
                    "sent": "Eh, and so there are still several things here that we still don't understand.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, so if you're interested in this work, we have a tech report which we uploaded to archive.",
                    "label": 1
                },
                {
                    "sent": "Or you can also look at the slides or talk to me later on.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So there's a starting point you would go for.",
                    "label": 0
                },
                {
                    "sent": "Analysis If you started with 64 and he said the rewards of all the actions which are nearby are the same, then you can identify the graph structure with agreement structure amongst the experts in use before.",
                    "label": 0
                },
                {
                    "sent": "So you're talking about dealing with the setting where the graph structure is unknown.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I'm just gonna repeat it.",
                    "label": 0
                },
                {
                    "sent": "The simplification of your settings OK, where the rewards of the of the other arms you learn about identical to the words of the arms you you pull.",
                    "label": 0
                },
                {
                    "sent": "OK, identify the graph structure with the agreement structure of the set of experts in these before.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is assuming that the rewards are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so now so so so then it gives you a way to kind of mapping announces over into your setting and now if you want it weeks before to deal with different rewards on these other actions.",
                    "label": 0
                },
                {
                    "sent": "There will be straightforward way to do that, because you could just be you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's like you have the reward and this other actions and function of the reward in the execute actually observe.",
                    "label": 0
                },
                {
                    "sent": "And the probability of this other action isn't going to be just the probability of of the.",
                    "label": 1
                },
                {
                    "sent": "The rather than probably this induced by by the projection of the distribution of experts on distribution over over your actions.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "So I think I'm understood what you meant, although OK, maybe we should take it offline.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Therein lies the XP for.",
                    "label": 0
                },
                {
                    "sent": "From this perspective, trying to reduce this cost of factors and they actually did something very similar like linear programming, saying to use the exploration so their observation was the expiration could be really too high.",
                    "label": 0
                },
                {
                    "sent": "If you have money expert that degree.",
                    "label": 0
                },
                {
                    "sent": "So I'm wondering about like what exactly the relationship between the two girls is, so I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Actually I talked with Brandon about that work and it didn't ring a Bell for him.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So there might be something different in the setting, but I'm not familiar with that work, but.",
                    "label": 0
                },
                {
                    "sent": "But if it's kind of the same idea that you bring, and then maybe there is even more general, you know underlying principle that we are not really exploiting yet, so there is a general underlying principle, which is that you need to impose a minimum probability on on the on the action and the linear programming is one way to do that.",
                    "label": 0
                },
                {
                    "sent": "Missing with uniform.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean, at least in our setting, just mixing it with uniform distribution really doesn't seem to be good enough, but.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, I guess I need to check out that work that you mention.",
                    "label": 0
                },
                {
                    "sent": "Next",
                    "label": 0
                }
            ]
        }
    }
}