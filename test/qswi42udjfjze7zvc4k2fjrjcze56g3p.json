{
    "id": "qswi42udjfjze7zvc4k2fjrjcze56g3p",
    "title": "The landscape of the loss surfaces of multilayer networks",
    "info": {
        "author": [
            "Anna Choromanska, Department of Electrical Engineering, Columbia University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_choromanska_multilayer_networks/",
    "segmentation": [
        [
            "So my name is under my scan.",
            "This is a joint work with young Mccowan Jurado panels.",
            "We're interested in trying to characterize the last surfaces of multilayer networks."
        ],
        [
            "And in particular, this is a very open problem in recent works there is little work done so far, understanding the last surfaces of multilayer networks can help us to better understand the answers to the following questions.",
            "For example, while the result of multiple experiments with multilayer networks give consistently the same performance despite the presence of many critical points in the optimization problem, what is the role of subtle points in the optimization problem, and is the surface that we are optimizing actually structured?",
            "Or not, and we are."
        ],
        [
            "Interested in trying to link the spherical spin class theory to explain the optimization paradigm in deep learning.",
            "Naturally to build this connection we need to make assumptions.",
            "Some of these assumptions are very heavy and unrealistic and actually open problem is connected with trying to help us to drop."
        ],
        [
            "This unrealistic assumptions, so let's consider the feed forward network feed forward network with a single output.",
            "The output of the network can be expressed as the sum over the input output parts of the network, and then we have the product of XI, which is the input to the path.",
            "We have the activation of the path the activation is connected with the fact that the units in the network have value nonlinearity, so essentially it acts as an activator or does activator of the path.",
            "And then if we have a product of weights of the path because the network is nothing more.",
            "Then just a weighted directed graph.",
            "And we consider a loss function.",
            "We will consider the hinge loss which is defined by the Max operator.",
            "I also want to define the quantity that I will refer to as the size which is Lambda equal to the number of paths in the network to the power of 1 / H and so."
        ],
        [
            "As I sat to establish the connection, we need to make assumptions of.",
            "So what we do, we will try to model Max operator and the activation of the path using Bernoulli random variables.",
            "We will also assume that instead of having a network where every edge is connected with different weight, we will have Lambda weights that are uniformly distributed in the network.",
            "What I mean by that is if I take the product of length H of the weights, each of these products will appear in the network with the same probability.",
            "We also impose a spherical constraint, namely that this Lambda weights live on the sphere, and we assumed."
        ],
        [
            "And inputs.",
            "And so as I said, we also need to make on realistic assumptions.",
            "In particular, we need to assume that the activation mechanism of the path is independent of the input.",
            "It's clearly well.",
            "Clearly there should be some dependence because the input activates the path that we have to assume that the path have independent inputs.",
            "Again, it is an unrealistic assumption because we know that there are parts which are connected with the same input."
        ],
        [
            "However, if this assumptions are going to be taken, we actually can show that the loss function becomes the Hamiltonian of this vertical spin glass model, as is shown on the slide.",
            "So our open problem is, can we establish a stronger connection between the loss function of the deep model and the spherical spin glass model by dropping the unrealistic assumptions, and so the question is why would we like to do that?",
            "So let me justify why it turns out that actually when the size of the network increases, what happens?",
            "Is the optimization, the landscape, the optimization?",
            "Landscape becomes."
        ],
        [
            "Doctor, So what I'm showing here is the experiment where on the left side we have a spin glass on the right side.",
            "We have a deep model.",
            "As I increase the size of the model.",
            "What happened?",
            "In both cases we will recover solutions which correspond to a smaller value of the loss function.",
            "Actually they will be more closely concentrated around the mean, so the variance will drop.",
            "This is the.",
            "This is the behavior that we observe both in a spin glass models and in deep models and more."
        ],
        [
            "Over certain empirical observations and deep learning that we we we observe can be very well explained using the spinning class theory.",
            "So let's look at some examples and a deep learning setting for we know that for large size networks we will typically recover similar solutions despite the fact that we have many critical points.",
            "Seems like local minima are really equivalent and yelled at the same performance.",
            "We also know that for large networks it is less likely to obtain bad quality.",
            "A solution for small sized networks actually in the spin class theory we know we actually can prove that, but critical points form an ordered structure, so there is an energy barrier, a certain value of the loss function above which we will have high index critical points and below which we will have low index critical points and local minima, and those are very well concentrated close to the energy barrier, so they almost all of them correspond to exactly the same value of the loss function.",
            "Moreover, it's almost it's.",
            "Actually unlikely to recover local minimum.",
            "That will be far away from the global minimum above the energy barrier."
        ],
        [
            "We also know from the deep learning something that there is a lot of subtle points in the optimization problem, and so actually the spincast theory is showing that the number of high index saddle points is actually exponentially large.",
            "Those are lying above the energy barrier and they're much more of them than than local minima and low index critical points and two."
        ],
        [
            "Finished, I just want to say that if we can establish a stronger connection between most models, we can try to understand whether it's actually necessary in a deep learning setting to find the global minimizer, because the spin class theory is telling us precisely that recovering the global minimum takes exponentially long time.",
            "It's in practice just infeasible."
        ],
        [
            "So just to repeat, we encourage everybody to tackle the problem of establishing the stronger connection between both models.",
            "And naturally we also work in this direction now.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my name is under my scan.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with young Mccowan Jurado panels.",
                    "label": 0
                },
                {
                    "sent": "We're interested in trying to characterize the last surfaces of multilayer networks.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in particular, this is a very open problem in recent works there is little work done so far, understanding the last surfaces of multilayer networks can help us to better understand the answers to the following questions.",
                    "label": 0
                },
                {
                    "sent": "For example, while the result of multiple experiments with multilayer networks give consistently the same performance despite the presence of many critical points in the optimization problem, what is the role of subtle points in the optimization problem, and is the surface that we are optimizing actually structured?",
                    "label": 1
                },
                {
                    "sent": "Or not, and we are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interested in trying to link the spherical spin class theory to explain the optimization paradigm in deep learning.",
                    "label": 1
                },
                {
                    "sent": "Naturally to build this connection we need to make assumptions.",
                    "label": 0
                },
                {
                    "sent": "Some of these assumptions are very heavy and unrealistic and actually open problem is connected with trying to help us to drop.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This unrealistic assumptions, so let's consider the feed forward network feed forward network with a single output.",
                    "label": 0
                },
                {
                    "sent": "The output of the network can be expressed as the sum over the input output parts of the network, and then we have the product of XI, which is the input to the path.",
                    "label": 1
                },
                {
                    "sent": "We have the activation of the path the activation is connected with the fact that the units in the network have value nonlinearity, so essentially it acts as an activator or does activator of the path.",
                    "label": 0
                },
                {
                    "sent": "And then if we have a product of weights of the path because the network is nothing more.",
                    "label": 0
                },
                {
                    "sent": "Then just a weighted directed graph.",
                    "label": 1
                },
                {
                    "sent": "And we consider a loss function.",
                    "label": 0
                },
                {
                    "sent": "We will consider the hinge loss which is defined by the Max operator.",
                    "label": 0
                },
                {
                    "sent": "I also want to define the quantity that I will refer to as the size which is Lambda equal to the number of paths in the network to the power of 1 / H and so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I sat to establish the connection, we need to make assumptions of.",
                    "label": 0
                },
                {
                    "sent": "So what we do, we will try to model Max operator and the activation of the path using Bernoulli random variables.",
                    "label": 1
                },
                {
                    "sent": "We will also assume that instead of having a network where every edge is connected with different weight, we will have Lambda weights that are uniformly distributed in the network.",
                    "label": 1
                },
                {
                    "sent": "What I mean by that is if I take the product of length H of the weights, each of these products will appear in the network with the same probability.",
                    "label": 0
                },
                {
                    "sent": "We also impose a spherical constraint, namely that this Lambda weights live on the sphere, and we assumed.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And inputs.",
                    "label": 0
                },
                {
                    "sent": "And so as I said, we also need to make on realistic assumptions.",
                    "label": 0
                },
                {
                    "sent": "In particular, we need to assume that the activation mechanism of the path is independent of the input.",
                    "label": 1
                },
                {
                    "sent": "It's clearly well.",
                    "label": 0
                },
                {
                    "sent": "Clearly there should be some dependence because the input activates the path that we have to assume that the path have independent inputs.",
                    "label": 0
                },
                {
                    "sent": "Again, it is an unrealistic assumption because we know that there are parts which are connected with the same input.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, if this assumptions are going to be taken, we actually can show that the loss function becomes the Hamiltonian of this vertical spin glass model, as is shown on the slide.",
                    "label": 0
                },
                {
                    "sent": "So our open problem is, can we establish a stronger connection between the loss function of the deep model and the spherical spin glass model by dropping the unrealistic assumptions, and so the question is why would we like to do that?",
                    "label": 1
                },
                {
                    "sent": "So let me justify why it turns out that actually when the size of the network increases, what happens?",
                    "label": 0
                },
                {
                    "sent": "Is the optimization, the landscape, the optimization?",
                    "label": 0
                },
                {
                    "sent": "Landscape becomes.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doctor, So what I'm showing here is the experiment where on the left side we have a spin glass on the right side.",
                    "label": 0
                },
                {
                    "sent": "We have a deep model.",
                    "label": 0
                },
                {
                    "sent": "As I increase the size of the model.",
                    "label": 0
                },
                {
                    "sent": "What happened?",
                    "label": 0
                },
                {
                    "sent": "In both cases we will recover solutions which correspond to a smaller value of the loss function.",
                    "label": 0
                },
                {
                    "sent": "Actually they will be more closely concentrated around the mean, so the variance will drop.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the behavior that we observe both in a spin glass models and in deep models and more.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over certain empirical observations and deep learning that we we we observe can be very well explained using the spinning class theory.",
                    "label": 0
                },
                {
                    "sent": "So let's look at some examples and a deep learning setting for we know that for large size networks we will typically recover similar solutions despite the fact that we have many critical points.",
                    "label": 0
                },
                {
                    "sent": "Seems like local minima are really equivalent and yelled at the same performance.",
                    "label": 1
                },
                {
                    "sent": "We also know that for large networks it is less likely to obtain bad quality.",
                    "label": 0
                },
                {
                    "sent": "A solution for small sized networks actually in the spin class theory we know we actually can prove that, but critical points form an ordered structure, so there is an energy barrier, a certain value of the loss function above which we will have high index critical points and below which we will have low index critical points and local minima, and those are very well concentrated close to the energy barrier, so they almost all of them correspond to exactly the same value of the loss function.",
                    "label": 1
                },
                {
                    "sent": "Moreover, it's almost it's.",
                    "label": 0
                },
                {
                    "sent": "Actually unlikely to recover local minimum.",
                    "label": 0
                },
                {
                    "sent": "That will be far away from the global minimum above the energy barrier.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also know from the deep learning something that there is a lot of subtle points in the optimization problem, and so actually the spincast theory is showing that the number of high index saddle points is actually exponentially large.",
                    "label": 0
                },
                {
                    "sent": "Those are lying above the energy barrier and they're much more of them than than local minima and low index critical points and two.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finished, I just want to say that if we can establish a stronger connection between most models, we can try to understand whether it's actually necessary in a deep learning setting to find the global minimizer, because the spin class theory is telling us precisely that recovering the global minimum takes exponentially long time.",
                    "label": 0
                },
                {
                    "sent": "It's in practice just infeasible.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to repeat, we encourage everybody to tackle the problem of establishing the stronger connection between both models.",
                    "label": 1
                },
                {
                    "sent": "And naturally we also work in this direction now.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}