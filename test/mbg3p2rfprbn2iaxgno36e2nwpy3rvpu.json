{
    "id": "mbg3p2rfprbn2iaxgno36e2nwpy3rvpu",
    "title": "Convergence rates of nested accelerated inexact proximal methods",
    "info": {
        "author": [
            "Silvia Villa, Istituto Italiano di Tecnologia"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_villa_methods/",
    "segmentation": [
        [
            "First of all, I want to thank my coauthors so various also look up all the sudden Alexander Barry.",
            "And the organizers of these interesting workshop."
        ],
        [
            "As you know, many problems as we have seen today, many problems machine learning lead to.",
            "The optimization of a structured objective function which can be written as the sum of a smooth term F."
        ],
        [
            "Having a Lipschitz continuous gradient."
        ],
        [
            "And then no smooth term G that I will assume having this form is the composition of Omega and linear function B between two Hilbert spaces."
        ],
        [
            "Recently proximal methods became quite popular and in this talk I'm going to focus on one instance of this class of methods that are that is accelerated forward backward splitting algorithm.",
            "As you can see, it's a splitting method.",
            "In fact, the contribution of this new term is here is in the gradient descent step and the contribution of the nonsmooth term is in this proxamol step, and it is an accelerated method since.",
            "XK plus one is computed starting from YK instead of the previous iterate and YK can be found as a linear and appropriate linear combination of the previous two iterates and these auxiliary sequence YK.",
            "Then yes, roving."
        ],
        [
            "1005 proof first proof that these kind of methods enjoy these convergence rate on the objective function and basically we can prove that F of XK minus the minimum is upper bounded by a constant over K ^2, so they are faster than the basic method that achieves only 1 / K convergence rate."
        ],
        [
            "At each step, if we want to implement these algorithms, we have to compute the proximal step, the approximate point of a given point Y.",
            "So we have to solve this strongly convex optimization problem in the simplest."
        ],
        [
            "This case if G, for instance is the indicator function of a closed and convex set.",
            "The proximal point.",
            "Coincide with the projection on this closed and convex set even in this."
        ],
        [
            "Simple case, then this approximate point cannot usually be computed exactly, but we can only expect to find a an approximation of it."
        ],
        [
            "So it is necessary.",
            "To give a convergence analysis for the accelerated algorithm where the exactness is taken into account.",
            "I will present so this is the focus of my talk and I will present these results in three steps.",
            "I will start from proving convergence rates for this exact implementation of the algorithm where I do not take into account the cost of computing the prox operator.",
            "Then I will focus on the definition of exactness that we need in order to prove these convergence theorem, and in particular I will show how these.",
            "Error concept that we use is in fact practically usable.",
            "And finally I will give convergence rates for a nested procedure, which takes into account the cost of computing the."
        ],
        [
            "At each step."
        ],
        [
            "So we start from the first point.",
            "So if the errors are of a specific specific type, I'm going to explain in a moment.",
            "If the decay sufficiently faster, as 1 / K to the cube, then the algorithm is convergent.",
            "If Q is greater than 1/2.",
            "But as you can see, the convergence rate depends on the Q and in fact, and in fact it is deteriorated if Q is not too big and can be worst than the basic.",
            "The one of the accelerated method, while if Q is bigger enough and two bigger than 3 / 2, then we get we recover.",
            "The convergence of the exact scheme.",
            "These results tells also us that it is not necessary to use Q bigger than three and half because it's a waste of time to compute approximations of the proxy or beyond this accuracy.",
            "I have to see that I have to say that more or less at the same time where we prove this result.",
            "Also this paper by each meet Lheureux and back appeared proving essentially the same theorem for a more general error concept, but showing a weaker dependence on the error decay."
        ],
        [
            "So now I'm going to explain why we think that the our error concept is useful and."
        ],
        [
            "Start from the definition.",
            "As you remember, the prox, the approximate point of Y with respect to the function G is the solution of that optimization problem and so we can write the 1st order condition and we obtain that that is the proxy of Y if and only if that condition is verified.",
            "So Y minus that over land that belonged to the subwoofer."
        ],
        [
            "Social and if we replace the subdifferential with the approximate subdifferential of the epsilon square over 2 Landes of differential G, we get the definition of errors that we consider.",
            "You may think that these condition is not easy to verify in practice, but I'm going to."
        ],
        [
            "Show that, at least for this class of functions, these conditions is indeed quite easily verify very easily verifiable.",
            "So in this case we have to minimize filanda in order to find the approximate point, and we can write the dual of these minimization problem which is this one and if strong duality old then we have the minimum of the duality gap that is found by summing up Philando Vax plus Silano V that you are in the primal is 0.",
            "And then."
        ],
        [
            "Is a classical theorem by Morrow.",
            "Relating the proxamol points with these duality theory.",
            "Proving that first first thing that we can compute the prox with respect to this function G at a given point Y.",
            "If we know the prox of the final duel and the 2nd at the gap evaluated on this pair of points is 0.",
            "What we proved."
        ],
        [
            "Is that we can.",
            "Have the same theorem where we replace the = with the approximation sign.",
            "So we can find approximations of the approximate with respect to G starting from approximation of approx.",
            "With respect to the star and more.",
            "Most important, we can characterize our approximations in terms of this duality gap.",
            "And as you know the quality gap can be computed because we know.",
            "We know how to compute it, so basically this gives us 2 information.",
            "First is a certificate of the level of approximation.",
            "And the second we can hope to find admissible approximation by minimizing the quality."
        ],
        [
            "And the result that we proved that in fact actually it is not necessary to minimize the duality gap, but it is enough to minimize the dual function, since if we have an algorithm minimizing the dual function in value.",
            "Then automatically we get an algorithm minimizing the duality gap as well.",
            "And also we have a convergence rate so we can pass from the convergence rate on the dual function to a convergence rate on the gap.",
            "So for instance, in many many cases the duality gap is computed.",
            "The approximate point is computed starting from the dual funk by minimizing the dual function.",
            "And for instance, if we use again a follow back code method on the dual, we get convergence rate 1 / sqrt N, while if we use an accelerated method we get convergence rate one over North.",
            "On the gap."
        ],
        [
            "So now combining these these results that I showed you with the first theorem we can get.",
            "Convergence rate for the nested procedure where we add to this inexact step, the loop computing the proxy."
        ],
        [
            "As you can see that the global complexity of the algorithm depends on two parameters, Q, which is the accuracy that we choose for computing the prox at each step.",
            "So it's the error decay and the efficiency of our solver.",
            "On the jewel."
        ],
        [
            "What we can prove is that the best choice that we can do is to try to choose Q close to three and ALP because the in this case the complexity.",
            "This is a lower bound bound for this complexity, so again, the theoretical result that we found at the beginning is confirmed.",
            "The best possible choices to stay close to three and half.",
            "And of course the complexity depends on the efficiency of the inner algorithm that we have and."
        ],
        [
            "For instance, we can also show that if P goes to Infinity, I assumed sublinear convergence for the algorithm.",
            "So for instance, if I have a linear convergent algorithm or simply goes to Infinity, we can recover the rate of an accelerated method.",
            "In the."
        ],
        [
            "Parents, I'm going to show you in a moment.",
            "We used to feast on the jewel so as a global convergence rate of the algorithm that you will see in the experiment we get the convergence rate of basically subgradient method.",
            "Right?"
        ],
        [
            "So do the experiments that we performed were on 2 examples.",
            "The first one is.",
            "OK, the blurring problem regularize with the total variation penalty.",
            "I showed in this table you can see we we we compared different algorithms starting from the time.",
            "Since these two iterations, the inner iteration and external iteration has two different computational costs and so reported both both the inauguration, the outer activations, and I what we compared is the accelerated method than an accelerated method and the benchmark 1st order method.",
            "That is, the primal dual method by Shamblin.",
            "Lock of 2011.",
            "And the only trick that we used in the implementation of the method is a warm restart procedure.",
            "So in order to compute the proxy at the step K plus one, we use as starting point the proxy that we computed at the step K, and this makes the difference because otherwise they accelerated forward backward with the exact proxy is not even comparable to the primal dual.",
            "As you can see, the primal dual is the fastest for low accuracies, but the accelerated follow backward become faster.",
            "Instead, for higher accuracies, and these experiments give also some insights on the choice of the error rate.",
            "Again, we can see also that in the experiments our theoretical results are more or less confirmed.",
            "the Q has to be closed the best Q that we found is close to the to the optimal one that was 1.5."
        ],
        [
            "And then we repeated the same experiment on a least squares regularised problem with an overlapping group last for penalty.",
            "Again.",
            "Also in this case the proxy is not available in closed form.",
            "And here the situation change as you can see and the accelerated method is the fast passes for lower accuracy, but then an accelerated method is in fact faster than the accelerated one.",
            "The reason of this is 1, the warm restart, the procedure and the fact that it is known that the North.",
            "The non accelerating method is much more stable with respect to errors and so takes advantage, takes more advantage of the warm start strategy."
        ],
        [
            "So finally I showed you a analysis of convergence for an inexact implementation of accelerated flow, backward method and accelerated.",
            "For backward algorithm I this.",
            "This has been done in three step.",
            "I characterized our notion of admissible errors by showing to you that admissible approximations are at least in some cases easy to find.",
            "And then I gave to you.",
            "Convergence rates for the exact procedure without taking into account the computational cost of computing the proxy, and finally also the one of computing the proxy and.",
            "But I.",
            "And at the end the numerical experiments.",
            "I hope that give you the feeling that the accelerated method is not always the best, and in particular if the proxy is not available in closed form, can be outperformed by the basic scheme.",
            "Let's"
        ],
        [
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, I want to thank my coauthors so various also look up all the sudden Alexander Barry.",
                    "label": 0
                },
                {
                    "sent": "And the organizers of these interesting workshop.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you know, many problems as we have seen today, many problems machine learning lead to.",
                    "label": 0
                },
                {
                    "sent": "The optimization of a structured objective function which can be written as the sum of a smooth term F.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having a Lipschitz continuous gradient.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then no smooth term G that I will assume having this form is the composition of Omega and linear function B between two Hilbert spaces.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently proximal methods became quite popular and in this talk I'm going to focus on one instance of this class of methods that are that is accelerated forward backward splitting algorithm.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it's a splitting method.",
                    "label": 0
                },
                {
                    "sent": "In fact, the contribution of this new term is here is in the gradient descent step and the contribution of the nonsmooth term is in this proxamol step, and it is an accelerated method since.",
                    "label": 0
                },
                {
                    "sent": "XK plus one is computed starting from YK instead of the previous iterate and YK can be found as a linear and appropriate linear combination of the previous two iterates and these auxiliary sequence YK.",
                    "label": 0
                },
                {
                    "sent": "Then yes, roving.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1005 proof first proof that these kind of methods enjoy these convergence rate on the objective function and basically we can prove that F of XK minus the minimum is upper bounded by a constant over K ^2, so they are faster than the basic method that achieves only 1 / K convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At each step, if we want to implement these algorithms, we have to compute the proximal step, the approximate point of a given point Y.",
                    "label": 0
                },
                {
                    "sent": "So we have to solve this strongly convex optimization problem in the simplest.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This case if G, for instance is the indicator function of a closed and convex set.",
                    "label": 0
                },
                {
                    "sent": "The proximal point.",
                    "label": 0
                },
                {
                    "sent": "Coincide with the projection on this closed and convex set even in this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple case, then this approximate point cannot usually be computed exactly, but we can only expect to find a an approximation of it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it is necessary.",
                    "label": 0
                },
                {
                    "sent": "To give a convergence analysis for the accelerated algorithm where the exactness is taken into account.",
                    "label": 0
                },
                {
                    "sent": "I will present so this is the focus of my talk and I will present these results in three steps.",
                    "label": 0
                },
                {
                    "sent": "I will start from proving convergence rates for this exact implementation of the algorithm where I do not take into account the cost of computing the prox operator.",
                    "label": 1
                },
                {
                    "sent": "Then I will focus on the definition of exactness that we need in order to prove these convergence theorem, and in particular I will show how these.",
                    "label": 0
                },
                {
                    "sent": "Error concept that we use is in fact practically usable.",
                    "label": 1
                },
                {
                    "sent": "And finally I will give convergence rates for a nested procedure, which takes into account the cost of computing the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At each step.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we start from the first point.",
                    "label": 0
                },
                {
                    "sent": "So if the errors are of a specific specific type, I'm going to explain in a moment.",
                    "label": 0
                },
                {
                    "sent": "If the decay sufficiently faster, as 1 / K to the cube, then the algorithm is convergent.",
                    "label": 0
                },
                {
                    "sent": "If Q is greater than 1/2.",
                    "label": 0
                },
                {
                    "sent": "But as you can see, the convergence rate depends on the Q and in fact, and in fact it is deteriorated if Q is not too big and can be worst than the basic.",
                    "label": 0
                },
                {
                    "sent": "The one of the accelerated method, while if Q is bigger enough and two bigger than 3 / 2, then we get we recover.",
                    "label": 0
                },
                {
                    "sent": "The convergence of the exact scheme.",
                    "label": 0
                },
                {
                    "sent": "These results tells also us that it is not necessary to use Q bigger than three and half because it's a waste of time to compute approximations of the proxy or beyond this accuracy.",
                    "label": 0
                },
                {
                    "sent": "I have to see that I have to say that more or less at the same time where we prove this result.",
                    "label": 0
                },
                {
                    "sent": "Also this paper by each meet Lheureux and back appeared proving essentially the same theorem for a more general error concept, but showing a weaker dependence on the error decay.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to explain why we think that the our error concept is useful and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start from the definition.",
                    "label": 0
                },
                {
                    "sent": "As you remember, the prox, the approximate point of Y with respect to the function G is the solution of that optimization problem and so we can write the 1st order condition and we obtain that that is the proxy of Y if and only if that condition is verified.",
                    "label": 0
                },
                {
                    "sent": "So Y minus that over land that belonged to the subwoofer.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Social and if we replace the subdifferential with the approximate subdifferential of the epsilon square over 2 Landes of differential G, we get the definition of errors that we consider.",
                    "label": 0
                },
                {
                    "sent": "You may think that these condition is not easy to verify in practice, but I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show that, at least for this class of functions, these conditions is indeed quite easily verify very easily verifiable.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have to minimize filanda in order to find the approximate point, and we can write the dual of these minimization problem which is this one and if strong duality old then we have the minimum of the duality gap that is found by summing up Philando Vax plus Silano V that you are in the primal is 0.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a classical theorem by Morrow.",
                    "label": 0
                },
                {
                    "sent": "Relating the proxamol points with these duality theory.",
                    "label": 0
                },
                {
                    "sent": "Proving that first first thing that we can compute the prox with respect to this function G at a given point Y.",
                    "label": 0
                },
                {
                    "sent": "If we know the prox of the final duel and the 2nd at the gap evaluated on this pair of points is 0.",
                    "label": 0
                },
                {
                    "sent": "What we proved.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that we can.",
                    "label": 0
                },
                {
                    "sent": "Have the same theorem where we replace the = with the approximation sign.",
                    "label": 0
                },
                {
                    "sent": "So we can find approximations of the approximate with respect to G starting from approximation of approx.",
                    "label": 0
                },
                {
                    "sent": "With respect to the star and more.",
                    "label": 0
                },
                {
                    "sent": "Most important, we can characterize our approximations in terms of this duality gap.",
                    "label": 0
                },
                {
                    "sent": "And as you know the quality gap can be computed because we know.",
                    "label": 0
                },
                {
                    "sent": "We know how to compute it, so basically this gives us 2 information.",
                    "label": 0
                },
                {
                    "sent": "First is a certificate of the level of approximation.",
                    "label": 0
                },
                {
                    "sent": "And the second we can hope to find admissible approximation by minimizing the quality.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the result that we proved that in fact actually it is not necessary to minimize the duality gap, but it is enough to minimize the dual function, since if we have an algorithm minimizing the dual function in value.",
                    "label": 0
                },
                {
                    "sent": "Then automatically we get an algorithm minimizing the duality gap as well.",
                    "label": 1
                },
                {
                    "sent": "And also we have a convergence rate so we can pass from the convergence rate on the dual function to a convergence rate on the gap.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in many many cases the duality gap is computed.",
                    "label": 0
                },
                {
                    "sent": "The approximate point is computed starting from the dual funk by minimizing the dual function.",
                    "label": 0
                },
                {
                    "sent": "And for instance, if we use again a follow back code method on the dual, we get convergence rate 1 / sqrt N, while if we use an accelerated method we get convergence rate one over North.",
                    "label": 0
                },
                {
                    "sent": "On the gap.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now combining these these results that I showed you with the first theorem we can get.",
                    "label": 0
                },
                {
                    "sent": "Convergence rate for the nested procedure where we add to this inexact step, the loop computing the proxy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you can see that the global complexity of the algorithm depends on two parameters, Q, which is the accuracy that we choose for computing the prox at each step.",
                    "label": 0
                },
                {
                    "sent": "So it's the error decay and the efficiency of our solver.",
                    "label": 0
                },
                {
                    "sent": "On the jewel.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we can prove is that the best choice that we can do is to try to choose Q close to three and ALP because the in this case the complexity.",
                    "label": 0
                },
                {
                    "sent": "This is a lower bound bound for this complexity, so again, the theoretical result that we found at the beginning is confirmed.",
                    "label": 0
                },
                {
                    "sent": "The best possible choices to stay close to three and half.",
                    "label": 0
                },
                {
                    "sent": "And of course the complexity depends on the efficiency of the inner algorithm that we have and.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance, we can also show that if P goes to Infinity, I assumed sublinear convergence for the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if I have a linear convergent algorithm or simply goes to Infinity, we can recover the rate of an accelerated method.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parents, I'm going to show you in a moment.",
                    "label": 0
                },
                {
                    "sent": "We used to feast on the jewel so as a global convergence rate of the algorithm that you will see in the experiment we get the convergence rate of basically subgradient method.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do the experiments that we performed were on 2 examples.",
                    "label": 0
                },
                {
                    "sent": "The first one is.",
                    "label": 0
                },
                {
                    "sent": "OK, the blurring problem regularize with the total variation penalty.",
                    "label": 0
                },
                {
                    "sent": "I showed in this table you can see we we we compared different algorithms starting from the time.",
                    "label": 0
                },
                {
                    "sent": "Since these two iterations, the inner iteration and external iteration has two different computational costs and so reported both both the inauguration, the outer activations, and I what we compared is the accelerated method than an accelerated method and the benchmark 1st order method.",
                    "label": 0
                },
                {
                    "sent": "That is, the primal dual method by Shamblin.",
                    "label": 0
                },
                {
                    "sent": "Lock of 2011.",
                    "label": 0
                },
                {
                    "sent": "And the only trick that we used in the implementation of the method is a warm restart procedure.",
                    "label": 0
                },
                {
                    "sent": "So in order to compute the proxy at the step K plus one, we use as starting point the proxy that we computed at the step K, and this makes the difference because otherwise they accelerated forward backward with the exact proxy is not even comparable to the primal dual.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the primal dual is the fastest for low accuracies, but the accelerated follow backward become faster.",
                    "label": 0
                },
                {
                    "sent": "Instead, for higher accuracies, and these experiments give also some insights on the choice of the error rate.",
                    "label": 0
                },
                {
                    "sent": "Again, we can see also that in the experiments our theoretical results are more or less confirmed.",
                    "label": 0
                },
                {
                    "sent": "the Q has to be closed the best Q that we found is close to the to the optimal one that was 1.5.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we repeated the same experiment on a least squares regularised problem with an overlapping group last for penalty.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "Also in this case the proxy is not available in closed form.",
                    "label": 0
                },
                {
                    "sent": "And here the situation change as you can see and the accelerated method is the fast passes for lower accuracy, but then an accelerated method is in fact faster than the accelerated one.",
                    "label": 0
                },
                {
                    "sent": "The reason of this is 1, the warm restart, the procedure and the fact that it is known that the North.",
                    "label": 0
                },
                {
                    "sent": "The non accelerating method is much more stable with respect to errors and so takes advantage, takes more advantage of the warm start strategy.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally I showed you a analysis of convergence for an inexact implementation of accelerated flow, backward method and accelerated.",
                    "label": 0
                },
                {
                    "sent": "For backward algorithm I this.",
                    "label": 0
                },
                {
                    "sent": "This has been done in three step.",
                    "label": 0
                },
                {
                    "sent": "I characterized our notion of admissible errors by showing to you that admissible approximations are at least in some cases easy to find.",
                    "label": 0
                },
                {
                    "sent": "And then I gave to you.",
                    "label": 0
                },
                {
                    "sent": "Convergence rates for the exact procedure without taking into account the computational cost of computing the proxy, and finally also the one of computing the proxy and.",
                    "label": 0
                },
                {
                    "sent": "But I.",
                    "label": 0
                },
                {
                    "sent": "And at the end the numerical experiments.",
                    "label": 0
                },
                {
                    "sent": "I hope that give you the feeling that the accelerated method is not always the best, and in particular if the proxy is not available in closed form, can be outperformed by the basic scheme.",
                    "label": 0
                },
                {
                    "sent": "Let's",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}