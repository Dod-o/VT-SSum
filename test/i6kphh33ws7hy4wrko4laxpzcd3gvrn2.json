{
    "id": "i6kphh33ws7hy4wrko4laxpzcd3gvrn2",
    "title": "Structured Low-Rank Approximation as Optimization on a Grassmann Manifold",
    "info": {
        "author": [
            "Konstantin Usevich, Department ELEC, Vrije Universiteit Brussel"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_usevich_grassmann_manifold/",
    "segmentation": [
        [
            "The topic of my talk today will be more about structure to rank approximation then optimization on Grassmann manifold.",
            "As you could see in abstract.",
            "So in the abstract it's more about optimization across manifold.",
            "So here I I continue the tradition of slightly changing the topic of talks.",
            "How enter this is Janet work with Markowski.",
            "We both together in free University Brussels so."
        ],
        [
            "What what the problem do we can see there?",
            "Suppose we have a structured matrix and we would like to approximate it by a low ranks matrix which is also structured.",
            "So what is?",
            "What is a structured matrix?",
            "I'll give precise definition later, but for now you may assume that you have some repeating patterns of elements here and you want to preserve it in your rank approximation.",
            "So we won't do we consider this problem basically because in many data modeling problems where you want to model your data with some low complexity model, you can reformulate this problem as a structure to rank approximation, where this complexity is encoded by rank.",
            "So what can we say about this problem?",
            "It's it's a nonconvex problem and accept of several solutions.",
            "We don't know how to solve it.",
            "People don't know how to solve it globally.",
            "For example, if it's.",
            "There is no structure and it's for being a snoring.",
            "You can solve it by translate some kids SVG.",
            "You will know it, and even if you add structure or even if you change the approximation criterion then problem becomes difficult and some people in this audience proved that if it's just unstructured and approximation criterion is different then it's NP hard problem.",
            "So but we still try to solve it.",
            "So this talk is also structured.",
            "This structure is very simple.",
            "First I will talk about structure to rank approximation and we try to give a lot of examples for you so.",
            "Next time we talk about this connection between translation across from one photon.",
            "In this sense, I will try to simplify the problem.",
            "Of course, for you yet missing not a simplification but a complication, but I will try to convince you that grassman manifold is not as scary as it sounds.",
            "So when I first heard this term eight years ago, I was really scared.",
            "And but when you need to optimize our over it.",
            "We can live with it."
        ],
        [
            "So let's start with Hunter matrices.",
            "Hunter matrices are maybe simplest case of structure, but in my talk it's very important 'cause I want to on Hunter meters is I want to make a connection between structure, organic approximation and sparse approximation.",
            "So what is Hunter structure?",
            "It's a Hunter matrix, it's a matrix where you have the same values onto outer diagonals and it can be indexed from left upper corner to right bottom corner.",
            "And you typically associated it with univariate time series, so you have all you can.",
            "Imagine that control matrix is constructed by taking some vectors of the original time series.",
            "So where is there is well known theorem of heineck that if the rank of Hunter matrix is bounded by some number which is less than the number of calls, so the matrix is rank deficient.",
            "So this happens if and only if the coefficients of times here is it is 5 what is called linear recurrence or different situations?",
            "There are different names for that, so you have constant coefficients and you have this linear recurrence which codes for all possible indices.",
            "So this theorem is said about if you are is equal to N -- 1.",
            "So this just means in this case that you have at least one vector, one non zero vector before this coefficients are non 010 vector in the left kernel and this is precisely this.",
            "So it's a little bit more involved if you want to prove it for for reaching R. But anyway thanks to this theorem we know how to run control matrices look like."
        ],
        [
            "Haven't done because we know what are the solution of linear recurrences.",
            "You you can remember it from the course of different differential equations, so it's like ordinary differential equations with constant coefficients.",
            "So you the solutions possible solutions of the linear currents are the time series of this form, where you have some of products of polynomials multiplied by exponential functions.",
            "Where are these exponents and exponential functions are precisely the roots of what is called characteristic polynomial.",
            "Of this linear recurrence and the degree of the polynomial is here corresponds to multiplicity of this root minus one.",
            "And because most of the time we have simple rules, then it will be a constant here, so it's a sum of complex.",
            "It will be a sum of complex exponentials here, so."
        ],
        [
            "I will show you some examples, so these are this time series which are products of polynomials and exponentials and we played some and this corresponds to linear linear recurrence, so I will provide example for real time series because it's the most.",
            "Thank you, I'm sorry if I'm if I'm codependent, but I think that it's very important.",
            "So for an exponential time series, you have that the former is here and it's satisfies a linear recurrence of order one and the corresponding characteristic polynomial is this.",
            "It's just one degree polynomial, so if you have four dump sign, so you have four exponentially dump dump percent and it has this for this formula and it's widely used in signal processing this.",
            "In this case, the correct spelling looks like this and that roots.",
            "It has two complex conjugate roots and also for example is of polynomial where we have polynomial here and this here is just one, so it it has order of linear recurrence 4.",
            "This is polynomial of degree 3 and it corresponds to multiple route one with multiplicity four.",
            "And here as we see from if you remember that this linear recurrence corresponds to.",
            "Alright come kilometers is and therefore I want to stress it that it's very, very interested on both legs elevation that all rank approximation is in fact equivalent to sparse approximation of this function.",
            "So this time series this is a function with this basis where the basis is monomial is multiplied by exponents and this basis has infinite number of elements becausw its ponent can be can be anything.",
            "So why do we can see?",
            "Why do we want to consider this costs this class of basis functions so."
        ],
        [
            "Supposed to have these are the functions from this console.",
            "You have exponent, you have exponentially modulated cosine, you have another exponentially motivated design which increases.",
            "You have signed.",
            "This is also exponentially moderated.",
            "Can sign this is sum sum of two and if you sum all these time series together and yet the little bit notes on top of it, you'll get you'll get a time series.",
            "So this is the time series of some.",
            "45 Australian wine sales.",
            "So of course in in your life you want to solve.",
            "This problem for a given time serious.",
            "You want to decompose it to to sum of functions.",
            "Functions like this and so this, for example will represent the general tendency or trend.",
            "This will correspond to some periodicity, some seasonality, components and.",
            "If you have not nice here.",
            "So this problem appears not only actually in many places in time series analysis, if you want to model impulse response, maybe it's more familiar for for some of you, or in signal processing this sum of dumped exponentials is quite quite a common model, and so this is a disconnection between 100 matrices and functions like this sparse approximation in this basis is well known and people use it.",
            "Quite a lot.",
            "But now I."
        ],
        [
            "Let me let me switch to something more complicated and more nonstandard, so this is.",
            "This is also important because this will be exactly the example where we test optimization on a Craftsman manifold.",
            "So suppose that we do not have a univariate time series, but on the other hand we have a Q valid time series.",
            "So each element of the time series is Q by one vector, and in this case we construct not hunting matrix.",
            "But I work on traumatic, so each element here is.",
            "Q By one vote.",
            "In this case there on the number of rows, but it will be like this.",
            "So this is the number of vocals and this is the dimension of the book.",
            "And if you if the matrix is rank deficient with this index of rank deficiency, this means that there is a unrelated matrix similar to what was in the case of univariate time series and complemented which emulates this until matrix.",
            "And this means that.",
            "Your time series satisfies a vector linear recurrence with matrix coefficients here, and sizes of matrix coefficients RP by here.",
            "So peace is round efficiency here and it was shown it was shown by.",
            "I'm interested in getting Williams that this vector.",
            "Vector linear recurrence with matrix coefficients exactly correspond to dynamical systems to trajectory's of dynamical systems with bounded complexity.",
            "Appointed number of locks and this number of inputs and the interesting feature here is that the inputs and outputs of the system can be permitted in this vector.",
            "So this means that by doing a low rank approximation of both counter matrices we do system identification from given data.",
            "Help.",
            "So it's so so this is a connection to systems identification.",
            "Of course, this class is linear time invariant dynamical systems.",
            "So the first exam."
        ],
        [
            "So you you could have seen it yesterday on a poster by Maria our colleague.",
            "So suppose that you have two polynomials, two univariate polynomials, and you construct the Sylvester matrix from it.",
            "It is well known fact in algebra then degree they have a common divisor of degree at least D if and only if this Sylvester matrix is running efficient with this rank deficiency D. So this basically means that walk around proximation of our Sylvester matrix is equivalent to finding forgiven two polynomials that causes two polynomials which have a common divisor of at least of given degree.",
            "So this is a little bit.",
            "This may be a little bit strange that you measure distance between polynomials, not in the sense of their roots, but in the sense of their coefficients.",
            "But basically this one of the one of the important problems in the field of computer algebra and people in this community really care about this problem, and there's a lot of papers on these.",
            "Structure approximation."
        ],
        [
            "Even more, you may wish to compute approximate GCD of several several univariate polynomials, or you may wish to compute approximate GCD of multivariate polynomials and or some kind of approximate groebner basis, because life is more various in multivariate case and deferred third example for final example is about multi level 100 matrices, so this this structure appears if you.",
            "Process and dimensional arrays.",
            "For example.",
            "Multi level low control matrices are comfortable.",
            "Control matrices are used in processing performer I imagine or in texture analysis and put what was recently approved that also symmetric tensor decomposition is equivalent to a structure or an proximation with.",
            "With multi level Hunter structure so it means that you have fun chilled until books and each focus also punctual and you can have infinite amount of books not not infinite of course fine.",
            "So these are I will not talk about this examples, but this is just to give you an idea of what kind of problems appear in the framework of or approximation."
        ],
        [
            "OK, so let me now formulate the problem as we as we consider it so we can see there.",
            "Structure which can be defined by a linear map from some parameter space to the space of N by N matrices.",
            "So image of this map will be a linear subspace and this will go linear matrix structure.",
            "So all of the examples which represented they fit fit into this.",
            "Framework and the structure to rank approximation.",
            "Here we formulate this whole so given given structure given some approximation criterion and given the E which corresponds to original structure until matrix which we acquired from somewhere.",
            "We want to find the closest vector B had such that the corresponding structure matrix is of no rank of rank less than than R. So as you may have noticed here here.",
            "It's the difference between parameter vectors, but you can show for linear Maps that it is just equivalent distance between parameter vectors and distance between structured matrices.",
            "We solve, we solve this problem and.",
            "How do we, what kind of approximation criterion?",
            "We concede consider we can see their weighted Euclidean seem seminar and this is a weighted sum of squares of elements of vector and we also allow whites seminorm because we allow zeros and infinities.",
            "Here we found this very, very convenient.",
            "So you can have 0 send Infinity.",
            "So what does it mean?",
            "Continue to here means that the this approximately this distance is.",
            "Is infinite if some some constraints are not satisfied.",
            "For this this way.",
            "So this means that if we have infinite weights, it means a constraint or some fixed values on the parameter vector.",
            "On the other hand, if we have zero wait.",
            "It means that in this approximation criterion, the elements really do not matter the original and approximating picky, so this corresponds to the case of missing values.",
            "Just just to give an example."
        ],
        [
            "Where this fixed on mission failures appear.",
            "So for example, Sylvester Matrix, which you have seen previously.",
            "If you flip it accounts then you have 200 meters is with fixed zeros.",
            "The case of missing Fitness is more interesting.",
            "I think you have already seen a lot of these matrices in the sense of approx matrix completion, so you have you have some matrix and you want to complete it such that the completed matrix is of all ranks.",
            "You can also do approximate matrix completion when you also want to divide, divide little bit from this entries so.",
            "But more interesting example, when you have both structure and missing values, this this more interesting case because this can be used in system identification with missing data.",
            "The problem is that if you have dynamical systems, your data is independent across time, and if if you miss some database may present a complication for your identification algorithm.",
            "Another example here is some kind of.",
            "Suggest example data driven control so you have no input, no output and you want to produce an input which tracks this this output."
        ],
        [
            "OK, so we have this problem and then we want to represent it.",
            "Tries it to finally come to the topic of my talk to optimization across manifold.",
            "So the general idea how do we do it?",
            "So this optimization problem we introduce some additional variable R that sensation variable here is behind and then after that we eliminate the variable behave.",
            "This idea, so how we do it?",
            "We recognize that there aren't constraint can be represented in kernel form.",
            "So you want known about image form drunken state of a matrix means that the matrix is product of two matrices, one of its thin and other is fed.",
            "But here we use a kernel form.",
            "So it means that rank is less than or equal to R if and only if there is exist.",
            "The Matrix D by MM is the number of rows here.",
            "And this is equal to M -- R, which is a four oh run and which annihilates this matrix.",
            "So this D is equal to current at least that you want to achieve.",
            "This means that there should exist full rank matrix in the kernel.",
            "This matrix such that it annihilates it and then we can reformulate our original problem.",
            "Here we can reformulate it as a double minimization problem.",
            "So I will try to explain it, so we have automation and we have innovation on the altar minimization level.",
            "We deal with some black box function F of R which we optimize over the set of full rank matrices.",
            "This will be exactly the optimization across manifold and on the inner level we have forgiven our find the best approximation vector which has this matrix in its kernel.",
            "So this as you can see from here it's it's linear.",
            "It's linear problem and it has an analytics solution.",
            "So by doing this we can do only with this optimization on the outer level, so we can deal with optimization of a black box function here and this.",
            "We know we know how to evaluate and what's the advantage here.",
            "Originally we have NP optimization variable and here we come to Z -- M optimization variables.",
            "So we reduced.",
            "The search space in this way, and typically we consider fair structured matrices, which perfectly makes sense."
        ],
        [
            "So this is a bit technical side here.",
            "You can skip it, but this is just while this is a linear problem, it's generalized list no problem, it has has an analytic solution, so this terms here comes from the missing data and it appears here for this matrix be, but.",
            "In the case where we do not have missing data in the case when we have only positive positive ways, so we also we also prove that this function is.",
            "So this function basically just to give you an idea how it looks like.",
            "It's a rational function, right?",
            "It's function, derivative and hash and can be evaluated in this number of hops, and as you remember, we have typically a fat matrix, so so this corresponds.",
            "To the number of our data points originally, so you have a linear complexity of iteration in in this case, so this may.",
            "May fall in the classification of your investor of tool or optimization."
        ],
        [
            "OK, So what happens on the outer level?",
            "So we have this bugs box function which we can evaluate which we can evaluate derivatives with second order derivatives.",
            "But we can know that it depends only on the row space of this matrix.",
            "It's by construction, so it depends only on the subspace which is funded by this rose.",
            "And this means that the function is equal if those funds are equal.",
            "So basically it's an optimization on a Grassmann manifold and.",
            "Custom manifold, what is it you have devy M4 around matrices and these are just dimensional subspaces of RM.",
            "So just this site will be."
        ],
        [
            "So how how can we do it?",
            "How do we optimize so the simplest idea that you can come up with?",
            "You can use.",
            "You can parameterise the dimensional subspaces by orthonormal basis.",
            "So we have this constraint and in the case of D equals to one, you have one dimensional subspaces.",
            "These are lines passing through origin and this corresponds to optimization of the sphere becausw as you remember the cost function.",
            "Here is constant on this subspace is just the same for 2 two points which represent the same subspace.",
            "So you can use here penalty method which becomes an exact method here.",
            "But I will not."
        ],
        [
            "Even in details here and actually optimization on the grassman manifold was quite a hot topic recently, and there is a group of people who are working on it, so this is the basic reference.",
            "This book and they want to consider it in the framework of Riemann manifolds, so.",
            "Rigorous framework, so basically I will call this methods.",
            "Attraction based methods.",
            "So you have some point on the manifold.",
            "You do some steps in tangent space and then you project back on the manifold because if you have a manifold you do you.",
            "If you get derivatives they're not in the in the original space, so this corresponds here that you have you have a point on a sphere you have a tangent point to it and you make a direction and then you project it back to the sphere by some some protection.",
            "And you can generalize a lot of methods from from ordinary optimization on Euclidean space, and we can come up with many different there is.",
            "There is a lot of papers.",
            "There is a lot of methods, but we want to.",
            "Finally, account to the."
        ],
        [
            "I mean part of my top maybe about optimization across my phone.",
            "We wanted to do something simpler.",
            "We didn't want to deal with this projections and reductions, so we wanted to formulate the problem as optimization on Euclidean space.",
            "So how can we do it?",
            "This is so.",
            "This is basically mean results if you if you want.",
            "So if you have a full rank matrix here, divide N. This means that there exists G linearly independent code codes.",
            "Here I will denoted by orange orange homes.",
            "So this means that for any for the subspace which corresponds to this R, there exists such X and there exists a permutation matrix P such that this subspace is represented by this matrix.",
            "So, so it's quite easy to see you know how to get the matrix of this form.",
            "From here you multiply it from the right by this sub metric from the left by inverse of this sub matrix and you get you get this representation after you permute this rose here.",
            "And then therefore you can minimize.",
            "This functions over all possible all possible permutations, so this basically corresponds to optimization over affine hyperplanes.",
            "Here and."
        ],
        [
            "And the problem is here that this optimization is unbounded and you have a lot of permutation matrices.",
            "So how to deal with unbounded?"
        ],
        [
            "Case there is a very nice result of move, which means that we can always find representation of this form with various bounded by absolute value by one.",
            "So in this case it corresponds that affine hyperplanes cut out of heap hypercube and for any subspace you can find a point on this hypercube.",
            "So.",
            "It's amazing that this holds only not, for example, with Z equals to one, but for Gen Z."
        ],
        [
            "And we propose how to deal with this.",
            "You have full control, number of possibilities and you can do basically local optimization of these functions for a fixed permutation.",
            "And you do work optimization of this function until it converges or it exceeds this face of the cube.",
            "And then you can switch a permutation so so it's not really an optimization algorithm, it's a way you can approach the problem.",
            "So if you have optimization on the grassman manifold, then you can turn it to optimization on Euclidean space."
        ],
        [
            "So we we just so it's it's really easy to implement the algorithm with that was on the previous slide, and it does quite well, so it doesn't use this specialized remaining geometry.",
            "So it's under the advantage here that you can use any optimization method for the subproblem on Euclidean space."
        ],
        [
            "So I'm unfortunately I don't have time to explain it, so here we have some conclusions.",
            "So these methods we do not try to compete with them because it's just a simple idea and is basically idea of North which was from 30 years ago.",
            "But here this is more adapted to local geometry, but for every optimization method you need to modify your optimization method to use this retractions and you don't really know much about properties of it but here.",
            "But doing this certification often faces of the cube.",
            "We can use any optimization method here, and this is this nice problem because it's on about it subset.",
            "But unfortunately there is no bumps on the number of switches here, so if you exceed the phase you need to switch to another hyperplane.",
            "But in practice it's very low.",
            "It's usually one or two switches, so so this is basically the main the main idea."
        ],
        [
            "And these are some references about optimization on the grassman manifold.",
            "These are papers about structure, torrent approximation, this valuable software with not opened our interfaces, and these experiments are also available online here, so."
        ],
        [
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The topic of my talk today will be more about structure to rank approximation then optimization on Grassmann manifold.",
                    "label": 1
                },
                {
                    "sent": "As you could see in abstract.",
                    "label": 0
                },
                {
                    "sent": "So in the abstract it's more about optimization across manifold.",
                    "label": 0
                },
                {
                    "sent": "So here I I continue the tradition of slightly changing the topic of talks.",
                    "label": 0
                },
                {
                    "sent": "How enter this is Janet work with Markowski.",
                    "label": 0
                },
                {
                    "sent": "We both together in free University Brussels so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What what the problem do we can see there?",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a structured matrix and we would like to approximate it by a low ranks matrix which is also structured.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                },
                {
                    "sent": "What is a structured matrix?",
                    "label": 0
                },
                {
                    "sent": "I'll give precise definition later, but for now you may assume that you have some repeating patterns of elements here and you want to preserve it in your rank approximation.",
                    "label": 0
                },
                {
                    "sent": "So we won't do we consider this problem basically because in many data modeling problems where you want to model your data with some low complexity model, you can reformulate this problem as a structure to rank approximation, where this complexity is encoded by rank.",
                    "label": 0
                },
                {
                    "sent": "So what can we say about this problem?",
                    "label": 0
                },
                {
                    "sent": "It's it's a nonconvex problem and accept of several solutions.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to solve it.",
                    "label": 0
                },
                {
                    "sent": "People don't know how to solve it globally.",
                    "label": 0
                },
                {
                    "sent": "For example, if it's.",
                    "label": 0
                },
                {
                    "sent": "There is no structure and it's for being a snoring.",
                    "label": 0
                },
                {
                    "sent": "You can solve it by translate some kids SVG.",
                    "label": 0
                },
                {
                    "sent": "You will know it, and even if you add structure or even if you change the approximation criterion then problem becomes difficult and some people in this audience proved that if it's just unstructured and approximation criterion is different then it's NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "So but we still try to solve it.",
                    "label": 0
                },
                {
                    "sent": "So this talk is also structured.",
                    "label": 0
                },
                {
                    "sent": "This structure is very simple.",
                    "label": 0
                },
                {
                    "sent": "First I will talk about structure to rank approximation and we try to give a lot of examples for you so.",
                    "label": 0
                },
                {
                    "sent": "Next time we talk about this connection between translation across from one photon.",
                    "label": 0
                },
                {
                    "sent": "In this sense, I will try to simplify the problem.",
                    "label": 0
                },
                {
                    "sent": "Of course, for you yet missing not a simplification but a complication, but I will try to convince you that grassman manifold is not as scary as it sounds.",
                    "label": 0
                },
                {
                    "sent": "So when I first heard this term eight years ago, I was really scared.",
                    "label": 0
                },
                {
                    "sent": "And but when you need to optimize our over it.",
                    "label": 0
                },
                {
                    "sent": "We can live with it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with Hunter matrices.",
                    "label": 0
                },
                {
                    "sent": "Hunter matrices are maybe simplest case of structure, but in my talk it's very important 'cause I want to on Hunter meters is I want to make a connection between structure, organic approximation and sparse approximation.",
                    "label": 0
                },
                {
                    "sent": "So what is Hunter structure?",
                    "label": 0
                },
                {
                    "sent": "It's a Hunter matrix, it's a matrix where you have the same values onto outer diagonals and it can be indexed from left upper corner to right bottom corner.",
                    "label": 0
                },
                {
                    "sent": "And you typically associated it with univariate time series, so you have all you can.",
                    "label": 1
                },
                {
                    "sent": "Imagine that control matrix is constructed by taking some vectors of the original time series.",
                    "label": 0
                },
                {
                    "sent": "So where is there is well known theorem of heineck that if the rank of Hunter matrix is bounded by some number which is less than the number of calls, so the matrix is rank deficient.",
                    "label": 0
                },
                {
                    "sent": "So this happens if and only if the coefficients of times here is it is 5 what is called linear recurrence or different situations?",
                    "label": 1
                },
                {
                    "sent": "There are different names for that, so you have constant coefficients and you have this linear recurrence which codes for all possible indices.",
                    "label": 0
                },
                {
                    "sent": "So this theorem is said about if you are is equal to N -- 1.",
                    "label": 0
                },
                {
                    "sent": "So this just means in this case that you have at least one vector, one non zero vector before this coefficients are non 010 vector in the left kernel and this is precisely this.",
                    "label": 0
                },
                {
                    "sent": "So it's a little bit more involved if you want to prove it for for reaching R. But anyway thanks to this theorem we know how to run control matrices look like.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Haven't done because we know what are the solution of linear recurrences.",
                    "label": 0
                },
                {
                    "sent": "You you can remember it from the course of different differential equations, so it's like ordinary differential equations with constant coefficients.",
                    "label": 0
                },
                {
                    "sent": "So you the solutions possible solutions of the linear currents are the time series of this form, where you have some of products of polynomials multiplied by exponential functions.",
                    "label": 0
                },
                {
                    "sent": "Where are these exponents and exponential functions are precisely the roots of what is called characteristic polynomial.",
                    "label": 0
                },
                {
                    "sent": "Of this linear recurrence and the degree of the polynomial is here corresponds to multiplicity of this root minus one.",
                    "label": 0
                },
                {
                    "sent": "And because most of the time we have simple rules, then it will be a constant here, so it's a sum of complex.",
                    "label": 0
                },
                {
                    "sent": "It will be a sum of complex exponentials here, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will show you some examples, so these are this time series which are products of polynomials and exponentials and we played some and this corresponds to linear linear recurrence, so I will provide example for real time series because it's the most.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I'm sorry if I'm if I'm codependent, but I think that it's very important.",
                    "label": 0
                },
                {
                    "sent": "So for an exponential time series, you have that the former is here and it's satisfies a linear recurrence of order one and the corresponding characteristic polynomial is this.",
                    "label": 0
                },
                {
                    "sent": "It's just one degree polynomial, so if you have four dump sign, so you have four exponentially dump dump percent and it has this for this formula and it's widely used in signal processing this.",
                    "label": 0
                },
                {
                    "sent": "In this case, the correct spelling looks like this and that roots.",
                    "label": 0
                },
                {
                    "sent": "It has two complex conjugate roots and also for example is of polynomial where we have polynomial here and this here is just one, so it it has order of linear recurrence 4.",
                    "label": 0
                },
                {
                    "sent": "This is polynomial of degree 3 and it corresponds to multiple route one with multiplicity four.",
                    "label": 0
                },
                {
                    "sent": "And here as we see from if you remember that this linear recurrence corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Alright come kilometers is and therefore I want to stress it that it's very, very interested on both legs elevation that all rank approximation is in fact equivalent to sparse approximation of this function.",
                    "label": 0
                },
                {
                    "sent": "So this time series this is a function with this basis where the basis is monomial is multiplied by exponents and this basis has infinite number of elements becausw its ponent can be can be anything.",
                    "label": 0
                },
                {
                    "sent": "So why do we can see?",
                    "label": 0
                },
                {
                    "sent": "Why do we want to consider this costs this class of basis functions so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supposed to have these are the functions from this console.",
                    "label": 0
                },
                {
                    "sent": "You have exponent, you have exponentially modulated cosine, you have another exponentially motivated design which increases.",
                    "label": 0
                },
                {
                    "sent": "You have signed.",
                    "label": 0
                },
                {
                    "sent": "This is also exponentially moderated.",
                    "label": 0
                },
                {
                    "sent": "Can sign this is sum sum of two and if you sum all these time series together and yet the little bit notes on top of it, you'll get you'll get a time series.",
                    "label": 0
                },
                {
                    "sent": "So this is the time series of some.",
                    "label": 0
                },
                {
                    "sent": "45 Australian wine sales.",
                    "label": 0
                },
                {
                    "sent": "So of course in in your life you want to solve.",
                    "label": 0
                },
                {
                    "sent": "This problem for a given time serious.",
                    "label": 0
                },
                {
                    "sent": "You want to decompose it to to sum of functions.",
                    "label": 0
                },
                {
                    "sent": "Functions like this and so this, for example will represent the general tendency or trend.",
                    "label": 0
                },
                {
                    "sent": "This will correspond to some periodicity, some seasonality, components and.",
                    "label": 0
                },
                {
                    "sent": "If you have not nice here.",
                    "label": 0
                },
                {
                    "sent": "So this problem appears not only actually in many places in time series analysis, if you want to model impulse response, maybe it's more familiar for for some of you, or in signal processing this sum of dumped exponentials is quite quite a common model, and so this is a disconnection between 100 matrices and functions like this sparse approximation in this basis is well known and people use it.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot.",
                    "label": 0
                },
                {
                    "sent": "But now I.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me let me switch to something more complicated and more nonstandard, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is also important because this will be exactly the example where we test optimization on a Craftsman manifold.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we do not have a univariate time series, but on the other hand we have a Q valid time series.",
                    "label": 0
                },
                {
                    "sent": "So each element of the time series is Q by one vector, and in this case we construct not hunting matrix.",
                    "label": 0
                },
                {
                    "sent": "But I work on traumatic, so each element here is.",
                    "label": 0
                },
                {
                    "sent": "Q By one vote.",
                    "label": 0
                },
                {
                    "sent": "In this case there on the number of rows, but it will be like this.",
                    "label": 1
                },
                {
                    "sent": "So this is the number of vocals and this is the dimension of the book.",
                    "label": 0
                },
                {
                    "sent": "And if you if the matrix is rank deficient with this index of rank deficiency, this means that there is a unrelated matrix similar to what was in the case of univariate time series and complemented which emulates this until matrix.",
                    "label": 0
                },
                {
                    "sent": "And this means that.",
                    "label": 1
                },
                {
                    "sent": "Your time series satisfies a vector linear recurrence with matrix coefficients here, and sizes of matrix coefficients RP by here.",
                    "label": 0
                },
                {
                    "sent": "So peace is round efficiency here and it was shown it was shown by.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in getting Williams that this vector.",
                    "label": 0
                },
                {
                    "sent": "Vector linear recurrence with matrix coefficients exactly correspond to dynamical systems to trajectory's of dynamical systems with bounded complexity.",
                    "label": 0
                },
                {
                    "sent": "Appointed number of locks and this number of inputs and the interesting feature here is that the inputs and outputs of the system can be permitted in this vector.",
                    "label": 1
                },
                {
                    "sent": "So this means that by doing a low rank approximation of both counter matrices we do system identification from given data.",
                    "label": 0
                },
                {
                    "sent": "Help.",
                    "label": 0
                },
                {
                    "sent": "So it's so so this is a connection to systems identification.",
                    "label": 0
                },
                {
                    "sent": "Of course, this class is linear time invariant dynamical systems.",
                    "label": 0
                },
                {
                    "sent": "So the first exam.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you you could have seen it yesterday on a poster by Maria our colleague.",
                    "label": 0
                },
                {
                    "sent": "So suppose that you have two polynomials, two univariate polynomials, and you construct the Sylvester matrix from it.",
                    "label": 1
                },
                {
                    "sent": "It is well known fact in algebra then degree they have a common divisor of degree at least D if and only if this Sylvester matrix is running efficient with this rank deficiency D. So this basically means that walk around proximation of our Sylvester matrix is equivalent to finding forgiven two polynomials that causes two polynomials which have a common divisor of at least of given degree.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit.",
                    "label": 0
                },
                {
                    "sent": "This may be a little bit strange that you measure distance between polynomials, not in the sense of their roots, but in the sense of their coefficients.",
                    "label": 0
                },
                {
                    "sent": "But basically this one of the one of the important problems in the field of computer algebra and people in this community really care about this problem, and there's a lot of papers on these.",
                    "label": 0
                },
                {
                    "sent": "Structure approximation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even more, you may wish to compute approximate GCD of several several univariate polynomials, or you may wish to compute approximate GCD of multivariate polynomials and or some kind of approximate groebner basis, because life is more various in multivariate case and deferred third example for final example is about multi level 100 matrices, so this this structure appears if you.",
                    "label": 1
                },
                {
                    "sent": "Process and dimensional arrays.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Multi level low control matrices are comfortable.",
                    "label": 0
                },
                {
                    "sent": "Control matrices are used in processing performer I imagine or in texture analysis and put what was recently approved that also symmetric tensor decomposition is equivalent to a structure or an proximation with.",
                    "label": 0
                },
                {
                    "sent": "With multi level Hunter structure so it means that you have fun chilled until books and each focus also punctual and you can have infinite amount of books not not infinite of course fine.",
                    "label": 0
                },
                {
                    "sent": "So these are I will not talk about this examples, but this is just to give you an idea of what kind of problems appear in the framework of or approximation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me now formulate the problem as we as we consider it so we can see there.",
                    "label": 0
                },
                {
                    "sent": "Structure which can be defined by a linear map from some parameter space to the space of N by N matrices.",
                    "label": 1
                },
                {
                    "sent": "So image of this map will be a linear subspace and this will go linear matrix structure.",
                    "label": 0
                },
                {
                    "sent": "So all of the examples which represented they fit fit into this.",
                    "label": 0
                },
                {
                    "sent": "Framework and the structure to rank approximation.",
                    "label": 1
                },
                {
                    "sent": "Here we formulate this whole so given given structure given some approximation criterion and given the E which corresponds to original structure until matrix which we acquired from somewhere.",
                    "label": 0
                },
                {
                    "sent": "We want to find the closest vector B had such that the corresponding structure matrix is of no rank of rank less than than R. So as you may have noticed here here.",
                    "label": 0
                },
                {
                    "sent": "It's the difference between parameter vectors, but you can show for linear Maps that it is just equivalent distance between parameter vectors and distance between structured matrices.",
                    "label": 0
                },
                {
                    "sent": "We solve, we solve this problem and.",
                    "label": 0
                },
                {
                    "sent": "How do we, what kind of approximation criterion?",
                    "label": 0
                },
                {
                    "sent": "We concede consider we can see their weighted Euclidean seem seminar and this is a weighted sum of squares of elements of vector and we also allow whites seminorm because we allow zeros and infinities.",
                    "label": 0
                },
                {
                    "sent": "Here we found this very, very convenient.",
                    "label": 0
                },
                {
                    "sent": "So you can have 0 send Infinity.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Continue to here means that the this approximately this distance is.",
                    "label": 0
                },
                {
                    "sent": "Is infinite if some some constraints are not satisfied.",
                    "label": 1
                },
                {
                    "sent": "For this this way.",
                    "label": 0
                },
                {
                    "sent": "So this means that if we have infinite weights, it means a constraint or some fixed values on the parameter vector.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we have zero wait.",
                    "label": 0
                },
                {
                    "sent": "It means that in this approximation criterion, the elements really do not matter the original and approximating picky, so this corresponds to the case of missing values.",
                    "label": 1
                },
                {
                    "sent": "Just just to give an example.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where this fixed on mission failures appear.",
                    "label": 0
                },
                {
                    "sent": "So for example, Sylvester Matrix, which you have seen previously.",
                    "label": 1
                },
                {
                    "sent": "If you flip it accounts then you have 200 meters is with fixed zeros.",
                    "label": 1
                },
                {
                    "sent": "The case of missing Fitness is more interesting.",
                    "label": 0
                },
                {
                    "sent": "I think you have already seen a lot of these matrices in the sense of approx matrix completion, so you have you have some matrix and you want to complete it such that the completed matrix is of all ranks.",
                    "label": 1
                },
                {
                    "sent": "You can also do approximate matrix completion when you also want to divide, divide little bit from this entries so.",
                    "label": 0
                },
                {
                    "sent": "But more interesting example, when you have both structure and missing values, this this more interesting case because this can be used in system identification with missing data.",
                    "label": 1
                },
                {
                    "sent": "The problem is that if you have dynamical systems, your data is independent across time, and if if you miss some database may present a complication for your identification algorithm.",
                    "label": 0
                },
                {
                    "sent": "Another example here is some kind of.",
                    "label": 0
                },
                {
                    "sent": "Suggest example data driven control so you have no input, no output and you want to produce an input which tracks this this output.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have this problem and then we want to represent it.",
                    "label": 0
                },
                {
                    "sent": "Tries it to finally come to the topic of my talk to optimization across manifold.",
                    "label": 0
                },
                {
                    "sent": "So the general idea how do we do it?",
                    "label": 0
                },
                {
                    "sent": "So this optimization problem we introduce some additional variable R that sensation variable here is behind and then after that we eliminate the variable behave.",
                    "label": 0
                },
                {
                    "sent": "This idea, so how we do it?",
                    "label": 0
                },
                {
                    "sent": "We recognize that there aren't constraint can be represented in kernel form.",
                    "label": 0
                },
                {
                    "sent": "So you want known about image form drunken state of a matrix means that the matrix is product of two matrices, one of its thin and other is fed.",
                    "label": 0
                },
                {
                    "sent": "But here we use a kernel form.",
                    "label": 1
                },
                {
                    "sent": "So it means that rank is less than or equal to R if and only if there is exist.",
                    "label": 0
                },
                {
                    "sent": "The Matrix D by MM is the number of rows here.",
                    "label": 1
                },
                {
                    "sent": "And this is equal to M -- R, which is a four oh run and which annihilates this matrix.",
                    "label": 1
                },
                {
                    "sent": "So this D is equal to current at least that you want to achieve.",
                    "label": 1
                },
                {
                    "sent": "This means that there should exist full rank matrix in the kernel.",
                    "label": 0
                },
                {
                    "sent": "This matrix such that it annihilates it and then we can reformulate our original problem.",
                    "label": 0
                },
                {
                    "sent": "Here we can reformulate it as a double minimization problem.",
                    "label": 0
                },
                {
                    "sent": "So I will try to explain it, so we have automation and we have innovation on the altar minimization level.",
                    "label": 0
                },
                {
                    "sent": "We deal with some black box function F of R which we optimize over the set of full rank matrices.",
                    "label": 0
                },
                {
                    "sent": "This will be exactly the optimization across manifold and on the inner level we have forgiven our find the best approximation vector which has this matrix in its kernel.",
                    "label": 0
                },
                {
                    "sent": "So this as you can see from here it's it's linear.",
                    "label": 0
                },
                {
                    "sent": "It's linear problem and it has an analytics solution.",
                    "label": 0
                },
                {
                    "sent": "So by doing this we can do only with this optimization on the outer level, so we can deal with optimization of a black box function here and this.",
                    "label": 0
                },
                {
                    "sent": "We know we know how to evaluate and what's the advantage here.",
                    "label": 0
                },
                {
                    "sent": "Originally we have NP optimization variable and here we come to Z -- M optimization variables.",
                    "label": 0
                },
                {
                    "sent": "So we reduced.",
                    "label": 0
                },
                {
                    "sent": "The search space in this way, and typically we consider fair structured matrices, which perfectly makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a bit technical side here.",
                    "label": 0
                },
                {
                    "sent": "You can skip it, but this is just while this is a linear problem, it's generalized list no problem, it has has an analytic solution, so this terms here comes from the missing data and it appears here for this matrix be, but.",
                    "label": 0
                },
                {
                    "sent": "In the case where we do not have missing data in the case when we have only positive positive ways, so we also we also prove that this function is.",
                    "label": 1
                },
                {
                    "sent": "So this function basically just to give you an idea how it looks like.",
                    "label": 0
                },
                {
                    "sent": "It's a rational function, right?",
                    "label": 0
                },
                {
                    "sent": "It's function, derivative and hash and can be evaluated in this number of hops, and as you remember, we have typically a fat matrix, so so this corresponds.",
                    "label": 1
                },
                {
                    "sent": "To the number of our data points originally, so you have a linear complexity of iteration in in this case, so this may.",
                    "label": 0
                },
                {
                    "sent": "May fall in the classification of your investor of tool or optimization.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what happens on the outer level?",
                    "label": 0
                },
                {
                    "sent": "So we have this bugs box function which we can evaluate which we can evaluate derivatives with second order derivatives.",
                    "label": 0
                },
                {
                    "sent": "But we can know that it depends only on the row space of this matrix.",
                    "label": 1
                },
                {
                    "sent": "It's by construction, so it depends only on the subspace which is funded by this rose.",
                    "label": 0
                },
                {
                    "sent": "And this means that the function is equal if those funds are equal.",
                    "label": 0
                },
                {
                    "sent": "So basically it's an optimization on a Grassmann manifold and.",
                    "label": 1
                },
                {
                    "sent": "Custom manifold, what is it you have devy M4 around matrices and these are just dimensional subspaces of RM.",
                    "label": 0
                },
                {
                    "sent": "So just this site will be.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how how can we do it?",
                    "label": 0
                },
                {
                    "sent": "How do we optimize so the simplest idea that you can come up with?",
                    "label": 0
                },
                {
                    "sent": "You can use.",
                    "label": 0
                },
                {
                    "sent": "You can parameterise the dimensional subspaces by orthonormal basis.",
                    "label": 0
                },
                {
                    "sent": "So we have this constraint and in the case of D equals to one, you have one dimensional subspaces.",
                    "label": 0
                },
                {
                    "sent": "These are lines passing through origin and this corresponds to optimization of the sphere becausw as you remember the cost function.",
                    "label": 0
                },
                {
                    "sent": "Here is constant on this subspace is just the same for 2 two points which represent the same subspace.",
                    "label": 0
                },
                {
                    "sent": "So you can use here penalty method which becomes an exact method here.",
                    "label": 0
                },
                {
                    "sent": "But I will not.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even in details here and actually optimization on the grassman manifold was quite a hot topic recently, and there is a group of people who are working on it, so this is the basic reference.",
                    "label": 0
                },
                {
                    "sent": "This book and they want to consider it in the framework of Riemann manifolds, so.",
                    "label": 0
                },
                {
                    "sent": "Rigorous framework, so basically I will call this methods.",
                    "label": 0
                },
                {
                    "sent": "Attraction based methods.",
                    "label": 0
                },
                {
                    "sent": "So you have some point on the manifold.",
                    "label": 0
                },
                {
                    "sent": "You do some steps in tangent space and then you project back on the manifold because if you have a manifold you do you.",
                    "label": 0
                },
                {
                    "sent": "If you get derivatives they're not in the in the original space, so this corresponds here that you have you have a point on a sphere you have a tangent point to it and you make a direction and then you project it back to the sphere by some some protection.",
                    "label": 0
                },
                {
                    "sent": "And you can generalize a lot of methods from from ordinary optimization on Euclidean space, and we can come up with many different there is.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of papers.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of methods, but we want to.",
                    "label": 0
                },
                {
                    "sent": "Finally, account to the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean part of my top maybe about optimization across my phone.",
                    "label": 0
                },
                {
                    "sent": "We wanted to do something simpler.",
                    "label": 0
                },
                {
                    "sent": "We didn't want to deal with this projections and reductions, so we wanted to formulate the problem as optimization on Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So how can we do it?",
                    "label": 0
                },
                {
                    "sent": "This is so.",
                    "label": 0
                },
                {
                    "sent": "This is basically mean results if you if you want.",
                    "label": 0
                },
                {
                    "sent": "So if you have a full rank matrix here, divide N. This means that there exists G linearly independent code codes.",
                    "label": 0
                },
                {
                    "sent": "Here I will denoted by orange orange homes.",
                    "label": 0
                },
                {
                    "sent": "So this means that for any for the subspace which corresponds to this R, there exists such X and there exists a permutation matrix P such that this subspace is represented by this matrix.",
                    "label": 1
                },
                {
                    "sent": "So, so it's quite easy to see you know how to get the matrix of this form.",
                    "label": 0
                },
                {
                    "sent": "From here you multiply it from the right by this sub metric from the left by inverse of this sub matrix and you get you get this representation after you permute this rose here.",
                    "label": 0
                },
                {
                    "sent": "And then therefore you can minimize.",
                    "label": 0
                },
                {
                    "sent": "This functions over all possible all possible permutations, so this basically corresponds to optimization over affine hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "Here and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the problem is here that this optimization is unbounded and you have a lot of permutation matrices.",
                    "label": 0
                },
                {
                    "sent": "So how to deal with unbounded?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case there is a very nice result of move, which means that we can always find representation of this form with various bounded by absolute value by one.",
                    "label": 0
                },
                {
                    "sent": "So in this case it corresponds that affine hyperplanes cut out of heap hypercube and for any subspace you can find a point on this hypercube.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's amazing that this holds only not, for example, with Z equals to one, but for Gen Z.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we propose how to deal with this.",
                    "label": 1
                },
                {
                    "sent": "You have full control, number of possibilities and you can do basically local optimization of these functions for a fixed permutation.",
                    "label": 1
                },
                {
                    "sent": "And you do work optimization of this function until it converges or it exceeds this face of the cube.",
                    "label": 0
                },
                {
                    "sent": "And then you can switch a permutation so so it's not really an optimization algorithm, it's a way you can approach the problem.",
                    "label": 0
                },
                {
                    "sent": "So if you have optimization on the grassman manifold, then you can turn it to optimization on Euclidean space.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we we just so it's it's really easy to implement the algorithm with that was on the previous slide, and it does quite well, so it doesn't use this specialized remaining geometry.",
                    "label": 0
                },
                {
                    "sent": "So it's under the advantage here that you can use any optimization method for the subproblem on Euclidean space.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm unfortunately I don't have time to explain it, so here we have some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So these methods we do not try to compete with them because it's just a simple idea and is basically idea of North which was from 30 years ago.",
                    "label": 0
                },
                {
                    "sent": "But here this is more adapted to local geometry, but for every optimization method you need to modify your optimization method to use this retractions and you don't really know much about properties of it but here.",
                    "label": 1
                },
                {
                    "sent": "But doing this certification often faces of the cube.",
                    "label": 0
                },
                {
                    "sent": "We can use any optimization method here, and this is this nice problem because it's on about it subset.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately there is no bumps on the number of switches here, so if you exceed the phase you need to switch to another hyperplane.",
                    "label": 1
                },
                {
                    "sent": "But in practice it's very low.",
                    "label": 0
                },
                {
                    "sent": "It's usually one or two switches, so so this is basically the main the main idea.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are some references about optimization on the grassman manifold.",
                    "label": 0
                },
                {
                    "sent": "These are papers about structure, torrent approximation, this valuable software with not opened our interfaces, and these experiments are also available online here, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}