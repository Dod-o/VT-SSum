{
    "id": "5ddzwibpxcmw74o2fc3moezjwdpesshn",
    "title": "Difficulty-controllable Multi-hop Question Generation From Knowledge Graphs",
    "info": {
        "author": [
            "Yuncheng Hua, Monash University"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_hua_knowledge_graphs/",
    "segmentation": [
        [
            "So good afternoon everyone and my name is Yuan Shanghai and I'm a joint PhD student from South East University and Monash University, and I'm very, I'm very appreciate today that I could have a chance to stand here to share our work and our work is about difficulty, controllable multihop question generation from knowledge graphs."
        ],
        [
            "So this is the work that cooperated with which budget you have found.",
            "Danish greeting and lonely and which budget is the first author of our work.",
            "And I will categorize our presentation into five parts today and 1st."
        ],
        [
            "1st and as we all know that knowledge graph nodes answering over node graphs a question answering over knowledge graphs which is key to a yes in Cancer Research focus nowadays and it's a very useful methods that to transform the natural language questions into the structure queries to help the user to retrieve the entities that they want.",
            "So we could category that typically into simple questions, multiple questions and more complex questions.",
            "This kind of more complex questions, yes.",
            "Reasoning based on the different triples in the knowledge graph.",
            "So we have many models that could be have been proposed these days, but all of these models are most of these orders based on the neural network, and as we all know, if we want to train a neural network to work as we expect it, we need to feed them with a huge amount of data so that we are so that we are have.",
            "Problems that we if we don't have the sufficient data, what can we do to train this models?",
            "So as we mentioned that the limited availability of data set yes hinder here to train the models."
        ],
        [
            "So we have some data set nowadays and we have some single hop questions.",
            "The amount of this single hop question is really large, but it is only a single hop question.",
            "Is the simple questions, but not a complex questions and we have some data set for the multi hub questions for example else quad complex web complex web questions where question is P and past past questions.",
            "But all these questions the size of these questions maybe around us.",
            "A few 1000 so it is hard to train a deep learning model and also we have a complex question data set which is CS QA.",
            "But this kind of data set is established based on some.",
            "Predefined templates, so we cannot ultimately to establish to create the data set."
        ],
        [
            "So is this a challenge?",
            "Here is limited insight, so we don't have enough data set and all the data set created for QA, especially the multihop complex questions are manually created on stemming manually created, which is we need to collect templates and based on their templates we need we could.",
            "Establish to create the questions."
        ],
        [
            "So here is some typical thing.",
            "A sample of the multiple questions.",
            "The first one is we need to use a sequence of triples to solve a problem.",
            "For example, here we need to know who is the Henry I Duke of Guise is Mount and who is her spouse.",
            "Obviously it is his his father.",
            "So minimally so we at least we need to know who is hungry.",
            "I Dukes.",
            "Father and then we need to know who is, uh, what is his name?",
            "So we need to.",
            "We need to we need a minimum two triplets to solve this problem and the next sample.",
            "Yes, stuff format sample which is that we have a root entity which is no way but we have many things, many saying relations that.",
            "Linked with this route, the relation is the official language, so in this problem we need to count how many languages or what the languages are.",
            "Two spoken in Norway.",
            "So these are two typical examples of multiple questions."
        ],
        [
            "And our contributions is to use Ultimate ultimately automated function to automatically create this kind of multiple questions.",
            "And we use the answer also encoding to update our model and we use difficulty embedding or difficulty estimation to estimate how difficult the question is and our technique could.",
            "Generated the questions of the high quality and we share our datasets and codes here.",
            "So."
        ],
        [
            "So we have some QA models for upcoming models, for example with the cost of the simple or multihop or complex questions, but most of these models need huge amount of data set to train because it is a neural network models and also we have some QG which is question generation over cages.",
            "We have some QG models but most of these models are based on the templates and we have some.",
            "Your network based model to generate questions, but it is about a single triple questions which is a simple questions.",
            "And we have some models could generate the questions from the text, but the the information source here is changed from the text to the large graph."
        ],
        [
            "So here's our model.",
            "Our model actually is a graph to sequence model.",
            "While we called it a graph to sequence model cause in this model the input is not a sequence of data, it is a graph.",
            "You know, graph is not a sequence, so we can't regarded as the sequence to sequence model.",
            "But we use some.",
            "But we use a method to try to.",
            "Embed try to encode a try to encode this graph and suppose we have a sub graph of the facts we have.",
            "We have infects and we need to take this on facts as the import and also we take the original questions at the training.",
            "Supervision signals to train our model, so actually our.",
            "The objective of our model is to try to find the most appropriate, most appropriate distribution probability of the words in the output vocabulary."
        ],
        [
            "So this is our model, the model is transformer based model.",
            "So we have some difficulty estimation on the encoding here to try to update the model."
        ],
        [
            "And here is for encoding encoding.",
            "Yes, typical transformer encoding model here.",
            "But we use some tricks here and the first layer.",
            "Firstly it is the answer embedding which is that.",
            "We have triples.",
            "For us we have a sub graph and in this graph we have multiple triples, but in this trouble some entities are answer but some things are not answered.",
            "So we need to find a way to distinguish this answer entities.",
            "Waste none on the entities, so we found the.",
            "So we use a linear linear layer here to try to embedding this kind of 1 hot vector into the.",
            "Into the into the model sized vector.",
            "Why we use this one hot vector, which is that we use this one?",
            "How vector to distinguish whether this entity is answer or not and when we got this kind of mark we could embedding this mark into a vector and we stand this vector with the input embedding and to form our importing beddings and after we have importing embeddings we could use the transformer to try to encode it is embedded.",
            "And this is this is our how our encoder works.",
            "And."
        ],
        [
            "Is that how we estimate the echo difficulty or how we encode the difficulty?",
            "Here we have some.",
            "We have two metrics.",
            "When it's confidence at another is selectivity, the confidence is about is about that.",
            "If the mentions in the question are easy to link to the entities in the cagey so we think that this kind of mention have higher confidence.",
            "Which means if this kind of measure has higher and young.",
            "Go, then we think that this kind of mention have higher confidence and the selectivity is that suppose we have two name.",
            "Of entities in the cage when name is John Smith, you know there are many John Smiths in the world we can't recognize or distinguish this John Smith with another John Smith, but if we try to find a who is Anna, must I think there's only one animal in the world, so it is easier to distinguish it.",
            "So we think that if the selectivity is.",
            "Lower than the question is more confusing and it is harder to.",
            "It is harder to solve this kind of question so so our matrix is that if the confidence is higher, the question is easier to ask if the selectivity selectivity is lower and this question is harder to ask.",
            "So that's how we estimate the difficulty."
        ],
        [
            "After we estimated difficulty, we have stretch hold if the value is under the threshold, we think that it is a easy question.",
            "If it's higher, we think that it is hard question and we could distinguish the question into two parts.",
            "Two groups.",
            "When is the easier and another day hard?",
            "So we use one hot vector to represent this easy and hard and after that to be embedding this kind of run part vectors to the output embedding.",
            "Then after we have the output embedding based on the typical transformer operation, we could get the output of the optimist output is the predicted predicted questions and with the losses is the blue or the error some something like this?",
            "We could use this log.",
            "We could use the largest cross entropy loss story losses, cross entropy loss.",
            "We could use this loss to train our model.",
            "So that's the end."
        ],
        [
            "Our model and this datasets we try to use the we try to use the data sets.",
            "The questions that in this three kind of datasets and the training the training signals.",
            "Here is the sub graph and questions we use the sub graph and questions as a pair to train our model.",
            "Suppose we have the sub graph we embed.",
            "We transform this sub graph to into embedding into a vector and we use the.",
            "Original questions add the supervision signals of the of the decoder to try to train our model.",
            "So we use this three data set to train our model."
        ],
        [
            "And we have some baseline methods here.",
            "The baseline was here is L2AL2A is LSTM based sequence to sequence model because it is a segment of sequence model.",
            "So we need to try to transform the graph into a sequence of sequence of triples to make the input as to make the input change into a sequence.",
            "And then we could run this second to secrets model which is L2A.",
            "Uh, which is proposed as heir to 8 so L 2A is the state of art.",
            "QG model it.",
            "But it is generated questions from the text we are generating the questions from the Knowledge Graph, so the software information source are different and we used some automatic automatic value evaluation.",
            "We have some metrics to automatically evaluate our model based on the Blue Rock, air and mature.",
            "And also we have some manual evaluation which which means that we have some participants.",
            "We have some volunteers and they could manually evaluate Hall correct?",
            "This question is yes, and what's the quality of the same text?",
            "What's the quality of the semantics and of this question and whether this the difficulty level of this output question is, as is the same as we expected?",
            "For example, we input the easy.",
            "We imported the difficulty difficult.",
            "We input the hard difficulty level into this model, but the model outputs some easy questions, so we think that it is not.",
            "It is not good output, it is or not good generated question."
        ],
        [
            "And here is some statistics statistics.",
            "We could find that in all this in all these two datasets and for all these three different metrics, our model have the state of state of the art results and a here is about answer encoding.",
            "If we remove the answer encoding from our model, we could find that our model will be impaired.",
            "Performance of our model will be impaired.",
            "So it could be found at AE or we caught on.",
            "The encoding is important here in armor."
        ],
        [
            "Yeah, and this is the manual evaluation which, which means that the percentage of the percentage of the.",
            "Other questions that we generate, yes good.",
            "The percentage and number in the parenthesis is about that agreement with participants, so it could be found that our model still have the state of art result."
        ],
        [
            "And here is on some example.",
            "For example, we have the same knowledge graph submerged graph but we have different difficulty level and we could form different different questions.",
            "We could find that the harder one is obviously more difficult than the easier."
        ],
        [
            "And this is how we used three different models to generate the questions.",
            "We could find that based on that image, could image QG plus AE which is the best model could generated.",
            "Could generate the question with the highest quality and it is the most fluent.",
            "And here's a."
        ],
        [
            "Another sample that how our model generated the questions from the sub graph and the different and different models.",
            "So."
        ],
        [
            "So we propose a novel transformer based model for generating multiple questions from subgraphs.",
            "We try to solve the challenges that the input is graph, but not sequence how we use the sequence to sequence model to solve this problem.",
            "And also we use the difficulty in modeling the answer in coding to try to update our model and the future work is that we could generate even more complex questions or we could take the structure of the sub graph into account.",
            "To try to estimate the difficulty, but not just the ambiguity of the mentions in the in our questions or the Knowledge Graph.",
            "And also we want to try to combine query and QG into us in do a task and we want try to solve these two tasks task symmetry symmetry.",
            "And that's the end of my presentation.",
            "Thank you very much, thank you, thank you.",
            "Any questions?",
            "I I've got few so it's very interesting work.",
            "I mean, obviously if you want to properly do this generation you need to kind of have a kind of classification of different kind of difficulties.",
            "So what are the key dimensions?",
            "In your framework for the difficult questions.",
            "Difficult questions here.",
            "Yeah, we have been training and testing.",
            "We have two methods to try to control this difficulty level and when training the difficulty level here is based on the ambiguity of the mentions in the questions and the Knowledge Graph.",
            "Suppose that we have mentioned Turkey here in our questions, but you don't know whether this Turkey is about the chicken or about the country.",
            "So if this kind of mission.",
            "If if we have many of these kind of ambiguous mentions in your question, so we think that it is hard to answer this question because we need many different in relevant irrelevant triples to try to solve this problem.",
            "So we think that this kind of question is more difficult.",
            "That's how we estimate our difficulty in our work.",
            "Yeah, one observation obviously is that.",
            "You model this problem as graph to sequence, but sometimes some of the difficulty of the questions are not necessarily from the graph, But you know it's well known that if you have negation in your question, it will be quite bit harder then those.",
            "We down negation.",
            "And there are other things that you could actually appear in the question, but not necessarily from the graph.",
            "So do you have any?",
            "Yeah yeah, yeah sorry about that.",
            "Yeah.",
            "So we had mentioned that we have a manual estimation.",
            "Of this question, so besides of this kind of ambiguity of the entity mentions also also the some volunteers or some we have some many work to estimate their questions based on the based on how hard it could be reason between these different triples and the how complex the whole complex the question is.",
            "And we have this manual estimation.",
            "And also we could also we use this kind of menu.",
            "Evaluation of difficulty level as the training signal and try to encoding it into our work.",
            "So actually the difficulty level here is combined by two things.",
            "One is the latest from the the How complex the Knowledge graph is and another is how hard it is.",
            "Kind of question yes.",
            "So obviously I think in your presentation at least two angles right.",
            "One is question understanding.",
            "The other part is kind of question reasoning.",
            "So in in a generation.",
            "So what is the balance between the two?",
            "If it's kind of more focused on the reasoning side or more focused on the understanding.",
            "So currently we focus more on the ambiguity of the entity mentioned and in later in future work we will try to focus more on the under another thing, OK?",
            "Any other questions?",
            "If not, maybe I could ask the last one as well.",
            "So out to a rhyme, and obviously as you said it is sequence to sequence, but you know in in the graph we don't have order for the notes for the entities, so it is fair to use L2A as baseline.",
            "I mean, have you done anything not trivial to actually properly use L2A?",
            "Yeah, actually why how we use L2A is similar to ours, but it can't be the same as ours and we try to.",
            "In our in our when we try to compare with their works and we try to put the sequence output that ripples in a sequence and this sequence is some related is at some extent related to the to the questions.",
            "For example, for example that the examples I've mentioned before we need to 1st find a person's father and then we need to fund its father's name.",
            "So we put the triples here.",
            "The first triple is about.",
            "The people at the person's father and the next triple is about the name of this person, so we try to put this kind of triples in order and try to regard it as a sequence of triples.",
            "So we did some work to try to compare our work with L2A in a fairway.",
            "OK, yeah, thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good afternoon everyone and my name is Yuan Shanghai and I'm a joint PhD student from South East University and Monash University, and I'm very, I'm very appreciate today that I could have a chance to stand here to share our work and our work is about difficulty, controllable multihop question generation from knowledge graphs.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the work that cooperated with which budget you have found.",
                    "label": 0
                },
                {
                    "sent": "Danish greeting and lonely and which budget is the first author of our work.",
                    "label": 0
                },
                {
                    "sent": "And I will categorize our presentation into five parts today and 1st.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st and as we all know that knowledge graph nodes answering over node graphs a question answering over knowledge graphs which is key to a yes in Cancer Research focus nowadays and it's a very useful methods that to transform the natural language questions into the structure queries to help the user to retrieve the entities that they want.",
                    "label": 1
                },
                {
                    "sent": "So we could category that typically into simple questions, multiple questions and more complex questions.",
                    "label": 0
                },
                {
                    "sent": "This kind of more complex questions, yes.",
                    "label": 0
                },
                {
                    "sent": "Reasoning based on the different triples in the knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So we have many models that could be have been proposed these days, but all of these models are most of these orders based on the neural network, and as we all know, if we want to train a neural network to work as we expect it, we need to feed them with a huge amount of data so that we are so that we are have.",
                    "label": 1
                },
                {
                    "sent": "Problems that we if we don't have the sufficient data, what can we do to train this models?",
                    "label": 0
                },
                {
                    "sent": "So as we mentioned that the limited availability of data set yes hinder here to train the models.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have some data set nowadays and we have some single hop questions.",
                    "label": 0
                },
                {
                    "sent": "The amount of this single hop question is really large, but it is only a single hop question.",
                    "label": 0
                },
                {
                    "sent": "Is the simple questions, but not a complex questions and we have some data set for the multi hub questions for example else quad complex web complex web questions where question is P and past past questions.",
                    "label": 0
                },
                {
                    "sent": "But all these questions the size of these questions maybe around us.",
                    "label": 0
                },
                {
                    "sent": "A few 1000 so it is hard to train a deep learning model and also we have a complex question data set which is CS QA.",
                    "label": 0
                },
                {
                    "sent": "But this kind of data set is established based on some.",
                    "label": 0
                },
                {
                    "sent": "Predefined templates, so we cannot ultimately to establish to create the data set.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is this a challenge?",
                    "label": 0
                },
                {
                    "sent": "Here is limited insight, so we don't have enough data set and all the data set created for QA, especially the multihop complex questions are manually created on stemming manually created, which is we need to collect templates and based on their templates we need we could.",
                    "label": 1
                },
                {
                    "sent": "Establish to create the questions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is some typical thing.",
                    "label": 0
                },
                {
                    "sent": "A sample of the multiple questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is we need to use a sequence of triples to solve a problem.",
                    "label": 0
                },
                {
                    "sent": "For example, here we need to know who is the Henry I Duke of Guise is Mount and who is her spouse.",
                    "label": 1
                },
                {
                    "sent": "Obviously it is his his father.",
                    "label": 0
                },
                {
                    "sent": "So minimally so we at least we need to know who is hungry.",
                    "label": 0
                },
                {
                    "sent": "I Dukes.",
                    "label": 0
                },
                {
                    "sent": "Father and then we need to know who is, uh, what is his name?",
                    "label": 0
                },
                {
                    "sent": "So we need to.",
                    "label": 0
                },
                {
                    "sent": "We need to we need a minimum two triplets to solve this problem and the next sample.",
                    "label": 0
                },
                {
                    "sent": "Yes, stuff format sample which is that we have a root entity which is no way but we have many things, many saying relations that.",
                    "label": 0
                },
                {
                    "sent": "Linked with this route, the relation is the official language, so in this problem we need to count how many languages or what the languages are.",
                    "label": 0
                },
                {
                    "sent": "Two spoken in Norway.",
                    "label": 0
                },
                {
                    "sent": "So these are two typical examples of multiple questions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our contributions is to use Ultimate ultimately automated function to automatically create this kind of multiple questions.",
                    "label": 0
                },
                {
                    "sent": "And we use the answer also encoding to update our model and we use difficulty embedding or difficulty estimation to estimate how difficult the question is and our technique could.",
                    "label": 1
                },
                {
                    "sent": "Generated the questions of the high quality and we share our datasets and codes here.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have some QA models for upcoming models, for example with the cost of the simple or multihop or complex questions, but most of these models need huge amount of data set to train because it is a neural network models and also we have some QG which is question generation over cages.",
                    "label": 0
                },
                {
                    "sent": "We have some QG models but most of these models are based on the templates and we have some.",
                    "label": 0
                },
                {
                    "sent": "Your network based model to generate questions, but it is about a single triple questions which is a simple questions.",
                    "label": 0
                },
                {
                    "sent": "And we have some models could generate the questions from the text, but the the information source here is changed from the text to the large graph.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's our model.",
                    "label": 0
                },
                {
                    "sent": "Our model actually is a graph to sequence model.",
                    "label": 0
                },
                {
                    "sent": "While we called it a graph to sequence model cause in this model the input is not a sequence of data, it is a graph.",
                    "label": 0
                },
                {
                    "sent": "You know, graph is not a sequence, so we can't regarded as the sequence to sequence model.",
                    "label": 0
                },
                {
                    "sent": "But we use some.",
                    "label": 0
                },
                {
                    "sent": "But we use a method to try to.",
                    "label": 0
                },
                {
                    "sent": "Embed try to encode a try to encode this graph and suppose we have a sub graph of the facts we have.",
                    "label": 0
                },
                {
                    "sent": "We have infects and we need to take this on facts as the import and also we take the original questions at the training.",
                    "label": 0
                },
                {
                    "sent": "Supervision signals to train our model, so actually our.",
                    "label": 0
                },
                {
                    "sent": "The objective of our model is to try to find the most appropriate, most appropriate distribution probability of the words in the output vocabulary.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our model, the model is transformer based model.",
                    "label": 0
                },
                {
                    "sent": "So we have some difficulty estimation on the encoding here to try to update the model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is for encoding encoding.",
                    "label": 0
                },
                {
                    "sent": "Yes, typical transformer encoding model here.",
                    "label": 0
                },
                {
                    "sent": "But we use some tricks here and the first layer.",
                    "label": 0
                },
                {
                    "sent": "Firstly it is the answer embedding which is that.",
                    "label": 0
                },
                {
                    "sent": "We have triples.",
                    "label": 0
                },
                {
                    "sent": "For us we have a sub graph and in this graph we have multiple triples, but in this trouble some entities are answer but some things are not answered.",
                    "label": 0
                },
                {
                    "sent": "So we need to find a way to distinguish this answer entities.",
                    "label": 0
                },
                {
                    "sent": "Waste none on the entities, so we found the.",
                    "label": 0
                },
                {
                    "sent": "So we use a linear linear layer here to try to embedding this kind of 1 hot vector into the.",
                    "label": 0
                },
                {
                    "sent": "Into the into the model sized vector.",
                    "label": 0
                },
                {
                    "sent": "Why we use this one hot vector, which is that we use this one?",
                    "label": 0
                },
                {
                    "sent": "How vector to distinguish whether this entity is answer or not and when we got this kind of mark we could embedding this mark into a vector and we stand this vector with the input embedding and to form our importing beddings and after we have importing embeddings we could use the transformer to try to encode it is embedded.",
                    "label": 0
                },
                {
                    "sent": "And this is this is our how our encoder works.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that how we estimate the echo difficulty or how we encode the difficulty?",
                    "label": 0
                },
                {
                    "sent": "Here we have some.",
                    "label": 0
                },
                {
                    "sent": "We have two metrics.",
                    "label": 0
                },
                {
                    "sent": "When it's confidence at another is selectivity, the confidence is about is about that.",
                    "label": 0
                },
                {
                    "sent": "If the mentions in the question are easy to link to the entities in the cagey so we think that this kind of mention have higher confidence.",
                    "label": 1
                },
                {
                    "sent": "Which means if this kind of measure has higher and young.",
                    "label": 0
                },
                {
                    "sent": "Go, then we think that this kind of mention have higher confidence and the selectivity is that suppose we have two name.",
                    "label": 0
                },
                {
                    "sent": "Of entities in the cage when name is John Smith, you know there are many John Smiths in the world we can't recognize or distinguish this John Smith with another John Smith, but if we try to find a who is Anna, must I think there's only one animal in the world, so it is easier to distinguish it.",
                    "label": 0
                },
                {
                    "sent": "So we think that if the selectivity is.",
                    "label": 1
                },
                {
                    "sent": "Lower than the question is more confusing and it is harder to.",
                    "label": 0
                },
                {
                    "sent": "It is harder to solve this kind of question so so our matrix is that if the confidence is higher, the question is easier to ask if the selectivity selectivity is lower and this question is harder to ask.",
                    "label": 0
                },
                {
                    "sent": "So that's how we estimate the difficulty.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After we estimated difficulty, we have stretch hold if the value is under the threshold, we think that it is a easy question.",
                    "label": 0
                },
                {
                    "sent": "If it's higher, we think that it is hard question and we could distinguish the question into two parts.",
                    "label": 0
                },
                {
                    "sent": "Two groups.",
                    "label": 0
                },
                {
                    "sent": "When is the easier and another day hard?",
                    "label": 0
                },
                {
                    "sent": "So we use one hot vector to represent this easy and hard and after that to be embedding this kind of run part vectors to the output embedding.",
                    "label": 0
                },
                {
                    "sent": "Then after we have the output embedding based on the typical transformer operation, we could get the output of the optimist output is the predicted predicted questions and with the losses is the blue or the error some something like this?",
                    "label": 0
                },
                {
                    "sent": "We could use this log.",
                    "label": 0
                },
                {
                    "sent": "We could use the largest cross entropy loss story losses, cross entropy loss.",
                    "label": 0
                },
                {
                    "sent": "We could use this loss to train our model.",
                    "label": 0
                },
                {
                    "sent": "So that's the end.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model and this datasets we try to use the we try to use the data sets.",
                    "label": 0
                },
                {
                    "sent": "The questions that in this three kind of datasets and the training the training signals.",
                    "label": 0
                },
                {
                    "sent": "Here is the sub graph and questions we use the sub graph and questions as a pair to train our model.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have the sub graph we embed.",
                    "label": 0
                },
                {
                    "sent": "We transform this sub graph to into embedding into a vector and we use the.",
                    "label": 0
                },
                {
                    "sent": "Original questions add the supervision signals of the of the decoder to try to train our model.",
                    "label": 0
                },
                {
                    "sent": "So we use this three data set to train our model.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have some baseline methods here.",
                    "label": 0
                },
                {
                    "sent": "The baseline was here is L2AL2A is LSTM based sequence to sequence model because it is a segment of sequence model.",
                    "label": 0
                },
                {
                    "sent": "So we need to try to transform the graph into a sequence of sequence of triples to make the input as to make the input change into a sequence.",
                    "label": 0
                },
                {
                    "sent": "And then we could run this second to secrets model which is L2A.",
                    "label": 0
                },
                {
                    "sent": "Uh, which is proposed as heir to 8 so L 2A is the state of art.",
                    "label": 0
                },
                {
                    "sent": "QG model it.",
                    "label": 0
                },
                {
                    "sent": "But it is generated questions from the text we are generating the questions from the Knowledge Graph, so the software information source are different and we used some automatic automatic value evaluation.",
                    "label": 0
                },
                {
                    "sent": "We have some metrics to automatically evaluate our model based on the Blue Rock, air and mature.",
                    "label": 0
                },
                {
                    "sent": "And also we have some manual evaluation which which means that we have some participants.",
                    "label": 0
                },
                {
                    "sent": "We have some volunteers and they could manually evaluate Hall correct?",
                    "label": 0
                },
                {
                    "sent": "This question is yes, and what's the quality of the same text?",
                    "label": 0
                },
                {
                    "sent": "What's the quality of the semantics and of this question and whether this the difficulty level of this output question is, as is the same as we expected?",
                    "label": 0
                },
                {
                    "sent": "For example, we input the easy.",
                    "label": 0
                },
                {
                    "sent": "We imported the difficulty difficult.",
                    "label": 0
                },
                {
                    "sent": "We input the hard difficulty level into this model, but the model outputs some easy questions, so we think that it is not.",
                    "label": 0
                },
                {
                    "sent": "It is not good output, it is or not good generated question.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is some statistics statistics.",
                    "label": 0
                },
                {
                    "sent": "We could find that in all this in all these two datasets and for all these three different metrics, our model have the state of state of the art results and a here is about answer encoding.",
                    "label": 0
                },
                {
                    "sent": "If we remove the answer encoding from our model, we could find that our model will be impaired.",
                    "label": 0
                },
                {
                    "sent": "Performance of our model will be impaired.",
                    "label": 0
                },
                {
                    "sent": "So it could be found at AE or we caught on.",
                    "label": 0
                },
                {
                    "sent": "The encoding is important here in armor.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and this is the manual evaluation which, which means that the percentage of the percentage of the.",
                    "label": 0
                },
                {
                    "sent": "Other questions that we generate, yes good.",
                    "label": 0
                },
                {
                    "sent": "The percentage and number in the parenthesis is about that agreement with participants, so it could be found that our model still have the state of art result.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is on some example.",
                    "label": 0
                },
                {
                    "sent": "For example, we have the same knowledge graph submerged graph but we have different difficulty level and we could form different different questions.",
                    "label": 0
                },
                {
                    "sent": "We could find that the harder one is obviously more difficult than the easier.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is how we used three different models to generate the questions.",
                    "label": 0
                },
                {
                    "sent": "We could find that based on that image, could image QG plus AE which is the best model could generated.",
                    "label": 0
                },
                {
                    "sent": "Could generate the question with the highest quality and it is the most fluent.",
                    "label": 0
                },
                {
                    "sent": "And here's a.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another sample that how our model generated the questions from the sub graph and the different and different models.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we propose a novel transformer based model for generating multiple questions from subgraphs.",
                    "label": 1
                },
                {
                    "sent": "We try to solve the challenges that the input is graph, but not sequence how we use the sequence to sequence model to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "And also we use the difficulty in modeling the answer in coding to try to update our model and the future work is that we could generate even more complex questions or we could take the structure of the sub graph into account.",
                    "label": 0
                },
                {
                    "sent": "To try to estimate the difficulty, but not just the ambiguity of the mentions in the in our questions or the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "And also we want to try to combine query and QG into us in do a task and we want try to solve these two tasks task symmetry symmetry.",
                    "label": 0
                },
                {
                    "sent": "And that's the end of my presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much, thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "I I've got few so it's very interesting work.",
                    "label": 0
                },
                {
                    "sent": "I mean, obviously if you want to properly do this generation you need to kind of have a kind of classification of different kind of difficulties.",
                    "label": 0
                },
                {
                    "sent": "So what are the key dimensions?",
                    "label": 0
                },
                {
                    "sent": "In your framework for the difficult questions.",
                    "label": 0
                },
                {
                    "sent": "Difficult questions here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have been training and testing.",
                    "label": 0
                },
                {
                    "sent": "We have two methods to try to control this difficulty level and when training the difficulty level here is based on the ambiguity of the mentions in the questions and the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have mentioned Turkey here in our questions, but you don't know whether this Turkey is about the chicken or about the country.",
                    "label": 0
                },
                {
                    "sent": "So if this kind of mission.",
                    "label": 0
                },
                {
                    "sent": "If if we have many of these kind of ambiguous mentions in your question, so we think that it is hard to answer this question because we need many different in relevant irrelevant triples to try to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So we think that this kind of question is more difficult.",
                    "label": 0
                },
                {
                    "sent": "That's how we estimate our difficulty in our work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, one observation obviously is that.",
                    "label": 0
                },
                {
                    "sent": "You model this problem as graph to sequence, but sometimes some of the difficulty of the questions are not necessarily from the graph, But you know it's well known that if you have negation in your question, it will be quite bit harder then those.",
                    "label": 0
                },
                {
                    "sent": "We down negation.",
                    "label": 0
                },
                {
                    "sent": "And there are other things that you could actually appear in the question, but not necessarily from the graph.",
                    "label": 0
                },
                {
                    "sent": "So do you have any?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So we had mentioned that we have a manual estimation.",
                    "label": 0
                },
                {
                    "sent": "Of this question, so besides of this kind of ambiguity of the entity mentions also also the some volunteers or some we have some many work to estimate their questions based on the based on how hard it could be reason between these different triples and the how complex the whole complex the question is.",
                    "label": 0
                },
                {
                    "sent": "And we have this manual estimation.",
                    "label": 0
                },
                {
                    "sent": "And also we could also we use this kind of menu.",
                    "label": 0
                },
                {
                    "sent": "Evaluation of difficulty level as the training signal and try to encoding it into our work.",
                    "label": 0
                },
                {
                    "sent": "So actually the difficulty level here is combined by two things.",
                    "label": 0
                },
                {
                    "sent": "One is the latest from the the How complex the Knowledge graph is and another is how hard it is.",
                    "label": 0
                },
                {
                    "sent": "Kind of question yes.",
                    "label": 0
                },
                {
                    "sent": "So obviously I think in your presentation at least two angles right.",
                    "label": 0
                },
                {
                    "sent": "One is question understanding.",
                    "label": 0
                },
                {
                    "sent": "The other part is kind of question reasoning.",
                    "label": 0
                },
                {
                    "sent": "So in in a generation.",
                    "label": 0
                },
                {
                    "sent": "So what is the balance between the two?",
                    "label": 0
                },
                {
                    "sent": "If it's kind of more focused on the reasoning side or more focused on the understanding.",
                    "label": 0
                },
                {
                    "sent": "So currently we focus more on the ambiguity of the entity mentioned and in later in future work we will try to focus more on the under another thing, OK?",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "If not, maybe I could ask the last one as well.",
                    "label": 0
                },
                {
                    "sent": "So out to a rhyme, and obviously as you said it is sequence to sequence, but you know in in the graph we don't have order for the notes for the entities, so it is fair to use L2A as baseline.",
                    "label": 0
                },
                {
                    "sent": "I mean, have you done anything not trivial to actually properly use L2A?",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually why how we use L2A is similar to ours, but it can't be the same as ours and we try to.",
                    "label": 0
                },
                {
                    "sent": "In our in our when we try to compare with their works and we try to put the sequence output that ripples in a sequence and this sequence is some related is at some extent related to the to the questions.",
                    "label": 0
                },
                {
                    "sent": "For example, for example that the examples I've mentioned before we need to 1st find a person's father and then we need to fund its father's name.",
                    "label": 0
                },
                {
                    "sent": "So we put the triples here.",
                    "label": 0
                },
                {
                    "sent": "The first triple is about.",
                    "label": 0
                },
                {
                    "sent": "The people at the person's father and the next triple is about the name of this person, so we try to put this kind of triples in order and try to regard it as a sequence of triples.",
                    "label": 0
                },
                {
                    "sent": "So we did some work to try to compare our work with L2A in a fairway.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}