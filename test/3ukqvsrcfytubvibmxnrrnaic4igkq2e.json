{
    "id": "3ukqvsrcfytubvibmxnrrnaic4igkq2e",
    "title": "Large-Margin Thresholded Ensembles for Ordinal Regression",
    "info": {
        "author": [
            "Hsuan-Tien Lin, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Ensemble Methods",
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_lin_lmteo/",
    "segmentation": [
        [
            "Hello OK.",
            "Thank you for staying for the last talk for today and this is Shannon Link.",
            "From California Institute of Technology doing my PhD studies there and today, I'm going to talk about large margin source code it on samples for ordinal regression and this is joint work with my colleague Linley and will be presented in the algorithmic Learning Theory Conference this October."
        ],
        [
            "OK, so let's just start with an overview of this work is all about, so I copied Johns slide slide about reduction method and in this work we're actually doing reduction for interesting problem.",
            "So junk taught us to do is first we need to identify the type of learning problem and we find the reduction and then.",
            "Algorithm and then we can build a predictor.",
            "So let's see what we do."
        ],
        [
            "We identified the type of learning problem which is ordinal regression.",
            "And our pre made reduction would be the thresholded ensemble model.",
            "An article learning algorithm is the famous Adaboost algorithm in on sample learning.",
            "And then we build a regression rule using our newly designed algorithm called ordinal regression boosting plus the data.",
            "We also do another part."
        ],
        [
            "Of the reduction here, which I called a theoretical reduction.",
            "So the first 2 steps are similar, but then we start by analyzing the non generalization bounds in binary classification, which are the large margin ensemble bonds and then using the reduction we're able to derive new bounds based on combining the reduction and the known bound."
        ],
        [
            "So I think overview this work is actually a concrete instance of reductions."
        ],
        [
            "OK, so let's start by describing the ordinal regression problem and the problem with look look like this.",
            "So what's the age group of the person in the picture?"
        ],
        [
            "The picture looked like this.",
            "We would say the person in the picture is."
        ],
        [
            "Current and if the picture looked like this, we would say she's a child."
        ],
        [
            "And if the picture looked like this, we would say he's a teenager."
        ],
        [
            "And if the picture looked like this, we would say he's an adult, so we can represent this."
        ],
        [
            "Age curves by a finite order set of labels, which we call the ranks, and we assume that they are from one to K."
        ],
        [
            "So we can replace this age groups by this numbers and we hope to find an ordinal regression."
        ],
        [
            "So that correctly tells us the unknown picture is offering to."
        ],
        [
            "So this is the ordinary regression problem.",
            "We're given a training set like a bunch of pictures and their associated labels, and we want to find a decision function that creates the rings of unseen."
        ],
        [
            "Example as well.",
            "So there are many applications of this kind of problem.",
            "For example, we can rent movies like say a movies very bad, bad, OK, good or very good or we can rank by Doc."
        ],
        [
            "Coments relevance so because this idea of ranking or early no regression matches human preferences.",
            "It has many applications in social science in information retrieval."
        ],
        [
            "So let's look at the ordinal regression problem in more detail.",
            "We can think of it as a regression problem, but we found the metric information in our previous example about the age group.",
            "There's possibly a metric underlying like the age, but it's not encoded in the ranks 1234."
        ],
        [
            "We can also think of it as a classification problem with order categories.",
            "What I mean by ordered is that it will be a small mistake to classify a teenager as a child because very rings are close by, but it would be a big mistake to say that an infant is an adult."
        ],
        [
            "So there are some common loss functions to describe this properties of ordinal regression.",
            "First, it's kind of a multi class classification problem, so we can use the classification error to measure the performance."
        ],
        [
            "Determine the category or another commonly used loss function is.",
            "We want to have a close prediction in the category so we can use the absolute error to measure the performance."
        ],
        [
            "Nothing urgent or regression rule.",
            "In this talk, because of the time, I'll talk about the absolute error only and similar results happening as can be derived for the classification cost."
        ],
        [
            "OK, so first I'll introduce the thresholded model for ordinal regression.",
            "This model is widely used in previous algorithm designs.",
            "We start by thinking about a knife algorithm for ordinary regression.",
            "So first we can just treat the rings as general labels and just do a general correct regression."
        ],
        [
            "Estimated function.",
            "And then we round up the regression resource we get for exactly if we get 3.4, we round it to three, or if we get 1.8 we rounded to two hour final prediction.",
            "This method has two prop."
        ],
        [
            "Points.",
            "So first we said that the ordinal regression problem can be solved as regression without metric and general regression algorithms performs badly without the match."
        ],
        [
            "Information.",
            "And second, the round of operation here, which is essentially a uniform quantization, can cause large error because of the.",
            "Skewed distribution underline.",
            "So improved and generalized algorithm that people often use is first to estimate a potential function F of X that hopefully take care of the order of ribs.",
            "And then they quantized potential function by some ordered thresholds to get the final rule."
        ],
        [
            "So.",
            "So in illustration of this model, which is called the thresholded model is like this.",
            "So if we first map everything by this potential function and then if the potential value falls between likes it a one seater two we predict is a threat two if it's between Season 2 and season three we predict US rank 3."
        ],
        [
            "In our proposed thresholded ensemble model, the potential function at X is computed by a weighted on sample.",
            "So basically we can consider like an ensemble of T hypothesis and they have associated weights."
        ],
        [
            "The intuition of this model is we want to combine preferences from for every individual hypothesis to estimate the overall confidence."
        ],
        [
            "An example of this intuition is, say if many people we call them HT say that a movie is good, then if we combine them by positive weights, the confidence of the movie should be high."
        ],
        [
            "So in our results here, the hypothesis can be binary, multi valued or continuous, basically reflecting."
        ],
        [
            "The preferences and also we allow the weights to be positive or negative so we can reverse the bad preferences."
        ],
        [
            "And essentially the source loaded on sample model is an extension of ensemble learning in the area of ordinal regression."
        ],
        [
            "Before we go into deriving the theoretical results of 1st loaded Ensemble model, we first talk about an important concept called the margin and the idea of the margin is that we want to be saved from the boundaries.",
            "So for example for this example here we want its value to be away from this.",
            "So the thresholds here.",
            "This results here and this result."
        ],
        [
            "We can define the margin like this, which basically captures the distance between the potential value and this result for each threshold here.",
            "Singer."
        ],
        [
            "To the case of binary classification, a negative margin would indicate around prediction.",
            "So important observation here is that if for a pair of example XY, if we sum all its margin violations, then we basically get the term we want to use in the absolute loss."
        ],
        [
            "So now comes our first results using the reduction, so the reduction is based on this observation.",
            "So if we look at.",
            "Or you know regression problem here, but we only look at one source code.",
            "We can see that it's very similar to a binary classification problem.",
            "We want these examples to be on one side of the first hold, and these examples to be on the other side of this result.",
            "And we have K -- 1 thresholds here.",
            "So basically have K -- 1 binary classification problems.",
            "Now we can use one classifier trick that jump this yesterday.",
            "And basically we encode the same here.",
            "Can have one classifier there."
        ],
        [
            "Then we can apply the bounds in binary classification.",
            "So this is a famous founding binary classification that basically says OK if you have IID samples here and you're out of sample error can be bounded by the example margin violation and some complexity terms.",
            "So this basically tells us that if I can guarantee that I have a large margin classifiers, the Delta term can be set large.",
            "Then this bound can be like small.",
            "And basically the generalization error can be bounded well."
        ],
        [
            "So this is the analysis found that we describe based on the reduction.",
            "So in ordinal regression with similar settings, if we consider the thresholded ensembles then basically we have a similar bound, so this here is the out of sample absolute error and here we have the number of margin violations and here we have a complexity term."
        ],
        [
            "Which tells us that large margin shares audit on samples can generalize."
        ],
        [
            "And in addition to the theoretic reduction, I also mentioned that we do have rhythmic reduction to edibles, so I think we may learn more about edibles in the next week.",
            "But all we need to know now is edibles is the binary classification algorithm that can be explained by operationally optimizing this term, which is kind of maximizing the minimum margin here.",
            "So we derive the large margin bound here in the previous slide.",
            "So basically we want to maximize the minimum."
        ],
        [
            "In some way, so this is our formulation.",
            "Basically we have two formulations, but maybe this one would be similar to look at.",
            "So basically we try to minimize the summation of exponential negative margins, which if we look at here we are doing the next moment maximization of minimum."
        ],
        [
            "And this is basically our Everest MIC reduction to Adaboost."
        ],
        [
            "We can look at some advantages of our boost here.",
            "So first here is some idea from ensemble learning, so we can combine simple preferences to approximate."
        ],
        [
            "Complex cards.",
            "And second, it has the idea of thresholding, which can be sort of adaptively estimated scales to perform ordinal regression.",
            "Also, there are some interesting properties that it inherits from Adaboost.",
            "So for example, we can have simple implementation and guarantee on minimizing the margin loss here."
        ],
        [
            "I also observed that it's practically less vulnerable to overfitting, so also a similar property to add a boost."
        ],
        [
            "And the lesson we learned is useful.",
            "Properties are inherited with the reduction methods that we use here."
        ],
        [
            "Now I'm going to show you the result of some ordinal regression boosting experiments.",
            "So on the horizontal axis here there are eight real world datasets and vertical line.",
            "Here is the error.",
            "So basically the lower the better and blue one is ranked Boost algorithm which is which was the best algorithm for combining preferences to estimate the overall confidence and the green one is our our boost algorithm.",
            "And Red One is an SVM based algorithm which is state of the art algorithm in performing ordinal regression tasks.",
            "What we see here is the our post algorithm performs much better than rank boost.",
            "So if we see the results here, the blue the blue bars are usually much higher than the green bars."
        ],
        [
            "And we see that our post performs similar to SVM based algorithm.",
            "So in some datasets are books performs better in some datasets or boost performance worse and overall we don't see a significant difference in the datasets that we observed.",
            "However, notes that are both runs much faster than SVM based algorithm.",
            "So for example in this data set here there are 6000 training examples and SVM based algorithm.",
            "Which has a time consuming automatic parameter selection step would take more than four days to finish one run, but for or boost it takes only like an hour to finish."
        ],
        [
            "Run.",
            "And also we have similar results for another setting of our boost."
        ],
        [
            "So in conclusion, we proposed threshold on sample model which was shown to be useful for ordinal regression using this model with performance theoretical reduction and derive new large margin bounds.",
            "And we also perform algorithmic reduction which leads to new training algorithms called or."
        ],
        [
            "Host and or both has simplicity over existing boosting algorithms and comparable performance to state of the art algorithms, and it enjoys fast training and is less vulnerable to overfitting."
        ],
        [
            "We also have ongoing work that use similar reduction techniques for other theoretical and algorithmic results using more general loss functions."
        ],
        [
            "So thank you for your attention.",
            "Are there any questions?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you for staying for the last talk for today and this is Shannon Link.",
                    "label": 0
                },
                {
                    "sent": "From California Institute of Technology doing my PhD studies there and today, I'm going to talk about large margin source code it on samples for ordinal regression and this is joint work with my colleague Linley and will be presented in the algorithmic Learning Theory Conference this October.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's just start with an overview of this work is all about, so I copied Johns slide slide about reduction method and in this work we're actually doing reduction for interesting problem.",
                    "label": 0
                },
                {
                    "sent": "So junk taught us to do is first we need to identify the type of learning problem and we find the reduction and then.",
                    "label": 1
                },
                {
                    "sent": "Algorithm and then we can build a predictor.",
                    "label": 0
                },
                {
                    "sent": "So let's see what we do.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We identified the type of learning problem which is ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "And our pre made reduction would be the thresholded ensemble model.",
                    "label": 0
                },
                {
                    "sent": "An article learning algorithm is the famous Adaboost algorithm in on sample learning.",
                    "label": 1
                },
                {
                    "sent": "And then we build a regression rule using our newly designed algorithm called ordinal regression boosting plus the data.",
                    "label": 0
                },
                {
                    "sent": "We also do another part.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the reduction here, which I called a theoretical reduction.",
                    "label": 0
                },
                {
                    "sent": "So the first 2 steps are similar, but then we start by analyzing the non generalization bounds in binary classification, which are the large margin ensemble bonds and then using the reduction we're able to derive new bounds based on combining the reduction and the known bound.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think overview this work is actually a concrete instance of reductions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start by describing the ordinal regression problem and the problem with look look like this.",
                    "label": 0
                },
                {
                    "sent": "So what's the age group of the person in the picture?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The picture looked like this.",
                    "label": 0
                },
                {
                    "sent": "We would say the person in the picture is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Current and if the picture looked like this, we would say she's a child.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if the picture looked like this, we would say he's a teenager.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if the picture looked like this, we would say he's an adult, so we can represent this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Age curves by a finite order set of labels, which we call the ranks, and we assume that they are from one to K.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can replace this age groups by this numbers and we hope to find an ordinal regression.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that correctly tells us the unknown picture is offering to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the ordinary regression problem.",
                    "label": 0
                },
                {
                    "sent": "We're given a training set like a bunch of pictures and their associated labels, and we want to find a decision function that creates the rings of unseen.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example as well.",
                    "label": 0
                },
                {
                    "sent": "So there are many applications of this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "For example, we can rent movies like say a movies very bad, bad, OK, good or very good or we can rank by Doc.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coments relevance so because this idea of ranking or early no regression matches human preferences.",
                    "label": 0
                },
                {
                    "sent": "It has many applications in social science in information retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the ordinal regression problem in more detail.",
                    "label": 1
                },
                {
                    "sent": "We can think of it as a regression problem, but we found the metric information in our previous example about the age group.",
                    "label": 0
                },
                {
                    "sent": "There's possibly a metric underlying like the age, but it's not encoded in the ranks 1234.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also think of it as a classification problem with order categories.",
                    "label": 0
                },
                {
                    "sent": "What I mean by ordered is that it will be a small mistake to classify a teenager as a child because very rings are close by, but it would be a big mistake to say that an infant is an adult.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some common loss functions to describe this properties of ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "First, it's kind of a multi class classification problem, so we can use the classification error to measure the performance.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Determine the category or another commonly used loss function is.",
                    "label": 0
                },
                {
                    "sent": "We want to have a close prediction in the category so we can use the absolute error to measure the performance.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing urgent or regression rule.",
                    "label": 0
                },
                {
                    "sent": "In this talk, because of the time, I'll talk about the absolute error only and similar results happening as can be derived for the classification cost.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first I'll introduce the thresholded model for ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "This model is widely used in previous algorithm designs.",
                    "label": 1
                },
                {
                    "sent": "We start by thinking about a knife algorithm for ordinary regression.",
                    "label": 0
                },
                {
                    "sent": "So first we can just treat the rings as general labels and just do a general correct regression.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimated function.",
                    "label": 0
                },
                {
                    "sent": "And then we round up the regression resource we get for exactly if we get 3.4, we round it to three, or if we get 1.8 we rounded to two hour final prediction.",
                    "label": 0
                },
                {
                    "sent": "This method has two prop.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points.",
                    "label": 0
                },
                {
                    "sent": "So first we said that the ordinal regression problem can be solved as regression without metric and general regression algorithms performs badly without the match.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "And second, the round of operation here, which is essentially a uniform quantization, can cause large error because of the.",
                    "label": 1
                },
                {
                    "sent": "Skewed distribution underline.",
                    "label": 0
                },
                {
                    "sent": "So improved and generalized algorithm that people often use is first to estimate a potential function F of X that hopefully take care of the order of ribs.",
                    "label": 1
                },
                {
                    "sent": "And then they quantized potential function by some ordered thresholds to get the final rule.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in illustration of this model, which is called the thresholded model is like this.",
                    "label": 0
                },
                {
                    "sent": "So if we first map everything by this potential function and then if the potential value falls between likes it a one seater two we predict is a threat two if it's between Season 2 and season three we predict US rank 3.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our proposed thresholded ensemble model, the potential function at X is computed by a weighted on sample.",
                    "label": 0
                },
                {
                    "sent": "So basically we can consider like an ensemble of T hypothesis and they have associated weights.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The intuition of this model is we want to combine preferences from for every individual hypothesis to estimate the overall confidence.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example of this intuition is, say if many people we call them HT say that a movie is good, then if we combine them by positive weights, the confidence of the movie should be high.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our results here, the hypothesis can be binary, multi valued or continuous, basically reflecting.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The preferences and also we allow the weights to be positive or negative so we can reverse the bad preferences.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And essentially the source loaded on sample model is an extension of ensemble learning in the area of ordinal regression.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we go into deriving the theoretical results of 1st loaded Ensemble model, we first talk about an important concept called the margin and the idea of the margin is that we want to be saved from the boundaries.",
                    "label": 0
                },
                {
                    "sent": "So for example for this example here we want its value to be away from this.",
                    "label": 0
                },
                {
                    "sent": "So the thresholds here.",
                    "label": 0
                },
                {
                    "sent": "This results here and this result.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can define the margin like this, which basically captures the distance between the potential value and this result for each threshold here.",
                    "label": 0
                },
                {
                    "sent": "Singer.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the case of binary classification, a negative margin would indicate around prediction.",
                    "label": 0
                },
                {
                    "sent": "So important observation here is that if for a pair of example XY, if we sum all its margin violations, then we basically get the term we want to use in the absolute loss.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now comes our first results using the reduction, so the reduction is based on this observation.",
                    "label": 0
                },
                {
                    "sent": "So if we look at.",
                    "label": 0
                },
                {
                    "sent": "Or you know regression problem here, but we only look at one source code.",
                    "label": 0
                },
                {
                    "sent": "We can see that it's very similar to a binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "We want these examples to be on one side of the first hold, and these examples to be on the other side of this result.",
                    "label": 0
                },
                {
                    "sent": "And we have K -- 1 thresholds here.",
                    "label": 0
                },
                {
                    "sent": "So basically have K -- 1 binary classification problems.",
                    "label": 1
                },
                {
                    "sent": "Now we can use one classifier trick that jump this yesterday.",
                    "label": 0
                },
                {
                    "sent": "And basically we encode the same here.",
                    "label": 0
                },
                {
                    "sent": "Can have one classifier there.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can apply the bounds in binary classification.",
                    "label": 0
                },
                {
                    "sent": "So this is a famous founding binary classification that basically says OK if you have IID samples here and you're out of sample error can be bounded by the example margin violation and some complexity terms.",
                    "label": 0
                },
                {
                    "sent": "So this basically tells us that if I can guarantee that I have a large margin classifiers, the Delta term can be set large.",
                    "label": 0
                },
                {
                    "sent": "Then this bound can be like small.",
                    "label": 0
                },
                {
                    "sent": "And basically the generalization error can be bounded well.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the analysis found that we describe based on the reduction.",
                    "label": 0
                },
                {
                    "sent": "So in ordinal regression with similar settings, if we consider the thresholded ensembles then basically we have a similar bound, so this here is the out of sample absolute error and here we have the number of margin violations and here we have a complexity term.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which tells us that large margin shares audit on samples can generalize.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in addition to the theoretic reduction, I also mentioned that we do have rhythmic reduction to edibles, so I think we may learn more about edibles in the next week.",
                    "label": 1
                },
                {
                    "sent": "But all we need to know now is edibles is the binary classification algorithm that can be explained by operationally optimizing this term, which is kind of maximizing the minimum margin here.",
                    "label": 1
                },
                {
                    "sent": "So we derive the large margin bound here in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So basically we want to maximize the minimum.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In some way, so this is our formulation.",
                    "label": 0
                },
                {
                    "sent": "Basically we have two formulations, but maybe this one would be similar to look at.",
                    "label": 0
                },
                {
                    "sent": "So basically we try to minimize the summation of exponential negative margins, which if we look at here we are doing the next moment maximization of minimum.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is basically our Everest MIC reduction to Adaboost.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can look at some advantages of our boost here.",
                    "label": 0
                },
                {
                    "sent": "So first here is some idea from ensemble learning, so we can combine simple preferences to approximate.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Complex cards.",
                    "label": 0
                },
                {
                    "sent": "And second, it has the idea of thresholding, which can be sort of adaptively estimated scales to perform ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "Also, there are some interesting properties that it inherits from Adaboost.",
                    "label": 1
                },
                {
                    "sent": "So for example, we can have simple implementation and guarantee on minimizing the margin loss here.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I also observed that it's practically less vulnerable to overfitting, so also a similar property to add a boost.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the lesson we learned is useful.",
                    "label": 0
                },
                {
                    "sent": "Properties are inherited with the reduction methods that we use here.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to show you the result of some ordinal regression boosting experiments.",
                    "label": 0
                },
                {
                    "sent": "So on the horizontal axis here there are eight real world datasets and vertical line.",
                    "label": 0
                },
                {
                    "sent": "Here is the error.",
                    "label": 0
                },
                {
                    "sent": "So basically the lower the better and blue one is ranked Boost algorithm which is which was the best algorithm for combining preferences to estimate the overall confidence and the green one is our our boost algorithm.",
                    "label": 0
                },
                {
                    "sent": "And Red One is an SVM based algorithm which is state of the art algorithm in performing ordinal regression tasks.",
                    "label": 0
                },
                {
                    "sent": "What we see here is the our post algorithm performs much better than rank boost.",
                    "label": 0
                },
                {
                    "sent": "So if we see the results here, the blue the blue bars are usually much higher than the green bars.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we see that our post performs similar to SVM based algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in some datasets are books performs better in some datasets or boost performance worse and overall we don't see a significant difference in the datasets that we observed.",
                    "label": 0
                },
                {
                    "sent": "However, notes that are both runs much faster than SVM based algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for example in this data set here there are 6000 training examples and SVM based algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which has a time consuming automatic parameter selection step would take more than four days to finish one run, but for or boost it takes only like an hour to finish.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run.",
                    "label": 0
                },
                {
                    "sent": "And also we have similar results for another setting of our boost.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we proposed threshold on sample model which was shown to be useful for ordinal regression using this model with performance theoretical reduction and derive new large margin bounds.",
                    "label": 0
                },
                {
                    "sent": "And we also perform algorithmic reduction which leads to new training algorithms called or.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Host and or both has simplicity over existing boosting algorithms and comparable performance to state of the art algorithms, and it enjoys fast training and is less vulnerable to overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have ongoing work that use similar reduction techniques for other theoretical and algorithmic results using more general loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}