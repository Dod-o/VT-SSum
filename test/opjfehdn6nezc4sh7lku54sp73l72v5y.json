{
    "id": "opjfehdn6nezc4sh7lku54sp73l72v5y",
    "title": "Dense message passing for sparse principal component analysis",
    "info": {
        "author": [
            "Kevin Sharp, School of Computer Science, University of Manchester"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2010_sharp_dmpfs/",
    "segmentation": [
        [
            "OK, hi everybody, I'm Kevin Sharp.",
            "This is joint work with my PhD supervisor at the University of Manchester.",
            "Yes, there's a lot of going on in the middle row.",
            "There were going to say actually that I was filling in for my student 'cause he had a hangover.",
            "And my my PhD supervisor, Magnus Rattray."
        ],
        [
            "So I'm going to talk about a deterministic approximate inference method for performing sparse Bayesian PCA with a zero norm prior, so the algorithm is a form of message passing and it's on a densely connected graphical model, and this technique draws inspiration from ideas in statistical mechanics.",
            "So I'll just begin by briefly mot."
        ],
        [
            "Inviting the work.",
            "Then I'll describe the algorithm and I'll briefly indicate the connection with statistical mechanics.",
            "I'll present some results, not all of which are in the paper, and then I'll summarize and indicate directions in which we're trying to extend the work.",
            "So the original motivating application was a biological problem and we're looking at factor analysis models of gene regulation.",
            "So the data we have a microarray datasets which typically consists of thousands of genes.",
            "So expression levels of thousands of genes over a much smaller number of experimental conditions.",
            "So we're very much in the large P small end regime, and.",
            "A particular feature of the problem is that the explanatory factors have sparse loadings.",
            "There's genuinely sparse, so in a Bayesian context we want to model this with the factor analysis model with the sparsity inducing prior, and we'd like that prior to truly reflect our belief, so we'd like it to assign finite probability mass to truly sparse solutions.",
            "So unlikely sort of very popular shrinkage priors as zero norm prior has this nice property.",
            "And it also has the property that if you have prior knowledge about network structure then it offers a very natural way to encode that.",
            "Unfortunately, the problem with zero norm priors is that they lead to highly multimodal posteriors, so this is very difficult for inference.",
            "MCMC methods tend to be prohibitively slow and variational or standard variational methods such as variational Bayes approach to trapping in local optimum.",
            "So we're looking for a different solution."
        ],
        [
            "So just to be concrete, we have a likelihood model which is probabilistic PCA.",
            "So the NTH data point.",
            "Why would be generated by a two stage process where we first draw a latent variable X from a zero mean unit variance Gaussian distribution.",
            "We multiply it by the sparse parameter vector W and we add some Gaussian noise.",
            "This is preliminary work, so we're restricting ourselves to the case of a single factor.",
            "And for this description, just to simplify it, we assume that the noise is unit variance.",
            "So from this model we integrate out the latent factor to obtain this likelihood."
        ],
        [
            "Now the implementation zero Norm prior is a so called spike and slab prior.",
            "So the spike is a Delta function and the slab is a broad Gaussian with precision Lambda and each parameter is drawn from this mixture distribution where the probability of drawing A0 or nonzero parameter is determined by C. OK, so we make a couple of observation."
        ],
        [
            "Things about this prime.",
            "The first is that there's a standard way in which we can express it in a factorized form by introducing a binary variable Z.",
            "So now each of the parameters that we labeled W in the previous slide is expressed as the product of Z with another real valued variable V. And it's important to do this for the development of our algorithm."
        ],
        [
            "More importantly.",
            "We observe that this prior has certain properties in high dimensions.",
            "So firstly we know that these binary variables are distributed under Byrne dairy distribution with parameters C. So that means that the total number of non zero parameters is binomially distributed and in high dimension this distribution will be very highly peaked about C. If we look at the non 0."
        ],
        [
            "Parameters so they are Gaussian distributed and the independently distributed.",
            "So the length of the parameter vector.",
            "We can make a central limit theorem argument to say that that is approximately Gaussian distributed in high dimension, and we can go further and we can say that because in high dimensions nearly all of the probability mass of a Gaussian is located in a thin spherical shell, that this distribution is approximately spherical."
        ],
        [
            "So the conclusion is that we can approximate this bike and slab prior by a constraint based prior, and we can express that in this way with Dirac Delta functions and this is very useful for the development of this algorithm.",
            "So."
        ],
        [
            "We have a factor graph representation of our posterior, so the I'm sure you're all familiar with factor graphs.",
            "The circular nodes secular circles represent variable nodes, and so the top we've got nodes representing the data points indexed by little N and at the bottom we've got circles representing the parameters and the square factor nodes each correspond to a likelihood term corresponding to the associated data point, why?",
            "And we'd like to do inference here using belief propagation, so I guess again, most of you are familiar with it.",
            "If anybody is not familiar with belief propagation, the basic idea is that we want to compute a global function on this graph efficiently by splitting the computation into a set of smaller local computations, and the results of those are passed from node to node as messages.",
            "So we have two types of messages, those indexed those.",
            "The left hand side, which have passed from variable to factor nodes and those on the right hand side which are passed from factor to variable nodes.",
            "Can everybody hear me?",
            "I think I might be drifting in and out of the microphone.",
            "Is it OK?",
            "Yeah, OK, so belief propagation is is exact on a tree line graph.",
            "It's known to give good solutions in loopy graphs when the graph is locally treelike, but in this sort of graphical model, it's not clear that it would give any sensible solution.",
            "But to make progress?"
        ],
        [
            "We consider the cavity distribution that we obtain if we remove a single data point.",
            "And we assume that the parameters are sufficiently weakly converge that re introduction of this data point only causes a slight perturbation in the joint distribution."
        ],
        [
            "And then we passed messages in the standard belief propagation way, so we have messages are passed from a factor to a parameter node that combine information from the reintroduced factor.",
            "That's the F in the middle of that equation.",
            "The bottom with the information from all other parameter nodes.",
            "That's the the product over the messages.",
            "At the end there.",
            "And we passed messages in the other direction from variable to fact and no."
        ],
        [
            "It's which combine information from all the factors or messages from the factors coming into that single parameter node with the prior over that parameter."
        ],
        [
            "And we iterate this and after T iterations we have an approximation to the posterior marginal belief which we obtain just by combining information from all incoming factors with the prior over that parameter, and we normalize."
        ],
        [
            "Now.",
            "Unfortunately, there are two problems with this scheme.",
            "The first of those is that the expression for the factor to variable nodes is extremely hard to compute, not least because it involves a sum over 2 to the power PP dimensional binary variables.",
            "The second problem is that we just don't expect that belief propagation would converge to a sensible answer on such a densely connected graph.",
            "So to solve these problems, we do two things to solve.",
            "The first problem we."
        ],
        [
            "Select the high dimensionality of the problem and we invoke a Gaussian approximation to render those message updates tractable, which I'll explain shortly.",
            "And to solve the convergence problem we use the constraint based view of the prior that I mentioned earlier to enforce sparsity and length constraints self consistently at each iteration.",
            "So this is based on some work by two Japanese physicists would and kabashima the references at the bottom there who developed a sparse Bayesian classifier using similar ideas."
        ],
        [
            "So just to briefly describe the Gaussian approximation, we begin by noticing that you can re express the likelihood in terms of a sum over P independent terms.",
            "So the Delta end there represents this sum and we make a central limit theorem argument that for large P we can make a Gaussian approximation for this."
        ],
        [
            "So we can replace this Delta an in this way we separate out the term depending on the elf parameter to which we're passing the message, and we replace the remaining some by this Gaussian approximation with BMS represent the means under the current cavity distribution for these parameters and the term V at the end there represents variance."
        ],
        [
            "And this variance we can also derive an approximation for again using Central limit theorem arguments.",
            "We notice again that it depends on the sum of P independent terms, so these these wise here are data terms.",
            "They represent different elements of the NTH data vector.",
            "And so again for large P using the central limit theorem argument, we can assume that the fluctuations in this quantity V are small in the order of one over root P, and so we say that this this quantity is approximately self averaging I.",
            "It tends towards it sample mean independently of the data set.",
            "And so that means that we can write it in terms of the constrained length of the parameter vector and the posterior mean belief of the marginals."
        ],
        [
            "So the second key part of the algorithm.",
            "Is the implementation of this constraint based prime?",
            "And to do this we we re parameterized the spike and slam prior in the way that you see in the top equation there.",
            "So we introduced 2 new new hyperparameters gamma which is a sparsity hyperparameter and G hyperparameters to control the length of the vector.",
            "And.",
            "We are."
        ],
        [
            "Just those at each iteration.",
            "So as we satisfy the constraint prior on average in this way that we've expressed here."
        ],
        [
            "It's important to note the convergence.",
            "The values of G and gamma are not no longer there prior values, but the prior constraint is satisfied.",
            "Anne, this is."
        ],
        [
            "Consistent with the statistical mechanics theory that you can apply to this problem."
        ],
        [
            "So I'm not going to say very much tool about the statistical mechanics theory, just briefly to indicate what it enables us to do.",
            "It's a method for calculating the average over all possible data sets of the log of the marginal likelihood in the so-called thermodynamic limit.",
            "So where both the dimension apyan the number of samples, N tends to Infinity, but they do so in a fixed ratio Alpha.",
            "It's a it's not a mathematically rigorous technique, but it is a useful tool."
        ],
        [
            "And what it enables us to do is for firstly we can derive expressions for the length of the squared posterior mean vector.",
            "And we can also derive an expression for its overlap with the true parameter vector.",
            "We can also show that the algorithm updates are consistent with the replica analysis.",
            "And finally price.",
            "Most importantly, it provides us with the means of comparing the performance of the algorithm with the theory.",
            "We do this by looking at the cosine angle Rd in this equation, problem here.",
            "So the cosine angle between the posterior mean vector and the true vector.",
            "So if we do this."
        ],
        [
            "What do we observe?",
            "So we've plotted here on the vertical axis, the cosine angle between in Ferd, posterior mean vector and the truth on the bottom axis is the degree of sparsity.",
            "And the solid lines represent the theoretical optimum performance calculated from the statistical mechanics theory.",
            "Each of the different colors represents a different value of Alpha.",
            "This ratio between number of samples and the dimension of the problem and the lower the value of Alpha, the harder the inference problem essentially.",
            "Now on the right hand side, we've got a plot to show and give something.",
            "Really, we did this just as a validation of the theoretical calculation.",
            "But if we look at the left hand plot, we can see that the approximate inference method out dense message passing performs pretty well in comparison to the optimal theoretical performance shows generally the right behavior and it peaks at the correct level of sparsity, which in this case was .1."
        ],
        [
            "So we also compared the method to two other relatively recently published methods.",
            "The first is.",
            "EMP ECA, which was method of Sagan Buhman from ICL 2008.",
            "This method is ultimately based on L1 regularization.",
            "We can see the from the right hand plot that it doesn't really perform quite as well compared to theory, and it consistently tends to overestimate the level of sparsity."
        ],
        [
            "The sparse PCA algorithm of zoo hastiin tips, Ronnie does rather better against this synthetic data, but again, it's slightly less optimal than the dense message passing.",
            "So we also."
        ],
        [
            "Compared the performance on a couple of real datasets, these are real gene expression datasets.",
            "The values the dimension of the number of samples from these micro radio sets given the bottom and these were two datasets that we used in the original valuations of the MPC ASPCA algorithms and we can see that in both cases the message passing algorithm outperforms the sparse PCA.",
            "I should say, sorry, I should have mentioned that the vertical axis here measures the.",
            "Fraction of variance explained by the sparse principal component relative to that explained by the full non sparse solution and it's sort of standard measure.",
            "I'm sure you're aware of measuring these things and the performance compared to SPCA is.",
            "Pretty good and the performance relative to EMP."
        ],
        [
            "EA is comfortable.",
            "One advantage obviously to using Bayesian methods, is that we hope we can get an estimate of the marginal likelihood so that we can do hyperparameter estimation and perhaps do model averaging or model selection and an estimate of the marginal likelihood is quite sort of readily available from the algorithm, and we can plot for any given data set, we can put contours of this as we vary the two.",
            "Hyper parameters so on the bottom again we have the sparsity hyperparameters see control the level sparsity and on the vertical axis we're following the ratio between the inverse width of the constraint prior and the true inverse width.",
            "And we can see if we compare to the plot of Gibbs sampling, where we've got a posterior over the joint posterior over those two hyper parameters that we get quite a good correspondence.",
            "The mode is in approximately the same place, and there is a single as a single mode.",
            "It's unimodal, so this holds out good possibility that we can use this method to do some efficient hyperparameter estimation."
        ],
        [
            "So, just to summarize, we've presented here a novel message passing algorithm for sparse Bayesian PCA in high dimension."
        ],
        [
            "Ends the message.",
            "Updates are rendered tractable by a Gaussian proxy."
        ],
        [
            "Nation.",
            "We achieve convergence by posing some consistency requirements derived from statistical mechanics."
        ],
        [
            "Theory.",
            "And inference of posterior marginals exhibits near optimal performance compared to this theory."
        ],
        [
            "We outperformed two other recently published algorithms on Synthetic data."
        ],
        [
            "And a couple of real datasets.",
            "And we also have an approximation to the marginal likelihood."
        ],
        [
            "So for the future the the two obvious directions to to develop this work of firstly to use the estimate the marginal likelihood for hyperparameter estimation."
        ],
        [
            "And Secondly, we would like to extend this to multiple factors.",
            "For the case when the factors can be assumed to be orthogonal, this should be relatively straightforward, although it will require require efficient estimation of hyperparameters.",
            "But for the non orthogonal case this is really a subject of ongoing research."
        ],
        [
            "So thank you very much for listening, and if anybody is sufficiently interested, there is code available on this website and you can download it and reproduce the results in the paper.",
            "Thank you.",
            "Observation."
        ],
        [
            "Rigor, the physicists at these things, that statistical mechanics have been rigorous for about a century or more, right?",
            "Application but statistical mechanics really is mailed down.",
            "Yeah, I don't.",
            "I don't think I don't think anybody claims that the replica analysis is wholly rigorous.",
            "I don't think I'm not sure if there's any case where they've actually shown it so rigorously.",
            "So did you think that running Max product instead of some product?",
            "Will that be easier with this model?",
            "We didn't think about it.",
            "I. I'm not sure why.",
            "Why do you ask that for this model?",
            "Point proximation with yeah, thinking maybe that would be computationally easier to maximize rather than some.",
            "It would be meaningful because it would be the maximum of the posterior right?",
            "Yeah, I mean, I guess we were after him as fully Bayesian approaches we could get.",
            "So the short answer is no, we didn't look at it at all.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, hi everybody, I'm Kevin Sharp.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my PhD supervisor at the University of Manchester.",
                    "label": 1
                },
                {
                    "sent": "Yes, there's a lot of going on in the middle row.",
                    "label": 0
                },
                {
                    "sent": "There were going to say actually that I was filling in for my student 'cause he had a hangover.",
                    "label": 0
                },
                {
                    "sent": "And my my PhD supervisor, Magnus Rattray.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about a deterministic approximate inference method for performing sparse Bayesian PCA with a zero norm prior, so the algorithm is a form of message passing and it's on a densely connected graphical model, and this technique draws inspiration from ideas in statistical mechanics.",
                    "label": 0
                },
                {
                    "sent": "So I'll just begin by briefly mot.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inviting the work.",
                    "label": 0
                },
                {
                    "sent": "Then I'll describe the algorithm and I'll briefly indicate the connection with statistical mechanics.",
                    "label": 0
                },
                {
                    "sent": "I'll present some results, not all of which are in the paper, and then I'll summarize and indicate directions in which we're trying to extend the work.",
                    "label": 0
                },
                {
                    "sent": "So the original motivating application was a biological problem and we're looking at factor analysis models of gene regulation.",
                    "label": 0
                },
                {
                    "sent": "So the data we have a microarray datasets which typically consists of thousands of genes.",
                    "label": 0
                },
                {
                    "sent": "So expression levels of thousands of genes over a much smaller number of experimental conditions.",
                    "label": 0
                },
                {
                    "sent": "So we're very much in the large P small end regime, and.",
                    "label": 1
                },
                {
                    "sent": "A particular feature of the problem is that the explanatory factors have sparse loadings.",
                    "label": 1
                },
                {
                    "sent": "There's genuinely sparse, so in a Bayesian context we want to model this with the factor analysis model with the sparsity inducing prior, and we'd like that prior to truly reflect our belief, so we'd like it to assign finite probability mass to truly sparse solutions.",
                    "label": 1
                },
                {
                    "sent": "So unlikely sort of very popular shrinkage priors as zero norm prior has this nice property.",
                    "label": 0
                },
                {
                    "sent": "And it also has the property that if you have prior knowledge about network structure then it offers a very natural way to encode that.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the problem with zero norm priors is that they lead to highly multimodal posteriors, so this is very difficult for inference.",
                    "label": 0
                },
                {
                    "sent": "MCMC methods tend to be prohibitively slow and variational or standard variational methods such as variational Bayes approach to trapping in local optimum.",
                    "label": 0
                },
                {
                    "sent": "So we're looking for a different solution.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to be concrete, we have a likelihood model which is probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "So the NTH data point.",
                    "label": 1
                },
                {
                    "sent": "Why would be generated by a two stage process where we first draw a latent variable X from a zero mean unit variance Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "We multiply it by the sparse parameter vector W and we add some Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "This is preliminary work, so we're restricting ourselves to the case of a single factor.",
                    "label": 0
                },
                {
                    "sent": "And for this description, just to simplify it, we assume that the noise is unit variance.",
                    "label": 0
                },
                {
                    "sent": "So from this model we integrate out the latent factor to obtain this likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the implementation zero Norm prior is a so called spike and slab prior.",
                    "label": 0
                },
                {
                    "sent": "So the spike is a Delta function and the slab is a broad Gaussian with precision Lambda and each parameter is drawn from this mixture distribution where the probability of drawing A0 or nonzero parameter is determined by C. OK, so we make a couple of observation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things about this prime.",
                    "label": 0
                },
                {
                    "sent": "The first is that there's a standard way in which we can express it in a factorized form by introducing a binary variable Z.",
                    "label": 0
                },
                {
                    "sent": "So now each of the parameters that we labeled W in the previous slide is expressed as the product of Z with another real valued variable V. And it's important to do this for the development of our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More importantly.",
                    "label": 0
                },
                {
                    "sent": "We observe that this prior has certain properties in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So firstly we know that these binary variables are distributed under Byrne dairy distribution with parameters C. So that means that the total number of non zero parameters is binomially distributed and in high dimension this distribution will be very highly peaked about C. If we look at the non 0.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameters so they are Gaussian distributed and the independently distributed.",
                    "label": 0
                },
                {
                    "sent": "So the length of the parameter vector.",
                    "label": 0
                },
                {
                    "sent": "We can make a central limit theorem argument to say that that is approximately Gaussian distributed in high dimension, and we can go further and we can say that because in high dimensions nearly all of the probability mass of a Gaussian is located in a thin spherical shell, that this distribution is approximately spherical.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the conclusion is that we can approximate this bike and slab prior by a constraint based prior, and we can express that in this way with Dirac Delta functions and this is very useful for the development of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a factor graph representation of our posterior, so the I'm sure you're all familiar with factor graphs.",
                    "label": 0
                },
                {
                    "sent": "The circular nodes secular circles represent variable nodes, and so the top we've got nodes representing the data points indexed by little N and at the bottom we've got circles representing the parameters and the square factor nodes each correspond to a likelihood term corresponding to the associated data point, why?",
                    "label": 0
                },
                {
                    "sent": "And we'd like to do inference here using belief propagation, so I guess again, most of you are familiar with it.",
                    "label": 0
                },
                {
                    "sent": "If anybody is not familiar with belief propagation, the basic idea is that we want to compute a global function on this graph efficiently by splitting the computation into a set of smaller local computations, and the results of those are passed from node to node as messages.",
                    "label": 0
                },
                {
                    "sent": "So we have two types of messages, those indexed those.",
                    "label": 0
                },
                {
                    "sent": "The left hand side, which have passed from variable to factor nodes and those on the right hand side which are passed from factor to variable nodes.",
                    "label": 0
                },
                {
                    "sent": "Can everybody hear me?",
                    "label": 0
                },
                {
                    "sent": "I think I might be drifting in and out of the microphone.",
                    "label": 0
                },
                {
                    "sent": "Is it OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so belief propagation is is exact on a tree line graph.",
                    "label": 0
                },
                {
                    "sent": "It's known to give good solutions in loopy graphs when the graph is locally treelike, but in this sort of graphical model, it's not clear that it would give any sensible solution.",
                    "label": 0
                },
                {
                    "sent": "But to make progress?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We consider the cavity distribution that we obtain if we remove a single data point.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the parameters are sufficiently weakly converge that re introduction of this data point only causes a slight perturbation in the joint distribution.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we passed messages in the standard belief propagation way, so we have messages are passed from a factor to a parameter node that combine information from the reintroduced factor.",
                    "label": 0
                },
                {
                    "sent": "That's the F in the middle of that equation.",
                    "label": 0
                },
                {
                    "sent": "The bottom with the information from all other parameter nodes.",
                    "label": 0
                },
                {
                    "sent": "That's the the product over the messages.",
                    "label": 0
                },
                {
                    "sent": "At the end there.",
                    "label": 0
                },
                {
                    "sent": "And we passed messages in the other direction from variable to fact and no.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's which combine information from all the factors or messages from the factors coming into that single parameter node with the prior over that parameter.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we iterate this and after T iterations we have an approximation to the posterior marginal belief which we obtain just by combining information from all incoming factors with the prior over that parameter, and we normalize.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there are two problems with this scheme.",
                    "label": 0
                },
                {
                    "sent": "The first of those is that the expression for the factor to variable nodes is extremely hard to compute, not least because it involves a sum over 2 to the power PP dimensional binary variables.",
                    "label": 0
                },
                {
                    "sent": "The second problem is that we just don't expect that belief propagation would converge to a sensible answer on such a densely connected graph.",
                    "label": 0
                },
                {
                    "sent": "So to solve these problems, we do two things to solve.",
                    "label": 0
                },
                {
                    "sent": "The first problem we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Select the high dimensionality of the problem and we invoke a Gaussian approximation to render those message updates tractable, which I'll explain shortly.",
                    "label": 0
                },
                {
                    "sent": "And to solve the convergence problem we use the constraint based view of the prior that I mentioned earlier to enforce sparsity and length constraints self consistently at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So this is based on some work by two Japanese physicists would and kabashima the references at the bottom there who developed a sparse Bayesian classifier using similar ideas.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to briefly describe the Gaussian approximation, we begin by noticing that you can re express the likelihood in terms of a sum over P independent terms.",
                    "label": 0
                },
                {
                    "sent": "So the Delta end there represents this sum and we make a central limit theorem argument that for large P we can make a Gaussian approximation for this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can replace this Delta an in this way we separate out the term depending on the elf parameter to which we're passing the message, and we replace the remaining some by this Gaussian approximation with BMS represent the means under the current cavity distribution for these parameters and the term V at the end there represents variance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this variance we can also derive an approximation for again using Central limit theorem arguments.",
                    "label": 0
                },
                {
                    "sent": "We notice again that it depends on the sum of P independent terms, so these these wise here are data terms.",
                    "label": 0
                },
                {
                    "sent": "They represent different elements of the NTH data vector.",
                    "label": 1
                },
                {
                    "sent": "And so again for large P using the central limit theorem argument, we can assume that the fluctuations in this quantity V are small in the order of one over root P, and so we say that this this quantity is approximately self averaging I.",
                    "label": 0
                },
                {
                    "sent": "It tends towards it sample mean independently of the data set.",
                    "label": 0
                },
                {
                    "sent": "And so that means that we can write it in terms of the constrained length of the parameter vector and the posterior mean belief of the marginals.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second key part of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is the implementation of this constraint based prime?",
                    "label": 0
                },
                {
                    "sent": "And to do this we we re parameterized the spike and slam prior in the way that you see in the top equation there.",
                    "label": 0
                },
                {
                    "sent": "So we introduced 2 new new hyperparameters gamma which is a sparsity hyperparameter and G hyperparameters to control the length of the vector.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just those at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So as we satisfy the constraint prior on average in this way that we've expressed here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's important to note the convergence.",
                    "label": 0
                },
                {
                    "sent": "The values of G and gamma are not no longer there prior values, but the prior constraint is satisfied.",
                    "label": 0
                },
                {
                    "sent": "Anne, this is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consistent with the statistical mechanics theory that you can apply to this problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not going to say very much tool about the statistical mechanics theory, just briefly to indicate what it enables us to do.",
                    "label": 0
                },
                {
                    "sent": "It's a method for calculating the average over all possible data sets of the log of the marginal likelihood in the so-called thermodynamic limit.",
                    "label": 0
                },
                {
                    "sent": "So where both the dimension apyan the number of samples, N tends to Infinity, but they do so in a fixed ratio Alpha.",
                    "label": 0
                },
                {
                    "sent": "It's a it's not a mathematically rigorous technique, but it is a useful tool.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what it enables us to do is for firstly we can derive expressions for the length of the squared posterior mean vector.",
                    "label": 0
                },
                {
                    "sent": "And we can also derive an expression for its overlap with the true parameter vector.",
                    "label": 0
                },
                {
                    "sent": "We can also show that the algorithm updates are consistent with the replica analysis.",
                    "label": 0
                },
                {
                    "sent": "And finally price.",
                    "label": 0
                },
                {
                    "sent": "Most importantly, it provides us with the means of comparing the performance of the algorithm with the theory.",
                    "label": 0
                },
                {
                    "sent": "We do this by looking at the cosine angle Rd in this equation, problem here.",
                    "label": 0
                },
                {
                    "sent": "So the cosine angle between the posterior mean vector and the true vector.",
                    "label": 0
                },
                {
                    "sent": "So if we do this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do we observe?",
                    "label": 0
                },
                {
                    "sent": "So we've plotted here on the vertical axis, the cosine angle between in Ferd, posterior mean vector and the truth on the bottom axis is the degree of sparsity.",
                    "label": 0
                },
                {
                    "sent": "And the solid lines represent the theoretical optimum performance calculated from the statistical mechanics theory.",
                    "label": 1
                },
                {
                    "sent": "Each of the different colors represents a different value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "This ratio between number of samples and the dimension of the problem and the lower the value of Alpha, the harder the inference problem essentially.",
                    "label": 0
                },
                {
                    "sent": "Now on the right hand side, we've got a plot to show and give something.",
                    "label": 0
                },
                {
                    "sent": "Really, we did this just as a validation of the theoretical calculation.",
                    "label": 0
                },
                {
                    "sent": "But if we look at the left hand plot, we can see that the approximate inference method out dense message passing performs pretty well in comparison to the optimal theoretical performance shows generally the right behavior and it peaks at the correct level of sparsity, which in this case was .1.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also compared the method to two other relatively recently published methods.",
                    "label": 0
                },
                {
                    "sent": "The first is.",
                    "label": 0
                },
                {
                    "sent": "EMP ECA, which was method of Sagan Buhman from ICL 2008.",
                    "label": 0
                },
                {
                    "sent": "This method is ultimately based on L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "We can see the from the right hand plot that it doesn't really perform quite as well compared to theory, and it consistently tends to overestimate the level of sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sparse PCA algorithm of zoo hastiin tips, Ronnie does rather better against this synthetic data, but again, it's slightly less optimal than the dense message passing.",
                    "label": 0
                },
                {
                    "sent": "So we also.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared the performance on a couple of real datasets, these are real gene expression datasets.",
                    "label": 0
                },
                {
                    "sent": "The values the dimension of the number of samples from these micro radio sets given the bottom and these were two datasets that we used in the original valuations of the MPC ASPCA algorithms and we can see that in both cases the message passing algorithm outperforms the sparse PCA.",
                    "label": 0
                },
                {
                    "sent": "I should say, sorry, I should have mentioned that the vertical axis here measures the.",
                    "label": 0
                },
                {
                    "sent": "Fraction of variance explained by the sparse principal component relative to that explained by the full non sparse solution and it's sort of standard measure.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you're aware of measuring these things and the performance compared to SPCA is.",
                    "label": 0
                },
                {
                    "sent": "Pretty good and the performance relative to EMP.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "EA is comfortable.",
                    "label": 0
                },
                {
                    "sent": "One advantage obviously to using Bayesian methods, is that we hope we can get an estimate of the marginal likelihood so that we can do hyperparameter estimation and perhaps do model averaging or model selection and an estimate of the marginal likelihood is quite sort of readily available from the algorithm, and we can plot for any given data set, we can put contours of this as we vary the two.",
                    "label": 0
                },
                {
                    "sent": "Hyper parameters so on the bottom again we have the sparsity hyperparameters see control the level sparsity and on the vertical axis we're following the ratio between the inverse width of the constraint prior and the true inverse width.",
                    "label": 0
                },
                {
                    "sent": "And we can see if we compare to the plot of Gibbs sampling, where we've got a posterior over the joint posterior over those two hyper parameters that we get quite a good correspondence.",
                    "label": 0
                },
                {
                    "sent": "The mode is in approximately the same place, and there is a single as a single mode.",
                    "label": 0
                },
                {
                    "sent": "It's unimodal, so this holds out good possibility that we can use this method to do some efficient hyperparameter estimation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just to summarize, we've presented here a novel message passing algorithm for sparse Bayesian PCA in high dimension.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ends the message.",
                    "label": 0
                },
                {
                    "sent": "Updates are rendered tractable by a Gaussian proxy.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "We achieve convergence by posing some consistency requirements derived from statistical mechanics.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theory.",
                    "label": 0
                },
                {
                    "sent": "And inference of posterior marginals exhibits near optimal performance compared to this theory.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We outperformed two other recently published algorithms on Synthetic data.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a couple of real datasets.",
                    "label": 0
                },
                {
                    "sent": "And we also have an approximation to the marginal likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the future the the two obvious directions to to develop this work of firstly to use the estimate the marginal likelihood for hyperparameter estimation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Secondly, we would like to extend this to multiple factors.",
                    "label": 1
                },
                {
                    "sent": "For the case when the factors can be assumed to be orthogonal, this should be relatively straightforward, although it will require require efficient estimation of hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "But for the non orthogonal case this is really a subject of ongoing research.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much for listening, and if anybody is sufficiently interested, there is code available on this website and you can download it and reproduce the results in the paper.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Observation.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rigor, the physicists at these things, that statistical mechanics have been rigorous for about a century or more, right?",
                    "label": 0
                },
                {
                    "sent": "Application but statistical mechanics really is mailed down.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think I don't think anybody claims that the replica analysis is wholly rigorous.",
                    "label": 0
                },
                {
                    "sent": "I don't think I'm not sure if there's any case where they've actually shown it so rigorously.",
                    "label": 0
                },
                {
                    "sent": "So did you think that running Max product instead of some product?",
                    "label": 0
                },
                {
                    "sent": "Will that be easier with this model?",
                    "label": 0
                },
                {
                    "sent": "We didn't think about it.",
                    "label": 0
                },
                {
                    "sent": "I. I'm not sure why.",
                    "label": 0
                },
                {
                    "sent": "Why do you ask that for this model?",
                    "label": 0
                },
                {
                    "sent": "Point proximation with yeah, thinking maybe that would be computationally easier to maximize rather than some.",
                    "label": 0
                },
                {
                    "sent": "It would be meaningful because it would be the maximum of the posterior right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I guess we were after him as fully Bayesian approaches we could get.",
                    "label": 0
                },
                {
                    "sent": "So the short answer is no, we didn't look at it at all.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}