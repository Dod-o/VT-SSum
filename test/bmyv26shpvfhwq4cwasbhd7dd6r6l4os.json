{
    "id": "bmyv26shpvfhwq4cwasbhd7dd6r6l4os",
    "title": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries",
    "info": {
        "author": [
            "Ankur Moitra, Department of Mathematics, Massachusetts Institute of Technology, MIT"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_moitra_learning/",
    "segmentation": [
        [
            "Radar, thanks for coming.",
            "Thanks for waking up for those of you who are jet lag like myself."
        ],
        [
            "Let me tell you about this paper by first setting the broader context into which this work fits."
        ],
        [
            "Namely, that of sparse representations.",
            "So this is a powerful idea that's had a big impact across many fields, but the starting point is really the observation that many types of data turned out to be sparse and appropriately chosen basis.",
            "This will be the setting for this talk."
        ],
        [
            "Let me describe this in a bit more detail.",
            "So for our purposes, a set of data will be a collection of vectors in RN.",
            "You can think about it as a collection of images or other types of signals."
        ],
        [
            "And the important thing is that for some dictionary, which is just an N by N matrix, we can write Bisa buys the product of a * X abide here.",
            "I'll refer to A is the dictionary.",
            "It's over complete.",
            "If there are more columns than rows and I'll refer to exhibi as the representation of the data.",
            "Now for a given dictionary there of course many choices for how we could represent the given piece of data, but it really won't be interested in today."
        ],
        [
            "Is looking for some notion of a simple representation if it exists, so in particular, sparse 11 with at most K nonzeros.",
            "So many of you now may be familiar with things like sparse recovery, which is really the setting where a is known and by is known and we'd like to compute a sparse exhibi if it exists that solves the linear system.",
            "Now dictionary learning instead."
        ],
        [
            "Is the very natural generalization from a learning context to the setting where a is unknown.",
            "So the basic question will ask is can we learn a from random examples?",
            "Can we automatically fit the basis to our data and find a basis that enables a sparse representation if one exists?"
        ],
        [
            "So before I get into the details of the actual stochastic model, will study, let me tell you a little bit about some of the history and applications of dictionary learning, so it's also referred to as sparse coding.",
            "You'll hear that often."
        ],
        [
            "Now, as you might expect, that has many applications than things like signal processing and statistics, where of course you know hand design dictionaries which are various families of wave.",
            "Let's play big role, so you'd expect that automated tools for finding bases are also powerful may play key role in some of the fundamental tasks like denoising, edge detection and super resolution and also blocked."
        ],
        [
            "Fashion now their uses in machine learning.",
            "Follow a slightly different pattern where the idea is that really sparsity is used as a regularizer to enforce that you're finding a simple model, so that you hope when you learn a classifier on top of it.",
            "It generalizes well even played a key role in some recent work in deep learning, which really takes it to another extreme, or instead of justice looking for sparse representation, you're looking for composing sparse representations."
        ],
        [
            "And curiously, the first introduction of this work was not from any of these fields, but was from computational neuroscience, where the idea is that if you buy dictionary learning to a collection of natural images, it turns out that what you end up with our filters that have a lot of the same properties that the neural response patterns do in the visual cortex."
        ],
        [
            "So now let me actually get into what the stochastic model is now.",
            "Of course there's a long history of sparse recovery, but I just want to summer."
        ],
        [
            "Does it tell you one result?",
            "Because we'll only be interested in studying dictionary learning.",
            "In cases where we can perform sparser covering, it only makes sense that Ryan learn dictionaries, which afterwards you can then use the dictionary to actually perform sparse recovery."
        ],
        [
            "So with that in mind, there's actually this slightly less known work on sparse coverage.",
            "I think it's really beautiful work by Donoho and Stark and Donna.",
            "Hello on what are called incoherent dictionaries, so this is really nice and general framework that captures many things that were studied before a special cases.",
            "The idea is to define a parameter mu called the incoherence which measures the Max over all pairs of columns of the inner product between them divided by root N. So the point is that the smaller mu is, the more uncorrelated the columns are.",
            "And as it turns out, the easier it is to perform sparse recovery, and in particular, this work really clarified a lot of the work that happened in signal processing beforehand, because people had studied specific dictionaries and studied recovery for them, things like spikes and signs, which turned out to just be a special case because they're in fact incoherent.",
            "So the main work was that first of all, Donahoe and Stark and Donovan well proved that information.",
            "Theoretically, you can recover unique solutions."
        ],
        [
            "Because in fact, if X is case bars, then if K is at most Rudan over 2 mu, then in fact X is the uniquely sparsest solution of this given linear system.",
            "So it's at least uniquely defined.",
            "And Moreover, algorithmically you can find it too, because in fact, L1 minimization does the trick.",
            "So I want to emphasize this point because many of you are probably mostly familiar with things like the restricted isometry property and compressed sensing, which I would argue that in fact that's a little bit more applicable to settings where you get to choose A and then you can work up to linear sparsity, whereas a much richer family of dictionaries turn out to be incoherent, and it's much more natural setting the study when you think about the dictionary is something you want to learn that you don't have control over, but it's intrinsic to the problem type.",
            "So."
        ],
        [
            "So now you know with this sort of goal in mind that we want to study dictionary learning in cases where we actually can solve sparse recovery.",
            "Well, what cases can we actually solve sparse recovery?"
        ],
        [
            "The first one is rather simple case for sparse recovery as well.",
            "If a itself is full column rank, then I can certainly find sparse solutions because I can forget about the fact that they are sparse and just solve the linear system."
        ],
        [
            "It turns out that the dictionary learning problem here is much more interesting, and there was this beautiful work in last year's cult by Spielman, Wang and Right, which informally all stated.",
            "For now, there's a polynomial time algorithm to learn dictionaries which have full column rank, provided that the sparsity of the observations you observe is about ruden."
        ],
        [
            "And the model in which they studied it is the natural stochastic model thing, so there's some unknown dictionary which would like to learn some basis which enables a sparse representation.",
            "And what we observe are random samples.",
            "The Form 8 * X where X is chosen to be case bars.",
            "By definition we choose IT support uniformly at random among all sets of size K and condition on its support.",
            "We choose the nonzero values independent."
        ],
        [
            "So I'll mention that what's nice about these types of models in these types of results is that they tend to work and much broader.",
            "You know they're not too brittle to these types of stochastic assumptions, so this is in contrast to things like independent component analysis that are quite brittle to their assumptions."
        ],
        [
            "Now the main work that we consider in this paper is that Ryan generalized to get algorithms for Overcomplete dictionaries.",
            "Remember that these are dictionaries have more columns than rows and the reason for really considering these dictionaries that these are the ones that have played the key role in a lot of signal processing and statistics.",
            "It's because Overcomplete dictionaries are more flexible when you get to choose more columns than rows, you can better fit your basis to a given collection of data."
        ],
        [
            "So in this sense, Overcomplete dictionaries turn out to be more expressive in terms of what families of signals you can sparsely represent using them."
        ],
        [
            "And the main result here is we consider this case where a is incoherent.",
            "Going back to this work of Donna and Stark and Don Aliquo and remember."
        ],
        [
            "That they were able to get sparse recovery as long as K is about roonan over to mute.",
            "And that was exactly the right information theoretic threshold.",
            "Beyond that, X is not necessarily the uniquely sparsest solution.",
            "So our main work is in the same stochastic model we're able to do dictionary learning for almost the same sparsity.",
            "So instead, K goes up to root N over mu login.",
            "So we're losing an extra log in factor and getting almost up to the Donna Hello bound."
        ],
        [
            "I'll mention briefly that what I'll show you actually the sort of main technical point that I want to get across in this talk, is that actually the main connection is between this problem and a purely graph theoretic question about overlapping clustering in random graphs.",
            "See most of the heuristics for dictionary learning.",
            "They have this flavor of alternating minimization, where you guess one side and compute the other, fix one an alternate back to the other.",
            "So there's really a chicken and egg problem of if you knew the dictionary, you could find the sparse representation and vice versa.",
            "And really the question will be motivated based on if we don't know the dictionary.",
            "What can we still learn about the representation beforehand?",
            "That's a sense in which we'll get an overlapping clustering."
        ],
        [
            "I'll mention that independently and concurrently Agarwal at all, which will be presented in this conference, gave algorithm for learning incoherent overcomplete dictionaries with quite different techniques that works up to K equals enter the quarter overview."
        ],
        [
            "So now let me quickly tell you about the connections that overlapping clustering.",
            "This is the main sort of technical points that I want to get across.",
            "Remember that what I really want to understand is what can we learn about the representations without knowing the dictionary and let me start off with a basic question."
        ],
        [
            "What if I give you 2?"
        ],
        [
            "Samples B&B Prime, which have representations X&X prime, then a natural thing we could ask is do X&X prime do their supports intersect?",
            "Is there a common column of the dictionary that they both contain and using the representation?"
        ],
        [
            "So now let me denote by white squares the zeros and black squares the nonzeros, so indeed."
        ],
        [
            "Here they have an intersection."
        ],
        [
            "And Moreover, if we take X&X prime and take their inner product, it's easy to see that if that inner product is nonzero, they definitely have an intersection.",
            "Now, if it's zero, we don't really know, because you know, maybe they do.",
            "Maybe they don't.",
            "Maybe they do, and we just had the wrong cancellation and they happen to be 0."
        ],
        [
            "The main point about what makes dictionary learning easier in the incoherent case is that there are these various uncertainty principles that are really the basis for sparse recovery.",
            "I won't get into those, but what they in essence show the corollary for our purposes is that for K sparse vectors X&X prime, it turns out that with high probability, the inner product between B prime and B turns out to be very close to X&X prime."
        ],
        [
            "So really, the way I want you to think about this is just by giving access to the samples by taking their inner product.",
            "We get an answer to a simple pairwise test."
        ],
        [
            "We should think about it as now if we build a graph on the samples and connect pairs of samples whose inner product is at least 1/2 in magnitude, we should think about it as a simplified random process just for the purpose of this talk that."
        ],
        [
            "Really, the probability of an edge between B&B Prime is 0 if they don't intersect in their supports.",
            "And let's say it's half.",
            "If they do so, let's pretend this are random graph model."
        ],
        [
            "And now let's get to the main need.",
            "So the point is, where are the clusters in this overlapping clustering problem?",
            "Well, we can look at all of the examples.",
            "All the samples be where exercise non zero.",
            "We can call that a cluster CI.",
            "Now the key point is that this is a little bit different than the standard clustering in random graphs model, because these clusters are overlapping after all CI and CJ.",
            "There are plenty of examples which have both XI not equal to 0 and XJ not equal to 0.",
            "So these clusters will have considerable overlap."
        ],
        [
            "And really, the main question is can we find these clusters efficiently in this random graph model?"
        ],
        [
            "And the main starting point is the answer.",
            "This technical question think about the following thought experiment.",
            "What if I gave you 3 examples with representations XX prime X double prime and I told you that pairwise each of their supports intersect?",
            "So there's one cluster that contains both X&X prime.",
            "There's one cluster that contains both X&X double prime in one cluster that contains X prime and X double prime.",
            "That doesn't necessarily mean that there's one cluster that contains all of them together, so that's the main question.",
            "Is is this the scenario from our pairwise tests?"
        ],
        [
            "Or is this the scenario where they just happen to intersect in different clusters?",
            "It's the fact that the clusters are overlapping.",
            "That makes this an interesting quote."
        ],
        [
            "Action and the main point."
        ],
        [
            "So that actually we can use fresh examples to be able to figure out which of these two cases occur.",
            "So think about it."
        ],
        [
            "This thought experiment, the first cases where they all three do intersect in one column.",
            "Well, what it means is how easy is it for a new sample Y to come along and hit all three of them pairwise all.",
            "Why would have to do is it would have to contain this one element."
        ],
        [
            "So the probability that Y intersects all three is at least K / M because we're choosing K sparse vectors over a domain of size M."
        ],
        [
            "Yet in contrast, now if we have three that don't intersect in a common intersection point, but they pairwise do."
        ],
        [
            "Well, the key point is that with high probability, if you take any pair of samples, their common intersection will be constant size at most C, and then you can show through simple comment oral argument that the probability Y intersects all three is about oh of K ^3 / M ^2.",
            "So with this allows us to do is just by counting the number of examples that hit all three together we can give a simple triple test that will distinguish between.",
            "Do these triples have a common intersection or do they not?",
            "Now we'll need higher order tests."
        ],
        [
            "The real version, which I won't get into."
        ],
        [
            "But essentially, I claim that this really solves our overlapping clustering problem.",
            "So what if we take this test as a primitive?",
            "The fact that we can answer these sorts of queries.",
            "Do these triples have a common intersection or not?"
        ],
        [
            "Now, how can we build the entire cluster?",
            "What we could do is we could take a pair of examples, X&X prime and compute all the X double primes which that triple satisfies the pass."
        ],
        [
            "What that really outputs is a union of clusters corresponding to the intersection of the support of X in the support of X prime.",
            "So in general this triple test gives us a way to output clusters which the minimal elements will actually be the true overlapping clusters we're looking."
        ],
        [
            "After all, for each cluster filled with high probability, be some pair that uniquely identifies at some pair XNXX prime, which the intersection between them is exactly I.",
            "So when we do the triple tests for that will get only the cluster.",
            "See ya."
        ],
        [
            "So at the end of the day, we can clean it up just by Outputing inclusion wise minimal sets in this."
        ],
        [
            "Play.",
            "And as I mentioned, in order to actually get up to this Donahoe quote bound of roughly rude an overview with use higher order tests and analysis through essentially piercing numbers, but I'll."
        ],
        [
            "I'll take that offline and what I really want to emphasize is that now our problem is easy once we have some idea of what the representations look like, it's easy to get unbiased estimators for the dictionary columns.",
            "In fact, let me give you two different ways of."
        ],
        [
            "So the first one is, what if we can take these clusters?",
            "CI, which are all the examples where XI is nonzero?",
            "And what if we can partition this cluster into the things where XI is greater than zero, and where XI is less than zero?",
            "This in turn we would just have to average one of these sub clusters and would get an unbiased estimator of AI."
        ],
        [
            "That's one way to do it.",
            "In fact, you can even go back to some of the popular approaches and practice things like K. SVD, but I won't get into which you could even just take these clusters where exhibi is non zero and the intuition is that if you look at the direction of largest variance, that should somewhat be in the direction of AI because you're oversampling the things that use that column, so that should be the direction of largest variance."
        ],
        [
            "Roughly and then finally we show that altering minimization works when we're close enough to the true dictionary.",
            "So what that allows us to do is eventually get a geometric rate of convergence for the decay in our error.",
            "That's what allows us to do things like get log one over epsilon for the dependency in the sample complex in the runtime."
        ],
        [
            "Alright, so I guess I'm how much.",
            "What's the time?",
            "2 minutes alright."
        ],
        [
            "I guess I'll just say thanks and take questions, so this was helped by at least emphasize the main points about why dictionary learning is natural.",
            "I think incoherent is really the right place to study these things, and really, I think there's an interesting connection between dictionary learning and overlapping clustering in random graphs, so I'll leave it at that for now, but thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radar, thanks for coming.",
                    "label": 0
                },
                {
                    "sent": "Thanks for waking up for those of you who are jet lag like myself.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me tell you about this paper by first setting the broader context into which this work fits.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Namely, that of sparse representations.",
                    "label": 1
                },
                {
                    "sent": "So this is a powerful idea that's had a big impact across many fields, but the starting point is really the observation that many types of data turned out to be sparse and appropriately chosen basis.",
                    "label": 0
                },
                {
                    "sent": "This will be the setting for this talk.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me describe this in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "So for our purposes, a set of data will be a collection of vectors in RN.",
                    "label": 0
                },
                {
                    "sent": "You can think about it as a collection of images or other types of signals.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the important thing is that for some dictionary, which is just an N by N matrix, we can write Bisa buys the product of a * X abide here.",
                    "label": 0
                },
                {
                    "sent": "I'll refer to A is the dictionary.",
                    "label": 0
                },
                {
                    "sent": "It's over complete.",
                    "label": 0
                },
                {
                    "sent": "If there are more columns than rows and I'll refer to exhibi as the representation of the data.",
                    "label": 0
                },
                {
                    "sent": "Now for a given dictionary there of course many choices for how we could represent the given piece of data, but it really won't be interested in today.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is looking for some notion of a simple representation if it exists, so in particular, sparse 11 with at most K nonzeros.",
                    "label": 1
                },
                {
                    "sent": "So many of you now may be familiar with things like sparse recovery, which is really the setting where a is known and by is known and we'd like to compute a sparse exhibi if it exists that solves the linear system.",
                    "label": 0
                },
                {
                    "sent": "Now dictionary learning instead.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the very natural generalization from a learning context to the setting where a is unknown.",
                    "label": 0
                },
                {
                    "sent": "So the basic question will ask is can we learn a from random examples?",
                    "label": 1
                },
                {
                    "sent": "Can we automatically fit the basis to our data and find a basis that enables a sparse representation if one exists?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I get into the details of the actual stochastic model, will study, let me tell you a little bit about some of the history and applications of dictionary learning, so it's also referred to as sparse coding.",
                    "label": 0
                },
                {
                    "sent": "You'll hear that often.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, as you might expect, that has many applications than things like signal processing and statistics, where of course you know hand design dictionaries which are various families of wave.",
                    "label": 0
                },
                {
                    "sent": "Let's play big role, so you'd expect that automated tools for finding bases are also powerful may play key role in some of the fundamental tasks like denoising, edge detection and super resolution and also blocked.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fashion now their uses in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Follow a slightly different pattern where the idea is that really sparsity is used as a regularizer to enforce that you're finding a simple model, so that you hope when you learn a classifier on top of it.",
                    "label": 1
                },
                {
                    "sent": "It generalizes well even played a key role in some recent work in deep learning, which really takes it to another extreme, or instead of justice looking for sparse representation, you're looking for composing sparse representations.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And curiously, the first introduction of this work was not from any of these fields, but was from computational neuroscience, where the idea is that if you buy dictionary learning to a collection of natural images, it turns out that what you end up with our filters that have a lot of the same properties that the neural response patterns do in the visual cortex.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let me actually get into what the stochastic model is now.",
                    "label": 0
                },
                {
                    "sent": "Of course there's a long history of sparse recovery, but I just want to summer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does it tell you one result?",
                    "label": 0
                },
                {
                    "sent": "Because we'll only be interested in studying dictionary learning.",
                    "label": 0
                },
                {
                    "sent": "In cases where we can perform sparser covering, it only makes sense that Ryan learn dictionaries, which afterwards you can then use the dictionary to actually perform sparse recovery.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that in mind, there's actually this slightly less known work on sparse coverage.",
                    "label": 0
                },
                {
                    "sent": "I think it's really beautiful work by Donoho and Stark and Donna.",
                    "label": 0
                },
                {
                    "sent": "Hello on what are called incoherent dictionaries, so this is really nice and general framework that captures many things that were studied before a special cases.",
                    "label": 0
                },
                {
                    "sent": "The idea is to define a parameter mu called the incoherence which measures the Max over all pairs of columns of the inner product between them divided by root N. So the point is that the smaller mu is, the more uncorrelated the columns are.",
                    "label": 0
                },
                {
                    "sent": "And as it turns out, the easier it is to perform sparse recovery, and in particular, this work really clarified a lot of the work that happened in signal processing beforehand, because people had studied specific dictionaries and studied recovery for them, things like spikes and signs, which turned out to just be a special case because they're in fact incoherent.",
                    "label": 0
                },
                {
                    "sent": "So the main work was that first of all, Donahoe and Stark and Donovan well proved that information.",
                    "label": 0
                },
                {
                    "sent": "Theoretically, you can recover unique solutions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because in fact, if X is case bars, then if K is at most Rudan over 2 mu, then in fact X is the uniquely sparsest solution of this given linear system.",
                    "label": 1
                },
                {
                    "sent": "So it's at least uniquely defined.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, algorithmically you can find it too, because in fact, L1 minimization does the trick.",
                    "label": 0
                },
                {
                    "sent": "So I want to emphasize this point because many of you are probably mostly familiar with things like the restricted isometry property and compressed sensing, which I would argue that in fact that's a little bit more applicable to settings where you get to choose A and then you can work up to linear sparsity, whereas a much richer family of dictionaries turn out to be incoherent, and it's much more natural setting the study when you think about the dictionary is something you want to learn that you don't have control over, but it's intrinsic to the problem type.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now you know with this sort of goal in mind that we want to study dictionary learning in cases where we actually can solve sparse recovery.",
                    "label": 0
                },
                {
                    "sent": "Well, what cases can we actually solve sparse recovery?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one is rather simple case for sparse recovery as well.",
                    "label": 0
                },
                {
                    "sent": "If a itself is full column rank, then I can certainly find sparse solutions because I can forget about the fact that they are sparse and just solve the linear system.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that the dictionary learning problem here is much more interesting, and there was this beautiful work in last year's cult by Spielman, Wang and Right, which informally all stated.",
                    "label": 0
                },
                {
                    "sent": "For now, there's a polynomial time algorithm to learn dictionaries which have full column rank, provided that the sparsity of the observations you observe is about ruden.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the model in which they studied it is the natural stochastic model thing, so there's some unknown dictionary which would like to learn some basis which enables a sparse representation.",
                    "label": 0
                },
                {
                    "sent": "And what we observe are random samples.",
                    "label": 0
                },
                {
                    "sent": "The Form 8 * X where X is chosen to be case bars.",
                    "label": 0
                },
                {
                    "sent": "By definition we choose IT support uniformly at random among all sets of size K and condition on its support.",
                    "label": 0
                },
                {
                    "sent": "We choose the nonzero values independent.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll mention that what's nice about these types of models in these types of results is that they tend to work and much broader.",
                    "label": 0
                },
                {
                    "sent": "You know they're not too brittle to these types of stochastic assumptions, so this is in contrast to things like independent component analysis that are quite brittle to their assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the main work that we consider in this paper is that Ryan generalized to get algorithms for Overcomplete dictionaries.",
                    "label": 0
                },
                {
                    "sent": "Remember that these are dictionaries have more columns than rows and the reason for really considering these dictionaries that these are the ones that have played the key role in a lot of signal processing and statistics.",
                    "label": 0
                },
                {
                    "sent": "It's because Overcomplete dictionaries are more flexible when you get to choose more columns than rows, you can better fit your basis to a given collection of data.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this sense, Overcomplete dictionaries turn out to be more expressive in terms of what families of signals you can sparsely represent using them.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the main result here is we consider this case where a is incoherent.",
                    "label": 0
                },
                {
                    "sent": "Going back to this work of Donna and Stark and Don Aliquo and remember.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That they were able to get sparse recovery as long as K is about roonan over to mute.",
                    "label": 0
                },
                {
                    "sent": "And that was exactly the right information theoretic threshold.",
                    "label": 0
                },
                {
                    "sent": "Beyond that, X is not necessarily the uniquely sparsest solution.",
                    "label": 0
                },
                {
                    "sent": "So our main work is in the same stochastic model we're able to do dictionary learning for almost the same sparsity.",
                    "label": 0
                },
                {
                    "sent": "So instead, K goes up to root N over mu login.",
                    "label": 0
                },
                {
                    "sent": "So we're losing an extra log in factor and getting almost up to the Donna Hello bound.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll mention briefly that what I'll show you actually the sort of main technical point that I want to get across in this talk, is that actually the main connection is between this problem and a purely graph theoretic question about overlapping clustering in random graphs.",
                    "label": 0
                },
                {
                    "sent": "See most of the heuristics for dictionary learning.",
                    "label": 1
                },
                {
                    "sent": "They have this flavor of alternating minimization, where you guess one side and compute the other, fix one an alternate back to the other.",
                    "label": 0
                },
                {
                    "sent": "So there's really a chicken and egg problem of if you knew the dictionary, you could find the sparse representation and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And really the question will be motivated based on if we don't know the dictionary.",
                    "label": 0
                },
                {
                    "sent": "What can we still learn about the representation beforehand?",
                    "label": 0
                },
                {
                    "sent": "That's a sense in which we'll get an overlapping clustering.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll mention that independently and concurrently Agarwal at all, which will be presented in this conference, gave algorithm for learning incoherent overcomplete dictionaries with quite different techniques that works up to K equals enter the quarter overview.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let me quickly tell you about the connections that overlapping clustering.",
                    "label": 0
                },
                {
                    "sent": "This is the main sort of technical points that I want to get across.",
                    "label": 0
                },
                {
                    "sent": "Remember that what I really want to understand is what can we learn about the representations without knowing the dictionary and let me start off with a basic question.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What if I give you 2?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples B&B Prime, which have representations X&X prime, then a natural thing we could ask is do X&X prime do their supports intersect?",
                    "label": 0
                },
                {
                    "sent": "Is there a common column of the dictionary that they both contain and using the representation?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let me denote by white squares the zeros and black squares the nonzeros, so indeed.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here they have an intersection.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Moreover, if we take X&X prime and take their inner product, it's easy to see that if that inner product is nonzero, they definitely have an intersection.",
                    "label": 0
                },
                {
                    "sent": "Now, if it's zero, we don't really know, because you know, maybe they do.",
                    "label": 0
                },
                {
                    "sent": "Maybe they don't.",
                    "label": 0
                },
                {
                    "sent": "Maybe they do, and we just had the wrong cancellation and they happen to be 0.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main point about what makes dictionary learning easier in the incoherent case is that there are these various uncertainty principles that are really the basis for sparse recovery.",
                    "label": 0
                },
                {
                    "sent": "I won't get into those, but what they in essence show the corollary for our purposes is that for K sparse vectors X&X prime, it turns out that with high probability, the inner product between B prime and B turns out to be very close to X&X prime.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So really, the way I want you to think about this is just by giving access to the samples by taking their inner product.",
                    "label": 0
                },
                {
                    "sent": "We get an answer to a simple pairwise test.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We should think about it as now if we build a graph on the samples and connect pairs of samples whose inner product is at least 1/2 in magnitude, we should think about it as a simplified random process just for the purpose of this talk that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, the probability of an edge between B&B Prime is 0 if they don't intersect in their supports.",
                    "label": 0
                },
                {
                    "sent": "And let's say it's half.",
                    "label": 0
                },
                {
                    "sent": "If they do so, let's pretend this are random graph model.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now let's get to the main need.",
                    "label": 0
                },
                {
                    "sent": "So the point is, where are the clusters in this overlapping clustering problem?",
                    "label": 1
                },
                {
                    "sent": "Well, we can look at all of the examples.",
                    "label": 0
                },
                {
                    "sent": "All the samples be where exercise non zero.",
                    "label": 0
                },
                {
                    "sent": "We can call that a cluster CI.",
                    "label": 0
                },
                {
                    "sent": "Now the key point is that this is a little bit different than the standard clustering in random graphs model, because these clusters are overlapping after all CI and CJ.",
                    "label": 0
                },
                {
                    "sent": "There are plenty of examples which have both XI not equal to 0 and XJ not equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So these clusters will have considerable overlap.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And really, the main question is can we find these clusters efficiently in this random graph model?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the main starting point is the answer.",
                    "label": 0
                },
                {
                    "sent": "This technical question think about the following thought experiment.",
                    "label": 0
                },
                {
                    "sent": "What if I gave you 3 examples with representations XX prime X double prime and I told you that pairwise each of their supports intersect?",
                    "label": 0
                },
                {
                    "sent": "So there's one cluster that contains both X&X prime.",
                    "label": 0
                },
                {
                    "sent": "There's one cluster that contains both X&X double prime in one cluster that contains X prime and X double prime.",
                    "label": 0
                },
                {
                    "sent": "That doesn't necessarily mean that there's one cluster that contains all of them together, so that's the main question.",
                    "label": 0
                },
                {
                    "sent": "Is is this the scenario from our pairwise tests?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or is this the scenario where they just happen to intersect in different clusters?",
                    "label": 0
                },
                {
                    "sent": "It's the fact that the clusters are overlapping.",
                    "label": 0
                },
                {
                    "sent": "That makes this an interesting quote.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action and the main point.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that actually we can use fresh examples to be able to figure out which of these two cases occur.",
                    "label": 0
                },
                {
                    "sent": "So think about it.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This thought experiment, the first cases where they all three do intersect in one column.",
                    "label": 0
                },
                {
                    "sent": "Well, what it means is how easy is it for a new sample Y to come along and hit all three of them pairwise all.",
                    "label": 0
                },
                {
                    "sent": "Why would have to do is it would have to contain this one element.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the probability that Y intersects all three is at least K / M because we're choosing K sparse vectors over a domain of size M.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet in contrast, now if we have three that don't intersect in a common intersection point, but they pairwise do.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the key point is that with high probability, if you take any pair of samples, their common intersection will be constant size at most C, and then you can show through simple comment oral argument that the probability Y intersects all three is about oh of K ^3 / M ^2.",
                    "label": 0
                },
                {
                    "sent": "So with this allows us to do is just by counting the number of examples that hit all three together we can give a simple triple test that will distinguish between.",
                    "label": 1
                },
                {
                    "sent": "Do these triples have a common intersection or do they not?",
                    "label": 0
                },
                {
                    "sent": "Now we'll need higher order tests.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The real version, which I won't get into.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But essentially, I claim that this really solves our overlapping clustering problem.",
                    "label": 0
                },
                {
                    "sent": "So what if we take this test as a primitive?",
                    "label": 0
                },
                {
                    "sent": "The fact that we can answer these sorts of queries.",
                    "label": 0
                },
                {
                    "sent": "Do these triples have a common intersection or not?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, how can we build the entire cluster?",
                    "label": 0
                },
                {
                    "sent": "What we could do is we could take a pair of examples, X&X prime and compute all the X double primes which that triple satisfies the pass.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What that really outputs is a union of clusters corresponding to the intersection of the support of X in the support of X prime.",
                    "label": 0
                },
                {
                    "sent": "So in general this triple test gives us a way to output clusters which the minimal elements will actually be the true overlapping clusters we're looking.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After all, for each cluster filled with high probability, be some pair that uniquely identifies at some pair XNXX prime, which the intersection between them is exactly I.",
                    "label": 1
                },
                {
                    "sent": "So when we do the triple tests for that will get only the cluster.",
                    "label": 1
                },
                {
                    "sent": "See ya.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at the end of the day, we can clean it up just by Outputing inclusion wise minimal sets in this.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, in order to actually get up to this Donahoe quote bound of roughly rude an overview with use higher order tests and analysis through essentially piercing numbers, but I'll.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll take that offline and what I really want to emphasize is that now our problem is easy once we have some idea of what the representations look like, it's easy to get unbiased estimators for the dictionary columns.",
                    "label": 0
                },
                {
                    "sent": "In fact, let me give you two different ways of.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first one is, what if we can take these clusters?",
                    "label": 1
                },
                {
                    "sent": "CI, which are all the examples where XI is nonzero?",
                    "label": 1
                },
                {
                    "sent": "And what if we can partition this cluster into the things where XI is greater than zero, and where XI is less than zero?",
                    "label": 0
                },
                {
                    "sent": "This in turn we would just have to average one of these sub clusters and would get an unbiased estimator of AI.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's one way to do it.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can even go back to some of the popular approaches and practice things like K. SVD, but I won't get into which you could even just take these clusters where exhibi is non zero and the intuition is that if you look at the direction of largest variance, that should somewhat be in the direction of AI because you're oversampling the things that use that column, so that should be the direction of largest variance.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roughly and then finally we show that altering minimization works when we're close enough to the true dictionary.",
                    "label": 0
                },
                {
                    "sent": "So what that allows us to do is eventually get a geometric rate of convergence for the decay in our error.",
                    "label": 0
                },
                {
                    "sent": "That's what allows us to do things like get log one over epsilon for the dependency in the sample complex in the runtime.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I guess I'm how much.",
                    "label": 0
                },
                {
                    "sent": "What's the time?",
                    "label": 0
                },
                {
                    "sent": "2 minutes alright.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess I'll just say thanks and take questions, so this was helped by at least emphasize the main points about why dictionary learning is natural.",
                    "label": 0
                },
                {
                    "sent": "I think incoherent is really the right place to study these things, and really, I think there's an interesting connection between dictionary learning and overlapping clustering in random graphs, so I'll leave it at that for now, but thanks.",
                    "label": 0
                }
            ]
        }
    }
}