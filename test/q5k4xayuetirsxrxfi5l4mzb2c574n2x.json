{
    "id": "q5k4xayuetirsxrxfi5l4mzb2c574n2x",
    "title": "Recognizing Textual Entailment with LCC\u00b4s GROUNDHOG System",
    "info": {
        "author": [
            "Andrew Hickl, Language Computer Corporation"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2006",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/pcw06_hickl_rtelg/",
    "segmentation": [
        [
            "Good afternoon, I'm analytical and sorry I'm Andrew Hickl and I'm going to be spending some time today talking to you a little bit about the first of LCS two submissions to this year's RT challenge.",
            "This group, this.",
            "System was developed under the direction of Sanda Harbhajan and it's worked, coauthored with my colleagues back at LCC.",
            "So we're really grateful for that.",
            "There are opportunities participate in this year's RT to challenge.",
            "This really was our first formal exposure to Artie.",
            "We had a preliminary exposure in the fall of 2005 with the acquaint knowledge base evaluation and this included all the another number of other Pascal veterans, including University, Colorado University, Illinois, Stanford University of Texas at Dallas, and Else sees other research group led by Doctor Dan Mold IMO.",
            "While this year's revaluation really."
        ],
        [
            "Represented our first 484 A into the RT taskbar.",
            "Groups really worked extensively towards performing the types of textual inference.",
            "Their crucial for a number of NLP applications.",
            "These include question answering, information extraction, multi document summarization named any recognition, temporal and spatial normalization, and things like semantic parsing.",
            "So just as an overview of where we're going to go today, start with a little bit of an introduction, and then I'm going to go through each of the components that are in our system, which is affectionately known as Groundhog, just as in the slide here, the original RT deadline was on February 2nd, which in the United States is Groundhog Day.",
            "For a variety of reasons, which I won't go into here, so my team renamed our system, which had some other name at that point to the lovely Groundhog which you see here.",
            "Basically only."
        ],
        [
            "Derivation of the name.",
            "Then we'll go through each of these components in order, and then I will talk a little bit about our evaluation.",
            "Both that was conducted from the RT organizers, and then we'll conclude with a few points.",
            "So let me see the first introduction to the ground type system would give you is kind of just walk you through each of the components and a lot of this will be familiar from the other presentations that you've seen today.",
            "So we start with a pre prod."
        ],
        [
            "And so we preprocess both the test set and a set of training corpora that we've acquired.",
            "And this includes a whole host or rich semantic annotations including named entity recognition, temporal normalization and ordering information, syntactic and semantic parsing, name aliasing, name coreference, and a variety of semantic annotations that are performed using heuristics designed to capture kinds of propositional semantics.",
            "These kinds of annotations then are sent to a lexical alignment component, and we use a machine learning classifier to be able to perform to identify which components are which constituents between the text and the hypothesis contain corresponding information.",
            "These the highest confidence entities that are aligned out of this lexical line of classifier, then sent to a paraphrasing or a phrase level alternation acquisition module which uses the web as its corpus to be able to drive these alternations.",
            "These features, including everything from the preprocessing, the lexical alignment, and the paraphrase acquisition, are then sent to a feature extraction module which feeds it entailment classification module based on decision trees in order to produce an output, whether yes or no, and a confidence value.",
            "So as I talk about each of these components, I'm going to be motivating or grounding."
        ],
        [
            "In this example here, and this is a yes example that we got right.",
            "Fortunately, and it basically has to do with JP Losman who's the quarterback for the Bills and the American football team.",
            "the Buffalo Bills.",
            "And so this is looking at this kind of example.",
            "Let us to ask a number of questions.",
            "So first, what are the important portions that should be considered by a system?",
            "The text contains a lot of information, three different predicates and number of different entities.",
            "And really one of the first task that we wanted to solve was to kind of hone in on the string or this.",
            "You know, the set of tokens that was most important to be able to perform aarti and so we you know our hypothesis was that lexical alignment could be used to identify these strings.",
            "So here if I understand the bills and JP Losman are the two people that are.",
            "Two entities that I'm interested in and I only have to perform some sort of computation across these two strings that are underlined in the figure above.",
            "Additionally, we need to have some mechanism to determine if the same meaning is being conveyed by phrases that may not be lexically related.",
            "So in this case here we have the colocation of the idiom hand the reins over and give the starting job too, so we need some sort of mechanism and phrase level alternations or what we're referring to loosely here, as paraphrases may be able to help.",
            "I'm finally we need.",
            "We all need some sort of mechanism to be able to deal with the complexity that reduces the effectiveness of our parsers.",
            "We've talked about this time and time again when these be syntactic parsers are semantic parses.",
            "Here you can see in the white box for the text I've got the semantic parts produced by our system, and you know if I look focusing right here just on the predicate hand, I see that I've got an R0.",
            "You know these prop bank style annotations of the bills and the brains, but the Arc 2 here is just that.",
            "You know the the that NP ahead of that Proposition 2 one.",
            "So there's a longer NP here that starts with one and goes all the way down to JP Losman and the idea is that you know we can use coreference information to reconstruct some of that, but we need to know that JP Losman.",
            "As some sort of semantic dependency back to the predicate hand and our semantic parser as it stands now, doesn't get this for us.",
            "OK, so."
        ],
        [
            "Let's talk about the each of these components in order so just briefly to talk about the different kinds of rich semantics that Groundhog offers first named entity recognition.",
            "So we use else sees in how Cicero light name any recognition software to categorize entities with about 150 different types of named entities.",
            "So you can see we get things like we recognize the bills are not just an organization, but there are sports organization.",
            "Recognize JP Losman as a person recognized variety of time, experience, expressions.",
            "Anna broken Leg is for better or worse or body part.",
            "We also do name aliasing and coreference using grammars and heuristics that are found natively in Cicero, light and so we're able to recognize that the bills is not only coreferential with the possessive pronoun there in the text.",
            "It's also coreferential with the bills in the hypothesis.",
            "Also, JP Losman is.",
            "We know that you know, based on the kind of noun noun compound quarterback JP Losman, that he's preferential that is correct wrenshall and we also know that he's coreferential with relative pronoun who.",
            "Moving right along we also we used."
        ],
        [
            "Cici's teiser, which is a Timex system, developed with last year's ace valuations, with attorney valuations in Ace to be able to normalize time expressions to their ISO 9000 values.",
            "This is.",
            "There's also some rules in this system to compute the relative order of time expressions within a context, so we can map things like now to a document time here it's just defaulted to the first day, January in 2006, and then range operators such things like a year ago.",
            "So with regards to some sort of document time.",
            "We also used LCC zone implementation of the Brill POF stagger and the Collins parser to be able to syntactically parsed sentence is and we also used a series of post processing rules to be able to identify phrase chunks, phrase heads, relative clauses, appositives and parenthetical.",
            "Some other kinds of syntactic constructions.",
            "Semantic parsing."
        ],
        [
            "Was used with another system developed at LCC that was it's performed using Maxent based semantic role labeling system that's been trained on Prop bank annotations and just to see how this works here.",
            "We've got three different predicates in our text so the first one could be something like appear."
        ],
        [
            "Already and we have an R0 and R GAM, which is the temporal adjunct.",
            "Just those two argues that predicate for the predicate hand.",
            "I've got the bills of my heart 0.",
            "Again here I have the Arg, one is the rains and then depending on the constituents that you use either one or one of their two topics as they are two.",
            "I have also have this predicate missed which doesn't shouldn't feature in our car entailment calculation which has a.",
            "Subject going around a graph, the subject of who here or that race, and then most of last season.",
            "It's hard one and a broken leg as its other arguing.",
            "And we could do this with the hypothesis as well."
        ],
        [
            "The last component process.",
            "Then we'll talk about today is semantic annotation, and we use it as just mentioned.",
            "We used also used heuristics to annotate a number of different kinds of following different types of semantic information, and some of these you'll find familiar from other presentations today, and most notably the Stanford presentation.",
            "So we like a number of other groups we annotated for clarity so when predicates are nominals, event in anomalies were found, the scope of negative marker, or associated with indignation, denoting verbal and refuse.",
            "We also annotated factive verbs, which we define as predicates that conventionally imply the truth of their complements.",
            "So when I have a verb like admit, I know that necessarily that the speaker and the context is committed to the truth of the predicate be unlikely.",
            "Also we annotated non factive verbs, so this is a larger class and."
        ],
        [
            "Probably require should be decomposed into a number of different number different categories, so we each of these types, whether they were speech act verbs, nonfactor speech, actors like denier, Claymore, psych, verbs like think or believe, or verbs of uncertainty or likelihood would be uncertain.",
            "Would be likely intentions or plans scheme plot want or conditional context which aren't the same kind of thing, but they have the same kind of effect on predicates.",
            "We mark these as unresolved, so namely, we couldn't tell whether they are true or not, or.",
            "In the current context and so here's an example that indicates here you know, we claim that these when someone claims that something is in contradiction, we don't necessarily know if that's just their claim, or if that's actually the reality of the situation.",
            "And."
        ],
        [
            "The category of semantic annotations.",
            "We perform where what Huddleston and Pullum or Geoff Pullum at Santa Cruz refers to as supplemental expressions, and these are.",
            "Syntactic constructions such as the positives or epithets or name aliases or as clauses or non restrictive relative clauses that necessarily trigger conventional implicatures.",
            "Let's take a look at these.",
            "These do so.",
            "I haven't a nominal appositive construction like this where I have Shia pilgrims Virgin Karbala to mark the depth Hussein the Prophet Muhammad's grandson 1300 years ago, and what we did to be able to simplify the process in the piece was we extracted that a positive construction and append it to the end of the text or hypothesis that was founded.",
            "So here we have she pilgrims converge in carbs up to mark the deficit Hussein 1300 years ago, directly from the text and namely Hussein is the Prophet Muhammad's grandson and this allowed.",
            "It is provided to small advantage in terms of later processing.",
            "You will see in a second we did this for named aliases.",
            "So we have the reference to Ali Al Timimi as the radical Islamic cleric and we made sure that was explicitly mentioned in the text."
        ],
        [
            "As clauses, so the LMI was set up by Mr. Torvalds as a non profit organization.",
            "We just made these things explicitly.",
            "OK."
        ],
        [
            "So all of these kinds of all this preprocessing serves to inform our later components and the next one that we're going to see in our pipeline.",
            "Here is the issue.",
            "There are the notion of lexical alignment, so we really believe that these kinds of lexico semantic information, along with things like the lemma of the actual form of the word, can provide us with the input to be able to find corresponding tokens between the tax in the hypothesis.",
            "So in most cases, in a lot of cases we just have textual overlap, so the bills are in the taxes.",
            "Also, can should be correspond should correspond to the bills in the hypothesis.",
            "JP Losman, with the same way, but we have, you know, tougher things like hand and give so hand the reins over and give the starting job too.",
            "We need to know that these two things are similar and word net can help us here in our semantic annotations can help us here but we need you know we need.",
            "We think the problem requires more than just this just.",
            "These kinds of maybe surface semantic approaches also."
        ],
        [
            "Things like the rains and the starting job even though they occupy the same semantic role in the sentence, we need to figure out whether these two things should be whether these things contain corresponding semantic information or not.",
            "So we use the Maxent classifier to compute the probability that an element computed selected from a text corresponds to or shorthand aligns with an element selected by populists with three step process.",
            "So we decompose sentences into a set of alignable chunks that was essentially the output from a chunk parser in a colocation detection system.",
            "We put those chunks into a matrix and then we pull each pair chunks out of that matrix and submit it to the classifier.",
            "The classifier output the probability that the pair represented a positive example of alignment.",
            "We use four sets of features and these are just."
        ],
        [
            "Some of the some of the features that we use in each of these categories so things things like a statistical features like cosine similarity or Glickman into guns, lexical entailment, probability, lexico semantic features like word, net similarity that we have several other groups have used Ted Peterson's package, Wordnet, synonymy, and antinomy, named entity features alternations that we had acquired previous for previous applications.",
            "Like you, a series of purely string based features like Levenshtein, edit distance or morphological stem equality, and then syntactic features such as the maximal category that the token was found in.",
            "Whether the token was ahead of its own phrase or end, things like the structure of the NP the token was founded, weather included.",
            "Modifyers weather.",
            "Was that subordinate, PP was within an NP NP compound Dino Combo."
        ],
        [
            "So to be able to train this classifier, we annotated a set of 10,000 alignment chunkers taken from the development set as either positive negative examples of alignment and then we train to different classifiers that Hill climber just for kicks Anna Maxent classifier an we got relatively decent performance, but we found that when we sample we tested on different portions of that 10,000 trained and tested different portions of that annotated set.",
            "The F measure varied significantly and so we wanted to look for something that was more robust.",
            "So they could deal with unseen data a little bit better, so we were able to perform."
        ],
        [
            "Basically experiment with together at two different techniques for gathering training data.",
            "Set that type of so we used following Burger and Farro.",
            "We created a corpus about 100,000 positive examples of entailment by pairing the headline of a document with the first sentence of the newswire document.",
            "So for example, here we've got this first line.",
            "Sydney Newspapers made a secret bid not to report on the funding, and spending made through the city's successful bid for the 2000 Olympics and headline was papers said to protect Sydney bid.",
            "Now we filtered these examples extensively and then we will Smit only a small portion of these for evaluation and we didn't use the test of full strict logical entailment or even textual entailment.",
            "On these examples.",
            "We just wanted to make sure that we we were OK with dealing with some noise in the data and so when we had our attitudes look back at this, we want to make sure you know is.",
            "Could this be considered to be similar enough that in some context in some case they could be considered to be?",
            "Either paraphrase or entailment, using some sort of semantic correspondence between these two."
        ],
        [
            "We also to be able to build a balanced rains that we've gathered.",
            "120,000 negative examples of textual entailment.",
            "We use two strategies to do this.",
            "We selected sequential sentences for newswire text that featured are repeat mention named Entity.",
            "For example, you know and this core structure will predict this shouldn't actually occur, but we found a number of examples.",
            "So one player losing a close friend is Japanese.",
            "Pitcher Hideki Irabu was befriended by Wills during spring training.",
            "Are Abu.",
            "Here's the correct of the entity so that he will take wells up dinner when the Yankees visit Toronto.",
            "We also extracted pairs of sentences.",
            "They would list linked by discourse connectives that we expected to have similar kinds of properties, things like even though although in contrast.",
            "So according to the professor present methods of cleaning up oil slicks are extremely costly and are never completely efficient, so we have this hypothesis clean magazine, 100% pollution retrieval rate is low cost and can be recycled, so these are just negative examples.",
            "So this was just basically to balance our training set, and we could probably refine this collection and come up with a little bit better techniques.",
            "Rather than these kinds of examples, which tend to soften be related but not in related in the same kinds of ways that the negative examples from RT set are."
        ],
        [
            "So performance reasons.",
            "Strictly we use the Hill climber trained on those 10,000 human annotated pairs to annotated selection of 450,000 chunk pairs selected from equally from these two corpora.",
            "And these annotations were then used to train of final Maxent classifier that was used in our final submission.",
            "So you can see that the performance of our classifiers went up from where the Maxim was.",
            "The Hill climber was at about 84% precision, about 80% F at this point and then when we use them, accent trained on the large set automatically.",
            "Imitated set decision, went up to about 90% or F1 up over the 90 mark so we felt this was sufficient for to go ahead into the next component.",
            "So the neck."
        ],
        [
            "Component which we initially entitled, paraphrases acquisition, but I'm working.",
            "We're settling back towards something.",
            "Our initial concept with this was gathering phrase level alternations for each text hypothesis pair, and this follows from a rich tradition of work.",
            "You know, starting most recently by Dolan bars, Liam Lee's seminal work in here Antion, Yama at all 2002 had an excellent work in terms of acquiring these kinds of paraphrases or phrase level alternations for information extraction applications.",
            "So we use the output from the alignment classifier to determine a target region of high correspondence between a text and hypothesis.",
            "And here's what I mean.",
            "So we have go back to our familiar example here, so I know that JP Losman and the bills are the most highly alone.",
            "Most of the entity alignments that I'm most highly confident about, and then I go ahead and just consider the text that falls in between those and then we have.",
            "We had a set of rules to be able to remove to basically perform some kind of sense compression.",
            "This was not a focus of our work, so this was.",
            "We did this only applied in certain cases.",
            "So I've got these two underlying passages that I'm interested in and this is what I'm trying to acquire some sort of alternation for.",
            "So if I can find an alternation between these two, appear ready to hand the reins over to or just hand the reins over to an give the starting job too, I may have strong evidence that the two sentences existing entailment relationship.",
            "So.",
            "So here's this example writ large.",
            "So if I can find so I have my examples here that I just showed you.",
            "These were three of the Top Rank paraphrases that we got back from the web, so the bills may go with quarterback JP Losman, that's all."
        ],
        [
            "Could decide to put their trust in JP Lawson?",
            "Yeah, that's very might turn the keys of the offense over to OK.",
            "This is sports writing, so it's colloquial.",
            "So OK, yeah, but not all sentences that we acquired containing both aligned entities were true paraphrases.",
            "So we have things like the Bills benched JP Bledsoe in favor of JP Losman, which doesn't mean it could be an affect or cause of.",
            "Of these kinds of outcomes here in green, or is thinking about cutting JP Losman, which has no relationships with what the examples were looking at.",
            "OK, so."
        ],
        [
            "Like Barzelay and Lee are approach focuses on creating clusters of paraphrases extracted automatically from the web.",
            "So we'll go briefly through the steps here, so we like I said before we took the two entities with the highest alignment confidence from each pair, and then we extracted the text passages that contain both aligned entities in the context window of EM words from each sentence.",
            "Then we use the aligned at the lined entities to extract to retrieve the top 500 documents containing each pair entities from Google and only kept the sentence that contain both entities.",
            "Sorry, I've got four and five here.",
            "Text messages were then the same kind of classes that we extracted.",
            "the Texan hypothesis.",
            "We've also extracting these new sentences from the web, and then the web passages that I got back in the original passages that I have were clustered using a complete link clustering algorithm that's outlined in bars line late and we threw away any cluster that had less than 10 passages, even if they include the original text boxes."
        ],
        [
            "So these paraphrases were had their best, most most useful application in terms of our entailment classifier.",
            "So like a number of other approximation based approaches to RT one you just heard about, we used the supervised machine learning classifier based on decision trees in order to determine whether the entailment relationship exists for a particular pair.",
            "So experiment with a number of machine learning techniques, including SPMS, maxene and decision trees.",
            "And in February 2006 major submission, a decision tree is significantly outperformed both Maxent next VM's.",
            "After some refactoring, features Mac sense, it still lags a little bit behind decision trees, but it's pretty much comparable and SVM still lag behind.",
            "So let's take a."
        ],
        [
            "Some kinds of features that we used, so we use the set of alignment features.",
            "So examples of these include something like the longest common substring.",
            "Common to both text and hypothesis.",
            "Underlying chunk, which counted the number of chunks in the hypothesis that were not aligned with chunks in the text dependency.",
            "Features that came from our semantic parsers.",
            "So we.",
            "These were believe some of these Boolean features so entity role match feature was a Boolean feature that fired when aligned entities were assigned the same role.",
            "Or so we kind of when they were exactly assigned the same ruler when they were nearly assigned the same role.",
            "So we knew our semantic parser made mistakes, often between hard ones or twos, or between the different types of our games.",
            "So we collapse those and also we had a stricter feature with predicate rules where if given a predicate, did it assign given to align predicates?",
            "Did they assign the exact same number of arguments in the exact same types of arguments?"
        ],
        [
            "More types of features we had paraphrase features which so the single paraphrase match was paraphrased from surviving Boolean feature that fired when a paraphrase from surviving cluster mast, either to text with the hypothesis and this kind of told us something useful.",
            "Namely, did we just select the correct entities, that alignment, and if I if I select the incorrect and incorrect set of entities alignment?",
            "You know, I may not know that, and so they had knowing that if I couldn't find a possible paraphrase of those two things, that leads me to believe that I picked the wrong pair of entities to start with.",
            "And you know this is kind of more existential question.",
            "Are we really dealing with something can be expressed in multiple ways, and you know if the answer is yes, then it makes sense to look for entailment.",
            "If answer is no, then we should probably think about returning no judgement.",
            "How am I doing on time?",
            "So something like.",
            "Just kind of skipping through here so the category match and this is 1 where if I found a paraphrase that matched the Texan, I found another paraphrase that's different from the Panthers paraphrase that match the hypothesis an I found both.",
            "Both of these paraphrases in the same surviving cluster therapists.",
            "This kind of suggested me that I found some other way of expressing the meaning about the text in the prothesis and the clustering algorithm believes ultimately put them in the same cluster because they expressed similar enough information.",
            "The last class of features and this is something we'd like to do more, much more in is in terms of semantic features, so we've since this is really our first time participating in the RT task and we really got started in late December of 2005, and the deadline was February 2nd, so you know, we really had to kind of do cost benefit analysis to be able to figure out exactly what we wanted to explore, so we only use two types of semantic features.",
            "We use the truth value mismatch, which said, which was a bully feature that fired when.",
            "Align predicates differed in any truth value, whether you know one was signed.",
            "True assignment, unresolved, almost sign falls on the sign.",
            "True, that would fire for the truth value mismatch and then we had a polarity mismatch which would fire when aligned predicates were assigned true values of opposite polarity.",
            "So just true and false.",
            "Like my slide here is screwed up so many good through all of it.",
            "OK, starting at the top.",
            "So let's take a look.",
            "Go back to our example if you can remember at this point.",
            "So just kind of recap the kind of impact of each of these classes of features.",
            "So we had alignment features and so this idea that idea was to try to figure out what align elements align in the text of the hypothesis.",
            "And we have two examples of good along."
        ],
        [
            "JP Losman and the Bills and we have two examples of marginal or passive alignment in the hand and give benefit from word net similarity and arrangement starting job well, since it's idiomatic, it's it's hard to find something you have the proof positive.",
            "Other short of a Dictionary of idioms that would say that these two things should be aligned.",
            "Dependency features really tried to determine whether to the same dependencies were assigned a corresponding entities in the text in the hypothesis.",
            "And keep in mind here, we don't necessarily have a notion of graphs like we heard about in the Stanford system.",
            "So we were just dealing with pairs of entities that have been aligned.",
            "So we have our zero.",
            "OK, that's great for the bills are one is great for this thing that we thought was marginally aligned.",
            "JP Losman poses a little bit of a problem because in the text he's not signed an argument role.",
            "And in the hypothesis is then paraphrase features move on to try to tell us whether there were any paraphrases that could be found that could be considered to be paraphrases.",
            "Portions of the text of the hypothesis, and we saw two of these have gone with quarterback or has or may have turned the keys of the offense over 2.",
            "I estimated features like I just described a second ago, were just to provide a last kind of check to see whether predicates were assigned the same truth values and so all these things kind of point towards the fact that this yes, in fact is a likely alignment.",
            "Her as we all."
        ],
        [
            "No, not all examples included so many complementary features.",
            "I selected this example on purpose, so I have this other example of 734 which is an example of no.",
            "And here we've got the government's economic development.",
            "First priority did not initially recognize the need for preventative measures to halt pollution, which may have slowed economic growth.",
            "In this corresponds to a hypothesis that the government took measures to reduce pollution.",
            "Well, no, the government didn't recognize need.",
            "It didn't take any steps for disclosure, so let's see what happens."
        ],
        [
            "The King or the alignments with the government's priority lines with government did not recognize lines with took the need for preventive measures.",
            "This whole chunk here lines with just the spare impede their plural measures.",
            "Hall aligns with reducing pollution.",
            "Realize with pollution.",
            "OK, so the government spa priority lining with the government.",
            "This is bad for a number of reasons, right?",
            "It's partial alignment, even though it does have an article match, it hasn't any category mismatch and it's only a passable.",
            "Not recognize and took is even worse.",
            "It lines in part of speech, but it mismatches in clarity and we couldn't find any way of saying that these things even close to amend the same thing.",
            "We got a little bit better alignment for the partial alignment between the need for preventive measures.",
            "OK, that's fine.",
            "Paulton reduce.",
            "There's a nice.",
            "There's a nice alignment there, and then pollution pollution.",
            "So evil Emma matches.",
            "This is nice example one.",
            "But in contrast here few paraphrases could be clustered, found that included passes extracted from the text right about the Sewart target entities.",
            "Here was the government and the government's priority and pollution.",
            "And so you know the ones in yellow or the original text that were extracted from the text of the hypothesis.",
            "And then I have three things that weren't the top things that came back were not considered to be paraphrases.",
            "So in fact, these three things were the only cluster that came back and survived, and they didn't need that extra hypothesis.",
            "That's why they're labeled in red.",
            "So has allowed companies to get away with.",
            "It is looking for ways to deal with wants to forget about.",
            "These were selected from that cluster.",
            "All these things add up to an unlikely entailment, and that's where our system reduced produced.",
            "So."
        ],
        [
            "Groundhog this year recognized entailment.",
            "About 75% of examples in the RTT set and so notable numbers here were like everyone else we did best in the summarization set and we had the highest average precision in the IE test.",
            "Overall, we were pleased with our both our average precision numbers, but since it's a new metric, we're not really necessarily sure we have to perform more analysis to be able to see really what this means as opposed to last year's metric, or really what having a high average precision means for us, our performance did.",
            "Many other other peoples experience did differ markedly across the four sub tasks, and this was significant and we believe this what you know it does have.",
            "There's a component that has to do with the makeup of each of these tasks, but it also has something to do with our training data.",
            "So our headline corpus features a large number of some sentence compression, like examples.",
            "If you can think back to the example I showed before, but the Sydney Olympics in that case there's lots of lexical overlap.",
            "There's alot of fewer words in the hypothesis.",
            "And so when we remove, we took a lot of those examples.",
            "Then we train Groundhog on a balance training corpus that how late you know had much fewer examples from basically only 800 examples from the training corpus.",
            "From that headline corpus, the performance in summarization task fell to 79.3%, so we'd need to run more tests to be able to kind of get you defended numbers on this story and to kind of perform some sort of an ounce of analysis of variance to be able to figure out exactly what the culprit is here.",
            "But one culprit could be the headline corpus that we used.",
            "Training data did play an important role in boosting are over."
        ],
        [
            "So performance in February Boot was increased from 65% to about 75% when we use about 200,000 examples from our training corpus.",
            "In the intervening months, we've since we only had about six weeks to put the system together in the 1st place.",
            "Re fact we've been going through the last couple of six weeks and we've had one or two people refactoring features and experimenting with different combinations of features.",
            "One of the disadvantages of working with 200,000 examples is that it takes a heck of a long time to train and that you have to go back.",
            "So we spend a lot of our future development with smaller corpora and then when we only use the larger corporate for the final submission.",
            "So we've been able to increase our overall performance from the 2006 Death Step by about 5%.",
            "And you can see here that the performance increase seems to be tapering off now.",
            "We don't get the same kind of big gains me before you saw a 6% jump up between going from 50,000 examples.",
            "100,000 examples.",
            "Now we don't see that.",
            "I mean, we've kind of we've kind of been able to roll a lot of that into the initial score.",
            "And we're only able to go up using the 200,000 examples.",
            "Only gets up about 6%, so we're not sure necessarily if this represents a stealing.",
            "It will keep it posted as we learn more about this.",
            "Our best results in."
        ],
        [
            "And ablation study across those four sets of features using the Taylor classifier.",
            "Our best results were obtained by combining all four sets of features.",
            "As you can see, here's our 75.38 number, but our largest gains for whatever reason were observed by adding paraphrase features so.",
            "By adding paraphrase features, we want to 4% here, another 4% there, and so you know this.",
            "But we have a lot of mileage still to be gained out of the paraphrase features themselves.",
            "A lot of what we did was just reimplementing bars.",
            "Lion, Lee and.",
            "With some lexical semantic annotations that we have at LCC, you know kind of trying to enhance some of that enhance the clustering algorithm that we use, but pretty much it was a straight up implementation of our slightly so we have some ideas of how we can move forward with you know, better representing the passage is better represented, better detecting fuzzy matches between the passes that are extracted in the passages that we get from the web and trying to kind of boost performance in the system.",
            "And also we've used rather simplistic features in our paraphrase component within the classifier now, and so we feel there's gains that we have in that.",
            "So to wrap up, we've introduced A3 tiered approach to recognizing textual entailment, and this kind of hinges on kind of a nob we've all talked."
        ],
        [
            "Alignment in number, different ways, and we feel that our approach alignment is is somewhat novel in terms that we're only comparing.",
            "We're basically looking every possible combination of constituents from attacks and hypothesis and comparing them based on a wide range of lexical semantic features.",
            "Then we have this kind of very nascent paraphrase acquisition module that arise phrase level alternations for passages contained in high confidence for aligned entities, and then we have a decision tree based entailment classified are fire that works to combine all of these features in order to make an intelligent decision.",
            "So, and the other takeaway point here that I'd like you to walk away with is that you know we've showed that it's possible.",
            "You know, somehow you know by in our own shop by relaxing the notion of strict element in order to train training corporate that can prove effective in training systems for the R key task, you don't necessarily.",
            "I mean, I would not recommend this in terms of creating new development sets from next years are key challenge, but you don't necessarily have to train on perfect examples of entailment to do well at recognizing entailment.",
            "Recognizing so we were able to acquire about 200,000 examples using some relatively simple heuristics and some elbow grease in terms of filtering.",
            "Thanks so much.",
            "One place in space related today.",
            "Increasing employee performance that you had from February 2016.",
            "Terabytes per second value.",
            "We shooting increasement of the pharmacies using 100 samples and if I can also assume questions at least.",
            "I missed again in the house.",
            "Negative effects samples from doing so.",
            "More than happy to dress code for this.",
            "See it, I wish I could tell you something you know some greater insight, but a lot of the gains performance gains that we've had in the last two months or so has stemmed from just different feature combination.",
            "So we thought we were at a maximum in terms of combining different features that we were playing with.",
            "It basically comes down to the fact that we developed this system in six weeks, and so in the last eight weeks we've been able to identify the other feature combinations which perform a little bit better.",
            "Enter, take out some of the bugs in the features that we were using, so there were simple.",
            "Lack of better term, just coding mistakes that were brought on by the pressures of having to do this in six weeks.",
            "In terms of the negative examples.",
            "So we use two different approaches, one which was we tried to find sequential senses that feature the repeat mention editing and this was something that was facilitated rest by the fact that we have good named entity recognition and rudimentary coreference detection.",
            "So here in this example we were able to take Hideki Irabu and Irabu.",
            "Here in this case and so we knew that both of these sentences featured some 2 facts about for lack of a better word about Hideki Irabu.",
            "But we realized that most discourses that you probably wouldn't you would violate informativeness to say the same thing twice in a row, specially in a newswire text.",
            "I wouldn't tell you that here.",
            "It's Hideki Irabu 64, and then say the hit the six foot four Hideki Irabu.",
            "Highlights greissing informative, informativeness and so you know it's it's a relatively simple assumption, but one that got us a lot of examples that a lot of that lexical overlap.",
            "So I have Irabu and wells here, so this looks like you know prototypical example that we would want to classify.",
            "We would find are Abu and Wells as our target entities, and then we try to see whether the passage or the passage within the context you know said he would take out well or take wells out, or who was befriended by and to see if those two things were paraphrase or to those two things were an example of entailment.",
            "So this is the classic example, I mean and truth be told, when we first started training on this, examples in these were the kinds of things that we would.",
            "Falsely predict yes on another thing we did was use discourse connectives so the idea is that discourse connectives are going things like either though or although we're going to tell Me 2 complementary facts about a certain entity, so I'm going to tell you X and then, but I would say, but you know, some conventional implicature that you may have, or even conversational, because we may have doesn't isn't true, so I'm going to tell you something, and I'm going to cancel some part.",
            "You know what the normal interpretation of that is.",
            "So here you know, cleaning up oil slicks is extremely costly.",
            "And then I have.",
            "But you know this is not the best example here, but since I don't have oil slicks mentioned again and that's what crusade would need to know about you.",
            "But I guess the take home message here is that we were able to train on these examples which you know we were skeptical about in the 1st place and we expected to see beautiful examples of entailment pulled directly from the newswire documents.",
            "But we were able to.",
            "They were able to ultimately benefit the system in the end by learning it by performing.",
            "You know there's a lot to be learned here in terms of just lexical correspondence in terms of alignment in terms of trying to figure out the part of the configuration of certain kinds of features.",
            "No, we had.",
            "We had great well I mean hand annotation is is a is a dirty word but we used we had grammars to deal with negation.",
            "Oh, I mean, I'm sorry, negative examples in negation we didn't hand entertain any of this.",
            "We felt mean.",
            "The closest we came to was we spot checked it and filtered it based on topic and keyboards and things like that.",
            "I apologize something."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, I'm analytical and sorry I'm Andrew Hickl and I'm going to be spending some time today talking to you a little bit about the first of LCS two submissions to this year's RT challenge.",
                    "label": 0
                },
                {
                    "sent": "This group, this.",
                    "label": 0
                },
                {
                    "sent": "System was developed under the direction of Sanda Harbhajan and it's worked, coauthored with my colleagues back at LCC.",
                    "label": 0
                },
                {
                    "sent": "So we're really grateful for that.",
                    "label": 0
                },
                {
                    "sent": "There are opportunities participate in this year's RT to challenge.",
                    "label": 0
                },
                {
                    "sent": "This really was our first formal exposure to Artie.",
                    "label": 0
                },
                {
                    "sent": "We had a preliminary exposure in the fall of 2005 with the acquaint knowledge base evaluation and this included all the another number of other Pascal veterans, including University, Colorado University, Illinois, Stanford University of Texas at Dallas, and Else sees other research group led by Doctor Dan Mold IMO.",
                    "label": 0
                },
                {
                    "sent": "While this year's revaluation really.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Represented our first 484 A into the RT taskbar.",
                    "label": 1
                },
                {
                    "sent": "Groups really worked extensively towards performing the types of textual inference.",
                    "label": 1
                },
                {
                    "sent": "Their crucial for a number of NLP applications.",
                    "label": 1
                },
                {
                    "sent": "These include question answering, information extraction, multi document summarization named any recognition, temporal and spatial normalization, and things like semantic parsing.",
                    "label": 0
                },
                {
                    "sent": "So just as an overview of where we're going to go today, start with a little bit of an introduction, and then I'm going to go through each of the components that are in our system, which is affectionately known as Groundhog, just as in the slide here, the original RT deadline was on February 2nd, which in the United States is Groundhog Day.",
                    "label": 0
                },
                {
                    "sent": "For a variety of reasons, which I won't go into here, so my team renamed our system, which had some other name at that point to the lovely Groundhog which you see here.",
                    "label": 0
                },
                {
                    "sent": "Basically only.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Derivation of the name.",
                    "label": 0
                },
                {
                    "sent": "Then we'll go through each of these components in order, and then I will talk a little bit about our evaluation.",
                    "label": 0
                },
                {
                    "sent": "Both that was conducted from the RT organizers, and then we'll conclude with a few points.",
                    "label": 0
                },
                {
                    "sent": "So let me see the first introduction to the ground type system would give you is kind of just walk you through each of the components and a lot of this will be familiar from the other presentations that you've seen today.",
                    "label": 0
                },
                {
                    "sent": "So we start with a pre prod.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we preprocess both the test set and a set of training corpora that we've acquired.",
                    "label": 0
                },
                {
                    "sent": "And this includes a whole host or rich semantic annotations including named entity recognition, temporal normalization and ordering information, syntactic and semantic parsing, name aliasing, name coreference, and a variety of semantic annotations that are performed using heuristics designed to capture kinds of propositional semantics.",
                    "label": 1
                },
                {
                    "sent": "These kinds of annotations then are sent to a lexical alignment component, and we use a machine learning classifier to be able to perform to identify which components are which constituents between the text and the hypothesis contain corresponding information.",
                    "label": 0
                },
                {
                    "sent": "These the highest confidence entities that are aligned out of this lexical line of classifier, then sent to a paraphrasing or a phrase level alternation acquisition module which uses the web as its corpus to be able to drive these alternations.",
                    "label": 1
                },
                {
                    "sent": "These features, including everything from the preprocessing, the lexical alignment, and the paraphrase acquisition, are then sent to a feature extraction module which feeds it entailment classification module based on decision trees in order to produce an output, whether yes or no, and a confidence value.",
                    "label": 0
                },
                {
                    "sent": "So as I talk about each of these components, I'm going to be motivating or grounding.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this example here, and this is a yes example that we got right.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, and it basically has to do with JP Losman who's the quarterback for the Bills and the American football team.",
                    "label": 0
                },
                {
                    "sent": "the Buffalo Bills.",
                    "label": 0
                },
                {
                    "sent": "And so this is looking at this kind of example.",
                    "label": 0
                },
                {
                    "sent": "Let us to ask a number of questions.",
                    "label": 0
                },
                {
                    "sent": "So first, what are the important portions that should be considered by a system?",
                    "label": 0
                },
                {
                    "sent": "The text contains a lot of information, three different predicates and number of different entities.",
                    "label": 0
                },
                {
                    "sent": "And really one of the first task that we wanted to solve was to kind of hone in on the string or this.",
                    "label": 0
                },
                {
                    "sent": "You know, the set of tokens that was most important to be able to perform aarti and so we you know our hypothesis was that lexical alignment could be used to identify these strings.",
                    "label": 0
                },
                {
                    "sent": "So here if I understand the bills and JP Losman are the two people that are.",
                    "label": 0
                },
                {
                    "sent": "Two entities that I'm interested in and I only have to perform some sort of computation across these two strings that are underlined in the figure above.",
                    "label": 0
                },
                {
                    "sent": "Additionally, we need to have some mechanism to determine if the same meaning is being conveyed by phrases that may not be lexically related.",
                    "label": 0
                },
                {
                    "sent": "So in this case here we have the colocation of the idiom hand the reins over and give the starting job too, so we need some sort of mechanism and phrase level alternations or what we're referring to loosely here, as paraphrases may be able to help.",
                    "label": 0
                },
                {
                    "sent": "I'm finally we need.",
                    "label": 0
                },
                {
                    "sent": "We all need some sort of mechanism to be able to deal with the complexity that reduces the effectiveness of our parsers.",
                    "label": 0
                },
                {
                    "sent": "We've talked about this time and time again when these be syntactic parsers are semantic parses.",
                    "label": 0
                },
                {
                    "sent": "Here you can see in the white box for the text I've got the semantic parts produced by our system, and you know if I look focusing right here just on the predicate hand, I see that I've got an R0.",
                    "label": 0
                },
                {
                    "sent": "You know these prop bank style annotations of the bills and the brains, but the Arc 2 here is just that.",
                    "label": 0
                },
                {
                    "sent": "You know the the that NP ahead of that Proposition 2 one.",
                    "label": 0
                },
                {
                    "sent": "So there's a longer NP here that starts with one and goes all the way down to JP Losman and the idea is that you know we can use coreference information to reconstruct some of that, but we need to know that JP Losman.",
                    "label": 0
                },
                {
                    "sent": "As some sort of semantic dependency back to the predicate hand and our semantic parser as it stands now, doesn't get this for us.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's talk about the each of these components in order so just briefly to talk about the different kinds of rich semantics that Groundhog offers first named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "So we use else sees in how Cicero light name any recognition software to categorize entities with about 150 different types of named entities.",
                    "label": 0
                },
                {
                    "sent": "So you can see we get things like we recognize the bills are not just an organization, but there are sports organization.",
                    "label": 0
                },
                {
                    "sent": "Recognize JP Losman as a person recognized variety of time, experience, expressions.",
                    "label": 0
                },
                {
                    "sent": "Anna broken Leg is for better or worse or body part.",
                    "label": 0
                },
                {
                    "sent": "We also do name aliasing and coreference using grammars and heuristics that are found natively in Cicero, light and so we're able to recognize that the bills is not only coreferential with the possessive pronoun there in the text.",
                    "label": 0
                },
                {
                    "sent": "It's also coreferential with the bills in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Also, JP Losman is.",
                    "label": 0
                },
                {
                    "sent": "We know that you know, based on the kind of noun noun compound quarterback JP Losman, that he's preferential that is correct wrenshall and we also know that he's coreferential with relative pronoun who.",
                    "label": 0
                },
                {
                    "sent": "Moving right along we also we used.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cici's teiser, which is a Timex system, developed with last year's ace valuations, with attorney valuations in Ace to be able to normalize time expressions to their ISO 9000 values.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "There's also some rules in this system to compute the relative order of time expressions within a context, so we can map things like now to a document time here it's just defaulted to the first day, January in 2006, and then range operators such things like a year ago.",
                    "label": 0
                },
                {
                    "sent": "So with regards to some sort of document time.",
                    "label": 0
                },
                {
                    "sent": "We also used LCC zone implementation of the Brill POF stagger and the Collins parser to be able to syntactically parsed sentence is and we also used a series of post processing rules to be able to identify phrase chunks, phrase heads, relative clauses, appositives and parenthetical.",
                    "label": 0
                },
                {
                    "sent": "Some other kinds of syntactic constructions.",
                    "label": 0
                },
                {
                    "sent": "Semantic parsing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was used with another system developed at LCC that was it's performed using Maxent based semantic role labeling system that's been trained on Prop bank annotations and just to see how this works here.",
                    "label": 0
                },
                {
                    "sent": "We've got three different predicates in our text so the first one could be something like appear.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already and we have an R0 and R GAM, which is the temporal adjunct.",
                    "label": 0
                },
                {
                    "sent": "Just those two argues that predicate for the predicate hand.",
                    "label": 0
                },
                {
                    "sent": "I've got the bills of my heart 0.",
                    "label": 0
                },
                {
                    "sent": "Again here I have the Arg, one is the rains and then depending on the constituents that you use either one or one of their two topics as they are two.",
                    "label": 0
                },
                {
                    "sent": "I have also have this predicate missed which doesn't shouldn't feature in our car entailment calculation which has a.",
                    "label": 0
                },
                {
                    "sent": "Subject going around a graph, the subject of who here or that race, and then most of last season.",
                    "label": 0
                },
                {
                    "sent": "It's hard one and a broken leg as its other arguing.",
                    "label": 0
                },
                {
                    "sent": "And we could do this with the hypothesis as well.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last component process.",
                    "label": 0
                },
                {
                    "sent": "Then we'll talk about today is semantic annotation, and we use it as just mentioned.",
                    "label": 1
                },
                {
                    "sent": "We used also used heuristics to annotate a number of different kinds of following different types of semantic information, and some of these you'll find familiar from other presentations today, and most notably the Stanford presentation.",
                    "label": 0
                },
                {
                    "sent": "So we like a number of other groups we annotated for clarity so when predicates are nominals, event in anomalies were found, the scope of negative marker, or associated with indignation, denoting verbal and refuse.",
                    "label": 0
                },
                {
                    "sent": "We also annotated factive verbs, which we define as predicates that conventionally imply the truth of their complements.",
                    "label": 1
                },
                {
                    "sent": "So when I have a verb like admit, I know that necessarily that the speaker and the context is committed to the truth of the predicate be unlikely.",
                    "label": 0
                },
                {
                    "sent": "Also we annotated non factive verbs, so this is a larger class and.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably require should be decomposed into a number of different number different categories, so we each of these types, whether they were speech act verbs, nonfactor speech, actors like denier, Claymore, psych, verbs like think or believe, or verbs of uncertainty or likelihood would be uncertain.",
                    "label": 1
                },
                {
                    "sent": "Would be likely intentions or plans scheme plot want or conditional context which aren't the same kind of thing, but they have the same kind of effect on predicates.",
                    "label": 0
                },
                {
                    "sent": "We mark these as unresolved, so namely, we couldn't tell whether they are true or not, or.",
                    "label": 0
                },
                {
                    "sent": "In the current context and so here's an example that indicates here you know, we claim that these when someone claims that something is in contradiction, we don't necessarily know if that's just their claim, or if that's actually the reality of the situation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The category of semantic annotations.",
                    "label": 0
                },
                {
                    "sent": "We perform where what Huddleston and Pullum or Geoff Pullum at Santa Cruz refers to as supplemental expressions, and these are.",
                    "label": 0
                },
                {
                    "sent": "Syntactic constructions such as the positives or epithets or name aliases or as clauses or non restrictive relative clauses that necessarily trigger conventional implicatures.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at these.",
                    "label": 0
                },
                {
                    "sent": "These do so.",
                    "label": 0
                },
                {
                    "sent": "I haven't a nominal appositive construction like this where I have Shia pilgrims Virgin Karbala to mark the depth Hussein the Prophet Muhammad's grandson 1300 years ago, and what we did to be able to simplify the process in the piece was we extracted that a positive construction and append it to the end of the text or hypothesis that was founded.",
                    "label": 0
                },
                {
                    "sent": "So here we have she pilgrims converge in carbs up to mark the deficit Hussein 1300 years ago, directly from the text and namely Hussein is the Prophet Muhammad's grandson and this allowed.",
                    "label": 0
                },
                {
                    "sent": "It is provided to small advantage in terms of later processing.",
                    "label": 0
                },
                {
                    "sent": "You will see in a second we did this for named aliases.",
                    "label": 0
                },
                {
                    "sent": "So we have the reference to Ali Al Timimi as the radical Islamic cleric and we made sure that was explicitly mentioned in the text.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As clauses, so the LMI was set up by Mr. Torvalds as a non profit organization.",
                    "label": 0
                },
                {
                    "sent": "We just made these things explicitly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all of these kinds of all this preprocessing serves to inform our later components and the next one that we're going to see in our pipeline.",
                    "label": 0
                },
                {
                    "sent": "Here is the issue.",
                    "label": 0
                },
                {
                    "sent": "There are the notion of lexical alignment, so we really believe that these kinds of lexico semantic information, along with things like the lemma of the actual form of the word, can provide us with the input to be able to find corresponding tokens between the tax in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So in most cases, in a lot of cases we just have textual overlap, so the bills are in the taxes.",
                    "label": 0
                },
                {
                    "sent": "Also, can should be correspond should correspond to the bills in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "JP Losman, with the same way, but we have, you know, tougher things like hand and give so hand the reins over and give the starting job too.",
                    "label": 0
                },
                {
                    "sent": "We need to know that these two things are similar and word net can help us here in our semantic annotations can help us here but we need you know we need.",
                    "label": 0
                },
                {
                    "sent": "We think the problem requires more than just this just.",
                    "label": 0
                },
                {
                    "sent": "These kinds of maybe surface semantic approaches also.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like the rains and the starting job even though they occupy the same semantic role in the sentence, we need to figure out whether these two things should be whether these things contain corresponding semantic information or not.",
                    "label": 0
                },
                {
                    "sent": "So we use the Maxent classifier to compute the probability that an element computed selected from a text corresponds to or shorthand aligns with an element selected by populists with three step process.",
                    "label": 0
                },
                {
                    "sent": "So we decompose sentences into a set of alignable chunks that was essentially the output from a chunk parser in a colocation detection system.",
                    "label": 0
                },
                {
                    "sent": "We put those chunks into a matrix and then we pull each pair chunks out of that matrix and submit it to the classifier.",
                    "label": 0
                },
                {
                    "sent": "The classifier output the probability that the pair represented a positive example of alignment.",
                    "label": 0
                },
                {
                    "sent": "We use four sets of features and these are just.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some of the some of the features that we use in each of these categories so things things like a statistical features like cosine similarity or Glickman into guns, lexical entailment, probability, lexico semantic features like word, net similarity that we have several other groups have used Ted Peterson's package, Wordnet, synonymy, and antinomy, named entity features alternations that we had acquired previous for previous applications.",
                    "label": 1
                },
                {
                    "sent": "Like you, a series of purely string based features like Levenshtein, edit distance or morphological stem equality, and then syntactic features such as the maximal category that the token was found in.",
                    "label": 1
                },
                {
                    "sent": "Whether the token was ahead of its own phrase or end, things like the structure of the NP the token was founded, weather included.",
                    "label": 0
                },
                {
                    "sent": "Modifyers weather.",
                    "label": 0
                },
                {
                    "sent": "Was that subordinate, PP was within an NP NP compound Dino Combo.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to be able to train this classifier, we annotated a set of 10,000 alignment chunkers taken from the development set as either positive negative examples of alignment and then we train to different classifiers that Hill climber just for kicks Anna Maxent classifier an we got relatively decent performance, but we found that when we sample we tested on different portions of that 10,000 trained and tested different portions of that annotated set.",
                    "label": 1
                },
                {
                    "sent": "The F measure varied significantly and so we wanted to look for something that was more robust.",
                    "label": 0
                },
                {
                    "sent": "So they could deal with unseen data a little bit better, so we were able to perform.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically experiment with together at two different techniques for gathering training data.",
                    "label": 1
                },
                {
                    "sent": "Set that type of so we used following Burger and Farro.",
                    "label": 0
                },
                {
                    "sent": "We created a corpus about 100,000 positive examples of entailment by pairing the headline of a document with the first sentence of the newswire document.",
                    "label": 1
                },
                {
                    "sent": "So for example, here we've got this first line.",
                    "label": 0
                },
                {
                    "sent": "Sydney Newspapers made a secret bid not to report on the funding, and spending made through the city's successful bid for the 2000 Olympics and headline was papers said to protect Sydney bid.",
                    "label": 1
                },
                {
                    "sent": "Now we filtered these examples extensively and then we will Smit only a small portion of these for evaluation and we didn't use the test of full strict logical entailment or even textual entailment.",
                    "label": 0
                },
                {
                    "sent": "On these examples.",
                    "label": 0
                },
                {
                    "sent": "We just wanted to make sure that we we were OK with dealing with some noise in the data and so when we had our attitudes look back at this, we want to make sure you know is.",
                    "label": 0
                },
                {
                    "sent": "Could this be considered to be similar enough that in some context in some case they could be considered to be?",
                    "label": 0
                },
                {
                    "sent": "Either paraphrase or entailment, using some sort of semantic correspondence between these two.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also to be able to build a balanced rains that we've gathered.",
                    "label": 0
                },
                {
                    "sent": "120,000 negative examples of textual entailment.",
                    "label": 0
                },
                {
                    "sent": "We use two strategies to do this.",
                    "label": 0
                },
                {
                    "sent": "We selected sequential sentences for newswire text that featured are repeat mention named Entity.",
                    "label": 0
                },
                {
                    "sent": "For example, you know and this core structure will predict this shouldn't actually occur, but we found a number of examples.",
                    "label": 0
                },
                {
                    "sent": "So one player losing a close friend is Japanese.",
                    "label": 0
                },
                {
                    "sent": "Pitcher Hideki Irabu was befriended by Wills during spring training.",
                    "label": 0
                },
                {
                    "sent": "Are Abu.",
                    "label": 0
                },
                {
                    "sent": "Here's the correct of the entity so that he will take wells up dinner when the Yankees visit Toronto.",
                    "label": 0
                },
                {
                    "sent": "We also extracted pairs of sentences.",
                    "label": 0
                },
                {
                    "sent": "They would list linked by discourse connectives that we expected to have similar kinds of properties, things like even though although in contrast.",
                    "label": 0
                },
                {
                    "sent": "So according to the professor present methods of cleaning up oil slicks are extremely costly and are never completely efficient, so we have this hypothesis clean magazine, 100% pollution retrieval rate is low cost and can be recycled, so these are just negative examples.",
                    "label": 0
                },
                {
                    "sent": "So this was just basically to balance our training set, and we could probably refine this collection and come up with a little bit better techniques.",
                    "label": 0
                },
                {
                    "sent": "Rather than these kinds of examples, which tend to soften be related but not in related in the same kinds of ways that the negative examples from RT set are.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So performance reasons.",
                    "label": 0
                },
                {
                    "sent": "Strictly we use the Hill climber trained on those 10,000 human annotated pairs to annotated selection of 450,000 chunk pairs selected from equally from these two corpora.",
                    "label": 0
                },
                {
                    "sent": "And these annotations were then used to train of final Maxent classifier that was used in our final submission.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the performance of our classifiers went up from where the Maxim was.",
                    "label": 0
                },
                {
                    "sent": "The Hill climber was at about 84% precision, about 80% F at this point and then when we use them, accent trained on the large set automatically.",
                    "label": 0
                },
                {
                    "sent": "Imitated set decision, went up to about 90% or F1 up over the 90 mark so we felt this was sufficient for to go ahead into the next component.",
                    "label": 0
                },
                {
                    "sent": "So the neck.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Component which we initially entitled, paraphrases acquisition, but I'm working.",
                    "label": 0
                },
                {
                    "sent": "We're settling back towards something.",
                    "label": 0
                },
                {
                    "sent": "Our initial concept with this was gathering phrase level alternations for each text hypothesis pair, and this follows from a rich tradition of work.",
                    "label": 0
                },
                {
                    "sent": "You know, starting most recently by Dolan bars, Liam Lee's seminal work in here Antion, Yama at all 2002 had an excellent work in terms of acquiring these kinds of paraphrases or phrase level alternations for information extraction applications.",
                    "label": 0
                },
                {
                    "sent": "So we use the output from the alignment classifier to determine a target region of high correspondence between a text and hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And here's what I mean.",
                    "label": 0
                },
                {
                    "sent": "So we have go back to our familiar example here, so I know that JP Losman and the bills are the most highly alone.",
                    "label": 0
                },
                {
                    "sent": "Most of the entity alignments that I'm most highly confident about, and then I go ahead and just consider the text that falls in between those and then we have.",
                    "label": 0
                },
                {
                    "sent": "We had a set of rules to be able to remove to basically perform some kind of sense compression.",
                    "label": 0
                },
                {
                    "sent": "This was not a focus of our work, so this was.",
                    "label": 0
                },
                {
                    "sent": "We did this only applied in certain cases.",
                    "label": 0
                },
                {
                    "sent": "So I've got these two underlying passages that I'm interested in and this is what I'm trying to acquire some sort of alternation for.",
                    "label": 0
                },
                {
                    "sent": "So if I can find an alternation between these two, appear ready to hand the reins over to or just hand the reins over to an give the starting job too, I may have strong evidence that the two sentences existing entailment relationship.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here's this example writ large.",
                    "label": 0
                },
                {
                    "sent": "So if I can find so I have my examples here that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "These were three of the Top Rank paraphrases that we got back from the web, so the bills may go with quarterback JP Losman, that's all.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could decide to put their trust in JP Lawson?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's very might turn the keys of the offense over to OK.",
                    "label": 0
                },
                {
                    "sent": "This is sports writing, so it's colloquial.",
                    "label": 0
                },
                {
                    "sent": "So OK, yeah, but not all sentences that we acquired containing both aligned entities were true paraphrases.",
                    "label": 1
                },
                {
                    "sent": "So we have things like the Bills benched JP Bledsoe in favor of JP Losman, which doesn't mean it could be an affect or cause of.",
                    "label": 0
                },
                {
                    "sent": "Of these kinds of outcomes here in green, or is thinking about cutting JP Losman, which has no relationships with what the examples were looking at.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like Barzelay and Lee are approach focuses on creating clusters of paraphrases extracted automatically from the web.",
                    "label": 1
                },
                {
                    "sent": "So we'll go briefly through the steps here, so we like I said before we took the two entities with the highest alignment confidence from each pair, and then we extracted the text passages that contain both aligned entities in the context window of EM words from each sentence.",
                    "label": 1
                },
                {
                    "sent": "Then we use the aligned at the lined entities to extract to retrieve the top 500 documents containing each pair entities from Google and only kept the sentence that contain both entities.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I've got four and five here.",
                    "label": 0
                },
                {
                    "sent": "Text messages were then the same kind of classes that we extracted.",
                    "label": 1
                },
                {
                    "sent": "the Texan hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We've also extracting these new sentences from the web, and then the web passages that I got back in the original passages that I have were clustered using a complete link clustering algorithm that's outlined in bars line late and we threw away any cluster that had less than 10 passages, even if they include the original text boxes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these paraphrases were had their best, most most useful application in terms of our entailment classifier.",
                    "label": 0
                },
                {
                    "sent": "So like a number of other approximation based approaches to RT one you just heard about, we used the supervised machine learning classifier based on decision trees in order to determine whether the entailment relationship exists for a particular pair.",
                    "label": 0
                },
                {
                    "sent": "So experiment with a number of machine learning techniques, including SPMS, maxene and decision trees.",
                    "label": 0
                },
                {
                    "sent": "And in February 2006 major submission, a decision tree is significantly outperformed both Maxent next VM's.",
                    "label": 0
                },
                {
                    "sent": "After some refactoring, features Mac sense, it still lags a little bit behind decision trees, but it's pretty much comparable and SVM still lag behind.",
                    "label": 0
                },
                {
                    "sent": "So let's take a.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some kinds of features that we used, so we use the set of alignment features.",
                    "label": 0
                },
                {
                    "sent": "So examples of these include something like the longest common substring.",
                    "label": 0
                },
                {
                    "sent": "Common to both text and hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Underlying chunk, which counted the number of chunks in the hypothesis that were not aligned with chunks in the text dependency.",
                    "label": 1
                },
                {
                    "sent": "Features that came from our semantic parsers.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "These were believe some of these Boolean features so entity role match feature was a Boolean feature that fired when aligned entities were assigned the same role.",
                    "label": 0
                },
                {
                    "sent": "Or so we kind of when they were exactly assigned the same ruler when they were nearly assigned the same role.",
                    "label": 1
                },
                {
                    "sent": "So we knew our semantic parser made mistakes, often between hard ones or twos, or between the different types of our games.",
                    "label": 0
                },
                {
                    "sent": "So we collapse those and also we had a stricter feature with predicate rules where if given a predicate, did it assign given to align predicates?",
                    "label": 0
                },
                {
                    "sent": "Did they assign the exact same number of arguments in the exact same types of arguments?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More types of features we had paraphrase features which so the single paraphrase match was paraphrased from surviving Boolean feature that fired when a paraphrase from surviving cluster mast, either to text with the hypothesis and this kind of told us something useful.",
                    "label": 0
                },
                {
                    "sent": "Namely, did we just select the correct entities, that alignment, and if I if I select the incorrect and incorrect set of entities alignment?",
                    "label": 0
                },
                {
                    "sent": "You know, I may not know that, and so they had knowing that if I couldn't find a possible paraphrase of those two things, that leads me to believe that I picked the wrong pair of entities to start with.",
                    "label": 0
                },
                {
                    "sent": "And you know this is kind of more existential question.",
                    "label": 0
                },
                {
                    "sent": "Are we really dealing with something can be expressed in multiple ways, and you know if the answer is yes, then it makes sense to look for entailment.",
                    "label": 0
                },
                {
                    "sent": "If answer is no, then we should probably think about returning no judgement.",
                    "label": 0
                },
                {
                    "sent": "How am I doing on time?",
                    "label": 0
                },
                {
                    "sent": "So something like.",
                    "label": 0
                },
                {
                    "sent": "Just kind of skipping through here so the category match and this is 1 where if I found a paraphrase that matched the Texan, I found another paraphrase that's different from the Panthers paraphrase that match the hypothesis an I found both.",
                    "label": 0
                },
                {
                    "sent": "Both of these paraphrases in the same surviving cluster therapists.",
                    "label": 1
                },
                {
                    "sent": "This kind of suggested me that I found some other way of expressing the meaning about the text in the prothesis and the clustering algorithm believes ultimately put them in the same cluster because they expressed similar enough information.",
                    "label": 0
                },
                {
                    "sent": "The last class of features and this is something we'd like to do more, much more in is in terms of semantic features, so we've since this is really our first time participating in the RT task and we really got started in late December of 2005, and the deadline was February 2nd, so you know, we really had to kind of do cost benefit analysis to be able to figure out exactly what we wanted to explore, so we only use two types of semantic features.",
                    "label": 0
                },
                {
                    "sent": "We use the truth value mismatch, which said, which was a bully feature that fired when.",
                    "label": 0
                },
                {
                    "sent": "Align predicates differed in any truth value, whether you know one was signed.",
                    "label": 0
                },
                {
                    "sent": "True assignment, unresolved, almost sign falls on the sign.",
                    "label": 0
                },
                {
                    "sent": "True, that would fire for the truth value mismatch and then we had a polarity mismatch which would fire when aligned predicates were assigned true values of opposite polarity.",
                    "label": 0
                },
                {
                    "sent": "So just true and false.",
                    "label": 0
                },
                {
                    "sent": "Like my slide here is screwed up so many good through all of it.",
                    "label": 0
                },
                {
                    "sent": "OK, starting at the top.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look.",
                    "label": 0
                },
                {
                    "sent": "Go back to our example if you can remember at this point.",
                    "label": 0
                },
                {
                    "sent": "So just kind of recap the kind of impact of each of these classes of features.",
                    "label": 0
                },
                {
                    "sent": "So we had alignment features and so this idea that idea was to try to figure out what align elements align in the text of the hypothesis.",
                    "label": 1
                },
                {
                    "sent": "And we have two examples of good along.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "JP Losman and the Bills and we have two examples of marginal or passive alignment in the hand and give benefit from word net similarity and arrangement starting job well, since it's idiomatic, it's it's hard to find something you have the proof positive.",
                    "label": 0
                },
                {
                    "sent": "Other short of a Dictionary of idioms that would say that these two things should be aligned.",
                    "label": 0
                },
                {
                    "sent": "Dependency features really tried to determine whether to the same dependencies were assigned a corresponding entities in the text in the hypothesis.",
                    "label": 1
                },
                {
                    "sent": "And keep in mind here, we don't necessarily have a notion of graphs like we heard about in the Stanford system.",
                    "label": 0
                },
                {
                    "sent": "So we were just dealing with pairs of entities that have been aligned.",
                    "label": 0
                },
                {
                    "sent": "So we have our zero.",
                    "label": 0
                },
                {
                    "sent": "OK, that's great for the bills are one is great for this thing that we thought was marginally aligned.",
                    "label": 0
                },
                {
                    "sent": "JP Losman poses a little bit of a problem because in the text he's not signed an argument role.",
                    "label": 0
                },
                {
                    "sent": "And in the hypothesis is then paraphrase features move on to try to tell us whether there were any paraphrases that could be found that could be considered to be paraphrases.",
                    "label": 1
                },
                {
                    "sent": "Portions of the text of the hypothesis, and we saw two of these have gone with quarterback or has or may have turned the keys of the offense over 2.",
                    "label": 1
                },
                {
                    "sent": "I estimated features like I just described a second ago, were just to provide a last kind of check to see whether predicates were assigned the same truth values and so all these things kind of point towards the fact that this yes, in fact is a likely alignment.",
                    "label": 0
                },
                {
                    "sent": "Her as we all.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, not all examples included so many complementary features.",
                    "label": 0
                },
                {
                    "sent": "I selected this example on purpose, so I have this other example of 734 which is an example of no.",
                    "label": 0
                },
                {
                    "sent": "And here we've got the government's economic development.",
                    "label": 0
                },
                {
                    "sent": "First priority did not initially recognize the need for preventative measures to halt pollution, which may have slowed economic growth.",
                    "label": 0
                },
                {
                    "sent": "In this corresponds to a hypothesis that the government took measures to reduce pollution.",
                    "label": 0
                },
                {
                    "sent": "Well, no, the government didn't recognize need.",
                    "label": 0
                },
                {
                    "sent": "It didn't take any steps for disclosure, so let's see what happens.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The King or the alignments with the government's priority lines with government did not recognize lines with took the need for preventive measures.",
                    "label": 1
                },
                {
                    "sent": "This whole chunk here lines with just the spare impede their plural measures.",
                    "label": 0
                },
                {
                    "sent": "Hall aligns with reducing pollution.",
                    "label": 0
                },
                {
                    "sent": "Realize with pollution.",
                    "label": 1
                },
                {
                    "sent": "OK, so the government spa priority lining with the government.",
                    "label": 1
                },
                {
                    "sent": "This is bad for a number of reasons, right?",
                    "label": 0
                },
                {
                    "sent": "It's partial alignment, even though it does have an article match, it hasn't any category mismatch and it's only a passable.",
                    "label": 0
                },
                {
                    "sent": "Not recognize and took is even worse.",
                    "label": 1
                },
                {
                    "sent": "It lines in part of speech, but it mismatches in clarity and we couldn't find any way of saying that these things even close to amend the same thing.",
                    "label": 0
                },
                {
                    "sent": "We got a little bit better alignment for the partial alignment between the need for preventive measures.",
                    "label": 0
                },
                {
                    "sent": "OK, that's fine.",
                    "label": 0
                },
                {
                    "sent": "Paulton reduce.",
                    "label": 0
                },
                {
                    "sent": "There's a nice.",
                    "label": 0
                },
                {
                    "sent": "There's a nice alignment there, and then pollution pollution.",
                    "label": 0
                },
                {
                    "sent": "So evil Emma matches.",
                    "label": 0
                },
                {
                    "sent": "This is nice example one.",
                    "label": 1
                },
                {
                    "sent": "But in contrast here few paraphrases could be clustered, found that included passes extracted from the text right about the Sewart target entities.",
                    "label": 0
                },
                {
                    "sent": "Here was the government and the government's priority and pollution.",
                    "label": 0
                },
                {
                    "sent": "And so you know the ones in yellow or the original text that were extracted from the text of the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And then I have three things that weren't the top things that came back were not considered to be paraphrases.",
                    "label": 0
                },
                {
                    "sent": "So in fact, these three things were the only cluster that came back and survived, and they didn't need that extra hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's why they're labeled in red.",
                    "label": 0
                },
                {
                    "sent": "So has allowed companies to get away with.",
                    "label": 1
                },
                {
                    "sent": "It is looking for ways to deal with wants to forget about.",
                    "label": 1
                },
                {
                    "sent": "These were selected from that cluster.",
                    "label": 0
                },
                {
                    "sent": "All these things add up to an unlikely entailment, and that's where our system reduced produced.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Groundhog this year recognized entailment.",
                    "label": 1
                },
                {
                    "sent": "About 75% of examples in the RTT set and so notable numbers here were like everyone else we did best in the summarization set and we had the highest average precision in the IE test.",
                    "label": 1
                },
                {
                    "sent": "Overall, we were pleased with our both our average precision numbers, but since it's a new metric, we're not really necessarily sure we have to perform more analysis to be able to see really what this means as opposed to last year's metric, or really what having a high average precision means for us, our performance did.",
                    "label": 0
                },
                {
                    "sent": "Many other other peoples experience did differ markedly across the four sub tasks, and this was significant and we believe this what you know it does have.",
                    "label": 0
                },
                {
                    "sent": "There's a component that has to do with the makeup of each of these tasks, but it also has something to do with our training data.",
                    "label": 1
                },
                {
                    "sent": "So our headline corpus features a large number of some sentence compression, like examples.",
                    "label": 0
                },
                {
                    "sent": "If you can think back to the example I showed before, but the Sydney Olympics in that case there's lots of lexical overlap.",
                    "label": 0
                },
                {
                    "sent": "There's alot of fewer words in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And so when we remove, we took a lot of those examples.",
                    "label": 0
                },
                {
                    "sent": "Then we train Groundhog on a balance training corpus that how late you know had much fewer examples from basically only 800 examples from the training corpus.",
                    "label": 0
                },
                {
                    "sent": "From that headline corpus, the performance in summarization task fell to 79.3%, so we'd need to run more tests to be able to kind of get you defended numbers on this story and to kind of perform some sort of an ounce of analysis of variance to be able to figure out exactly what the culprit is here.",
                    "label": 0
                },
                {
                    "sent": "But one culprit could be the headline corpus that we used.",
                    "label": 0
                },
                {
                    "sent": "Training data did play an important role in boosting are over.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So performance in February Boot was increased from 65% to about 75% when we use about 200,000 examples from our training corpus.",
                    "label": 1
                },
                {
                    "sent": "In the intervening months, we've since we only had about six weeks to put the system together in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "Re fact we've been going through the last couple of six weeks and we've had one or two people refactoring features and experimenting with different combinations of features.",
                    "label": 0
                },
                {
                    "sent": "One of the disadvantages of working with 200,000 examples is that it takes a heck of a long time to train and that you have to go back.",
                    "label": 0
                },
                {
                    "sent": "So we spend a lot of our future development with smaller corpora and then when we only use the larger corporate for the final submission.",
                    "label": 0
                },
                {
                    "sent": "So we've been able to increase our overall performance from the 2006 Death Step by about 5%.",
                    "label": 1
                },
                {
                    "sent": "And you can see here that the performance increase seems to be tapering off now.",
                    "label": 1
                },
                {
                    "sent": "We don't get the same kind of big gains me before you saw a 6% jump up between going from 50,000 examples.",
                    "label": 0
                },
                {
                    "sent": "100,000 examples.",
                    "label": 1
                },
                {
                    "sent": "Now we don't see that.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've kind of we've kind of been able to roll a lot of that into the initial score.",
                    "label": 0
                },
                {
                    "sent": "And we're only able to go up using the 200,000 examples.",
                    "label": 0
                },
                {
                    "sent": "Only gets up about 6%, so we're not sure necessarily if this represents a stealing.",
                    "label": 0
                },
                {
                    "sent": "It will keep it posted as we learn more about this.",
                    "label": 0
                },
                {
                    "sent": "Our best results in.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And ablation study across those four sets of features using the Taylor classifier.",
                    "label": 0
                },
                {
                    "sent": "Our best results were obtained by combining all four sets of features.",
                    "label": 0
                },
                {
                    "sent": "As you can see, here's our 75.38 number, but our largest gains for whatever reason were observed by adding paraphrase features so.",
                    "label": 0
                },
                {
                    "sent": "By adding paraphrase features, we want to 4% here, another 4% there, and so you know this.",
                    "label": 0
                },
                {
                    "sent": "But we have a lot of mileage still to be gained out of the paraphrase features themselves.",
                    "label": 0
                },
                {
                    "sent": "A lot of what we did was just reimplementing bars.",
                    "label": 0
                },
                {
                    "sent": "Lion, Lee and.",
                    "label": 0
                },
                {
                    "sent": "With some lexical semantic annotations that we have at LCC, you know kind of trying to enhance some of that enhance the clustering algorithm that we use, but pretty much it was a straight up implementation of our slightly so we have some ideas of how we can move forward with you know, better representing the passage is better represented, better detecting fuzzy matches between the passes that are extracted in the passages that we get from the web and trying to kind of boost performance in the system.",
                    "label": 0
                },
                {
                    "sent": "And also we've used rather simplistic features in our paraphrase component within the classifier now, and so we feel there's gains that we have in that.",
                    "label": 0
                },
                {
                    "sent": "So to wrap up, we've introduced A3 tiered approach to recognizing textual entailment, and this kind of hinges on kind of a nob we've all talked.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alignment in number, different ways, and we feel that our approach alignment is is somewhat novel in terms that we're only comparing.",
                    "label": 0
                },
                {
                    "sent": "We're basically looking every possible combination of constituents from attacks and hypothesis and comparing them based on a wide range of lexical semantic features.",
                    "label": 0
                },
                {
                    "sent": "Then we have this kind of very nascent paraphrase acquisition module that arise phrase level alternations for passages contained in high confidence for aligned entities, and then we have a decision tree based entailment classified are fire that works to combine all of these features in order to make an intelligent decision.",
                    "label": 1
                },
                {
                    "sent": "So, and the other takeaway point here that I'd like you to walk away with is that you know we've showed that it's possible.",
                    "label": 0
                },
                {
                    "sent": "You know, somehow you know by in our own shop by relaxing the notion of strict element in order to train training corporate that can prove effective in training systems for the R key task, you don't necessarily.",
                    "label": 1
                },
                {
                    "sent": "I mean, I would not recommend this in terms of creating new development sets from next years are key challenge, but you don't necessarily have to train on perfect examples of entailment to do well at recognizing entailment.",
                    "label": 0
                },
                {
                    "sent": "Recognizing so we were able to acquire about 200,000 examples using some relatively simple heuristics and some elbow grease in terms of filtering.",
                    "label": 0
                },
                {
                    "sent": "Thanks so much.",
                    "label": 0
                },
                {
                    "sent": "One place in space related today.",
                    "label": 0
                },
                {
                    "sent": "Increasing employee performance that you had from February 2016.",
                    "label": 0
                },
                {
                    "sent": "Terabytes per second value.",
                    "label": 0
                },
                {
                    "sent": "We shooting increasement of the pharmacies using 100 samples and if I can also assume questions at least.",
                    "label": 0
                },
                {
                    "sent": "I missed again in the house.",
                    "label": 0
                },
                {
                    "sent": "Negative effects samples from doing so.",
                    "label": 0
                },
                {
                    "sent": "More than happy to dress code for this.",
                    "label": 0
                },
                {
                    "sent": "See it, I wish I could tell you something you know some greater insight, but a lot of the gains performance gains that we've had in the last two months or so has stemmed from just different feature combination.",
                    "label": 0
                },
                {
                    "sent": "So we thought we were at a maximum in terms of combining different features that we were playing with.",
                    "label": 0
                },
                {
                    "sent": "It basically comes down to the fact that we developed this system in six weeks, and so in the last eight weeks we've been able to identify the other feature combinations which perform a little bit better.",
                    "label": 0
                },
                {
                    "sent": "Enter, take out some of the bugs in the features that we were using, so there were simple.",
                    "label": 1
                },
                {
                    "sent": "Lack of better term, just coding mistakes that were brought on by the pressures of having to do this in six weeks.",
                    "label": 0
                },
                {
                    "sent": "In terms of the negative examples.",
                    "label": 0
                },
                {
                    "sent": "So we use two different approaches, one which was we tried to find sequential senses that feature the repeat mention editing and this was something that was facilitated rest by the fact that we have good named entity recognition and rudimentary coreference detection.",
                    "label": 0
                },
                {
                    "sent": "So here in this example we were able to take Hideki Irabu and Irabu.",
                    "label": 0
                },
                {
                    "sent": "Here in this case and so we knew that both of these sentences featured some 2 facts about for lack of a better word about Hideki Irabu.",
                    "label": 0
                },
                {
                    "sent": "But we realized that most discourses that you probably wouldn't you would violate informativeness to say the same thing twice in a row, specially in a newswire text.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't tell you that here.",
                    "label": 0
                },
                {
                    "sent": "It's Hideki Irabu 64, and then say the hit the six foot four Hideki Irabu.",
                    "label": 0
                },
                {
                    "sent": "Highlights greissing informative, informativeness and so you know it's it's a relatively simple assumption, but one that got us a lot of examples that a lot of that lexical overlap.",
                    "label": 0
                },
                {
                    "sent": "So I have Irabu and wells here, so this looks like you know prototypical example that we would want to classify.",
                    "label": 0
                },
                {
                    "sent": "We would find are Abu and Wells as our target entities, and then we try to see whether the passage or the passage within the context you know said he would take out well or take wells out, or who was befriended by and to see if those two things were paraphrase or to those two things were an example of entailment.",
                    "label": 0
                },
                {
                    "sent": "So this is the classic example, I mean and truth be told, when we first started training on this, examples in these were the kinds of things that we would.",
                    "label": 0
                },
                {
                    "sent": "Falsely predict yes on another thing we did was use discourse connectives so the idea is that discourse connectives are going things like either though or although we're going to tell Me 2 complementary facts about a certain entity, so I'm going to tell you X and then, but I would say, but you know, some conventional implicature that you may have, or even conversational, because we may have doesn't isn't true, so I'm going to tell you something, and I'm going to cancel some part.",
                    "label": 0
                },
                {
                    "sent": "You know what the normal interpretation of that is.",
                    "label": 0
                },
                {
                    "sent": "So here you know, cleaning up oil slicks is extremely costly.",
                    "label": 0
                },
                {
                    "sent": "And then I have.",
                    "label": 0
                },
                {
                    "sent": "But you know this is not the best example here, but since I don't have oil slicks mentioned again and that's what crusade would need to know about you.",
                    "label": 0
                },
                {
                    "sent": "But I guess the take home message here is that we were able to train on these examples which you know we were skeptical about in the 1st place and we expected to see beautiful examples of entailment pulled directly from the newswire documents.",
                    "label": 0
                },
                {
                    "sent": "But we were able to.",
                    "label": 0
                },
                {
                    "sent": "They were able to ultimately benefit the system in the end by learning it by performing.",
                    "label": 0
                },
                {
                    "sent": "You know there's a lot to be learned here in terms of just lexical correspondence in terms of alignment in terms of trying to figure out the part of the configuration of certain kinds of features.",
                    "label": 0
                },
                {
                    "sent": "No, we had.",
                    "label": 0
                },
                {
                    "sent": "We had great well I mean hand annotation is is a is a dirty word but we used we had grammars to deal with negation.",
                    "label": 0
                },
                {
                    "sent": "Oh, I mean, I'm sorry, negative examples in negation we didn't hand entertain any of this.",
                    "label": 0
                },
                {
                    "sent": "We felt mean.",
                    "label": 0
                },
                {
                    "sent": "The closest we came to was we spot checked it and filtered it based on topic and keyboards and things like that.",
                    "label": 0
                },
                {
                    "sent": "I apologize something.",
                    "label": 0
                }
            ]
        }
    }
}