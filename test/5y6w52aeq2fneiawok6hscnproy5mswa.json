{
    "id": "5y6w52aeq2fneiawok6hscnproy5mswa",
    "title": "Introduction to Kernel Methods",
    "info": {
        "author": [
            "Liva Ralaivola, Aix-Marseille Universit\u00e9"
        ],
        "published": "Aug. 5, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/bootcamp2010_ralaivola_ikm/",
    "segmentation": [
        [
            "OK, so we should get started.",
            "Normally there is someone who should introduce me, but nobody loves me, so.",
            "I'm leyva.",
            "And I'm going to talk to you about kernel methods, and I think I'm going to start with the very basics of kernels an little by little.",
            "I'll go to very, very theoretical things, but normally I shouldn't have time to to get there.",
            "But still you have everything on the slide soon.",
            "OK."
        ],
        [
            "So here is the outline of the talk.",
            "At first I will build a very, very, very simple kernel classifier.",
            "Anne.",
            "That you can you can.",
            "You can if you are a little bit familiar with octave or or Matlab, you can program it in 5 minutes.",
            "I did it in 5 minutes yesterday.",
            "I'm not a very good programmer, so so it's it's it's very simple to do it.",
            "Then I I will talk to you bout supervector machines.",
            "Because this this is the.",
            "The main reasons the main reason why current methods are so popular because of this this very specific algorithm.",
            "And after that I will take will take a short break an after the break will I'll talk to you about some example of kernels for structures because this is one of the very interesting points of kernels is that you can use kernel methods to work with the structured data like graphs, trees, sequences which you cannot do easily with other kinds of methods.",
            "An then this is the last part, the lost treasure of this talk is the theoretical part, and I will talk to you about the Representer theorem, which is very crucial an.",
            "I will talk to you very, very, very little about generalization, bounds and complexity of class of class of family of classifiers.",
            "I'll show you how you can derive generalization bound.",
            "I'll show you that it's very easy to do that.",
            "And that's gonna be it.",
            "I'm for this tutorial.",
            "I took many sources of inspiration and primarily I I worked with the with the.",
            "A tutorial by Bear National Cup.",
            "I know if you had the chance to to see the tutorial, but it's very.",
            "It's really nice.",
            "It's it's very easy to understand and I'm trying to to do as as well as it does, but I don't know if he if I'll."
        ],
        [
            "Exit so.",
            "To warm up we will going to construct our first kernel classifier."
        ],
        [
            "And um, first of all.",
            "For all all these talk, I'll focus focus on on binary classification.",
            "Essentially, I will not talk about regression density, estimation, ranking, or other tasks.",
            "I will just talk about binary classification.",
            "That is, I have two classes minus 1 + 1.",
            "I want to predict the class of 1 one data saying that it's positive or negative.",
            "So I think that you already have a good grasp.",
            "How how classification can be used for real world tasks?",
            "So some tasks that can that you can imagine is for instance prediction.",
            "Predicting how serious is a disease for a patient given some measurements of his blood pressure, heart rate, fat rate and things like that.",
            "Something that he talked to you about land Monday was detecting detecting faces in pictures.",
            "And a very popular application of of machine learning is spam filtering.",
            "So the question is just to be to to have.",
            "In a filter that can identify spam or regular Mail and the idea is that you have to train these machines so that you can.",
            "Accurately identify regular Mail or or spam.",
            "So the reason why classification or automating this this kind of tasks is very important are numerous, some some.",
            "Some of the of the reasons are the following.",
            "So there is there is a question of very simple question of dealing with a very huge amount of data.",
            "So computers are very handy to do that.",
            "You we humans cannot work with a 10,000.",
            "Of the data, whereas it's very, it's very easy for a for computer.",
            "It's so speed cost, it's very it's.",
            "It's very.",
            "For instance, if you talk about problems in by Infomatics, it's very.",
            "It's very expensive to do real experiments on human or animals, and sometimes you want to.",
            "To identify, for instance, molecules that can be active or inactive towards some disease, and before doing that and and trying these these molecules on real animals and real living living beings, it's it's more easy, and it's much less expensive to try it on on a computer and try to estimate if the molecules that you consider that you consider our our.",
            "Active or inactive?"
        ],
        [
            "So.",
            "The very important notions when you you you try to build a classifier is that you have a limited number of training data from which you try to infer model that you're going to use to do predictions an what you you aim at is to have a model that has a very low probability of error, that is, it has a very low risk.",
            "And the the idea of having a low risk is is is the idea of generalization.",
            "So to sum up you you just have a limited number of data, you try to infer model and you want it to be very very accurate on new data.",
            "That's a share.",
            "The same statistical irregularities as the data that she had in training.",
            "Here is some notation that I'll be using too.",
            "Throughout this talk, 1st I'll call X the input space, the space where the data I'll be working on our leave.",
            "Why the space space of classes though?",
            "It's just minus 1 + 1 and F is the model that I'm going to learn, and sometimes I call it H, so F or H. Other models, it depends from one slide to the other.",
            "It changes.",
            "This is too.",
            "To keep you waking up so an here is a what we're gonna do is simply that there is.",
            "We have two sets of points and we're going to try and find a linear classification classifier.",
            "There is a linear separation between the blue points and the red points.",
            "We're not going to.",
            "We're not going."
        ],
        [
            "Do something.",
            "More difficult.",
            "So, um.",
            "Even though you are all familiar with this and I'm sure that.",
            "Many of you are going to say that it's useless.",
            "I'm going to spend a little bit of time reminding you some very basics basics about linear algebra, an sum of vectors.",
            "You already had something with something, but I think that's it's.",
            "It's going to be even easier than what she she taught you yesterday, yesterday, Tuesday.",
            "Tuesday is OK.",
            "The day before yesterday.",
            "So.",
            "Here you have examples of vectors you have UVWC.",
            "And what can I say about that so?",
            "So those are vectors.",
            "The space is R2 and for instance if you do the difference between U&V you have this vector which is W. And if you remember what she did when you were in high school, it doesn't matter whether these WW is here or here or here.",
            "This is exactly the same vector you don't care about the precise location where it is.",
            "If you do, the sum of U&V you have these long vector here, which is just obtained by adding you just moved along this vector and then you add this vector to the end of this vector.",
            "So you do this.",
            "Path and then you end up with these these long vector.",
            "And if you divide it by two here, you have the middle of these two points.",
            "OK. And the the last thing I don't even know if it's necessary to talk to you about that.",
            "But let's talk to you about that.",
            "If you have a scalar here Lambda, which is between zero and one.",
            "Then the resulting vector of the multiplication of Lambda by V is this vector, so it's a vector which is between, which is, which leaves be on this on this line.",
            "And depending of the value of Lambda, it's closer to here, oh here."
        ],
        [
            "Then in a product, so in our products, if you if you if you if you missed this slide.",
            "If you don't understand this slide.",
            "It's done for you.",
            "Because.",
            "What I'm going to talk to you about for the remaining of this talk is just how you deal with.",
            "Inner product or dot products?",
            "Maybe a little bit more elaborate, elaborate dot products, but I'm just going to talk about Dot products so.",
            "So again, this is something that you you learn in high school.",
            "So this is something that is symmetric, which means that you you take the arguments.",
            "You can switch them, you'd still have the same value, it's be linear.",
            "OK, positive.",
            "Meaning that if you take one vector and take the the inner product by in by himself, you will have value that is non negative and definite, meaning that if the scalar product of vector by himself is equal to 0 then it means that you is zero itself.",
            "And So what does an inner product or a dot product else tell tell us about the space?",
            "It provides a structure on the space where you live in.",
            "Actually it's it provides a structure because if you do a little bit of topology that the main tool is distance.",
            "An scalar product induces a distance, so there is a here.",
            "You can see the norm and from this normally can you can derive a distance.",
            "So this is already related.",
            "So if you don't like the mathematical parts of of things, just think of.",
            "Inner product as a similarity.",
            "You have a similarity function and you're happy.",
            "So this is one of the first message that you have to take home today, so it's it's at the beginning of the talk, but still, it's very important.",
            "Message message if you have a similarity function between the object, you're the objects that you are going to work on then you happy you can do a lot of things.",
            "There is a slight thing that I don't say when I say that because there are some.",
            "Harry date details on the similarities.",
            "But OK, just just say that if you have a similarity, you're almost happy.",
            "And the last thing that is very important is that if you want to compute the norm, that is the length of a vector, you just take the inner product of you and U and you take the square root and you give you the distance.",
            "The norm of the vector.",
            "So let's stay at the high school level.",
            "In R2 you have one vector with two coordinates and another one, and the product the natural inner product is just the sum of the product between of the corresponding coordinates."
        ],
        [
            "So.",
            "Now we're back with a little sketch here.",
            "So what?",
            "What is important?",
            "What I want you to show is how the inner product is correlated to the fact that two vectors point to the same direction or not.",
            "So here if you look at this vector.",
            "You minus V this this vector or this vector and you look at E. Then you see that the.",
            "Roughly point to the same direction.",
            "To the upper left corner corner of this wall.",
            "And when this is the case, then you have the inner product of those.",
            "Those two guys which is positive.",
            "On the other hand hand if you take.",
            "Yeah, if you look at you minus V&G.",
            "There pointing towards opposing opposite directions an in this situation the dot product is is negative.",
            "Otherwise, if the two vectors are perpendicular or orthogonal, then the dot product is equal to 0."
        ],
        [
            "So that's it's Jeff questions about inner product.",
            "OK, so this is the first very very very simple classifier.",
            "So imagine that you have two sets of points, two sets of labeled points from each, from from, from which you're going to to learn a classifier.",
            "So you have the blue points here.",
            "The red one here.",
            "Those are your training points.",
            "Those are the points that you are going to use to construct your model.",
            "And don't look at the details for now.",
            "Just the very simple thing.",
            "I say that I'm going to take the middle of the the red points here.",
            "The middle of those.",
            "The blue points here.",
            "I'm going to draw a line between those two middle points, and I'll say that I'm going to say that if a point is closer to C minus than C plus, then I will say that this point should be classified as either minus.",
            "As it has a negative point.",
            "Otherwise, if a point is closer to C plus than C minus, then I'll say that it should be classified as a plus spot, so this is very, very simple.",
            "There is nothing to do it and you'll see that the the.",
            "Um mathematics is also a very simple so.",
            "Here you have the.",
            "Center of mass.",
            "Yeah, that should be something like that.",
            "The center of Mass of the plus point, the positive points, and the center of mass mass of the negative points.",
            "So you have C plus that is just the mean of all the exercise.",
            "Here this C plus you have C minus which is the mean or the average points of all the minus points.",
            "You have a the middle of C minus an C plus, which is just.",
            "1/2 of C + + C minus.",
            "OK, this is something that I told you about just before.",
            "If you want to to compute the middle of two points, you just add the two vectors and you divide it by two and you have the middle of the points.",
            "And the vector that I'm going to use to do classification is called W An.",
            "I think I'm consistent with that.",
            "I'll always call the classification vector W. Throughout the slides.",
            "An answer, the decision function is very simple.",
            "I said that I would classify points, test points X according to which of the two class means C plus or similar.",
            "C minus is closer.",
            "So it's very easy to do that.",
            "You just take the inner product between.",
            "W here.",
            "And the vector X -- C. Again, if those two vectors point to the same direction, then you're going to say that.",
            "That the inner product is positive and you're going to say that the point X is positive.",
            "Otherwise it's just the other way round.",
            "If the inner product is negative, then it means that X -- C and W .2 opposite directions and you're going to classify X as a minus point negative point.",
            "And the classifier is just this very simple function here.",
            "So it's based on this on this real value.",
            "The function which is edge of X which is equal to W inner product with X -- C. And in order to predict the class, just take the sign of these these these inner product.",
            "If the sign is positive.",
            "If the if the inner product is positive, then you say that X should be classified as a positive example.",
            "If the inner product is negative, then you say that the test point should be classified as as a negative negative point.",
            "And the decision surface that is induced by this decision rule is just.",
            "These hyperplanes.",
            "So remember that what what someone told you.",
            "Tuesday.",
            "She told you that hyperplane in a dimension D dimensional space is just a D -- 1 dimensional dimensional space.",
            "So here these are two dimensional space.",
            "Hyperplane is just a line.",
            "In three dimensional space the hyperplane is.",
            "A plane.",
            "Sorry."
        ],
        [
            "Well I am.",
            "Now we are we are going to go into the details of computing H of X again.",
            "This is very easy.",
            "Don't be freaked out just here.",
            "If you compute edge of XI, say that this is a the inner product between W an X -- C. What I do here is that I replace all the quantity WC by all those, the quantity that quantities that you can find here.",
            "And I use the linearity of the inner product.",
            "I do the computation and I get this.",
            "OK, let's let's let's go through through the computation so.",
            "If you take the inner product between W and X -- C. Actually.",
            "So if you take this, you just use the linearity of the inner product.",
            "It gives you this.",
            "Then W is just this guy, so you replace W by C + -- T minus and use the linearity of the inner product again and you get this an As for this part.",
            "You just take again the definition of W. The definition of C and you have this guy.",
            "I think that there is a missing 1/2 something somewhere here.",
            "And then I I steal again.",
            "I replace C plus by its definition on the top on the top of this slide, C minus by its definition, and I get this, I use again the linearity of the inner product and I get that.",
            "HFX is computed as.",
            "This.",
            "I am the second very important message here.",
            "Is that the decision function is just computed as something like that.",
            "So yeah, first some of.",
            "You know product between the training patterns and the test point.",
            "And then a constant.",
            "And that is all there is to it.",
            "If you have a linear algorithm, usually you will end up with something like this.",
            "It is very important to see that the only thing that you need to know is to compute.",
            "You know how to compute the inner product between two pairs of points.",
            "And once you know how to do that, you're good.",
            "The the second thing that is very important to see that be here.",
            "These are real.",
            "This is color and it's very easy to compute.",
            "Is just set C plus this.",
            "This summer for interprofessional products.",
            "And it just depends.",
            "On the training data.",
            "So if you can compute the inner product between the training data which you you know how to do.",
            "Then you good because computer B is computing B is very easy an the rest of it is just easy so.",
            "I intentionally used these, this writing here with a sum of Alpha I and dot products plus B.",
            "Because this this is what is going to be a very crucial point of the rest of the talk.",
            "And here you have very simple very.",
            "Um?",
            "Handy thing, which is that the Alpha eyes can be computed analytically.",
            "This is the reason why I say that this is a very simple classifier here.",
            "If I is a positive point, then Alpha I is.",
            "1 / N plus.",
            "If I is a negative point, that is, its label is minus one, then Alpha.",
            "I is equal to minus 1 / N minus.",
            "So this only this.",
            "Here this here, sorry is a compact way to write this here with Alpha I defined as as this.",
            "OK. Anne.",
            "So B is computed like that.",
            "An so if you replace C by its definition, you do a little bit of of calculations.",
            "Then you end up with this.",
            "OK."
        ],
        [
            "So to summarize, the very important thing is that.",
            "Once you are able to compute Dot products, you're happy.",
            "There's nothing more than than that because you know precisely how to compute Alpha I, and you know precisely how to compute B.",
            "An in order to.",
            "To anticipate on what I'm going to say for SVM's, for instance, for instance.",
            "Villa.",
            "Learning phases just intended to too.",
            "Assign values to Alpha I and be here.",
            "It's very simple here.",
            "You can do it analytically.",
            "You can program it, you can do it in just just five 5 minutes.",
            "You can do everything I told you about.",
            "But for SVM's, this is what we're going to do in the practical and tomorrow's practical session.",
            "We're going to program or.",
            "And SVM.",
            "So if you already done that then don't come, but otherwise you can come.",
            "Of the question that is very important.",
            "That and that we are going to.",
            "To address throughout the this, this talk is.",
            "What if the data sets at hand is not linearly separable, which is the case here.",
            "You can try to take this line, put it here or here or here or whatever you want wherever you want.",
            "You won't be able to separate the.",
            "The train set training sets so."
        ],
        [
            "Kermath methods just provide a principled way to address this kind of problem.",
            "With this kind of model.",
            "And this one is very powerful on a kernel method.",
            "So the idea is the kernel trick.",
            "I'm sure a lot of.",
            "You guys know about the kernel trick, but."
        ],
        [
            "This is the crooks.",
            "So the kernel trick.",
            "Here it is in a very SIM.",
            "Wait, so you have a nonlinearly separable data set S?",
            "On one side, on the other side, you know that you have a very simple algorithm.",
            "Let's say not simple, but just a linear algorithm.",
            "You have something that is capable to.",
            "Computer hyperplane.",
            "So your focus is hyperplane.",
            "You know how to compute hyperplane.",
            "Now now what you want to do is to use your your favorite algorithm to compute hyperplanes in a situation where hyperplanes are not sufficient to do classification.",
            "So the idea behind the kernel trick and the idea is a little bit reversed with the way I'm going to present it is a little bit reversed with what is actually meant by kernel trick, but it helps understand how how it works.",
            "So the idea is if you have a nonlinear nonlinear, a nonlinear problem is that you may want to try to map all your data.",
            "Into another space.",
            "Usually high dimensional space where the linear separation can be can be sought for.",
            "And.",
            "An so these these mapping from the the input space towards these special high dimensional space which we're going to call the feature space is usually denoted as file.",
            "So this this guy here.",
            "And what you want is that this this space H. You want it to be equipped with the dot product, because if it's equipped with the dot product then you can work in this space.",
            "Try to find a linear separator in this space and then only work with the images of the training data by fine and not the the training data by themselves.",
            "An I think that this picture summarize the idea is that you have these non linear separable problem.",
            "Use mapping file.",
            "That goes from X to some feature space H and here you have all the images of the right point in this space and the images of the blue points in this space and in this place you can do linear separation.",
            "And again, the crooks is that you assume that there is an inner product in this space, because if there is an inner product, we just saw that it's possible to find a classifier.",
            "It's it's just easy if you have a dot product in this space you work in to derive and to compute decision surface.",
            "So this is, this is the idea of the."
        ],
        [
            "Of the.",
            "A kernel trick, but it's not precisely the idea I'll come to it in a bit.",
            "So if I'm going back to the previous algorithm, the very simple linear algorithm, that means that.",
            "Instead of having the DOT product between the excise and the X, then you will have the DOT products between the fire effects I.",
            "And the fireworks.",
            "So if you have your mapping fi and you can work with your mapping, then it's just very easy to compute a nonlinear classifier by using exactly the same algorithm as before, provided that you have.",
            "And you know, product in the space age."
        ],
        [
            "And here is actually the kernel trick.",
            "Um?",
            "The it's OK, this is just the step before the actual kernel trick.",
            "So the current trick can be applied.",
            "So these are trick.",
            "OK, I'm going to take care.",
            "I'm going to the.",
            "Future.",
            "So the kernel trick is just a very simple thing which say that if you have a linear algorithm that works with that product, then you do something very silly but very powerful.",
            "Every time you see a dot product will you replace it by kernel function K?",
            "Because if you have a kernel function K, then you know that it's using kernel function.",
            "K is precisely like as if you were mapping all your data from an input space to a well design feature space.",
            "So why is that?",
            "Why is that?",
            "Because what you're going to use is Mercer or Mercer, or positive definite kernels?",
            "Positive definite kernels are our function taking two arguments, an out putting a value in R in the real set.",
            "And if it's it's positive definite if computing computing K of UV is equivalent to 1st mapping U&V to some hidden space and then to compute the dot product between those those Maps instances.",
            "And the key points actually on the way I'm going to.",
            "I'm trying to to bring you to the kernel kernel, tricky that the emphasis is usually when you want to use the kernel trick and the idea of mapping.",
            "Actually the the emphasis in the.",
            "The very important thing is not to find the file transformation.",
            "The file transform define mapping, but actually it's more to find the kernel K. Because again, if you have a kernel K, then it means a kernel positive kernel positive definite kernel K. It means that somewhere implicitly you have.",
            "Am mapping five that comes together.",
            "OK, so I'll talk about having a bit more about that after that.",
            "So so.",
            "Kernels must verify mercers property.",
            "I'll come back to this later on so, but the what is important here.",
            "The technical details on the Mercers property are not very important, but it's just important to know that if you have Mercer kernel then it means that there exists a space H and a map and mapping files such that when you evaluate the value of K of UV then it's exactly as if you were doing the mapping.",
            "Of you to fire few and V to 5V and you were checking the taking the inner product between fire few and 5V.",
            "Meaning that again.",
            "All you have to do is to find a good kernel, a kernel that is a Mercer kernel.",
            "You don't have to focus on on on on finding a good mapping file, but you have to find a good kernel K. OK, so this is slight remark.",
            "Yeah, you you have people that worked on non valid non Mercer kernels to to do classification.",
            "It's possible actually it's it's totally possible, but they did it with the.",
            "Algorithms that are designed to work with the Mercer Kernels.",
            "So which is a bit weird, but it's OK, it works.",
            "And again, for those of you who don't care about Mercer or a positive definite kernel, just think of K as a similarity measure.",
            "So."
        ],
        [
            "Here is a kernel trick recipe.",
            "First, you can see the the sketch and the drawing on the upper part of the slide.",
            "But the kernel trick recipe is very simple.",
            "First you choose your linear classification.",
            "Our algorithm.",
            "You have everyone that is expressed in terms of DOT products, just as the first algorithm that I presented you at the beginning of the talk.",
            "An you do a very very, very simple thing and this is a kernel trick.",
            "You replace Everything Everywhere, every every.",
            "Every instance is every instance of.",
            "The usual inner product you replace it by K. You don't even think about what you're doing.",
            "You say, OK, I have an algorithm.",
            "I see that there there is a dot product I don't care about the dot product, I replace it by KK of if I see inner product between U&VI replace it by K or few and V and that's it.",
            "And that's your kernel trick.",
            "And the magic behind that is that if you have chosen.",
            "A kernel that is a positive definite kernels, but the positive definite kernel then.",
            "You are in insured that there exists.",
            "A mapping Phi that is a society with the with the K. Search that computing K of XIX is just like computing the inner product between 5XI and 5X.",
            "So This is why it's it's very simple.",
            "The kernel trick is a very simple thing.",
            "It's it's obvious, obviously restaurants very strong mathematical tools, but it's if you just want to use it if you want to construct your own kernel method, which is a little late because there are a lot of things that have been done in this area, but still you can try to to find a new kernel kernel.",
            "I don't know kernelized whatever you want.",
            "Then the idea is is that you just have to find that product first.",
            "You have to express everything in terms of dot products.",
            "Once you're done with that, then you know that you can do.",
            "You can kernelized everything.",
            "And the obtained classifier now is just expressed as a sum of Alpha YK of XI X + B."
        ],
        [
            "So I'm going back to the very simple classifier.",
            "I talked about the very beginning.",
            "I remember that we have the.",
            "Red Point we have the blue point.",
            "I think I take the the the mean of the red points, the mean of the blue points are drawn line that passes through the middle of those two points and I say that all the points that fall on one side are plus point are a positive point and the others are negative points.",
            "And what we had before is that we knew that instead of this we had the classical inner product.",
            "And now I don't change anything except that I did my my my little trick with the kernel trick.",
            "I replaced the inner product that used to appear here by AK of Zyex and the value of Alpha I is unchanged is still the same value as before, and the values of B is a little bit changed because if you remember these expressed us the difference of the square norm of C minus and C plus.",
            "And if you you do the computation, you see that actually the for instance, the square norm of C minus is this.",
            "OK.",
            "So if you want to see how it works.",
            "You have C minus that is equal to.",
            "So this is the definition of C minus.",
            "If you take the norm.",
            "The square norm of this is just.",
            "This an so you use the linearity and everything that you know about the inner product, so it's.",
            "And here you're happy because you have dot products.",
            "And you say OK instead of that.",
            "I use.",
            "That, and that's it.",
            "So this is how I get this and this is how I get that."
        ],
        [
            "So now I didn't.",
            "I didn't specify any any particular kernel an I'm going to to show you.",
            "Classical kernels that are usually there used are that are well known and.",
            "That are used when you work with the vectors.",
            "First, the Gaussian kernel.",
            "Usually when you try to do.",
            "Colonel when you first meet with the kernel methods and you want to try you first kernel algorithm, what you're going to use is the Gaussian kernel becausw because it works, there is a.",
            "It's a parameterized kernel because you have this Sigma here.",
            "Here they need be.",
            "Parameterized properly, but usually.",
            "You can manage to find a good value and you happy with your first kernel kernel method.",
            "The corresponding and hidden implicit H is of infinite dimension.",
            "Again, I'll come back on this a little bit later on.",
            "Actually, at the end of the talk when I'll talk about more theoretical things and more theoretically grounded things.",
            "The polynomial kernel.",
            "It's very simple, it's just the usual dot product between U&V plus constant and to the power of D. So you can see that the usual inner product which correspond to the situation where C is equal to 0 and is equal to 1.",
            "So the usual inner product is just the kernel as well.",
            "And another kernel that has been used with the support vector machines even though it's not, it's not necessarily a Mercer kernel that is.",
            "You cannot say what kind of mapping is associated with.",
            "With exists is goes with.",
            "With this kernel, is that the the touch?",
            "So the it's not the tangent kernel but is the hyper hyperbolic tangent kernels.",
            "So it's in that you can.",
            "I don't know.",
            "I've never worked with it, but I know that."
        ],
        [
            "Would have so, um.",
            "What I want to want want to show you here is is how we can for for some examples of kernel kernels, how you can identify the mapping that goes with the kernel that you choose.",
            "For instance, you take the polynomial kernel K defined here.",
            "So this just the square of the inner product is not very, very it's.",
            "It's not very elaborate, but it's it's a kernel, so you take the square.",
            "And this corresponds to the situation where C is equal to 0 and is equal to 2 two in the previous definition.",
            "And you can, you can identify mapping that works, that is a mapping search that when you take the kernel between U&V then you have that KFU&V is equal to five of to the dot product between 5:00 you and fire V. So the the function Phi the mapping file that as I proposed to to look at is this one.",
            "You take X that is made of two coordinates.",
            "It's made of X1F and X2 and you build 3 dimensional vector that is.",
            "X1 square sqrt 2 X 1 * 6 Two and X2 square.",
            "These are three dimensional vector an you see that if you take the inner product between Fire view and Fire V. This is just the inner product of this 3 dimensional vector with this 3 dimensional vector.",
            "Then you see that it's equal to.",
            "OK, it's maybe a bit quick, but if you do the math and you compute everything, you'll see that this is just.",
            "U1V1 plus U2V2 to the square squared.",
            "Which is exactly the inner product between U&V Square and is the kernel that you you started with.",
            "Meaning that you can.",
            "You can see it like as if I'm trying to say that if you use this kernel, then it's precisely as if you were trying to use.",
            "Um?",
            "In formation of degree 2 hour if you want to use.",
            "Quadratic information about your coordinates, so it's very easy.",
            "For instance, if you use this kernel to derive with to derive using a linear algorithm to derive.",
            "Decisiones surface that are like... high probability hyper.",
            "Hyper Bulls, I don't know if it's.",
            "Yeah, it shouldn't work.",
            "Anne and things like that."
        ],
        [
            "And now we're back.",
            "We've very simple kernelized classifier.",
            "So the one with the average plus points and the average negative points.",
            "Anne.",
            "Again, we have the value of Alpha.",
            "We have the value of B an.",
            "I use.",
            "A Gaussian kernel.",
            "So when you use a Gaussian kernel, you have this parameter Sigma that you have to deal with.",
            "I'm not going to tell you how to deal with that this guy because it's it's a pain actually.",
            "But I'm going to show you what kind of division surface you can have using an RBF kernel.",
            "This very simple classifier that I showed you on a nonlinear class."
        ],
        [
            "Location problem.",
            "So here is the training data.",
            "Here.",
            "Yeah, the blue points that are a positive points the red one that are negative points and as you can see there is no no line, no hyperplane that could could separate blue points from red points.",
            "And I use the very simple classifier that I showed you with the Gaussian with a Gaussian kernel of different with the Sigma parameters is called the width of the other kernel.",
            "Then I use Sigma is equal to 0.05, zero point 1, zero point 2.",
            "An 4 here is 0.05.",
            "Here is zero point.",
            "One here is 0.2.",
            "And what those pictures show is the value of H given the location where it's computed.",
            "The only thing that you have to remember and to see that.",
            "The red corresponds to the blue OK, here the red here.",
            "Is like the blue here.",
            "Anne the yellow here.",
            "Or even the blue here is like the red here.",
            "OK. Nice choice.",
            "I didn't realize that.",
            "OK, OK.",
            "So the only thing that you have to remember is that I used a very simple algorithm.",
            "Where I can analytically compute the Alpha eyes?",
            "I can analytically compute B.",
            "B the value of B.",
            "Am I just plug a Gaussian kernel into this very simple algorithm?",
            "I run it with this non linearly separable training data and I get I get those nonlinear decision surface because you can imagine that the the.",
            "I don't know if you can imagine but here.",
            "There is a.",
            "The border between the red and the yellow.",
            "This is the location where you you're going to do.",
            "Say everything that is in this part is going to be.",
            "Positive and everything that is going to be that is in this part is going to be negative.",
            "So.",
            "Again, we used a very simple linear algorithm we used, we just picked.",
            "A kernel that is well known.",
            "We picked the Gaussian kernel.",
            "We used it.",
            "And we switched the colors an we had these results.",
            "It's nice.",
            "They should be very happy now.",
            "Are you?",
            "Yes, nice."
        ],
        [
            "OK. Anne.",
            "A small thing about kernels.",
            "This is a glimpse of what is going to happen to you in in 2 hours.",
            "Or never because I think that I'm a little behind on my schedule, but.",
            "Here Instagram matrix of a kernel.",
            "This is given an input set S made of data X1 to XN, the gram the gram matrix of X of K4S is just the matrix made of all the.",
            "The DOT products between XI XJ, XJ, so this is obviously a symmetric matrix because K of XIXJ is equal to KFXJ XI an this there is a very nice property of.",
            "Offer positive definite kernels is that?",
            "Whatever the the training set or the set S that you choose, you always have this property that.",
            "There is this property here.",
            "So you take any set S of X1 to XN.",
            "You compute the gram, the gram matrix of K with respect to S. You'll see that.",
            "You will have this property.",
            "In other words, the eigenvalues of gram matrix are all non negative.",
            "OK.",
            "So if you construct your own kernel and you want to show that this is a valid kernel to do that is.",
            "Everything is valid.",
            "Actually, if you want to show that this is a positive definite kernel or if you want to, or in other words you want to see it to show that it fulfills Mercer's property, you just have to show that.",
            "It give rise to gram matrices that have non negative eigenvalues.",
            "It's not always the either easier way to prove that the kernel is a is a Mercer kernel, but these away."
        ],
        [
            "And this entails a few results that can be used if you want to construct kernels from kernels.",
            "If you take the power of a kernel, imagine that K1 and K2 are two Mercer kernels then.",
            "K1 to the PTH is also a Mercer kernel and if you take the positive combination of two, kernel is still a Mercer kernel.",
            "If you take the the if you take K 1 * K Two it's also Mercer kernel.",
            "So this is usually an exercise that I give when I teach.",
            "Kernel methods and this is usually.",
            "An exercise that nobody is able to do.",
            "So we're not going to do."
        ],
        [
            "So here is a partial conclusion.",
            "So the kernel trick does there is.",
            "This is the first thing that you have to remember.",
            "Is the kernel trick you pick your favorite linear classifier and kernelized it?",
            "And you you end up with a nonlinear classifier.",
            "Again, kernel Ising linear algorithm just consisting replacing all dot products by kernels, and you're done.",
            "Then we talked a bit about the Mercer's condition.",
            "Mercer's condition is is the thing on the positive, non negative eigenvalues and it's I'd like to stress out that it means that if you have a kernel that fulfills Mercer's condition that means that it's there exists a mapping file such that when you compute K of UV it's equal to the dot product between 5:00 or few and 5V.",
            "There are a few existing kernels that I showed you.",
            "Actually the again one that is the two that are very popular are the RBF kernels and the polynomial kernels.",
            "Uh, an very important thing.",
            "That I'm going to talk about more later is that.",
            "Doing classification with structured data such as trees, graphs, sequences.",
            "Is made possible by the idea of kernels and current methods, meaning that.",
            "The only the only thing that you have to worry about is to define a proper kernel.",
            "If you want to classify graphs, you just have to define Mercer, kernel and graphs and you can use this very silly dumb algorithm that I showed you and you can do classification on graphs.",
            "If you want to do classification on sequences, you just have to derive a new kernel sequences and then use any linear algorithm that you know.",
            "For instance, the perceptron algorithm that young showed you Monday, and you can use it.",
            "You can kernel eyes it and you will end up with a nonlinear with a classifier for sequences.",
            "So.",
            "The the important thing that is that with the kernel method trying to tackle problems where that involve structured data is is a Mount to the problem of finding good kernels.",
            "Anne.",
            "Algorithms?",
            "Exist?",
            "There is no no no, no need to worry about algorithms.",
            "The questions that go with the with the kernel trick is an.",
            "I'm not going to answer this because I don't exactly know how to to answer it is how you choose the right kernel.",
            "Because yeah, it's difficult.",
            "How you can combine kernels so there are a lot of works that address the problem for combining kernels.",
            "Usually the combination of kernel is made as a convex combination.",
            "That is, it's almost like the combination I talked to you about that is this."
        ],
        [
            "This combination here.",
            "With the with the convex constraints on the."
        ],
        [
            "The coefficients.",
            "Then there's a question of building kernels for a structured data.",
            "And there's another question I don't know if it's a question or something that I didn't say much about his.",
            "What kind of algorithm that you can use, or you can use as a kernel algorithm.",
            "So here here's a list of very of some algorithm to it's Ridge regression.",
            "The Kernel, Ridge regression.",
            "I don't know if you know what is regression, it's just least square regression where you want to to predict real value from X.",
            "From from some input data X an you use a square.",
            "At least square methods and you do.",
            "You had you had some regularization and it gives you a Ridge regression and you can kernelized this.",
            "You can do very closely related.",
            "Algorithm which is a kernel Fisher discriminant.",
            "You can do a kernel PCA, so it's a principle component analysis.",
            "It's an unsupervised method that you can use an you can do SVM's.",
            "There is support vector machines and this is the topic of the next.",
            "The next 15 minutes."
        ],
        [
            "OK. Are we good?",
            "OK, so so Marie asked me, asks me if if if you have some kind of knowledge.",
            "As how the chosen kernel is going to to make your data separable or not.",
            "An you cannot say, except that if you work in a specific domains, usually if you know that the the kernel that you derive is is.",
            "Sensible makes sense for for the experts of the domain then.",
            "You have good chance it's it's more probable that you can have separation, but otherwise you cannot say that's why I say that the choosing the right kernel is.",
            "It's a very difficult and you cannot even know whether you go to the good direction or you choose a variable.",
            "Totally wrong kernel.",
            "So there is a new answer to this.",
            "Wait, I think I heard that there are like theories and said that to who are you?",
            "Finally you are very weird.",
            "Yeah, sure.",
            "Can you find a division problem your way?",
            "Yeah, sure.",
            "I think everybody heard what you say that.",
            "I'll say it again.",
            "Obviously if you you take points in very high dimension then if it's the dimension is very high you can always find a hyperplane that separates all the data, but then there is not another thing that enters into account is the richness or the capacity of the class of functions that you consider because what you say is that of course if you take a very very, very rich feature space and then you can separate everything.",
            "But the thing that you have to pay for that.",
            "If you are not careful is that you will have very poor generalization capacity of your model.",
            "That is, you're going to be able to separate all of your training data perfectly, but then you will not be able to say anything about new test point.",
            "So but right?",
            "So I'm going to talk a bit bit about regularization Ann here for the SVM and then in the in the last part of the talk.",
            "Some in kind of team picture from which we can know that we're choosing right, not right, whether it's better, what?",
            "How to know that?",
            "OK. What I sense in your questions, not not yours particularly is that.",
            "But all your questions that.",
            "Your questions is are more related to some theoretical guarantees that say OK, if I choose a kernel, how can I be ensure that this kernel is going to help me classify the data an?",
            "From what I understand from your question, there is some kind of theoretical thing.",
            "Hidden here an I'm going to answer on the non theoretical thing.",
            "The thing is usually if you want to measure if you if you want to say that your kernel is better than another kernel or is well suited to the problem that you use, you just do some cross validation thing and you estimate the generalization property.",
            "You cannot say OK, let me look at the data and I know that that this kind of kernel is going to be OK or not.",
            "You can do it sometimes.",
            "For instance if you work with images or sounds, then you can use wavelet kernels and things, but otherwise it's very difficult to know that.",
            "An and designing good kernels.",
            "Very effective kernels is really difficult, and even if you have good kernels, sometimes you have small parameters.",
            "You want to say that these parameters don't count, and it's very easy to to to to choose the right values for the parameters, but it's not sure if you really want to tackle specific application.",
            "If you want to have very good results and then you want to use a kernel classifier, then at some point you have to find the good Colonel, the good, the good parameters, and so on.",
            "And it's just.",
            "It's it's it's very difficult."
        ],
        [
            "So.",
            "Anne.",
            "Let's talk about SVM.",
            "So this is something that.",
            "Came out in the beginning of the 90s.",
            "Anne.",
            "It was a spur by the work, the theoretical work of Vatnik on statistical learning.",
            "And this is something that.",
            "So that came out in 90 two and was refined a little bit in a 95 an.",
            "I think that in some ways it changed a lot of things in machine learning.",
            "Not that I was doing machine learning before that, but it changed a lot of things.",
            "So.",
            "Anne."
        ],
        [
            "Again, I'll come back to this later.",
            "So why?",
            "Why was it a breakthrough in machine learning?",
            "Because of the following.",
            "Four points.",
            "First, OK, before going to the four points, look at those two pictures.",
            "These were we are.",
            "We are heading out.",
            "Here you have black points.",
            "White points have the same black points.",
            "We have the same white points and you want to find a linear classifier that puts that puts all the white points on one side and all the black points on the other side.",
            "An you have here two different classifiers, two different hyperplanes that do that do so.",
            "The first one here.",
            "Is OK, you have all the white points on one side on one side and all the black points on the other side.",
            "And you have this classifier here where you have also all the white points on the one side on one side and all the black points on the other side.",
            "So I stop just a little bit and I'm trying.",
            "Yesterday I was looking at the at the picture and I lost something very important is that.",
            "The white points and the black points are look localized at the same position on the left and on the right, even though this is quite disturbing, having the hyperplane is quite disturbing.",
            "If you can do a mental thing to withdraw and to remove the hyperplane, you'll see that.",
            "The white points and the black points are exactly the same on the under under two under 2.",
            "On the two graph, no.",
            "Yes.",
            "They are.",
            "I tell you.",
            "Because I did just a copy pasta copy paste.",
            "Yes, that's true it's.",
            "OK, there is a missing 1.",
            "Maybe maybe.",
            "Maybe yes, no?",
            "This one we don't care about this guy.",
            "This one.",
            "OK, doesn't doesn't count.",
            "No, it's OK.",
            "It's OK, it works.",
            "If it had been this this point or something very close to their hyperplane, I would have been sad, but it's OK, I don't care so.",
            "OK, they're almost the same.",
            "But trust me, for the following dungeon.",
            "Just trust me just what I say, not what you see.",
            "So why is it very important?",
            "Why was it a breakthrough?",
            "Because of of?",
            "Among others, the four points that I mentioned here.",
            "First it's use.",
            "It was one of the very first algorithm that used kernels and all the kernel kernel machinery actually, kernels were known in the 70s and things like that, but there were no algorithm to properly wear algorithm, not nothing, but not as efficient as effective as VM's.",
            "It's rests on the idea of margin that is.",
            "Between those two hyperplanes, usually people prefer this one.",
            "I don't ask you if you prefer this one because.",
            "Because I'm afraid of you, but this one is better.",
            "Just because the distance between the closest point.",
            "The closest white or black point to the hyperplane is larger here than here, meaning that you have more security in classifying your data because.",
            "Anne.",
            "And what I'm going to say is related to the statistical learning theory thing is that your assumption when when you're doing statistical learning is that all your data are.",
            "In the independently and identically distributed data.",
            "Meaning that this is something that is supposed to show you how the data are in the space.",
            "An if you see.",
            "This plot and this plot you may imagine that it's not.",
            "It's very probable to see like.",
            "GNU GNU test points new Black point following here, here, here, here, here, here or here, and you white points falling here or here or here or here an if you put more security and larger margin here then what you ensure that you don't insure it.",
            "With hyper you enter it with a high probability.",
            "You do ensure it with a high probability that it's it's.",
            "Very like unlikely that new points coming from the same distribution will fall on the wrong side of the hyperplane.",
            "If you take a large margin, the larger the margin, the larger the security, the smaller the probability that a new point will fall on the wrong side of the hyperplane.",
            "So this is one of the points that is very important with VM's is that it came along with the idea of kernels.",
            "The idea of margin.",
            "The very strong theoretical background.",
            "Anne.",
            "A convex optimization problem.",
            "Meaning that someone talked to you about convex optimization just a little bit on Tuesday.",
            "And this is something that is very very, very nice, meaning that at some point in the the way you're going to model the classification problem with the supervector machines, you will have to minimize something.",
            "And the thing that you're going to minimize is is convex.",
            "Functional with convex constraints, meaning that the problem that you have is a convex optimization problem.",
            "And the nice thing about convex optimization problem is that you have global minimum.",
            "It's not like with.",
            "I don't know if you know if some of you have already worked with the.",
            "Multilayer perceptions, then you minimize something and you don't exactly know how and where you're going to end up it really.",
            "It really depends on on how and where it started when you you started your optimization process.",
            "Here with with the SVM's you just have a convex optimization problem.",
            "To solve an.",
            "Another thing is that it's a quadratic programming problem.",
            "A quadratic programming is a very well known field of research for people doing optimizations, so there exists a lot of different implementations of.",
            "For.",
            "Offer of code doing optimization and Ann.",
            "It's it's a very well known field, so you have all those four points coming together and it was was actually, I think the first time that all those .4 points were coming.",
            "Not exactly together, but almost together.",
            "And it's very important.",
            "This is the reason why.",
            "It adds it gained a lot of attention, and there's another thing that I didn't mention is that.",
            "They also used as VM, so this is a practical practical point that I didn't mention here.",
            "They used supervector machines to do handwritten digits classification, and they were capable of reaching the same accuracies as people working on neural networks and tuning the neural networks.",
            "And it was.",
            "I mean there is the practical thing that also is an asset of DMS.",
            "It's just not just something that is nice, easy to optimize, and that has a strong theoretical guarantees, but it also works in practice.",
            "So you have everything in one thing.",
            "Almost everything except that you have."
        ],
        [
            "To choose the right kernel.",
            "OK, so I'm going to start with the basics, which is the hard margin linear SVM.",
            "So the setting that we consider is.",
            "We have an input space SpaceX with DOT product that I have changed my notation.",
            "OK, just to stimulate your brain so this the dot product here is there just dot.",
            "The target space is a Y and we have a separable training space set that is called S. It's there is.",
            "It's a linearly separable training set.",
            "And what you want to to find when you're doing?",
            "SVM is to find what is called the optimal hyperplane, which is the hyperplane that maximizes the margin which maximizes the distance between the closest points to it.",
            "The closest ones OK, so between those two guys.",
            "When you're going to do.",
            "SVM's you're going to try and and and learn this classifier because the margin is larger.",
            "Then just to anticipate a bit about about the next slide.",
            "OK."
        ],
        [
            "Let's go to the next slide.",
            "We're going to consider.",
            "Just let me say what we're going to do is 222.",
            "I'm what I'm going to do is to show you how you end up with the convex optimization problem that I talked to you about just just before.",
            "So.",
            "When you want to find a hyperplane, you will have to to make some assumptions to put some constraints on how the hyperplanes defined.",
            "Because imagine that you have these hyperplanes here, which is totally parameterized by W&B.",
            "You can multiply W&B by the same constant and you will have exactly the same hyperplane.",
            "So there is a degree of degree of freedom that that has to be taken into account.",
            "And in order to take it into account, we are going to consider to only consider Canonical hyperplane with respect to the training set we have.",
            "A Canonical hyperplane is such that the closest point to the hyperplane verify www.x I + B is equal to 1 and 4 minus point www.x I plus basic is equal to minus one.",
            "This is my the way of.",
            "Constraining the thing about the.",
            "The fact that you can multiply W&B by some constant.",
            "And again, something that you did in high school.",
            "The margin that is the distance between this one and this one.",
            "Those two hyperplane is 2 divided by the normal W OK. Because just the distance of Point XI that realizes this, that is such that www.x I + B is equal.",
            "One the distance of such point to the hyperplane is 1 divided by norm of W."
        ],
        [
            "So when you you, you're doing.",
            "SVM you you, you, you you try to maximize the margin.",
            "And at the same time you try to have a W such that all the positive point fall on one part on one side of the hyperplane and all the negative points on the other side.",
            "An formally, you can write it down like this.",
            "You say that.",
            "You want to maximize the margin.",
            "So maximizing the maximising the margin is like minimizing the inverse of the margin.",
            "So I want to minimize the inverse of the margin.",
            "I take a square because that's how I feel good.",
            "And I have this constraints which say that for all positive points that is, all points with label Y equal to plus one.",
            "I want this to be realized that is WXRW dot X I + B. I want it to be greater than plus one and for all the negative points.",
            "I want the this this value to be lower than than minus one.",
            "Looking, looking at the this at the pictures, it says that.",
            "I'm just looking for a Canonical hyperplane.",
            "We just say that.",
            "OK."
        ],
        [
            "And those two constraints can be summarized into these constraints.",
            "Because if Y is equal to 1, you can multiply.",
            "The two parts by why I and you end up with a YIW's dot X I + B greater or equal to 1.",
            "And if Yi is equal to minus one, you multiply the two parts by Y and you don't forget to invert the sign.",
            "And minus one by minus one is equal to 1 and you also have the the the same the same result.",
            "So normally, if you you're familiar with optimization.",
            "You're already good.",
            "You say, OK, that's good.",
            "That's a quadratic program.",
            "I know how to do that.",
            "If you're not familiar, you'll say OK. What can I do with that?",
            "And The thing is.",
            "This might be a little bit complicated that might look a bit complicated.",
            "And I'll show you how to deal with this.",
            "A little bit complicated constraints.",
            "We're going to do.",
            "Relaxation."
        ],
        [
            "OK.",
            "So.",
            "We're going to introduce LaGrange multipliers.",
            "The solution WB that we're going where we're looking for can be found by be by baby.",
            "It's not beats by by solving the following problem.",
            "Minimizing the maximum of these function.",
            "And this function is defined as that.",
            "This mysterious function.",
            "Has been studied by people doing optimization an what is nice about this function is that it's.",
            "It helps you.",
            "Record an an rewrite the optimization problem that I showed you just before.",
            "In an unconstrained manner, OK, it's not very easy.",
            "It's not very easy to deal with Min, Max, etc.",
            "But still you're happy because you should be happy.",
            "You must be happy because it's unconstrained, except for the small constraint here on the on the Alpha.",
            "But it's nothing.",
            "So, like I say, it's another formulation of the constraint optimization problem.",
            "Um?",
            "One way of getting some intuition about how it works is to see that if one of the constraint is violated is is not, is not fulfilled.",
            "For instance, you have Yi times the value XI plus B -- 1 that is lower strictly lower than 0.",
            "Which is which means that W&B violates the constraint on Y&XI.",
            "Then this value here is negative.",
            "Here you have a minus, minus and minus is A plus you want to maximize with respect to Alpha, which is positive.",
            "Then you can take Alpha equal to Infinity and then you have something that is equal to Infinity.",
            "That is, you're you're trying to minimize something.",
            "That is always equal to Infinity, and you know that it's not going to be your solution.",
            "And this is necessarily you.",
            "You have that all the constraints.",
            "Will be fulfilled at the end of your optimization thing, because because of these these remark.",
            "And Conversely, you have.",
            "It's not exactly.",
            "Conversely, you also have that if you have a point that fulfills the constraint, then necessarily you have Alpha I that is equal to 0.",
            "Here, suppose that this is positive.",
            "Then here you have something that is positive.",
            "Then you have something that is minus something positive.",
            "And if you want to maximize that, you have to minimize this guy where, where, where?",
            "This is positive an knowing that Alpha is cannot be lower than zero.",
            "Then you know that the optimal value for this specific constraint is a time attained for Alpha I equal to 0.",
            "So this is what for?",
            "For those of you who are familiar with the optimization, this what how you can get the optimality optimality conditions or the courage kentucker conditions."
        ],
        [
            "So here is the important thing on the minimization.",
            "Is that?",
            "I said if you want to find W&B, you just have to solve this.",
            "Annual have the correct WNB.",
            "So the mean of the Max of L and there is a theorem that you can find in many optimization book which say that L is concave with respect to Alpha and convex with respect to WNB.",
            "Concavity convexity is something that you you saw with the with the Sandrina lost last Tuesday.",
            "Then if you have this property that this function is concave with respect to the thing that you maximize and convex with respect to the thing that you minimize.",
            "Then you can switch the Max in the mean.",
            "And you have the same optimal points.",
            "That is, you have the same WB&R fact, that is, that are good for you.",
            "And now you have something that is very nice, which is that this problem here if you just forget about the Max and you consider just the minimum of L with respect to W&B, this is an unconstrained optimization problem.",
            "Because you don't have constraints on WNBA.",
            "And more than that, it's an unconstrained optimization problem.",
            "It's convex optimization problem.",
            "It's a differentiable optimization problem.",
            "It's a twice differentiable optimization problem, which means that.",
            "It's convex, so a necessary condition for it, for for having an an optimum is to have the.",
            "Gradient equal to 0.",
            "An if you want to add the gradient be equal to 0, just compute the gradient through the gradient was with respect to W, is this?",
            "W minus some of YIXI.",
            "Oh there is an Alpha I hear missing.",
            "Just remember.",
            "OK.",
            "There is an Alpha I hear.",
            "And if you take the derivative with respect to B, you have the sum of Alpha Yi.",
            "Am am needing via gradient requiring the gradient to be equal to 0.",
            "Implies those two constraints, so there is still an Alpha I missing here.",
            "I'll see it again later when you have the printouts.",
            "And you have those two constraints an you can what we're going to do is to plug back this this this.",
            "This expression of W, which is just linear combination of all the excise inside this.",
            "This function an will retain these constraints.",
            "Ann will get this problem."
        ],
        [
            "So what I did is that I know that because this is a convex function is differentiable.",
            "It's is.",
            "Everything is nice, I just take the derivative, I take the gradient, I'm making it equal to 0.",
            "Then it gives me an expression for WI mean it gives me another constraint and I used to those two constraints and the expression of W and they replace it into L, replacing and then the mean disappears because I intentionally took the gradient and made it equal to 0 to have the mean of that.",
            "So I replace W&BI, plug them back into the function L, doing some computations, very simple computations, and not forgetting about the Alpha adds.",
            "You have that the problem that you have to deal with is this problem.",
            "So this problem is is a maximization problem with respect to Alpha and the constraints that you have.",
            "Are this constraint which is pretty simple and these otherwise other constraint that just say that all the Alpha should be should be non negative?",
            "And the function.",
            "The objective function that you consider here is this function, which is a quadratic function.",
            "That is expressed in terms of Alpha and Alpha J of our five or the Alpha eyes, and this is a convex function and this is.",
            "This is a convex programming convex program.",
            "These quadratic program.",
            "And now you should be already thrilled and excited about something.",
            "I can see the excitement in your eyes, but you should be excited because you just have dot products here.",
            "So that's good because at some point we have to Colonel Eyes SVM's.",
            "And to criminalize them, you just.",
            "We'll just do our famous well known kernel trick and replace everything where we see that product by AK of XI.",
            "XJ."
        ],
        [
            "So the quadratic programming problem you end up with is this one.",
            "This is exactly the same as before.",
            "And the the remarks that I'm doing that the number of of variables that you have to consider is.",
            "The number of data that you have at the beginning, so this is something that is a problem in some way, because if you have 1 million data then you have a program and optimization optimization optimization problem defined with respect to 1 million variables.",
            "It's very difficult to solve, but we're getting at it.",
            "We I don't know if we but people are providing solutions for that, and there's something also very interesting.",
            "Didn't mention it, I think is that there are very few.",
            "Alpha eyes that are different from zero at the end of the optimization process.",
            "And just remember that I think that I wrote it down in a few slides, but remember that the only there is one constraint there is 1A I per training data for each site there is 1A I.",
            "An at the end of the optimization process.",
            "There are very few Alpha eyes that are different from zero.",
            "And those very few are five that are different from zero corresponds to correspond to what we call the support vectors.",
            "So remember that.",
            "So.",
            "We are this is a convex quadratic program.",
            "There are a lot of of the shelf libraries that you can use to solve this, provided the program is not too big.",
            "Otherwise, you have to resort to two specific algorithms.",
            "Very nice, very beautiful algorithm to solve as VMS.",
            "They are very, very nice algorithms.",
            "I'm not going to talk to you about those algorithms because they are.",
            "Involved.",
            "More less involved.",
            "And and So what I told you is that you, we only have dot products that appear here when you once you can compute the dot product, you're happy.",
            "There is something that that has disappeared here in some ways.",
            "Is that big?",
            "Is not anywhere here.",
            "It's not a problem because you can.",
            "We can retrieve the value of of B using the key KKT conditions.",
            "That is the optimality conditions.",
            "I will not enter the details here an at the optimum.",
            "You know that W start this.",
            "The optimum value is computed as some of Alpha istar why I XI so Alpha I star is just the solution of this problem.",
            "And here I haven't forgotten the Alpha and the expression of a W, which is a nice thing.",
            "And you see that F or H or watch FHF.",
            "Let's say 8 F is computed as some of Alpha IYIXI dot X + B star.",
            "Meaning that again if you want to come to to to compute the prediction of F for specific point X, you just have to be able to compute DOT products.",
            "Here.",
            "These guys going to be kernelized."
        ],
        [
            "And the last thing is about support vectors.",
            "So the Super vectors are those points for which Alpha I star is strictly positive.",
            "The support the margin, so those are the support vectors.",
            "Meaning that if you want to, if you minimize this for this training set at the end of the the optimization process, you'll end up with these hyperplanes with W expressed with respect to the Alpha ice, and the thing that you only have 3 points of all those points that are going to have Alpha are different from zero, and those three points correspond to those that are the closest to the separating hyperplane, and those are the support vectors.",
            "Anne.",
            "So something that I'm I haven't written down on the slides is that if you want to rerun the SVM learning algorithm only on those three points, then you have exactly the same solution.",
            "That is, the Super vectors contain contain all the information about the decision that you've computed."
        ],
        [
            "No, I'm going to talk about soft margin linear SVM.",
            "And when you work with real data then usually you have outliers, meaning that you have the points that count.",
            "That can't be.",
            "In separate with hyperplanes.",
            "For instance, you have this black point here and here.",
            "And they can't be nicely separated.",
            "An yes.",
            "An OK there is an R missing here.",
            "So what you do to deal with that is that you introduce what you call Slack variables you will penalize.",
            "The fact that these black point is not on the right side of the hyperplane and you will measure you will define how you penalize these.",
            "These points with a distance of this point to the.",
            "The hyperplane WX plus B is equal to minus one or plus one.",
            "So if you look at the minimization problem, or the formalization of the problem that you you can, you can derive you can write it down like this.",
            "This is the again one norm soft margin SVM.",
            "You have still the thing that you want to minimize, or to maximize the margin and then you say that at the same time you same time you don't want too many points to be to fall on the wrong side of the hyperplane.",
            "So you want to minimize the SIII where the eyes are just.",
            "This distance.",
            "Looks eye eyes are these distances OK, so you want to have?",
            "This distance is as small as possible.",
            "And this can be written like this.",
            "OK, so I've I've put in, read the only differences between.",
            "The hard margin SVM's that I showed you before and the soft margin formulation.",
            "And for.",
            "Those of you who.",
            "Ask me questions about how to choose the right kernel an how do you know that the kernel is good?",
            "Then there is another guy here that enters into play that doesn't help us.",
            "In life which is see.",
            "The reason is that another parameter that needs to be tuned very carefully.",
            "Well.",
            "So it's it's not easy.",
            "You usually should.",
            "You should do cross validation or there are nice things about generalization bounds that can be used or used, or estimation of the leave one out error that I've been derived, but.",
            "It's not a common practice to use them.",
            "Ann, usually people resort to cross validation thing and computationally intensive things.",
            "So when you use all the machinery that I talked to you about, that is the Lagrangian you introduce all the LaGrange multipliers.",
            "Then the quadratic problem program that you end up with is this one.",
            "So.",
            "If you add if you consider that you may have points that don't fall in the on the on the right side of the hyperplane, then you write the program like that.",
            "You use all the thing about geology and the LaGrange multipliers and you end up with a problem like this which is almost the same as the hard margin problem, except that here you have this box.",
            "What you call box constraint, meaning that now.",
            "Alpha is required to be between zero and C. Before we just had that Alpha should be between zero and Alpha and now because we have outliers and we want to deal with outliers, we say that Alpha.",
            "Has to be between zero and see.",
            "So there is meaning that this slight difference doesn't is not very difficult to take into account if you have already a. Algorithm to solve the quadratic program for the merging problem."
        ],
        [
            "And there is a two norm two norm SVM.",
            "Soft margin SVM where here you put a square.",
            "Just before I didn't put a square, there was a if you want to one an the difference so you do all the trick with the LaGrange multipliers and so on and you end up with these these.",
            "Learning this up this.",
            "Quadratic program where now the difference is just here.",
            "Before you didn't have this disturb, you just had excite dot XJ.",
            "And now you have excite dot exemples.",
            "Delta I J, / C, Delta IJ is equal to 1 when I is equal to J and is equal to 0 otherwise, meaning that.",
            "1st.",
            "In this hard margin case, you had to consider the gram matrix.",
            "The gram matrix made of.",
            "Let's use the same notation.",
            "So in the hard margin case you had all the information you needed to solve the SVM problem.",
            "In this gram matrix you had everything because the only thing that you needed.",
            "Where the the why the?",
            "Why eyes you?",
            "You still have them and then the dot products.",
            "In the hard margin case, you had to consider this.",
            "And then what I say here is that if you consider the soft margin the two norm, soft margin, linear SVM, then you will have to consider.",
            "This gram matrix.",
            "Where only the diagonal elements have been changed.",
            "An everything else remains the same.",
            "So again, from a purely algorithmic point of view.",
            "If you had something working for the hard margin SVM, you have something something working for the two norm soft margin SVM."
        ],
        [
            "An something that I wanted to to talk to you about.",
            "Because we're going to program.",
            "This tomorrow, but with the colonels.",
            "So you can have unconstrained version of the one norm SVM, soft margin, SVM and the two norm soft margin SVM.",
            "Those are the primal forms, meaning that I don't use LaGrange LaGrange multipliers percent, so this is just another.",
            "Another formulation of the problem of the previous problem.",
            "I say that I want to minimize to maximize the margin and then I have these slack.",
            "This exactly the same thing as.",
            "The distance between this point and the the corresponding hyperplane.",
            "So, and this is the hinge loss.",
            "I think a submarine mentioned it to you on Tuesday.",
            "This is just the Max of the Taecyeon 0.",
            "And here you have to this function this unconstrained problem, because you don't have constrained on W&B.",
            "Those are the only ones.",
            "The only one variables that you're going to do your optimization on, and they are not constrained.",
            "And The thing is that the dysfunction is still convex, so the image loss is a convex function.",
            "So the sum of convex functions is still a convex function.",
            "Is convex dirty?",
            "This convex, which is good, but.",
            "At the same time, is not differentiable differentiable.",
            "So you cannot directly use the gradient decent algorithm that that you you you did two days ago.",
            "And but there exists there exist solutions to do this.",
            "I'm not going to tell you about these solutions.",
            "And but this problem.",
            "Which corresponds to a primal form of via two norm SVM is is that you just put a square here?",
            "And this problem is very nice because it's convex and it's differentiable.",
            "And this is what we're going to program tomorrow.",
            "We're going to do a Newton decent on this function and hopefully in four hours will be able to program an SVM.",
            "This is also something that I tried with my students.",
            "And with the same results as before."
        ],
        [
            "But I trust you guys.",
            "No.",
            "Where I'm going to to end with I?",
            "I think I I still have two slides or something like that.",
            "Something very very easy is now we're going to do nonlinear separation.",
            "Before the kernel time, before the kernel method time.",
            "Moving from linear to nonlinear things was difficult.",
            "Here is going to be just easy.",
            "So I took some time to explain you how you you derive the optimization problems for one norms of margins and two norms of margins, soft margin, SVM's.",
            "Is Jens and now it's just easy.",
            "It's just the kernel trick to have a nonlinear decision function.",
            "So the only thing that I did.",
            "In comparison with what I said earlier is that I had that product here.",
            "I just replaced the dot product by K. So this is the current kernel trick.",
            "And here again, and now just doing that I have.",
            "A way to derive a large margin separating hyperplane in some high dimensional space that you don't know.",
            "But you can do you have all the theory that goes with not exactly all, but to much of the theory that goes with linear separation that carries over.",
            "With the with the Kernels an you don't have anything to do, it's just easy.",
            "So the size of the problems scale with the number of data that I told you that before.",
            "There are very efficient and very elegant methods to solve these problems.",
            "Not noticeably exploit the sparsity of this solution.",
            "Ann K&C are two things that it's not exactly hyperparameters for K, but OK. Let's say that it's something that you have.",
            "It's in hyperparameters hyperparameters.",
            "It's something that you really need to choose carefully."
        ],
        [
            "If you want to really have have good results.",
            "On practical applications so.",
            "The important things about machine support vector machines are the following.",
            "Their based on margins.",
            "There they use kernels.",
            "There is a nice convex optimization problem that you can obtain and solve.",
            "It's very effective if you try.",
            "It's very effective in the sense that.",
            "Experts on SVM are non experts, are capable of achieving very good of achieving state of the art results on different applications.",
            "So it's practically it works.",
            "It's not just a nice thing from the theoretical point of view.",
            "And something that I didn't talk about here is the algorithmic details.",
            "Like how the algorithms to solve these problems work.",
            "So there's the terms I wanted to to to.",
            "To let you know that the terms like chunking decomposition, active set, stochastic gradients, there are.",
            "Those are things that are used for efficient optimization of the problem.",
            "I didn't talk to you about learning the in the promo.",
            "This is something that we're going to do tomorrow.",
            "And and I didn't talk to you about theory.",
            "And I said, be patient, because if we have time, we'll talk about the theory later.",
            "So that's."
        ],
        [
            "But for the first 2 hours, I think we're gonna have a break.",
            "Yeah, but if you have questions.",
            "Or if you want to have coffee, let's go there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we should get started.",
                    "label": 0
                },
                {
                    "sent": "Normally there is someone who should introduce me, but nobody loves me, so.",
                    "label": 0
                },
                {
                    "sent": "I'm leyva.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk to you about kernel methods, and I think I'm going to start with the very basics of kernels an little by little.",
                    "label": 0
                },
                {
                    "sent": "I'll go to very, very theoretical things, but normally I shouldn't have time to to get there.",
                    "label": 0
                },
                {
                    "sent": "But still you have everything on the slide soon.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "At first I will build a very, very, very simple kernel classifier.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That you can you can.",
                    "label": 0
                },
                {
                    "sent": "You can if you are a little bit familiar with octave or or Matlab, you can program it in 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "I did it in 5 minutes yesterday.",
                    "label": 0
                },
                {
                    "sent": "I'm not a very good programmer, so so it's it's it's very simple to do it.",
                    "label": 0
                },
                {
                    "sent": "Then I I will talk to you bout supervector machines.",
                    "label": 0
                },
                {
                    "sent": "Because this this is the.",
                    "label": 0
                },
                {
                    "sent": "The main reasons the main reason why current methods are so popular because of this this very specific algorithm.",
                    "label": 0
                },
                {
                    "sent": "And after that I will take will take a short break an after the break will I'll talk to you about some example of kernels for structures because this is one of the very interesting points of kernels is that you can use kernel methods to work with the structured data like graphs, trees, sequences which you cannot do easily with other kinds of methods.",
                    "label": 1
                },
                {
                    "sent": "An then this is the last part, the lost treasure of this talk is the theoretical part, and I will talk to you about the Representer theorem, which is very crucial an.",
                    "label": 0
                },
                {
                    "sent": "I will talk to you very, very, very little about generalization, bounds and complexity of class of class of family of classifiers.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how you can derive generalization bound.",
                    "label": 0
                },
                {
                    "sent": "I'll show you that it's very easy to do that.",
                    "label": 0
                },
                {
                    "sent": "And that's gonna be it.",
                    "label": 0
                },
                {
                    "sent": "I'm for this tutorial.",
                    "label": 0
                },
                {
                    "sent": "I took many sources of inspiration and primarily I I worked with the with the.",
                    "label": 0
                },
                {
                    "sent": "A tutorial by Bear National Cup.",
                    "label": 0
                },
                {
                    "sent": "I know if you had the chance to to see the tutorial, but it's very.",
                    "label": 0
                },
                {
                    "sent": "It's really nice.",
                    "label": 0
                },
                {
                    "sent": "It's it's very easy to understand and I'm trying to to do as as well as it does, but I don't know if he if I'll.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exit so.",
                    "label": 0
                },
                {
                    "sent": "To warm up we will going to construct our first kernel classifier.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And um, first of all.",
                    "label": 0
                },
                {
                    "sent": "For all all these talk, I'll focus focus on on binary classification.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I will not talk about regression density, estimation, ranking, or other tasks.",
                    "label": 0
                },
                {
                    "sent": "I will just talk about binary classification.",
                    "label": 1
                },
                {
                    "sent": "That is, I have two classes minus 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "I want to predict the class of 1 one data saying that it's positive or negative.",
                    "label": 1
                },
                {
                    "sent": "So I think that you already have a good grasp.",
                    "label": 0
                },
                {
                    "sent": "How how classification can be used for real world tasks?",
                    "label": 0
                },
                {
                    "sent": "So some tasks that can that you can imagine is for instance prediction.",
                    "label": 0
                },
                {
                    "sent": "Predicting how serious is a disease for a patient given some measurements of his blood pressure, heart rate, fat rate and things like that.",
                    "label": 0
                },
                {
                    "sent": "Something that he talked to you about land Monday was detecting detecting faces in pictures.",
                    "label": 0
                },
                {
                    "sent": "And a very popular application of of machine learning is spam filtering.",
                    "label": 0
                },
                {
                    "sent": "So the question is just to be to to have.",
                    "label": 0
                },
                {
                    "sent": "In a filter that can identify spam or regular Mail and the idea is that you have to train these machines so that you can.",
                    "label": 1
                },
                {
                    "sent": "Accurately identify regular Mail or or spam.",
                    "label": 1
                },
                {
                    "sent": "So the reason why classification or automating this this kind of tasks is very important are numerous, some some.",
                    "label": 0
                },
                {
                    "sent": "Some of the of the reasons are the following.",
                    "label": 0
                },
                {
                    "sent": "So there is there is a question of very simple question of dealing with a very huge amount of data.",
                    "label": 0
                },
                {
                    "sent": "So computers are very handy to do that.",
                    "label": 0
                },
                {
                    "sent": "You we humans cannot work with a 10,000.",
                    "label": 0
                },
                {
                    "sent": "Of the data, whereas it's very, it's very easy for a for computer.",
                    "label": 1
                },
                {
                    "sent": "It's so speed cost, it's very it's.",
                    "label": 0
                },
                {
                    "sent": "It's very.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you talk about problems in by Infomatics, it's very.",
                    "label": 0
                },
                {
                    "sent": "It's very expensive to do real experiments on human or animals, and sometimes you want to.",
                    "label": 0
                },
                {
                    "sent": "To identify, for instance, molecules that can be active or inactive towards some disease, and before doing that and and trying these these molecules on real animals and real living living beings, it's it's more easy, and it's much less expensive to try it on on a computer and try to estimate if the molecules that you consider that you consider our our.",
                    "label": 0
                },
                {
                    "sent": "Active or inactive?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The very important notions when you you you try to build a classifier is that you have a limited number of training data from which you try to infer model that you're going to use to do predictions an what you you aim at is to have a model that has a very low probability of error, that is, it has a very low risk.",
                    "label": 1
                },
                {
                    "sent": "And the the idea of having a low risk is is is the idea of generalization.",
                    "label": 0
                },
                {
                    "sent": "So to sum up you you just have a limited number of data, you try to infer model and you want it to be very very accurate on new data.",
                    "label": 1
                },
                {
                    "sent": "That's a share.",
                    "label": 0
                },
                {
                    "sent": "The same statistical irregularities as the data that she had in training.",
                    "label": 0
                },
                {
                    "sent": "Here is some notation that I'll be using too.",
                    "label": 0
                },
                {
                    "sent": "Throughout this talk, 1st I'll call X the input space, the space where the data I'll be working on our leave.",
                    "label": 1
                },
                {
                    "sent": "Why the space space of classes though?",
                    "label": 0
                },
                {
                    "sent": "It's just minus 1 + 1 and F is the model that I'm going to learn, and sometimes I call it H, so F or H. Other models, it depends from one slide to the other.",
                    "label": 0
                },
                {
                    "sent": "It changes.",
                    "label": 0
                },
                {
                    "sent": "This is too.",
                    "label": 0
                },
                {
                    "sent": "To keep you waking up so an here is a what we're gonna do is simply that there is.",
                    "label": 0
                },
                {
                    "sent": "We have two sets of points and we're going to try and find a linear classification classifier.",
                    "label": 0
                },
                {
                    "sent": "There is a linear separation between the blue points and the red points.",
                    "label": 0
                },
                {
                    "sent": "We're not going to.",
                    "label": 0
                },
                {
                    "sent": "We're not going.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do something.",
                    "label": 0
                },
                {
                    "sent": "More difficult.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Even though you are all familiar with this and I'm sure that.",
                    "label": 0
                },
                {
                    "sent": "Many of you are going to say that it's useless.",
                    "label": 0
                },
                {
                    "sent": "I'm going to spend a little bit of time reminding you some very basics basics about linear algebra, an sum of vectors.",
                    "label": 0
                },
                {
                    "sent": "You already had something with something, but I think that's it's.",
                    "label": 0
                },
                {
                    "sent": "It's going to be even easier than what she she taught you yesterday, yesterday, Tuesday.",
                    "label": 0
                },
                {
                    "sent": "Tuesday is OK.",
                    "label": 0
                },
                {
                    "sent": "The day before yesterday.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here you have examples of vectors you have UVWC.",
                    "label": 0
                },
                {
                    "sent": "And what can I say about that so?",
                    "label": 0
                },
                {
                    "sent": "So those are vectors.",
                    "label": 0
                },
                {
                    "sent": "The space is R2 and for instance if you do the difference between U&V you have this vector which is W. And if you remember what she did when you were in high school, it doesn't matter whether these WW is here or here or here.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the same vector you don't care about the precise location where it is.",
                    "label": 0
                },
                {
                    "sent": "If you do, the sum of U&V you have these long vector here, which is just obtained by adding you just moved along this vector and then you add this vector to the end of this vector.",
                    "label": 0
                },
                {
                    "sent": "So you do this.",
                    "label": 0
                },
                {
                    "sent": "Path and then you end up with these these long vector.",
                    "label": 0
                },
                {
                    "sent": "And if you divide it by two here, you have the middle of these two points.",
                    "label": 0
                },
                {
                    "sent": "OK. And the the last thing I don't even know if it's necessary to talk to you about that.",
                    "label": 0
                },
                {
                    "sent": "But let's talk to you about that.",
                    "label": 0
                },
                {
                    "sent": "If you have a scalar here Lambda, which is between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Then the resulting vector of the multiplication of Lambda by V is this vector, so it's a vector which is between, which is, which leaves be on this on this line.",
                    "label": 0
                },
                {
                    "sent": "And depending of the value of Lambda, it's closer to here, oh here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then in a product, so in our products, if you if you if you if you missed this slide.",
                    "label": 0
                },
                {
                    "sent": "If you don't understand this slide.",
                    "label": 0
                },
                {
                    "sent": "It's done for you.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to talk to you about for the remaining of this talk is just how you deal with.",
                    "label": 0
                },
                {
                    "sent": "Inner product or dot products?",
                    "label": 0
                },
                {
                    "sent": "Maybe a little bit more elaborate, elaborate dot products, but I'm just going to talk about Dot products so.",
                    "label": 0
                },
                {
                    "sent": "So again, this is something that you you learn in high school.",
                    "label": 0
                },
                {
                    "sent": "So this is something that is symmetric, which means that you you take the arguments.",
                    "label": 0
                },
                {
                    "sent": "You can switch them, you'd still have the same value, it's be linear.",
                    "label": 0
                },
                {
                    "sent": "OK, positive.",
                    "label": 0
                },
                {
                    "sent": "Meaning that if you take one vector and take the the inner product by in by himself, you will have value that is non negative and definite, meaning that if the scalar product of vector by himself is equal to 0 then it means that you is zero itself.",
                    "label": 0
                },
                {
                    "sent": "And So what does an inner product or a dot product else tell tell us about the space?",
                    "label": 0
                },
                {
                    "sent": "It provides a structure on the space where you live in.",
                    "label": 0
                },
                {
                    "sent": "Actually it's it provides a structure because if you do a little bit of topology that the main tool is distance.",
                    "label": 0
                },
                {
                    "sent": "An scalar product induces a distance, so there is a here.",
                    "label": 0
                },
                {
                    "sent": "You can see the norm and from this normally can you can derive a distance.",
                    "label": 0
                },
                {
                    "sent": "So this is already related.",
                    "label": 0
                },
                {
                    "sent": "So if you don't like the mathematical parts of of things, just think of.",
                    "label": 0
                },
                {
                    "sent": "Inner product as a similarity.",
                    "label": 1
                },
                {
                    "sent": "You have a similarity function and you're happy.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the first message that you have to take home today, so it's it's at the beginning of the talk, but still, it's very important.",
                    "label": 0
                },
                {
                    "sent": "Message message if you have a similarity function between the object, you're the objects that you are going to work on then you happy you can do a lot of things.",
                    "label": 0
                },
                {
                    "sent": "There is a slight thing that I don't say when I say that because there are some.",
                    "label": 0
                },
                {
                    "sent": "Harry date details on the similarities.",
                    "label": 0
                },
                {
                    "sent": "But OK, just just say that if you have a similarity, you're almost happy.",
                    "label": 0
                },
                {
                    "sent": "And the last thing that is very important is that if you want to compute the norm, that is the length of a vector, you just take the inner product of you and U and you take the square root and you give you the distance.",
                    "label": 0
                },
                {
                    "sent": "The norm of the vector.",
                    "label": 0
                },
                {
                    "sent": "So let's stay at the high school level.",
                    "label": 0
                },
                {
                    "sent": "In R2 you have one vector with two coordinates and another one, and the product the natural inner product is just the sum of the product between of the corresponding coordinates.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we're back with a little sketch here.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What is important?",
                    "label": 0
                },
                {
                    "sent": "What I want you to show is how the inner product is correlated to the fact that two vectors point to the same direction or not.",
                    "label": 1
                },
                {
                    "sent": "So here if you look at this vector.",
                    "label": 0
                },
                {
                    "sent": "You minus V this this vector or this vector and you look at E. Then you see that the.",
                    "label": 0
                },
                {
                    "sent": "Roughly point to the same direction.",
                    "label": 1
                },
                {
                    "sent": "To the upper left corner corner of this wall.",
                    "label": 0
                },
                {
                    "sent": "And when this is the case, then you have the inner product of those.",
                    "label": 0
                },
                {
                    "sent": "Those two guys which is positive.",
                    "label": 0
                },
                {
                    "sent": "On the other hand hand if you take.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you look at you minus V&G.",
                    "label": 0
                },
                {
                    "sent": "There pointing towards opposing opposite directions an in this situation the dot product is is negative.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if the two vectors are perpendicular or orthogonal, then the dot product is equal to 0.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's it's Jeff questions about inner product.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the first very very very simple classifier.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you have two sets of points, two sets of labeled points from each, from from, from which you're going to to learn a classifier.",
                    "label": 0
                },
                {
                    "sent": "So you have the blue points here.",
                    "label": 0
                },
                {
                    "sent": "The red one here.",
                    "label": 0
                },
                {
                    "sent": "Those are your training points.",
                    "label": 0
                },
                {
                    "sent": "Those are the points that you are going to use to construct your model.",
                    "label": 0
                },
                {
                    "sent": "And don't look at the details for now.",
                    "label": 0
                },
                {
                    "sent": "Just the very simple thing.",
                    "label": 0
                },
                {
                    "sent": "I say that I'm going to take the middle of the the red points here.",
                    "label": 0
                },
                {
                    "sent": "The middle of those.",
                    "label": 0
                },
                {
                    "sent": "The blue points here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to draw a line between those two middle points, and I'll say that I'm going to say that if a point is closer to C minus than C plus, then I will say that this point should be classified as either minus.",
                    "label": 0
                },
                {
                    "sent": "As it has a negative point.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if a point is closer to C plus than C minus, then I'll say that it should be classified as a plus spot, so this is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "There is nothing to do it and you'll see that the the.",
                    "label": 0
                },
                {
                    "sent": "Um mathematics is also a very simple so.",
                    "label": 0
                },
                {
                    "sent": "Here you have the.",
                    "label": 0
                },
                {
                    "sent": "Center of mass.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that should be something like that.",
                    "label": 0
                },
                {
                    "sent": "The center of Mass of the plus point, the positive points, and the center of mass mass of the negative points.",
                    "label": 0
                },
                {
                    "sent": "So you have C plus that is just the mean of all the exercise.",
                    "label": 0
                },
                {
                    "sent": "Here this C plus you have C minus which is the mean or the average points of all the minus points.",
                    "label": 0
                },
                {
                    "sent": "You have a the middle of C minus an C plus, which is just.",
                    "label": 0
                },
                {
                    "sent": "1/2 of C + + C minus.",
                    "label": 1
                },
                {
                    "sent": "OK, this is something that I told you about just before.",
                    "label": 0
                },
                {
                    "sent": "If you want to to compute the middle of two points, you just add the two vectors and you divide it by two and you have the middle of the points.",
                    "label": 0
                },
                {
                    "sent": "And the vector that I'm going to use to do classification is called W An.",
                    "label": 0
                },
                {
                    "sent": "I think I'm consistent with that.",
                    "label": 0
                },
                {
                    "sent": "I'll always call the classification vector W. Throughout the slides.",
                    "label": 0
                },
                {
                    "sent": "An answer, the decision function is very simple.",
                    "label": 1
                },
                {
                    "sent": "I said that I would classify points, test points X according to which of the two class means C plus or similar.",
                    "label": 1
                },
                {
                    "sent": "C minus is closer.",
                    "label": 0
                },
                {
                    "sent": "So it's very easy to do that.",
                    "label": 1
                },
                {
                    "sent": "You just take the inner product between.",
                    "label": 0
                },
                {
                    "sent": "W here.",
                    "label": 0
                },
                {
                    "sent": "And the vector X -- C. Again, if those two vectors point to the same direction, then you're going to say that.",
                    "label": 0
                },
                {
                    "sent": "That the inner product is positive and you're going to say that the point X is positive.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's just the other way round.",
                    "label": 0
                },
                {
                    "sent": "If the inner product is negative, then it means that X -- C and W .2 opposite directions and you're going to classify X as a minus point negative point.",
                    "label": 0
                },
                {
                    "sent": "And the classifier is just this very simple function here.",
                    "label": 0
                },
                {
                    "sent": "So it's based on this on this real value.",
                    "label": 0
                },
                {
                    "sent": "The function which is edge of X which is equal to W inner product with X -- C. And in order to predict the class, just take the sign of these these these inner product.",
                    "label": 0
                },
                {
                    "sent": "If the sign is positive.",
                    "label": 0
                },
                {
                    "sent": "If the if the inner product is positive, then you say that X should be classified as a positive example.",
                    "label": 0
                },
                {
                    "sent": "If the inner product is negative, then you say that the test point should be classified as as a negative negative point.",
                    "label": 0
                },
                {
                    "sent": "And the decision surface that is induced by this decision rule is just.",
                    "label": 0
                },
                {
                    "sent": "These hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "So remember that what what someone told you.",
                    "label": 0
                },
                {
                    "sent": "Tuesday.",
                    "label": 0
                },
                {
                    "sent": "She told you that hyperplane in a dimension D dimensional space is just a D -- 1 dimensional dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So here these are two dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Hyperplane is just a line.",
                    "label": 0
                },
                {
                    "sent": "In three dimensional space the hyperplane is.",
                    "label": 0
                },
                {
                    "sent": "A plane.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well I am.",
                    "label": 0
                },
                {
                    "sent": "Now we are we are going to go into the details of computing H of X again.",
                    "label": 0
                },
                {
                    "sent": "This is very easy.",
                    "label": 0
                },
                {
                    "sent": "Don't be freaked out just here.",
                    "label": 0
                },
                {
                    "sent": "If you compute edge of XI, say that this is a the inner product between W an X -- C. What I do here is that I replace all the quantity WC by all those, the quantity that quantities that you can find here.",
                    "label": 0
                },
                {
                    "sent": "And I use the linearity of the inner product.",
                    "label": 0
                },
                {
                    "sent": "I do the computation and I get this.",
                    "label": 0
                },
                {
                    "sent": "OK, let's let's let's go through through the computation so.",
                    "label": 0
                },
                {
                    "sent": "If you take the inner product between W and X -- C. Actually.",
                    "label": 0
                },
                {
                    "sent": "So if you take this, you just use the linearity of the inner product.",
                    "label": 0
                },
                {
                    "sent": "It gives you this.",
                    "label": 0
                },
                {
                    "sent": "Then W is just this guy, so you replace W by C + -- T minus and use the linearity of the inner product again and you get this an As for this part.",
                    "label": 0
                },
                {
                    "sent": "You just take again the definition of W. The definition of C and you have this guy.",
                    "label": 0
                },
                {
                    "sent": "I think that there is a missing 1/2 something somewhere here.",
                    "label": 0
                },
                {
                    "sent": "And then I I steal again.",
                    "label": 0
                },
                {
                    "sent": "I replace C plus by its definition on the top on the top of this slide, C minus by its definition, and I get this, I use again the linearity of the inner product and I get that.",
                    "label": 0
                },
                {
                    "sent": "HFX is computed as.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "I am the second very important message here.",
                    "label": 0
                },
                {
                    "sent": "Is that the decision function is just computed as something like that.",
                    "label": 0
                },
                {
                    "sent": "So yeah, first some of.",
                    "label": 0
                },
                {
                    "sent": "You know product between the training patterns and the test point.",
                    "label": 0
                },
                {
                    "sent": "And then a constant.",
                    "label": 0
                },
                {
                    "sent": "And that is all there is to it.",
                    "label": 0
                },
                {
                    "sent": "If you have a linear algorithm, usually you will end up with something like this.",
                    "label": 0
                },
                {
                    "sent": "It is very important to see that the only thing that you need to know is to compute.",
                    "label": 0
                },
                {
                    "sent": "You know how to compute the inner product between two pairs of points.",
                    "label": 0
                },
                {
                    "sent": "And once you know how to do that, you're good.",
                    "label": 0
                },
                {
                    "sent": "The the second thing that is very important to see that be here.",
                    "label": 0
                },
                {
                    "sent": "These are real.",
                    "label": 0
                },
                {
                    "sent": "This is color and it's very easy to compute.",
                    "label": 0
                },
                {
                    "sent": "Is just set C plus this.",
                    "label": 0
                },
                {
                    "sent": "This summer for interprofessional products.",
                    "label": 0
                },
                {
                    "sent": "And it just depends.",
                    "label": 0
                },
                {
                    "sent": "On the training data.",
                    "label": 0
                },
                {
                    "sent": "So if you can compute the inner product between the training data which you you know how to do.",
                    "label": 0
                },
                {
                    "sent": "Then you good because computer B is computing B is very easy an the rest of it is just easy so.",
                    "label": 0
                },
                {
                    "sent": "I intentionally used these, this writing here with a sum of Alpha I and dot products plus B.",
                    "label": 0
                },
                {
                    "sent": "Because this this is what is going to be a very crucial point of the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "And here you have very simple very.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Handy thing, which is that the Alpha eyes can be computed analytically.",
                    "label": 0
                },
                {
                    "sent": "This is the reason why I say that this is a very simple classifier here.",
                    "label": 0
                },
                {
                    "sent": "If I is a positive point, then Alpha I is.",
                    "label": 0
                },
                {
                    "sent": "1 / N plus.",
                    "label": 0
                },
                {
                    "sent": "If I is a negative point, that is, its label is minus one, then Alpha.",
                    "label": 0
                },
                {
                    "sent": "I is equal to minus 1 / N minus.",
                    "label": 0
                },
                {
                    "sent": "So this only this.",
                    "label": 0
                },
                {
                    "sent": "Here this here, sorry is a compact way to write this here with Alpha I defined as as this.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "So B is computed like that.",
                    "label": 0
                },
                {
                    "sent": "An so if you replace C by its definition, you do a little bit of of calculations.",
                    "label": 0
                },
                {
                    "sent": "Then you end up with this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, the very important thing is that.",
                    "label": 0
                },
                {
                    "sent": "Once you are able to compute Dot products, you're happy.",
                    "label": 1
                },
                {
                    "sent": "There's nothing more than than that because you know precisely how to compute Alpha I, and you know precisely how to compute B.",
                    "label": 0
                },
                {
                    "sent": "An in order to.",
                    "label": 0
                },
                {
                    "sent": "To anticipate on what I'm going to say for SVM's, for instance, for instance.",
                    "label": 0
                },
                {
                    "sent": "Villa.",
                    "label": 0
                },
                {
                    "sent": "Learning phases just intended to too.",
                    "label": 0
                },
                {
                    "sent": "Assign values to Alpha I and be here.",
                    "label": 0
                },
                {
                    "sent": "It's very simple here.",
                    "label": 0
                },
                {
                    "sent": "You can do it analytically.",
                    "label": 0
                },
                {
                    "sent": "You can program it, you can do it in just just five 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "You can do everything I told you about.",
                    "label": 0
                },
                {
                    "sent": "But for SVM's, this is what we're going to do in the practical and tomorrow's practical session.",
                    "label": 0
                },
                {
                    "sent": "We're going to program or.",
                    "label": 0
                },
                {
                    "sent": "And SVM.",
                    "label": 0
                },
                {
                    "sent": "So if you already done that then don't come, but otherwise you can come.",
                    "label": 0
                },
                {
                    "sent": "Of the question that is very important.",
                    "label": 0
                },
                {
                    "sent": "That and that we are going to.",
                    "label": 0
                },
                {
                    "sent": "To address throughout the this, this talk is.",
                    "label": 0
                },
                {
                    "sent": "What if the data sets at hand is not linearly separable, which is the case here.",
                    "label": 1
                },
                {
                    "sent": "You can try to take this line, put it here or here or here or whatever you want wherever you want.",
                    "label": 0
                },
                {
                    "sent": "You won't be able to separate the.",
                    "label": 0
                },
                {
                    "sent": "The train set training sets so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kermath methods just provide a principled way to address this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "With this kind of model.",
                    "label": 0
                },
                {
                    "sent": "And this one is very powerful on a kernel method.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "I'm sure a lot of.",
                    "label": 0
                },
                {
                    "sent": "You guys know about the kernel trick, but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the crooks.",
                    "label": 0
                },
                {
                    "sent": "So the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "Here it is in a very SIM.",
                    "label": 0
                },
                {
                    "sent": "Wait, so you have a nonlinearly separable data set S?",
                    "label": 1
                },
                {
                    "sent": "On one side, on the other side, you know that you have a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let's say not simple, but just a linear algorithm.",
                    "label": 1
                },
                {
                    "sent": "You have something that is capable to.",
                    "label": 0
                },
                {
                    "sent": "Computer hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So your focus is hyperplane.",
                    "label": 0
                },
                {
                    "sent": "You know how to compute hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Now now what you want to do is to use your your favorite algorithm to compute hyperplanes in a situation where hyperplanes are not sufficient to do classification.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind the kernel trick and the idea is a little bit reversed with the way I'm going to present it is a little bit reversed with what is actually meant by kernel trick, but it helps understand how how it works.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if you have a nonlinear nonlinear, a nonlinear problem is that you may want to try to map all your data.",
                    "label": 1
                },
                {
                    "sent": "Into another space.",
                    "label": 0
                },
                {
                    "sent": "Usually high dimensional space where the linear separation can be can be sought for.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "An so these these mapping from the the input space towards these special high dimensional space which we're going to call the feature space is usually denoted as file.",
                    "label": 0
                },
                {
                    "sent": "So this this guy here.",
                    "label": 0
                },
                {
                    "sent": "And what you want is that this this space H. You want it to be equipped with the dot product, because if it's equipped with the dot product then you can work in this space.",
                    "label": 0
                },
                {
                    "sent": "Try to find a linear separator in this space and then only work with the images of the training data by fine and not the the training data by themselves.",
                    "label": 0
                },
                {
                    "sent": "An I think that this picture summarize the idea is that you have these non linear separable problem.",
                    "label": 0
                },
                {
                    "sent": "Use mapping file.",
                    "label": 0
                },
                {
                    "sent": "That goes from X to some feature space H and here you have all the images of the right point in this space and the images of the blue points in this space and in this place you can do linear separation.",
                    "label": 0
                },
                {
                    "sent": "And again, the crooks is that you assume that there is an inner product in this space, because if there is an inner product, we just saw that it's possible to find a classifier.",
                    "label": 1
                },
                {
                    "sent": "It's it's just easy if you have a dot product in this space you work in to derive and to compute decision surface.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is the idea of the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "A kernel trick, but it's not precisely the idea I'll come to it in a bit.",
                    "label": 1
                },
                {
                    "sent": "So if I'm going back to the previous algorithm, the very simple linear algorithm, that means that.",
                    "label": 0
                },
                {
                    "sent": "Instead of having the DOT product between the excise and the X, then you will have the DOT products between the fire effects I.",
                    "label": 0
                },
                {
                    "sent": "And the fireworks.",
                    "label": 0
                },
                {
                    "sent": "So if you have your mapping fi and you can work with your mapping, then it's just very easy to compute a nonlinear classifier by using exactly the same algorithm as before, provided that you have.",
                    "label": 0
                },
                {
                    "sent": "And you know, product in the space age.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is actually the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The it's OK, this is just the step before the actual kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So the current trick can be applied.",
                    "label": 1
                },
                {
                    "sent": "So these are trick.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to take care.",
                    "label": 0
                },
                {
                    "sent": "I'm going to the.",
                    "label": 0
                },
                {
                    "sent": "Future.",
                    "label": 0
                },
                {
                    "sent": "So the kernel trick is just a very simple thing which say that if you have a linear algorithm that works with that product, then you do something very silly but very powerful.",
                    "label": 0
                },
                {
                    "sent": "Every time you see a dot product will you replace it by kernel function K?",
                    "label": 0
                },
                {
                    "sent": "Because if you have a kernel function K, then you know that it's using kernel function.",
                    "label": 0
                },
                {
                    "sent": "K is precisely like as if you were mapping all your data from an input space to a well design feature space.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Because what you're going to use is Mercer or Mercer, or positive definite kernels?",
                    "label": 0
                },
                {
                    "sent": "Positive definite kernels are our function taking two arguments, an out putting a value in R in the real set.",
                    "label": 0
                },
                {
                    "sent": "And if it's it's positive definite if computing computing K of UV is equivalent to 1st mapping U&V to some hidden space and then to compute the dot product between those those Maps instances.",
                    "label": 0
                },
                {
                    "sent": "And the key points actually on the way I'm going to.",
                    "label": 1
                },
                {
                    "sent": "I'm trying to to bring you to the kernel kernel, tricky that the emphasis is usually when you want to use the kernel trick and the idea of mapping.",
                    "label": 0
                },
                {
                    "sent": "Actually the the emphasis in the.",
                    "label": 0
                },
                {
                    "sent": "The very important thing is not to find the file transformation.",
                    "label": 0
                },
                {
                    "sent": "The file transform define mapping, but actually it's more to find the kernel K. Because again, if you have a kernel K, then it means a kernel positive kernel positive definite kernel K. It means that somewhere implicitly you have.",
                    "label": 0
                },
                {
                    "sent": "Am mapping five that comes together.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll talk about having a bit more about that after that.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "Kernels must verify mercers property.",
                    "label": 1
                },
                {
                    "sent": "I'll come back to this later on so, but the what is important here.",
                    "label": 0
                },
                {
                    "sent": "The technical details on the Mercers property are not very important, but it's just important to know that if you have Mercer kernel then it means that there exists a space H and a map and mapping files such that when you evaluate the value of K of UV then it's exactly as if you were doing the mapping.",
                    "label": 1
                },
                {
                    "sent": "Of you to fire few and V to 5V and you were checking the taking the inner product between fire few and 5V.",
                    "label": 1
                },
                {
                    "sent": "Meaning that again.",
                    "label": 0
                },
                {
                    "sent": "All you have to do is to find a good kernel, a kernel that is a Mercer kernel.",
                    "label": 0
                },
                {
                    "sent": "You don't have to focus on on on on finding a good mapping file, but you have to find a good kernel K. OK, so this is slight remark.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you you have people that worked on non valid non Mercer kernels to to do classification.",
                    "label": 1
                },
                {
                    "sent": "It's possible actually it's it's totally possible, but they did it with the.",
                    "label": 0
                },
                {
                    "sent": "Algorithms that are designed to work with the Mercer Kernels.",
                    "label": 0
                },
                {
                    "sent": "So which is a bit weird, but it's OK, it works.",
                    "label": 0
                },
                {
                    "sent": "And again, for those of you who don't care about Mercer or a positive definite kernel, just think of K as a similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a kernel trick recipe.",
                    "label": 1
                },
                {
                    "sent": "First, you can see the the sketch and the drawing on the upper part of the slide.",
                    "label": 1
                },
                {
                    "sent": "But the kernel trick recipe is very simple.",
                    "label": 0
                },
                {
                    "sent": "First you choose your linear classification.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm.",
                    "label": 0
                },
                {
                    "sent": "You have everyone that is expressed in terms of DOT products, just as the first algorithm that I presented you at the beginning of the talk.",
                    "label": 0
                },
                {
                    "sent": "An you do a very very, very simple thing and this is a kernel trick.",
                    "label": 0
                },
                {
                    "sent": "You replace Everything Everywhere, every every.",
                    "label": 0
                },
                {
                    "sent": "Every instance is every instance of.",
                    "label": 0
                },
                {
                    "sent": "The usual inner product you replace it by K. You don't even think about what you're doing.",
                    "label": 0
                },
                {
                    "sent": "You say, OK, I have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "I see that there there is a dot product I don't care about the dot product, I replace it by KK of if I see inner product between U&VI replace it by K or few and V and that's it.",
                    "label": 0
                },
                {
                    "sent": "And that's your kernel trick.",
                    "label": 0
                },
                {
                    "sent": "And the magic behind that is that if you have chosen.",
                    "label": 0
                },
                {
                    "sent": "A kernel that is a positive definite kernels, but the positive definite kernel then.",
                    "label": 0
                },
                {
                    "sent": "You are in insured that there exists.",
                    "label": 0
                },
                {
                    "sent": "A mapping Phi that is a society with the with the K. Search that computing K of XIX is just like computing the inner product between 5XI and 5X.",
                    "label": 0
                },
                {
                    "sent": "So This is why it's it's very simple.",
                    "label": 0
                },
                {
                    "sent": "The kernel trick is a very simple thing.",
                    "label": 0
                },
                {
                    "sent": "It's it's obvious, obviously restaurants very strong mathematical tools, but it's if you just want to use it if you want to construct your own kernel method, which is a little late because there are a lot of things that have been done in this area, but still you can try to to find a new kernel kernel.",
                    "label": 0
                },
                {
                    "sent": "I don't know kernelized whatever you want.",
                    "label": 0
                },
                {
                    "sent": "Then the idea is is that you just have to find that product first.",
                    "label": 0
                },
                {
                    "sent": "You have to express everything in terms of dot products.",
                    "label": 1
                },
                {
                    "sent": "Once you're done with that, then you know that you can do.",
                    "label": 0
                },
                {
                    "sent": "You can kernelized everything.",
                    "label": 0
                },
                {
                    "sent": "And the obtained classifier now is just expressed as a sum of Alpha YK of XI X + B.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going back to the very simple classifier.",
                    "label": 1
                },
                {
                    "sent": "I talked about the very beginning.",
                    "label": 0
                },
                {
                    "sent": "I remember that we have the.",
                    "label": 0
                },
                {
                    "sent": "Red Point we have the blue point.",
                    "label": 0
                },
                {
                    "sent": "I think I take the the the mean of the red points, the mean of the blue points are drawn line that passes through the middle of those two points and I say that all the points that fall on one side are plus point are a positive point and the others are negative points.",
                    "label": 0
                },
                {
                    "sent": "And what we had before is that we knew that instead of this we had the classical inner product.",
                    "label": 0
                },
                {
                    "sent": "And now I don't change anything except that I did my my my little trick with the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "I replaced the inner product that used to appear here by AK of Zyex and the value of Alpha I is unchanged is still the same value as before, and the values of B is a little bit changed because if you remember these expressed us the difference of the square norm of C minus and C plus.",
                    "label": 0
                },
                {
                    "sent": "And if you you do the computation, you see that actually the for instance, the square norm of C minus is this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if you want to see how it works.",
                    "label": 0
                },
                {
                    "sent": "You have C minus that is equal to.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition of C minus.",
                    "label": 0
                },
                {
                    "sent": "If you take the norm.",
                    "label": 0
                },
                {
                    "sent": "The square norm of this is just.",
                    "label": 0
                },
                {
                    "sent": "This an so you use the linearity and everything that you know about the inner product, so it's.",
                    "label": 0
                },
                {
                    "sent": "And here you're happy because you have dot products.",
                    "label": 0
                },
                {
                    "sent": "And you say OK instead of that.",
                    "label": 0
                },
                {
                    "sent": "I use.",
                    "label": 0
                },
                {
                    "sent": "That, and that's it.",
                    "label": 0
                },
                {
                    "sent": "So this is how I get this and this is how I get that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't specify any any particular kernel an I'm going to to show you.",
                    "label": 0
                },
                {
                    "sent": "Classical kernels that are usually there used are that are well known and.",
                    "label": 0
                },
                {
                    "sent": "That are used when you work with the vectors.",
                    "label": 0
                },
                {
                    "sent": "First, the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Usually when you try to do.",
                    "label": 0
                },
                {
                    "sent": "Colonel when you first meet with the kernel methods and you want to try you first kernel algorithm, what you're going to use is the Gaussian kernel becausw because it works, there is a.",
                    "label": 0
                },
                {
                    "sent": "It's a parameterized kernel because you have this Sigma here.",
                    "label": 0
                },
                {
                    "sent": "Here they need be.",
                    "label": 0
                },
                {
                    "sent": "Parameterized properly, but usually.",
                    "label": 0
                },
                {
                    "sent": "You can manage to find a good value and you happy with your first kernel kernel method.",
                    "label": 0
                },
                {
                    "sent": "The corresponding and hidden implicit H is of infinite dimension.",
                    "label": 1
                },
                {
                    "sent": "Again, I'll come back on this a little bit later on.",
                    "label": 0
                },
                {
                    "sent": "Actually, at the end of the talk when I'll talk about more theoretical things and more theoretically grounded things.",
                    "label": 0
                },
                {
                    "sent": "The polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "It's very simple, it's just the usual dot product between U&V plus constant and to the power of D. So you can see that the usual inner product which correspond to the situation where C is equal to 0 and is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So the usual inner product is just the kernel as well.",
                    "label": 0
                },
                {
                    "sent": "And another kernel that has been used with the support vector machines even though it's not, it's not necessarily a Mercer kernel that is.",
                    "label": 0
                },
                {
                    "sent": "You cannot say what kind of mapping is associated with.",
                    "label": 0
                },
                {
                    "sent": "With exists is goes with.",
                    "label": 0
                },
                {
                    "sent": "With this kernel, is that the the touch?",
                    "label": 0
                },
                {
                    "sent": "So the it's not the tangent kernel but is the hyper hyperbolic tangent kernels.",
                    "label": 0
                },
                {
                    "sent": "So it's in that you can.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I've never worked with it, but I know that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would have so, um.",
                    "label": 0
                },
                {
                    "sent": "What I want to want want to show you here is is how we can for for some examples of kernel kernels, how you can identify the mapping that goes with the kernel that you choose.",
                    "label": 0
                },
                {
                    "sent": "For instance, you take the polynomial kernel K defined here.",
                    "label": 1
                },
                {
                    "sent": "So this just the square of the inner product is not very, very it's.",
                    "label": 0
                },
                {
                    "sent": "It's not very elaborate, but it's it's a kernel, so you take the square.",
                    "label": 0
                },
                {
                    "sent": "And this corresponds to the situation where C is equal to 0 and is equal to 2 two in the previous definition.",
                    "label": 0
                },
                {
                    "sent": "And you can, you can identify mapping that works, that is a mapping search that when you take the kernel between U&V then you have that KFU&V is equal to five of to the dot product between 5:00 you and fire V. So the the function Phi the mapping file that as I proposed to to look at is this one.",
                    "label": 0
                },
                {
                    "sent": "You take X that is made of two coordinates.",
                    "label": 0
                },
                {
                    "sent": "It's made of X1F and X2 and you build 3 dimensional vector that is.",
                    "label": 0
                },
                {
                    "sent": "X1 square sqrt 2 X 1 * 6 Two and X2 square.",
                    "label": 0
                },
                {
                    "sent": "These are three dimensional vector an you see that if you take the inner product between Fire view and Fire V. This is just the inner product of this 3 dimensional vector with this 3 dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "Then you see that it's equal to.",
                    "label": 0
                },
                {
                    "sent": "OK, it's maybe a bit quick, but if you do the math and you compute everything, you'll see that this is just.",
                    "label": 0
                },
                {
                    "sent": "U1V1 plus U2V2 to the square squared.",
                    "label": 0
                },
                {
                    "sent": "Which is exactly the inner product between U&V Square and is the kernel that you you started with.",
                    "label": 0
                },
                {
                    "sent": "Meaning that you can.",
                    "label": 0
                },
                {
                    "sent": "You can see it like as if I'm trying to say that if you use this kernel, then it's precisely as if you were trying to use.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In formation of degree 2 hour if you want to use.",
                    "label": 0
                },
                {
                    "sent": "Quadratic information about your coordinates, so it's very easy.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you use this kernel to derive with to derive using a linear algorithm to derive.",
                    "label": 0
                },
                {
                    "sent": "Decisiones surface that are like... high probability hyper.",
                    "label": 0
                },
                {
                    "sent": "Hyper Bulls, I don't know if it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it shouldn't work.",
                    "label": 0
                },
                {
                    "sent": "Anne and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we're back.",
                    "label": 0
                },
                {
                    "sent": "We've very simple kernelized classifier.",
                    "label": 1
                },
                {
                    "sent": "So the one with the average plus points and the average negative points.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "Again, we have the value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "We have the value of B an.",
                    "label": 0
                },
                {
                    "sent": "I use.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "So when you use a Gaussian kernel, you have this parameter Sigma that you have to deal with.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to tell you how to deal with that this guy because it's it's a pain actually.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to show you what kind of division surface you can have using an RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "This very simple classifier that I showed you on a nonlinear class.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Location problem.",
                    "label": 0
                },
                {
                    "sent": "So here is the training data.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the blue points that are a positive points the red one that are negative points and as you can see there is no no line, no hyperplane that could could separate blue points from red points.",
                    "label": 0
                },
                {
                    "sent": "And I use the very simple classifier that I showed you with the Gaussian with a Gaussian kernel of different with the Sigma parameters is called the width of the other kernel.",
                    "label": 0
                },
                {
                    "sent": "Then I use Sigma is equal to 0.05, zero point 1, zero point 2.",
                    "label": 0
                },
                {
                    "sent": "An 4 here is 0.05.",
                    "label": 0
                },
                {
                    "sent": "Here is zero point.",
                    "label": 0
                },
                {
                    "sent": "One here is 0.2.",
                    "label": 0
                },
                {
                    "sent": "And what those pictures show is the value of H given the location where it's computed.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you have to remember and to see that.",
                    "label": 0
                },
                {
                    "sent": "The red corresponds to the blue OK, here the red here.",
                    "label": 1
                },
                {
                    "sent": "Is like the blue here.",
                    "label": 0
                },
                {
                    "sent": "Anne the yellow here.",
                    "label": 0
                },
                {
                    "sent": "Or even the blue here is like the red here.",
                    "label": 0
                },
                {
                    "sent": "OK. Nice choice.",
                    "label": 0
                },
                {
                    "sent": "I didn't realize that.",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                },
                {
                    "sent": "So the only thing that you have to remember is that I used a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Where I can analytically compute the Alpha eyes?",
                    "label": 0
                },
                {
                    "sent": "I can analytically compute B.",
                    "label": 0
                },
                {
                    "sent": "B the value of B.",
                    "label": 0
                },
                {
                    "sent": "Am I just plug a Gaussian kernel into this very simple algorithm?",
                    "label": 0
                },
                {
                    "sent": "I run it with this non linearly separable training data and I get I get those nonlinear decision surface because you can imagine that the the.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can imagine but here.",
                    "label": 0
                },
                {
                    "sent": "There is a.",
                    "label": 0
                },
                {
                    "sent": "The border between the red and the yellow.",
                    "label": 0
                },
                {
                    "sent": "This is the location where you you're going to do.",
                    "label": 0
                },
                {
                    "sent": "Say everything that is in this part is going to be.",
                    "label": 0
                },
                {
                    "sent": "Positive and everything that is going to be that is in this part is going to be negative.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again, we used a very simple linear algorithm we used, we just picked.",
                    "label": 0
                },
                {
                    "sent": "A kernel that is well known.",
                    "label": 0
                },
                {
                    "sent": "We picked the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "We used it.",
                    "label": 0
                },
                {
                    "sent": "And we switched the colors an we had these results.",
                    "label": 0
                },
                {
                    "sent": "It's nice.",
                    "label": 0
                },
                {
                    "sent": "They should be very happy now.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "Yes, nice.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "A small thing about kernels.",
                    "label": 0
                },
                {
                    "sent": "This is a glimpse of what is going to happen to you in in 2 hours.",
                    "label": 0
                },
                {
                    "sent": "Or never because I think that I'm a little behind on my schedule, but.",
                    "label": 0
                },
                {
                    "sent": "Here Instagram matrix of a kernel.",
                    "label": 0
                },
                {
                    "sent": "This is given an input set S made of data X1 to XN, the gram the gram matrix of X of K4S is just the matrix made of all the.",
                    "label": 0
                },
                {
                    "sent": "The DOT products between XI XJ, XJ, so this is obviously a symmetric matrix because K of XIXJ is equal to KFXJ XI an this there is a very nice property of.",
                    "label": 0
                },
                {
                    "sent": "Offer positive definite kernels is that?",
                    "label": 0
                },
                {
                    "sent": "Whatever the the training set or the set S that you choose, you always have this property that.",
                    "label": 0
                },
                {
                    "sent": "There is this property here.",
                    "label": 0
                },
                {
                    "sent": "So you take any set S of X1 to XN.",
                    "label": 0
                },
                {
                    "sent": "You compute the gram, the gram matrix of K with respect to S. You'll see that.",
                    "label": 1
                },
                {
                    "sent": "You will have this property.",
                    "label": 0
                },
                {
                    "sent": "In other words, the eigenvalues of gram matrix are all non negative.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if you construct your own kernel and you want to show that this is a valid kernel to do that is.",
                    "label": 0
                },
                {
                    "sent": "Everything is valid.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you want to show that this is a positive definite kernel or if you want to, or in other words you want to see it to show that it fulfills Mercer's property, you just have to show that.",
                    "label": 0
                },
                {
                    "sent": "It give rise to gram matrices that have non negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "It's not always the either easier way to prove that the kernel is a is a Mercer kernel, but these away.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this entails a few results that can be used if you want to construct kernels from kernels.",
                    "label": 0
                },
                {
                    "sent": "If you take the power of a kernel, imagine that K1 and K2 are two Mercer kernels then.",
                    "label": 1
                },
                {
                    "sent": "K1 to the PTH is also a Mercer kernel and if you take the positive combination of two, kernel is still a Mercer kernel.",
                    "label": 0
                },
                {
                    "sent": "If you take the the if you take K 1 * K Two it's also Mercer kernel.",
                    "label": 0
                },
                {
                    "sent": "So this is usually an exercise that I give when I teach.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods and this is usually.",
                    "label": 0
                },
                {
                    "sent": "An exercise that nobody is able to do.",
                    "label": 0
                },
                {
                    "sent": "So we're not going to do.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a partial conclusion.",
                    "label": 1
                },
                {
                    "sent": "So the kernel trick does there is.",
                    "label": 0
                },
                {
                    "sent": "This is the first thing that you have to remember.",
                    "label": 0
                },
                {
                    "sent": "Is the kernel trick you pick your favorite linear classifier and kernelized it?",
                    "label": 1
                },
                {
                    "sent": "And you you end up with a nonlinear classifier.",
                    "label": 0
                },
                {
                    "sent": "Again, kernel Ising linear algorithm just consisting replacing all dot products by kernels, and you're done.",
                    "label": 0
                },
                {
                    "sent": "Then we talked a bit about the Mercer's condition.",
                    "label": 1
                },
                {
                    "sent": "Mercer's condition is is the thing on the positive, non negative eigenvalues and it's I'd like to stress out that it means that if you have a kernel that fulfills Mercer's condition that means that it's there exists a mapping file such that when you compute K of UV it's equal to the dot product between 5:00 or few and 5V.",
                    "label": 0
                },
                {
                    "sent": "There are a few existing kernels that I showed you.",
                    "label": 0
                },
                {
                    "sent": "Actually the again one that is the two that are very popular are the RBF kernels and the polynomial kernels.",
                    "label": 1
                },
                {
                    "sent": "Uh, an very important thing.",
                    "label": 0
                },
                {
                    "sent": "That I'm going to talk about more later is that.",
                    "label": 0
                },
                {
                    "sent": "Doing classification with structured data such as trees, graphs, sequences.",
                    "label": 0
                },
                {
                    "sent": "Is made possible by the idea of kernels and current methods, meaning that.",
                    "label": 0
                },
                {
                    "sent": "The only the only thing that you have to worry about is to define a proper kernel.",
                    "label": 0
                },
                {
                    "sent": "If you want to classify graphs, you just have to define Mercer, kernel and graphs and you can use this very silly dumb algorithm that I showed you and you can do classification on graphs.",
                    "label": 0
                },
                {
                    "sent": "If you want to do classification on sequences, you just have to derive a new kernel sequences and then use any linear algorithm that you know.",
                    "label": 0
                },
                {
                    "sent": "For instance, the perceptron algorithm that young showed you Monday, and you can use it.",
                    "label": 0
                },
                {
                    "sent": "You can kernel eyes it and you will end up with a nonlinear with a classifier for sequences.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The the important thing that is that with the kernel method trying to tackle problems where that involve structured data is is a Mount to the problem of finding good kernels.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "Exist?",
                    "label": 0
                },
                {
                    "sent": "There is no no no, no need to worry about algorithms.",
                    "label": 0
                },
                {
                    "sent": "The questions that go with the with the kernel trick is an.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to answer this because I don't exactly know how to to answer it is how you choose the right kernel.",
                    "label": 0
                },
                {
                    "sent": "Because yeah, it's difficult.",
                    "label": 0
                },
                {
                    "sent": "How you can combine kernels so there are a lot of works that address the problem for combining kernels.",
                    "label": 0
                },
                {
                    "sent": "Usually the combination of kernel is made as a convex combination.",
                    "label": 0
                },
                {
                    "sent": "That is, it's almost like the combination I talked to you about that is this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This combination here.",
                    "label": 0
                },
                {
                    "sent": "With the with the convex constraints on the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The coefficients.",
                    "label": 0
                },
                {
                    "sent": "Then there's a question of building kernels for a structured data.",
                    "label": 1
                },
                {
                    "sent": "And there's another question I don't know if it's a question or something that I didn't say much about his.",
                    "label": 0
                },
                {
                    "sent": "What kind of algorithm that you can use, or you can use as a kernel algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here here's a list of very of some algorithm to it's Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "The Kernel, Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you know what is regression, it's just least square regression where you want to to predict real value from X.",
                    "label": 0
                },
                {
                    "sent": "From from some input data X an you use a square.",
                    "label": 0
                },
                {
                    "sent": "At least square methods and you do.",
                    "label": 0
                },
                {
                    "sent": "You had you had some regularization and it gives you a Ridge regression and you can kernelized this.",
                    "label": 0
                },
                {
                    "sent": "You can do very closely related.",
                    "label": 1
                },
                {
                    "sent": "Algorithm which is a kernel Fisher discriminant.",
                    "label": 1
                },
                {
                    "sent": "You can do a kernel PCA, so it's a principle component analysis.",
                    "label": 1
                },
                {
                    "sent": "It's an unsupervised method that you can use an you can do SVM's.",
                    "label": 0
                },
                {
                    "sent": "There is support vector machines and this is the topic of the next.",
                    "label": 0
                },
                {
                    "sent": "The next 15 minutes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Are we good?",
                    "label": 0
                },
                {
                    "sent": "OK, so so Marie asked me, asks me if if if you have some kind of knowledge.",
                    "label": 0
                },
                {
                    "sent": "As how the chosen kernel is going to to make your data separable or not.",
                    "label": 0
                },
                {
                    "sent": "An you cannot say, except that if you work in a specific domains, usually if you know that the the kernel that you derive is is.",
                    "label": 0
                },
                {
                    "sent": "Sensible makes sense for for the experts of the domain then.",
                    "label": 0
                },
                {
                    "sent": "You have good chance it's it's more probable that you can have separation, but otherwise you cannot say that's why I say that the choosing the right kernel is.",
                    "label": 0
                },
                {
                    "sent": "It's a very difficult and you cannot even know whether you go to the good direction or you choose a variable.",
                    "label": 0
                },
                {
                    "sent": "Totally wrong kernel.",
                    "label": 0
                },
                {
                    "sent": "So there is a new answer to this.",
                    "label": 0
                },
                {
                    "sent": "Wait, I think I heard that there are like theories and said that to who are you?",
                    "label": 0
                },
                {
                    "sent": "Finally you are very weird.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Can you find a division problem your way?",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "I think everybody heard what you say that.",
                    "label": 0
                },
                {
                    "sent": "I'll say it again.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you you take points in very high dimension then if it's the dimension is very high you can always find a hyperplane that separates all the data, but then there is not another thing that enters into account is the richness or the capacity of the class of functions that you consider because what you say is that of course if you take a very very, very rich feature space and then you can separate everything.",
                    "label": 0
                },
                {
                    "sent": "But the thing that you have to pay for that.",
                    "label": 0
                },
                {
                    "sent": "If you are not careful is that you will have very poor generalization capacity of your model.",
                    "label": 0
                },
                {
                    "sent": "That is, you're going to be able to separate all of your training data perfectly, but then you will not be able to say anything about new test point.",
                    "label": 0
                },
                {
                    "sent": "So but right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk a bit bit about regularization Ann here for the SVM and then in the in the last part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Some in kind of team picture from which we can know that we're choosing right, not right, whether it's better, what?",
                    "label": 0
                },
                {
                    "sent": "How to know that?",
                    "label": 0
                },
                {
                    "sent": "OK. What I sense in your questions, not not yours particularly is that.",
                    "label": 0
                },
                {
                    "sent": "But all your questions that.",
                    "label": 0
                },
                {
                    "sent": "Your questions is are more related to some theoretical guarantees that say OK, if I choose a kernel, how can I be ensure that this kernel is going to help me classify the data an?",
                    "label": 0
                },
                {
                    "sent": "From what I understand from your question, there is some kind of theoretical thing.",
                    "label": 0
                },
                {
                    "sent": "Hidden here an I'm going to answer on the non theoretical thing.",
                    "label": 0
                },
                {
                    "sent": "The thing is usually if you want to measure if you if you want to say that your kernel is better than another kernel or is well suited to the problem that you use, you just do some cross validation thing and you estimate the generalization property.",
                    "label": 0
                },
                {
                    "sent": "You cannot say OK, let me look at the data and I know that that this kind of kernel is going to be OK or not.",
                    "label": 0
                },
                {
                    "sent": "You can do it sometimes.",
                    "label": 0
                },
                {
                    "sent": "For instance if you work with images or sounds, then you can use wavelet kernels and things, but otherwise it's very difficult to know that.",
                    "label": 0
                },
                {
                    "sent": "An and designing good kernels.",
                    "label": 0
                },
                {
                    "sent": "Very effective kernels is really difficult, and even if you have good kernels, sometimes you have small parameters.",
                    "label": 0
                },
                {
                    "sent": "You want to say that these parameters don't count, and it's very easy to to to to choose the right values for the parameters, but it's not sure if you really want to tackle specific application.",
                    "label": 0
                },
                {
                    "sent": "If you want to have very good results and then you want to use a kernel classifier, then at some point you have to find the good Colonel, the good, the good parameters, and so on.",
                    "label": 0
                },
                {
                    "sent": "And it's just.",
                    "label": 0
                },
                {
                    "sent": "It's it's it's very difficult.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about SVM.",
                    "label": 0
                },
                {
                    "sent": "So this is something that.",
                    "label": 0
                },
                {
                    "sent": "Came out in the beginning of the 90s.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It was a spur by the work, the theoretical work of Vatnik on statistical learning.",
                    "label": 0
                },
                {
                    "sent": "And this is something that.",
                    "label": 0
                },
                {
                    "sent": "So that came out in 90 two and was refined a little bit in a 95 an.",
                    "label": 0
                },
                {
                    "sent": "I think that in some ways it changed a lot of things in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Not that I was doing machine learning before that, but it changed a lot of things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, I'll come back to this later.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Why was it a breakthrough in machine learning?",
                    "label": 1
                },
                {
                    "sent": "Because of the following.",
                    "label": 0
                },
                {
                    "sent": "Four points.",
                    "label": 0
                },
                {
                    "sent": "First, OK, before going to the four points, look at those two pictures.",
                    "label": 0
                },
                {
                    "sent": "These were we are.",
                    "label": 0
                },
                {
                    "sent": "We are heading out.",
                    "label": 0
                },
                {
                    "sent": "Here you have black points.",
                    "label": 0
                },
                {
                    "sent": "White points have the same black points.",
                    "label": 0
                },
                {
                    "sent": "We have the same white points and you want to find a linear classifier that puts that puts all the white points on one side and all the black points on the other side.",
                    "label": 0
                },
                {
                    "sent": "An you have here two different classifiers, two different hyperplanes that do that do so.",
                    "label": 0
                },
                {
                    "sent": "The first one here.",
                    "label": 0
                },
                {
                    "sent": "Is OK, you have all the white points on one side on one side and all the black points on the other side.",
                    "label": 0
                },
                {
                    "sent": "And you have this classifier here where you have also all the white points on the one side on one side and all the black points on the other side.",
                    "label": 0
                },
                {
                    "sent": "So I stop just a little bit and I'm trying.",
                    "label": 0
                },
                {
                    "sent": "Yesterday I was looking at the at the picture and I lost something very important is that.",
                    "label": 0
                },
                {
                    "sent": "The white points and the black points are look localized at the same position on the left and on the right, even though this is quite disturbing, having the hyperplane is quite disturbing.",
                    "label": 0
                },
                {
                    "sent": "If you can do a mental thing to withdraw and to remove the hyperplane, you'll see that.",
                    "label": 0
                },
                {
                    "sent": "The white points and the black points are exactly the same on the under under two under 2.",
                    "label": 0
                },
                {
                    "sent": "On the two graph, no.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "I tell you.",
                    "label": 0
                },
                {
                    "sent": "Because I did just a copy pasta copy paste.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's true it's.",
                    "label": 0
                },
                {
                    "sent": "OK, there is a missing 1.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe yes, no?",
                    "label": 0
                },
                {
                    "sent": "This one we don't care about this guy.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "OK, doesn't doesn't count.",
                    "label": 0
                },
                {
                    "sent": "No, it's OK.",
                    "label": 0
                },
                {
                    "sent": "It's OK, it works.",
                    "label": 0
                },
                {
                    "sent": "If it had been this this point or something very close to their hyperplane, I would have been sad, but it's OK, I don't care so.",
                    "label": 0
                },
                {
                    "sent": "OK, they're almost the same.",
                    "label": 0
                },
                {
                    "sent": "But trust me, for the following dungeon.",
                    "label": 0
                },
                {
                    "sent": "Just trust me just what I say, not what you see.",
                    "label": 0
                },
                {
                    "sent": "So why is it very important?",
                    "label": 0
                },
                {
                    "sent": "Why was it a breakthrough?",
                    "label": 0
                },
                {
                    "sent": "Because of of?",
                    "label": 0
                },
                {
                    "sent": "Among others, the four points that I mentioned here.",
                    "label": 0
                },
                {
                    "sent": "First it's use.",
                    "label": 0
                },
                {
                    "sent": "It was one of the very first algorithm that used kernels and all the kernel kernel machinery actually, kernels were known in the 70s and things like that, but there were no algorithm to properly wear algorithm, not nothing, but not as efficient as effective as VM's.",
                    "label": 0
                },
                {
                    "sent": "It's rests on the idea of margin that is.",
                    "label": 0
                },
                {
                    "sent": "Between those two hyperplanes, usually people prefer this one.",
                    "label": 0
                },
                {
                    "sent": "I don't ask you if you prefer this one because.",
                    "label": 0
                },
                {
                    "sent": "Because I'm afraid of you, but this one is better.",
                    "label": 0
                },
                {
                    "sent": "Just because the distance between the closest point.",
                    "label": 0
                },
                {
                    "sent": "The closest white or black point to the hyperplane is larger here than here, meaning that you have more security in classifying your data because.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to say is related to the statistical learning theory thing is that your assumption when when you're doing statistical learning is that all your data are.",
                    "label": 0
                },
                {
                    "sent": "In the independently and identically distributed data.",
                    "label": 0
                },
                {
                    "sent": "Meaning that this is something that is supposed to show you how the data are in the space.",
                    "label": 0
                },
                {
                    "sent": "An if you see.",
                    "label": 0
                },
                {
                    "sent": "This plot and this plot you may imagine that it's not.",
                    "label": 0
                },
                {
                    "sent": "It's very probable to see like.",
                    "label": 0
                },
                {
                    "sent": "GNU GNU test points new Black point following here, here, here, here, here, here or here, and you white points falling here or here or here or here an if you put more security and larger margin here then what you ensure that you don't insure it.",
                    "label": 0
                },
                {
                    "sent": "With hyper you enter it with a high probability.",
                    "label": 0
                },
                {
                    "sent": "You do ensure it with a high probability that it's it's.",
                    "label": 0
                },
                {
                    "sent": "Very like unlikely that new points coming from the same distribution will fall on the wrong side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "If you take a large margin, the larger the margin, the larger the security, the smaller the probability that a new point will fall on the wrong side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the points that is very important with VM's is that it came along with the idea of kernels.",
                    "label": 0
                },
                {
                    "sent": "The idea of margin.",
                    "label": 0
                },
                {
                    "sent": "The very strong theoretical background.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "A convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Meaning that someone talked to you about convex optimization just a little bit on Tuesday.",
                    "label": 0
                },
                {
                    "sent": "And this is something that is very very, very nice, meaning that at some point in the the way you're going to model the classification problem with the supervector machines, you will have to minimize something.",
                    "label": 0
                },
                {
                    "sent": "And the thing that you're going to minimize is is convex.",
                    "label": 0
                },
                {
                    "sent": "Functional with convex constraints, meaning that the problem that you have is a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about convex optimization problem is that you have global minimum.",
                    "label": 0
                },
                {
                    "sent": "It's not like with.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you know if some of you have already worked with the.",
                    "label": 0
                },
                {
                    "sent": "Multilayer perceptions, then you minimize something and you don't exactly know how and where you're going to end up it really.",
                    "label": 0
                },
                {
                    "sent": "It really depends on on how and where it started when you you started your optimization process.",
                    "label": 0
                },
                {
                    "sent": "Here with with the SVM's you just have a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "To solve an.",
                    "label": 1
                },
                {
                    "sent": "Another thing is that it's a quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "A quadratic programming is a very well known field of research for people doing optimizations, so there exists a lot of different implementations of.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Offer of code doing optimization and Ann.",
                    "label": 0
                },
                {
                    "sent": "It's it's a very well known field, so you have all those four points coming together and it was was actually, I think the first time that all those .4 points were coming.",
                    "label": 0
                },
                {
                    "sent": "Not exactly together, but almost together.",
                    "label": 0
                },
                {
                    "sent": "And it's very important.",
                    "label": 0
                },
                {
                    "sent": "This is the reason why.",
                    "label": 0
                },
                {
                    "sent": "It adds it gained a lot of attention, and there's another thing that I didn't mention is that.",
                    "label": 0
                },
                {
                    "sent": "They also used as VM, so this is a practical practical point that I didn't mention here.",
                    "label": 0
                },
                {
                    "sent": "They used supervector machines to do handwritten digits classification, and they were capable of reaching the same accuracies as people working on neural networks and tuning the neural networks.",
                    "label": 0
                },
                {
                    "sent": "And it was.",
                    "label": 0
                },
                {
                    "sent": "I mean there is the practical thing that also is an asset of DMS.",
                    "label": 0
                },
                {
                    "sent": "It's just not just something that is nice, easy to optimize, and that has a strong theoretical guarantees, but it also works in practice.",
                    "label": 0
                },
                {
                    "sent": "So you have everything in one thing.",
                    "label": 0
                },
                {
                    "sent": "Almost everything except that you have.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To choose the right kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to start with the basics, which is the hard margin linear SVM.",
                    "label": 1
                },
                {
                    "sent": "So the setting that we consider is.",
                    "label": 1
                },
                {
                    "sent": "We have an input space SpaceX with DOT product that I have changed my notation.",
                    "label": 0
                },
                {
                    "sent": "OK, just to stimulate your brain so this the dot product here is there just dot.",
                    "label": 0
                },
                {
                    "sent": "The target space is a Y and we have a separable training space set that is called S. It's there is.",
                    "label": 1
                },
                {
                    "sent": "It's a linearly separable training set.",
                    "label": 0
                },
                {
                    "sent": "And what you want to to find when you're doing?",
                    "label": 0
                },
                {
                    "sent": "SVM is to find what is called the optimal hyperplane, which is the hyperplane that maximizes the margin which maximizes the distance between the closest points to it.",
                    "label": 0
                },
                {
                    "sent": "The closest ones OK, so between those two guys.",
                    "label": 0
                },
                {
                    "sent": "When you're going to do.",
                    "label": 0
                },
                {
                    "sent": "SVM's you're going to try and and and learn this classifier because the margin is larger.",
                    "label": 0
                },
                {
                    "sent": "Then just to anticipate a bit about about the next slide.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's go to the next slide.",
                    "label": 0
                },
                {
                    "sent": "We're going to consider.",
                    "label": 0
                },
                {
                    "sent": "Just let me say what we're going to do is 222.",
                    "label": 0
                },
                {
                    "sent": "I'm what I'm going to do is to show you how you end up with the convex optimization problem that I talked to you about just just before.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When you want to find a hyperplane, you will have to to make some assumptions to put some constraints on how the hyperplanes defined.",
                    "label": 0
                },
                {
                    "sent": "Because imagine that you have these hyperplanes here, which is totally parameterized by W&B.",
                    "label": 0
                },
                {
                    "sent": "You can multiply W&B by the same constant and you will have exactly the same hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So there is a degree of degree of freedom that that has to be taken into account.",
                    "label": 0
                },
                {
                    "sent": "And in order to take it into account, we are going to consider to only consider Canonical hyperplane with respect to the training set we have.",
                    "label": 0
                },
                {
                    "sent": "A Canonical hyperplane is such that the closest point to the hyperplane verify www.x I + B is equal to 1 and 4 minus point www.x I plus basic is equal to minus one.",
                    "label": 1
                },
                {
                    "sent": "This is my the way of.",
                    "label": 0
                },
                {
                    "sent": "Constraining the thing about the.",
                    "label": 0
                },
                {
                    "sent": "The fact that you can multiply W&B by some constant.",
                    "label": 0
                },
                {
                    "sent": "And again, something that you did in high school.",
                    "label": 0
                },
                {
                    "sent": "The margin that is the distance between this one and this one.",
                    "label": 0
                },
                {
                    "sent": "Those two hyperplane is 2 divided by the normal W OK. Because just the distance of Point XI that realizes this, that is such that www.x I + B is equal.",
                    "label": 0
                },
                {
                    "sent": "One the distance of such point to the hyperplane is 1 divided by norm of W.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you you, you're doing.",
                    "label": 0
                },
                {
                    "sent": "SVM you you, you, you you try to maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "And at the same time you try to have a W such that all the positive point fall on one part on one side of the hyperplane and all the negative points on the other side.",
                    "label": 0
                },
                {
                    "sent": "An formally, you can write it down like this.",
                    "label": 0
                },
                {
                    "sent": "You say that.",
                    "label": 0
                },
                {
                    "sent": "You want to maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "So maximizing the maximising the margin is like minimizing the inverse of the margin.",
                    "label": 0
                },
                {
                    "sent": "So I want to minimize the inverse of the margin.",
                    "label": 0
                },
                {
                    "sent": "I take a square because that's how I feel good.",
                    "label": 0
                },
                {
                    "sent": "And I have this constraints which say that for all positive points that is, all points with label Y equal to plus one.",
                    "label": 0
                },
                {
                    "sent": "I want this to be realized that is WXRW dot X I + B. I want it to be greater than plus one and for all the negative points.",
                    "label": 0
                },
                {
                    "sent": "I want the this this value to be lower than than minus one.",
                    "label": 0
                },
                {
                    "sent": "Looking, looking at the this at the pictures, it says that.",
                    "label": 0
                },
                {
                    "sent": "I'm just looking for a Canonical hyperplane.",
                    "label": 0
                },
                {
                    "sent": "We just say that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And those two constraints can be summarized into these constraints.",
                    "label": 0
                },
                {
                    "sent": "Because if Y is equal to 1, you can multiply.",
                    "label": 0
                },
                {
                    "sent": "The two parts by why I and you end up with a YIW's dot X I + B greater or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And if Yi is equal to minus one, you multiply the two parts by Y and you don't forget to invert the sign.",
                    "label": 0
                },
                {
                    "sent": "And minus one by minus one is equal to 1 and you also have the the the same the same result.",
                    "label": 0
                },
                {
                    "sent": "So normally, if you you're familiar with optimization.",
                    "label": 0
                },
                {
                    "sent": "You're already good.",
                    "label": 0
                },
                {
                    "sent": "You say, OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "That's a quadratic program.",
                    "label": 0
                },
                {
                    "sent": "I know how to do that.",
                    "label": 0
                },
                {
                    "sent": "If you're not familiar, you'll say OK. What can I do with that?",
                    "label": 0
                },
                {
                    "sent": "And The thing is.",
                    "label": 0
                },
                {
                    "sent": "This might be a little bit complicated that might look a bit complicated.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you how to deal with this.",
                    "label": 0
                },
                {
                    "sent": "A little bit complicated constraints.",
                    "label": 0
                },
                {
                    "sent": "We're going to do.",
                    "label": 0
                },
                {
                    "sent": "Relaxation.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're going to introduce LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "The solution WB that we're going where we're looking for can be found by be by baby.",
                    "label": 1
                },
                {
                    "sent": "It's not beats by by solving the following problem.",
                    "label": 1
                },
                {
                    "sent": "Minimizing the maximum of these function.",
                    "label": 0
                },
                {
                    "sent": "And this function is defined as that.",
                    "label": 0
                },
                {
                    "sent": "This mysterious function.",
                    "label": 0
                },
                {
                    "sent": "Has been studied by people doing optimization an what is nice about this function is that it's.",
                    "label": 0
                },
                {
                    "sent": "It helps you.",
                    "label": 0
                },
                {
                    "sent": "Record an an rewrite the optimization problem that I showed you just before.",
                    "label": 0
                },
                {
                    "sent": "In an unconstrained manner, OK, it's not very easy.",
                    "label": 0
                },
                {
                    "sent": "It's not very easy to deal with Min, Max, etc.",
                    "label": 0
                },
                {
                    "sent": "But still you're happy because you should be happy.",
                    "label": 0
                },
                {
                    "sent": "You must be happy because it's unconstrained, except for the small constraint here on the on the Alpha.",
                    "label": 0
                },
                {
                    "sent": "But it's nothing.",
                    "label": 1
                },
                {
                    "sent": "So, like I say, it's another formulation of the constraint optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One way of getting some intuition about how it works is to see that if one of the constraint is violated is is not, is not fulfilled.",
                    "label": 0
                },
                {
                    "sent": "For instance, you have Yi times the value XI plus B -- 1 that is lower strictly lower than 0.",
                    "label": 0
                },
                {
                    "sent": "Which is which means that W&B violates the constraint on Y&XI.",
                    "label": 0
                },
                {
                    "sent": "Then this value here is negative.",
                    "label": 0
                },
                {
                    "sent": "Here you have a minus, minus and minus is A plus you want to maximize with respect to Alpha, which is positive.",
                    "label": 0
                },
                {
                    "sent": "Then you can take Alpha equal to Infinity and then you have something that is equal to Infinity.",
                    "label": 0
                },
                {
                    "sent": "That is, you're you're trying to minimize something.",
                    "label": 0
                },
                {
                    "sent": "That is always equal to Infinity, and you know that it's not going to be your solution.",
                    "label": 0
                },
                {
                    "sent": "And this is necessarily you.",
                    "label": 0
                },
                {
                    "sent": "You have that all the constraints.",
                    "label": 0
                },
                {
                    "sent": "Will be fulfilled at the end of your optimization thing, because because of these these remark.",
                    "label": 0
                },
                {
                    "sent": "And Conversely, you have.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly.",
                    "label": 0
                },
                {
                    "sent": "Conversely, you also have that if you have a point that fulfills the constraint, then necessarily you have Alpha I that is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Here, suppose that this is positive.",
                    "label": 0
                },
                {
                    "sent": "Then here you have something that is positive.",
                    "label": 0
                },
                {
                    "sent": "Then you have something that is minus something positive.",
                    "label": 0
                },
                {
                    "sent": "And if you want to maximize that, you have to minimize this guy where, where, where?",
                    "label": 0
                },
                {
                    "sent": "This is positive an knowing that Alpha is cannot be lower than zero.",
                    "label": 0
                },
                {
                    "sent": "Then you know that the optimal value for this specific constraint is a time attained for Alpha I equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is what for?",
                    "label": 0
                },
                {
                    "sent": "For those of you who are familiar with the optimization, this what how you can get the optimality optimality conditions or the courage kentucker conditions.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the important thing on the minimization.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "I said if you want to find W&B, you just have to solve this.",
                    "label": 0
                },
                {
                    "sent": "Annual have the correct WNB.",
                    "label": 0
                },
                {
                    "sent": "So the mean of the Max of L and there is a theorem that you can find in many optimization book which say that L is concave with respect to Alpha and convex with respect to WNB.",
                    "label": 1
                },
                {
                    "sent": "Concavity convexity is something that you you saw with the with the Sandrina lost last Tuesday.",
                    "label": 0
                },
                {
                    "sent": "Then if you have this property that this function is concave with respect to the thing that you maximize and convex with respect to the thing that you minimize.",
                    "label": 1
                },
                {
                    "sent": "Then you can switch the Max in the mean.",
                    "label": 1
                },
                {
                    "sent": "And you have the same optimal points.",
                    "label": 1
                },
                {
                    "sent": "That is, you have the same WB&R fact, that is, that are good for you.",
                    "label": 0
                },
                {
                    "sent": "And now you have something that is very nice, which is that this problem here if you just forget about the Max and you consider just the minimum of L with respect to W&B, this is an unconstrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Because you don't have constraints on WNBA.",
                    "label": 0
                },
                {
                    "sent": "And more than that, it's an unconstrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's a differentiable optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's a twice differentiable optimization problem, which means that.",
                    "label": 1
                },
                {
                    "sent": "It's convex, so a necessary condition for it, for for having an an optimum is to have the.",
                    "label": 0
                },
                {
                    "sent": "Gradient equal to 0.",
                    "label": 0
                },
                {
                    "sent": "An if you want to add the gradient be equal to 0, just compute the gradient through the gradient was with respect to W, is this?",
                    "label": 0
                },
                {
                    "sent": "W minus some of YIXI.",
                    "label": 0
                },
                {
                    "sent": "Oh there is an Alpha I hear missing.",
                    "label": 0
                },
                {
                    "sent": "Just remember.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There is an Alpha I hear.",
                    "label": 0
                },
                {
                    "sent": "And if you take the derivative with respect to B, you have the sum of Alpha Yi.",
                    "label": 0
                },
                {
                    "sent": "Am am needing via gradient requiring the gradient to be equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Implies those two constraints, so there is still an Alpha I missing here.",
                    "label": 0
                },
                {
                    "sent": "I'll see it again later when you have the printouts.",
                    "label": 0
                },
                {
                    "sent": "And you have those two constraints an you can what we're going to do is to plug back this this this.",
                    "label": 0
                },
                {
                    "sent": "This expression of W, which is just linear combination of all the excise inside this.",
                    "label": 0
                },
                {
                    "sent": "This function an will retain these constraints.",
                    "label": 0
                },
                {
                    "sent": "Ann will get this problem.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I did is that I know that because this is a convex function is differentiable.",
                    "label": 0
                },
                {
                    "sent": "It's is.",
                    "label": 0
                },
                {
                    "sent": "Everything is nice, I just take the derivative, I take the gradient, I'm making it equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Then it gives me an expression for WI mean it gives me another constraint and I used to those two constraints and the expression of W and they replace it into L, replacing and then the mean disappears because I intentionally took the gradient and made it equal to 0 to have the mean of that.",
                    "label": 0
                },
                {
                    "sent": "So I replace W&BI, plug them back into the function L, doing some computations, very simple computations, and not forgetting about the Alpha adds.",
                    "label": 0
                },
                {
                    "sent": "You have that the problem that you have to deal with is this problem.",
                    "label": 0
                },
                {
                    "sent": "So this problem is is a maximization problem with respect to Alpha and the constraints that you have.",
                    "label": 0
                },
                {
                    "sent": "Are this constraint which is pretty simple and these otherwise other constraint that just say that all the Alpha should be should be non negative?",
                    "label": 0
                },
                {
                    "sent": "And the function.",
                    "label": 0
                },
                {
                    "sent": "The objective function that you consider here is this function, which is a quadratic function.",
                    "label": 0
                },
                {
                    "sent": "That is expressed in terms of Alpha and Alpha J of our five or the Alpha eyes, and this is a convex function and this is.",
                    "label": 0
                },
                {
                    "sent": "This is a convex programming convex program.",
                    "label": 0
                },
                {
                    "sent": "These quadratic program.",
                    "label": 0
                },
                {
                    "sent": "And now you should be already thrilled and excited about something.",
                    "label": 0
                },
                {
                    "sent": "I can see the excitement in your eyes, but you should be excited because you just have dot products here.",
                    "label": 0
                },
                {
                    "sent": "So that's good because at some point we have to Colonel Eyes SVM's.",
                    "label": 0
                },
                {
                    "sent": "And to criminalize them, you just.",
                    "label": 0
                },
                {
                    "sent": "We'll just do our famous well known kernel trick and replace everything where we see that product by AK of XI.",
                    "label": 0
                },
                {
                    "sent": "XJ.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the quadratic programming problem you end up with is this one.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the same as before.",
                    "label": 0
                },
                {
                    "sent": "And the the remarks that I'm doing that the number of of variables that you have to consider is.",
                    "label": 1
                },
                {
                    "sent": "The number of data that you have at the beginning, so this is something that is a problem in some way, because if you have 1 million data then you have a program and optimization optimization optimization problem defined with respect to 1 million variables.",
                    "label": 0
                },
                {
                    "sent": "It's very difficult to solve, but we're getting at it.",
                    "label": 0
                },
                {
                    "sent": "We I don't know if we but people are providing solutions for that, and there's something also very interesting.",
                    "label": 0
                },
                {
                    "sent": "Didn't mention it, I think is that there are very few.",
                    "label": 0
                },
                {
                    "sent": "Alpha eyes that are different from zero at the end of the optimization process.",
                    "label": 0
                },
                {
                    "sent": "And just remember that I think that I wrote it down in a few slides, but remember that the only there is one constraint there is 1A I per training data for each site there is 1A I.",
                    "label": 0
                },
                {
                    "sent": "An at the end of the optimization process.",
                    "label": 0
                },
                {
                    "sent": "There are very few Alpha eyes that are different from zero.",
                    "label": 0
                },
                {
                    "sent": "And those very few are five that are different from zero corresponds to correspond to what we call the support vectors.",
                    "label": 0
                },
                {
                    "sent": "So remember that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We are this is a convex quadratic program.",
                    "label": 1
                },
                {
                    "sent": "There are a lot of of the shelf libraries that you can use to solve this, provided the program is not too big.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, you have to resort to two specific algorithms.",
                    "label": 0
                },
                {
                    "sent": "Very nice, very beautiful algorithm to solve as VMS.",
                    "label": 0
                },
                {
                    "sent": "They are very, very nice algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk to you about those algorithms because they are.",
                    "label": 0
                },
                {
                    "sent": "Involved.",
                    "label": 0
                },
                {
                    "sent": "More less involved.",
                    "label": 0
                },
                {
                    "sent": "And and So what I told you is that you, we only have dot products that appear here when you once you can compute the dot product, you're happy.",
                    "label": 0
                },
                {
                    "sent": "There is something that that has disappeared here in some ways.",
                    "label": 0
                },
                {
                    "sent": "Is that big?",
                    "label": 0
                },
                {
                    "sent": "Is not anywhere here.",
                    "label": 1
                },
                {
                    "sent": "It's not a problem because you can.",
                    "label": 0
                },
                {
                    "sent": "We can retrieve the value of of B using the key KKT conditions.",
                    "label": 0
                },
                {
                    "sent": "That is the optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "I will not enter the details here an at the optimum.",
                    "label": 0
                },
                {
                    "sent": "You know that W start this.",
                    "label": 0
                },
                {
                    "sent": "The optimum value is computed as some of Alpha istar why I XI so Alpha I star is just the solution of this problem.",
                    "label": 0
                },
                {
                    "sent": "And here I haven't forgotten the Alpha and the expression of a W, which is a nice thing.",
                    "label": 0
                },
                {
                    "sent": "And you see that F or H or watch FHF.",
                    "label": 0
                },
                {
                    "sent": "Let's say 8 F is computed as some of Alpha IYIXI dot X + B star.",
                    "label": 0
                },
                {
                    "sent": "Meaning that again if you want to come to to to compute the prediction of F for specific point X, you just have to be able to compute DOT products.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "These guys going to be kernelized.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last thing is about support vectors.",
                    "label": 1
                },
                {
                    "sent": "So the Super vectors are those points for which Alpha I star is strictly positive.",
                    "label": 1
                },
                {
                    "sent": "The support the margin, so those are the support vectors.",
                    "label": 0
                },
                {
                    "sent": "Meaning that if you want to, if you minimize this for this training set at the end of the the optimization process, you'll end up with these hyperplanes with W expressed with respect to the Alpha ice, and the thing that you only have 3 points of all those points that are going to have Alpha are different from zero, and those three points correspond to those that are the closest to the separating hyperplane, and those are the support vectors.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So something that I'm I haven't written down on the slides is that if you want to rerun the SVM learning algorithm only on those three points, then you have exactly the same solution.",
                    "label": 0
                },
                {
                    "sent": "That is, the Super vectors contain contain all the information about the decision that you've computed.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, I'm going to talk about soft margin linear SVM.",
                    "label": 1
                },
                {
                    "sent": "And when you work with real data then usually you have outliers, meaning that you have the points that count.",
                    "label": 0
                },
                {
                    "sent": "That can't be.",
                    "label": 0
                },
                {
                    "sent": "In separate with hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "For instance, you have this black point here and here.",
                    "label": 0
                },
                {
                    "sent": "And they can't be nicely separated.",
                    "label": 0
                },
                {
                    "sent": "An yes.",
                    "label": 0
                },
                {
                    "sent": "An OK there is an R missing here.",
                    "label": 0
                },
                {
                    "sent": "So what you do to deal with that is that you introduce what you call Slack variables you will penalize.",
                    "label": 0
                },
                {
                    "sent": "The fact that these black point is not on the right side of the hyperplane and you will measure you will define how you penalize these.",
                    "label": 0
                },
                {
                    "sent": "These points with a distance of this point to the.",
                    "label": 0
                },
                {
                    "sent": "The hyperplane WX plus B is equal to minus one or plus one.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the minimization problem, or the formalization of the problem that you you can, you can derive you can write it down like this.",
                    "label": 0
                },
                {
                    "sent": "This is the again one norm soft margin SVM.",
                    "label": 0
                },
                {
                    "sent": "You have still the thing that you want to minimize, or to maximize the margin and then you say that at the same time you same time you don't want too many points to be to fall on the wrong side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So you want to minimize the SIII where the eyes are just.",
                    "label": 0
                },
                {
                    "sent": "This distance.",
                    "label": 0
                },
                {
                    "sent": "Looks eye eyes are these distances OK, so you want to have?",
                    "label": 0
                },
                {
                    "sent": "This distance is as small as possible.",
                    "label": 0
                },
                {
                    "sent": "And this can be written like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've I've put in, read the only differences between.",
                    "label": 0
                },
                {
                    "sent": "The hard margin SVM's that I showed you before and the soft margin formulation.",
                    "label": 0
                },
                {
                    "sent": "And for.",
                    "label": 0
                },
                {
                    "sent": "Those of you who.",
                    "label": 0
                },
                {
                    "sent": "Ask me questions about how to choose the right kernel an how do you know that the kernel is good?",
                    "label": 0
                },
                {
                    "sent": "Then there is another guy here that enters into play that doesn't help us.",
                    "label": 0
                },
                {
                    "sent": "In life which is see.",
                    "label": 0
                },
                {
                    "sent": "The reason is that another parameter that needs to be tuned very carefully.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So it's it's not easy.",
                    "label": 0
                },
                {
                    "sent": "You usually should.",
                    "label": 0
                },
                {
                    "sent": "You should do cross validation or there are nice things about generalization bounds that can be used or used, or estimation of the leave one out error that I've been derived, but.",
                    "label": 0
                },
                {
                    "sent": "It's not a common practice to use them.",
                    "label": 0
                },
                {
                    "sent": "Ann, usually people resort to cross validation thing and computationally intensive things.",
                    "label": 0
                },
                {
                    "sent": "So when you use all the machinery that I talked to you about, that is the Lagrangian you introduce all the LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "Then the quadratic problem program that you end up with is this one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you add if you consider that you may have points that don't fall in the on the on the right side of the hyperplane, then you write the program like that.",
                    "label": 0
                },
                {
                    "sent": "You use all the thing about geology and the LaGrange multipliers and you end up with a problem like this which is almost the same as the hard margin problem, except that here you have this box.",
                    "label": 0
                },
                {
                    "sent": "What you call box constraint, meaning that now.",
                    "label": 0
                },
                {
                    "sent": "Alpha is required to be between zero and C. Before we just had that Alpha should be between zero and Alpha and now because we have outliers and we want to deal with outliers, we say that Alpha.",
                    "label": 0
                },
                {
                    "sent": "Has to be between zero and see.",
                    "label": 0
                },
                {
                    "sent": "So there is meaning that this slight difference doesn't is not very difficult to take into account if you have already a. Algorithm to solve the quadratic program for the merging problem.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is a two norm two norm SVM.",
                    "label": 0
                },
                {
                    "sent": "Soft margin SVM where here you put a square.",
                    "label": 0
                },
                {
                    "sent": "Just before I didn't put a square, there was a if you want to one an the difference so you do all the trick with the LaGrange multipliers and so on and you end up with these these.",
                    "label": 0
                },
                {
                    "sent": "Learning this up this.",
                    "label": 0
                },
                {
                    "sent": "Quadratic program where now the difference is just here.",
                    "label": 0
                },
                {
                    "sent": "Before you didn't have this disturb, you just had excite dot XJ.",
                    "label": 0
                },
                {
                    "sent": "And now you have excite dot exemples.",
                    "label": 0
                },
                {
                    "sent": "Delta I J, / C, Delta IJ is equal to 1 when I is equal to J and is equal to 0 otherwise, meaning that.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "In this hard margin case, you had to consider the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "The gram matrix made of.",
                    "label": 0
                },
                {
                    "sent": "Let's use the same notation.",
                    "label": 0
                },
                {
                    "sent": "So in the hard margin case you had all the information you needed to solve the SVM problem.",
                    "label": 0
                },
                {
                    "sent": "In this gram matrix you had everything because the only thing that you needed.",
                    "label": 0
                },
                {
                    "sent": "Where the the why the?",
                    "label": 0
                },
                {
                    "sent": "Why eyes you?",
                    "label": 0
                },
                {
                    "sent": "You still have them and then the dot products.",
                    "label": 0
                },
                {
                    "sent": "In the hard margin case, you had to consider this.",
                    "label": 0
                },
                {
                    "sent": "And then what I say here is that if you consider the soft margin the two norm, soft margin, linear SVM, then you will have to consider.",
                    "label": 1
                },
                {
                    "sent": "This gram matrix.",
                    "label": 0
                },
                {
                    "sent": "Where only the diagonal elements have been changed.",
                    "label": 0
                },
                {
                    "sent": "An everything else remains the same.",
                    "label": 0
                },
                {
                    "sent": "So again, from a purely algorithmic point of view.",
                    "label": 0
                },
                {
                    "sent": "If you had something working for the hard margin SVM, you have something something working for the two norm soft margin SVM.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An something that I wanted to to talk to you about.",
                    "label": 0
                },
                {
                    "sent": "Because we're going to program.",
                    "label": 0
                },
                {
                    "sent": "This tomorrow, but with the colonels.",
                    "label": 0
                },
                {
                    "sent": "So you can have unconstrained version of the one norm SVM, soft margin, SVM and the two norm soft margin SVM.",
                    "label": 0
                },
                {
                    "sent": "Those are the primal forms, meaning that I don't use LaGrange LaGrange multipliers percent, so this is just another.",
                    "label": 0
                },
                {
                    "sent": "Another formulation of the problem of the previous problem.",
                    "label": 0
                },
                {
                    "sent": "I say that I want to minimize to maximize the margin and then I have these slack.",
                    "label": 0
                },
                {
                    "sent": "This exactly the same thing as.",
                    "label": 0
                },
                {
                    "sent": "The distance between this point and the the corresponding hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So, and this is the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "I think a submarine mentioned it to you on Tuesday.",
                    "label": 0
                },
                {
                    "sent": "This is just the Max of the Taecyeon 0.",
                    "label": 0
                },
                {
                    "sent": "And here you have to this function this unconstrained problem, because you don't have constrained on W&B.",
                    "label": 0
                },
                {
                    "sent": "Those are the only ones.",
                    "label": 0
                },
                {
                    "sent": "The only one variables that you're going to do your optimization on, and they are not constrained.",
                    "label": 0
                },
                {
                    "sent": "And The thing is that the dysfunction is still convex, so the image loss is a convex function.",
                    "label": 0
                },
                {
                    "sent": "So the sum of convex functions is still a convex function.",
                    "label": 0
                },
                {
                    "sent": "Is convex dirty?",
                    "label": 0
                },
                {
                    "sent": "This convex, which is good, but.",
                    "label": 0
                },
                {
                    "sent": "At the same time, is not differentiable differentiable.",
                    "label": 0
                },
                {
                    "sent": "So you cannot directly use the gradient decent algorithm that that you you you did two days ago.",
                    "label": 0
                },
                {
                    "sent": "And but there exists there exist solutions to do this.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to tell you about these solutions.",
                    "label": 0
                },
                {
                    "sent": "And but this problem.",
                    "label": 0
                },
                {
                    "sent": "Which corresponds to a primal form of via two norm SVM is is that you just put a square here?",
                    "label": 0
                },
                {
                    "sent": "And this problem is very nice because it's convex and it's differentiable.",
                    "label": 0
                },
                {
                    "sent": "And this is what we're going to program tomorrow.",
                    "label": 0
                },
                {
                    "sent": "We're going to do a Newton decent on this function and hopefully in four hours will be able to program an SVM.",
                    "label": 0
                },
                {
                    "sent": "This is also something that I tried with my students.",
                    "label": 0
                },
                {
                    "sent": "And with the same results as before.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I trust you guys.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Where I'm going to to end with I?",
                    "label": 0
                },
                {
                    "sent": "I think I I still have two slides or something like that.",
                    "label": 0
                },
                {
                    "sent": "Something very very easy is now we're going to do nonlinear separation.",
                    "label": 0
                },
                {
                    "sent": "Before the kernel time, before the kernel method time.",
                    "label": 0
                },
                {
                    "sent": "Moving from linear to nonlinear things was difficult.",
                    "label": 0
                },
                {
                    "sent": "Here is going to be just easy.",
                    "label": 0
                },
                {
                    "sent": "So I took some time to explain you how you you derive the optimization problems for one norms of margins and two norms of margins, soft margin, SVM's.",
                    "label": 0
                },
                {
                    "sent": "Is Jens and now it's just easy.",
                    "label": 0
                },
                {
                    "sent": "It's just the kernel trick to have a nonlinear decision function.",
                    "label": 0
                },
                {
                    "sent": "So the only thing that I did.",
                    "label": 0
                },
                {
                    "sent": "In comparison with what I said earlier is that I had that product here.",
                    "label": 0
                },
                {
                    "sent": "I just replaced the dot product by K. So this is the current kernel trick.",
                    "label": 0
                },
                {
                    "sent": "And here again, and now just doing that I have.",
                    "label": 0
                },
                {
                    "sent": "A way to derive a large margin separating hyperplane in some high dimensional space that you don't know.",
                    "label": 0
                },
                {
                    "sent": "But you can do you have all the theory that goes with not exactly all, but to much of the theory that goes with linear separation that carries over.",
                    "label": 0
                },
                {
                    "sent": "With the with the Kernels an you don't have anything to do, it's just easy.",
                    "label": 0
                },
                {
                    "sent": "So the size of the problems scale with the number of data that I told you that before.",
                    "label": 1
                },
                {
                    "sent": "There are very efficient and very elegant methods to solve these problems.",
                    "label": 0
                },
                {
                    "sent": "Not noticeably exploit the sparsity of this solution.",
                    "label": 0
                },
                {
                    "sent": "Ann K&C are two things that it's not exactly hyperparameters for K, but OK. Let's say that it's something that you have.",
                    "label": 0
                },
                {
                    "sent": "It's in hyperparameters hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "It's something that you really need to choose carefully.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you want to really have have good results.",
                    "label": 0
                },
                {
                    "sent": "On practical applications so.",
                    "label": 0
                },
                {
                    "sent": "The important things about machine support vector machines are the following.",
                    "label": 0
                },
                {
                    "sent": "Their based on margins.",
                    "label": 0
                },
                {
                    "sent": "There they use kernels.",
                    "label": 0
                },
                {
                    "sent": "There is a nice convex optimization problem that you can obtain and solve.",
                    "label": 1
                },
                {
                    "sent": "It's very effective if you try.",
                    "label": 1
                },
                {
                    "sent": "It's very effective in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Experts on SVM are non experts, are capable of achieving very good of achieving state of the art results on different applications.",
                    "label": 0
                },
                {
                    "sent": "So it's practically it works.",
                    "label": 0
                },
                {
                    "sent": "It's not just a nice thing from the theoretical point of view.",
                    "label": 0
                },
                {
                    "sent": "And something that I didn't talk about here is the algorithmic details.",
                    "label": 0
                },
                {
                    "sent": "Like how the algorithms to solve these problems work.",
                    "label": 0
                },
                {
                    "sent": "So there's the terms I wanted to to to.",
                    "label": 0
                },
                {
                    "sent": "To let you know that the terms like chunking decomposition, active set, stochastic gradients, there are.",
                    "label": 1
                },
                {
                    "sent": "Those are things that are used for efficient optimization of the problem.",
                    "label": 0
                },
                {
                    "sent": "I didn't talk to you about learning the in the promo.",
                    "label": 0
                },
                {
                    "sent": "This is something that we're going to do tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And and I didn't talk to you about theory.",
                    "label": 0
                },
                {
                    "sent": "And I said, be patient, because if we have time, we'll talk about the theory later.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But for the first 2 hours, I think we're gonna have a break.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but if you have questions.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to have coffee, let's go there.",
                    "label": 0
                }
            ]
        }
    }
}