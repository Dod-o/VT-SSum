{
    "id": "ttsn5iv4loeoo7gwrbkygwhccarhckov",
    "title": "Learning Parameters in Discrete Naive Bayes Models by Computing Fibers of the Parametrization map",
    "info": {
        "author": [
            "Vincent Auvray, University of Li\u00e8ge",
            "Louis Wehenkel, University of Li\u00e8ge"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/aml08_auvray_lpdnbmcfpm/",
    "segmentation": [
        [
            "Sue we will talk about this learning parameters for discrete naive Bayes models by fibers of the prioritization method.",
            "OK, so don't be scared by the title, it's very simple the idea."
        ],
        [
            "OK, so just to start the night vision networks are also called latent class model.",
            "There are different names so you have a distribution on the discrete variables X1, XN, here.",
            "And it's simply a mixture model here where you have your Class C and then conditionally on your class, all the variables are independent.",
            "So I will talk about this type of model."
        ],
        [
            "So what I'm going to tackle is given some night vision distribution P. I will try to compute the set of parameters, so here the.",
            "Probabilities of the hidden variable.",
            "So the mixture components, the weights of the mixture and these parameters.",
            "So that's the setting.",
            "So why do you want?",
            "Why do we want to do that?",
            "So I think it will lead us to a better understanding of the model.",
            "It can also lead us to find new methods to estimate parameters, so the common way to estimate parameters in this with this model is simply the EM algorithm.",
            "It can also help us to do model selection, so this was one of the original motivation for me to do this work.",
            "It's to calculate a synthetic evaluation of the marginal likelihood of a night vision network.",
            "And also give us some clue on the identifiability of the parameters."
        ],
        [
            "So it's very simple outline.",
            "First, I will present my result, which is really the core of this talk.",
            "Now will talk about some potential applications, so it's really ongoing work, so the applications are not really there yet.",
            "But I do have some very interesting results.",
            "And I'll talk about some extensions when you can.",
            "Once you can compute like the parameters mapped to a naive Bayes distribution.",
            "While you can use that to extend this to larger class of models like hierarchical latent class models."
        ],
        [
            "OK, so.",
            "Out first introduce some notation so it's.",
            "It's I'll give you a very narrow talk.",
            "Some notations so.",
            "I have the parameter will repair, tries the distribution.",
            "So first a simple notation.",
            "Here the weights of the mixture.",
            "I'll denote them by W. And here so the probability of observing one variable given certain class.",
            "Instead of using this parameter, I will use this one simply by removing the marginal probability, very simple.",
            "So."
        ],
        [
            "I will also introduce another notation.",
            "So instead of using the distribution.",
            "I will introduce this notation Q notation, so the exact expression is not very interesting in itself.",
            "So it's defined recursively in some way.",
            "OK, so for any subset of the variables.",
            "So instead of using marginal I will use this.",
            "OK, so that's the notation very simple.",
            "But one thing that's interesting with this notation.",
            "Is that if you multiply the way?",
            "The weights with this parameter you get 0, so that's a really simple equation that will allow us to do a lot actually.",
            "So before talking about that, here is an example of my notation.",
            "So instead of having a marginal distribution, P of XII have Q of X, which is always 0.",
            "So here for two variables Q of XIXJ simply the probability minus the product of the marginals.",
            "So it kind of measures the independence.",
            "Between the variables and here is the expression for three variables.",
            "So if we have a distribution P. If it's given, then we can also observe that.",
            "So there is no parameter involved.",
            "And so this Q is actually exactly the.",
            "Correspondes really well to the P&R new parameterisation 'cause I get the same type of some type of expression.",
            "With this additional equation, that's going to turn out to be very interesting.",
            "So with this equation, actually we see that the weight vector."
        ],
        [
            "Is normal is to some pictures from column some here these vectors.",
            "OK.",
            "So once you know that and if you have N -- 1 vectors where M is the number of mixture components.",
            "So if you have this, if you have some vectors like this then you can write an equation like this that basically says that W is a normal to the space.",
            "Spanned by these vectors.",
            "And if if this is normal, if you want some its component, it's different from zero, then you can scale it.",
            "And then you can obtain your.",
            "You're a W from these.",
            "From these parameters, new parameters.",
            "With the vectors a basis.",
            "OK so.",
            "Here."
        ],
        [
            "This so if you have some value XI have some random variable.",
            "So AT of X, XI is simply the probability of observing XI in the mixture component denoted by T minus the marginal probability.",
            "So I'm just given one value of some random variable.",
            "I consider the vector T is the index so exactly here.",
            "OK, same for W. It's the vector.",
            "Of a mixture weights and when you multiply them, they're easy to see.",
            "If you have some here.",
            "With the W then you get the mixture and then you get marginal, so it's zero.",
            "It's very easy."
        ],
        [
            "But then you can deduce W from urn.",
            "These vectors.",
            "So this is just some notation here.",
            "It's a matrix with the column.",
            "Cones are given and this notation is.",
            "I remove the teeth line.",
            "So we had some.",
            "Victorial interpretation.",
            "But it's it's simply the normal.",
            "And I just want to ensure that I can normalize it."
        ],
        [
            "OK.",
            "So a second result that really is derived from our first normal equations is.",
            "I found a polynomial that's of degree M. Whose roots are exactly my parameters.",
            "So in the case of equal to, so it's a case that's already been solved.",
            "By other people.",
            "So you obtain this if you write this polynomial in S. Then in fact, it's factorizes like this, so you can find the roots.",
            "Everything is observable here.",
            "For Amical three, you have a similar type of polynomial.",
            "It's kind of complicated.",
            "What you have to look at is the leading coefficient of the polynomial.",
            "Here, here, so in the general case.",
            "You have one."
        ],
        [
            "Ali Nomial, whose leading coefficient is the determinant of this matrix.",
            "And you have other terms here and there, quite complicated to write there.",
            "Just sum of sums of determinants of other matrices of the same dimension.",
            "OK, and so again.",
            "The roots of this polynomials are the parameters were looking for.",
            "So of course to be able to write something like that, you need to have.",
            "Sufficient you need to be able to write this exactly, so you need to have in a variables to be able to write this.",
            "And here of course you cannot have.",
            "Two values of the same variable 'cause it wouldn't make sense.",
            "You cannot write probability to observe.",
            "Exercise is equal to 0 and XI is equal to 1.",
            "So you have constraints to be able to write these polynomials.",
            "In terms of.",
            "How the observable variables are and there are cardinalities.",
            "OK, so that's."
        ],
        [
            "First big result.",
            "And my other big result is this equation again.",
            "So.",
            "Here again you consider a set of values U one to U N -- 1 and minus one values and you consider here some number of other values.",
            "X1X key?",
            "So if you write this equation, you actually have a polynomial in these variables and something that's observable.",
            "To calculate determinant, it's easy.",
            "By signing on this column.",
            "And then here on the other side, you have one determinant that's.",
            "Only consists of observable variables again.",
            "So you have simple constraints on the parameters.",
            "And it's actually.",
            "It was really hard to get to this result because the original problem is all the mixtures components are mixed.",
            "And here I obtained something that's very nice in my eyes.",
            "And once again you have the same polynomial here.",
            "That is, the leading coefficient of our.",
            "Previous pulling over the same determinant, that's the leading coefficient over previous polynomial.",
            "OK."
        ],
        [
            "So one example, if for my product here on these values I just use one value and I simply have a linear equation.",
            "And here this term is equal to 0.",
            "So I obtained this.",
            "Another example, if M is equal to three, so if I have three components.",
            "And I use this.",
            "These values are the same as here.",
            "That's good 'cause I obtained one.",
            "I can obtain one, obtain an equation of polynomial equation linking these two coefficient so I can express 1 as a function of the other."
        ],
        [
            "Alright, so um.",
            "I have all these determinants and actually.",
            "That's a nice decomposition.",
            "So if you consider sets of values.",
            "Like this and you write this determinant.",
            "And actually, if you look at the parameters, so if these distribution Q is a night vision distribution with M classes, then it will be you can write it like this.",
            "So it's simply the product of the weights time, a determinant time, another determinant.",
            "Of this matrix.",
            "So with this we can write.",
            "This determinant?",
            "So we'll simply have the product.",
            "Of the weights determinant like this and the same determinants with EU instead of the visa.",
            "So.",
            "When you look at these.",
            "When you look at this, it's nice because if you have one.",
            "If you have one distribution that's you can represent it with M classes, night vision distribution, but you write this with mplus, one class.",
            "Then you can parameterise your distribution with one weight equal to 0, so you have."
        ],
        [
            "In some implicit equations like this.",
            "But you can just use for example for model selection.",
            "I think that's one potential application for that.",
            "And if you look at the.",
            "At this determinant.",
            "You see, you have lots of degrees of freedom.",
            "For example, in the choice of V1.",
            "It doesn't appear.",
            "Here and so you can write lots of lots of equations like that, but actually they're all the same because you have some.",
            "Some condition here that tells you that they're just going to be multiples of.",
            "The others.",
            "So we have to be careful which equations are independent.",
            "OK, so these are my main results."
        ],
        [
            "Now let's talk about some applications, so the."
        ],
        [
            "Easiest application I see from that is OK. We can have one distribution.",
            "We can compute the parameters so well we can.",
            "Just if you want to do some learning with that, we can just have some distribution of observed frequencies and then we can try to do the same basically.",
            "And it works in theory, but in practice doesn't really work.",
            "But I'll talk about that a little later after.",
            "So you can also have some sufficient conditions for parameter identifiability.",
            "So, um.",
            "I will also talk a little bit about that.",
            "But if you can solve your equation, then you can talk about the identifiability of the model, or at least you can identify some conditions where it's probably won't be identifiable.",
            "And so this was my original motivation.",
            "If you want to compute the analytical synthetic approximation to the marginal likelihood, than one method that uses the results of Watanabe, we had some previous talk was about that too.",
            "Or is related.",
            "Then you need to compute the preimage of distribution.",
            "And you need to pay attention to the shape of this set.",
            "So I think this is a first step.",
            "It has been it's very easy to do in the case where you have two hidden classes equal to.",
            "Gets really complicated in my experience when you have more components.",
            "And also my implicit equations are probably relevant for model selection, in particular in learning of hidden causes causes, so you have some.",
            "Tetra Delta constraints that are used in a.",
            "To learn hidden causes and probably mostly with the coaching variables.",
            "But if you also have some similar ones.",
            "In with this crew."
        ],
        [
            "Variables or the results I've seen are with.",
            "Again, the case M Equal 2 and they this is can be seen as a generalization of these results.",
            "These equations.",
            "OK.",
            "So now if I want to focus more on computing the.",
            "The preimage in details in my results.",
            "So I have here very important assumption, so I have to be able to find.",
            "Three sets of values like this.",
            "And I have to be able to write these determinants so it impose constraints on these sets of values, 'cause I have to be able like this variable.",
            "T1 must not be the same as variable you want.",
            "And so on.",
            "So if constraint on the number of variables and their cardinality again and then I require that these three determinants are different from zero basically so I can solve my polynomials reitzes correspond to leading coefficient of my polynomial."
        ],
        [
            "OK, so.",
            "If you look at the determinant, these are.",
            "These three are equation of determinants different from zero.",
            "Basically amounts to these three conditions.",
            "So, Dion the product of the weight, must be different from zero.",
            "This determinant must be different from zero, so it implies that these vectors must be linearly independent.",
            "And so on for all three vectors, all three matrices.",
            "OK, and then in this case, while we can just solve our equation, this may not be a good idea to use this equation like this if you want to do it in practice, because it's not very stable numerically, but since it's normal it's there are better methods to do that.",
            "OK, so if I have.",
            "These vectors I can compute the weights.",
            "OK, now I can simply use one of my first result here."
        ],
        [
            "Or one simple version of my other results.",
            "I have one linear equation between all these.",
            "All these values.",
            "So OK, so if I suppose I know this one.",
            "Then I can solve everything just like this.",
            "So it's simply you compute inverse matrix where you don't even have to invert it, but you just solve some system linear system and then you just multiply it by some other.",
            "Some other vector and boom, you get all the other coefficients.",
            "So there is a little you have to be careful 'cause here your X must be distinct from V, otherwise you cannot write these equations.",
            "But since we made enough assumptions.",
            "Like we require that this determinant determinant of this matrix is different from zero, but we also have other determinant, so it all works out.",
            "So now the problem reduces to finding the."
        ],
        [
            "Is.",
            "These vectors.",
            "So OK, so.",
            "Here I don't really have a good solution to do that.",
            "But you still have my polynomial.",
            "And with my polynomial I can find the roots.",
            "So basically I can find these sets.",
            "But these are roots, so they're not ordered, so I can say, OK, I cannot say this.",
            "This corresponds to the first class, and this one corresponds to the first class two.",
            "I can't do that, they're just real numbers, and I can distinguish them.",
            "And also when you want to compute the parameters map to distribution, you have some trivial non identifiable E in that you can just permute all the.",
            "Although you can just relabel your states of your hidden variable.",
            "So you can just.",
            "To get rid of that, you just fix some arbitrary order on the 11 set."
        ],
        [
            "And so it's basic ideas.",
            "OK, well we fix one ordering.",
            "Then we can just try all of them.",
            "And then one OK, so we obtain all the parameters so we can test.",
            "Oh are the parameters we obtain or do they sum to one or they positive and so on?",
            "So you can?",
            "And if so then you can just test.",
            "Are they map to the distribution we given?",
            "It's really naive way to do it 'cause there are like a lot of parameters to test.",
            "So one side observation, if.",
            "OK, we have the hypothesis.",
            "And we are are three.",
            "Determinants are different from zero.",
            "Well then we can see that if this is verified then we have at most a finite number of.",
            "Parameters map to the distribution, which is, I think, an interesting result in its health.",
            "It probably has some implications for the dimension of the model in that case.",
            "Although I haven't worked out the details.",
            "OK.",
            "So if you."
        ],
        [
            "To be a little less naive, you can use some other results I found.",
            "Which basically is another constraint on the parameters.",
            "So here, in a few words, this really looks like the original parameterisation, except I got rid of the weights.",
            "So I have some equation.",
            "And everything else is observable.",
            "OK with some matrix again.",
            "So what we can do is I can just write this result.",
            "And.",
            "So this is for any set of values.",
            "Again, we just want just 22 values for the value, which I know 'cause I ordered them recurly and another one so I can just test.",
            "So instead of testing all the orderings at the same time, all the permutations at the same time, I just have one is fixed and I test all the permutation of the other vector.",
            "So.",
            "So this will reduce the complexity of your algorithm, but I still think it's not the right way to do it.",
            "OK."
        ],
        [
            "So probably a better way to do it is to use other results, so I haven't done it yet because I only recently generalized them.",
            "But as I told you, in the case M = 3, you can easily solve that and you also get some probably very interesting conditions under which you cannot.",
            "Invert for solve polynomial.",
            "Anyway, so once you have these, any algorithm to invert parameters.",
            "You can try to turn that into."
        ],
        [
            "Algorithm to estimate parameters.",
            "So as I told you, basically of the distribution of the observed frequencies, we just try to apply the algorithm.",
            "So.",
            "So of course, if you just have your, you just compute the pre image.",
            "If the distribution that you observe is not exactly when distribution, then can expect to have some.",
            "Empty pre image.",
            "But actually all the procedures are described are continuous in some sense, so if you're.",
            "Or they can be easily modified to be continuous.",
            "So if you have your own unknown distribution and as you get more and more samples it will converge to the true distribution an by continuity.",
            "Basically your estimate will also converge to the true parameter.",
            "And so will it.",
            "All works out actually.",
            "So I was also testing parameters like free quality.",
            "Is this parameter map to distribution and the easy way to the obvious way to change that is simply too?",
            "Once you have a parameter, just look if it's minimizes the entropy, the relative entropy so decaled averaging through the observed distribution.",
            "But in practice."
        ],
        [
            "As I told you already, it doesn't work.",
            "So first you have an extremely fast growth of the complexity with the number of your M. So it's probably workable for lo M. I don't know, probably into 7 eight or something like that.",
            "I haven't tried it or haven't.",
            "I don't have any.",
            "Patient implementation to try that.",
            "So, but the bigger problem is that the estimates are really numerically unstable.",
            "So when you try to solve the polynomial, sometimes you'll get you will not even get real roots.",
            "So what do you do in this case?",
            "Well, you can do anything.",
            "And so you will not have any answer 'cause you will not have any parameter that satisfies the constraints.",
            "Asymptotically, it's supposed to work, but in practice isn't really work.",
            "But so and also we have lots of degrees of freedom in the choice of TU&V.",
            "If you have variables.",
            "So it's probably important how you pick those.",
            "And also you could probably try to somehow pull data 'cause you only use small part of the data.",
            "So the nice thing is that you only use Q like the function Q with the two or three variables.",
            "So maybe you mean you don't need to have very.",
            "It's probably easier to estimate.",
            "Correctly, when you when you have small number of variables.",
            "OK, and in practice it's really not competitive with the EM algorithm.",
            "So it's Maura.",
            "Yeah, practically, I don't think it has any value at this stage.",
            "So once you have."
        ],
        [
            "And the algorithm, or a way to compute the preimage?",
            "And Ivs distribution, and in some cases you can extend it to these type of trees or you have.",
            "Here the leaves are observed and the internal nodes are hidden.",
            "Because you have simple observation.",
            "OK, if you look at these marginal on these three variables, then it's a.",
            "It's small you're interested in, so it's a latent class model naive Bayesian distribution.",
            "If you look at X2X3 and X4, then the same you can.",
            "It's also an advising distribution, and so on, so you can.",
            "If you estimate the parameters of these three distribution, then you can deduce all the parameters associated to these links, and it's also possible to estimate the parameters associated to those links.",
            "It's just some linear algebra.",
            "I haven't figured out all the details or generalize it properly, but.",
            "It would be really nice if we could compute the preimage exactly, not just with some algorithm as I presented."
        ],
        [
            "OK, so to conclude, I think I presented some very simple polynomial equations.",
            "So when you try to actually invert that, you quickly get very very complicated polynomials, and I think the results are obtained are really nice because they're really simple.",
            "So I talked to you about how to apply that to a.",
            "Estimate parameters for the data.",
            "I don't think that's the right thing to do, but you also also get some nice implicit equations and I think they can be used to learn hidden causes from data or maybe.",
            "Maybe decide which.",
            "Which number of hidden classes are, if you have a distribution but you don't know which one is?",
            "And you can try to test for that too.",
            "Well, that's all."
        ],
        [
            "Do you know that these are all?",
            "Apartment.",
            "Hold on.",
            "These are all powerful.",
            "All the possible.",
            "Oh I I I don't know.",
            "I don't know, just have some very limited results.",
            "But I think it's a good.",
            "It's a good clue.",
            "This determinant is a good clue into which direction or conditions where I can't solve my equations are.",
            "What motivated their choice in the change of coordinates?",
            "Well, I don't really have any.",
            "There is no motivation.",
            "Basically I just all the work was done by induction.",
            "It's OK, you have some.",
            "You have some results for M Equal 2.",
            "Then you can try to work it out.",
            "By hand, basically you're with some.",
            "Computer try to generalize to three variables and then you get some equation and I just.",
            "I just generalize it by hand.",
            "So there is no insight."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sue we will talk about this learning parameters for discrete naive Bayes models by fibers of the prioritization method.",
                    "label": 0
                },
                {
                    "sent": "OK, so don't be scared by the title, it's very simple the idea.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to start the night vision networks are also called latent class model.",
                    "label": 0
                },
                {
                    "sent": "There are different names so you have a distribution on the discrete variables X1, XN, here.",
                    "label": 1
                },
                {
                    "sent": "And it's simply a mixture model here where you have your Class C and then conditionally on your class, all the variables are independent.",
                    "label": 0
                },
                {
                    "sent": "So I will talk about this type of model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to tackle is given some night vision distribution P. I will try to compute the set of parameters, so here the.",
                    "label": 1
                },
                {
                    "sent": "Probabilities of the hidden variable.",
                    "label": 0
                },
                {
                    "sent": "So the mixture components, the weights of the mixture and these parameters.",
                    "label": 0
                },
                {
                    "sent": "So that's the setting.",
                    "label": 0
                },
                {
                    "sent": "So why do you want?",
                    "label": 0
                },
                {
                    "sent": "Why do we want to do that?",
                    "label": 0
                },
                {
                    "sent": "So I think it will lead us to a better understanding of the model.",
                    "label": 1
                },
                {
                    "sent": "It can also lead us to find new methods to estimate parameters, so the common way to estimate parameters in this with this model is simply the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "It can also help us to do model selection, so this was one of the original motivation for me to do this work.",
                    "label": 1
                },
                {
                    "sent": "It's to calculate a synthetic evaluation of the marginal likelihood of a night vision network.",
                    "label": 0
                },
                {
                    "sent": "And also give us some clue on the identifiability of the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's very simple outline.",
                    "label": 0
                },
                {
                    "sent": "First, I will present my result, which is really the core of this talk.",
                    "label": 0
                },
                {
                    "sent": "Now will talk about some potential applications, so it's really ongoing work, so the applications are not really there yet.",
                    "label": 0
                },
                {
                    "sent": "But I do have some very interesting results.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about some extensions when you can.",
                    "label": 0
                },
                {
                    "sent": "Once you can compute like the parameters mapped to a naive Bayes distribution.",
                    "label": 0
                },
                {
                    "sent": "While you can use that to extend this to larger class of models like hierarchical latent class models.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Out first introduce some notation so it's.",
                    "label": 0
                },
                {
                    "sent": "It's I'll give you a very narrow talk.",
                    "label": 0
                },
                {
                    "sent": "Some notations so.",
                    "label": 0
                },
                {
                    "sent": "I have the parameter will repair, tries the distribution.",
                    "label": 0
                },
                {
                    "sent": "So first a simple notation.",
                    "label": 0
                },
                {
                    "sent": "Here the weights of the mixture.",
                    "label": 0
                },
                {
                    "sent": "I'll denote them by W. And here so the probability of observing one variable given certain class.",
                    "label": 0
                },
                {
                    "sent": "Instead of using this parameter, I will use this one simply by removing the marginal probability, very simple.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will also introduce another notation.",
                    "label": 0
                },
                {
                    "sent": "So instead of using the distribution.",
                    "label": 0
                },
                {
                    "sent": "I will introduce this notation Q notation, so the exact expression is not very interesting in itself.",
                    "label": 0
                },
                {
                    "sent": "So it's defined recursively in some way.",
                    "label": 0
                },
                {
                    "sent": "OK, so for any subset of the variables.",
                    "label": 0
                },
                {
                    "sent": "So instead of using marginal I will use this.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the notation very simple.",
                    "label": 0
                },
                {
                    "sent": "But one thing that's interesting with this notation.",
                    "label": 1
                },
                {
                    "sent": "Is that if you multiply the way?",
                    "label": 0
                },
                {
                    "sent": "The weights with this parameter you get 0, so that's a really simple equation that will allow us to do a lot actually.",
                    "label": 0
                },
                {
                    "sent": "So before talking about that, here is an example of my notation.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a marginal distribution, P of XII have Q of X, which is always 0.",
                    "label": 0
                },
                {
                    "sent": "So here for two variables Q of XIXJ simply the probability minus the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "So it kind of measures the independence.",
                    "label": 0
                },
                {
                    "sent": "Between the variables and here is the expression for three variables.",
                    "label": 1
                },
                {
                    "sent": "So if we have a distribution P. If it's given, then we can also observe that.",
                    "label": 0
                },
                {
                    "sent": "So there is no parameter involved.",
                    "label": 0
                },
                {
                    "sent": "And so this Q is actually exactly the.",
                    "label": 0
                },
                {
                    "sent": "Correspondes really well to the P&R new parameterisation 'cause I get the same type of some type of expression.",
                    "label": 0
                },
                {
                    "sent": "With this additional equation, that's going to turn out to be very interesting.",
                    "label": 1
                },
                {
                    "sent": "So with this equation, actually we see that the weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is normal is to some pictures from column some here these vectors.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So once you know that and if you have N -- 1 vectors where M is the number of mixture components.",
                    "label": 0
                },
                {
                    "sent": "So if you have this, if you have some vectors like this then you can write an equation like this that basically says that W is a normal to the space.",
                    "label": 1
                },
                {
                    "sent": "Spanned by these vectors.",
                    "label": 0
                },
                {
                    "sent": "And if if this is normal, if you want some its component, it's different from zero, then you can scale it.",
                    "label": 0
                },
                {
                    "sent": "And then you can obtain your.",
                    "label": 0
                },
                {
                    "sent": "You're a W from these.",
                    "label": 0
                },
                {
                    "sent": "From these parameters, new parameters.",
                    "label": 0
                },
                {
                    "sent": "With the vectors a basis.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This so if you have some value XI have some random variable.",
                    "label": 0
                },
                {
                    "sent": "So AT of X, XI is simply the probability of observing XI in the mixture component denoted by T minus the marginal probability.",
                    "label": 0
                },
                {
                    "sent": "So I'm just given one value of some random variable.",
                    "label": 0
                },
                {
                    "sent": "I consider the vector T is the index so exactly here.",
                    "label": 0
                },
                {
                    "sent": "OK, same for W. It's the vector.",
                    "label": 0
                },
                {
                    "sent": "Of a mixture weights and when you multiply them, they're easy to see.",
                    "label": 0
                },
                {
                    "sent": "If you have some here.",
                    "label": 0
                },
                {
                    "sent": "With the W then you get the mixture and then you get marginal, so it's zero.",
                    "label": 0
                },
                {
                    "sent": "It's very easy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then you can deduce W from urn.",
                    "label": 0
                },
                {
                    "sent": "These vectors.",
                    "label": 0
                },
                {
                    "sent": "So this is just some notation here.",
                    "label": 0
                },
                {
                    "sent": "It's a matrix with the column.",
                    "label": 0
                },
                {
                    "sent": "Cones are given and this notation is.",
                    "label": 0
                },
                {
                    "sent": "I remove the teeth line.",
                    "label": 0
                },
                {
                    "sent": "So we had some.",
                    "label": 0
                },
                {
                    "sent": "Victorial interpretation.",
                    "label": 0
                },
                {
                    "sent": "But it's it's simply the normal.",
                    "label": 0
                },
                {
                    "sent": "And I just want to ensure that I can normalize it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So a second result that really is derived from our first normal equations is.",
                    "label": 0
                },
                {
                    "sent": "I found a polynomial that's of degree M. Whose roots are exactly my parameters.",
                    "label": 0
                },
                {
                    "sent": "So in the case of equal to, so it's a case that's already been solved.",
                    "label": 0
                },
                {
                    "sent": "By other people.",
                    "label": 0
                },
                {
                    "sent": "So you obtain this if you write this polynomial in S. Then in fact, it's factorizes like this, so you can find the roots.",
                    "label": 0
                },
                {
                    "sent": "Everything is observable here.",
                    "label": 0
                },
                {
                    "sent": "For Amical three, you have a similar type of polynomial.",
                    "label": 0
                },
                {
                    "sent": "It's kind of complicated.",
                    "label": 0
                },
                {
                    "sent": "What you have to look at is the leading coefficient of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "Here, here, so in the general case.",
                    "label": 0
                },
                {
                    "sent": "You have one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ali Nomial, whose leading coefficient is the determinant of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And you have other terms here and there, quite complicated to write there.",
                    "label": 0
                },
                {
                    "sent": "Just sum of sums of determinants of other matrices of the same dimension.",
                    "label": 1
                },
                {
                    "sent": "OK, and so again.",
                    "label": 0
                },
                {
                    "sent": "The roots of this polynomials are the parameters were looking for.",
                    "label": 1
                },
                {
                    "sent": "So of course to be able to write something like that, you need to have.",
                    "label": 0
                },
                {
                    "sent": "Sufficient you need to be able to write this exactly, so you need to have in a variables to be able to write this.",
                    "label": 0
                },
                {
                    "sent": "And here of course you cannot have.",
                    "label": 0
                },
                {
                    "sent": "Two values of the same variable 'cause it wouldn't make sense.",
                    "label": 0
                },
                {
                    "sent": "You cannot write probability to observe.",
                    "label": 0
                },
                {
                    "sent": "Exercise is equal to 0 and XI is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So you have constraints to be able to write these polynomials.",
                    "label": 0
                },
                {
                    "sent": "In terms of.",
                    "label": 0
                },
                {
                    "sent": "How the observable variables are and there are cardinalities.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First big result.",
                    "label": 0
                },
                {
                    "sent": "And my other big result is this equation again.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here again you consider a set of values U one to U N -- 1 and minus one values and you consider here some number of other values.",
                    "label": 0
                },
                {
                    "sent": "X1X key?",
                    "label": 0
                },
                {
                    "sent": "So if you write this equation, you actually have a polynomial in these variables and something that's observable.",
                    "label": 0
                },
                {
                    "sent": "To calculate determinant, it's easy.",
                    "label": 0
                },
                {
                    "sent": "By signing on this column.",
                    "label": 0
                },
                {
                    "sent": "And then here on the other side, you have one determinant that's.",
                    "label": 0
                },
                {
                    "sent": "Only consists of observable variables again.",
                    "label": 0
                },
                {
                    "sent": "So you have simple constraints on the parameters.",
                    "label": 0
                },
                {
                    "sent": "And it's actually.",
                    "label": 0
                },
                {
                    "sent": "It was really hard to get to this result because the original problem is all the mixtures components are mixed.",
                    "label": 0
                },
                {
                    "sent": "And here I obtained something that's very nice in my eyes.",
                    "label": 0
                },
                {
                    "sent": "And once again you have the same polynomial here.",
                    "label": 0
                },
                {
                    "sent": "That is, the leading coefficient of our.",
                    "label": 0
                },
                {
                    "sent": "Previous pulling over the same determinant, that's the leading coefficient over previous polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one example, if for my product here on these values I just use one value and I simply have a linear equation.",
                    "label": 0
                },
                {
                    "sent": "And here this term is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So I obtained this.",
                    "label": 0
                },
                {
                    "sent": "Another example, if M is equal to three, so if I have three components.",
                    "label": 0
                },
                {
                    "sent": "And I use this.",
                    "label": 0
                },
                {
                    "sent": "These values are the same as here.",
                    "label": 0
                },
                {
                    "sent": "That's good 'cause I obtained one.",
                    "label": 0
                },
                {
                    "sent": "I can obtain one, obtain an equation of polynomial equation linking these two coefficient so I can express 1 as a function of the other.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so um.",
                    "label": 0
                },
                {
                    "sent": "I have all these determinants and actually.",
                    "label": 0
                },
                {
                    "sent": "That's a nice decomposition.",
                    "label": 0
                },
                {
                    "sent": "So if you consider sets of values.",
                    "label": 1
                },
                {
                    "sent": "Like this and you write this determinant.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you look at the parameters, so if these distribution Q is a night vision distribution with M classes, then it will be you can write it like this.",
                    "label": 0
                },
                {
                    "sent": "So it's simply the product of the weights time, a determinant time, another determinant.",
                    "label": 0
                },
                {
                    "sent": "Of this matrix.",
                    "label": 0
                },
                {
                    "sent": "So with this we can write.",
                    "label": 0
                },
                {
                    "sent": "This determinant?",
                    "label": 0
                },
                {
                    "sent": "So we'll simply have the product.",
                    "label": 0
                },
                {
                    "sent": "Of the weights determinant like this and the same determinants with EU instead of the visa.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When you look at these.",
                    "label": 0
                },
                {
                    "sent": "When you look at this, it's nice because if you have one.",
                    "label": 0
                },
                {
                    "sent": "If you have one distribution that's you can represent it with M classes, night vision distribution, but you write this with mplus, one class.",
                    "label": 0
                },
                {
                    "sent": "Then you can parameterise your distribution with one weight equal to 0, so you have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some implicit equations like this.",
                    "label": 1
                },
                {
                    "sent": "But you can just use for example for model selection.",
                    "label": 0
                },
                {
                    "sent": "I think that's one potential application for that.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the.",
                    "label": 0
                },
                {
                    "sent": "At this determinant.",
                    "label": 0
                },
                {
                    "sent": "You see, you have lots of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "For example, in the choice of V1.",
                    "label": 0
                },
                {
                    "sent": "It doesn't appear.",
                    "label": 0
                },
                {
                    "sent": "Here and so you can write lots of lots of equations like that, but actually they're all the same because you have some.",
                    "label": 0
                },
                {
                    "sent": "Some condition here that tells you that they're just going to be multiples of.",
                    "label": 0
                },
                {
                    "sent": "The others.",
                    "label": 1
                },
                {
                    "sent": "So we have to be careful which equations are independent.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are my main results.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's talk about some applications, so the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Easiest application I see from that is OK. We can have one distribution.",
                    "label": 0
                },
                {
                    "sent": "We can compute the parameters so well we can.",
                    "label": 0
                },
                {
                    "sent": "Just if you want to do some learning with that, we can just have some distribution of observed frequencies and then we can try to do the same basically.",
                    "label": 1
                },
                {
                    "sent": "And it works in theory, but in practice doesn't really work.",
                    "label": 0
                },
                {
                    "sent": "But I'll talk about that a little later after.",
                    "label": 0
                },
                {
                    "sent": "So you can also have some sufficient conditions for parameter identifiability.",
                    "label": 1
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "I will also talk a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "But if you can solve your equation, then you can talk about the identifiability of the model, or at least you can identify some conditions where it's probably won't be identifiable.",
                    "label": 0
                },
                {
                    "sent": "And so this was my original motivation.",
                    "label": 1
                },
                {
                    "sent": "If you want to compute the analytical synthetic approximation to the marginal likelihood, than one method that uses the results of Watanabe, we had some previous talk was about that too.",
                    "label": 0
                },
                {
                    "sent": "Or is related.",
                    "label": 0
                },
                {
                    "sent": "Then you need to compute the preimage of distribution.",
                    "label": 0
                },
                {
                    "sent": "And you need to pay attention to the shape of this set.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a first step.",
                    "label": 0
                },
                {
                    "sent": "It has been it's very easy to do in the case where you have two hidden classes equal to.",
                    "label": 0
                },
                {
                    "sent": "Gets really complicated in my experience when you have more components.",
                    "label": 1
                },
                {
                    "sent": "And also my implicit equations are probably relevant for model selection, in particular in learning of hidden causes causes, so you have some.",
                    "label": 0
                },
                {
                    "sent": "Tetra Delta constraints that are used in a.",
                    "label": 0
                },
                {
                    "sent": "To learn hidden causes and probably mostly with the coaching variables.",
                    "label": 0
                },
                {
                    "sent": "But if you also have some similar ones.",
                    "label": 0
                },
                {
                    "sent": "In with this crew.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variables or the results I've seen are with.",
                    "label": 0
                },
                {
                    "sent": "Again, the case M Equal 2 and they this is can be seen as a generalization of these results.",
                    "label": 0
                },
                {
                    "sent": "These equations.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now if I want to focus more on computing the.",
                    "label": 0
                },
                {
                    "sent": "The preimage in details in my results.",
                    "label": 0
                },
                {
                    "sent": "So I have here very important assumption, so I have to be able to find.",
                    "label": 0
                },
                {
                    "sent": "Three sets of values like this.",
                    "label": 1
                },
                {
                    "sent": "And I have to be able to write these determinants so it impose constraints on these sets of values, 'cause I have to be able like this variable.",
                    "label": 0
                },
                {
                    "sent": "T1 must not be the same as variable you want.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So if constraint on the number of variables and their cardinality again and then I require that these three determinants are different from zero basically so I can solve my polynomials reitzes correspond to leading coefficient of my polynomial.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you look at the determinant, these are.",
                    "label": 0
                },
                {
                    "sent": "These three are equation of determinants different from zero.",
                    "label": 0
                },
                {
                    "sent": "Basically amounts to these three conditions.",
                    "label": 0
                },
                {
                    "sent": "So, Dion the product of the weight, must be different from zero.",
                    "label": 0
                },
                {
                    "sent": "This determinant must be different from zero, so it implies that these vectors must be linearly independent.",
                    "label": 0
                },
                {
                    "sent": "And so on for all three vectors, all three matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, and then in this case, while we can just solve our equation, this may not be a good idea to use this equation like this if you want to do it in practice, because it's not very stable numerically, but since it's normal it's there are better methods to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I have.",
                    "label": 0
                },
                {
                    "sent": "These vectors I can compute the weights.",
                    "label": 0
                },
                {
                    "sent": "OK, now I can simply use one of my first result here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or one simple version of my other results.",
                    "label": 0
                },
                {
                    "sent": "I have one linear equation between all these.",
                    "label": 0
                },
                {
                    "sent": "All these values.",
                    "label": 0
                },
                {
                    "sent": "So OK, so if I suppose I know this one.",
                    "label": 0
                },
                {
                    "sent": "Then I can solve everything just like this.",
                    "label": 0
                },
                {
                    "sent": "So it's simply you compute inverse matrix where you don't even have to invert it, but you just solve some system linear system and then you just multiply it by some other.",
                    "label": 0
                },
                {
                    "sent": "Some other vector and boom, you get all the other coefficients.",
                    "label": 0
                },
                {
                    "sent": "So there is a little you have to be careful 'cause here your X must be distinct from V, otherwise you cannot write these equations.",
                    "label": 0
                },
                {
                    "sent": "But since we made enough assumptions.",
                    "label": 0
                },
                {
                    "sent": "Like we require that this determinant determinant of this matrix is different from zero, but we also have other determinant, so it all works out.",
                    "label": 0
                },
                {
                    "sent": "So now the problem reduces to finding the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "These vectors.",
                    "label": 0
                },
                {
                    "sent": "So OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here I don't really have a good solution to do that.",
                    "label": 0
                },
                {
                    "sent": "But you still have my polynomial.",
                    "label": 0
                },
                {
                    "sent": "And with my polynomial I can find the roots.",
                    "label": 1
                },
                {
                    "sent": "So basically I can find these sets.",
                    "label": 1
                },
                {
                    "sent": "But these are roots, so they're not ordered, so I can say, OK, I cannot say this.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to the first class, and this one corresponds to the first class two.",
                    "label": 0
                },
                {
                    "sent": "I can't do that, they're just real numbers, and I can distinguish them.",
                    "label": 0
                },
                {
                    "sent": "And also when you want to compute the parameters map to distribution, you have some trivial non identifiable E in that you can just permute all the.",
                    "label": 0
                },
                {
                    "sent": "Although you can just relabel your states of your hidden variable.",
                    "label": 0
                },
                {
                    "sent": "So you can just.",
                    "label": 0
                },
                {
                    "sent": "To get rid of that, you just fix some arbitrary order on the 11 set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so it's basic ideas.",
                    "label": 0
                },
                {
                    "sent": "OK, well we fix one ordering.",
                    "label": 0
                },
                {
                    "sent": "Then we can just try all of them.",
                    "label": 0
                },
                {
                    "sent": "And then one OK, so we obtain all the parameters so we can test.",
                    "label": 0
                },
                {
                    "sent": "Oh are the parameters we obtain or do they sum to one or they positive and so on?",
                    "label": 0
                },
                {
                    "sent": "So you can?",
                    "label": 0
                },
                {
                    "sent": "And if so then you can just test.",
                    "label": 1
                },
                {
                    "sent": "Are they map to the distribution we given?",
                    "label": 1
                },
                {
                    "sent": "It's really naive way to do it 'cause there are like a lot of parameters to test.",
                    "label": 1
                },
                {
                    "sent": "So one side observation, if.",
                    "label": 0
                },
                {
                    "sent": "OK, we have the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And we are are three.",
                    "label": 0
                },
                {
                    "sent": "Determinants are different from zero.",
                    "label": 1
                },
                {
                    "sent": "Well then we can see that if this is verified then we have at most a finite number of.",
                    "label": 0
                },
                {
                    "sent": "Parameters map to the distribution, which is, I think, an interesting result in its health.",
                    "label": 0
                },
                {
                    "sent": "It probably has some implications for the dimension of the model in that case.",
                    "label": 0
                },
                {
                    "sent": "Although I haven't worked out the details.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be a little less naive, you can use some other results I found.",
                    "label": 0
                },
                {
                    "sent": "Which basically is another constraint on the parameters.",
                    "label": 0
                },
                {
                    "sent": "So here, in a few words, this really looks like the original parameterisation, except I got rid of the weights.",
                    "label": 0
                },
                {
                    "sent": "So I have some equation.",
                    "label": 0
                },
                {
                    "sent": "And everything else is observable.",
                    "label": 0
                },
                {
                    "sent": "OK with some matrix again.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is I can just write this result.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this is for any set of values.",
                    "label": 0
                },
                {
                    "sent": "Again, we just want just 22 values for the value, which I know 'cause I ordered them recurly and another one so I can just test.",
                    "label": 0
                },
                {
                    "sent": "So instead of testing all the orderings at the same time, all the permutations at the same time, I just have one is fixed and I test all the permutation of the other vector.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this will reduce the complexity of your algorithm, but I still think it's not the right way to do it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So probably a better way to do it is to use other results, so I haven't done it yet because I only recently generalized them.",
                    "label": 0
                },
                {
                    "sent": "But as I told you, in the case M = 3, you can easily solve that and you also get some probably very interesting conditions under which you cannot.",
                    "label": 0
                },
                {
                    "sent": "Invert for solve polynomial.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so once you have these, any algorithm to invert parameters.",
                    "label": 0
                },
                {
                    "sent": "You can try to turn that into.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm to estimate parameters.",
                    "label": 0
                },
                {
                    "sent": "So as I told you, basically of the distribution of the observed frequencies, we just try to apply the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So of course, if you just have your, you just compute the pre image.",
                    "label": 0
                },
                {
                    "sent": "If the distribution that you observe is not exactly when distribution, then can expect to have some.",
                    "label": 0
                },
                {
                    "sent": "Empty pre image.",
                    "label": 0
                },
                {
                    "sent": "But actually all the procedures are described are continuous in some sense, so if you're.",
                    "label": 0
                },
                {
                    "sent": "Or they can be easily modified to be continuous.",
                    "label": 1
                },
                {
                    "sent": "So if you have your own unknown distribution and as you get more and more samples it will converge to the true distribution an by continuity.",
                    "label": 0
                },
                {
                    "sent": "Basically your estimate will also converge to the true parameter.",
                    "label": 1
                },
                {
                    "sent": "And so will it.",
                    "label": 0
                },
                {
                    "sent": "All works out actually.",
                    "label": 0
                },
                {
                    "sent": "So I was also testing parameters like free quality.",
                    "label": 0
                },
                {
                    "sent": "Is this parameter map to distribution and the easy way to the obvious way to change that is simply too?",
                    "label": 0
                },
                {
                    "sent": "Once you have a parameter, just look if it's minimizes the entropy, the relative entropy so decaled averaging through the observed distribution.",
                    "label": 1
                },
                {
                    "sent": "But in practice.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I told you already, it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So first you have an extremely fast growth of the complexity with the number of your M. So it's probably workable for lo M. I don't know, probably into 7 eight or something like that.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried it or haven't.",
                    "label": 0
                },
                {
                    "sent": "I don't have any.",
                    "label": 0
                },
                {
                    "sent": "Patient implementation to try that.",
                    "label": 0
                },
                {
                    "sent": "So, but the bigger problem is that the estimates are really numerically unstable.",
                    "label": 0
                },
                {
                    "sent": "So when you try to solve the polynomial, sometimes you'll get you will not even get real roots.",
                    "label": 0
                },
                {
                    "sent": "So what do you do in this case?",
                    "label": 0
                },
                {
                    "sent": "Well, you can do anything.",
                    "label": 0
                },
                {
                    "sent": "And so you will not have any answer 'cause you will not have any parameter that satisfies the constraints.",
                    "label": 0
                },
                {
                    "sent": "Asymptotically, it's supposed to work, but in practice isn't really work.",
                    "label": 0
                },
                {
                    "sent": "But so and also we have lots of degrees of freedom in the choice of TU&V.",
                    "label": 1
                },
                {
                    "sent": "If you have variables.",
                    "label": 0
                },
                {
                    "sent": "So it's probably important how you pick those.",
                    "label": 0
                },
                {
                    "sent": "And also you could probably try to somehow pull data 'cause you only use small part of the data.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing is that you only use Q like the function Q with the two or three variables.",
                    "label": 0
                },
                {
                    "sent": "So maybe you mean you don't need to have very.",
                    "label": 0
                },
                {
                    "sent": "It's probably easier to estimate.",
                    "label": 0
                },
                {
                    "sent": "Correctly, when you when you have small number of variables.",
                    "label": 1
                },
                {
                    "sent": "OK, and in practice it's really not competitive with the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's Maura.",
                    "label": 0
                },
                {
                    "sent": "Yeah, practically, I don't think it has any value at this stage.",
                    "label": 0
                },
                {
                    "sent": "So once you have.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the algorithm, or a way to compute the preimage?",
                    "label": 0
                },
                {
                    "sent": "And Ivs distribution, and in some cases you can extend it to these type of trees or you have.",
                    "label": 0
                },
                {
                    "sent": "Here the leaves are observed and the internal nodes are hidden.",
                    "label": 0
                },
                {
                    "sent": "Because you have simple observation.",
                    "label": 0
                },
                {
                    "sent": "OK, if you look at these marginal on these three variables, then it's a.",
                    "label": 0
                },
                {
                    "sent": "It's small you're interested in, so it's a latent class model naive Bayesian distribution.",
                    "label": 1
                },
                {
                    "sent": "If you look at X2X3 and X4, then the same you can.",
                    "label": 0
                },
                {
                    "sent": "It's also an advising distribution, and so on, so you can.",
                    "label": 1
                },
                {
                    "sent": "If you estimate the parameters of these three distribution, then you can deduce all the parameters associated to these links, and it's also possible to estimate the parameters associated to those links.",
                    "label": 0
                },
                {
                    "sent": "It's just some linear algebra.",
                    "label": 0
                },
                {
                    "sent": "I haven't figured out all the details or generalize it properly, but.",
                    "label": 0
                },
                {
                    "sent": "It would be really nice if we could compute the preimage exactly, not just with some algorithm as I presented.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude, I think I presented some very simple polynomial equations.",
                    "label": 1
                },
                {
                    "sent": "So when you try to actually invert that, you quickly get very very complicated polynomials, and I think the results are obtained are really nice because they're really simple.",
                    "label": 0
                },
                {
                    "sent": "So I talked to you about how to apply that to a.",
                    "label": 1
                },
                {
                    "sent": "Estimate parameters for the data.",
                    "label": 0
                },
                {
                    "sent": "I don't think that's the right thing to do, but you also also get some nice implicit equations and I think they can be used to learn hidden causes from data or maybe.",
                    "label": 1
                },
                {
                    "sent": "Maybe decide which.",
                    "label": 0
                },
                {
                    "sent": "Which number of hidden classes are, if you have a distribution but you don't know which one is?",
                    "label": 0
                },
                {
                    "sent": "And you can try to test for that too.",
                    "label": 0
                },
                {
                    "sent": "Well, that's all.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you know that these are all?",
                    "label": 0
                },
                {
                    "sent": "Apartment.",
                    "label": 0
                },
                {
                    "sent": "Hold on.",
                    "label": 0
                },
                {
                    "sent": "These are all powerful.",
                    "label": 0
                },
                {
                    "sent": "All the possible.",
                    "label": 0
                },
                {
                    "sent": "Oh I I I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know, just have some very limited results.",
                    "label": 0
                },
                {
                    "sent": "But I think it's a good.",
                    "label": 0
                },
                {
                    "sent": "It's a good clue.",
                    "label": 0
                },
                {
                    "sent": "This determinant is a good clue into which direction or conditions where I can't solve my equations are.",
                    "label": 0
                },
                {
                    "sent": "What motivated their choice in the change of coordinates?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't really have any.",
                    "label": 0
                },
                {
                    "sent": "There is no motivation.",
                    "label": 0
                },
                {
                    "sent": "Basically I just all the work was done by induction.",
                    "label": 0
                },
                {
                    "sent": "It's OK, you have some.",
                    "label": 0
                },
                {
                    "sent": "You have some results for M Equal 2.",
                    "label": 0
                },
                {
                    "sent": "Then you can try to work it out.",
                    "label": 0
                },
                {
                    "sent": "By hand, basically you're with some.",
                    "label": 0
                },
                {
                    "sent": "Computer try to generalize to three variables and then you get some equation and I just.",
                    "label": 0
                },
                {
                    "sent": "I just generalize it by hand.",
                    "label": 0
                },
                {
                    "sent": "So there is no insight.",
                    "label": 0
                }
            ]
        }
    }
}