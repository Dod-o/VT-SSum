{
    "id": "lrx4rz3koybhmmcky4ssghsmfa45jhif",
    "title": "Graph Embedding in Vector Spaces by Means of Prototype Selection",
    "info": {
        "author": [
            "Kaspar Riesen, University of Bern"
        ],
        "published": "July 12, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Preprocessing",
            "Top->Computer Science->Machine Learning->Structured Data"
        ]
    },
    "url": "http://videolectures.net/gbr07_riesen_gevs/",
    "segmentation": [
        [
            "OK, good morning everyone and welcome to my talk about graph embedding in real vector spaces by means of prototype selection."
        ],
        [
            "So here's the outline of my talk.",
            "First of all, I will talk about the general.",
            "Graph matching paradigm of graph edit distance.",
            "Since this is a ascential part of our proposal.",
            "And then the main part of my talk will be about the.",
            "Proceed the procedure of graph embedding by means of prototype selection and graph edit distance.",
            "A crucial point in our proposal here is that we have to select prototypes, so I will just discuss different prototype selection methods.",
            "Finally I will show some experimental results we have achieved with our proposal and finally draw conclusion."
        ],
        [
            "So the main contribution of this talk here, this work here is that we want to bridge the gap between the domain of graph based and feature based object representation.",
            "This means we want to map the graphs explicitly into real vector space.",
            "And in fact, our proposal can be seen as a new graph kernel.",
            "Since we want to benefit from both the high representational power of graphs and the large amount of algorithms applicable to feature vectors."
        ],
        [
            "So let's start with a brief introduction into graph edit distance graph it distance basically defines the dissimilarity between two graphs by the minimum amount of distortion that is needed to transform one graph into another.",
            "So these distortions are given by edit operations, which consists of deletions, insertions and substitutions of both nodes and edges.",
            "So then sequence of such edit Operation Edit Operations which transforms a graph into another is called Edit Path.",
            "And we have to introduce cost functions, which measures the strength of a given distortion, and then consequently the added distance is given by the minimum cost edit path between 2:00."
        ],
        [
            "Two graphs.",
            "So here's the example we have seen twice on this conference, so we delete, delete some edges, delete notes, insert notes, insert edges, and substitute possibly some nodes or edges.",
            "So the benef."
        ],
        [
            "Data Graph Edit Distance is its high flexibility.",
            "That is graph edit distance allows allows us to handle any label alphabet so the labels can be out of a set of integers.",
            "It can be the set of reals.",
            "We can label both the nodes and edges with feature vectors or discrete symbols, or a combination of two or more alphabets."
        ],
        [
            "So we can conclude that graphs provided us with a very powerful and also very flexible representation formalism.",
            "But the major drawback of graphs is that the domain of graphs contains almost no mathematical structure.",
            "So all the basic mathematic operations like computing sums or in product or means are not defined in a standardized way.",
            "So although the graph edit Distance provides us with a very general dissimilarity measure directly applicable in the domain of graphs, this is still not sufficient for most of the standard pattern recognition algorithms.",
            "So and in fact that distance based graph matching, which is applied directly in the domain of graphs, is basically limited to two nearest neighbor classification.",
            "So in recent years and emerging interest can be observed in kernel methods and kernel kernel machines.",
            "In especially graph kernels offer a very elegant solution to this limitation of graph matching, and in fact our proposed method can be seen as a Gray."
        ],
        [
            "Colonel so as I said before, our approach will benefit from both the high representational power of graphs, but also the large amount of algorithms for classification in vector spaces.",
            "So the key idea is that we want to map our graphs embeddable graphs.",
            "Basically, into the N dimensional real vector space and we use prototype selection and edit distance and consequently if you do so, the lack of suitable methods for graph classification is instantly eliminated.",
            "So Please note that this idea was originally developed for this similarity.",
            "Embeddings of feature vectors, and we adapted here to the domain of graphs to map the graphs into a vector space."
        ],
        [
            "So here's the basic idea of graph embedding.",
            "So let us assume we have given a graph, said she.",
            "Then we have a graph, the similarity model.",
            "This is in our case graph edit distance, but if you don't like graph edit distance, you can use any other this distance model of this similarity model.",
            "Then we have to select a prototype set P and then the mapping from the domain of graphs to a real vector spaces defined by this function Phi here.",
            "So here's an example.",
            "Let us assume this is the graph that G, which is often labeled.",
            "We select some prototypes marked with a circle here and then we have an input graph.",
            "Here this input graph can come out of this set itself, but it can also come from any other set.",
            "So we can use the test set, validation set or whatever, and we compute Now the edit distance of this input graph to all of these prototypes.",
            "And if we have picked before an prototype, still this will lead to unreal numbers here.",
            "And these unreal numbers can be interpreted as a real vector."
        ],
        [
            "So again, the graph embedding is a mapping from the domain of graphs into a real vector space where the individual dimensions of directed space represents the distances of this input graph to the prototypes.",
            "So, and based on this graph embedding procedure, we can define a valid graph kernel.",
            "What you have to do is you have to compute the standard dot product for instance in the resulting vector space."
        ],
        [
            "OK, for further insight into this procedure.",
            "Our method, we look at the Euclidean distance of two graph Maps in the resulting vector space.",
            "OK, this is here.",
            "We reformulate the Euclidean distance in terms of standard dot product.",
            "Only then we expressed the dot product with sums and finally we see that the Euclidean distance of two graph Maps in the resulting vector space is equal to the sum of squared differences between the edit distances of both graphs to all the prototypes.",
            "OK."
        ],
        [
            "So to make this point clear, here's a small example.",
            "Let us assume we have two very similar graphs here, Chienti prime and we have only one prototype selected.",
            "So it is most likely since they are very similar, they will have very similar edit distance to this prototype, and that's the difference will be small and so also the squared difference will be small and the square difference is equal to the Euclidean distance in the resulting vector space so."
        ],
        [
            "We assume a second prototype.",
            "Again, it is very likely that the distance between both graphs to this prototype are very similar, so the difference will be small and the second advent here, which is the squared difference, will be small too and."
        ],
        [
            "We can continue like this and add a third, 4th and so and so on prototype and finally conclude that the more similar Ciancia Prime are the smaller will be their Euclidean distance in the vector space."
        ],
        [
            "OK, this is another example.",
            "We have two dissimilar graphs G&G Prime and here it is most likely that they will have very different edit distance to this prototype here.",
            "So the difference will be large and also the squared difference will be large, which corresponds exactly to the Euclidean distance in the resulting vector space."
        ],
        [
            "So and again here when this, if we assume the second prototype, the difference here will be large.",
            "Second addend, which is the squared difference will be large."
        ],
        [
            "And we can continue like this and conclude finally, that dissimilar graphs Chi and Qi prime have large Euclidean distance in the vector space."
        ],
        [
            "So I think as we have seen now, our method depends partly on an appropriate choice of the prototype set P. So we have to select a prototype set that leads to a good performance of the resulting classifier in the vector space.",
            "So intuitively we should avoid redundancies in terms of selection of similar graphs.",
            "So here in this work we propose five different prototype selection methods.",
            "And all of them can be applied class wise or class independent.",
            "So class wise means we select the prototypes individually for each of the K different classes and class independent means that we select the graphs over the whole graph set at once.",
            "And of course one can think of many other prototype selection methods.",
            "Others and are we discuss here and we can even think of combination of two different prototype selection.",
            "Method, but however the intention remains the same for all the prototype selection methods we want to select the good.",
            "Prototype cell which leads to good performing classified."
        ],
        [
            "OK, I will now talk about about their prototype selection methods.",
            "First is a random prototype selected as.",
            "The name says, it selects randomly and prototypes from the graphs.",
            "It's G and of course this provides no the terministic graph embedding.",
            "So each time you make the graph embedding this will lead to another rectorial description of the same underlying graph and but however it results in there quite good.",
            "Solution over the whole graph set.",
            "Here's an example.",
            "Of course, these are vectors.",
            "2 dimensional vectors and the bullet points are.",
            "The selected prototypes randomly."
        ],
        [
            "Next prototype select.",
            "We've implemented with this centers prototype selector.",
            "For this prototype select reused median of a graph set and the median of graph set is the graph whose sum of distances to all other graphs in this set is minimal.",
            "So we're trying to minimizing this some here.",
            "So then the set of prototypes is iteratively constructed by selecting medians out of the graph set minus the already selected prototypes.",
            "This is what happens here.",
            "So as we see in the picture, this prototype selected neither avoids redundancy, is not distribute the prototypes uniformly.",
            "So another idea that is not mentioned here with that would be that of Borders prototype selected.",
            "So for this we would use instead of the median.",
            "We reduced the marginal of a graph set which maximizes this here and then we would select the marginal graphs out of the graphs at minus the already selected prototypes and this would lead to prototype selected from the border."
        ],
        [
            "The next prototype selector is the targets for prototype selector, which is a prototype select method which first select a graph located in the center of the graph set.",
            "For instance, the set median graph.",
            "Then we are looking for the graph which is located the furthest away from this center graph, and then we're dividing this.",
            "The maximum distance from the center graph to the border graph into N -- 1 partitions with equal interval size.",
            "And then we select the N -- 2 graphs that are located the nearest to those interval borders in terms of edit distance of course and we see OK, this will distribute the graphs more or less uniformly over there said.",
            "But for selecting these prototypes we consider only one prototype selected so far.",
            "So this is the center graph and."
        ],
        [
            "In the next prototype selected, which is the spanning prototype selector, we.",
            "We look for all the already selected prototypes, so the first prototype select selector select it is the set median graph of G and then each additional prototype selected out of this graph set is the graph the furthest away from the already selected prototypes.",
            "So we're considering all prototypes selected so far, so we're maximizing the minimum distance through the prototypes.",
            "As we see in the picture, this is prototype selective.",
            "Typically picks of prototypes also from the border of a graph set.",
            "This is because we are trying to maximize this mini."
        ],
        [
            "Distance.",
            "And the last prototype selector is the case centers prototype selector which tries to select the graphs that are in a center of densely populated areas in the in the graph set.",
            "So what we're basically doing is we apply K medians clustering on the graph set and we said K equal to the number of prototypes that we want.",
            "We set K equal to N and then the median after the clustering has terminated, we select the median of each of the clusters.",
            "'s prototype and this picture here looks quite similar to the picture before, but if you."
        ],
        [
            "Compare it.",
            "We see."
        ],
        [
            "That this prototype selector.",
            "Avoid selecting grass from the border."
        ],
        [
            "OK, before we consider now experimental results, I want to give you a brief method summary.",
            "So what we are proposing here is a graph embedding method, a very general graph embedding method to transform any graphs into a real vector space.",
            "And this method of course enables the access to a wide range of pattern recognition procedures based on feature vectors applicable to graphs.",
            "So the intention of the now following experiments is that we want to show that certain classification tasks can be better solved with methods that use the resulting vectorial descriptions rather than the original graph representation.",
            "So we use four different classifiers.",
            "So first of all we use a KNN classifier.",
            "I applied directly on the graphs with graph edit distance.",
            "We use an SVM classifier in conjunction with a trivial graph kernel.",
            "So we use the negative distance and then we use two classifiers in the resulting vector space.",
            "The 1st is a KNN classifier, two in the vector space and then this is our main system.",
            "This is support vector machine trained on the resulting pictorial descriptions."
        ],
        [
            "OK, here's the experimental setup in each of our experiments we make use of three disjoint graph sets, a training set, validation set and test set.",
            "So the validation set is used to determine optimal meta parameters for graph embedding and classification.",
            "So two embedding parameter consists of the number of prototypes that we want to select.",
            "This means the dimensionality of the resulting vector space and also the best performing embedding method.",
            "So we have seen five different embedding methods and we have to determine the best one for a given graph set and we have several classification parameters.",
            "This is the K for the KNN classifier.",
            "And different SVM parameters too.",
            "And of course the parameter values and also the embedding methods that resulted in the lowest classification error were then applied to to the independent test set."
        ],
        [
            "OK, I think you know this data set already selected data set which consists of graphs representing capital letter line drawings out of 15 classes.",
            "To summarize, once more, we first manually constructed.",
            "Prototype line drawings of each letter.",
            "Then we apply different distortion operators on them.",
            "And finally we transform these drawings into graphs by representing lines by edges and ending points of lines by nodes.",
            "So and as you see, we have different distortion levels and this is the result.",
            "These are the results on a higher distortion level, so we see the K&N in the graph domain.",
            "These are the results with the trivial graph kernel and the SVM.",
            "Then we have the KNN director space and this is our main system.",
            "The embedding method in conjunction with support vector machine and we see that our embedding method outperforms all the other systems with a statistically significant results."
        ],
        [
            "This data set you know also this is the image data set which consists of graphs representing color images out of five classes.",
            "So we first segment images into regions and represent these regions by region adjacency graphs.",
            "And here again we see that our embedding method outperforms the KNN on in the vector space and also the K and then in the graph domain with statistically significant results.",
            "But the travel graph kernel is better than the embedding method but not statistically significant.",
            "So we are happy."
        ],
        [
            "And next data set this to fingerprint data set with four classes and we transform these fingerprints into graphs by extracting characteristic regions out of the fingerprints and then transformed this region center at rebooted graphs and hear the results are evident again.",
            "So the embedding method, in conjunction with the support vector machine outperforms all the other systems with a statistically significant."
        ],
        [
            "And also in the last data set which is the molecule data set.",
            "Which consists of graphs representing molecules out of two classes.",
            "And in this data set we converted these molecules by representing atoms's notes and bonds searches.",
            "And here again, results are the same.",
            "The embedding method with the aseum outperforms all the other systems with statistical significance."
        ],
        [
            "So what I've not mentioned so far discussed is which prototype selector and which dimensionality we have used for for these datasets, and we have to say that the prototype selector and also the number of prototypes depends hardly on the data set.",
            "So for instance, if we look at the letter data set, we see that.",
            "The case centers prototype selector class wise or the target sphere classifiers performed the best.",
            "It depends on which distortion level you you apply.",
            "Your prototype selection methods and also the number of prototypes varies from 60 up to 450 prototypes.",
            "And on the other data set, the image was the target sphere with 50 class independent on the fingerprint.",
            "It was also the target Sphere prototype selected class wise with 300 prototypes on the molecule data set.",
            "It was the spanning prototype selector with 150 prototypes.",
            "So we have to conclude that there is neither best prototype select nor universally optimal feature space dimensionality.",
            "So we have to validate both on a validation set but however.",
            "Can report that from a certain number of prototypes.",
            "It it depends not so much which prototype select we use.",
            "So given a certain number of prototypes, the differences among these prototypes selected or only marginal.",
            "So if you don't want to validate it seems to be a good idea to use a prototype selector which tries to distribute the prototypes over the whole graph set uniformly.",
            "And if class wiser class independent, we cannot say it is.",
            "In general.",
            "OK."
        ],
        [
            "So this was my talk.",
            "I will draw conclusions now.",
            "In this work here, we propose a very flexible method for mapping graphs explicitly into an N dimensional vector space.",
            "To descend, we have to select first and prototypes and then compute the graph edit distance from an input graph to all of these prototypes and these end distances can be interpreted as a real vector.",
            "This is the key idea, so we viewed the similarities to end prototypes as a vectorial, vectorial description of the graph.",
            "So we can say that in contrast with various other graph kernels, our method is applicable to any type of graph, and this is because we use graph edit distance and this allows us this high flexibility in our definition.",
            "And further key point is that our method can be applied not only in conjunction with support vector machines, so here it was applied with support vector machines.",
            "But however we could use any other statistical classifier like base classifier or neural network to do these graphs.",
            "So this is more powerful because we can apply it to classifiers which cannot be kernelized.",
            "OK, and from the results of our experiments we have seen that the SVM, in conjunction with the embedding method and the resulting vector space outperforms all the other approaches in the original graph domain.",
            "But it outperformed also the K&N in the vector space, so it seems that the sophisticated classifier given by the SVM in combination with the embedding method leads to a good classifier.",
            "OK, this was my talk.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good morning everyone and welcome to my talk about graph embedding in real vector spaces by means of prototype selection.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "First of all, I will talk about the general.",
                    "label": 0
                },
                {
                    "sent": "Graph matching paradigm of graph edit distance.",
                    "label": 0
                },
                {
                    "sent": "Since this is a ascential part of our proposal.",
                    "label": 0
                },
                {
                    "sent": "And then the main part of my talk will be about the.",
                    "label": 0
                },
                {
                    "sent": "Proceed the procedure of graph embedding by means of prototype selection and graph edit distance.",
                    "label": 1
                },
                {
                    "sent": "A crucial point in our proposal here is that we have to select prototypes, so I will just discuss different prototype selection methods.",
                    "label": 0
                },
                {
                    "sent": "Finally I will show some experimental results we have achieved with our proposal and finally draw conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main contribution of this talk here, this work here is that we want to bridge the gap between the domain of graph based and feature based object representation.",
                    "label": 1
                },
                {
                    "sent": "This means we want to map the graphs explicitly into real vector space.",
                    "label": 0
                },
                {
                    "sent": "And in fact, our proposal can be seen as a new graph kernel.",
                    "label": 0
                },
                {
                    "sent": "Since we want to benefit from both the high representational power of graphs and the large amount of algorithms applicable to feature vectors.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with a brief introduction into graph edit distance graph it distance basically defines the dissimilarity between two graphs by the minimum amount of distortion that is needed to transform one graph into another.",
                    "label": 1
                },
                {
                    "sent": "So these distortions are given by edit operations, which consists of deletions, insertions and substitutions of both nodes and edges.",
                    "label": 0
                },
                {
                    "sent": "So then sequence of such edit Operation Edit Operations which transforms a graph into another is called Edit Path.",
                    "label": 1
                },
                {
                    "sent": "And we have to introduce cost functions, which measures the strength of a given distortion, and then consequently the added distance is given by the minimum cost edit path between 2:00.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two graphs.",
                    "label": 0
                },
                {
                    "sent": "So here's the example we have seen twice on this conference, so we delete, delete some edges, delete notes, insert notes, insert edges, and substitute possibly some nodes or edges.",
                    "label": 0
                },
                {
                    "sent": "So the benef.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data Graph Edit Distance is its high flexibility.",
                    "label": 0
                },
                {
                    "sent": "That is graph edit distance allows allows us to handle any label alphabet so the labels can be out of a set of integers.",
                    "label": 1
                },
                {
                    "sent": "It can be the set of reals.",
                    "label": 0
                },
                {
                    "sent": "We can label both the nodes and edges with feature vectors or discrete symbols, or a combination of two or more alphabets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can conclude that graphs provided us with a very powerful and also very flexible representation formalism.",
                    "label": 0
                },
                {
                    "sent": "But the major drawback of graphs is that the domain of graphs contains almost no mathematical structure.",
                    "label": 1
                },
                {
                    "sent": "So all the basic mathematic operations like computing sums or in product or means are not defined in a standardized way.",
                    "label": 1
                },
                {
                    "sent": "So although the graph edit Distance provides us with a very general dissimilarity measure directly applicable in the domain of graphs, this is still not sufficient for most of the standard pattern recognition algorithms.",
                    "label": 1
                },
                {
                    "sent": "So and in fact that distance based graph matching, which is applied directly in the domain of graphs, is basically limited to two nearest neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "So in recent years and emerging interest can be observed in kernel methods and kernel kernel machines.",
                    "label": 0
                },
                {
                    "sent": "In especially graph kernels offer a very elegant solution to this limitation of graph matching, and in fact our proposed method can be seen as a Gray.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Colonel so as I said before, our approach will benefit from both the high representational power of graphs, but also the large amount of algorithms for classification in vector spaces.",
                    "label": 1
                },
                {
                    "sent": "So the key idea is that we want to map our graphs embeddable graphs.",
                    "label": 1
                },
                {
                    "sent": "Basically, into the N dimensional real vector space and we use prototype selection and edit distance and consequently if you do so, the lack of suitable methods for graph classification is instantly eliminated.",
                    "label": 0
                },
                {
                    "sent": "So Please note that this idea was originally developed for this similarity.",
                    "label": 0
                },
                {
                    "sent": "Embeddings of feature vectors, and we adapted here to the domain of graphs to map the graphs into a vector space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the basic idea of graph embedding.",
                    "label": 0
                },
                {
                    "sent": "So let us assume we have given a graph, said she.",
                    "label": 0
                },
                {
                    "sent": "Then we have a graph, the similarity model.",
                    "label": 0
                },
                {
                    "sent": "This is in our case graph edit distance, but if you don't like graph edit distance, you can use any other this distance model of this similarity model.",
                    "label": 0
                },
                {
                    "sent": "Then we have to select a prototype set P and then the mapping from the domain of graphs to a real vector spaces defined by this function Phi here.",
                    "label": 1
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Let us assume this is the graph that G, which is often labeled.",
                    "label": 0
                },
                {
                    "sent": "We select some prototypes marked with a circle here and then we have an input graph.",
                    "label": 0
                },
                {
                    "sent": "Here this input graph can come out of this set itself, but it can also come from any other set.",
                    "label": 0
                },
                {
                    "sent": "So we can use the test set, validation set or whatever, and we compute Now the edit distance of this input graph to all of these prototypes.",
                    "label": 0
                },
                {
                    "sent": "And if we have picked before an prototype, still this will lead to unreal numbers here.",
                    "label": 0
                },
                {
                    "sent": "And these unreal numbers can be interpreted as a real vector.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the graph embedding is a mapping from the domain of graphs into a real vector space where the individual dimensions of directed space represents the distances of this input graph to the prototypes.",
                    "label": 1
                },
                {
                    "sent": "So, and based on this graph embedding procedure, we can define a valid graph kernel.",
                    "label": 0
                },
                {
                    "sent": "What you have to do is you have to compute the standard dot product for instance in the resulting vector space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for further insight into this procedure.",
                    "label": 0
                },
                {
                    "sent": "Our method, we look at the Euclidean distance of two graph Maps in the resulting vector space.",
                    "label": 0
                },
                {
                    "sent": "OK, this is here.",
                    "label": 0
                },
                {
                    "sent": "We reformulate the Euclidean distance in terms of standard dot product.",
                    "label": 0
                },
                {
                    "sent": "Only then we expressed the dot product with sums and finally we see that the Euclidean distance of two graph Maps in the resulting vector space is equal to the sum of squared differences between the edit distances of both graphs to all the prototypes.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to make this point clear, here's a small example.",
                    "label": 0
                },
                {
                    "sent": "Let us assume we have two very similar graphs here, Chienti prime and we have only one prototype selected.",
                    "label": 0
                },
                {
                    "sent": "So it is most likely since they are very similar, they will have very similar edit distance to this prototype, and that's the difference will be small and so also the squared difference will be small and the square difference is equal to the Euclidean distance in the resulting vector space so.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We assume a second prototype.",
                    "label": 0
                },
                {
                    "sent": "Again, it is very likely that the distance between both graphs to this prototype are very similar, so the difference will be small and the second advent here, which is the squared difference, will be small too and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can continue like this and add a third, 4th and so and so on prototype and finally conclude that the more similar Ciancia Prime are the smaller will be their Euclidean distance in the vector space.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is another example.",
                    "label": 0
                },
                {
                    "sent": "We have two dissimilar graphs G&G Prime and here it is most likely that they will have very different edit distance to this prototype here.",
                    "label": 0
                },
                {
                    "sent": "So the difference will be large and also the squared difference will be large, which corresponds exactly to the Euclidean distance in the resulting vector space.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and again here when this, if we assume the second prototype, the difference here will be large.",
                    "label": 0
                },
                {
                    "sent": "Second addend, which is the squared difference will be large.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can continue like this and conclude finally, that dissimilar graphs Chi and Qi prime have large Euclidean distance in the vector space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think as we have seen now, our method depends partly on an appropriate choice of the prototype set P. So we have to select a prototype set that leads to a good performance of the resulting classifier in the vector space.",
                    "label": 1
                },
                {
                    "sent": "So intuitively we should avoid redundancies in terms of selection of similar graphs.",
                    "label": 0
                },
                {
                    "sent": "So here in this work we propose five different prototype selection methods.",
                    "label": 1
                },
                {
                    "sent": "And all of them can be applied class wise or class independent.",
                    "label": 0
                },
                {
                    "sent": "So class wise means we select the prototypes individually for each of the K different classes and class independent means that we select the graphs over the whole graph set at once.",
                    "label": 0
                },
                {
                    "sent": "And of course one can think of many other prototype selection methods.",
                    "label": 0
                },
                {
                    "sent": "Others and are we discuss here and we can even think of combination of two different prototype selection.",
                    "label": 0
                },
                {
                    "sent": "Method, but however the intention remains the same for all the prototype selection methods we want to select the good.",
                    "label": 0
                },
                {
                    "sent": "Prototype cell which leads to good performing classified.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I will now talk about about their prototype selection methods.",
                    "label": 0
                },
                {
                    "sent": "First is a random prototype selected as.",
                    "label": 1
                },
                {
                    "sent": "The name says, it selects randomly and prototypes from the graphs.",
                    "label": 1
                },
                {
                    "sent": "It's G and of course this provides no the terministic graph embedding.",
                    "label": 1
                },
                {
                    "sent": "So each time you make the graph embedding this will lead to another rectorial description of the same underlying graph and but however it results in there quite good.",
                    "label": 0
                },
                {
                    "sent": "Solution over the whole graph set.",
                    "label": 0
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "Of course, these are vectors.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional vectors and the bullet points are.",
                    "label": 0
                },
                {
                    "sent": "The selected prototypes randomly.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next prototype select.",
                    "label": 0
                },
                {
                    "sent": "We've implemented with this centers prototype selector.",
                    "label": 1
                },
                {
                    "sent": "For this prototype select reused median of a graph set and the median of graph set is the graph whose sum of distances to all other graphs in this set is minimal.",
                    "label": 1
                },
                {
                    "sent": "So we're trying to minimizing this some here.",
                    "label": 1
                },
                {
                    "sent": "So then the set of prototypes is iteratively constructed by selecting medians out of the graph set minus the already selected prototypes.",
                    "label": 0
                },
                {
                    "sent": "This is what happens here.",
                    "label": 0
                },
                {
                    "sent": "So as we see in the picture, this prototype selected neither avoids redundancy, is not distribute the prototypes uniformly.",
                    "label": 0
                },
                {
                    "sent": "So another idea that is not mentioned here with that would be that of Borders prototype selected.",
                    "label": 0
                },
                {
                    "sent": "So for this we would use instead of the median.",
                    "label": 0
                },
                {
                    "sent": "We reduced the marginal of a graph set which maximizes this here and then we would select the marginal graphs out of the graphs at minus the already selected prototypes and this would lead to prototype selected from the border.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next prototype selector is the targets for prototype selector, which is a prototype select method which first select a graph located in the center of the graph set.",
                    "label": 0
                },
                {
                    "sent": "For instance, the set median graph.",
                    "label": 0
                },
                {
                    "sent": "Then we are looking for the graph which is located the furthest away from this center graph, and then we're dividing this.",
                    "label": 0
                },
                {
                    "sent": "The maximum distance from the center graph to the border graph into N -- 1 partitions with equal interval size.",
                    "label": 1
                },
                {
                    "sent": "And then we select the N -- 2 graphs that are located the nearest to those interval borders in terms of edit distance of course and we see OK, this will distribute the graphs more or less uniformly over there said.",
                    "label": 1
                },
                {
                    "sent": "But for selecting these prototypes we consider only one prototype selected so far.",
                    "label": 0
                },
                {
                    "sent": "So this is the center graph and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the next prototype selected, which is the spanning prototype selector, we.",
                    "label": 1
                },
                {
                    "sent": "We look for all the already selected prototypes, so the first prototype select selector select it is the set median graph of G and then each additional prototype selected out of this graph set is the graph the furthest away from the already selected prototypes.",
                    "label": 1
                },
                {
                    "sent": "So we're considering all prototypes selected so far, so we're maximizing the minimum distance through the prototypes.",
                    "label": 0
                },
                {
                    "sent": "As we see in the picture, this is prototype selective.",
                    "label": 0
                },
                {
                    "sent": "Typically picks of prototypes also from the border of a graph set.",
                    "label": 0
                },
                {
                    "sent": "This is because we are trying to maximize this mini.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distance.",
                    "label": 0
                },
                {
                    "sent": "And the last prototype selector is the case centers prototype selector which tries to select the graphs that are in a center of densely populated areas in the in the graph set.",
                    "label": 1
                },
                {
                    "sent": "So what we're basically doing is we apply K medians clustering on the graph set and we said K equal to the number of prototypes that we want.",
                    "label": 1
                },
                {
                    "sent": "We set K equal to N and then the median after the clustering has terminated, we select the median of each of the clusters.",
                    "label": 0
                },
                {
                    "sent": "'s prototype and this picture here looks quite similar to the picture before, but if you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare it.",
                    "label": 0
                },
                {
                    "sent": "We see.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this prototype selector.",
                    "label": 0
                },
                {
                    "sent": "Avoid selecting grass from the border.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, before we consider now experimental results, I want to give you a brief method summary.",
                    "label": 0
                },
                {
                    "sent": "So what we are proposing here is a graph embedding method, a very general graph embedding method to transform any graphs into a real vector space.",
                    "label": 0
                },
                {
                    "sent": "And this method of course enables the access to a wide range of pattern recognition procedures based on feature vectors applicable to graphs.",
                    "label": 1
                },
                {
                    "sent": "So the intention of the now following experiments is that we want to show that certain classification tasks can be better solved with methods that use the resulting vectorial descriptions rather than the original graph representation.",
                    "label": 1
                },
                {
                    "sent": "So we use four different classifiers.",
                    "label": 0
                },
                {
                    "sent": "So first of all we use a KNN classifier.",
                    "label": 1
                },
                {
                    "sent": "I applied directly on the graphs with graph edit distance.",
                    "label": 0
                },
                {
                    "sent": "We use an SVM classifier in conjunction with a trivial graph kernel.",
                    "label": 0
                },
                {
                    "sent": "So we use the negative distance and then we use two classifiers in the resulting vector space.",
                    "label": 0
                },
                {
                    "sent": "The 1st is a KNN classifier, two in the vector space and then this is our main system.",
                    "label": 0
                },
                {
                    "sent": "This is support vector machine trained on the resulting pictorial descriptions.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here's the experimental setup in each of our experiments we make use of three disjoint graph sets, a training set, validation set and test set.",
                    "label": 1
                },
                {
                    "sent": "So the validation set is used to determine optimal meta parameters for graph embedding and classification.",
                    "label": 1
                },
                {
                    "sent": "So two embedding parameter consists of the number of prototypes that we want to select.",
                    "label": 0
                },
                {
                    "sent": "This means the dimensionality of the resulting vector space and also the best performing embedding method.",
                    "label": 0
                },
                {
                    "sent": "So we have seen five different embedding methods and we have to determine the best one for a given graph set and we have several classification parameters.",
                    "label": 0
                },
                {
                    "sent": "This is the K for the KNN classifier.",
                    "label": 1
                },
                {
                    "sent": "And different SVM parameters too.",
                    "label": 0
                },
                {
                    "sent": "And of course the parameter values and also the embedding methods that resulted in the lowest classification error were then applied to to the independent test set.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think you know this data set already selected data set which consists of graphs representing capital letter line drawings out of 15 classes.",
                    "label": 1
                },
                {
                    "sent": "To summarize, once more, we first manually constructed.",
                    "label": 0
                },
                {
                    "sent": "Prototype line drawings of each letter.",
                    "label": 0
                },
                {
                    "sent": "Then we apply different distortion operators on them.",
                    "label": 0
                },
                {
                    "sent": "And finally we transform these drawings into graphs by representing lines by edges and ending points of lines by nodes.",
                    "label": 0
                },
                {
                    "sent": "So and as you see, we have different distortion levels and this is the result.",
                    "label": 0
                },
                {
                    "sent": "These are the results on a higher distortion level, so we see the K&N in the graph domain.",
                    "label": 0
                },
                {
                    "sent": "These are the results with the trivial graph kernel and the SVM.",
                    "label": 0
                },
                {
                    "sent": "Then we have the KNN director space and this is our main system.",
                    "label": 0
                },
                {
                    "sent": "The embedding method in conjunction with support vector machine and we see that our embedding method outperforms all the other systems with a statistically significant results.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data set you know also this is the image data set which consists of graphs representing color images out of five classes.",
                    "label": 0
                },
                {
                    "sent": "So we first segment images into regions and represent these regions by region adjacency graphs.",
                    "label": 0
                },
                {
                    "sent": "And here again we see that our embedding method outperforms the KNN on in the vector space and also the K and then in the graph domain with statistically significant results.",
                    "label": 0
                },
                {
                    "sent": "But the travel graph kernel is better than the embedding method but not statistically significant.",
                    "label": 0
                },
                {
                    "sent": "So we are happy.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And next data set this to fingerprint data set with four classes and we transform these fingerprints into graphs by extracting characteristic regions out of the fingerprints and then transformed this region center at rebooted graphs and hear the results are evident again.",
                    "label": 0
                },
                {
                    "sent": "So the embedding method, in conjunction with the support vector machine outperforms all the other systems with a statistically significant.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also in the last data set which is the molecule data set.",
                    "label": 0
                },
                {
                    "sent": "Which consists of graphs representing molecules out of two classes.",
                    "label": 1
                },
                {
                    "sent": "And in this data set we converted these molecules by representing atoms's notes and bonds searches.",
                    "label": 0
                },
                {
                    "sent": "And here again, results are the same.",
                    "label": 0
                },
                {
                    "sent": "The embedding method with the aseum outperforms all the other systems with statistical significance.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I've not mentioned so far discussed is which prototype selector and which dimensionality we have used for for these datasets, and we have to say that the prototype selector and also the number of prototypes depends hardly on the data set.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if we look at the letter data set, we see that.",
                    "label": 0
                },
                {
                    "sent": "The case centers prototype selector class wise or the target sphere classifiers performed the best.",
                    "label": 0
                },
                {
                    "sent": "It depends on which distortion level you you apply.",
                    "label": 1
                },
                {
                    "sent": "Your prototype selection methods and also the number of prototypes varies from 60 up to 450 prototypes.",
                    "label": 0
                },
                {
                    "sent": "And on the other data set, the image was the target sphere with 50 class independent on the fingerprint.",
                    "label": 0
                },
                {
                    "sent": "It was also the target Sphere prototype selected class wise with 300 prototypes on the molecule data set.",
                    "label": 1
                },
                {
                    "sent": "It was the spanning prototype selector with 150 prototypes.",
                    "label": 1
                },
                {
                    "sent": "So we have to conclude that there is neither best prototype select nor universally optimal feature space dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So we have to validate both on a validation set but however.",
                    "label": 0
                },
                {
                    "sent": "Can report that from a certain number of prototypes.",
                    "label": 0
                },
                {
                    "sent": "It it depends not so much which prototype select we use.",
                    "label": 0
                },
                {
                    "sent": "So given a certain number of prototypes, the differences among these prototypes selected or only marginal.",
                    "label": 0
                },
                {
                    "sent": "So if you don't want to validate it seems to be a good idea to use a prototype selector which tries to distribute the prototypes over the whole graph set uniformly.",
                    "label": 0
                },
                {
                    "sent": "And if class wiser class independent, we cannot say it is.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was my talk.",
                    "label": 0
                },
                {
                    "sent": "I will draw conclusions now.",
                    "label": 0
                },
                {
                    "sent": "In this work here, we propose a very flexible method for mapping graphs explicitly into an N dimensional vector space.",
                    "label": 1
                },
                {
                    "sent": "To descend, we have to select first and prototypes and then compute the graph edit distance from an input graph to all of these prototypes and these end distances can be interpreted as a real vector.",
                    "label": 1
                },
                {
                    "sent": "This is the key idea, so we viewed the similarities to end prototypes as a vectorial, vectorial description of the graph.",
                    "label": 1
                },
                {
                    "sent": "So we can say that in contrast with various other graph kernels, our method is applicable to any type of graph, and this is because we use graph edit distance and this allows us this high flexibility in our definition.",
                    "label": 1
                },
                {
                    "sent": "And further key point is that our method can be applied not only in conjunction with support vector machines, so here it was applied with support vector machines.",
                    "label": 0
                },
                {
                    "sent": "But however we could use any other statistical classifier like base classifier or neural network to do these graphs.",
                    "label": 0
                },
                {
                    "sent": "So this is more powerful because we can apply it to classifiers which cannot be kernelized.",
                    "label": 1
                },
                {
                    "sent": "OK, and from the results of our experiments we have seen that the SVM, in conjunction with the embedding method and the resulting vector space outperforms all the other approaches in the original graph domain.",
                    "label": 0
                },
                {
                    "sent": "But it outperformed also the K&N in the vector space, so it seems that the sophisticated classifier given by the SVM in combination with the embedding method leads to a good classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, this was my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}