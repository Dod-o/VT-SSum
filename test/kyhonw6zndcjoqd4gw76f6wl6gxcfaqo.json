{
    "id": "kyhonw6zndcjoqd4gw76f6wl6gxcfaqo",
    "title": "Modern Bayesian Nonparametrics: beyond Dirichlet and Gaussian processes",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_ghahramani_bayesian/",
    "segmentation": [
        [
            "Thanks a lot for inviting me to this workshop.",
            "So my plan for this talk, I mean my impression of the workshop from the description, was that it was going to be mostly non Bayesian nonparametrics.",
            "Except for me as a token Bayesian, but I since looking at the poster spotlights, I notice that there is actually quite a lot of Bayesian nonparametrics here.",
            "So the big picture for my talk is that I'm going to start out by motivating.",
            "You know what I do and people in my community do and why we do it and why we're interested in this area of Bayesian nonparametrics.",
            "Then I'm going to talk a little bit about kind of.",
            "The historically important pieces of Bayesian nonparametrics, and then to emphasize the word modern, which is the first word in the title of this workshop.",
            "I guess I'm going to move on to more recent things beyond the two classical Bayesian nonparametric models.",
            "The deer slaying the Gaussian process and, you know, towards the end of the talk.",
            "Basically the material I'm going to talk about is just work that we've been doing.",
            "In my group, in sort of areas beyond the richly in the Gaussian process, and I'm hoping at the end to get to a slide where I try to talk about the relationships between Bayesian and classical Nonparametrics, which I think are going to be very fruitful area for further thinking and research."
        ],
        [
            "OK, so my starting point is basically that a lot of what we do is modeling, and in particular I feel like you know I might have started out interested in AI and how the brain works, but now I'm working with data and modeling, basically an the kind of modeling that we're going to think about in this talk is probabilistic modeling, so let's first just ask ourselves what's a model?",
            "And I would argue good way of thinking about a model is in terms of data that the model would generate.",
            "So model is just a description of data that could be observed from some system.",
            "It's hard to call a model anything else because you know, unless the model makes some predictions about data, you can validate whether it's a good model or not.",
            "And the probabilistic modeling approach basically is very, very simple.",
            "It says we're going to use the language of probability theory to express all the forms of uncertainty that we have in all aspects of our model.",
            "The noise in the data, the parameters that we don't know about the structure of the model, all that stuff is unknown.",
            "Don't even call it random, just say it's unknown because you know you just have data you don't really know what it is and we're just going to use probability theory as the calculus.",
            "For handling uncertain or unknown quantities, just like calculus is, the calculus for handling rates of change probability theory is the mathematical language for handling uncertainty.",
            "And now there are lots of rules in probability theory.",
            "Well, actually it turns out not very many.",
            "And Bayes rule is one of the more corollary of the two basic rules, and so it's very natural.",
            "Then to use Bayes rule to infer unknown quantities from data to learn from data to make predictions to do model comparison etc etc."
        ],
        [
            "So.",
            "Apologize for the first.",
            "You know the 1st third of the talk will be rather tutorial, but I always find it useful to do that to get everybody on the page in terms of how I think about things.",
            "So the great news for somebody like me who has a very poor memory is that everything follows from 2 rules.",
            "So when in doubt I just go back to these two rules.",
            "There's some rule, the product rule and if I'm violating those two rules, I should be uncomfortable, OK?",
            "The two rules are very straightforward.",
            "The sum rule says that the probability of X is the sum over Y of the joint probability of X&Y if Y is continuous, we replace that with an integral.",
            "The product rule says the joint probability of X&Y can be written as a product of the probability of X times the probability Y given X.",
            "It's all very basic, but the emphasis here is almost everything I'm going to say follows from that.",
            "So then we just replace exs and wise with things that we might be interested in machine learning.",
            "For example D for data Theta four parameters, M for some unknown model.",
            "And as a corollary of the sum and product rule we have Bayes rule which is basically the one rule you need to learn.",
            "Learning is the process of going from your prior over the parameters, which is your expression of uncertainty in your model.",
            "Before observing the data.",
            "Through the likelihood function that tells you for every setting of the parameters with the probability of the data is that sort of a requirement for your model.",
            "To be well defined into your posterior, which is everything you've learned about your parameters from the data.",
            "Then if you want to do prediction again, you do use the sum rule in the product rule.",
            "So if you want to predict some unknown quantity X, like the next datapoint or part of the data point that you've observed partially, or some any other unknown quantity of interest, then essentially the sum rule and product rule tell you that what you have to do is you take the predictions from each parameter value and you weight those predictions by this posterior probability.",
            "And that's it.",
            "So it naturally incorporates the whole idea of ensemble learning.",
            "All of Bayesian methods is about on some bling stuff, and it's the ensemble just comes from the sum rule here.",
            "Notice there is no optimized rule in probability theory, so optimization only comes at the point where you've incorporated all your uncertainty and you are forced to make some kind of decision.",
            "And then you make a decision that minimizes your expected loss in some way.",
            "So now if you want to model comparison again, you apply these rules at a slightly higher level of models and you compute things like marginal likelihoods, which you've heard about, and so on.",
            "You know if you haven't, then it's very straightforward, but worth thinking about.",
            "OK."
        ],
        [
            "So a lot of a lot of my talk is going to be about sort of models with complicated structures and sort of about maybe 10 years ago.",
            "I used to think about the world was that one of the important problems in machine learning.",
            "Is learning automatically structure from data so you know all over machine learning whether you're talking about clustering or dimensionality reduction or variable selection or fitting dynamical systems, the data or hidden Markov models, the data or discovering hidden sources in your signals or learning the structure of the graphical model, you know I found all of these problems very interesting, and they're all.",
            "Model structure learning or discovery problems and one way to do that is you come up with a set of models or some nested class of models and then you."
        ],
        [
            "Cute, you know these marginal likelihoods or different models?",
            "Then you do model comparison and you can you can select the model if you want.",
            "Or you can average over the models that way.",
            "That's the way a lot of people used to do things.",
            "You know, I still think that's very useful and very interesting in some settings, but."
        ],
        [
            "This sort of thinking has changed a bit rather than comparing a bunch of finite models.",
            "Very elegant thing to do is to consider what happens in the limit of these models as they become infinite.",
            "So so this."
        ],
        [
            "Brings us to Bayesian nonparametrics.",
            "So the basic idea in Bayesian nonparametrics, or rather why I work in Bayesian nonparametrics."
        ],
        [
            "Is for two reasons.",
            "The reason for doing a Bayesian approach is just the simplicity of the Bayesian framework.",
            "Everything follows from those two rules.",
            "So some nice simplicity and coherence arguments there.",
            "The reason for going towards Nonparametrics is that when you start dealing with real world data and you start thinking about priors for your models for real data, you realize that the real world isn't a textbook statistical model.",
            "Usually it's the real world is not a mixture of three Gaussians or a quadratic polynomial, it's very hard to find.",
            "It's really exceedingly hard to find real examples of things that fit simplistic model assumptions.",
            "Certainly I know have almost no real phenomena apart from some classical physics things that would be actually fit by linear model, although that's the most widely used thing in statistics.",
            "So we want to do nonparametrics just to deal with the complexity of the data in the real world phenomenon."
        ],
        [
            "So again, you know it's important to distinguish or understand what we mean by nonparametrics and the best way to understand that is just to start talking about parametric models of parametric model assumes some finite number of parameters, call them Theta and given those parameters they capture everything there is about the data.",
            "So when I want to make predictions about future data X given Theta I can forget I can throw away my training data.",
            "Essentially, I can throw away D. So there are very nice to analyze, but the complexity of the model is somehow bounded by the fact that I've chosen a finite number of parameters.",
            "So even if the amount of data is unbounded, my model is not going to be very flexible, and so actually funnily enough, in this world of, you know.",
            "Use the catch phrase big data.",
            "You know, it seems to make sense actually, to really start thinking about models that are flexible enough to extract interesting structure structure from the data.",
            "Now, nonparametric models assume that the data distribution cannot be defined in terms of this finite set of parameters, but can often be defined in terms of some infinite dimensional parameter vector.",
            "Or just think of that as a hidden function of some kind.",
            "And the nice thing about that infinite dimensional Theta is that the amount of information we can capture about the data grows with the size of the data set, and that's sort of a classical way of thinking about nonparametric models.",
            "It also means that nonparametric models tend to be memory based, so you need to store around growing numbers of data points or sufficient statistics of the data points, etc.",
            "As you get more data.",
            "But it makes some more flexible, which is sort of interesting for machine learning point of view.",
            "Um?"
        ],
        [
            "OK, so you know again to sort of motivate why we want to do nonparametrics.",
            "Really the emphasis is on flexibility.",
            "If you have a flexible model you're hoping to get better predictive performance, and I think there's sort of ample evidence out there that you can get better predictive performance from nonparametric models.",
            "And they tend to be more realistic.",
            "So as a subjective Bayesian, I would be uncomfortable.",
            "You know, assuming a linear model or something like that when I know that just can't be what's going on.",
            "Right?",
            "The other, really, you know, practically put the other reason to think about this is just to look around at what's been successful in the machine learning community, and I would argue that the things that have been really successful in machine learning are either nonparametric or pretty damn close to nonparametric.",
            "OK, So what I mean by that is, you know, if you think about the kernel machine revolution.",
            "From SVM's you know over 10 or 15 years ago, that was just sort of a nonparametric way of thinking about problems that suddenly jumped into the arena and made performance of methods extremely good compared to previous methods.",
            "But also if you think about the current craze on deep networks, well there are many, many interesting aspects of deep networks.",
            "But one of the aspects is that nobody talks about, you know, having fitted deep network with three hidden units or anything like that, all these deep networks have.",
            "You know crazy numbers of hidden units and layers and connections, and essentially they are just really damn flexible models for modeling data, so they might be technically speaking parametric as they have only 100 million parameters.",
            "But effectively speaking their operating in this nonparametric regime.",
            "And even stupid methods like K nearest neighbors seem to do very well in.",
            "These are nonparametric methods.",
            "So my only caveat here is that depends on how you define successful.",
            "Machine learning is also been very successful at just developing highly scalable methods, and obviously a very interesting challenge is making things highly scalable and still nonparametric.",
            "And obviously I'm interested in that as well.",
            "OK."
        ],
        [
            "So just to give sort of a.",
            "A table relating problems to parametric nonparametric things out there.",
            "Let's consider the following.",
            "So what I've done here is just give examples of parametric models corresponding or replacement nonparametric things that you could do in the applications that you would use them for.",
            "For example, if your application is function approximation then you know nice parametric thing might be to do polynomial regression and nonparametric thing would be to do Gaussian process regression.",
            "For classification you can, you know do logistic regression.",
            "Or you could do a kernelized Bayesian logistic regression, also known as Gaussian process classification OK etc etc.",
            "Mixture models for either clustering or density estimation.",
            "Then you take the infinite limit, you get the richly processed mixtures etc.",
            "OK, so you know it's very fruitful because lot of problems that we are interested in machine learning.",
            "The cast as problems in nonparametric Bayesian modeling.",
            "And so you know it can keep people in my community pretty busy.",
            "Now.",
            "Again, coming back to the word modern, the cornerstone of this field had the two cornerstones of this field have been Gaussian processes and Irish lay processes.",
            "And here is."
        ],
        [
            "It's sort of 1 slide of both things together, just to contrast them and compare them.",
            "Though Gaussian process defines the distribution on functions, so you can think of a function drawn from this Gaussian process prior with mean function mu, an covariance function K, and this is basically just an infinite dimensional Gaussian distribution.",
            "So the mean function is just the mean of the functions that you draw.",
            "Here is a little picture of a random function that was drawn from this Gaussian process prior.",
            "An the covariance function is the kernel.",
            "It's exactly the same sort of thing as the kernel in SVM.",
            "And it's the infinite dimensional analogue of a covariance matrix.",
            "OK, dear Ishly process defines a distribution on distributions.",
            "So now instead of drawing functions, I'm going to draw sort of think of 'em is non negative functions that integrate to one.",
            "So distributions and you can say G is wrong from dealership process prior with mean G0 and scaling or concentration parameter Alpha which play exactly the same well the mean plays the same role as the mean here and this also plays a role as a variance parameter.",
            "Dispersion parameter and.",
            "So when you draw from Additionally process you get some random probability distribution an it's the infinite dimensional analogue of additional distribution.",
            "OK, so these things have been studied.",
            "You know this for over 100 years this for now 40 or so years and these are very well understood and still very very useful because in a lot of modeling applications we either have a function that we don't know or distribution that we don't know so we can plug these things wherever that happens."
        ],
        [
            "Now for some of you who might be familiar with BMS and not familiar with Gaussian processes, it's useful to put these two on a slide together just to show how damn similar these things are."
        ],
        [
            "So I won't go through the derivation, but we can write the loss function minimized by a sort of.",
            "Bog standard ordinary.",
            "Support vector machine with hinge loss in the following form, you're minimizing over some vector F, some quadratic form of in F with the kernel matrix in there plus some regularization constant C times the sum over your data points of the hinge loss on your data points.",
            "This is for classification with Y being your class label and FI being the function value at your class label, and this little plus being this sort of.",
            "Classical hinge loss.",
            "So now if I take again, I'm not doing the derivation for you would show you the final result of it.",
            "If I take a Gaussian process.",
            "For classification.",
            "Then I can write the log probability of that Gaussian process for classification in the following form, the log of the Gaussian part gives me quadratic expression in F. The function values of at my end data points, which is exactly the same quadratic expression as here.",
            "And then each of my data points in my data set for classification give me a log probability of that observed label given the function value at that point plus some constants from the Gaussian.",
            "OK, so now you look at these.",
            "You stare at these two things and you say the same thing basically.",
            "Or very slight variance on them and it you know in some ways is true.",
            "The subtitle of this slide would be why I never use SVM's.",
            "The reason I never use SVM's is just because anytime somebody could use an SVM for something, I can just think about the same kernel and the same thing in a Bayesian nonparametric way and just do a Gaussian process.",
            "And I just feel more comfortable doing that because I have a lot more intuitions about what's going on there.",
            "But there's sort of more deep differences between these two things, so on the surface these things look the same, but you know the SVM use these sort of nonexistent optimize rule, which doesn't exist in probability theory, whereas the Gaussian process said, well, you know, if you had that prior on your function, you had that likelihood, then the thing you need to do is you need to integrate out or average out over this unknown function values F. So rather than minimizing this with respect.",
            "F We average integrate over F there.",
            "So essentially it handles the uncertainty in the unknown function by averaging, not minimization, and basically therefore you have a lot of the probabilistic semantics and so on, and some of the nice things that.",
            "Occurs a consequence of that is that you can learn the kernel parameters automatically from the data in a very straightforward way, and you can make the kernel quite flexible and etc.",
            "You can even learn this regularization parameter.",
            "See without any cross validation, because you can just rewrite this parameter C as a single scale parameter on your kernel.",
            "Single scaling on your kernel, and that's sort of a standard thing people actually learn in the Gaussian process literature since 10 years ago or so.",
            "And and I know in a sense I just.",
            "I'm just more comfortable because I find it more intuitive than I can sample for my prior to see exactly what my assumptions were that are kind of captured by the kernel, etc.",
            "You can do automatic feature selection, etc and you know you can do most of these things in SVM context as well.",
            "Let's just be clear, but it's nice to put the two on the same slide, and there's some nice MATLAB code if you are not familiar with GPS that you could play with."
        ],
        [
            "So in the big picture, and you know, for any of you who seen me give talks, I often tend to have stuff like this in my talks.",
            "I'm probably apologize for those of you see this particular cube, but basically we can relate a whole bunch of models together by starting from a simple humble model like linear regression and considering what happens to linear regression when you apply different operations to it.",
            "So one operation you can do to linear regression is to turn it into a classification model by applying some transformation on the outputs or some link function on the output.",
            "So an example of something that you get that's analogous to the linear regression is logistic regression.",
            "By applying that classification link.",
            "Another thing you can do to linear regression is, instead of optimizing your parameters by least squares or maximum likelihood, you can do Bayesian inference over your parameters, and then you get Bayesian linear regression.",
            "Standard textbook Bayesian stuff.",
            "And another thing we all know you can do to linear methods is you can kernelized them.",
            "So here you would get kernel linear regression.",
            "Kernelized linear regression and now with these three operations we can think about what happens if we apply them in different orders.",
            "So of course we can get kernel classification and an SVM for example would live in this corner of this cube.",
            "We can get Bayesian logistic regression here.",
            "The Bayesian form of kernel regression, or analogously the kernelized form of Bayesian linear regression is called Gaussian process regression which I just sort of described to you a little bit.",
            "And then if you combine all of these operations together you would get Gaussian process classification which if you wanted and you were perverse you could call something like Kernelized Bayesian linear classification.",
            "OK.",
            "So."
        ],
        [
            "Now.",
            "The idea in this talk is that I'm going to try to just move ahead from where we were and talk about a whole bunch of things very quickly.",
            "And mostly I just want you to get the high level idea of what's going on, and then you can follow up.",
            "Follow it up by reading the papers and stuff like that.",
            "So we have functions and densities or distributions that we can do nonparametrics on, and that's very well known, but we want to do nonparametric modeling on things like sparse matrices, situations where you have overlapping clusters, networks, exchangeable arrays, covariances, hierarchies, etc.",
            "So all sorts of other complicated stuff, and so just to give you an overview of stuff, at least that we've been working on in this area, and there are a lot of other people have been doing excellent work in this area as well.",
            "I'll just dive into this."
        ],
        [
            "So sparse binary matrices, you know, once you have a distribution on sparse binary matrices, it's amazing how many uses you can put that too, and I'll describe a couple of them.",
            "So here is one way of coming up with a distribution on sparse binary matrices that is non parametric in the sense that you're allowing yourself to have potentially infinitely many rows and columns.",
            "In a way that you can still do interesting computations and so on with.",
            "So the semantics of this that I want you to think about is we have some objects or data points and they can have some features an an element zed N K = 1 means that object N has some hidden feature K. And the basic idea is just like clustering where we might want to have K clusters and think of what happens when K goes to Infinity, and that's where, for example, you get additional process mixture.",
            "Here we want to think about models with factorial or distributed representations that are sparse where you allow yourself to have infinitely many features.",
            "So we're going to take the limit of the number of features going to Infinity, but the way we generate a finite matrix like this, the simplest way that we could think about.",
            "Is draw these ones and zeros from some Bernoulli distribution with parameters Theta, K and Theta K controls how popular a particular feature is, and this is an unknown parameter, so we have to give it a prior or nice prior we can give it.",
            "Is this beta distribution with parameters Alpha over Big K. The number of columns in this matrix, one is sort of the simplest thing we can think about, and the reason we're scaling by Big K the number of columns.",
            "Is because then we can take the limit as the number of columns goes to Infinity and get some very nice properties that make this both computationally attractive and attractive from the probability theory point of view.",
            "So in particular what happens is that as the number of columns goes to Infinity, the matrix gets bigger and bigger, but it also gets sparser and sparser.",
            "So essentially the expected number of non zero entries is still bounded.",
            "So if you have any rose on average in each row, you're going to have Alpha number of ones, and in fact there are going to be drawn from the number of ones are going to be drawn from plus an Alpha distribution.",
            "So this infinite limit is the Indian buffet process.",
            "Which is now being used in a bunch of different applications where you can stake a sparse matrix in usefully."
        ],
        [
            "OK so here is 1 particular kind of application from meets at all, which is a form of binary matrix factorization.",
            "So what I mean by binary matrix factorization is that is not the matrix that of data is binary, but that we're going to do is we're going to factor this observed data, which could be something like here it's jeans by patients gene expression data.",
            "It could be users by movies or whatever you want.",
            "That's expressible in a matrix.",
            "We're going to factor it by saying that associated with every row of the matrix, we have a binary vector, the hidden binary vector and associated with every column of the matrix.",
            "We also have a hidden binary vector, and that's going to be our representation that's used to model the stuff that's inside the matrix, so it's a generalization of, for example Co clustering where you just cluster the rows and columns simultaneously.",
            "Here you're doing something kind of more.",
            "Rich or expressive?",
            "Using these binary vectors."
        ],
        [
            "So again, just to put things in perspective, if we start from something like a finite mixture model, like a mixture of Gaussians, we can consider different kinds of conceptual operations we could do to that we can.",
            "We can consider non parametric version of that where the number of components goes to Infinity and that would be the Dirichlet process mixture.",
            "We can consider time series generalizations of mixture models, and that's how you get hidden Markov models and we can consider factorial models and factorial models.",
            "You can think of them as well in a mixture model.",
            "Data point belongs to only one cluster, one and only one cluster in a factorial model, you know data points can belong to multiple overlapping clusters at the same time, or equivalently they have some sort of factor.",
            "There's some facts factored hidden state space that models the data.",
            "So again, combining these two things, we get models for example based on the Indian buffet process.",
            "Or you know, time series models generalizing the Dirichlet process mixture like the infinite hidden Markov model or HDP HMM?",
            "And you know more complicated things like the infinite factorial hidden Markov model, etc.",
            "So it's just useful to kind of understand how things are related, because for any modeling application, you have to sort of wonder where in the space of models should you start to explore."
        ],
        [
            "OK, so now let's talk about networks and I'll try to keep this brief.",
            "I'm actually speaking in the social network and social Media Workshop a little bit more about this.",
            "They'll be a little bit of overlap with that, but if you want to hear it again with more detail, you can come to that in the afternoon here."
        ],
        [
            "So we're interested in modeling networks.",
            "Alot of data can actually be thought of as networks or relational data.",
            "For example, protein, protein interaction networks.",
            "Obviously things like social networks, coauthorship networks, etc.",
            "And we want to have models that are able to predict missing links in for latent properties or classes from the objects, or generalize learn properties from small observed networks to larger networks.",
            "So these are the sorts of things that we want from our model and now."
        ],
        [
            "Now.",
            "Let's just think about interesting models in this category.",
            "So one very interesting kind of model is something called latent class models, which basically say that the nodes in my network have associated with them hidden class, which I've denoted here with this colored letter attached to each node.",
            "And basically there's a single hidden discrete variable associated with each node, and then probability of having a link between a pair of nodes depends on.",
            "On the latent classes of those two nodes."
        ],
        [
            "Now this corresponds to basically a clustering of the nodes or community detection as it's called in the network modeling literature.",
            "And there are a lot of very interesting models of this kind.",
            "Now."
        ],
        [
            "For example, nice Bayesian nonparametric model in this category is given by Kemp at all 2006, called The Infinite Relational Model, where the basic idea is that you're doing community detection in a network like this and you have hidden classes and you're just going to use a Chinese restaurant process to allow yourself to have an unbounded number of classes.",
            "And that's a pretty nice idea, because really you want to have.",
            "A model where if you get a new node in the network like you know you get a new person coming into Facebook or something, then that person can either belong to one of the existing clusters or it can form its own cluster in this very naturally incorporates this idea and then the probability of a link between two nodes just depends on the classes of those two nodes.",
            "By some simple probability transition matrix like that.",
            "The."
        ],
        [
            "OK, so this is a very nice idea.",
            "Here is a nice, even slightly more general idea called latent feature models and the basic idea here is, well, if you think about Facebook, you know I belong to a network associated with my colleagues at work, and I have a family as well and I went to high school somewhere.",
            "So really any particular agent or object in the network belongs to multiple different communities at the same time.",
            "So you really want models with overlapping clusters or communities.",
            "So the way to think about that is that associated with each.",
            "Node or a bunch of latent features denoted by these colored letters again and the link probability is going to determine be determined by the latent features of 1 node in the latent features of the other node.",
            "It's thinking about linking to or not.",
            "And again, obviously there are probably, you know, an infinite number of high schools out there, or workplaces out there, whatever.",
            "So you really want a model that can accommodate a potentially unbounded or infinite number of these latent features.",
            "So that model is called the latent feature relational model and essentially the way that work was done by Miller at all was to use an Indian buffet process instead of a Chinese restaurant process.",
            "In the similar context of the previous model.",
            "OK."
        ],
        [
            "So now here is an example of.",
            "And maybe even more complicated model for networks, but maybe more realistic again.",
            "So the idea between this work, which was done with Konstantina Paula and David Knowles ICL last year this year I guess.",
            "Is this infinitely attribute model for network data and the basic idea is that associated with each node object there's some number of latent attributes OK, and node can have an attribute or not have an attribute that's sort of basically like the ICP Indian buffet process, part of things.",
            "But then the attributes can have discrete values associated with them, so within an attribute then there is a clustering going on and the probability of a link between object or an object J depends on the attributes and their values.",
            "For node ion know J through some link function.",
            "This like this and I'm not going to go into the detail but it sort of has some.",
            "Intuitive explanation for what's going on here.",
            "So the idea is that you want to potentially unbounded number of attributes and values per attribute.",
            "The way you do the number of attributes is within ICP and in buffet process the way you do the number of values for attribute is with Chinese restaurant process and it's sort of nicely generalizes this infinite relational model by Kemp.",
            "It all in this non parametric latent feature relational model by Miller at all.",
            "Intuitively."
        ],
        [
            "The way you can think about this is if you have a student friendship network at a place like Cambridge, students belong to colleges, they might play sports.",
            "They might have political views or you know religious views or play some musical instrument or whatever.",
            "Those attributes could be, and those attributes can have values like we have here at essentially the probability of two people linking kind of depends somehow on this constellation of stuff associated with them.",
            "And all this stuff is not observed.",
            "The only thing that you observe are the links.",
            "So it seems surprising you can actually learn a whole bunch of hidden."
        ],
        [
            "Stuff like this from data, but here is just some example of applying this to both this classical kind of nips.",
            "Coauthorship Network data gene interaction network data, and the interesting thing about this table is, you know you could look either at the test there or the test log likelihood.",
            "Let's look at the test error here.",
            "Test their link prediction so the infinite relational model is giving us stay around 4%.",
            "This more powerful latent feature infinite relational model is giving us 2%.",
            "This infinitely attribute model is giving us 1%, so there's definitely looks like a significant drop there here 36 to 27 to about 7% error.",
            "And The funny thing about this table is that all of these are Bayesian nonparametric models of networks, right?",
            "So we're comparing all of these different models that have infinitely many potential hidden variables associated with them.",
            "But how you structure your infinitely many hidden variables?",
            "Maybe that's a lesson from here.",
            "Can have an effect on how well you do in terms of predictive performance.",
            "Now."
        ],
        [
            "If you were at the main conference.",
            "A couple days ago.",
            "We have some work in network modeling that makes use of some really interesting fundamental ideas in probability theory based on exchangeable arrays.",
            "So the idea here is if you want to do a network model and you want to have a network model where.",
            "The ordering in which you look at the nodes is irrelevant, so you could reorder a re label the nodes and you should get the same probability.",
            "Then you're living in the world of exchangeable arrays and exchangeable array.",
            "Some array here is exchangeable if its probability distribution is the same as the probability that you would get from permuting the rows and the columns of the matrix for every.",
            "Permit infinite permutation.",
            "An the there is a very nice classical result from probability theory called the Aldous Hoover representation Theorem, which says that random matrix or array is exchangeable if and only if we can represent it in terms of some random function operating on some hidden variables associated with the nodes and the columns and these pairs of nodes and columns.",
            "So pictorially.",
            "The way you, well, let's talk about the interpretation 1st and then let's talk about this picture.",
            "The interpretation is that any model of matrices or arrays or graphs where the order of the rows and columns or nodes is irrelevant, can be expressed by assuming some latent variables associated with each row and column, and some random function mapping these latent variables to the observations.",
            "Well, how does this relate to Bayesian Nonparametrics?",
            "Well, the catch here is that to be able to model any distribution over these matrices you have to have a flexible enough distribution on these random functions.",
            "Pictorially, the way this is represented is the following.",
            "Let's look at this last figure here.",
            "Imagine this is for example an adjacency matrix from some very large graph showing the links between nodes in some very large graph.",
            "Now what this is saying is that this adjacency matrix can be modeled by saying that every node is embedded in the unit interval somewhere.",
            "And then there is a function that Maps the locations of pairs of nodes into the probability that there is a link between them.",
            "And that function is this sort of smoothie kind of thing here.",
            "And the requirement for this sort of theorem is that you have to have flexible enough prior on these random functions to be able to capture these exchangeable race."
        ],
        [
            "OK, so just that nips this year.",
            "We had a paper obviously.",
            "You know, if you went to it, I apologize.",
            "I'm just doing 1 slide on this where we took this Aldous Hoover theorem representation very literally and we said let's build Bayesian nonparametric statistical model for networks based on this and essentially what we're going to do is we're going to say we have an unknown function, let's call a Theta.",
            "Here's a flexible prior, an unknown functions a Gaussian process.",
            "It has good.",
            "Kind of coverage properties, so we're just going to use Gaussian process for that unknown function defined a model like this and then do inference from data."
        ],
        [
            "And once we did this, the really interesting thing is that then we kind of went back to the literature and we looked at a lot of different models, both for graph data and for real valued arrays.",
            "So you know.",
            "You know the infinite relational model latent distance model.",
            "I can I can models etc.",
            "And for real value data arrays, things like probabilistic matrix, factorization's and Gaussian process, latent variable models, etc.",
            "And a lot of these things can just be expressed as a particular assumptions on the structure of this Aldous Hoover based model.",
            "OK."
        ],
        [
            "And you know, slightly to our surprise, it seemed to work.",
            "So actually, you know it actually did good link prediction on data, despite the fact that we were mostly motivated by trying to bring the results in probability theory into the machine learning community through a model.",
            "It actually seems to work to model things with the Gaussian process in as prior on that latent function.",
            "And I won't go into the details of that.",
            "Somebody who's chairing this tell me how much time I have so I can about 10 more minutes.",
            "OK, so.",
            "So just to just to sort of describe that, let me let me actually spend a minute talking about this particular adjacency matrix.",
            "So this is, I believe, from the protein interaction network.",
            "You have a bunch of nodes and you have a bunch of edges between them.",
            "And if you look at the adjacency matrix, suitably reordered by the latent space that we discovered in our algorithm, then the adjacency matrix has a funny structure that looks like this.",
            "I don't know if you can see it from the back there, but there is.",
            "There appears to be a block at the corner here, so there's some community that's very heavily interconnected, and then there appears to be.",
            "There some of those notes appear to be connected to a lot of other stuff, so they seem to be some kind of hubs in this community.",
            "Global hubs from that community.",
            "Then there is like a band structure here along the diagonal, which looks like the sort of distance based models, and then there is this funny butterfly type connectivity pattern here, which looks a bit like, you know, bipartite graph.",
            "Now if I sat down and try to come up with a model that had all of these different properties in it.",
            "Chances are I wouldn't, you know if I had not known that I was looking for a model with those properties, chances are I wouldn't have stumbled into a model like that.",
            "But this random function model nicely sort of just embeds the nodes into these latent spaces and says I'm just going to model that adjacency matrix.",
            "Now that I've embedded it, the nodes into the unit square.",
            "Or the unit interval for each node.",
            "Now I'm going to just use a funky function that looks like this that approximately models that adjacency matrix.",
            "So it seems to be pretty flexible and actually in terms of, you know, area under the curve.",
            "It kind of predicts better than some of these other models for networks."
        ],
        [
            "OK, so another interesting structure that we've been looking at, and of course also lots of other people, is nonparametric models for covariance matrices and the basic idea there is your object that you're trying to model is a covariance matrix, But you want to model that dependent on some covariates or input variables like time or other input variables, column X, and this is kind of relevant to areas like econometrics where people are interested.",
            "In things like multivariate stochastic volatility, volatility is a function of time, but you could of course make these covariance matrices between different say assets or whatever be functions of whatever else you want.",
            "Now."
        ],
        [
            "Again, just one slide on this sort of basic idea here, and the key point here is that once you have some tools in your Arsenal of Bayesian nonparametric modeling, then there are nice ways of combining them together and get getting more rich and powerful models.",
            "So for example, here the idea is we're trying to model covariance matrices that depend on time or some other variable.",
            "So the object that we're trying to model.",
            "Has a particular structure.",
            "It's a symmetric positive definite matrix.",
            "So we need to have distributions over symmetric positive definite matrices.",
            "You can go to standard textbook and you find that a nice distribution for those matrices is that we shared distribution the way you get a we share distribution is you take a bunch of multivariate Gaussian variables and you take a sum of their outer products, and then you're guaranteed to be symmetric.",
            "And if you take enough of terms in this sum, then you're also guaranteed to be positive definite.",
            "With high probability.",
            "So that's how a textbook would derive the Wishart distribution for you.",
            "And now you combine that idea very straightforwardly with this idea of alishar process.",
            "Sorry, Gaussian process, and you say OK, Now I want this matrix to be dependent on time.",
            "So what I'm going to do is I'm going to make these vectors instead of being Gaussian vectors.",
            "I'm going to make them be Gaussian processes in time.",
            "And then what you get is if the these are vector valued Gaussian processes.",
            "You take the sum of their outer product's and you get a wish, our process, and that's sort of something that's been known in probability theory for awhile to decades, but it's been completely underused or very very rarely used, and it hasn't really that community and econometrics community haven't really interacted that much with the a lot of the machine learning know how, in terms of how to scale up Gaussian processes.",
            "How to generalize Gaussian processes, etc.",
            "And so you can think of a lot of what we did is just bringing these ideas together and trying to play with them and apply them to real data.",
            "OK. OK. Good."
        ],
        [
            "The last kind of.",
            "Topic in terms of modern basic nonparametric.",
            "So I wanted to talk about was hierarchies.",
            "So there are lots of reasons why we want models with hierarchies in them.",
            "So for example, in the real world there are things that we believe might be generated from hierarchies like for example.",
            "Animals are naturally organized into hierarchies.",
            "We might want to use hierarchies inside our model to do parameter tying, so you know basically you want to type parameters closely nearby in the hierarchy and more loosely far away in the hierarchy.",
            "And hierarchies are just very useful for visualization and interpretability, and so they're widely used by lots of people you know.",
            "Especially popular.",
            "For example, in bioinformatics.",
            "Now the there are a lot of methods for doing hierarchical clustering.",
            "Again, you know you can go to one of my favorite textbooks dude and heart 1973 and you find like beautiful descriptions of various ways of doing hierarchical clustering.",
            "So what can we bring to the table?",
            "Well, if you start to think about doing hierarchical modeling in a Bayesian context, then what you need is priors on trees.",
            "Like this and in fact you need a little bit more than that, because given the tree structure, you might want to have not just the prior on the trees, but you want to have a prior on properties of the data that you could generate given the tree.",
            "And Moreover, if you're in the world of exchangeable data, then you don't want to prior on a finite tree with N nodes, which you could say learn on the data.",
            "But then you know not know what to do.",
            "If you have N + 1 nodes, you want priors on hierarchies that are coherent no matter how many leaves are in the hierarchy.",
            "So you want partners on infinite trees.",
            "So there's a lot of work in this area.",
            "You know, great really interesting ideas like Kingman's, coalescent and dearest lady Fusion trees and some of."
        ],
        [
            "R and I would certainly consider this to be, you know, in the era of modern Bayesian nonparametric so around 2001 Radford, Neal proposed a very nice idea called the Dearsley diffusion tree, which is exactly of this form is a prior on hierarchies.",
            "I won't go through this in great detail, but basically the way the hierarchy is generated is by."
        ],
        [
            "Imagining that you have a process like Brownian motion process that evolves in some sort of fictitious time so the time is not real time, it's just an object that's used to generate the hierarchy for you.",
            "The first data point follows this Brownian motion and sub somewhere.",
            "Now the second data point.",
            "What it does is it follows the path of the first one and then at some point given by some hazard function it diverges from the original path and ends up following a completely independent path after that point until time one.",
            "And then if you follow this process for multiple points, what happens is the Third Point comes along follows the path of the first 2.",
            "Then diverges at well.",
            "At some point it either follows the path of the first point or the second point in proportion to how many points have followed that path before.",
            "And then you know ends up somewhere etc.",
            "What you get is a hierarchy over your data.",
            "That live at the leaves here and this is the latent harkey that you've shown here.",
            "So essentially, this is a very nice model and all we did was really try to think about ways of extending this model so that you don't get binary trees.",
            "Here you get binary trees with probability one."
        ],
        [
            "And what we get is trees with arbitrary branching factors and slightly generalizing the branching process to follow something called the Pitman yor process.",
            "And this maintains exchange ability.",
            "In the data, in naturally extends the dishley diffusion tree to arbitrary non binary branching is still infinitely exchangeable in the data as I just said, and the prior is sort of the most general kind of exchangeable prior.",
            "You could have on trees.",
            "OK."
        ],
        [
            "So I'll summarize and then I'll go onto this sort of more interesting kind of discussion slide.",
            "So the summary is just that.",
            "I've tried to give you an overview of, you know why we do the things we do from a Bayesian nonparametric framework.",
            "And then I try to describe a whole bunch of more classical and more modern Bayesian nonparametrics.",
            "Of course heavily biased towards stuff that we've been doing in my group, so I you know it's not meant to be representative of everything in this community.",
            "But the open challenge, I think, is that there are these two incredibly thriving communities out there doing both Bayesian and classical Nonparametrics, and we need to bring these communities a little closer together.",
            "And there are several ways in which we can bring them closer together.",
            "First of all alot of Bayesian nonparametric models are proposed.",
            "But there is a little by the way of classical theory about them.",
            "So for example proving consistency or convergence rates ETC.",
            "For modern basic nonparametric models is an active area of research I think.",
            "For the Gaussian process in their sleep process, there is a lot of good work already in this area, but for some of these newer things that is not so well understood.",
            "And then some problems are just easier to handle in one framework than the other.",
            "So for example, you know density estimation if you say what's a nonparametric classical nonparametric way of doing density estimation?",
            "Well, kernel density estimation is one is super easy, right?",
            "You know you can explain it to anybody, you can implement it in a couple lines of code.",
            "Very painless.",
            "I mean, you know, sometimes it works beautifully as well, right?",
            "Whereas from a Bayesian POV, if you're trying to do then see estimation that you actually have to have a model of your unknown density.",
            "If you want to be nonparametric then you end up with something.",
            "The easiest thing I can think of is this Dirichlet process mixture.",
            "There might be easier things, but that's sort of the most classical thing to do, and that's just bit harder.",
            "You know you have to do MCMC or variational inference or something like that.",
            "And then, but on the other hand, for complex modeling problems, as I hope I've illustrated, basic methods are very nice because you can compose these components into more complicated components.",
            "Because you're sort of relying on Bayesian averaging, even though at every level you have infinitely many parameters, you're trying to average over them so it doesn't really matter.",
            "You don't have to worry about things like overfitting, you have to worry about making stupid assumptions, but you don't have to worry about overfitting.",
            "And so that's sort of nice that they compose and you don't have to worry about overfitting regularization and things like that.",
            "So I think what we should be doing more of is translating translating ideas for one framework to the other where possible, and we also need just more empirical and theoretical comparisons across these.",
            "These two different approaches, so very few papers will actually have comparisons involving both kinds of things.",
            "OK.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks a lot for inviting me to this workshop.",
                    "label": 0
                },
                {
                    "sent": "So my plan for this talk, I mean my impression of the workshop from the description, was that it was going to be mostly non Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Except for me as a token Bayesian, but I since looking at the poster spotlights, I notice that there is actually quite a lot of Bayesian nonparametrics here.",
                    "label": 0
                },
                {
                    "sent": "So the big picture for my talk is that I'm going to start out by motivating.",
                    "label": 0
                },
                {
                    "sent": "You know what I do and people in my community do and why we do it and why we're interested in this area of Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk a little bit about kind of.",
                    "label": 0
                },
                {
                    "sent": "The historically important pieces of Bayesian nonparametrics, and then to emphasize the word modern, which is the first word in the title of this workshop.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm going to move on to more recent things beyond the two classical Bayesian nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "The deer slaying the Gaussian process and, you know, towards the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "Basically the material I'm going to talk about is just work that we've been doing.",
                    "label": 0
                },
                {
                    "sent": "In my group, in sort of areas beyond the richly in the Gaussian process, and I'm hoping at the end to get to a slide where I try to talk about the relationships between Bayesian and classical Nonparametrics, which I think are going to be very fruitful area for further thinking and research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so my starting point is basically that a lot of what we do is modeling, and in particular I feel like you know I might have started out interested in AI and how the brain works, but now I'm working with data and modeling, basically an the kind of modeling that we're going to think about in this talk is probabilistic modeling, so let's first just ask ourselves what's a model?",
                    "label": 0
                },
                {
                    "sent": "And I would argue good way of thinking about a model is in terms of data that the model would generate.",
                    "label": 0
                },
                {
                    "sent": "So model is just a description of data that could be observed from some system.",
                    "label": 0
                },
                {
                    "sent": "It's hard to call a model anything else because you know, unless the model makes some predictions about data, you can validate whether it's a good model or not.",
                    "label": 0
                },
                {
                    "sent": "And the probabilistic modeling approach basically is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "It says we're going to use the language of probability theory to express all the forms of uncertainty that we have in all aspects of our model.",
                    "label": 1
                },
                {
                    "sent": "The noise in the data, the parameters that we don't know about the structure of the model, all that stuff is unknown.",
                    "label": 0
                },
                {
                    "sent": "Don't even call it random, just say it's unknown because you know you just have data you don't really know what it is and we're just going to use probability theory as the calculus.",
                    "label": 0
                },
                {
                    "sent": "For handling uncertain or unknown quantities, just like calculus is, the calculus for handling rates of change probability theory is the mathematical language for handling uncertainty.",
                    "label": 0
                },
                {
                    "sent": "And now there are lots of rules in probability theory.",
                    "label": 0
                },
                {
                    "sent": "Well, actually it turns out not very many.",
                    "label": 0
                },
                {
                    "sent": "And Bayes rule is one of the more corollary of the two basic rules, and so it's very natural.",
                    "label": 1
                },
                {
                    "sent": "Then to use Bayes rule to infer unknown quantities from data to learn from data to make predictions to do model comparison etc etc.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Apologize for the first.",
                    "label": 0
                },
                {
                    "sent": "You know the 1st third of the talk will be rather tutorial, but I always find it useful to do that to get everybody on the page in terms of how I think about things.",
                    "label": 0
                },
                {
                    "sent": "So the great news for somebody like me who has a very poor memory is that everything follows from 2 rules.",
                    "label": 1
                },
                {
                    "sent": "So when in doubt I just go back to these two rules.",
                    "label": 0
                },
                {
                    "sent": "There's some rule, the product rule and if I'm violating those two rules, I should be uncomfortable, OK?",
                    "label": 0
                },
                {
                    "sent": "The two rules are very straightforward.",
                    "label": 0
                },
                {
                    "sent": "The sum rule says that the probability of X is the sum over Y of the joint probability of X&Y if Y is continuous, we replace that with an integral.",
                    "label": 0
                },
                {
                    "sent": "The product rule says the joint probability of X&Y can be written as a product of the probability of X times the probability Y given X.",
                    "label": 0
                },
                {
                    "sent": "It's all very basic, but the emphasis here is almost everything I'm going to say follows from that.",
                    "label": 0
                },
                {
                    "sent": "So then we just replace exs and wise with things that we might be interested in machine learning.",
                    "label": 0
                },
                {
                    "sent": "For example D for data Theta four parameters, M for some unknown model.",
                    "label": 0
                },
                {
                    "sent": "And as a corollary of the sum and product rule we have Bayes rule which is basically the one rule you need to learn.",
                    "label": 0
                },
                {
                    "sent": "Learning is the process of going from your prior over the parameters, which is your expression of uncertainty in your model.",
                    "label": 0
                },
                {
                    "sent": "Before observing the data.",
                    "label": 0
                },
                {
                    "sent": "Through the likelihood function that tells you for every setting of the parameters with the probability of the data is that sort of a requirement for your model.",
                    "label": 0
                },
                {
                    "sent": "To be well defined into your posterior, which is everything you've learned about your parameters from the data.",
                    "label": 0
                },
                {
                    "sent": "Then if you want to do prediction again, you do use the sum rule in the product rule.",
                    "label": 1
                },
                {
                    "sent": "So if you want to predict some unknown quantity X, like the next datapoint or part of the data point that you've observed partially, or some any other unknown quantity of interest, then essentially the sum rule and product rule tell you that what you have to do is you take the predictions from each parameter value and you weight those predictions by this posterior probability.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "So it naturally incorporates the whole idea of ensemble learning.",
                    "label": 0
                },
                {
                    "sent": "All of Bayesian methods is about on some bling stuff, and it's the ensemble just comes from the sum rule here.",
                    "label": 0
                },
                {
                    "sent": "Notice there is no optimized rule in probability theory, so optimization only comes at the point where you've incorporated all your uncertainty and you are forced to make some kind of decision.",
                    "label": 0
                },
                {
                    "sent": "And then you make a decision that minimizes your expected loss in some way.",
                    "label": 0
                },
                {
                    "sent": "So now if you want to model comparison again, you apply these rules at a slightly higher level of models and you compute things like marginal likelihoods, which you've heard about, and so on.",
                    "label": 0
                },
                {
                    "sent": "You know if you haven't, then it's very straightforward, but worth thinking about.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a lot of a lot of my talk is going to be about sort of models with complicated structures and sort of about maybe 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "I used to think about the world was that one of the important problems in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Is learning automatically structure from data so you know all over machine learning whether you're talking about clustering or dimensionality reduction or variable selection or fitting dynamical systems, the data or hidden Markov models, the data or discovering hidden sources in your signals or learning the structure of the graphical model, you know I found all of these problems very interesting, and they're all.",
                    "label": 1
                },
                {
                    "sent": "Model structure learning or discovery problems and one way to do that is you come up with a set of models or some nested class of models and then you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cute, you know these marginal likelihoods or different models?",
                    "label": 0
                },
                {
                    "sent": "Then you do model comparison and you can you can select the model if you want.",
                    "label": 0
                },
                {
                    "sent": "Or you can average over the models that way.",
                    "label": 0
                },
                {
                    "sent": "That's the way a lot of people used to do things.",
                    "label": 0
                },
                {
                    "sent": "You know, I still think that's very useful and very interesting in some settings, but.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This sort of thinking has changed a bit rather than comparing a bunch of finite models.",
                    "label": 0
                },
                {
                    "sent": "Very elegant thing to do is to consider what happens in the limit of these models as they become infinite.",
                    "label": 0
                },
                {
                    "sent": "So so this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Brings us to Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea in Bayesian nonparametrics, or rather why I work in Bayesian nonparametrics.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is for two reasons.",
                    "label": 0
                },
                {
                    "sent": "The reason for doing a Bayesian approach is just the simplicity of the Bayesian framework.",
                    "label": 0
                },
                {
                    "sent": "Everything follows from those two rules.",
                    "label": 0
                },
                {
                    "sent": "So some nice simplicity and coherence arguments there.",
                    "label": 0
                },
                {
                    "sent": "The reason for going towards Nonparametrics is that when you start dealing with real world data and you start thinking about priors for your models for real data, you realize that the real world isn't a textbook statistical model.",
                    "label": 0
                },
                {
                    "sent": "Usually it's the real world is not a mixture of three Gaussians or a quadratic polynomial, it's very hard to find.",
                    "label": 0
                },
                {
                    "sent": "It's really exceedingly hard to find real examples of things that fit simplistic model assumptions.",
                    "label": 0
                },
                {
                    "sent": "Certainly I know have almost no real phenomena apart from some classical physics things that would be actually fit by linear model, although that's the most widely used thing in statistics.",
                    "label": 0
                },
                {
                    "sent": "So we want to do nonparametrics just to deal with the complexity of the data in the real world phenomenon.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, you know it's important to distinguish or understand what we mean by nonparametrics and the best way to understand that is just to start talking about parametric models of parametric model assumes some finite number of parameters, call them Theta and given those parameters they capture everything there is about the data.",
                    "label": 0
                },
                {
                    "sent": "So when I want to make predictions about future data X given Theta I can forget I can throw away my training data.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I can throw away D. So there are very nice to analyze, but the complexity of the model is somehow bounded by the fact that I've chosen a finite number of parameters.",
                    "label": 1
                },
                {
                    "sent": "So even if the amount of data is unbounded, my model is not going to be very flexible, and so actually funnily enough, in this world of, you know.",
                    "label": 1
                },
                {
                    "sent": "Use the catch phrase big data.",
                    "label": 0
                },
                {
                    "sent": "You know, it seems to make sense actually, to really start thinking about models that are flexible enough to extract interesting structure structure from the data.",
                    "label": 0
                },
                {
                    "sent": "Now, nonparametric models assume that the data distribution cannot be defined in terms of this finite set of parameters, but can often be defined in terms of some infinite dimensional parameter vector.",
                    "label": 1
                },
                {
                    "sent": "Or just think of that as a hidden function of some kind.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing about that infinite dimensional Theta is that the amount of information we can capture about the data grows with the size of the data set, and that's sort of a classical way of thinking about nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "It also means that nonparametric models tend to be memory based, so you need to store around growing numbers of data points or sufficient statistics of the data points, etc.",
                    "label": 0
                },
                {
                    "sent": "As you get more data.",
                    "label": 0
                },
                {
                    "sent": "But it makes some more flexible, which is sort of interesting for machine learning point of view.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you know again to sort of motivate why we want to do nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Really the emphasis is on flexibility.",
                    "label": 0
                },
                {
                    "sent": "If you have a flexible model you're hoping to get better predictive performance, and I think there's sort of ample evidence out there that you can get better predictive performance from nonparametric models.",
                    "label": 1
                },
                {
                    "sent": "And they tend to be more realistic.",
                    "label": 0
                },
                {
                    "sent": "So as a subjective Bayesian, I would be uncomfortable.",
                    "label": 0
                },
                {
                    "sent": "You know, assuming a linear model or something like that when I know that just can't be what's going on.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The other, really, you know, practically put the other reason to think about this is just to look around at what's been successful in the machine learning community, and I would argue that the things that have been really successful in machine learning are either nonparametric or pretty damn close to nonparametric.",
                    "label": 1
                },
                {
                    "sent": "OK, So what I mean by that is, you know, if you think about the kernel machine revolution.",
                    "label": 0
                },
                {
                    "sent": "From SVM's you know over 10 or 15 years ago, that was just sort of a nonparametric way of thinking about problems that suddenly jumped into the arena and made performance of methods extremely good compared to previous methods.",
                    "label": 0
                },
                {
                    "sent": "But also if you think about the current craze on deep networks, well there are many, many interesting aspects of deep networks.",
                    "label": 0
                },
                {
                    "sent": "But one of the aspects is that nobody talks about, you know, having fitted deep network with three hidden units or anything like that, all these deep networks have.",
                    "label": 0
                },
                {
                    "sent": "You know crazy numbers of hidden units and layers and connections, and essentially they are just really damn flexible models for modeling data, so they might be technically speaking parametric as they have only 100 million parameters.",
                    "label": 0
                },
                {
                    "sent": "But effectively speaking their operating in this nonparametric regime.",
                    "label": 0
                },
                {
                    "sent": "And even stupid methods like K nearest neighbors seem to do very well in.",
                    "label": 0
                },
                {
                    "sent": "These are nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "So my only caveat here is that depends on how you define successful.",
                    "label": 0
                },
                {
                    "sent": "Machine learning is also been very successful at just developing highly scalable methods, and obviously a very interesting challenge is making things highly scalable and still nonparametric.",
                    "label": 0
                },
                {
                    "sent": "And obviously I'm interested in that as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give sort of a.",
                    "label": 0
                },
                {
                    "sent": "A table relating problems to parametric nonparametric things out there.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the following.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here is just give examples of parametric models corresponding or replacement nonparametric things that you could do in the applications that you would use them for.",
                    "label": 0
                },
                {
                    "sent": "For example, if your application is function approximation then you know nice parametric thing might be to do polynomial regression and nonparametric thing would be to do Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "For classification you can, you know do logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Or you could do a kernelized Bayesian logistic regression, also known as Gaussian process classification OK etc etc.",
                    "label": 1
                },
                {
                    "sent": "Mixture models for either clustering or density estimation.",
                    "label": 0
                },
                {
                    "sent": "Then you take the infinite limit, you get the richly processed mixtures etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know it's very fruitful because lot of problems that we are interested in machine learning.",
                    "label": 0
                },
                {
                    "sent": "The cast as problems in nonparametric Bayesian modeling.",
                    "label": 0
                },
                {
                    "sent": "And so you know it can keep people in my community pretty busy.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Again, coming back to the word modern, the cornerstone of this field had the two cornerstones of this field have been Gaussian processes and Irish lay processes.",
                    "label": 0
                },
                {
                    "sent": "And here is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's sort of 1 slide of both things together, just to contrast them and compare them.",
                    "label": 0
                },
                {
                    "sent": "Though Gaussian process defines the distribution on functions, so you can think of a function drawn from this Gaussian process prior with mean function mu, an covariance function K, and this is basically just an infinite dimensional Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "So the mean function is just the mean of the functions that you draw.",
                    "label": 0
                },
                {
                    "sent": "Here is a little picture of a random function that was drawn from this Gaussian process prior.",
                    "label": 1
                },
                {
                    "sent": "An the covariance function is the kernel.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same sort of thing as the kernel in SVM.",
                    "label": 0
                },
                {
                    "sent": "And it's the infinite dimensional analogue of a covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "OK, dear Ishly process defines a distribution on distributions.",
                    "label": 0
                },
                {
                    "sent": "So now instead of drawing functions, I'm going to draw sort of think of 'em is non negative functions that integrate to one.",
                    "label": 0
                },
                {
                    "sent": "So distributions and you can say G is wrong from dealership process prior with mean G0 and scaling or concentration parameter Alpha which play exactly the same well the mean plays the same role as the mean here and this also plays a role as a variance parameter.",
                    "label": 0
                },
                {
                    "sent": "Dispersion parameter and.",
                    "label": 0
                },
                {
                    "sent": "So when you draw from Additionally process you get some random probability distribution an it's the infinite dimensional analogue of additional distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so these things have been studied.",
                    "label": 0
                },
                {
                    "sent": "You know this for over 100 years this for now 40 or so years and these are very well understood and still very very useful because in a lot of modeling applications we either have a function that we don't know or distribution that we don't know so we can plug these things wherever that happens.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for some of you who might be familiar with BMS and not familiar with Gaussian processes, it's useful to put these two on a slide together just to show how damn similar these things are.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I won't go through the derivation, but we can write the loss function minimized by a sort of.",
                    "label": 1
                },
                {
                    "sent": "Bog standard ordinary.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine with hinge loss in the following form, you're minimizing over some vector F, some quadratic form of in F with the kernel matrix in there plus some regularization constant C times the sum over your data points of the hinge loss on your data points.",
                    "label": 0
                },
                {
                    "sent": "This is for classification with Y being your class label and FI being the function value at your class label, and this little plus being this sort of.",
                    "label": 0
                },
                {
                    "sent": "Classical hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So now if I take again, I'm not doing the derivation for you would show you the final result of it.",
                    "label": 0
                },
                {
                    "sent": "If I take a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "For classification.",
                    "label": 0
                },
                {
                    "sent": "Then I can write the log probability of that Gaussian process for classification in the following form, the log of the Gaussian part gives me quadratic expression in F. The function values of at my end data points, which is exactly the same quadratic expression as here.",
                    "label": 0
                },
                {
                    "sent": "And then each of my data points in my data set for classification give me a log probability of that observed label given the function value at that point plus some constants from the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you look at these.",
                    "label": 0
                },
                {
                    "sent": "You stare at these two things and you say the same thing basically.",
                    "label": 0
                },
                {
                    "sent": "Or very slight variance on them and it you know in some ways is true.",
                    "label": 0
                },
                {
                    "sent": "The subtitle of this slide would be why I never use SVM's.",
                    "label": 0
                },
                {
                    "sent": "The reason I never use SVM's is just because anytime somebody could use an SVM for something, I can just think about the same kernel and the same thing in a Bayesian nonparametric way and just do a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And I just feel more comfortable doing that because I have a lot more intuitions about what's going on there.",
                    "label": 0
                },
                {
                    "sent": "But there's sort of more deep differences between these two things, so on the surface these things look the same, but you know the SVM use these sort of nonexistent optimize rule, which doesn't exist in probability theory, whereas the Gaussian process said, well, you know, if you had that prior on your function, you had that likelihood, then the thing you need to do is you need to integrate out or average out over this unknown function values F. So rather than minimizing this with respect.",
                    "label": 0
                },
                {
                    "sent": "F We average integrate over F there.",
                    "label": 1
                },
                {
                    "sent": "So essentially it handles the uncertainty in the unknown function by averaging, not minimization, and basically therefore you have a lot of the probabilistic semantics and so on, and some of the nice things that.",
                    "label": 1
                },
                {
                    "sent": "Occurs a consequence of that is that you can learn the kernel parameters automatically from the data in a very straightforward way, and you can make the kernel quite flexible and etc.",
                    "label": 1
                },
                {
                    "sent": "You can even learn this regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "See without any cross validation, because you can just rewrite this parameter C as a single scale parameter on your kernel.",
                    "label": 1
                },
                {
                    "sent": "Single scaling on your kernel, and that's sort of a standard thing people actually learn in the Gaussian process literature since 10 years ago or so.",
                    "label": 0
                },
                {
                    "sent": "And and I know in a sense I just.",
                    "label": 0
                },
                {
                    "sent": "I'm just more comfortable because I find it more intuitive than I can sample for my prior to see exactly what my assumptions were that are kind of captured by the kernel, etc.",
                    "label": 0
                },
                {
                    "sent": "You can do automatic feature selection, etc and you know you can do most of these things in SVM context as well.",
                    "label": 0
                },
                {
                    "sent": "Let's just be clear, but it's nice to put the two on the same slide, and there's some nice MATLAB code if you are not familiar with GPS that you could play with.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the big picture, and you know, for any of you who seen me give talks, I often tend to have stuff like this in my talks.",
                    "label": 0
                },
                {
                    "sent": "I'm probably apologize for those of you see this particular cube, but basically we can relate a whole bunch of models together by starting from a simple humble model like linear regression and considering what happens to linear regression when you apply different operations to it.",
                    "label": 0
                },
                {
                    "sent": "So one operation you can do to linear regression is to turn it into a classification model by applying some transformation on the outputs or some link function on the output.",
                    "label": 0
                },
                {
                    "sent": "So an example of something that you get that's analogous to the linear regression is logistic regression.",
                    "label": 0
                },
                {
                    "sent": "By applying that classification link.",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do to linear regression is, instead of optimizing your parameters by least squares or maximum likelihood, you can do Bayesian inference over your parameters, and then you get Bayesian linear regression.",
                    "label": 0
                },
                {
                    "sent": "Standard textbook Bayesian stuff.",
                    "label": 0
                },
                {
                    "sent": "And another thing we all know you can do to linear methods is you can kernelized them.",
                    "label": 0
                },
                {
                    "sent": "So here you would get kernel linear regression.",
                    "label": 0
                },
                {
                    "sent": "Kernelized linear regression and now with these three operations we can think about what happens if we apply them in different orders.",
                    "label": 0
                },
                {
                    "sent": "So of course we can get kernel classification and an SVM for example would live in this corner of this cube.",
                    "label": 0
                },
                {
                    "sent": "We can get Bayesian logistic regression here.",
                    "label": 1
                },
                {
                    "sent": "The Bayesian form of kernel regression, or analogously the kernelized form of Bayesian linear regression is called Gaussian process regression which I just sort of described to you a little bit.",
                    "label": 0
                },
                {
                    "sent": "And then if you combine all of these operations together you would get Gaussian process classification which if you wanted and you were perverse you could call something like Kernelized Bayesian linear classification.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The idea in this talk is that I'm going to try to just move ahead from where we were and talk about a whole bunch of things very quickly.",
                    "label": 0
                },
                {
                    "sent": "And mostly I just want you to get the high level idea of what's going on, and then you can follow up.",
                    "label": 0
                },
                {
                    "sent": "Follow it up by reading the papers and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So we have functions and densities or distributions that we can do nonparametrics on, and that's very well known, but we want to do nonparametric modeling on things like sparse matrices, situations where you have overlapping clusters, networks, exchangeable arrays, covariances, hierarchies, etc.",
                    "label": 1
                },
                {
                    "sent": "So all sorts of other complicated stuff, and so just to give you an overview of stuff, at least that we've been working on in this area, and there are a lot of other people have been doing excellent work in this area as well.",
                    "label": 0
                },
                {
                    "sent": "I'll just dive into this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So sparse binary matrices, you know, once you have a distribution on sparse binary matrices, it's amazing how many uses you can put that too, and I'll describe a couple of them.",
                    "label": 1
                },
                {
                    "sent": "So here is one way of coming up with a distribution on sparse binary matrices that is non parametric in the sense that you're allowing yourself to have potentially infinitely many rows and columns.",
                    "label": 0
                },
                {
                    "sent": "In a way that you can still do interesting computations and so on with.",
                    "label": 0
                },
                {
                    "sent": "So the semantics of this that I want you to think about is we have some objects or data points and they can have some features an an element zed N K = 1 means that object N has some hidden feature K. And the basic idea is just like clustering where we might want to have K clusters and think of what happens when K goes to Infinity, and that's where, for example, you get additional process mixture.",
                    "label": 0
                },
                {
                    "sent": "Here we want to think about models with factorial or distributed representations that are sparse where you allow yourself to have infinitely many features.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take the limit of the number of features going to Infinity, but the way we generate a finite matrix like this, the simplest way that we could think about.",
                    "label": 0
                },
                {
                    "sent": "Is draw these ones and zeros from some Bernoulli distribution with parameters Theta, K and Theta K controls how popular a particular feature is, and this is an unknown parameter, so we have to give it a prior or nice prior we can give it.",
                    "label": 0
                },
                {
                    "sent": "Is this beta distribution with parameters Alpha over Big K. The number of columns in this matrix, one is sort of the simplest thing we can think about, and the reason we're scaling by Big K the number of columns.",
                    "label": 0
                },
                {
                    "sent": "Is because then we can take the limit as the number of columns goes to Infinity and get some very nice properties that make this both computationally attractive and attractive from the probability theory point of view.",
                    "label": 0
                },
                {
                    "sent": "So in particular what happens is that as the number of columns goes to Infinity, the matrix gets bigger and bigger, but it also gets sparser and sparser.",
                    "label": 1
                },
                {
                    "sent": "So essentially the expected number of non zero entries is still bounded.",
                    "label": 1
                },
                {
                    "sent": "So if you have any rose on average in each row, you're going to have Alpha number of ones, and in fact there are going to be drawn from the number of ones are going to be drawn from plus an Alpha distribution.",
                    "label": 1
                },
                {
                    "sent": "So this infinite limit is the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "Which is now being used in a bunch of different applications where you can stake a sparse matrix in usefully.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so here is 1 particular kind of application from meets at all, which is a form of binary matrix factorization.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by binary matrix factorization is that is not the matrix that of data is binary, but that we're going to do is we're going to factor this observed data, which could be something like here it's jeans by patients gene expression data.",
                    "label": 0
                },
                {
                    "sent": "It could be users by movies or whatever you want.",
                    "label": 0
                },
                {
                    "sent": "That's expressible in a matrix.",
                    "label": 0
                },
                {
                    "sent": "We're going to factor it by saying that associated with every row of the matrix, we have a binary vector, the hidden binary vector and associated with every column of the matrix.",
                    "label": 0
                },
                {
                    "sent": "We also have a hidden binary vector, and that's going to be our representation that's used to model the stuff that's inside the matrix, so it's a generalization of, for example Co clustering where you just cluster the rows and columns simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Here you're doing something kind of more.",
                    "label": 0
                },
                {
                    "sent": "Rich or expressive?",
                    "label": 0
                },
                {
                    "sent": "Using these binary vectors.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, just to put things in perspective, if we start from something like a finite mixture model, like a mixture of Gaussians, we can consider different kinds of conceptual operations we could do to that we can.",
                    "label": 0
                },
                {
                    "sent": "We can consider non parametric version of that where the number of components goes to Infinity and that would be the Dirichlet process mixture.",
                    "label": 0
                },
                {
                    "sent": "We can consider time series generalizations of mixture models, and that's how you get hidden Markov models and we can consider factorial models and factorial models.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as well in a mixture model.",
                    "label": 0
                },
                {
                    "sent": "Data point belongs to only one cluster, one and only one cluster in a factorial model, you know data points can belong to multiple overlapping clusters at the same time, or equivalently they have some sort of factor.",
                    "label": 1
                },
                {
                    "sent": "There's some facts factored hidden state space that models the data.",
                    "label": 0
                },
                {
                    "sent": "So again, combining these two things, we get models for example based on the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "Or you know, time series models generalizing the Dirichlet process mixture like the infinite hidden Markov model or HDP HMM?",
                    "label": 0
                },
                {
                    "sent": "And you know more complicated things like the infinite factorial hidden Markov model, etc.",
                    "label": 0
                },
                {
                    "sent": "So it's just useful to kind of understand how things are related, because for any modeling application, you have to sort of wonder where in the space of models should you start to explore.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's talk about networks and I'll try to keep this brief.",
                    "label": 0
                },
                {
                    "sent": "I'm actually speaking in the social network and social Media Workshop a little bit more about this.",
                    "label": 1
                },
                {
                    "sent": "They'll be a little bit of overlap with that, but if you want to hear it again with more detail, you can come to that in the afternoon here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're interested in modeling networks.",
                    "label": 1
                },
                {
                    "sent": "Alot of data can actually be thought of as networks or relational data.",
                    "label": 1
                },
                {
                    "sent": "For example, protein, protein interaction networks.",
                    "label": 0
                },
                {
                    "sent": "Obviously things like social networks, coauthorship networks, etc.",
                    "label": 1
                },
                {
                    "sent": "And we want to have models that are able to predict missing links in for latent properties or classes from the objects, or generalize learn properties from small observed networks to larger networks.",
                    "label": 1
                },
                {
                    "sent": "So these are the sorts of things that we want from our model and now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Let's just think about interesting models in this category.",
                    "label": 0
                },
                {
                    "sent": "So one very interesting kind of model is something called latent class models, which basically say that the nodes in my network have associated with them hidden class, which I've denoted here with this colored letter attached to each node.",
                    "label": 1
                },
                {
                    "sent": "And basically there's a single hidden discrete variable associated with each node, and then probability of having a link between a pair of nodes depends on.",
                    "label": 1
                },
                {
                    "sent": "On the latent classes of those two nodes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this corresponds to basically a clustering of the nodes or community detection as it's called in the network modeling literature.",
                    "label": 1
                },
                {
                    "sent": "And there are a lot of very interesting models of this kind.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, nice Bayesian nonparametric model in this category is given by Kemp at all 2006, called The Infinite Relational Model, where the basic idea is that you're doing community detection in a network like this and you have hidden classes and you're just going to use a Chinese restaurant process to allow yourself to have an unbounded number of classes.",
                    "label": 0
                },
                {
                    "sent": "And that's a pretty nice idea, because really you want to have.",
                    "label": 0
                },
                {
                    "sent": "A model where if you get a new node in the network like you know you get a new person coming into Facebook or something, then that person can either belong to one of the existing clusters or it can form its own cluster in this very naturally incorporates this idea and then the probability of a link between two nodes just depends on the classes of those two nodes.",
                    "label": 1
                },
                {
                    "sent": "By some simple probability transition matrix like that.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a very nice idea.",
                    "label": 0
                },
                {
                    "sent": "Here is a nice, even slightly more general idea called latent feature models and the basic idea here is, well, if you think about Facebook, you know I belong to a network associated with my colleagues at work, and I have a family as well and I went to high school somewhere.",
                    "label": 0
                },
                {
                    "sent": "So really any particular agent or object in the network belongs to multiple different communities at the same time.",
                    "label": 0
                },
                {
                    "sent": "So you really want models with overlapping clusters or communities.",
                    "label": 1
                },
                {
                    "sent": "So the way to think about that is that associated with each.",
                    "label": 0
                },
                {
                    "sent": "Node or a bunch of latent features denoted by these colored letters again and the link probability is going to determine be determined by the latent features of 1 node in the latent features of the other node.",
                    "label": 1
                },
                {
                    "sent": "It's thinking about linking to or not.",
                    "label": 0
                },
                {
                    "sent": "And again, obviously there are probably, you know, an infinite number of high schools out there, or workplaces out there, whatever.",
                    "label": 1
                },
                {
                    "sent": "So you really want a model that can accommodate a potentially unbounded or infinite number of these latent features.",
                    "label": 0
                },
                {
                    "sent": "So that model is called the latent feature relational model and essentially the way that work was done by Miller at all was to use an Indian buffet process instead of a Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "In the similar context of the previous model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now here is an example of.",
                    "label": 0
                },
                {
                    "sent": "And maybe even more complicated model for networks, but maybe more realistic again.",
                    "label": 0
                },
                {
                    "sent": "So the idea between this work, which was done with Konstantina Paula and David Knowles ICL last year this year I guess.",
                    "label": 0
                },
                {
                    "sent": "Is this infinitely attribute model for network data and the basic idea is that associated with each node object there's some number of latent attributes OK, and node can have an attribute or not have an attribute that's sort of basically like the ICP Indian buffet process, part of things.",
                    "label": 1
                },
                {
                    "sent": "But then the attributes can have discrete values associated with them, so within an attribute then there is a clustering going on and the probability of a link between object or an object J depends on the attributes and their values.",
                    "label": 1
                },
                {
                    "sent": "For node ion know J through some link function.",
                    "label": 0
                },
                {
                    "sent": "This like this and I'm not going to go into the detail but it sort of has some.",
                    "label": 0
                },
                {
                    "sent": "Intuitive explanation for what's going on here.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that you want to potentially unbounded number of attributes and values per attribute.",
                    "label": 0
                },
                {
                    "sent": "The way you do the number of attributes is within ICP and in buffet process the way you do the number of values for attribute is with Chinese restaurant process and it's sort of nicely generalizes this infinite relational model by Kemp.",
                    "label": 0
                },
                {
                    "sent": "It all in this non parametric latent feature relational model by Miller at all.",
                    "label": 0
                },
                {
                    "sent": "Intuitively.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way you can think about this is if you have a student friendship network at a place like Cambridge, students belong to colleges, they might play sports.",
                    "label": 0
                },
                {
                    "sent": "They might have political views or you know religious views or play some musical instrument or whatever.",
                    "label": 0
                },
                {
                    "sent": "Those attributes could be, and those attributes can have values like we have here at essentially the probability of two people linking kind of depends somehow on this constellation of stuff associated with them.",
                    "label": 0
                },
                {
                    "sent": "And all this stuff is not observed.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you observe are the links.",
                    "label": 0
                },
                {
                    "sent": "So it seems surprising you can actually learn a whole bunch of hidden.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stuff like this from data, but here is just some example of applying this to both this classical kind of nips.",
                    "label": 0
                },
                {
                    "sent": "Coauthorship Network data gene interaction network data, and the interesting thing about this table is, you know you could look either at the test there or the test log likelihood.",
                    "label": 1
                },
                {
                    "sent": "Let's look at the test error here.",
                    "label": 1
                },
                {
                    "sent": "Test their link prediction so the infinite relational model is giving us stay around 4%.",
                    "label": 1
                },
                {
                    "sent": "This more powerful latent feature infinite relational model is giving us 2%.",
                    "label": 0
                },
                {
                    "sent": "This infinitely attribute model is giving us 1%, so there's definitely looks like a significant drop there here 36 to 27 to about 7% error.",
                    "label": 0
                },
                {
                    "sent": "And The funny thing about this table is that all of these are Bayesian nonparametric models of networks, right?",
                    "label": 0
                },
                {
                    "sent": "So we're comparing all of these different models that have infinitely many potential hidden variables associated with them.",
                    "label": 0
                },
                {
                    "sent": "But how you structure your infinitely many hidden variables?",
                    "label": 0
                },
                {
                    "sent": "Maybe that's a lesson from here.",
                    "label": 0
                },
                {
                    "sent": "Can have an effect on how well you do in terms of predictive performance.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you were at the main conference.",
                    "label": 0
                },
                {
                    "sent": "A couple days ago.",
                    "label": 0
                },
                {
                    "sent": "We have some work in network modeling that makes use of some really interesting fundamental ideas in probability theory based on exchangeable arrays.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is if you want to do a network model and you want to have a network model where.",
                    "label": 0
                },
                {
                    "sent": "The ordering in which you look at the nodes is irrelevant, so you could reorder a re label the nodes and you should get the same probability.",
                    "label": 0
                },
                {
                    "sent": "Then you're living in the world of exchangeable arrays and exchangeable array.",
                    "label": 0
                },
                {
                    "sent": "Some array here is exchangeable if its probability distribution is the same as the probability that you would get from permuting the rows and the columns of the matrix for every.",
                    "label": 0
                },
                {
                    "sent": "Permit infinite permutation.",
                    "label": 0
                },
                {
                    "sent": "An the there is a very nice classical result from probability theory called the Aldous Hoover representation Theorem, which says that random matrix or array is exchangeable if and only if we can represent it in terms of some random function operating on some hidden variables associated with the nodes and the columns and these pairs of nodes and columns.",
                    "label": 0
                },
                {
                    "sent": "So pictorially.",
                    "label": 0
                },
                {
                    "sent": "The way you, well, let's talk about the interpretation 1st and then let's talk about this picture.",
                    "label": 0
                },
                {
                    "sent": "The interpretation is that any model of matrices or arrays or graphs where the order of the rows and columns or nodes is irrelevant, can be expressed by assuming some latent variables associated with each row and column, and some random function mapping these latent variables to the observations.",
                    "label": 1
                },
                {
                    "sent": "Well, how does this relate to Bayesian Nonparametrics?",
                    "label": 0
                },
                {
                    "sent": "Well, the catch here is that to be able to model any distribution over these matrices you have to have a flexible enough distribution on these random functions.",
                    "label": 0
                },
                {
                    "sent": "Pictorially, the way this is represented is the following.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this last figure here.",
                    "label": 0
                },
                {
                    "sent": "Imagine this is for example an adjacency matrix from some very large graph showing the links between nodes in some very large graph.",
                    "label": 0
                },
                {
                    "sent": "Now what this is saying is that this adjacency matrix can be modeled by saying that every node is embedded in the unit interval somewhere.",
                    "label": 0
                },
                {
                    "sent": "And then there is a function that Maps the locations of pairs of nodes into the probability that there is a link between them.",
                    "label": 0
                },
                {
                    "sent": "And that function is this sort of smoothie kind of thing here.",
                    "label": 0
                },
                {
                    "sent": "And the requirement for this sort of theorem is that you have to have flexible enough prior on these random functions to be able to capture these exchangeable race.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just that nips this year.",
                    "label": 0
                },
                {
                    "sent": "We had a paper obviously.",
                    "label": 0
                },
                {
                    "sent": "You know, if you went to it, I apologize.",
                    "label": 0
                },
                {
                    "sent": "I'm just doing 1 slide on this where we took this Aldous Hoover theorem representation very literally and we said let's build Bayesian nonparametric statistical model for networks based on this and essentially what we're going to do is we're going to say we have an unknown function, let's call a Theta.",
                    "label": 0
                },
                {
                    "sent": "Here's a flexible prior, an unknown functions a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "It has good.",
                    "label": 0
                },
                {
                    "sent": "Kind of coverage properties, so we're just going to use Gaussian process for that unknown function defined a model like this and then do inference from data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once we did this, the really interesting thing is that then we kind of went back to the literature and we looked at a lot of different models, both for graph data and for real valued arrays.",
                    "label": 0
                },
                {
                    "sent": "So you know.",
                    "label": 0
                },
                {
                    "sent": "You know the infinite relational model latent distance model.",
                    "label": 0
                },
                {
                    "sent": "I can I can models etc.",
                    "label": 0
                },
                {
                    "sent": "And for real value data arrays, things like probabilistic matrix, factorization's and Gaussian process, latent variable models, etc.",
                    "label": 0
                },
                {
                    "sent": "And a lot of these things can just be expressed as a particular assumptions on the structure of this Aldous Hoover based model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know, slightly to our surprise, it seemed to work.",
                    "label": 0
                },
                {
                    "sent": "So actually, you know it actually did good link prediction on data, despite the fact that we were mostly motivated by trying to bring the results in probability theory into the machine learning community through a model.",
                    "label": 0
                },
                {
                    "sent": "It actually seems to work to model things with the Gaussian process in as prior on that latent function.",
                    "label": 0
                },
                {
                    "sent": "And I won't go into the details of that.",
                    "label": 0
                },
                {
                    "sent": "Somebody who's chairing this tell me how much time I have so I can about 10 more minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So just to just to sort of describe that, let me let me actually spend a minute talking about this particular adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is, I believe, from the protein interaction network.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of nodes and you have a bunch of edges between them.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the adjacency matrix, suitably reordered by the latent space that we discovered in our algorithm, then the adjacency matrix has a funny structure that looks like this.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see it from the back there, but there is.",
                    "label": 0
                },
                {
                    "sent": "There appears to be a block at the corner here, so there's some community that's very heavily interconnected, and then there appears to be.",
                    "label": 0
                },
                {
                    "sent": "There some of those notes appear to be connected to a lot of other stuff, so they seem to be some kind of hubs in this community.",
                    "label": 0
                },
                {
                    "sent": "Global hubs from that community.",
                    "label": 0
                },
                {
                    "sent": "Then there is like a band structure here along the diagonal, which looks like the sort of distance based models, and then there is this funny butterfly type connectivity pattern here, which looks a bit like, you know, bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "Now if I sat down and try to come up with a model that had all of these different properties in it.",
                    "label": 0
                },
                {
                    "sent": "Chances are I wouldn't, you know if I had not known that I was looking for a model with those properties, chances are I wouldn't have stumbled into a model like that.",
                    "label": 0
                },
                {
                    "sent": "But this random function model nicely sort of just embeds the nodes into these latent spaces and says I'm just going to model that adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "Now that I've embedded it, the nodes into the unit square.",
                    "label": 0
                },
                {
                    "sent": "Or the unit interval for each node.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to just use a funky function that looks like this that approximately models that adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So it seems to be pretty flexible and actually in terms of, you know, area under the curve.",
                    "label": 0
                },
                {
                    "sent": "It kind of predicts better than some of these other models for networks.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so another interesting structure that we've been looking at, and of course also lots of other people, is nonparametric models for covariance matrices and the basic idea there is your object that you're trying to model is a covariance matrix, But you want to model that dependent on some covariates or input variables like time or other input variables, column X, and this is kind of relevant to areas like econometrics where people are interested.",
                    "label": 0
                },
                {
                    "sent": "In things like multivariate stochastic volatility, volatility is a function of time, but you could of course make these covariance matrices between different say assets or whatever be functions of whatever else you want.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, just one slide on this sort of basic idea here, and the key point here is that once you have some tools in your Arsenal of Bayesian nonparametric modeling, then there are nice ways of combining them together and get getting more rich and powerful models.",
                    "label": 0
                },
                {
                    "sent": "So for example, here the idea is we're trying to model covariance matrices that depend on time or some other variable.",
                    "label": 0
                },
                {
                    "sent": "So the object that we're trying to model.",
                    "label": 0
                },
                {
                    "sent": "Has a particular structure.",
                    "label": 0
                },
                {
                    "sent": "It's a symmetric positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "So we need to have distributions over symmetric positive definite matrices.",
                    "label": 0
                },
                {
                    "sent": "You can go to standard textbook and you find that a nice distribution for those matrices is that we shared distribution the way you get a we share distribution is you take a bunch of multivariate Gaussian variables and you take a sum of their outer products, and then you're guaranteed to be symmetric.",
                    "label": 0
                },
                {
                    "sent": "And if you take enough of terms in this sum, then you're also guaranteed to be positive definite.",
                    "label": 0
                },
                {
                    "sent": "With high probability.",
                    "label": 0
                },
                {
                    "sent": "So that's how a textbook would derive the Wishart distribution for you.",
                    "label": 0
                },
                {
                    "sent": "And now you combine that idea very straightforwardly with this idea of alishar process.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Gaussian process, and you say OK, Now I want this matrix to be dependent on time.",
                    "label": 1
                },
                {
                    "sent": "So what I'm going to do is I'm going to make these vectors instead of being Gaussian vectors.",
                    "label": 1
                },
                {
                    "sent": "I'm going to make them be Gaussian processes in time.",
                    "label": 0
                },
                {
                    "sent": "And then what you get is if the these are vector valued Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "You take the sum of their outer product's and you get a wish, our process, and that's sort of something that's been known in probability theory for awhile to decades, but it's been completely underused or very very rarely used, and it hasn't really that community and econometrics community haven't really interacted that much with the a lot of the machine learning know how, in terms of how to scale up Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "How to generalize Gaussian processes, etc.",
                    "label": 0
                },
                {
                    "sent": "And so you can think of a lot of what we did is just bringing these ideas together and trying to play with them and apply them to real data.",
                    "label": 0
                },
                {
                    "sent": "OK. OK. Good.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last kind of.",
                    "label": 0
                },
                {
                    "sent": "Topic in terms of modern basic nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to talk about was hierarchies.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of reasons why we want models with hierarchies in them.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the real world there are things that we believe might be generated from hierarchies like for example.",
                    "label": 0
                },
                {
                    "sent": "Animals are naturally organized into hierarchies.",
                    "label": 0
                },
                {
                    "sent": "We might want to use hierarchies inside our model to do parameter tying, so you know basically you want to type parameters closely nearby in the hierarchy and more loosely far away in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And hierarchies are just very useful for visualization and interpretability, and so they're widely used by lots of people you know.",
                    "label": 1
                },
                {
                    "sent": "Especially popular.",
                    "label": 0
                },
                {
                    "sent": "For example, in bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "Now the there are a lot of methods for doing hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "Again, you know you can go to one of my favorite textbooks dude and heart 1973 and you find like beautiful descriptions of various ways of doing hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "So what can we bring to the table?",
                    "label": 0
                },
                {
                    "sent": "Well, if you start to think about doing hierarchical modeling in a Bayesian context, then what you need is priors on trees.",
                    "label": 0
                },
                {
                    "sent": "Like this and in fact you need a little bit more than that, because given the tree structure, you might want to have not just the prior on the trees, but you want to have a prior on properties of the data that you could generate given the tree.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, if you're in the world of exchangeable data, then you don't want to prior on a finite tree with N nodes, which you could say learn on the data.",
                    "label": 0
                },
                {
                    "sent": "But then you know not know what to do.",
                    "label": 0
                },
                {
                    "sent": "If you have N + 1 nodes, you want priors on hierarchies that are coherent no matter how many leaves are in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So you want partners on infinite trees.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of work in this area.",
                    "label": 0
                },
                {
                    "sent": "You know, great really interesting ideas like Kingman's, coalescent and dearest lady Fusion trees and some of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "R and I would certainly consider this to be, you know, in the era of modern Bayesian nonparametric so around 2001 Radford, Neal proposed a very nice idea called the Dearsley diffusion tree, which is exactly of this form is a prior on hierarchies.",
                    "label": 0
                },
                {
                    "sent": "I won't go through this in great detail, but basically the way the hierarchy is generated is by.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagining that you have a process like Brownian motion process that evolves in some sort of fictitious time so the time is not real time, it's just an object that's used to generate the hierarchy for you.",
                    "label": 0
                },
                {
                    "sent": "The first data point follows this Brownian motion and sub somewhere.",
                    "label": 0
                },
                {
                    "sent": "Now the second data point.",
                    "label": 0
                },
                {
                    "sent": "What it does is it follows the path of the first one and then at some point given by some hazard function it diverges from the original path and ends up following a completely independent path after that point until time one.",
                    "label": 0
                },
                {
                    "sent": "And then if you follow this process for multiple points, what happens is the Third Point comes along follows the path of the first 2.",
                    "label": 0
                },
                {
                    "sent": "Then diverges at well.",
                    "label": 0
                },
                {
                    "sent": "At some point it either follows the path of the first point or the second point in proportion to how many points have followed that path before.",
                    "label": 0
                },
                {
                    "sent": "And then you know ends up somewhere etc.",
                    "label": 0
                },
                {
                    "sent": "What you get is a hierarchy over your data.",
                    "label": 0
                },
                {
                    "sent": "That live at the leaves here and this is the latent harkey that you've shown here.",
                    "label": 0
                },
                {
                    "sent": "So essentially, this is a very nice model and all we did was really try to think about ways of extending this model so that you don't get binary trees.",
                    "label": 0
                },
                {
                    "sent": "Here you get binary trees with probability one.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we get is trees with arbitrary branching factors and slightly generalizing the branching process to follow something called the Pitman yor process.",
                    "label": 0
                },
                {
                    "sent": "And this maintains exchange ability.",
                    "label": 0
                },
                {
                    "sent": "In the data, in naturally extends the dishley diffusion tree to arbitrary non binary branching is still infinitely exchangeable in the data as I just said, and the prior is sort of the most general kind of exchangeable prior.",
                    "label": 1
                },
                {
                    "sent": "You could have on trees.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll summarize and then I'll go onto this sort of more interesting kind of discussion slide.",
                    "label": 0
                },
                {
                    "sent": "So the summary is just that.",
                    "label": 0
                },
                {
                    "sent": "I've tried to give you an overview of, you know why we do the things we do from a Bayesian nonparametric framework.",
                    "label": 0
                },
                {
                    "sent": "And then I try to describe a whole bunch of more classical and more modern Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Of course heavily biased towards stuff that we've been doing in my group, so I you know it's not meant to be representative of everything in this community.",
                    "label": 0
                },
                {
                    "sent": "But the open challenge, I think, is that there are these two incredibly thriving communities out there doing both Bayesian and classical Nonparametrics, and we need to bring these communities a little closer together.",
                    "label": 0
                },
                {
                    "sent": "And there are several ways in which we can bring them closer together.",
                    "label": 0
                },
                {
                    "sent": "First of all alot of Bayesian nonparametric models are proposed.",
                    "label": 0
                },
                {
                    "sent": "But there is a little by the way of classical theory about them.",
                    "label": 0
                },
                {
                    "sent": "So for example proving consistency or convergence rates ETC.",
                    "label": 0
                },
                {
                    "sent": "For modern basic nonparametric models is an active area of research I think.",
                    "label": 0
                },
                {
                    "sent": "For the Gaussian process in their sleep process, there is a lot of good work already in this area, but for some of these newer things that is not so well understood.",
                    "label": 0
                },
                {
                    "sent": "And then some problems are just easier to handle in one framework than the other.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know density estimation if you say what's a nonparametric classical nonparametric way of doing density estimation?",
                    "label": 0
                },
                {
                    "sent": "Well, kernel density estimation is one is super easy, right?",
                    "label": 0
                },
                {
                    "sent": "You know you can explain it to anybody, you can implement it in a couple lines of code.",
                    "label": 0
                },
                {
                    "sent": "Very painless.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know, sometimes it works beautifully as well, right?",
                    "label": 0
                },
                {
                    "sent": "Whereas from a Bayesian POV, if you're trying to do then see estimation that you actually have to have a model of your unknown density.",
                    "label": 0
                },
                {
                    "sent": "If you want to be nonparametric then you end up with something.",
                    "label": 0
                },
                {
                    "sent": "The easiest thing I can think of is this Dirichlet process mixture.",
                    "label": 0
                },
                {
                    "sent": "There might be easier things, but that's sort of the most classical thing to do, and that's just bit harder.",
                    "label": 0
                },
                {
                    "sent": "You know you have to do MCMC or variational inference or something like that.",
                    "label": 0
                },
                {
                    "sent": "And then, but on the other hand, for complex modeling problems, as I hope I've illustrated, basic methods are very nice because you can compose these components into more complicated components.",
                    "label": 0
                },
                {
                    "sent": "Because you're sort of relying on Bayesian averaging, even though at every level you have infinitely many parameters, you're trying to average over them so it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "You don't have to worry about things like overfitting, you have to worry about making stupid assumptions, but you don't have to worry about overfitting.",
                    "label": 0
                },
                {
                    "sent": "And so that's sort of nice that they compose and you don't have to worry about overfitting regularization and things like that.",
                    "label": 0
                },
                {
                    "sent": "So I think what we should be doing more of is translating translating ideas for one framework to the other where possible, and we also need just more empirical and theoretical comparisons across these.",
                    "label": 0
                },
                {
                    "sent": "These two different approaches, so very few papers will actually have comparisons involving both kinds of things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}