{
    "id": "d54zdcrf6hozvah7btyy3ur44ctu4s7i",
    "title": "Robust PCA and Collaborative Filtering: Rejecting Outliers, Identifying Manipulators",
    "info": {
        "author": [
            "Constantine Caramanis, Department of Electrical and Computer Engineering, University of Texas at Austin"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_caramanis_rcf/",
    "segmentation": [
        [
            "Thank you very much for for inviting me here.",
            "So there was a workshop yesterday on robust high dimensional statistics and we had a last minute swap and so for those of you that were there, this this talk parts of this talk were presented there, so my apologies that hadn't been hadn't been the plan, so I'm going to tell you.",
            "A story about similar to the last talk, just in the in the sense that we have some corruption and we want to think about how to remove it, but it's in it's in a much simpler problem.",
            "Principle component analysis."
        ],
        [
            "So there's going to be 2 problems at all that I'll be able to talk about today, if only briefly.",
            "The first one is is looking at the most basic, one of the most basic statistical procedures that you can use that's been very widely applied in a wide variety of fields.",
            "Principle component analysis, except I'm going to be interested in the case where we have outliers.",
            "Alot of outliers, not just a few, but a constant fraction like 5%, one percent, 20%, and these outliers.",
            "I'll give you the.",
            "Precise model later for for the set up here, but they are completely arbitrary so I've struggled with my with my coauthors here on what word to use, because when you say outliers, it means different things to different people, so we're not assuming that these have high variance or low variance, or that they are from a different distribution.",
            "We're going to assume that these outliers are generated in a completely arbitrary manner, possibly even by a malicious adversary, possibly by an adversary that seen the other points, and that knows the algorithm that we're going to use.",
            "So quite powerful, not limited in any way other than of course the number of outliers.",
            "So then I'm going to look at PCA again, but look at a different application and show how show how this problem in fact is.",
            "Can is going to be the driver, but behind solving the collaborative filtering problem where you have manipulators.",
            "So the easiest way to describe collaborative filtering is as referenced by now famous Netflix problem.",
            "So Netflix has movies and users, and they're very interested in giving predictions.",
            "To users based on very partial observation of of what movies they've seen in rated about what movies they think Netflix thinks that you're going to like.",
            "And so the question.",
            "Of course, these recommender systems are everywhere Amazon uses this.",
            "Their Yelp relies on this, so I'm interested in this in the setting where you have a lot of people that are trying to manipulate the system so people that are giving bogus ratings because they want Yelp to give 11 restaurant a better recommendation than another, or they want Amazon to say if you look at this book, you're going to like this other book or Netflix, so these are the two problems that I'm going to look at.",
            "And it turns out that there are quite related."
        ],
        [
            "And there's a.",
            "There's an underlying theme that ties these together, and that is the regime that is particularly interesting, and the regime that's challenging.",
            "Is that of high dimensional data, so this is.",
            "This is featured prominently in all of all of NIPS.",
            "There are many posters that have talked about this, but let me just briefly go over it.",
            "So what is what is the high dimensional regime as opposed to what I'll call the classical regime?",
            "So the high dimensional regime is the setting where the dimensionality is approximately equal to the number of observations.",
            "So if you think about basic results you know about in statistics or probability, even even the most simple thing like law of large numbers, you fix the dimension and you let the number of points go to Infinity, so that's not at all what I'm interested in here.",
            "I'm interested in the case where you have maybe 10,000 points and maybe there in 50,000 dimensions.",
            "Or maybe you have 500 points and maybe there in 500 dimensions, so that is the regime and we can obtain scaling result.",
            "So in this regime as well, except that you just scale those two numbers to."
        ],
        [
            "Other.",
            "So there's many reasons why you why you care about why we might care about this, primarily application driven many examples from biology and bioinformatics fall exactly into this picture.",
            "Indeed, any situation where you can look in where all you can do is look in more in higher resolution at something you immediately fall into the high dimensional data.",
            "So for example.",
            "Think about a patient with a rare disease now.",
            "OK, so there's thousands of patients that have some rare that have some rare cancer, let's say.",
            "At one point we had we had very limited amount of data we could collect from patients like this.",
            "But now you can sequence their DNA.",
            "You can look at proteins.",
            "You can look at protein structure.",
            "So while the number of patients we hope is not increasing or not increasing faster than the population.",
            "The description of one of these data points is now in the many thousands, millions or or maybe or maybe more so.",
            "This is why high dimensional data is really prevalent in many different applications.",
            "One particular application that I'm that I'm interested in, and I think that.",
            "Deserves deserves more attention is where high dimensional data appear in networks, but since I'm not talking."
        ],
        [
            "About any of us today I'll I'll move."
        ],
        [
            "The point is that in this high dimensional regime, for many problems many basic problems like principal component analysis, traditional statistic tools don't work, or at least let me just say that they don't work for me.",
            "And you know, maybe there's some way that they can be.",
            "They can be made to work.",
            "I'll put this in the specific context of of this."
        ],
        [
            "We see a problem that I want to talk about, so here are the parts of this talk I'm going to talk about PCA twice.",
            "And a main part I'd like to take time to describe, or what, so what's on new or what so challenging about this about this high dimensional regime?",
            "After all, PCA is a problem that has been around for a long time.",
            "People have thought about outliers for a very long time.",
            "Why am I taking your time from the slopes to tell you about this?",
            "Then we're going to look at PCA from a completely different perspective, and this is what's going to lead to this generalization of collaborative filtering, so I'd like to acknowledge my coauthors here, so in this in the first part of the talk.",
            "My Co authors are Shimon or who is a colleague of mine.",
            "He's at the Technion and also one shoe.",
            "He's a postdoc at Utah, Austin, working with me, and he's about to start careers and assistant professor at National University of Singapore for the second part.",
            "This is joint work with my colleagues to Jason Gabby at University of Texas and also my student you dungchen and again 1 shoe my postdoc."
        ],
        [
            "OK, so let me so let's start in with PCA Part one here.",
            "So again, let me let me briefly remind you, although I guess this crowd doesn't need much of that.",
            "So what's PCA you observe some some points in high dimensions, but there's a low dimensional explanation for them, and what we're interested in doing is finding the least square error subspace approximation, and there's many.",
            "There's many applications, of course."
        ],
        [
            "Spend many different disciplines, so let me show you in pictures how this works.",
            "Here's the generative model that I have in mind, so X is a low dimensional signal.",
            "In this picture it would be a 1 dimensional 1 dimensional, a Maps.",
            "This low dimensional signal to a higher dimensions.",
            "In this case load dimensions is 1.",
            "High Dimension says 2 and you add noise.",
            "So the typical picture that we're comfortable with is that you get a signal nice healthy signal and it's bumped off of this true subspace with.",
            "And by."
        ],
        [
            "Some noise so."
        ],
        [
            "You get one point.",
            "You get another point.",
            "You get a bunch of points, so you see this.",
            "You see this cloud of points and what you'd like to do is recover the true subspace.",
            "So recover the least best square or subspace.",
            "OK, so meaning that I take the I want the subspace that minimizes the square to all all of these points."
        ],
        [
            "OK, so how do we do this?"
        ],
        [
            "So we do this using singular value decomposition.",
            "Again, that's something that everyone knows and I think everyone knows it so well that we sometimes might ignore an important aspect of singular value decomposition.",
            "And that is that it's solving a nonconvex problem.",
            "So if you try to write down this least square subspace approximation, you'll find that it's nonconvex.",
            "Nevertheless, singular value decomposition solves it exactly efficiently.",
            "Why am I belaboring this point?",
            "Because you might say, well, we know what you're going to do, you're going to talk about about outliers, and you're going to tell us that you're going to tell us that there's a huge dependence on these outliers.",
            "Susceptibility to outliers.",
            "Because of this, because of this quadratic objective.",
            "So why don't we do something like, not minimize the squared error, but minimize the error that flattens out?",
            "Maybe it becomes linear, or maybe it becomes completely flat.",
            "That seems like a sensible thing to do so that you're so that an adversary can't hurt you by putting a few points far away.",
            "Well, that's not going to work, because again, this is a non convex problem and if you replace a quadratic objective with something like a linear objective, then I don't know how to solve that problem tractably, so I'm interested not only in approaches that are that are well grounded from a statistical perspective.",
            "So algorithms that are guaranteed to give you a good answer, but one that algorithms that can be run efficiently.",
            "So I'm unfortunately stuck with this."
        ],
        [
            "With this quadratic objective.",
            "OK, so the consequences that for standard PCA it's rather well known that even a single outlier can arbitrarily skew the output, so I'm not interested in just one outlier.",
            "I'm interested in a lot in a constant fraction."
        ],
        [
            "Outliers.",
            "OK so I showed you a picture before of basic PCA, but there's there's two main differences between that picture with the blue points and what I care about.",
            "So one is the high dimensionality.",
            "What I showed you had many many blue points and just two dimensions.",
            "Here the inequality is completely reversed, so I don't have.",
            "I don't have this.",
            "These are going to be approximately equal in this case, so it's not.",
            "It's not 100 points in two dimension.",
            "That's going to be 100 points in 100 dimensions.",
            "That kind of setting.",
            "And I also am interested in the case for a constant points constant fraction of the points arbitrarily."
        ],
        [
            "Corrupted, so let's take these one at a time so corrupted data.",
            "I mean something like these.",
            "These black triangles here and as you can see, if you try to do the least square subspace approximation, it's going to ask you what you think is the best.",
            "Is the best subspace.",
            "So there's some error here."
        ],
        [
            "So let's look at this picture and maybe this.",
            "Maybe there's something here that gives us hope so you look over here you look over here and then you say, well, these look a little bit suspicious.",
            "I would never fall for this.",
            "I would never do this because first of all these points have a large magnitude.",
            "If they didn't have a large magnitude.",
            "If they were very close and mixed in with the blue points, they couldn't produce a big skew.",
            "OK, so first of all they have a large magnitude.",
            "The second bullet says the same thing.",
            "They have a large magnitude even adjusting for skewness.",
            "That's really all that small.",
            "Anomis distance is another observation that characterizes these points, as if you look at the volume of the smallest ellipsoid with the smallest volume containing these points and these points, you'll see that these outliers somehow have to increase that volume alot.",
            "So this is yet another characterization, and indeed all three of these have been used.",
            "And in robust PCA."
        ],
        [
            "So the unfortunate fact is that.",
            "None of these.",
            "None of these three things hold in high dimensions, so if you thought that there was, this gave you some intuitive, some hope intuitively that there was a way forward, then you're out of luck.",
            "If you're going to.",
            "If you're going to, you're going to rely on these three ideas."
        ],
        [
            "OK.",
            "So.",
            "What I want.",
            "Is something that's tractable, so I'm looking for an algorithm that can deal with outliers in high dimensions, and I don't want to pay a lot more than PCA.",
            "Yeah, certainly don't want anything that's non convex.",
            "And I'd like it to be robust to outliers in two different senses.",
            "So here's one sense that's sort of the classical notion in statistics.",
            "I haven't seen it that much in machine learning, so I'll briefly explain to you what this is.",
            "It's known as the breakdown point, so the breakdown point of an algorithm asks what fraction of outliers can I have before my algorithm produces complete garbage.",
            "No better than guessing.",
            "OK, so if your if your algorithm has a 5% breakdown point, it means that if there's 4% outliers it produces something that's better than a complete guess.",
            "It doesn't say how much better, but, but it's definitely better and 6%, no.",
            "No promises, you might as well just guess.",
            "OK, so the simplest, simplest example so that you can have a concrete idea in mind is think about the mean versus the median.",
            "So if you're trying to compute the average of a few numbers, I can put just a single number in there and arbitrarily skew what you think the meanness.",
            "Meanwhile, with a median, even if I corrupt a constant fraction of the numbers, I can't arbitrarily fool you into thinking that the median is something else.",
            "OK, so the mean the mean has breakdown .0 the median in fact has breakdown point 50% and you can see that 50% is the best breakdown point you could ever hope to have.",
            "If you have more than 50% outliers than there's no telling fact from fiction.",
            "So in addition to this, I actually want to do something better.",
            "I want to tell you what the breakdown point is, and I want specific guarantees up to that breakdown point.",
            "So I want robustness in these two senses.",
            "I'd also like it to be asymptotically optimal, so if I have 0%.",
            "Outliers, meaning you can still have alot any constant number, one outliers enough to completely full PCA.",
            "But I want a lot of 1020 sqrt N outliers.",
            "I want perfect perfect recovery in that case and I want something that's easily easily kernel isable."
        ],
        [
            "So let me give away one punch line of this talk relating to breakdown point.",
            "So to the best of my knowledge.",
            "Under these, under the setting of arbitrary corruption, the model the exact model is coming in the next slides.",
            "Under this model of of arbitrary outlier corrupt arbitrary corruption where the outlet with the adversary is not limited in magnitude, Justin the number of points.",
            "Any known algorithm for principal component analysis or robust principal component analysis.",
            "Many has a breakdown point of 0, so you give.",
            "Point, it's possible.",
            "I'm not saying that any corrupted set, it can't recover, but it's possible to devilishly construct set of even point 1% outliers.",
            "That will completely fool completely screw up any currently known robust PCA algorithm.",
            "So the algorithm that I'll present you now has breakdown point 50% and again that's the best possible that you could hope for.",
            "OK, so.",
            "As a brief teaser of."
        ],
        [
            "Of the results here.",
            "So so here's the here's a precise problem set up, so I do have a generative model.",
            "OK, so I have low dimensional points.",
            "It's mapped by a matrix, a matrix, the matrix A is tall and skinny.",
            "And I add noise, so this is a crucial component of what I'm what I'm going to talk about in this first part, so this noise is is going to be standard Gaussian.",
            "You can relax this assumption, but it turns out that this this already is enough to give us a lot of problems.",
            "So here's, I don't want you to remember too much notation, but here are the basics.",
            "X is low dimensional.",
            "You can even think of this as being 1 dimensional if you want to recover one principle component A is this tall and skinny matrix.",
            "If we only have one principle component, ages has one, ages has one is 1 big vector and N is noise, so X is low dimensional, AX is high dimensional, N is high dimensional and therefore Z is is high dimensional."
        ],
        [
            "So what I get to observe is.",
            "A bunch of authentic points, but note that authentic points also have have this noise component, so there are not clean in that sense, and outliers and these are generated as I said in a completely arbitrary fashion, and unfortunately I just get to see the mix.",
            "I don't know what Susie and I don't know what."
        ],
        [
            "What is oh?",
            "So the regime of interest is the case where the number of points I get to see is approximately equal to the dimension, and I'm interested in the case where these scale together we have finite finite sample bounds, but the asymptotical analysis is interesting as well, and as you would expect, it's a bit cleaner.",
            "Basically, anything with the login it goes away.",
            "So these NP scale together.",
            "And you should think about about these being very large and you should think about D being something small, so you want to recover a few principle components.",
            "OK, just one if you if you want to think about it that way and what's your signal strength?",
            "Your signal strength is basically going to be related to the largest singular value of A and I need this to be big bigger than one.",
            "So when I scale NNP this will have to scale, but it can scale at any slow rate that you choose logarithmically, double, logarithmically anything the results, the results till the results still carry over.",
            "So this is something big but not quite as big as that.",
            "That's how you should."
        ],
        [
            "You should think about these so the object is.",
            "The objective is to retrieve the column."
        ],
        [
            "Space of a.",
            "So here is one of the critical features of the high dimensional regime.",
            "In one thing that makes things quite complicated.",
            "So remember that low dimensional picture I showed you with the blue points.",
            "It had this nice skew.",
            "It looked like this ellipse.",
            "It didn't look like a circle at all.",
            "If I showed you a sphere of points and asked you to recover the principle components, they would send me home.",
            "There's no way that you can do that.",
            "Unfortunately.",
            "That's exactly what happens in the high dimensional regime.",
            "This noise what's what's the magnitude of every noise vector?",
            "It's going to be approx very close to the square root of the dimension, and we know that we have.",
            "Very sharp concentration results if you draw high dimensional Gaussian.",
            "It doesn't look like our Gaussian are Bell curve in one dimension.",
            "It basically looks like a uniform distribution on the sphere on the sphere of radius square root of P. But look at our poor signal strength.",
            "Here are signal strength is going to be something like Sigma which is the largest singular value of a scales very slowly.",
            "Whatever times some constant square root of D. This is the number of principal components, but this isn't even scaling.",
            "So if you look at the signal to noise ratio, basically so.",
            "It will be Sigma over square root of P. This is going to zero very quickly, so you really are just seeing you're seeing just a cloud of points perfectly.",
            "That have no no, it did not identifiable skew, so the magnitude of the true samples might be much bigger than the outlier magnitude.",
            "OK, that's something that couldn't happen in low dimensions.",
            "If you're.",
            "If you're outliers were actually going to skew and also the direction of every sample will be approximately orthogonal to the direction of the signal."
        ],
        [
            "Let me show you that again in pictures here.",
            "So this is our old story.",
            "Nice strong signal, small noise."
        ],
        [
            "In high dimensions, that's not what we see.",
            "Will see a tiny signal and a large noise vector in some arbitrary direction.",
            "And what that means is that is that with overwhelming probability this direction is going to be perpendicular to the signal that you want to."
        ],
        [
            "So you."
        ],
        [
            "See points like this and ultimately what you'll get.",
            "I can only draw in two dimensions here, but this is a cloud of points and every point is equidistant from the origin and equidistant from every other point."
        ],
        [
            "And.",
            "And every single point that you see is going to be.",
            "Up is going to be asymptotically, it will be exactly perpendicular to the signal space you want to you want to recover.",
            "Think about how opposite this is from that cloud of blue points on the ellipse.",
            "There were many points that were made very acute angles with the direction I wanted to recover, so this is completely opposite from."
        ],
        [
            "From what we're used to, so as a consequence of this, here are some things that won't work.",
            "If you try to get a robust estimate of the covariance well, you can get a robust.",
            "You can get a very robust estimate that gives you that gives you an epsilon Erin Infinity Norm.",
            "So think about if you actually in principle component analysis, you care about the spectral norm.",
            "So think about us.",
            "Unfortunately, Infinity Norm on a matrix of epsilon translates into a terrable spectral spectral norm guarantee.",
            "Basically one that depends on N. You could do things like leave one out or subsample, but of course this won't work because we have so many outliers.",
            "Hey you can if you if you sample 10% well whatever that sample is going to have a lot of outliers in it.",
            "You could try to remove points that have large magnitude or look strange.",
            "Have have other other measures of sort of strangeness like large Mahalanobis distance or large saldano outlying this these things off."
        ],
        [
            "So won't work and let me let me briefly show you why they won't.",
            "So this is the picture you have.",
            "All these points all in different dimensions.",
            "This is this is the direction that we want to recover."
        ],
        [
            "So in high dimensions are devilish adversary.",
            "There's always another, an unexploited dimension, so the adversary can put a few points that are clustered together and much much closer to the origin than all the other points.",
            "And because these are All in all kinds of different directions, basically one point on every direction.",
            "If you have a few points that are clustered, even if they have small magnitude print."
        ],
        [
            "Component analysis is going to be fooled and choose this choose this day."
        ],
        [
            "OK, so basically all of these approaches that are based on any of these kind of ideas or jobs."
        ],
        [
            "Stations are are not going to work, so these are so there are other algorithms that are.",
            "So everything I've talked about so far, algorithms that won't work because they are statistically a bad idea, but there are some other ideas.",
            "Some other algorithms that work in low dimensions that don't work because of algorithmic tract ability or algorithmic LL posdnous and one of them is this minimum volume ellipsoid.",
            "So if you have 10 points in 11 dimensions there contained in.",
            "In 0 dimensional ellipsoid, so any approach based on that won't work.",
            "There are many univariate estimators of variance.",
            "That are very successful and very robust to outliers.",
            "So you could try to find the direction which maximizes some kind of univariate estimator.",
            "The problem is that this these things are non convex and so you can't.",
            "You can't hope to do this."
        ],
        [
            "In any tractable way.",
            "So what's my objective again?",
            "So in one dimension you can think about angle.",
            "That's that's reasonable.",
            "You want you want to have a small."
        ],
        [
            "Angle, but really what I want is I want to know so what's PCA about?",
            "It's about finding explanatory directions so."
        ],
        [
            "I wonder how much variance is captured.",
            "So if I think this is the right the right subspace and I project onto that, you can see that there's a lot of variance that's captured."
        ],
        [
            "If I do the wrong subspace, there's very little variance that's captured, so that's what I want to do, that's my."
        ],
        [
            "Find the direction."
        ],
        [
            "That captures the most variance, so I have to drag you through one more piece of notation before I give you the algorithm, which which as I promised, is not more complicated than doing PCA a number of times, and it's this robust.",
            "It's this robust variance estimator, So what is this for any?",
            "So think about having just one principle component.",
            "You project all the points good in value, can tell onto this one principle component, and you compute what's known as a trimmed variance, so basically.",
            "I look at this look at the squared distance, but only of the points that are not the points that are farthest away.",
            "So let's say I'm expecting 10% outliers.",
            "I would look at the 90%, the 90% closest points and compute this empirical variance.",
            "So the idea is that if the outliers are small, their impact is controlled on that they're not going to influence that a lot if the if the outliers are very big, their impact is controlled because they're not going to.",
            "They're not going to be counted in this."
        ],
        [
            "OK, so."
        ],
        [
            "Just to show you that again in pictures.",
            "There's basically 2 settings that."
        ],
        [
            "And that can happen.",
            "You project onto one dimension if you found the correct direction, then you're going to have a good healthy variance for your authentic points.",
            "And even though you're going to, you're going to compute the robust variance estimate that's going to give you something big, and something that's not influenced too much by those outliers.",
            "If you chose a bad direction, one or the one where your points were all concentrated, then when you compute this robust variance estimate again, it's not going to be influenced by by those points, so you win in either.",
            "In either case, the trick."
        ],
        [
            "Is that you need to find something, some good direction.",
            "So again, let's focus on that bad example that I gave.",
            "So if you if the outlier if the adversary concentrates all the bad points there."
        ],
        [
            "In your full, then you choose that direction.",
            "This is what the projection is going to look like.",
            "Why?",
            "Because all of these blue points that had huge that had had large, very large magnitude when they're projected onto a particular direction, they now just look like it's like a like a univariate normal, so they're going to be quite small and the black points are going to be out there."
        ],
        [
            "And if you chew."
        ],
        [
            "Is the right direction?",
            "Venna then you're going to get something like this, and so the key idea is that if I were to choose between those two directions, my robust variance estimator would be able to distinguish and tell me that one of them is better than."
        ],
        [
            "Other.",
            "So here's here's the algorithm, and I think it's quite simple.",
            "And the only computationally challenging part is performing PCA, but presumably we were prepared to do this in the 1st place.",
            "So you perform PCA on the empirical covariance of the original of the points.",
            "All the points.",
            "You may get something that's terrible.",
            "But let's let's move on so you get your candidate.",
            "You get your candidate directions and then you look at what the robust variance estimate is in those highest in those high."
        ],
        [
            "Directions.",
            "OK, so again I get candidate candidate principle components.",
            "I compute the my robust variance estimate on those on those components.",
            "If it's the biggest one I've seen yet, I record it and I write down what those principle components are.",
            "That's my.",
            "This is my best guess then, regardless of what this step did I randomly remove a point in proportion to its variance along these directions.",
            "So let's see what could happen if I find a good direction here.",
            "I know it because this is big.",
            "And then when I randomly remove a point, I might remove a good point, but who cares?",
            "I've already found a good direction.",
            "If I find a very bad direction.",
            "So then my robust variance estimate is going to tell me ignore this and also the only reason I chose this direction must have been because principle component analysis and step one was fooled and that means that step three will will very likely remove one of these one of these."
        ],
        [
            "Bad points.",
            "OK, so."
        ],
        [
            "So basically this means just you have to just do PCA a number of times.",
            "Here I've written repeat until all points are removed, but you don't actually have to do that.",
            "You can you can repeat it until just a small fraction of them are are removed, so you have to pay a price.",
            "It's an order of magnitude more expensive than PCA, 'cause you have to run PC."
        ],
        [
            "Many times there are several things that can go wrong.",
            "You might remove authentic points.",
            "We might not ultimately report the best outcome.",
            "This robust variance estimate is just a proxy for what we really want and corrupted points may contribute to what we ultimately report.",
            "This is not an outlier identification."
        ],
        [
            "Technique, however, all."
        ],
        [
            "These things are controlled and basically not to put all, not to drag you through the notation.",
            "This theorem.",
            "This algorithm gets your breakdown point of 1/2, which is the best possible.",
            "It gives you perfect recovery if you have little oh of N corrupted points and you also get explicit lower bounds for all the way up to breakdown."
        ],
        [
            "Point of of 1/2 and the basic proof idea.",
            "Is this blessing of dimensionality.",
            "Concentration inequality is in high dimension and this idea that random removal either something good has happened or you're about to remove an outlier and therefore early on before removing too many good points, you find a good solution.",
            "So how much time do I have minus one minute?",
            "Minus two OK, so then I'll just just tell you what collaborative."
        ],
        [
            "String is, so the observation is that similar to what Alan talked about before, robust principal component analysis really looks like separating a mixture of not low rank plus sparse, which is what he briefly mentioned.",
            "But low rank was column sparse, and so if you read his paper then you'd say I'm going to solve it using this approach.",
            "And so we would expect that this solves that this gets."
        ],
        [
            "The solution the problem is it doesn't.",
            "This is not the solution to.",
            "This is not going to recover the true L and the true.",
            "See it's not going to do that for you.",
            "It saves us is that we don't, and so if you try to follow if you try to follow the proof technique there and try to find a proof of optimality of LNC, it's going to be vacuous.",
            "You can't write down a certificate of optimality for the true L in the true see, so we need to."
        ],
        [
            "New idea and the new idea is that I don't actually care about the true L and the true.",
            "See if I'm getting to see Em.",
            "All I need is a low rank matrix that has the right column space.",
            "That's all you care about.",
            "In principle, component analysis and all you need is a matrix.",
            "See that has the right column support.",
            "Who are the outliers?",
            "That's all that you need, so you can find in.",
            "You can find in using an Oracle approach.",
            "Anyway, you can find a different L in a different seat, not the original ones that have those two properties.",
            "And then you can write down a certificate of optimality."
        ],
        [
            "Those."
        ],
        [
            "So this."
        ],
        [
            "Is exactly the thing."
        ],
        [
            "That you have in."
        ],
        [
            "Collaborative filtering, except you have only partial observations, so these are your outliers.",
            "Are your or your manipulators, and so now you solve exactly the same, exactly the same convex problem, but only subject to partial observation and using this key idea of certifying optimality of a different solution.",
            "That's good enough for what you what you want.",
            "You end up solving.",
            "You end up solving them."
        ],
        [
            "So that's that, concludes my talk.",
            "Sorry for keeping you from the slopes.",
            "You can find out more either from my webpage.",
            "These papers are up or of course, email me.",
            "Thanks very much.",
            "Questions.",
            "You need to get the first however many you want.",
            "How many would you need in your algorithm 5?",
            "Not like no no.",
            "Average because you repeatedly do PCA because of course it is not involved.",
            "Doing, is it finding an online algorithm for this would be really nice.",
            "And I thought about that, but.",
            "Yeah.",
            "Program so they have slightly different assumptions.",
            "The convex approach doesn't have this nice generative model, it's just telling you I've got a low rank plus plus column sparse.",
            "Or the short answer is that we haven't really pushed both of them on like hard datasets to see which which is better, but I think that's an interesting, interesting question and they have different objectives as well, not in terms of the variance of the so.",
            "So I'm saying the theoretical guarantees are different for each of them are not immediately compareable.",
            "Velocify in like really imagine application for like say we have very high dimensionality, low number of points and is an adverse aerial agents putting points to screw algorithm potentially.",
            "Is the Amazon exactly gate?",
            "Wouldn't be high dimensional and lower points right there be similar number of points you have similar number of points if number of users, number of points, But let's say you're doing like a DNA microarray.",
            "So one experiment has 50,000 points of 2550 thousand points.",
            "How many experiments are you going to do?",
            "A couple 1000?",
            "So let's say a few of those get corrupted.",
            "Sorry it was random.",
            "OK, fair fair enough I mean, but what model are you going to write down for your ad for?",
            "Yeah for sure.",
            "I guess against even an adversarial model.",
            "Then the random thing is simpler.",
            "US against adversarial model, right?",
            "But it's too conservative.",
            "But the Netflix problem is an example like that.",
            "If there, if there were in fact a lot of users that wanted to manipulate the, there's another problem that I swept under the rug, which is that.",
            "I'm assuming that I have random you randomly sample that matrix that you see, but of course that's not true if you want to manipulate.",
            "Not only are you going to choose bad ratings to try to manipulate the system, you are also going to choose which ones to write poorly so that that I don't know how to deal with that, But yeah.",
            "Taking to account some information about the adversary or something else in the Yelp setting.",
            "Good.",
            "You know what I mean?",
            "Awards maybe?",
            "Which could you potentially exploit that?",
            "I think that in the that.",
            "It's I don't know how, but I don't see a reason why you couldn't try.",
            "You can try to do that.",
            "There's another, I mean.",
            "Another way to weaken the adversary in the first model is to allow him to see the points but not the noise.",
            "That's a little bit unreasonable, so so there the adversary could.",
            "There's this, there's this large Gaussian noise, and you are allowing our adversary to align or do anything.",
            "But having seen that noise, that seems like it's like it's too much, and that seems like a reasonable way without making any specific assumptions about the adversary to weaken him in a very reasonable way.",
            "But I also don't know.",
            "But yeah, what you said?",
            "I think in both approaches could possibly be be exploited.",
            "It seems that some restaurants.",
            "Pay Yelp to manipulate the rating, so that's the adversarial setting is indeed there.",
            "I don't know to what extent.",
            "Yeah, it's also not clear what to extent that we have low low rank assumptions and sort of a pessimistic assumption about the creativity of the human spirit.",
            "Anymore questions.",
            "Computer construction camera calibrations and they tend to use these ransack type things.",
            "Do you think about that?",
            "Maybe they are.",
            "We just need to estimate and the horrible noise.",
            "I haven't looked at that specific application example.",
            "I think.",
            "Is highly nonrandom.",
            "Then it's about as bad as this area.",
            "That's that's true.",
            "That's what we found from our experiments too.",
            "Yeah.",
            "Haven't looked at beds.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for for inviting me here.",
                    "label": 0
                },
                {
                    "sent": "So there was a workshop yesterday on robust high dimensional statistics and we had a last minute swap and so for those of you that were there, this this talk parts of this talk were presented there, so my apologies that hadn't been hadn't been the plan, so I'm going to tell you.",
                    "label": 0
                },
                {
                    "sent": "A story about similar to the last talk, just in the in the sense that we have some corruption and we want to think about how to remove it, but it's in it's in a much simpler problem.",
                    "label": 0
                },
                {
                    "sent": "Principle component analysis.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's going to be 2 problems at all that I'll be able to talk about today, if only briefly.",
                    "label": 0
                },
                {
                    "sent": "The first one is is looking at the most basic, one of the most basic statistical procedures that you can use that's been very widely applied in a wide variety of fields.",
                    "label": 0
                },
                {
                    "sent": "Principle component analysis, except I'm going to be interested in the case where we have outliers.",
                    "label": 0
                },
                {
                    "sent": "Alot of outliers, not just a few, but a constant fraction like 5%, one percent, 20%, and these outliers.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the.",
                    "label": 0
                },
                {
                    "sent": "Precise model later for for the set up here, but they are completely arbitrary so I've struggled with my with my coauthors here on what word to use, because when you say outliers, it means different things to different people, so we're not assuming that these have high variance or low variance, or that they are from a different distribution.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that these outliers are generated in a completely arbitrary manner, possibly even by a malicious adversary, possibly by an adversary that seen the other points, and that knows the algorithm that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "So quite powerful, not limited in any way other than of course the number of outliers.",
                    "label": 0
                },
                {
                    "sent": "So then I'm going to look at PCA again, but look at a different application and show how show how this problem in fact is.",
                    "label": 0
                },
                {
                    "sent": "Can is going to be the driver, but behind solving the collaborative filtering problem where you have manipulators.",
                    "label": 0
                },
                {
                    "sent": "So the easiest way to describe collaborative filtering is as referenced by now famous Netflix problem.",
                    "label": 1
                },
                {
                    "sent": "So Netflix has movies and users, and they're very interested in giving predictions.",
                    "label": 0
                },
                {
                    "sent": "To users based on very partial observation of of what movies they've seen in rated about what movies they think Netflix thinks that you're going to like.",
                    "label": 0
                },
                {
                    "sent": "And so the question.",
                    "label": 0
                },
                {
                    "sent": "Of course, these recommender systems are everywhere Amazon uses this.",
                    "label": 0
                },
                {
                    "sent": "Their Yelp relies on this, so I'm interested in this in the setting where you have a lot of people that are trying to manipulate the system so people that are giving bogus ratings because they want Yelp to give 11 restaurant a better recommendation than another, or they want Amazon to say if you look at this book, you're going to like this other book or Netflix, so these are the two problems that I'm going to look at.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that there are quite related.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a.",
                    "label": 0
                },
                {
                    "sent": "There's an underlying theme that ties these together, and that is the regime that is particularly interesting, and the regime that's challenging.",
                    "label": 0
                },
                {
                    "sent": "Is that of high dimensional data, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is featured prominently in all of all of NIPS.",
                    "label": 0
                },
                {
                    "sent": "There are many posters that have talked about this, but let me just briefly go over it.",
                    "label": 0
                },
                {
                    "sent": "So what is what is the high dimensional regime as opposed to what I'll call the classical regime?",
                    "label": 0
                },
                {
                    "sent": "So the high dimensional regime is the setting where the dimensionality is approximately equal to the number of observations.",
                    "label": 0
                },
                {
                    "sent": "So if you think about basic results you know about in statistics or probability, even even the most simple thing like law of large numbers, you fix the dimension and you let the number of points go to Infinity, so that's not at all what I'm interested in here.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in the case where you have maybe 10,000 points and maybe there in 50,000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you have 500 points and maybe there in 500 dimensions, so that is the regime and we can obtain scaling result.",
                    "label": 0
                },
                {
                    "sent": "So in this regime as well, except that you just scale those two numbers to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other.",
                    "label": 0
                },
                {
                    "sent": "So there's many reasons why you why you care about why we might care about this, primarily application driven many examples from biology and bioinformatics fall exactly into this picture.",
                    "label": 0
                },
                {
                    "sent": "Indeed, any situation where you can look in where all you can do is look in more in higher resolution at something you immediately fall into the high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Think about a patient with a rare disease now.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's thousands of patients that have some rare that have some rare cancer, let's say.",
                    "label": 0
                },
                {
                    "sent": "At one point we had we had very limited amount of data we could collect from patients like this.",
                    "label": 0
                },
                {
                    "sent": "But now you can sequence their DNA.",
                    "label": 0
                },
                {
                    "sent": "You can look at proteins.",
                    "label": 0
                },
                {
                    "sent": "You can look at protein structure.",
                    "label": 0
                },
                {
                    "sent": "So while the number of patients we hope is not increasing or not increasing faster than the population.",
                    "label": 0
                },
                {
                    "sent": "The description of one of these data points is now in the many thousands, millions or or maybe or maybe more so.",
                    "label": 0
                },
                {
                    "sent": "This is why high dimensional data is really prevalent in many different applications.",
                    "label": 0
                },
                {
                    "sent": "One particular application that I'm that I'm interested in, and I think that.",
                    "label": 0
                },
                {
                    "sent": "Deserves deserves more attention is where high dimensional data appear in networks, but since I'm not talking.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About any of us today I'll I'll move.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point is that in this high dimensional regime, for many problems many basic problems like principal component analysis, traditional statistic tools don't work, or at least let me just say that they don't work for me.",
                    "label": 0
                },
                {
                    "sent": "And you know, maybe there's some way that they can be.",
                    "label": 0
                },
                {
                    "sent": "They can be made to work.",
                    "label": 0
                },
                {
                    "sent": "I'll put this in the specific context of of this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see a problem that I want to talk about, so here are the parts of this talk I'm going to talk about PCA twice.",
                    "label": 0
                },
                {
                    "sent": "And a main part I'd like to take time to describe, or what, so what's on new or what so challenging about this about this high dimensional regime?",
                    "label": 0
                },
                {
                    "sent": "After all, PCA is a problem that has been around for a long time.",
                    "label": 0
                },
                {
                    "sent": "People have thought about outliers for a very long time.",
                    "label": 0
                },
                {
                    "sent": "Why am I taking your time from the slopes to tell you about this?",
                    "label": 0
                },
                {
                    "sent": "Then we're going to look at PCA from a completely different perspective, and this is what's going to lead to this generalization of collaborative filtering, so I'd like to acknowledge my coauthors here, so in this in the first part of the talk.",
                    "label": 0
                },
                {
                    "sent": "My Co authors are Shimon or who is a colleague of mine.",
                    "label": 0
                },
                {
                    "sent": "He's at the Technion and also one shoe.",
                    "label": 0
                },
                {
                    "sent": "He's a postdoc at Utah, Austin, working with me, and he's about to start careers and assistant professor at National University of Singapore for the second part.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my colleagues to Jason Gabby at University of Texas and also my student you dungchen and again 1 shoe my postdoc.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me so let's start in with PCA Part one here.",
                    "label": 0
                },
                {
                    "sent": "So again, let me let me briefly remind you, although I guess this crowd doesn't need much of that.",
                    "label": 0
                },
                {
                    "sent": "So what's PCA you observe some some points in high dimensions, but there's a low dimensional explanation for them, and what we're interested in doing is finding the least square error subspace approximation, and there's many.",
                    "label": 0
                },
                {
                    "sent": "There's many applications, of course.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spend many different disciplines, so let me show you in pictures how this works.",
                    "label": 1
                },
                {
                    "sent": "Here's the generative model that I have in mind, so X is a low dimensional signal.",
                    "label": 0
                },
                {
                    "sent": "In this picture it would be a 1 dimensional 1 dimensional, a Maps.",
                    "label": 0
                },
                {
                    "sent": "This low dimensional signal to a higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "In this case load dimensions is 1.",
                    "label": 0
                },
                {
                    "sent": "High Dimension says 2 and you add noise.",
                    "label": 0
                },
                {
                    "sent": "So the typical picture that we're comfortable with is that you get a signal nice healthy signal and it's bumped off of this true subspace with.",
                    "label": 0
                },
                {
                    "sent": "And by.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some noise so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get one point.",
                    "label": 0
                },
                {
                    "sent": "You get another point.",
                    "label": 0
                },
                {
                    "sent": "You get a bunch of points, so you see this.",
                    "label": 0
                },
                {
                    "sent": "You see this cloud of points and what you'd like to do is recover the true subspace.",
                    "label": 0
                },
                {
                    "sent": "So recover the least best square or subspace.",
                    "label": 0
                },
                {
                    "sent": "OK, so meaning that I take the I want the subspace that minimizes the square to all all of these points.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do this?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we do this using singular value decomposition.",
                    "label": 1
                },
                {
                    "sent": "Again, that's something that everyone knows and I think everyone knows it so well that we sometimes might ignore an important aspect of singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "And that is that it's solving a nonconvex problem.",
                    "label": 1
                },
                {
                    "sent": "So if you try to write down this least square subspace approximation, you'll find that it's nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, singular value decomposition solves it exactly efficiently.",
                    "label": 0
                },
                {
                    "sent": "Why am I belaboring this point?",
                    "label": 0
                },
                {
                    "sent": "Because you might say, well, we know what you're going to do, you're going to talk about about outliers, and you're going to tell us that you're going to tell us that there's a huge dependence on these outliers.",
                    "label": 0
                },
                {
                    "sent": "Susceptibility to outliers.",
                    "label": 0
                },
                {
                    "sent": "Because of this, because of this quadratic objective.",
                    "label": 0
                },
                {
                    "sent": "So why don't we do something like, not minimize the squared error, but minimize the error that flattens out?",
                    "label": 0
                },
                {
                    "sent": "Maybe it becomes linear, or maybe it becomes completely flat.",
                    "label": 0
                },
                {
                    "sent": "That seems like a sensible thing to do so that you're so that an adversary can't hurt you by putting a few points far away.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not going to work, because again, this is a non convex problem and if you replace a quadratic objective with something like a linear objective, then I don't know how to solve that problem tractably, so I'm interested not only in approaches that are that are well grounded from a statistical perspective.",
                    "label": 0
                },
                {
                    "sent": "So algorithms that are guaranteed to give you a good answer, but one that algorithms that can be run efficiently.",
                    "label": 0
                },
                {
                    "sent": "So I'm unfortunately stuck with this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this quadratic objective.",
                    "label": 0
                },
                {
                    "sent": "OK, so the consequences that for standard PCA it's rather well known that even a single outlier can arbitrarily skew the output, so I'm not interested in just one outlier.",
                    "label": 1
                },
                {
                    "sent": "I'm interested in a lot in a constant fraction.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Outliers.",
                    "label": 0
                },
                {
                    "sent": "OK so I showed you a picture before of basic PCA, but there's there's two main differences between that picture with the blue points and what I care about.",
                    "label": 0
                },
                {
                    "sent": "So one is the high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "What I showed you had many many blue points and just two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Here the inequality is completely reversed, so I don't have.",
                    "label": 0
                },
                {
                    "sent": "I don't have this.",
                    "label": 0
                },
                {
                    "sent": "These are going to be approximately equal in this case, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not 100 points in two dimension.",
                    "label": 0
                },
                {
                    "sent": "That's going to be 100 points in 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "That kind of setting.",
                    "label": 0
                },
                {
                    "sent": "And I also am interested in the case for a constant points constant fraction of the points arbitrarily.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Corrupted, so let's take these one at a time so corrupted data.",
                    "label": 1
                },
                {
                    "sent": "I mean something like these.",
                    "label": 0
                },
                {
                    "sent": "These black triangles here and as you can see, if you try to do the least square subspace approximation, it's going to ask you what you think is the best.",
                    "label": 0
                },
                {
                    "sent": "Is the best subspace.",
                    "label": 0
                },
                {
                    "sent": "So there's some error here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at this picture and maybe this.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's something here that gives us hope so you look over here you look over here and then you say, well, these look a little bit suspicious.",
                    "label": 0
                },
                {
                    "sent": "I would never fall for this.",
                    "label": 0
                },
                {
                    "sent": "I would never do this because first of all these points have a large magnitude.",
                    "label": 0
                },
                {
                    "sent": "If they didn't have a large magnitude.",
                    "label": 0
                },
                {
                    "sent": "If they were very close and mixed in with the blue points, they couldn't produce a big skew.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all they have a large magnitude.",
                    "label": 1
                },
                {
                    "sent": "The second bullet says the same thing.",
                    "label": 0
                },
                {
                    "sent": "They have a large magnitude even adjusting for skewness.",
                    "label": 1
                },
                {
                    "sent": "That's really all that small.",
                    "label": 0
                },
                {
                    "sent": "Anomis distance is another observation that characterizes these points, as if you look at the volume of the smallest ellipsoid with the smallest volume containing these points and these points, you'll see that these outliers somehow have to increase that volume alot.",
                    "label": 0
                },
                {
                    "sent": "So this is yet another characterization, and indeed all three of these have been used.",
                    "label": 0
                },
                {
                    "sent": "And in robust PCA.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the unfortunate fact is that.",
                    "label": 0
                },
                {
                    "sent": "None of these.",
                    "label": 0
                },
                {
                    "sent": "None of these three things hold in high dimensions, so if you thought that there was, this gave you some intuitive, some hope intuitively that there was a way forward, then you're out of luck.",
                    "label": 0
                },
                {
                    "sent": "If you're going to.",
                    "label": 0
                },
                {
                    "sent": "If you're going to, you're going to rely on these three ideas.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I want.",
                    "label": 0
                },
                {
                    "sent": "Is something that's tractable, so I'm looking for an algorithm that can deal with outliers in high dimensions, and I don't want to pay a lot more than PCA.",
                    "label": 0
                },
                {
                    "sent": "Yeah, certainly don't want anything that's non convex.",
                    "label": 0
                },
                {
                    "sent": "And I'd like it to be robust to outliers in two different senses.",
                    "label": 1
                },
                {
                    "sent": "So here's one sense that's sort of the classical notion in statistics.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen it that much in machine learning, so I'll briefly explain to you what this is.",
                    "label": 0
                },
                {
                    "sent": "It's known as the breakdown point, so the breakdown point of an algorithm asks what fraction of outliers can I have before my algorithm produces complete garbage.",
                    "label": 0
                },
                {
                    "sent": "No better than guessing.",
                    "label": 0
                },
                {
                    "sent": "OK, so if your if your algorithm has a 5% breakdown point, it means that if there's 4% outliers it produces something that's better than a complete guess.",
                    "label": 0
                },
                {
                    "sent": "It doesn't say how much better, but, but it's definitely better and 6%, no.",
                    "label": 0
                },
                {
                    "sent": "No promises, you might as well just guess.",
                    "label": 0
                },
                {
                    "sent": "OK, so the simplest, simplest example so that you can have a concrete idea in mind is think about the mean versus the median.",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to compute the average of a few numbers, I can put just a single number in there and arbitrarily skew what you think the meanness.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, with a median, even if I corrupt a constant fraction of the numbers, I can't arbitrarily fool you into thinking that the median is something else.",
                    "label": 0
                },
                {
                    "sent": "OK, so the mean the mean has breakdown .0 the median in fact has breakdown point 50% and you can see that 50% is the best breakdown point you could ever hope to have.",
                    "label": 0
                },
                {
                    "sent": "If you have more than 50% outliers than there's no telling fact from fiction.",
                    "label": 0
                },
                {
                    "sent": "So in addition to this, I actually want to do something better.",
                    "label": 0
                },
                {
                    "sent": "I want to tell you what the breakdown point is, and I want specific guarantees up to that breakdown point.",
                    "label": 0
                },
                {
                    "sent": "So I want robustness in these two senses.",
                    "label": 1
                },
                {
                    "sent": "I'd also like it to be asymptotically optimal, so if I have 0%.",
                    "label": 0
                },
                {
                    "sent": "Outliers, meaning you can still have alot any constant number, one outliers enough to completely full PCA.",
                    "label": 1
                },
                {
                    "sent": "But I want a lot of 1020 sqrt N outliers.",
                    "label": 0
                },
                {
                    "sent": "I want perfect perfect recovery in that case and I want something that's easily easily kernel isable.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give away one punch line of this talk relating to breakdown point.",
                    "label": 0
                },
                {
                    "sent": "So to the best of my knowledge.",
                    "label": 0
                },
                {
                    "sent": "Under these, under the setting of arbitrary corruption, the model the exact model is coming in the next slides.",
                    "label": 0
                },
                {
                    "sent": "Under this model of of arbitrary outlier corrupt arbitrary corruption where the outlet with the adversary is not limited in magnitude, Justin the number of points.",
                    "label": 0
                },
                {
                    "sent": "Any known algorithm for principal component analysis or robust principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "Many has a breakdown point of 0, so you give.",
                    "label": 0
                },
                {
                    "sent": "Point, it's possible.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that any corrupted set, it can't recover, but it's possible to devilishly construct set of even point 1% outliers.",
                    "label": 0
                },
                {
                    "sent": "That will completely fool completely screw up any currently known robust PCA algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm that I'll present you now has breakdown point 50% and again that's the best possible that you could hope for.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "As a brief teaser of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the results here.",
                    "label": 0
                },
                {
                    "sent": "So so here's the here's a precise problem set up, so I do have a generative model.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have low dimensional points.",
                    "label": 0
                },
                {
                    "sent": "It's mapped by a matrix, a matrix, the matrix A is tall and skinny.",
                    "label": 0
                },
                {
                    "sent": "And I add noise, so this is a crucial component of what I'm what I'm going to talk about in this first part, so this noise is is going to be standard Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You can relax this assumption, but it turns out that this this already is enough to give us a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "So here's, I don't want you to remember too much notation, but here are the basics.",
                    "label": 0
                },
                {
                    "sent": "X is low dimensional.",
                    "label": 0
                },
                {
                    "sent": "You can even think of this as being 1 dimensional if you want to recover one principle component A is this tall and skinny matrix.",
                    "label": 0
                },
                {
                    "sent": "If we only have one principle component, ages has one, ages has one is 1 big vector and N is noise, so X is low dimensional, AX is high dimensional, N is high dimensional and therefore Z is is high dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I get to observe is.",
                    "label": 0
                },
                {
                    "sent": "A bunch of authentic points, but note that authentic points also have have this noise component, so there are not clean in that sense, and outliers and these are generated as I said in a completely arbitrary fashion, and unfortunately I just get to see the mix.",
                    "label": 0
                },
                {
                    "sent": "I don't know what Susie and I don't know what.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is oh?",
                    "label": 0
                },
                {
                    "sent": "So the regime of interest is the case where the number of points I get to see is approximately equal to the dimension, and I'm interested in the case where these scale together we have finite finite sample bounds, but the asymptotical analysis is interesting as well, and as you would expect, it's a bit cleaner.",
                    "label": 0
                },
                {
                    "sent": "Basically, anything with the login it goes away.",
                    "label": 0
                },
                {
                    "sent": "So these NP scale together.",
                    "label": 0
                },
                {
                    "sent": "And you should think about about these being very large and you should think about D being something small, so you want to recover a few principle components.",
                    "label": 0
                },
                {
                    "sent": "OK, just one if you if you want to think about it that way and what's your signal strength?",
                    "label": 0
                },
                {
                    "sent": "Your signal strength is basically going to be related to the largest singular value of A and I need this to be big bigger than one.",
                    "label": 0
                },
                {
                    "sent": "So when I scale NNP this will have to scale, but it can scale at any slow rate that you choose logarithmically, double, logarithmically anything the results, the results till the results still carry over.",
                    "label": 0
                },
                {
                    "sent": "So this is something big but not quite as big as that.",
                    "label": 0
                },
                {
                    "sent": "That's how you should.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You should think about these so the object is.",
                    "label": 0
                },
                {
                    "sent": "The objective is to retrieve the column.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space of a.",
                    "label": 0
                },
                {
                    "sent": "So here is one of the critical features of the high dimensional regime.",
                    "label": 1
                },
                {
                    "sent": "In one thing that makes things quite complicated.",
                    "label": 0
                },
                {
                    "sent": "So remember that low dimensional picture I showed you with the blue points.",
                    "label": 0
                },
                {
                    "sent": "It had this nice skew.",
                    "label": 0
                },
                {
                    "sent": "It looked like this ellipse.",
                    "label": 0
                },
                {
                    "sent": "It didn't look like a circle at all.",
                    "label": 0
                },
                {
                    "sent": "If I showed you a sphere of points and asked you to recover the principle components, they would send me home.",
                    "label": 0
                },
                {
                    "sent": "There's no way that you can do that.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what happens in the high dimensional regime.",
                    "label": 0
                },
                {
                    "sent": "This noise what's what's the magnitude of every noise vector?",
                    "label": 0
                },
                {
                    "sent": "It's going to be approx very close to the square root of the dimension, and we know that we have.",
                    "label": 0
                },
                {
                    "sent": "Very sharp concentration results if you draw high dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look like our Gaussian are Bell curve in one dimension.",
                    "label": 0
                },
                {
                    "sent": "It basically looks like a uniform distribution on the sphere on the sphere of radius square root of P. But look at our poor signal strength.",
                    "label": 0
                },
                {
                    "sent": "Here are signal strength is going to be something like Sigma which is the largest singular value of a scales very slowly.",
                    "label": 0
                },
                {
                    "sent": "Whatever times some constant square root of D. This is the number of principal components, but this isn't even scaling.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the signal to noise ratio, basically so.",
                    "label": 0
                },
                {
                    "sent": "It will be Sigma over square root of P. This is going to zero very quickly, so you really are just seeing you're seeing just a cloud of points perfectly.",
                    "label": 0
                },
                {
                    "sent": "That have no no, it did not identifiable skew, so the magnitude of the true samples might be much bigger than the outlier magnitude.",
                    "label": 1
                },
                {
                    "sent": "OK, that's something that couldn't happen in low dimensions.",
                    "label": 0
                },
                {
                    "sent": "If you're.",
                    "label": 0
                },
                {
                    "sent": "If you're outliers were actually going to skew and also the direction of every sample will be approximately orthogonal to the direction of the signal.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you that again in pictures here.",
                    "label": 0
                },
                {
                    "sent": "So this is our old story.",
                    "label": 0
                },
                {
                    "sent": "Nice strong signal, small noise.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In high dimensions, that's not what we see.",
                    "label": 1
                },
                {
                    "sent": "Will see a tiny signal and a large noise vector in some arbitrary direction.",
                    "label": 0
                },
                {
                    "sent": "And what that means is that is that with overwhelming probability this direction is going to be perpendicular to the signal that you want to.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See points like this and ultimately what you'll get.",
                    "label": 0
                },
                {
                    "sent": "I can only draw in two dimensions here, but this is a cloud of points and every point is equidistant from the origin and equidistant from every other point.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And every single point that you see is going to be.",
                    "label": 1
                },
                {
                    "sent": "Up is going to be asymptotically, it will be exactly perpendicular to the signal space you want to you want to recover.",
                    "label": 1
                },
                {
                    "sent": "Think about how opposite this is from that cloud of blue points on the ellipse.",
                    "label": 0
                },
                {
                    "sent": "There were many points that were made very acute angles with the direction I wanted to recover, so this is completely opposite from.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From what we're used to, so as a consequence of this, here are some things that won't work.",
                    "label": 0
                },
                {
                    "sent": "If you try to get a robust estimate of the covariance well, you can get a robust.",
                    "label": 1
                },
                {
                    "sent": "You can get a very robust estimate that gives you that gives you an epsilon Erin Infinity Norm.",
                    "label": 0
                },
                {
                    "sent": "So think about if you actually in principle component analysis, you care about the spectral norm.",
                    "label": 0
                },
                {
                    "sent": "So think about us.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, Infinity Norm on a matrix of epsilon translates into a terrable spectral spectral norm guarantee.",
                    "label": 0
                },
                {
                    "sent": "Basically one that depends on N. You could do things like leave one out or subsample, but of course this won't work because we have so many outliers.",
                    "label": 0
                },
                {
                    "sent": "Hey you can if you if you sample 10% well whatever that sample is going to have a lot of outliers in it.",
                    "label": 0
                },
                {
                    "sent": "You could try to remove points that have large magnitude or look strange.",
                    "label": 1
                },
                {
                    "sent": "Have have other other measures of sort of strangeness like large Mahalanobis distance or large saldano outlying this these things off.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So won't work and let me let me briefly show you why they won't.",
                    "label": 0
                },
                {
                    "sent": "So this is the picture you have.",
                    "label": 0
                },
                {
                    "sent": "All these points all in different dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is this is the direction that we want to recover.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in high dimensions are devilish adversary.",
                    "label": 1
                },
                {
                    "sent": "There's always another, an unexploited dimension, so the adversary can put a few points that are clustered together and much much closer to the origin than all the other points.",
                    "label": 0
                },
                {
                    "sent": "And because these are All in all kinds of different directions, basically one point on every direction.",
                    "label": 0
                },
                {
                    "sent": "If you have a few points that are clustered, even if they have small magnitude print.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Component analysis is going to be fooled and choose this choose this day.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically all of these approaches that are based on any of these kind of ideas or jobs.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stations are are not going to work, so these are so there are other algorithms that are.",
                    "label": 0
                },
                {
                    "sent": "So everything I've talked about so far, algorithms that won't work because they are statistically a bad idea, but there are some other ideas.",
                    "label": 0
                },
                {
                    "sent": "Some other algorithms that work in low dimensions that don't work because of algorithmic tract ability or algorithmic LL posdnous and one of them is this minimum volume ellipsoid.",
                    "label": 1
                },
                {
                    "sent": "So if you have 10 points in 11 dimensions there contained in.",
                    "label": 0
                },
                {
                    "sent": "In 0 dimensional ellipsoid, so any approach based on that won't work.",
                    "label": 0
                },
                {
                    "sent": "There are many univariate estimators of variance.",
                    "label": 0
                },
                {
                    "sent": "That are very successful and very robust to outliers.",
                    "label": 1
                },
                {
                    "sent": "So you could try to find the direction which maximizes some kind of univariate estimator.",
                    "label": 0
                },
                {
                    "sent": "The problem is that this these things are non convex and so you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't hope to do this.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In any tractable way.",
                    "label": 0
                },
                {
                    "sent": "So what's my objective again?",
                    "label": 0
                },
                {
                    "sent": "So in one dimension you can think about angle.",
                    "label": 0
                },
                {
                    "sent": "That's that's reasonable.",
                    "label": 0
                },
                {
                    "sent": "You want you want to have a small.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Angle, but really what I want is I want to know so what's PCA about?",
                    "label": 0
                },
                {
                    "sent": "It's about finding explanatory directions so.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wonder how much variance is captured.",
                    "label": 0
                },
                {
                    "sent": "So if I think this is the right the right subspace and I project onto that, you can see that there's a lot of variance that's captured.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I do the wrong subspace, there's very little variance that's captured, so that's what I want to do, that's my.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the direction.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That captures the most variance, so I have to drag you through one more piece of notation before I give you the algorithm, which which as I promised, is not more complicated than doing PCA a number of times, and it's this robust.",
                    "label": 0
                },
                {
                    "sent": "It's this robust variance estimator, So what is this for any?",
                    "label": 1
                },
                {
                    "sent": "So think about having just one principle component.",
                    "label": 0
                },
                {
                    "sent": "You project all the points good in value, can tell onto this one principle component, and you compute what's known as a trimmed variance, so basically.",
                    "label": 0
                },
                {
                    "sent": "I look at this look at the squared distance, but only of the points that are not the points that are farthest away.",
                    "label": 0
                },
                {
                    "sent": "So let's say I'm expecting 10% outliers.",
                    "label": 0
                },
                {
                    "sent": "I would look at the 90%, the 90% closest points and compute this empirical variance.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if the outliers are small, their impact is controlled on that they're not going to influence that a lot if the if the outliers are very big, their impact is controlled because they're not going to.",
                    "label": 1
                },
                {
                    "sent": "They're not going to be counted in this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to show you that again in pictures.",
                    "label": 0
                },
                {
                    "sent": "There's basically 2 settings that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that can happen.",
                    "label": 0
                },
                {
                    "sent": "You project onto one dimension if you found the correct direction, then you're going to have a good healthy variance for your authentic points.",
                    "label": 0
                },
                {
                    "sent": "And even though you're going to, you're going to compute the robust variance estimate that's going to give you something big, and something that's not influenced too much by those outliers.",
                    "label": 0
                },
                {
                    "sent": "If you chose a bad direction, one or the one where your points were all concentrated, then when you compute this robust variance estimate again, it's not going to be influenced by by those points, so you win in either.",
                    "label": 0
                },
                {
                    "sent": "In either case, the trick.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that you need to find something, some good direction.",
                    "label": 0
                },
                {
                    "sent": "So again, let's focus on that bad example that I gave.",
                    "label": 0
                },
                {
                    "sent": "So if you if the outlier if the adversary concentrates all the bad points there.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In your full, then you choose that direction.",
                    "label": 0
                },
                {
                    "sent": "This is what the projection is going to look like.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because all of these blue points that had huge that had had large, very large magnitude when they're projected onto a particular direction, they now just look like it's like a like a univariate normal, so they're going to be quite small and the black points are going to be out there.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you chew.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the right direction?",
                    "label": 0
                },
                {
                    "sent": "Venna then you're going to get something like this, and so the key idea is that if I were to choose between those two directions, my robust variance estimator would be able to distinguish and tell me that one of them is better than.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other.",
                    "label": 0
                },
                {
                    "sent": "So here's here's the algorithm, and I think it's quite simple.",
                    "label": 0
                },
                {
                    "sent": "And the only computationally challenging part is performing PCA, but presumably we were prepared to do this in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So you perform PCA on the empirical covariance of the original of the points.",
                    "label": 1
                },
                {
                    "sent": "All the points.",
                    "label": 0
                },
                {
                    "sent": "You may get something that's terrible.",
                    "label": 0
                },
                {
                    "sent": "But let's let's move on so you get your candidate.",
                    "label": 0
                },
                {
                    "sent": "You get your candidate directions and then you look at what the robust variance estimate is in those highest in those high.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Directions.",
                    "label": 0
                },
                {
                    "sent": "OK, so again I get candidate candidate principle components.",
                    "label": 0
                },
                {
                    "sent": "I compute the my robust variance estimate on those on those components.",
                    "label": 0
                },
                {
                    "sent": "If it's the biggest one I've seen yet, I record it and I write down what those principle components are.",
                    "label": 0
                },
                {
                    "sent": "That's my.",
                    "label": 0
                },
                {
                    "sent": "This is my best guess then, regardless of what this step did I randomly remove a point in proportion to its variance along these directions.",
                    "label": 1
                },
                {
                    "sent": "So let's see what could happen if I find a good direction here.",
                    "label": 0
                },
                {
                    "sent": "I know it because this is big.",
                    "label": 0
                },
                {
                    "sent": "And then when I randomly remove a point, I might remove a good point, but who cares?",
                    "label": 0
                },
                {
                    "sent": "I've already found a good direction.",
                    "label": 0
                },
                {
                    "sent": "If I find a very bad direction.",
                    "label": 0
                },
                {
                    "sent": "So then my robust variance estimate is going to tell me ignore this and also the only reason I chose this direction must have been because principle component analysis and step one was fooled and that means that step three will will very likely remove one of these one of these.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bad points.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically this means just you have to just do PCA a number of times.",
                    "label": 0
                },
                {
                    "sent": "Here I've written repeat until all points are removed, but you don't actually have to do that.",
                    "label": 1
                },
                {
                    "sent": "You can you can repeat it until just a small fraction of them are are removed, so you have to pay a price.",
                    "label": 0
                },
                {
                    "sent": "It's an order of magnitude more expensive than PCA, 'cause you have to run PC.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many times there are several things that can go wrong.",
                    "label": 1
                },
                {
                    "sent": "You might remove authentic points.",
                    "label": 0
                },
                {
                    "sent": "We might not ultimately report the best outcome.",
                    "label": 0
                },
                {
                    "sent": "This robust variance estimate is just a proxy for what we really want and corrupted points may contribute to what we ultimately report.",
                    "label": 0
                },
                {
                    "sent": "This is not an outlier identification.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Technique, however, all.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These things are controlled and basically not to put all, not to drag you through the notation.",
                    "label": 0
                },
                {
                    "sent": "This theorem.",
                    "label": 0
                },
                {
                    "sent": "This algorithm gets your breakdown point of 1/2, which is the best possible.",
                    "label": 0
                },
                {
                    "sent": "It gives you perfect recovery if you have little oh of N corrupted points and you also get explicit lower bounds for all the way up to breakdown.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point of of 1/2 and the basic proof idea.",
                    "label": 0
                },
                {
                    "sent": "Is this blessing of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Concentration inequality is in high dimension and this idea that random removal either something good has happened or you're about to remove an outlier and therefore early on before removing too many good points, you find a good solution.",
                    "label": 0
                },
                {
                    "sent": "So how much time do I have minus one minute?",
                    "label": 0
                },
                {
                    "sent": "Minus two OK, so then I'll just just tell you what collaborative.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "String is, so the observation is that similar to what Alan talked about before, robust principal component analysis really looks like separating a mixture of not low rank plus sparse, which is what he briefly mentioned.",
                    "label": 0
                },
                {
                    "sent": "But low rank was column sparse, and so if you read his paper then you'd say I'm going to solve it using this approach.",
                    "label": 0
                },
                {
                    "sent": "And so we would expect that this solves that this gets.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The solution the problem is it doesn't.",
                    "label": 0
                },
                {
                    "sent": "This is not the solution to.",
                    "label": 0
                },
                {
                    "sent": "This is not going to recover the true L and the true.",
                    "label": 0
                },
                {
                    "sent": "See it's not going to do that for you.",
                    "label": 0
                },
                {
                    "sent": "It saves us is that we don't, and so if you try to follow if you try to follow the proof technique there and try to find a proof of optimality of LNC, it's going to be vacuous.",
                    "label": 0
                },
                {
                    "sent": "You can't write down a certificate of optimality for the true L in the true see, so we need to.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "New idea and the new idea is that I don't actually care about the true L and the true.",
                    "label": 1
                },
                {
                    "sent": "See if I'm getting to see Em.",
                    "label": 0
                },
                {
                    "sent": "All I need is a low rank matrix that has the right column space.",
                    "label": 1
                },
                {
                    "sent": "That's all you care about.",
                    "label": 0
                },
                {
                    "sent": "In principle, component analysis and all you need is a matrix.",
                    "label": 0
                },
                {
                    "sent": "See that has the right column support.",
                    "label": 0
                },
                {
                    "sent": "Who are the outliers?",
                    "label": 0
                },
                {
                    "sent": "That's all that you need, so you can find in.",
                    "label": 0
                },
                {
                    "sent": "You can find in using an Oracle approach.",
                    "label": 0
                },
                {
                    "sent": "Anyway, you can find a different L in a different seat, not the original ones that have those two properties.",
                    "label": 0
                },
                {
                    "sent": "And then you can write down a certificate of optimality.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is exactly the thing.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you have in.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Collaborative filtering, except you have only partial observations, so these are your outliers.",
                    "label": 1
                },
                {
                    "sent": "Are your or your manipulators, and so now you solve exactly the same, exactly the same convex problem, but only subject to partial observation and using this key idea of certifying optimality of a different solution.",
                    "label": 0
                },
                {
                    "sent": "That's good enough for what you what you want.",
                    "label": 0
                },
                {
                    "sent": "You end up solving.",
                    "label": 0
                },
                {
                    "sent": "You end up solving them.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's that, concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Sorry for keeping you from the slopes.",
                    "label": 0
                },
                {
                    "sent": "You can find out more either from my webpage.",
                    "label": 0
                },
                {
                    "sent": "These papers are up or of course, email me.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "You need to get the first however many you want.",
                    "label": 0
                },
                {
                    "sent": "How many would you need in your algorithm 5?",
                    "label": 0
                },
                {
                    "sent": "Not like no no.",
                    "label": 0
                },
                {
                    "sent": "Average because you repeatedly do PCA because of course it is not involved.",
                    "label": 0
                },
                {
                    "sent": "Doing, is it finding an online algorithm for this would be really nice.",
                    "label": 0
                },
                {
                    "sent": "And I thought about that, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Program so they have slightly different assumptions.",
                    "label": 0
                },
                {
                    "sent": "The convex approach doesn't have this nice generative model, it's just telling you I've got a low rank plus plus column sparse.",
                    "label": 0
                },
                {
                    "sent": "Or the short answer is that we haven't really pushed both of them on like hard datasets to see which which is better, but I think that's an interesting, interesting question and they have different objectives as well, not in terms of the variance of the so.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying the theoretical guarantees are different for each of them are not immediately compareable.",
                    "label": 0
                },
                {
                    "sent": "Velocify in like really imagine application for like say we have very high dimensionality, low number of points and is an adverse aerial agents putting points to screw algorithm potentially.",
                    "label": 0
                },
                {
                    "sent": "Is the Amazon exactly gate?",
                    "label": 0
                },
                {
                    "sent": "Wouldn't be high dimensional and lower points right there be similar number of points you have similar number of points if number of users, number of points, But let's say you're doing like a DNA microarray.",
                    "label": 0
                },
                {
                    "sent": "So one experiment has 50,000 points of 2550 thousand points.",
                    "label": 0
                },
                {
                    "sent": "How many experiments are you going to do?",
                    "label": 0
                },
                {
                    "sent": "A couple 1000?",
                    "label": 0
                },
                {
                    "sent": "So let's say a few of those get corrupted.",
                    "label": 0
                },
                {
                    "sent": "Sorry it was random.",
                    "label": 0
                },
                {
                    "sent": "OK, fair fair enough I mean, but what model are you going to write down for your ad for?",
                    "label": 0
                },
                {
                    "sent": "Yeah for sure.",
                    "label": 0
                },
                {
                    "sent": "I guess against even an adversarial model.",
                    "label": 0
                },
                {
                    "sent": "Then the random thing is simpler.",
                    "label": 0
                },
                {
                    "sent": "US against adversarial model, right?",
                    "label": 0
                },
                {
                    "sent": "But it's too conservative.",
                    "label": 0
                },
                {
                    "sent": "But the Netflix problem is an example like that.",
                    "label": 0
                },
                {
                    "sent": "If there, if there were in fact a lot of users that wanted to manipulate the, there's another problem that I swept under the rug, which is that.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that I have random you randomly sample that matrix that you see, but of course that's not true if you want to manipulate.",
                    "label": 0
                },
                {
                    "sent": "Not only are you going to choose bad ratings to try to manipulate the system, you are also going to choose which ones to write poorly so that that I don't know how to deal with that, But yeah.",
                    "label": 0
                },
                {
                    "sent": "Taking to account some information about the adversary or something else in the Yelp setting.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "You know what I mean?",
                    "label": 0
                },
                {
                    "sent": "Awards maybe?",
                    "label": 0
                },
                {
                    "sent": "Which could you potentially exploit that?",
                    "label": 0
                },
                {
                    "sent": "I think that in the that.",
                    "label": 0
                },
                {
                    "sent": "It's I don't know how, but I don't see a reason why you couldn't try.",
                    "label": 0
                },
                {
                    "sent": "You can try to do that.",
                    "label": 0
                },
                {
                    "sent": "There's another, I mean.",
                    "label": 0
                },
                {
                    "sent": "Another way to weaken the adversary in the first model is to allow him to see the points but not the noise.",
                    "label": 0
                },
                {
                    "sent": "That's a little bit unreasonable, so so there the adversary could.",
                    "label": 0
                },
                {
                    "sent": "There's this, there's this large Gaussian noise, and you are allowing our adversary to align or do anything.",
                    "label": 0
                },
                {
                    "sent": "But having seen that noise, that seems like it's like it's too much, and that seems like a reasonable way without making any specific assumptions about the adversary to weaken him in a very reasonable way.",
                    "label": 0
                },
                {
                    "sent": "But I also don't know.",
                    "label": 0
                },
                {
                    "sent": "But yeah, what you said?",
                    "label": 0
                },
                {
                    "sent": "I think in both approaches could possibly be be exploited.",
                    "label": 0
                },
                {
                    "sent": "It seems that some restaurants.",
                    "label": 0
                },
                {
                    "sent": "Pay Yelp to manipulate the rating, so that's the adversarial setting is indeed there.",
                    "label": 0
                },
                {
                    "sent": "I don't know to what extent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's also not clear what to extent that we have low low rank assumptions and sort of a pessimistic assumption about the creativity of the human spirit.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Computer construction camera calibrations and they tend to use these ransack type things.",
                    "label": 0
                },
                {
                    "sent": "Do you think about that?",
                    "label": 0
                },
                {
                    "sent": "Maybe they are.",
                    "label": 0
                },
                {
                    "sent": "We just need to estimate and the horrible noise.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at that specific application example.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Is highly nonrandom.",
                    "label": 0
                },
                {
                    "sent": "Then it's about as bad as this area.",
                    "label": 0
                },
                {
                    "sent": "That's that's true.",
                    "label": 0
                },
                {
                    "sent": "That's what we found from our experiments too.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Haven't looked at beds.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}