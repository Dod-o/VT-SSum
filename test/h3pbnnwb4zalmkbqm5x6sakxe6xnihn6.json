{
    "id": "h3pbnnwb4zalmkbqm5x6sakxe6xnihn6",
    "title": "A Novel Stability based Feature Selection Framework for k-means Clustering",
    "info": {
        "author": [
            "Dimitrios Mavroeidis, Radboud University Nijmegen"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_mavroeidis_novel/",
    "segmentation": [
        [
            "I will present in my talk on a novel stability based feature selection framework for K means clustering.",
            "I should initially specify that we use the term stability in a slightly different way than the first talk, and we're mostly interested in the stability of the output of clustering rather than the stability of the feature selection itself."
        ],
        [
            "So I will start this talk by presenting the main novelty and intuitions of the proposed framework, and then I will present the preliminary notions that are needed to get the main messages of the talk and this related to K means and PCA.",
            "The stability of PCA, and the relationship between feature selection and sparse PCA.",
            "And then I will move on to present the proposed framework, then pericle results and the 1st."
        ],
        [
            "Our work so you're probably familiar that there you probably know that there are various conceptually different approaches to feature selection, and its approach really defines what is a good feature selection in a slightly different way.",
            "Most of the approaches are based on some notion of relevant features, and then the algorithm really aims to select the set of most relevant features that can describe the task.",
            "In the context of this work, we take a slightly different perspective and we adopt A bias variance view over the feature selection problem.",
            "This is not entirely new, but it has not been widely studied in the machine learning literature.",
            "And so our main intuition is that we want to select the features that contribute to cluster separation and have low contribution to the variance of the clustering model.",
            "And we achieve this through a stability maximizing sparse PCA algorithm."
        ],
        [
            "So I will start the preliminary notions that are needed by discussing the relationship of K means clustering and principal components analysis.",
            "So I guess everyone is familiar with.",
            "K means clustering problem, which essentially tries to identify the clustering structure that minimizes the distances of the objects that belong in the it's cluster to the cluster centers.",
            "The most popular heuristic is Lloyd's algorithm that any EM style algorithm that iteratively updates the cluster standards and up and assigns the objects to the closest.",
            "Cluster centers and an alternative is to use principle components analysis to do what is called the continuous approximation to the discrete clustering problem.",
            "So if we start with clustering, we know that this is a discrete.",
            "Optimization problem because the goal is to find this discrete vector that assigns our instances to cluster.",
            "If we if we perform the continuous relaxation and allow this assignment vector to contain continuous values, then it could be shown that this convenience values can be derived by the eigenvectors of the covariance matrix, so which is essentially PCA and the details of the connection between PCA and K means are nicely illustrated.",
            "An ICM L 2004."
        ],
        [
            "Paper.",
            "So this slide is to understand the relationship between feature selection for gaming and sparse PCA.",
            "So we have seen that PCA can be used in order to provide continuous solution to the K means clustering problem.",
            "And in general, if we want to consider a baseline feature selection for K means it would be a problem of selecting the subset of features that approximates the Caymans objective in the continuous approximation case, a baseline feature selection for PC for principle components analysis would be really to select the subset of features that approximates the K means continuous clustering objective.",
            "But in the PCA case, the objective function is simply the eigenvalues of the covariance matrix and the features are simply the rows and columns of the covariance matrix.",
            "So feature selection in this case really reduces to selecting the rows and columns of the covariance matrix that approximate the eigenvalues.",
            "Of the covariance matrix, which is essentially a sparse PCA problem."
        ],
        [
            "I have said in the start of this talk that we want to select the fixtures such that the stability of the clustering output is maximized.",
            "So what we really want to do is to maximize the stability of the continuous solution that is derived by principal components analysis and we can do this by maximizing the difference between certain eigenvalues of the covariance matrix.",
            "And in order to achieve this, we should in order to maximize the stability of the K -- 1 dominant Egan vectors of the covariance matrix.",
            "What we should do is maximize the difference in the K management to the K eigenvalues of the covariance matrix.",
            "And of course, now that we have seen how we can maximize stability, a good question is if we select the features that maximize the stability of the eigenvectors, what would be the semantics of doing this of solving this optimization?"
        ],
        [
            "Problem.",
            "And so this is a slide with too many formulas.",
            "But let me go slowly from them one by one.",
            "So this is the covariance matrix.",
            "Along with feature selection.",
            "So you here is the vector that contains this 01 values and essentially performs the feature selection process.",
            "And here are the.",
            "Center data presentation.",
            "So this is the standard covariance matrix and this.",
            "Mother says really ensure that we perform.",
            "We select rows and columns for this covariance matrix.",
            "And we want to select.",
            "The rows and columns of the covariance matrix such that stability is maximized, so we won't select rows and columns of the covariance matrix such that this Eggen value difference that control stability is maximized.",
            "So if I wanted to do this directly, I should write the objective in the form, maximize with respect to the selected features which are contained in this vector such that Lambda 1 minus Lambda two is maximized.",
            "But you can see that I'm not doing exactly this, I'm doing something slightly different in order to facilitate the formulation of the objective function, and I'm essentially maximizing the difference of the first eigenvalue to the rest.",
            "And I'm taking the average, so this is slightly different, But since this is a maximization problem, we know that the objective function will get a better incentive of maximizing the difference between the two first eigenvalues that are closer to each other.",
            "So if I start by this objective function that's related to maximizing the stability of the output.",
            "We can easily derive this.",
            "In this form, which is a maximization with respect to the selected features of the first leg and value of the covariance matrix minus the trace.",
            "And if you're familiar with the sparse PC problems then you can easily recognize that if I.",
            "Omit this term then this is simply a standard sparse PCA problem.",
            "And what is interesting about interpreting what is achieved if someone maximizes stability?",
            "Be shown that this is equivalent to selecting features that on one hand maximize an index that measures that this discrimination between the two clusters and on the other hand selects the features that have low variance.",
            "So it can be shown.",
            "And it's not very difficult to show that if one starts by this feature selection problem.",
            "But selects the features that minimize this cluster separation minus the variance, and applies the standard continuous relaxation.",
            "To the cluster assignments.",
            "Then he simply.",
            "Derives stability based sparse PCA problem.",
            "And I can say that the main one of the main novelties of the work is that showing this nice interpretation of stability based sparse PCA as a feature selection mechanism that on one hand selects the features that maximize the discrimination between the clusters, and on the other hand minimizes the variance."
        ],
        [
            "So we have a sparse PCA problem to select the features and we achieve this by performing greedy forward search that optimizes certain lower bound on the objective.",
            "And this lower bound is shown here.",
            "It seems a bit complicated, but the main.",
            "Property of this bound is that it can be efficiently computed, so we can compute this bound by performing only one negative vector computation per greedy step.",
            "And.",
            "We can see that this is the eigenvector that we need to compute, and this XFC are essentially the centered feature representations.",
            "And this approach for optimizing sparse PCA is very similar to a nice ML2000 paper that did something similar.",
            "So they also derive some lower bound on the sparse PCA objective, but this was not stability based, so the difference here is that we derive a similar bound, but for the stability maximizing case."
        ],
        [
            "So the algorithm is quite straightforward after we have derived this bound and we simply select the features that maximize this bound and we continue.",
            "Alternatively until we have reached the desired number of features.",
            "Anne.",
            "You can observe that in the formulas that I have shown in the previous slide, and essentially trying to maximize the difference between the Lambda one and Lambda two eigenvalues.",
            "So this is this can help me compute only one stay Blegen vector of principle components analysis.",
            "In the case I want to compute multiple stable Egan vectors.",
            "I have"
        ],
        [
            "To use deflation and we also showing the paper how to do efficient deflation that is equivalent to sure compliment deflation which is achieved simply by using this formula and in the paper we show that this type of formula update is equivalent to compliment deflation."
        ],
        [
            "We have empirically evaluated our work against for Cancer Research datasets and we have used three methods with standard sparse PCA method with select the features that maximize solely the maximum eigenvalue of the covariance matrix.",
            "The stable sparse PCA method that maximizes the objective that I have shown you.",
            "And also a low variance sparse PCA method that gives higher weight to the higher penalty to the variance of the features.",
            "And we have performed both quantitative evaluation becausw the datasets we have.",
            "We know that the class labels, so we can use measures such as normalized mutual information.",
            "And we have also performed a qualitative evaluation becausw.",
            "We also looked at the relevant biology literature and we saw whether the jeans that were selected by our algorithm where meaningful in a qualitative sense."
        ],
        [
            "And here are the clustering the quantitative results.",
            "So here in the he axis we have the number of features that were retained by the algorithm in the axis we have the normalized mutual information.",
            "And we have compared the three sparse PCA methods against Laplacian score, which is a standard unsupervised feature selection algorithm and also with the recently proposed feature selection algorithm in KDD 2010 and also the baseline approach of simply selecting the features that have maximum variance and this maximum variance.",
            "Is a very commonly used baseline for unsupervised clustering algorithms.",
            "And we can see that.",
            "In all experiments.",
            "One of the three approaches is usually better.",
            "Or at least competitive."
        ],
        [
            "Two other feature selection approaches.",
            "And we can see this."
        ],
        [
            "All four datasets that we have used."
        ],
        [
            "And we have also performed the qualitative evaluation based mostly on the go loop data set, which is very well studied in the biology literature, and we have specifically looked for the features that we're uniquely selected by our algorithm and not by the others.",
            "And we have found that argument was able to detect some interesting features, which in this case with word jeans that were important for the specific problem and also were missed by the competitive methods, and we consider that the results illustrate that using a cluster separation versus various tradeoff is something that.",
            "Give some interesting results and warrants further research and I stress this becauses they usually feature selection algorithms unsupervised.",
            "One consider the high variance of a feature as a good characteristic cause it can.",
            "Be used to describe the data nicely, but the argument here is that.",
            "Maybe?",
            "This should be used as a tradeoff between cluster separation and variance.",
            "And not rely on variance solely by itself."
        ],
        [
            "As a further work, we will consider alternative optimization approaches instead of solely forward greedy search.",
            "We will also consider to extend our approach to kernel K means and spectral clustering, and one very interesting problem that we consider is also parameter tuning for the separation.",
            "Cluster separation versus various tradeoffs.",
            "So in this case we are currently looking for how one can automatically find the amount of penalty that should be put to variance in order to formulate this Lambda ones minus trace objective.",
            "And with this I conclude my talk and thank you very much for your attention.",
            "Do I will sign personal?",
            "I have a question.",
            "I think I missed what you said about the vacation and assured that patient.",
            "Would you mind?",
            "Yes if someone so if you if you go back and so."
        ],
        [
            "So.",
            "This is the basic way we do the greedy forward search.",
            "So what it's greedy step we need to compute this bound and in order to compute this bound with we need 2 elements.",
            "One is this V which is essentially the eigenvector that's computed based on the features we have.",
            "And this is the.",
            "Feature representation.",
            "So if someone applies to complement deflation directly, he will derive some new covariance matrix and then he would have to decompose it in order to derive the new feature representation in order to compute this bound.",
            "So this would add an extra cost to the algorithm because we would have first to do.",
            "The deflation, then the composition of the new covariance matrix is something like X * X transpose with it.",
            "So let's see the composition and then use the new feature representation in order to compute the formula.",
            "And in the case."
        ],
        [
            "So we avoid this burden by doing that deflation directly on the feature representations.",
            "So by using this formula we get directly the new feature representation that we can plug in the bound that we're computing.",
            "And so this gives us certain efficiency gains.",
            "And this is the main idea of.",
            "This deflation process.",
            "Question.",
            "Yep, I have one question.",
            "Very nice talk.",
            "So did you run any because you had in all these datasets, or at least most of these from from your talk.",
            "What I want to hear you had the labels.",
            "Did you try any supervised feature selection methods and compare what genes were selected?",
            "With this you know?",
            "I mean, that would be really nice because if you could show that the gene selected were equivalent because the labeling process itself is very costly.",
            "To get these kind of datasets like the goal of one, you it's not easy to get those and if this method ascentia Lee gives you the pig overlap of the features then that would be very useful and we have not done this experiment.",
            "We have mostly focused on unsupervised feature selection and OK.",
            "If I.",
            "Try to answer the different question.",
            "So what would be the best application for this type of?",
            "Cluster separation versus various tradeoff.",
            "I think that it's very important if we can get some input from domain experts such as biologists in the specific application of.",
            "What would be the practical symantics of penalizing variance?",
            "So we currently don't have that, but it would be interesting if we can get some more feedback from the domain expert in this application.",
            "Yeah, I guess one simple and I have to think a little more about this, but one simple.",
            "Explanation in favor of this approach will be that so, for example, in microarray datasets the inherent noise due to UN repeatability might be one factor on why you want to.",
            "Control the variance in the first place.",
            "But I mean, yeah, that that is something that has to be thought about."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will present in my talk on a novel stability based feature selection framework for K means clustering.",
                    "label": 0
                },
                {
                    "sent": "I should initially specify that we use the term stability in a slightly different way than the first talk, and we're mostly interested in the stability of the output of clustering rather than the stability of the feature selection itself.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will start this talk by presenting the main novelty and intuitions of the proposed framework, and then I will present the preliminary notions that are needed to get the main messages of the talk and this related to K means and PCA.",
                    "label": 0
                },
                {
                    "sent": "The stability of PCA, and the relationship between feature selection and sparse PCA.",
                    "label": 1
                },
                {
                    "sent": "And then I will move on to present the proposed framework, then pericle results and the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our work so you're probably familiar that there you probably know that there are various conceptually different approaches to feature selection, and its approach really defines what is a good feature selection in a slightly different way.",
                    "label": 0
                },
                {
                    "sent": "Most of the approaches are based on some notion of relevant features, and then the algorithm really aims to select the set of most relevant features that can describe the task.",
                    "label": 0
                },
                {
                    "sent": "In the context of this work, we take a slightly different perspective and we adopt A bias variance view over the feature selection problem.",
                    "label": 1
                },
                {
                    "sent": "This is not entirely new, but it has not been widely studied in the machine learning literature.",
                    "label": 0
                },
                {
                    "sent": "And so our main intuition is that we want to select the features that contribute to cluster separation and have low contribution to the variance of the clustering model.",
                    "label": 0
                },
                {
                    "sent": "And we achieve this through a stability maximizing sparse PCA algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will start the preliminary notions that are needed by discussing the relationship of K means clustering and principal components analysis.",
                    "label": 0
                },
                {
                    "sent": "So I guess everyone is familiar with.",
                    "label": 0
                },
                {
                    "sent": "K means clustering problem, which essentially tries to identify the clustering structure that minimizes the distances of the objects that belong in the it's cluster to the cluster centers.",
                    "label": 0
                },
                {
                    "sent": "The most popular heuristic is Lloyd's algorithm that any EM style algorithm that iteratively updates the cluster standards and up and assigns the objects to the closest.",
                    "label": 1
                },
                {
                    "sent": "Cluster centers and an alternative is to use principle components analysis to do what is called the continuous approximation to the discrete clustering problem.",
                    "label": 0
                },
                {
                    "sent": "So if we start with clustering, we know that this is a discrete.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem because the goal is to find this discrete vector that assigns our instances to cluster.",
                    "label": 0
                },
                {
                    "sent": "If we if we perform the continuous relaxation and allow this assignment vector to contain continuous values, then it could be shown that this convenience values can be derived by the eigenvectors of the covariance matrix, so which is essentially PCA and the details of the connection between PCA and K means are nicely illustrated.",
                    "label": 0
                },
                {
                    "sent": "An ICM L 2004.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "So this slide is to understand the relationship between feature selection for gaming and sparse PCA.",
                    "label": 1
                },
                {
                    "sent": "So we have seen that PCA can be used in order to provide continuous solution to the K means clustering problem.",
                    "label": 0
                },
                {
                    "sent": "And in general, if we want to consider a baseline feature selection for K means it would be a problem of selecting the subset of features that approximates the Caymans objective in the continuous approximation case, a baseline feature selection for PC for principle components analysis would be really to select the subset of features that approximates the K means continuous clustering objective.",
                    "label": 1
                },
                {
                    "sent": "But in the PCA case, the objective function is simply the eigenvalues of the covariance matrix and the features are simply the rows and columns of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So feature selection in this case really reduces to selecting the rows and columns of the covariance matrix that approximate the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Of the covariance matrix, which is essentially a sparse PCA problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have said in the start of this talk that we want to select the fixtures such that the stability of the clustering output is maximized.",
                    "label": 0
                },
                {
                    "sent": "So what we really want to do is to maximize the stability of the continuous solution that is derived by principal components analysis and we can do this by maximizing the difference between certain eigenvalues of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And in order to achieve this, we should in order to maximize the stability of the K -- 1 dominant Egan vectors of the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "What we should do is maximize the difference in the K management to the K eigenvalues of the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "And of course, now that we have seen how we can maximize stability, a good question is if we select the features that maximize the stability of the eigenvectors, what would be the semantics of doing this of solving this optimization?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "And so this is a slide with too many formulas.",
                    "label": 0
                },
                {
                    "sent": "But let me go slowly from them one by one.",
                    "label": 0
                },
                {
                    "sent": "So this is the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Along with feature selection.",
                    "label": 0
                },
                {
                    "sent": "So you here is the vector that contains this 01 values and essentially performs the feature selection process.",
                    "label": 0
                },
                {
                    "sent": "And here are the.",
                    "label": 0
                },
                {
                    "sent": "Center data presentation.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard covariance matrix and this.",
                    "label": 0
                },
                {
                    "sent": "Mother says really ensure that we perform.",
                    "label": 0
                },
                {
                    "sent": "We select rows and columns for this covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And we want to select.",
                    "label": 0
                },
                {
                    "sent": "The rows and columns of the covariance matrix such that stability is maximized, so we won't select rows and columns of the covariance matrix such that this Eggen value difference that control stability is maximized.",
                    "label": 0
                },
                {
                    "sent": "So if I wanted to do this directly, I should write the objective in the form, maximize with respect to the selected features which are contained in this vector such that Lambda 1 minus Lambda two is maximized.",
                    "label": 0
                },
                {
                    "sent": "But you can see that I'm not doing exactly this, I'm doing something slightly different in order to facilitate the formulation of the objective function, and I'm essentially maximizing the difference of the first eigenvalue to the rest.",
                    "label": 0
                },
                {
                    "sent": "And I'm taking the average, so this is slightly different, But since this is a maximization problem, we know that the objective function will get a better incentive of maximizing the difference between the two first eigenvalues that are closer to each other.",
                    "label": 0
                },
                {
                    "sent": "So if I start by this objective function that's related to maximizing the stability of the output.",
                    "label": 0
                },
                {
                    "sent": "We can easily derive this.",
                    "label": 0
                },
                {
                    "sent": "In this form, which is a maximization with respect to the selected features of the first leg and value of the covariance matrix minus the trace.",
                    "label": 0
                },
                {
                    "sent": "And if you're familiar with the sparse PC problems then you can easily recognize that if I.",
                    "label": 0
                },
                {
                    "sent": "Omit this term then this is simply a standard sparse PCA problem.",
                    "label": 0
                },
                {
                    "sent": "And what is interesting about interpreting what is achieved if someone maximizes stability?",
                    "label": 0
                },
                {
                    "sent": "Be shown that this is equivalent to selecting features that on one hand maximize an index that measures that this discrimination between the two clusters and on the other hand selects the features that have low variance.",
                    "label": 0
                },
                {
                    "sent": "So it can be shown.",
                    "label": 0
                },
                {
                    "sent": "And it's not very difficult to show that if one starts by this feature selection problem.",
                    "label": 0
                },
                {
                    "sent": "But selects the features that minimize this cluster separation minus the variance, and applies the standard continuous relaxation.",
                    "label": 0
                },
                {
                    "sent": "To the cluster assignments.",
                    "label": 0
                },
                {
                    "sent": "Then he simply.",
                    "label": 0
                },
                {
                    "sent": "Derives stability based sparse PCA problem.",
                    "label": 1
                },
                {
                    "sent": "And I can say that the main one of the main novelties of the work is that showing this nice interpretation of stability based sparse PCA as a feature selection mechanism that on one hand selects the features that maximize the discrimination between the clusters, and on the other hand minimizes the variance.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have a sparse PCA problem to select the features and we achieve this by performing greedy forward search that optimizes certain lower bound on the objective.",
                    "label": 1
                },
                {
                    "sent": "And this lower bound is shown here.",
                    "label": 1
                },
                {
                    "sent": "It seems a bit complicated, but the main.",
                    "label": 0
                },
                {
                    "sent": "Property of this bound is that it can be efficiently computed, so we can compute this bound by performing only one negative vector computation per greedy step.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We can see that this is the eigenvector that we need to compute, and this XFC are essentially the centered feature representations.",
                    "label": 0
                },
                {
                    "sent": "And this approach for optimizing sparse PCA is very similar to a nice ML2000 paper that did something similar.",
                    "label": 0
                },
                {
                    "sent": "So they also derive some lower bound on the sparse PCA objective, but this was not stability based, so the difference here is that we derive a similar bound, but for the stability maximizing case.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the algorithm is quite straightforward after we have derived this bound and we simply select the features that maximize this bound and we continue.",
                    "label": 0
                },
                {
                    "sent": "Alternatively until we have reached the desired number of features.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You can observe that in the formulas that I have shown in the previous slide, and essentially trying to maximize the difference between the Lambda one and Lambda two eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So this is this can help me compute only one stay Blegen vector of principle components analysis.",
                    "label": 0
                },
                {
                    "sent": "In the case I want to compute multiple stable Egan vectors.",
                    "label": 0
                },
                {
                    "sent": "I have",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To use deflation and we also showing the paper how to do efficient deflation that is equivalent to sure compliment deflation which is achieved simply by using this formula and in the paper we show that this type of formula update is equivalent to compliment deflation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have empirically evaluated our work against for Cancer Research datasets and we have used three methods with standard sparse PCA method with select the features that maximize solely the maximum eigenvalue of the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "The stable sparse PCA method that maximizes the objective that I have shown you.",
                    "label": 0
                },
                {
                    "sent": "And also a low variance sparse PCA method that gives higher weight to the higher penalty to the variance of the features.",
                    "label": 0
                },
                {
                    "sent": "And we have performed both quantitative evaluation becausw the datasets we have.",
                    "label": 0
                },
                {
                    "sent": "We know that the class labels, so we can use measures such as normalized mutual information.",
                    "label": 0
                },
                {
                    "sent": "And we have also performed a qualitative evaluation becausw.",
                    "label": 0
                },
                {
                    "sent": "We also looked at the relevant biology literature and we saw whether the jeans that were selected by our algorithm where meaningful in a qualitative sense.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are the clustering the quantitative results.",
                    "label": 0
                },
                {
                    "sent": "So here in the he axis we have the number of features that were retained by the algorithm in the axis we have the normalized mutual information.",
                    "label": 0
                },
                {
                    "sent": "And we have compared the three sparse PCA methods against Laplacian score, which is a standard unsupervised feature selection algorithm and also with the recently proposed feature selection algorithm in KDD 2010 and also the baseline approach of simply selecting the features that have maximum variance and this maximum variance.",
                    "label": 0
                },
                {
                    "sent": "Is a very commonly used baseline for unsupervised clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "And we can see that.",
                    "label": 0
                },
                {
                    "sent": "In all experiments.",
                    "label": 0
                },
                {
                    "sent": "One of the three approaches is usually better.",
                    "label": 0
                },
                {
                    "sent": "Or at least competitive.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two other feature selection approaches.",
                    "label": 0
                },
                {
                    "sent": "And we can see this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All four datasets that we have used.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have also performed the qualitative evaluation based mostly on the go loop data set, which is very well studied in the biology literature, and we have specifically looked for the features that we're uniquely selected by our algorithm and not by the others.",
                    "label": 1
                },
                {
                    "sent": "And we have found that argument was able to detect some interesting features, which in this case with word jeans that were important for the specific problem and also were missed by the competitive methods, and we consider that the results illustrate that using a cluster separation versus various tradeoff is something that.",
                    "label": 0
                },
                {
                    "sent": "Give some interesting results and warrants further research and I stress this becauses they usually feature selection algorithms unsupervised.",
                    "label": 0
                },
                {
                    "sent": "One consider the high variance of a feature as a good characteristic cause it can.",
                    "label": 0
                },
                {
                    "sent": "Be used to describe the data nicely, but the argument here is that.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "This should be used as a tradeoff between cluster separation and variance.",
                    "label": 0
                },
                {
                    "sent": "And not rely on variance solely by itself.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a further work, we will consider alternative optimization approaches instead of solely forward greedy search.",
                    "label": 1
                },
                {
                    "sent": "We will also consider to extend our approach to kernel K means and spectral clustering, and one very interesting problem that we consider is also parameter tuning for the separation.",
                    "label": 1
                },
                {
                    "sent": "Cluster separation versus various tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "So in this case we are currently looking for how one can automatically find the amount of penalty that should be put to variance in order to formulate this Lambda ones minus trace objective.",
                    "label": 0
                },
                {
                    "sent": "And with this I conclude my talk and thank you very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "Do I will sign personal?",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "I think I missed what you said about the vacation and assured that patient.",
                    "label": 0
                },
                {
                    "sent": "Would you mind?",
                    "label": 0
                },
                {
                    "sent": "Yes if someone so if you if you go back and so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the basic way we do the greedy forward search.",
                    "label": 0
                },
                {
                    "sent": "So what it's greedy step we need to compute this bound and in order to compute this bound with we need 2 elements.",
                    "label": 0
                },
                {
                    "sent": "One is this V which is essentially the eigenvector that's computed based on the features we have.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "Feature representation.",
                    "label": 0
                },
                {
                    "sent": "So if someone applies to complement deflation directly, he will derive some new covariance matrix and then he would have to decompose it in order to derive the new feature representation in order to compute this bound.",
                    "label": 0
                },
                {
                    "sent": "So this would add an extra cost to the algorithm because we would have first to do.",
                    "label": 0
                },
                {
                    "sent": "The deflation, then the composition of the new covariance matrix is something like X * X transpose with it.",
                    "label": 0
                },
                {
                    "sent": "So let's see the composition and then use the new feature representation in order to compute the formula.",
                    "label": 0
                },
                {
                    "sent": "And in the case.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we avoid this burden by doing that deflation directly on the feature representations.",
                    "label": 0
                },
                {
                    "sent": "So by using this formula we get directly the new feature representation that we can plug in the bound that we're computing.",
                    "label": 0
                },
                {
                    "sent": "And so this gives us certain efficiency gains.",
                    "label": 0
                },
                {
                    "sent": "And this is the main idea of.",
                    "label": 0
                },
                {
                    "sent": "This deflation process.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yep, I have one question.",
                    "label": 0
                },
                {
                    "sent": "Very nice talk.",
                    "label": 0
                },
                {
                    "sent": "So did you run any because you had in all these datasets, or at least most of these from from your talk.",
                    "label": 0
                },
                {
                    "sent": "What I want to hear you had the labels.",
                    "label": 0
                },
                {
                    "sent": "Did you try any supervised feature selection methods and compare what genes were selected?",
                    "label": 0
                },
                {
                    "sent": "With this you know?",
                    "label": 0
                },
                {
                    "sent": "I mean, that would be really nice because if you could show that the gene selected were equivalent because the labeling process itself is very costly.",
                    "label": 0
                },
                {
                    "sent": "To get these kind of datasets like the goal of one, you it's not easy to get those and if this method ascentia Lee gives you the pig overlap of the features then that would be very useful and we have not done this experiment.",
                    "label": 0
                },
                {
                    "sent": "We have mostly focused on unsupervised feature selection and OK.",
                    "label": 0
                },
                {
                    "sent": "If I.",
                    "label": 0
                },
                {
                    "sent": "Try to answer the different question.",
                    "label": 0
                },
                {
                    "sent": "So what would be the best application for this type of?",
                    "label": 0
                },
                {
                    "sent": "Cluster separation versus various tradeoff.",
                    "label": 0
                },
                {
                    "sent": "I think that it's very important if we can get some input from domain experts such as biologists in the specific application of.",
                    "label": 0
                },
                {
                    "sent": "What would be the practical symantics of penalizing variance?",
                    "label": 0
                },
                {
                    "sent": "So we currently don't have that, but it would be interesting if we can get some more feedback from the domain expert in this application.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess one simple and I have to think a little more about this, but one simple.",
                    "label": 0
                },
                {
                    "sent": "Explanation in favor of this approach will be that so, for example, in microarray datasets the inherent noise due to UN repeatability might be one factor on why you want to.",
                    "label": 0
                },
                {
                    "sent": "Control the variance in the first place.",
                    "label": 0
                },
                {
                    "sent": "But I mean, yeah, that that is something that has to be thought about.",
                    "label": 1
                }
            ]
        }
    }
}