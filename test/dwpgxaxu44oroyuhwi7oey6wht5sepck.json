{
    "id": "dwpgxaxu44oroyuhwi7oey6wht5sepck",
    "title": "Supervised Learning of Graph Structure",
    "info": {
        "author": [
            "Luca Rossi, Ca' Foscari University of Venice"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/simbad2011_rossi_structure/",
    "segmentation": [
        [
            "Pipe I'm looking to say I'm a PhD student of Professor University and in the next 30 minutes we will be talking about supervised learning of graph truck."
        ],
        [
            "So.",
            "So first of all, why do we need graph based representations at all?",
            "We need graph based representation because basically they naturally stand from a number of problem that goes from chem informatics to proteomics computational biology in general is a data mining, computer vision and complex system.",
            "So let's say rough for a very good way to model alot of problems.",
            "And they have several advantages over future based representations.",
            "For example your name.",
            "If you we can graph can capture relation arrangement between the object primitives, whereas a feature based representation count.",
            "And then, of course, these ability to capture a relational arrangement can help us providing contextual information to designing great in party identification.",
            "And also graph invariant.",
            "Of course the transformation such as change of scale, change of viewpoint or for example rotation just to name a few."
        ],
        [
            "The problem with graph learning is that most of the standard pattern recognition techniques as you know, work on electorial space, so they're kind of Victorian input.",
            "But the problem with graphs is that we cannot easily embed them into vectorial space and there are two reasons for that.",
            "First, there is no natural ordering of nodes and edges of a graph.",
            "If you have a set of graph, you know if you want to establish an an ordering of the nodes we have first to establish correspondences between these graphs.",
            "Second, even if we are able to establish this correspondence is an map the graph into a vector, then due to due to the variability of the nodes and edges number, the graph won't have the same length.",
            "The vector won't have the same length.",
            "So summing up, quantities such as mean and covariance are not easily characterized, and then we not.",
            "We are not able to easily summarize our object using these quantities.",
            "Be fair, there have been some successful attempt at embedding graphs into Victoria spaces.",
            "For example, approaches based on the spectral decomposition of the graphs.",
            "But what these approaches are not able to do is to characterize her mouth.",
            "A variation of the set."
        ],
        [
            "And this is actually what we want to do.",
            "What we want to do is this.",
            "Given a set on undirected graphs, the training set we want to learn a generative model.",
            "That can describe the distribution of the data as well as the structural variation of the set.",
            "We make an important assumption which actually is not that much important.",
            "You will see that later, which is that we want to learn a mixture model which is a mixture or knife models, each of which is working under the assumption that the edge and node observation are independent of the others.",
            "Each name graph model is composed of two parts.",
            "The first part is a structural part which basically includes the number of nodes that the model can generate.",
            "A number of possible edges that we know that we can observe and the results are stochastic parts which includes the variability of the observed."
        ],
        [
            "So for example, we define binary random variable which governs the possibility to observe an order or not.",
            "Same for the edges.",
            "But of course for the edges we have to condition this probability on the presence of both the endpoints of the edge.",
            "An we may as well defined generative models for the attributes on our node or edges.",
            "This is a simple toy example.",
            "We have this complete graph with three nodes and three edges and each node and edge is labeled with the probability of observing or sampling that nor edge.",
            "An these to the right is the about the possible graphs that can be sampled or observed from this model, but I want you to notice that usually when we when we observe this data, we don't have.",
            "We don't know the correspondences between the originating model and the sample graphs, so the distribution that we actually observe is the one to the right.",
            "Because of course we cannot distinguish between the 2nd."
        ],
        [
            "So graph here.",
            "The model described so far as a few limitation.",
            "The most important is that as you may have noticed, the number of nodes that our sample graph can have is can be at most V the size of the graph of the generative model.",
            "So we can say that our model is descriptive rather than predictive.",
            "And it also means that it has to encode all these structure that we observe in the data, including the noise.",
            "And this is something that we don't want.",
            "So what we do is the following their model described so far is defined as the core model.",
            "But then we also want to be able to generate external nodes.",
            "Nodes outside this model.",
            "We do this with the following geometric distribution and while for the edges we define simply a binary random variable telling us if there is an edge connecting an external node or not.",
            "We do the same for the attribute of node and edges of external and external edges with define some generative models and so somehow these external nodes represent the noise that we don't want to model with the core model."
        ],
        [
            "OK, so now we may be wondering what is the probability given this model of observing a graph.",
            "Well, first of all I want you to notice."
        ],
        [
            "But that we said before once we lose track of the correspondences between the the December graphs and the generative model, we somehow get observe a different business."
        ],
        [
            "Distribution, so we have to say we can model this by saying that the random permutation has been applied to the node of the samples and then the observation probability of a graph actually depends not only on the generative model but also on the set of correspondences that we defined between the sample graph and the generative model.",
            "So once we've seen that the probability of observing a graph given a model and set the first London sees it basically given by this formula where we exploit exploit, the fact that we've used that independence assumption earlier."
        ],
        [
            "OK, so now the problem is how do we estimate in the correspondences?",
            "Typically we could do a graph matching.",
            "We could use a graph matching approach, which means that we select as the set of correspondence with Sigma.",
            "The set that maximizes the probability of observing the graph given the Model G and the set of correspondences.",
            "However, as these simple example example shows you, this inducer bias in the model estimation, a single correspondence estimation in general in this bias in the model estimation.",
            "In fact, since this is the distribution that we observe.",
            "The model that we know that we learn when, but by maximizing the probability and doing just one estimation of the correspondence, is is this one, which is not the original model."
        ],
        [
            "So what we can do is instead of maximizing the probability, thinking the expectation over all the possible correspondences.",
            "Unfortunately, averaging over all possible correspondences is not possible due to the Super exponential growth of the space.",
            "And So what we do is we resort to an important sampling approach as done by to say hello in 2008.",
            "And this means that we compute.",
            "Basically computer has converging estimate of this probability."
        ],
        [
            "OK, just briefly, how the correspondence center works, we start from an initial guess from initial from an initialization of the metric of correspondences that we call EM.",
            "And we iteratively simpler correspondence and then we condition the metrics.",
            "I'm to this sample.",
            "We do this several times until we get to complete the set of correspond."
        ],
        [
            "This is.",
            "OK, so once we are able to establish the correspondence to sample correspondence, we can in fact estimate the parameters of the node edge and attributes.",
            "And what we do is just taking a simple maximum likelihood estimation approach and what we can do.",
            "And here it is, where we actually use the independence assumption, we can maximize the likelihood or the log likelihood of the model of each node and edge separately because of the independence assumption.",
            "The model is transfer, of course, is critically dependent on the initial choice of them.",
            "So what we doing this work is very simple.",
            "We put the mih equal to the probability of the node model I of the modern or die to generate the attributes of the graph node H and what we do then is to refine this metric and by updating it with the role after the first round of sampling will follow shown in this slide."
        ],
        [
            "So summing up.",
            "We initialize the model and the metrics.",
            "For example, the Bernoulli and Parimeter, which describes the possibility of a sub in an old or an edge.",
            "We initialize the model and the metrics time, and then we repeat the learning process several fixed number of times.",
            "We sample a few correspondences.",
            "Feature set of correspondences and each time we sample a correspondence, we can add observation of node and edges to the model.",
            "So after this we can update the parameter using the maximum likelihood estimation as shown before.",
            "But in the beginning, if I, if you recall, I say that I wanted to learn a mixture of generative models, not a single generative model as I said, so far.",
            "So what we do is we start with an oversight mixture with a lot of components and a lot of nodes, and we iteratively learn the model and try to prune it in the best way.",
            "What does the best way mean in this case?",
            "While we followed the approach of Toast Island out of 2008 in adopting an ML minimum message length criterion, but this time we use it to guide our pruning technique.",
            "So briefly, if you don't know how your minimum message like."
        ],
        [
            "It works.",
            "We can say the simplicity is formalized as the joint cost of the describing the model for the data and describing the data given the model.",
            "So this actually translates into a two part message, shown here, where D is the number of parameters of the model and thus the Carbonite of S is actually the size of the sample set.",
            "So what we do is, as I said, is greedily choosing the action.",
            "The pruning action.",
            "Actually, we can be removing the mixture component or reducing the size of the graph.",
            "The printing action we took maximizes the reduction in message length.",
            "And in order what we do actually here is bit of a smart finger.",
            "In order to compute the election in messaging in message length in curd, when we remove a node with a wide sampling, the correspondence is we compute the matching probability, not only of the current model but also of the current model without a node.",
            "So we keep track of this information and in the end we use it when choosing the best pruning action."
        ],
        [
            "OK buddy and the attributes.",
            "I didn't say anything in particular because you might as well choose the generative model is sweet through the best.",
            "So what we did for modeling our attributes in this set of experiment experiment, I'm going to show you is for the node.",
            "We used electrified Goshen model as done by Towson Hancock in their work on three unions.",
            "Quite simply, we have single stochastic node observation Model X.",
            "We sample from this and if the sample is greater than zero then the node is observed and with weight.",
            "Equal to the sample, if the.",
            "On the other hand, on the other case, the node is just not observed at all, so the probability of observing and is equal to 1 minus the complementary error function as shown in slide on the other end.",
            "For the edges, we decided to use a combination of two independent models of Bernoulli model and negoshian model.",
            "The Bernoulli variable tell us whether if the node is present or not and if the node is present, then independently independently we pick.",
            "Value from my Goshen model that I lost the weight of the edge."
        ],
        [
            "So for the experiments, what we did was using a classical cross validation approach.",
            "To evaluate the precision and recall of the of our algorithm and it was compared to classical baseline similarity based approaches such as nearest neighbor and nearest prototype.",
            "OK, this is the first set of examples here we have.",
            "10 classes of 15 shapes each, so it's a 2D shape recognition task.",
            "And what we did was extracting the shock graphs of these shapes.",
            "For those of you who don't know what the show graph is, simply you given a shape, you can extract the skeleton of a shape.",
            "The skeleton.",
            "You can think of it as.",
            "Imagine you want to evolve the boundary of the shape in Word.",
            "The locus of the point where the boundary collapses is the skeleton simply and given the skeleton which is made by a number of skeleton branches segment, you can construct a graph where each node correspond to a skeleton bridge.",
            "And these graphs are node attributed in the sense that you know reflects somehow the proportion of the boundary that has created its character branch."
        ],
        [
            "So this is the distance metrics of the added cost of the graphs, and it's multidimensional scaling that you can't see very well probably.",
            "But anyhow, I can tell you that there is a lot of class overlap between different classes and a lot of variability in the same class.",
            "So this task is supposed to be quite hard for nearest neighbor and get US prototype which are similarity based approach."
        ],
        [
            "In fact, as shown by the disruption precision at this part, time patches on a precision recall, our approach actually performs independently of the training set size, 15% at least 15% better than both nearest neighbor and nearest prototype.",
            "OK, so this means that we're actually able to capture these structural variation.",
            "Which nearest neighbor an nearest prototype."
        ],
        [
            "And.",
            "As a second set of examples of example, we wanted to test the approach on a 3D shape recognition task, which is very similar to the other problem.",
            "Actually, this time we compute the three dimensional skeleton, which is also called medial surface.",
            "Once again, we have an attributed graph where the weights are on the note."
        ],
        [
            "But this time discuss the task is quite easy as you can see the distance metrics and these multidimensional scaling.",
            "So there is absolutely no overlap between different classes, so we expect actually nearest neighboring nearest prototype to perform both quite fine."
        ],
        [
            "And that's indeed the case, because they almost always perform independently of the training set size they obtain, achieve 100% precision and recall.",
            "Whereas our model I don't know if you can see it properly, but performs very good.",
            "OK, because it achieves more than 95% precision error code.",
            "But there is a small gap between the nearest neighbor and his prototype.",
            "Another approach, and these we think is probably due to the very naive way of estimating the initial correspondences of the metrics.",
            "Of course, if you can, if you use a better model, you get a better result in the."
        ],
        [
            "OK. Then we wanted to learn also generative models where the waves were on the edges and then we.",
            "We took a subset of the coin 20 data set.",
            "For each image as you can see I extracted the most salient corner points using a counterpoint detector and then it constructively deloney graph of these.",
            "With this this point.",
            "As you can see, probably I didn't tune the parameters so well.",
            "Or maybe the corner detector wasn't working so well at home, but that was fine with us because we wanted the problem to be hard, so there is much variability.",
            "The graph and not very consistent.",
            "There is a lot of noise and we like."
        ],
        [
            "In fact, the distance metric, the edit distance metrics and these multidimensional scaling show it's a completely mess because it's almost impossible to, or at least it's very difficult to distinguish between example of the same class or other classes, you just the nearest neighbor or nearest prototype approach, or at least we guess that given the this block."
        ],
        [
            "And that's indeed the case.",
            "As you can see, both the nearest neighbor and nearest prototype struggle to reach a mere 40% precision and recall.",
            "So while our approach actually reaches about the 70% precision and recall, so as you can see, our model is able to learn this part of the noise of the data, which is very noisy, is able to learn genetic model that can describe the distribution and the structural variation of the set quite quite well."
        ],
        [
            "OK, as a last set of examples of experiments, we wanted to generate some synthetic data, and in order to do so, I picked 6 generative models.",
            "I define six generative models and I sampled 15 graphs from each of these models.",
            "So I did that in order to get space where the site where the variation was very high.",
            "I wanted to be a tough problem and once again these graphs, if I remember correctly, they are attributed only on nodes.",
            "So this is the added cost distance metrics and it's multidimensional scaling."
        ],
        [
            "And once again we perform very well an what's important better than nearest neighbor and nearest prototype.",
            "We achieve almost perfect precision and recall when the training set size grows.",
            "While both precision and recall an recalled for a nearest neighbor and nearest prototype cannot go above the 80% limit."
        ],
        [
            "So concluding, we have addressed the problem of graph learning for from graph samples, we were able to learn a model based on an F know depends on the independence node and edges independence assumption.",
            "But putting these models into mixture, we are able to come out to extract some correlation between the nodes.",
            "We use the fast fast sampling approach to overcome the bias of a single estimation of the correspondences, and we use maximum likelihood estimation to estimate the the northern edge parameters and the minimum message length criterion for guiding the pruning.",
            "The experiment that we perform well on a wide range of real world Object recognition task to the shape recognition 3D shape recognition object recognition with the call 20 datasets and synthetic data as well.",
            "And that showed that our approach outperform outperforms both nearest neighbor nearest prototype quite clearly.",
            "And and that's important regardless of the matching algorithm and the distance metric adopted, which were the state of the art."
        ],
        [
            "So that's concludes my talk.",
            "And if you have any question, you're welcome.",
            "Help.",
            "On the Slide 12 golf line.",
            "Oh yeah, OK, which is active.",
            "Can you tell us something about the convergence of this?",
            "Yeah, that.",
            "Art brush OK you mean the number of iterations that we need to converge?",
            "Well, that was something I did in my master thesis thesis.",
            "I did not really test the worthy convergence parameters, but actually it's I just plotted the precision recall for different number of iterations and correspondence is assembled.",
            "And actually the model the our approach is able to estimate a good model even with a very low set number of iteration, and correspondence is of course the more you sample, and I iterate the planning process, the more the better.",
            "It is your your model, especially the most important parameter to set this.",
            "Probably the number of correspondences sample because as I shown earlier.",
            "The If you don't estimate if you estimate just one correspondence, you get the bias, but if you don't estimate enough or.",
            "Yes, then the does not want to be that good.",
            "But anyway, even if we develop with a very low set, a number of correspondences.",
            "I say 20 for example sample or a very low number of iterations.",
            "You can get very good precision and recall so.",
            "I hope that answers your question.",
            "Outside the question, so about this example.",
            "You have a general and also you should be able to generate profits and possibly also restricted images from.",
            "So my question would be also how do they look?",
            "We don't do that, but it's quite simple because actually of course it can generate new data as I did for generating this synthetic sample data set.",
            "For example, but for this task in particular, that would be quite difficult because you know, I think you lose a lot of information from the from the skeleton to the shape.",
            "You can still keep some information even if the quantization errors introduced passing by the continuum from the continuum to the.",
            "So the discrete space introduce a lot of your computation errors, but I think you can.",
            "You can still keep the information when you go to the skeletal representation, but then when you go from the skeletal representation to this chart graph, then somehow a node represents a single skeletal branch.",
            "So you should somehow encoding that note.",
            "I don't know I vector or something describing that particular segment or a prototype of that segment, so it is possible, maybe, but it is hard and in.",
            "3D shape recognition.",
            "More because then you each node represent the South face.",
            "So somehow in that node you have to model with waves the describe the surface.",
            "It is possible, but it is very difficult learning.",
            "We didn't try that.",
            "Questions.",
            "The problem this time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pipe I'm looking to say I'm a PhD student of Professor University and in the next 30 minutes we will be talking about supervised learning of graph truck.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So first of all, why do we need graph based representations at all?",
                    "label": 0
                },
                {
                    "sent": "We need graph based representation because basically they naturally stand from a number of problem that goes from chem informatics to proteomics computational biology in general is a data mining, computer vision and complex system.",
                    "label": 1
                },
                {
                    "sent": "So let's say rough for a very good way to model alot of problems.",
                    "label": 1
                },
                {
                    "sent": "And they have several advantages over future based representations.",
                    "label": 0
                },
                {
                    "sent": "For example your name.",
                    "label": 0
                },
                {
                    "sent": "If you we can graph can capture relation arrangement between the object primitives, whereas a feature based representation count.",
                    "label": 0
                },
                {
                    "sent": "And then, of course, these ability to capture a relational arrangement can help us providing contextual information to designing great in party identification.",
                    "label": 1
                },
                {
                    "sent": "And also graph invariant.",
                    "label": 0
                },
                {
                    "sent": "Of course the transformation such as change of scale, change of viewpoint or for example rotation just to name a few.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem with graph learning is that most of the standard pattern recognition techniques as you know, work on electorial space, so they're kind of Victorian input.",
                    "label": 0
                },
                {
                    "sent": "But the problem with graphs is that we cannot easily embed them into vectorial space and there are two reasons for that.",
                    "label": 0
                },
                {
                    "sent": "First, there is no natural ordering of nodes and edges of a graph.",
                    "label": 1
                },
                {
                    "sent": "If you have a set of graph, you know if you want to establish an an ordering of the nodes we have first to establish correspondences between these graphs.",
                    "label": 1
                },
                {
                    "sent": "Second, even if we are able to establish this correspondence is an map the graph into a vector, then due to due to the variability of the nodes and edges number, the graph won't have the same length.",
                    "label": 1
                },
                {
                    "sent": "The vector won't have the same length.",
                    "label": 0
                },
                {
                    "sent": "So summing up, quantities such as mean and covariance are not easily characterized, and then we not.",
                    "label": 1
                },
                {
                    "sent": "We are not able to easily summarize our object using these quantities.",
                    "label": 1
                },
                {
                    "sent": "Be fair, there have been some successful attempt at embedding graphs into Victoria spaces.",
                    "label": 0
                },
                {
                    "sent": "For example, approaches based on the spectral decomposition of the graphs.",
                    "label": 0
                },
                {
                    "sent": "But what these approaches are not able to do is to characterize her mouth.",
                    "label": 0
                },
                {
                    "sent": "A variation of the set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is actually what we want to do.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is this.",
                    "label": 0
                },
                {
                    "sent": "Given a set on undirected graphs, the training set we want to learn a generative model.",
                    "label": 1
                },
                {
                    "sent": "That can describe the distribution of the data as well as the structural variation of the set.",
                    "label": 1
                },
                {
                    "sent": "We make an important assumption which actually is not that much important.",
                    "label": 1
                },
                {
                    "sent": "You will see that later, which is that we want to learn a mixture model which is a mixture or knife models, each of which is working under the assumption that the edge and node observation are independent of the others.",
                    "label": 0
                },
                {
                    "sent": "Each name graph model is composed of two parts.",
                    "label": 1
                },
                {
                    "sent": "The first part is a structural part which basically includes the number of nodes that the model can generate.",
                    "label": 0
                },
                {
                    "sent": "A number of possible edges that we know that we can observe and the results are stochastic parts which includes the variability of the observed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, we define binary random variable which governs the possibility to observe an order or not.",
                    "label": 0
                },
                {
                    "sent": "Same for the edges.",
                    "label": 0
                },
                {
                    "sent": "But of course for the edges we have to condition this probability on the presence of both the endpoints of the edge.",
                    "label": 1
                },
                {
                    "sent": "An we may as well defined generative models for the attributes on our node or edges.",
                    "label": 1
                },
                {
                    "sent": "This is a simple toy example.",
                    "label": 0
                },
                {
                    "sent": "We have this complete graph with three nodes and three edges and each node and edge is labeled with the probability of observing or sampling that nor edge.",
                    "label": 0
                },
                {
                    "sent": "An these to the right is the about the possible graphs that can be sampled or observed from this model, but I want you to notice that usually when we when we observe this data, we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't know the correspondences between the originating model and the sample graphs, so the distribution that we actually observe is the one to the right.",
                    "label": 0
                },
                {
                    "sent": "Because of course we cannot distinguish between the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So graph here.",
                    "label": 0
                },
                {
                    "sent": "The model described so far as a few limitation.",
                    "label": 1
                },
                {
                    "sent": "The most important is that as you may have noticed, the number of nodes that our sample graph can have is can be at most V the size of the graph of the generative model.",
                    "label": 0
                },
                {
                    "sent": "So we can say that our model is descriptive rather than predictive.",
                    "label": 0
                },
                {
                    "sent": "And it also means that it has to encode all these structure that we observe in the data, including the noise.",
                    "label": 0
                },
                {
                    "sent": "And this is something that we don't want.",
                    "label": 0
                },
                {
                    "sent": "So what we do is the following their model described so far is defined as the core model.",
                    "label": 0
                },
                {
                    "sent": "But then we also want to be able to generate external nodes.",
                    "label": 0
                },
                {
                    "sent": "Nodes outside this model.",
                    "label": 0
                },
                {
                    "sent": "We do this with the following geometric distribution and while for the edges we define simply a binary random variable telling us if there is an edge connecting an external node or not.",
                    "label": 1
                },
                {
                    "sent": "We do the same for the attribute of node and edges of external and external edges with define some generative models and so somehow these external nodes represent the noise that we don't want to model with the core model.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we may be wondering what is the probability given this model of observing a graph.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all I want you to notice.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that we said before once we lose track of the correspondences between the the December graphs and the generative model, we somehow get observe a different business.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution, so we have to say we can model this by saying that the random permutation has been applied to the node of the samples and then the observation probability of a graph actually depends not only on the generative model but also on the set of correspondences that we defined between the sample graph and the generative model.",
                    "label": 0
                },
                {
                    "sent": "So once we've seen that the probability of observing a graph given a model and set the first London sees it basically given by this formula where we exploit exploit, the fact that we've used that independence assumption earlier.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the problem is how do we estimate in the correspondences?",
                    "label": 0
                },
                {
                    "sent": "Typically we could do a graph matching.",
                    "label": 0
                },
                {
                    "sent": "We could use a graph matching approach, which means that we select as the set of correspondence with Sigma.",
                    "label": 0
                },
                {
                    "sent": "The set that maximizes the probability of observing the graph given the Model G and the set of correspondences.",
                    "label": 1
                },
                {
                    "sent": "However, as these simple example example shows you, this inducer bias in the model estimation, a single correspondence estimation in general in this bias in the model estimation.",
                    "label": 1
                },
                {
                    "sent": "In fact, since this is the distribution that we observe.",
                    "label": 0
                },
                {
                    "sent": "The model that we know that we learn when, but by maximizing the probability and doing just one estimation of the correspondence, is is this one, which is not the original model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we can do is instead of maximizing the probability, thinking the expectation over all the possible correspondences.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, averaging over all possible correspondences is not possible due to the Super exponential growth of the space.",
                    "label": 1
                },
                {
                    "sent": "And So what we do is we resort to an important sampling approach as done by to say hello in 2008.",
                    "label": 0
                },
                {
                    "sent": "And this means that we compute.",
                    "label": 0
                },
                {
                    "sent": "Basically computer has converging estimate of this probability.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, just briefly, how the correspondence center works, we start from an initial guess from initial from an initialization of the metric of correspondences that we call EM.",
                    "label": 1
                },
                {
                    "sent": "And we iteratively simpler correspondence and then we condition the metrics.",
                    "label": 0
                },
                {
                    "sent": "I'm to this sample.",
                    "label": 0
                },
                {
                    "sent": "We do this several times until we get to complete the set of correspond.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we are able to establish the correspondence to sample correspondence, we can in fact estimate the parameters of the node edge and attributes.",
                    "label": 1
                },
                {
                    "sent": "And what we do is just taking a simple maximum likelihood estimation approach and what we can do.",
                    "label": 1
                },
                {
                    "sent": "And here it is, where we actually use the independence assumption, we can maximize the likelihood or the log likelihood of the model of each node and edge separately because of the independence assumption.",
                    "label": 0
                },
                {
                    "sent": "The model is transfer, of course, is critically dependent on the initial choice of them.",
                    "label": 0
                },
                {
                    "sent": "So what we doing this work is very simple.",
                    "label": 0
                },
                {
                    "sent": "We put the mih equal to the probability of the node model I of the modern or die to generate the attributes of the graph node H and what we do then is to refine this metric and by updating it with the role after the first round of sampling will follow shown in this slide.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So summing up.",
                    "label": 0
                },
                {
                    "sent": "We initialize the model and the metrics.",
                    "label": 1
                },
                {
                    "sent": "For example, the Bernoulli and Parimeter, which describes the possibility of a sub in an old or an edge.",
                    "label": 0
                },
                {
                    "sent": "We initialize the model and the metrics time, and then we repeat the learning process several fixed number of times.",
                    "label": 0
                },
                {
                    "sent": "We sample a few correspondences.",
                    "label": 1
                },
                {
                    "sent": "Feature set of correspondences and each time we sample a correspondence, we can add observation of node and edges to the model.",
                    "label": 1
                },
                {
                    "sent": "So after this we can update the parameter using the maximum likelihood estimation as shown before.",
                    "label": 0
                },
                {
                    "sent": "But in the beginning, if I, if you recall, I say that I wanted to learn a mixture of generative models, not a single generative model as I said, so far.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we start with an oversight mixture with a lot of components and a lot of nodes, and we iteratively learn the model and try to prune it in the best way.",
                    "label": 1
                },
                {
                    "sent": "What does the best way mean in this case?",
                    "label": 0
                },
                {
                    "sent": "While we followed the approach of Toast Island out of 2008 in adopting an ML minimum message length criterion, but this time we use it to guide our pruning technique.",
                    "label": 0
                },
                {
                    "sent": "So briefly, if you don't know how your minimum message like.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works.",
                    "label": 0
                },
                {
                    "sent": "We can say the simplicity is formalized as the joint cost of the describing the model for the data and describing the data given the model.",
                    "label": 1
                },
                {
                    "sent": "So this actually translates into a two part message, shown here, where D is the number of parameters of the model and thus the Carbonite of S is actually the size of the sample set.",
                    "label": 0
                },
                {
                    "sent": "So what we do is, as I said, is greedily choosing the action.",
                    "label": 1
                },
                {
                    "sent": "The pruning action.",
                    "label": 1
                },
                {
                    "sent": "Actually, we can be removing the mixture component or reducing the size of the graph.",
                    "label": 1
                },
                {
                    "sent": "The printing action we took maximizes the reduction in message length.",
                    "label": 0
                },
                {
                    "sent": "And in order what we do actually here is bit of a smart finger.",
                    "label": 0
                },
                {
                    "sent": "In order to compute the election in messaging in message length in curd, when we remove a node with a wide sampling, the correspondence is we compute the matching probability, not only of the current model but also of the current model without a node.",
                    "label": 1
                },
                {
                    "sent": "So we keep track of this information and in the end we use it when choosing the best pruning action.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK buddy and the attributes.",
                    "label": 1
                },
                {
                    "sent": "I didn't say anything in particular because you might as well choose the generative model is sweet through the best.",
                    "label": 0
                },
                {
                    "sent": "So what we did for modeling our attributes in this set of experiment experiment, I'm going to show you is for the node.",
                    "label": 1
                },
                {
                    "sent": "We used electrified Goshen model as done by Towson Hancock in their work on three unions.",
                    "label": 0
                },
                {
                    "sent": "Quite simply, we have single stochastic node observation Model X.",
                    "label": 1
                },
                {
                    "sent": "We sample from this and if the sample is greater than zero then the node is observed and with weight.",
                    "label": 1
                },
                {
                    "sent": "Equal to the sample, if the.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, on the other case, the node is just not observed at all, so the probability of observing and is equal to 1 minus the complementary error function as shown in slide on the other end.",
                    "label": 1
                },
                {
                    "sent": "For the edges, we decided to use a combination of two independent models of Bernoulli model and negoshian model.",
                    "label": 0
                },
                {
                    "sent": "The Bernoulli variable tell us whether if the node is present or not and if the node is present, then independently independently we pick.",
                    "label": 0
                },
                {
                    "sent": "Value from my Goshen model that I lost the weight of the edge.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the experiments, what we did was using a classical cross validation approach.",
                    "label": 0
                },
                {
                    "sent": "To evaluate the precision and recall of the of our algorithm and it was compared to classical baseline similarity based approaches such as nearest neighbor and nearest prototype.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the first set of examples here we have.",
                    "label": 0
                },
                {
                    "sent": "10 classes of 15 shapes each, so it's a 2D shape recognition task.",
                    "label": 0
                },
                {
                    "sent": "And what we did was extracting the shock graphs of these shapes.",
                    "label": 1
                },
                {
                    "sent": "For those of you who don't know what the show graph is, simply you given a shape, you can extract the skeleton of a shape.",
                    "label": 0
                },
                {
                    "sent": "The skeleton.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as.",
                    "label": 0
                },
                {
                    "sent": "Imagine you want to evolve the boundary of the shape in Word.",
                    "label": 0
                },
                {
                    "sent": "The locus of the point where the boundary collapses is the skeleton simply and given the skeleton which is made by a number of skeleton branches segment, you can construct a graph where each node correspond to a skeleton bridge.",
                    "label": 0
                },
                {
                    "sent": "And these graphs are node attributed in the sense that you know reflects somehow the proportion of the boundary that has created its character branch.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the distance metrics of the added cost of the graphs, and it's multidimensional scaling that you can't see very well probably.",
                    "label": 0
                },
                {
                    "sent": "But anyhow, I can tell you that there is a lot of class overlap between different classes and a lot of variability in the same class.",
                    "label": 0
                },
                {
                    "sent": "So this task is supposed to be quite hard for nearest neighbor and get US prototype which are similarity based approach.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, as shown by the disruption precision at this part, time patches on a precision recall, our approach actually performs independently of the training set size, 15% at least 15% better than both nearest neighbor and nearest prototype.",
                    "label": 0
                },
                {
                    "sent": "OK, so this means that we're actually able to capture these structural variation.",
                    "label": 0
                },
                {
                    "sent": "Which nearest neighbor an nearest prototype.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As a second set of examples of example, we wanted to test the approach on a 3D shape recognition task, which is very similar to the other problem.",
                    "label": 0
                },
                {
                    "sent": "Actually, this time we compute the three dimensional skeleton, which is also called medial surface.",
                    "label": 0
                },
                {
                    "sent": "Once again, we have an attributed graph where the weights are on the note.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this time discuss the task is quite easy as you can see the distance metrics and these multidimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "So there is absolutely no overlap between different classes, so we expect actually nearest neighboring nearest prototype to perform both quite fine.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's indeed the case, because they almost always perform independently of the training set size they obtain, achieve 100% precision and recall.",
                    "label": 1
                },
                {
                    "sent": "Whereas our model I don't know if you can see it properly, but performs very good.",
                    "label": 0
                },
                {
                    "sent": "OK, because it achieves more than 95% precision error code.",
                    "label": 0
                },
                {
                    "sent": "But there is a small gap between the nearest neighbor and his prototype.",
                    "label": 0
                },
                {
                    "sent": "Another approach, and these we think is probably due to the very naive way of estimating the initial correspondences of the metrics.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you can, if you use a better model, you get a better result in the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Then we wanted to learn also generative models where the waves were on the edges and then we.",
                    "label": 0
                },
                {
                    "sent": "We took a subset of the coin 20 data set.",
                    "label": 0
                },
                {
                    "sent": "For each image as you can see I extracted the most salient corner points using a counterpoint detector and then it constructively deloney graph of these.",
                    "label": 0
                },
                {
                    "sent": "With this this point.",
                    "label": 0
                },
                {
                    "sent": "As you can see, probably I didn't tune the parameters so well.",
                    "label": 0
                },
                {
                    "sent": "Or maybe the corner detector wasn't working so well at home, but that was fine with us because we wanted the problem to be hard, so there is much variability.",
                    "label": 0
                },
                {
                    "sent": "The graph and not very consistent.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of noise and we like.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, the distance metric, the edit distance metrics and these multidimensional scaling show it's a completely mess because it's almost impossible to, or at least it's very difficult to distinguish between example of the same class or other classes, you just the nearest neighbor or nearest prototype approach, or at least we guess that given the this block.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's indeed the case.",
                    "label": 0
                },
                {
                    "sent": "As you can see, both the nearest neighbor and nearest prototype struggle to reach a mere 40% precision and recall.",
                    "label": 1
                },
                {
                    "sent": "So while our approach actually reaches about the 70% precision and recall, so as you can see, our model is able to learn this part of the noise of the data, which is very noisy, is able to learn genetic model that can describe the distribution and the structural variation of the set quite quite well.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, as a last set of examples of experiments, we wanted to generate some synthetic data, and in order to do so, I picked 6 generative models.",
                    "label": 1
                },
                {
                    "sent": "I define six generative models and I sampled 15 graphs from each of these models.",
                    "label": 0
                },
                {
                    "sent": "So I did that in order to get space where the site where the variation was very high.",
                    "label": 0
                },
                {
                    "sent": "I wanted to be a tough problem and once again these graphs, if I remember correctly, they are attributed only on nodes.",
                    "label": 0
                },
                {
                    "sent": "So this is the added cost distance metrics and it's multidimensional scaling.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And once again we perform very well an what's important better than nearest neighbor and nearest prototype.",
                    "label": 0
                },
                {
                    "sent": "We achieve almost perfect precision and recall when the training set size grows.",
                    "label": 1
                },
                {
                    "sent": "While both precision and recall an recalled for a nearest neighbor and nearest prototype cannot go above the 80% limit.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So concluding, we have addressed the problem of graph learning for from graph samples, we were able to learn a model based on an F know depends on the independence node and edges independence assumption.",
                    "label": 1
                },
                {
                    "sent": "But putting these models into mixture, we are able to come out to extract some correlation between the nodes.",
                    "label": 1
                },
                {
                    "sent": "We use the fast fast sampling approach to overcome the bias of a single estimation of the correspondences, and we use maximum likelihood estimation to estimate the the northern edge parameters and the minimum message length criterion for guiding the pruning.",
                    "label": 1
                },
                {
                    "sent": "The experiment that we perform well on a wide range of real world Object recognition task to the shape recognition 3D shape recognition object recognition with the call 20 datasets and synthetic data as well.",
                    "label": 1
                },
                {
                    "sent": "And that showed that our approach outperform outperforms both nearest neighbor nearest prototype quite clearly.",
                    "label": 0
                },
                {
                    "sent": "And and that's important regardless of the matching algorithm and the distance metric adopted, which were the state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "And if you have any question, you're welcome.",
                    "label": 0
                },
                {
                    "sent": "Help.",
                    "label": 0
                },
                {
                    "sent": "On the Slide 12 golf line.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, OK, which is active.",
                    "label": 0
                },
                {
                    "sent": "Can you tell us something about the convergence of this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that.",
                    "label": 0
                },
                {
                    "sent": "Art brush OK you mean the number of iterations that we need to converge?",
                    "label": 0
                },
                {
                    "sent": "Well, that was something I did in my master thesis thesis.",
                    "label": 0
                },
                {
                    "sent": "I did not really test the worthy convergence parameters, but actually it's I just plotted the precision recall for different number of iterations and correspondence is assembled.",
                    "label": 0
                },
                {
                    "sent": "And actually the model the our approach is able to estimate a good model even with a very low set number of iteration, and correspondence is of course the more you sample, and I iterate the planning process, the more the better.",
                    "label": 0
                },
                {
                    "sent": "It is your your model, especially the most important parameter to set this.",
                    "label": 0
                },
                {
                    "sent": "Probably the number of correspondences sample because as I shown earlier.",
                    "label": 0
                },
                {
                    "sent": "The If you don't estimate if you estimate just one correspondence, you get the bias, but if you don't estimate enough or.",
                    "label": 0
                },
                {
                    "sent": "Yes, then the does not want to be that good.",
                    "label": 0
                },
                {
                    "sent": "But anyway, even if we develop with a very low set, a number of correspondences.",
                    "label": 0
                },
                {
                    "sent": "I say 20 for example sample or a very low number of iterations.",
                    "label": 0
                },
                {
                    "sent": "You can get very good precision and recall so.",
                    "label": 0
                },
                {
                    "sent": "I hope that answers your question.",
                    "label": 0
                },
                {
                    "sent": "Outside the question, so about this example.",
                    "label": 0
                },
                {
                    "sent": "You have a general and also you should be able to generate profits and possibly also restricted images from.",
                    "label": 0
                },
                {
                    "sent": "So my question would be also how do they look?",
                    "label": 0
                },
                {
                    "sent": "We don't do that, but it's quite simple because actually of course it can generate new data as I did for generating this synthetic sample data set.",
                    "label": 0
                },
                {
                    "sent": "For example, but for this task in particular, that would be quite difficult because you know, I think you lose a lot of information from the from the skeleton to the shape.",
                    "label": 0
                },
                {
                    "sent": "You can still keep some information even if the quantization errors introduced passing by the continuum from the continuum to the.",
                    "label": 0
                },
                {
                    "sent": "So the discrete space introduce a lot of your computation errors, but I think you can.",
                    "label": 0
                },
                {
                    "sent": "You can still keep the information when you go to the skeletal representation, but then when you go from the skeletal representation to this chart graph, then somehow a node represents a single skeletal branch.",
                    "label": 0
                },
                {
                    "sent": "So you should somehow encoding that note.",
                    "label": 0
                },
                {
                    "sent": "I don't know I vector or something describing that particular segment or a prototype of that segment, so it is possible, maybe, but it is hard and in.",
                    "label": 0
                },
                {
                    "sent": "3D shape recognition.",
                    "label": 0
                },
                {
                    "sent": "More because then you each node represent the South face.",
                    "label": 0
                },
                {
                    "sent": "So somehow in that node you have to model with waves the describe the surface.",
                    "label": 0
                },
                {
                    "sent": "It is possible, but it is very difficult learning.",
                    "label": 0
                },
                {
                    "sent": "We didn't try that.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "The problem this time.",
                    "label": 0
                }
            ]
        }
    }
}