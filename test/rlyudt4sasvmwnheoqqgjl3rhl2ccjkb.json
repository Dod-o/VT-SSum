{
    "id": "rlyudt4sasvmwnheoqqgjl3rhl2ccjkb",
    "title": "Learning Using Local Membership Queries",
    "info": {
        "author": [
            "Pranjal Awasthi, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_awasthi_queries/",
    "segmentation": [
        [
            "So this is joint work with elements from IBM Ann Veronica today from UC Berkeley."
        ],
        [
            "And in this work we study the membership query model of learning which was introduced by angling an in this model as in the standard pack setting.",
            "The learning algorithm has access to a distribution Oracle using which it can generate random examples from the distribution which are labeled according to the function F that one is trying to learn.",
            "And in addition, one also has black box query access to the function F, so the learning algorithm can propose any X of its choice and get the label of that corresponding function F of X.",
            "And the goal is to output a hypothesis of low error.",
            "An in this work what we do is study a model in which instead of allowing the function to have arbitrary query axis, we restrict the function to only.",
            "I ask for certain specific kind of queries that we call us local queries an for the rest of the talk I'm going to describe the motivation behind studying our model an I'll show you that even with these local queries, one can still do a lot of interesting things."
        ],
        [
            "So let me first mention that the membership query model itself is a very powerful model and we have had a lot of interesting algorithmic ideas for a lot of problems that currently seem out of reach in the standard pack model.",
            "For example, we know how to efficiently learn decision trees using queries.",
            "And there's also been work on agnostic learning of decision trees under specific distributions.",
            "Then there is a celebrated result of Jackson who showed how to learn DNS using queries under product distributions.",
            "And there's also been work on learning intersection of half spaces.",
            "So a lot of nice theory exists in this area, but the membership query model itself is not very popular in practice."
        ],
        [
            "And one of the reasons for this is that the power that the algorithm gains by having arbitrary query access to the function also turns out to be a weakness of the model.",
            "Because in a real life scenario this black box query is going to go to some human labeler.",
            "An in most learning problems.",
            "Not every X that the algorithm generates is going to make sense.",
            "Or in other words, not every setting of the features that you can come up with will correspond to an object which has a semantic meaning associated with it.",
            "So if the algorithm queries for such examples, then one can imagine a lot of noise in the human responses, and the algorithm might fail.",
            "In"
        ],
        [
            "This was observed in a famous experiment by Lang and Baum, who tried to apply query algorithms for the task of handwritten digit recognition.",
            "And they observed that these algorithms perform poorly, because quite often they query for labels of images that look like random blurs.",
            "So this is the motivation behind studying our model."
        ],
        [
            "From the Oracle's point of view or from a human labeler's point of view, more reasonable algorithm or a more reasonable model would be which generates queries which are not too far from a typical point which is being generated by the distribution.",
            "For example, as human labelers, most of us would be OK in labeling a query point which looks something like this.",
            "You would perhaps be OK to label a query point which looks something like this, but it would be unreasonable for an algorithm to ask us to label an image which looks something like this."
        ],
        [
            "So now let me formally define the model that we study, which we call is the local membership query model.",
            "So the setting is the same as in the MQ model, but instead we restrict the algorithm to only ask local queries and formally we say that a query X is our local.",
            "If there exists a point in your training data such that the distance of X2 X prime is at most R, or in other words, you're not allowed to go very far from your training data when you are generating these membership queries.",
            "And depending on the problem domain one is in and depending on the feature representation one is using, one could think of an appropriate definition of a distance function.",
            "For the purposes of this talk, I'm going to assume that X is the Boolean hypercube and the distance between any two points is the Hamming distance.",
            "So the goal now is to."
        ],
        [
            "And what we can do with these local membership queries?",
            "Annizah warm up.",
            "Let me try to convince you that even one local membership queries are very powerful.",
            "So my claim is that by just using one local membership queries, you can learn noisy parities, so the noisy parity problem is that the data is generated according to distribution and it's labeled according to an unknown parity function, and then each label is flipped independently with some probability.",
            "And I claim that if you compute the following statistic, which is what is the probability that for a random X the label changes if I flip a particular bit.",
            "Then all the variables.",
            "It's not hard to see that all the variables which are present in the unknown parity function are going to have a higher value of this statistic as compared to all the other variables.",
            "It's also easy to see that this can be computed using local membership queries just by flipping a single bit from your training data.",
            "Using a similar strategy, we can also see that you can learn K juntas under uniform distribution using one local queries.",
            "So already just one local queries.",
            "There are a lot of non trivial things one can do.",
            "Let me now move on to the main results of our paper."
        ],
        [
            "So our first result is on learning sparse polynomials, so we give an efficient algorithm that learns sparse polynomials.",
            "And the algorithm is order log in local.",
            "So the kind of queries it generates are it flips at most up to log in bits from the training data.",
            "And it works under a broad class of distributions that we call as log Lipschitz distributions.",
            "An in a few slides I'm going to define what these mean.",
            "Notice that this class as a subclass contains the special case of log depth decision trees.",
            "Our second is."
        ],
        [
            "It is for learning polynomial size decision trees, so we give an algorithm which can learn polynomial size decision trees under constant bias product distributions under product distributions where the bias of each coordinate is a constant and again the algorithm is order log in local so it makes flips at most log in bits from the training data.",
            "At."
        ],
        [
            "Our 3rd result is on learning DNF formulas under uniform distribution.",
            "Again, we give an order log in local algorithm.",
            "Unfortunately, in this case we don't have a polynomial time algorithm yet, or algorithm runs in time and to the log log.",
            "So now I'm going to go into the details of some of our techniques.",
            "And I'm going to describe the."
        ],
        [
            "Some of the ideas behind our first result, which is on learning sparse polynomials."
        ],
        [
            "So let's look at the setting again.",
            "So we are in the Boolean hypercube and I'm going to assume that the distribution D is Alpha log Lipschitz for some constant Alpha.",
            "And what this means is that the log of the PDF is a Lipschitz function.",
            "Or in other words, if you look at any two points X&X prime which are neighbors, the probability mass over X and the probability mass over extreme is bounded by Alpha.",
            "So if you set Alpha equals one, it's easy to see that it captures uniform distribution.",
            "These distributions also capture constant bias product distributions, and they also capture smooth distributions which were studied for learning problems in the work of collateral.",
            "Notice that in a log lectures distribution we're not making any assumption about the independence of of the end coordinates.",
            "Voice isn't exactly one, or for anything voice with this is exposed.",
            "So in the Boolean hypercube setting, it's going to be exactly 1.",
            "It's cool, yeah."
        ],
        [
            "Someone rotation, so I'm going to let's represent the target polynomial that we're trying to learn as F of X, and I'm going to write it in the standard sum of product setting.",
            "So C sub of South here represents the coefficient for the monomial corresponding stress.",
            "And we're going to assume that these coefficients are bounded and the support of X is small, so support of F is polynomial in.",
            "And again, the goal is to output hypothesis H of LO L2 error."
        ],
        [
            "So the result is going to go into three steps.",
            "The first step is going to be a low degree approximation, where we're going to argue that under these log Lipschitz distributions these sparse polynomials can be approximated by the low degree components.",
            "The second step is going to be a procedure to identify a set of candidate monomials with the hope that it contains these low degree components of the target function.",
            "And the last step is going to be an L2 regression algorithm run over the candidate monomials that we find in Step 2.",
            "So the key point here is going to be to run Step 2 using local queries.",
            "So let me go describe the 1st."
        ],
        [
            "Step now, so the claim is that."
        ],
        [
            "If you look at the function F of X.",
            "And if you look at the function F of G of X, which represents the low degree components of F components, monomers of size at most D for D equals Logn over epsilon.",
            "Then XD is a good approximation to F. Or in other words, the higher degree monomials of F have very less contribution.",
            "And this follows from the claim that if you look at any monomial of size S under a log Lipschitz distribution, the probability that this model is one is going to be is going to depend exponentially in the size of this.",
            "For example, if you set Alpha equals one, you get 1 / 2 to the size office, which is what you would expect for the uniform distribution."
        ],
        [
            "Now I'm going to go into the second part, which is identifying the candidate low degree monomials.",
            "Anne."
        ],
        [
            "Let's assume for now that we are in the uniform distribution setting and some of you might be thinking a very natural thing to do here is to run the famous levels Mansour algorithm.",
            "So this is a very powerful algorithm which given query access to a sparse function F outputs all the heavy coefficients of the polynomial.",
            "By heavy I mean all the monomials whose coefficients are at least Theta.",
            "Ann, let me briefly describe how this algorithm works because it's."
        ],
        [
            "Going to be very relevant to our final algorithm, so let's as an example, let's take this function of three variables.",
            "You can view the Kushilevitz Mansour algorithm or the Cam algorithm is doing some sort of a depth first search.",
            "Exploration of this tree.",
            "So let's pick variables in some order an let's branch on them.",
            "So let's branch on X1 first, so we can decompose the function F into two parts F1 and F, sub minus one.",
            "X of 1 represents the part of the function that contains X one and F sub.",
            "Minus one represents a part of the function that does not contain X1.",
            "Then the algorithm tries to gauge the importance of these two terms by running a test on them, which I will call us the L2 test.",
            "And the test asks is expected value of F sub one squared large?",
            "And if the answer is yes, then these."
        ],
        [
            "Nodes become green nodes and then you recursively branch on other variables.",
            "So this procedure is continued until you reach the leaves of the tree.",
            "Which are going."
        ],
        [
            "Correspond to meals.",
            "And all the monomials at the leaf switch pass the test.",
            "Are going to be the important monomials that the algorithm is going to output, so this is a very savvy algorithm in the sense that it outputs all the important monomials of the function and it outputs only the important monomials of the functions.",
            "So it's not wasteful.",
            "Another nice property of this algorithm is that."
        ],
        [
            "At each step, the active nodes in the tree partition the polynomial.",
            "For example, at step one F1 and F sub minus one partition the polynomial into two disjoint sets.",
            "And this property leads to a very elegant procedure which bounds the width of this tree at each level.",
            "So it's a very nice algorithm.",
            "The problem is that it's not a local algorithm, and during the course of its execution it might actually generate queries by flipping up to order N bits from the given training data.",
            "So we want to make it local and natural approach or obvious idea is to do."
        ],
        [
            "First, search instead of depth first search, so that's what we're going to do.",
            "So again, let's start with this function F. And we're going to brunch on all the three variables at the same time.",
            "So I'm going to look at functions F1F2 and F3, which captured the part of the function that depends on X1X2 and X3 respectively.",
            "Notice that it's not true anymore that these nodes partition the function, so F1 and F2 might have mono meals which are in common to both of them.",
            "And again, we're going to run some tests to find out the importance of these nodes.",
            "And we're going to slightly different tests, which I'm going to call as the non zero test which asks is probability of F are not equal to 0 noticeably.",
            "And the reason is that we want our algorithm to work for log Lipschitz distributions, and this test turns out to be easier to analyze in that case.",
            "And again, let."
        ],
        [
            "Say F1 and F3 pass the test, so then we are going to recursively branch."
        ],
        [
            "On other variables to go one level down.",
            "And we're going to."
        ],
        [
            "Build this tree to a bounded depth, so we're going to go up to the depth log of an over epsilon, and then we're going to stop.",
            "So now the question is, how do we extract the monomials that this procedure is going to output?",
            "So the point here is that we're not going to run this tree up till completion and instead of.",
            "Doing a depth first search instead of choosing an order over variables and then.",
            "Going in that order, for example, if the Cam algorithm, if sub one passes the test, then you choose another variable.",
            "To branch on and then you continue, we're going to do this these things in parallel, so we're going to branch in all the variables simultaneously.",
            "So it makes the analysis a bit trickier.",
            "But as I'm going to show later, it ensures that we can implement these steps using local queries.",
            "OK, so.",
            "OK, so we're going to stop at depth.",
            "Log in over epsilon."
        ],
        [
            "As candidate monomials, we're going to output any set S IF service was artist.",
            "For example, in this case F1F3F3, two or service.",
            "So we're going to add monomials X one X3 and X3, two over set.",
            "So.",
            "Forget one other.",
            "It's useful algorithm.",
            "Let me at least try to convince you that it's a local algorithm, and in order to do that, I have to tell you how to implement the test.",
            "The non zero test."
        ],
        [
            "And I'm going to do that by using the example of single monomial, let's say X1.",
            "So again, I can write F as X 1 * F sub one and plus F sub minus one.",
            "And I want to figure out whether probability of F1 not equal to 0 is noticeable.",
            "And the claim is that this test can be implemented using local queries."
        ],
        [
            "And the way to do it is very simple.",
            "You just try to factor out X one.",
            "So I'm going to take take an input and substitute X one equals one and then substitute X 1 = 0 and notice that the value of F sub minus one does not change in these two settings.",
            "So if I just subtract out these two equations, I get the value of F1 of X given the value of F of X on these two inputs.",
            "So the expression is going to look something like this, so here we are summing up over values of X1.",
            "So in fact this test can be implemented using one local query."
        ],
        [
            "In general, if you have a set S, the expression for F service of X is going to look like this, so we are factoring out S variables.",
            "So it's going to use size of South local queries.",
            "Just that.",
            "It's the."
        ],
        [
            "Depth of the tree exactly.",
            "So since we are stopping the tree at depth Logn over epsilon, we're going to use log in local queries.",
            "So that's fine, but as a result what happens is we lose a lot of the nice properties of the Cam algorithm.",
            "For example, this algorithm is going to be wasteful.",
            "It's going to add potentially at a lot of monomials which are not really important.",
            "Also, if you look at as I mentioned before, the nodes in the tree might have overlaps in them, so it's you have to be a bit careful in arguing that we don't explore the search space in this case."
        ],
        [
            "So you have to trust me on that.",
            "That can be done and the final step is going to be an L2 regression over all the terms that we added.",
            "I am not going to go into the details of the analysis, but it relies on the properties of log Lipschitz distributions."
        ],
        [
            "And it works.",
            "So let."
        ],
        [
            "I just called the conclusions directly so we study this local membership query model an we show that you can do quite a few interesting things using local membership queries.",
            "Of the interesting there a lot of interesting open problems we would like to understand the limitations of such local membership queries.",
            "For example, as Nina asked, is Logn locality necessary for learning these classes?",
            "We don't know.",
            "Let me mention some."
        ],
        [
            "Pacific open problems.",
            "Under log Lipschitz distributions we gave algorithms to learn log depth decision trees.",
            "It would be nice to be able to learn polynomial size decision trees using local algorithms.",
            "As I mentioned before, our DNF learning algorithm is not polynomial time, so that will be nice too.",
            "And it will also be interesting to study local membership query algorithms for agnostic learning.",
            "In particular, if one could agnostically learn parities using local algorithms, that would be a way to resolve open problem 2.",
            "And we have some reason to believe that this might be hard, so we can.",
            "For example, we can show that if you want to do agnostic learning, then a constant local queries are not going to be enough.",
            "And that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with elements from IBM Ann Veronica today from UC Berkeley.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this work we study the membership query model of learning which was introduced by angling an in this model as in the standard pack setting.",
                    "label": 0
                },
                {
                    "sent": "The learning algorithm has access to a distribution Oracle using which it can generate random examples from the distribution which are labeled according to the function F that one is trying to learn.",
                    "label": 1
                },
                {
                    "sent": "And in addition, one also has black box query access to the function F, so the learning algorithm can propose any X of its choice and get the label of that corresponding function F of X.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to output a hypothesis of low error.",
                    "label": 0
                },
                {
                    "sent": "An in this work what we do is study a model in which instead of allowing the function to have arbitrary query axis, we restrict the function to only.",
                    "label": 0
                },
                {
                    "sent": "I ask for certain specific kind of queries that we call us local queries an for the rest of the talk I'm going to describe the motivation behind studying our model an I'll show you that even with these local queries, one can still do a lot of interesting things.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me first mention that the membership query model itself is a very powerful model and we have had a lot of interesting algorithmic ideas for a lot of problems that currently seem out of reach in the standard pack model.",
                    "label": 0
                },
                {
                    "sent": "For example, we know how to efficiently learn decision trees using queries.",
                    "label": 1
                },
                {
                    "sent": "And there's also been work on agnostic learning of decision trees under specific distributions.",
                    "label": 1
                },
                {
                    "sent": "Then there is a celebrated result of Jackson who showed how to learn DNS using queries under product distributions.",
                    "label": 0
                },
                {
                    "sent": "And there's also been work on learning intersection of half spaces.",
                    "label": 1
                },
                {
                    "sent": "So a lot of nice theory exists in this area, but the membership query model itself is not very popular in practice.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the reasons for this is that the power that the algorithm gains by having arbitrary query access to the function also turns out to be a weakness of the model.",
                    "label": 0
                },
                {
                    "sent": "Because in a real life scenario this black box query is going to go to some human labeler.",
                    "label": 0
                },
                {
                    "sent": "An in most learning problems.",
                    "label": 0
                },
                {
                    "sent": "Not every X that the algorithm generates is going to make sense.",
                    "label": 1
                },
                {
                    "sent": "Or in other words, not every setting of the features that you can come up with will correspond to an object which has a semantic meaning associated with it.",
                    "label": 0
                },
                {
                    "sent": "So if the algorithm queries for such examples, then one can imagine a lot of noise in the human responses, and the algorithm might fail.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was observed in a famous experiment by Lang and Baum, who tried to apply query algorithms for the task of handwritten digit recognition.",
                    "label": 0
                },
                {
                    "sent": "And they observed that these algorithms perform poorly, because quite often they query for labels of images that look like random blurs.",
                    "label": 0
                },
                {
                    "sent": "So this is the motivation behind studying our model.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the Oracle's point of view or from a human labeler's point of view, more reasonable algorithm or a more reasonable model would be which generates queries which are not too far from a typical point which is being generated by the distribution.",
                    "label": 0
                },
                {
                    "sent": "For example, as human labelers, most of us would be OK in labeling a query point which looks something like this.",
                    "label": 0
                },
                {
                    "sent": "You would perhaps be OK to label a query point which looks something like this, but it would be unreasonable for an algorithm to ask us to label an image which looks something like this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me formally define the model that we study, which we call is the local membership query model.",
                    "label": 0
                },
                {
                    "sent": "So the setting is the same as in the MQ model, but instead we restrict the algorithm to only ask local queries and formally we say that a query X is our local.",
                    "label": 1
                },
                {
                    "sent": "If there exists a point in your training data such that the distance of X2 X prime is at most R, or in other words, you're not allowed to go very far from your training data when you are generating these membership queries.",
                    "label": 0
                },
                {
                    "sent": "And depending on the problem domain one is in and depending on the feature representation one is using, one could think of an appropriate definition of a distance function.",
                    "label": 0
                },
                {
                    "sent": "For the purposes of this talk, I'm going to assume that X is the Boolean hypercube and the distance between any two points is the Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "So the goal now is to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we can do with these local membership queries?",
                    "label": 0
                },
                {
                    "sent": "Annizah warm up.",
                    "label": 0
                },
                {
                    "sent": "Let me try to convince you that even one local membership queries are very powerful.",
                    "label": 0
                },
                {
                    "sent": "So my claim is that by just using one local membership queries, you can learn noisy parities, so the noisy parity problem is that the data is generated according to distribution and it's labeled according to an unknown parity function, and then each label is flipped independently with some probability.",
                    "label": 1
                },
                {
                    "sent": "And I claim that if you compute the following statistic, which is what is the probability that for a random X the label changes if I flip a particular bit.",
                    "label": 0
                },
                {
                    "sent": "Then all the variables.",
                    "label": 0
                },
                {
                    "sent": "It's not hard to see that all the variables which are present in the unknown parity function are going to have a higher value of this statistic as compared to all the other variables.",
                    "label": 0
                },
                {
                    "sent": "It's also easy to see that this can be computed using local membership queries just by flipping a single bit from your training data.",
                    "label": 0
                },
                {
                    "sent": "Using a similar strategy, we can also see that you can learn K juntas under uniform distribution using one local queries.",
                    "label": 1
                },
                {
                    "sent": "So already just one local queries.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of non trivial things one can do.",
                    "label": 0
                },
                {
                    "sent": "Let me now move on to the main results of our paper.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our first result is on learning sparse polynomials, so we give an efficient algorithm that learns sparse polynomials.",
                    "label": 1
                },
                {
                    "sent": "And the algorithm is order log in local.",
                    "label": 0
                },
                {
                    "sent": "So the kind of queries it generates are it flips at most up to log in bits from the training data.",
                    "label": 0
                },
                {
                    "sent": "And it works under a broad class of distributions that we call as log Lipschitz distributions.",
                    "label": 0
                },
                {
                    "sent": "An in a few slides I'm going to define what these mean.",
                    "label": 1
                },
                {
                    "sent": "Notice that this class as a subclass contains the special case of log depth decision trees.",
                    "label": 0
                },
                {
                    "sent": "Our second is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is for learning polynomial size decision trees, so we give an algorithm which can learn polynomial size decision trees under constant bias product distributions under product distributions where the bias of each coordinate is a constant and again the algorithm is order log in local so it makes flips at most log in bits from the training data.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our 3rd result is on learning DNF formulas under uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "Again, we give an order log in local algorithm.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, in this case we don't have a polynomial time algorithm yet, or algorithm runs in time and to the log log.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to go into the details of some of our techniques.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to describe the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the ideas behind our first result, which is on learning sparse polynomials.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the setting again.",
                    "label": 0
                },
                {
                    "sent": "So we are in the Boolean hypercube and I'm going to assume that the distribution D is Alpha log Lipschitz for some constant Alpha.",
                    "label": 1
                },
                {
                    "sent": "And what this means is that the log of the PDF is a Lipschitz function.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, if you look at any two points X&X prime which are neighbors, the probability mass over X and the probability mass over extreme is bounded by Alpha.",
                    "label": 0
                },
                {
                    "sent": "So if you set Alpha equals one, it's easy to see that it captures uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "These distributions also capture constant bias product distributions, and they also capture smooth distributions which were studied for learning problems in the work of collateral.",
                    "label": 1
                },
                {
                    "sent": "Notice that in a log lectures distribution we're not making any assumption about the independence of of the end coordinates.",
                    "label": 0
                },
                {
                    "sent": "Voice isn't exactly one, or for anything voice with this is exposed.",
                    "label": 0
                },
                {
                    "sent": "So in the Boolean hypercube setting, it's going to be exactly 1.",
                    "label": 0
                },
                {
                    "sent": "It's cool, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Someone rotation, so I'm going to let's represent the target polynomial that we're trying to learn as F of X, and I'm going to write it in the standard sum of product setting.",
                    "label": 0
                },
                {
                    "sent": "So C sub of South here represents the coefficient for the monomial corresponding stress.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume that these coefficients are bounded and the support of X is small, so support of F is polynomial in.",
                    "label": 0
                },
                {
                    "sent": "And again, the goal is to output hypothesis H of LO L2 error.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the result is going to go into three steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is going to be a low degree approximation, where we're going to argue that under these log Lipschitz distributions these sparse polynomials can be approximated by the low degree components.",
                    "label": 1
                },
                {
                    "sent": "The second step is going to be a procedure to identify a set of candidate monomials with the hope that it contains these low degree components of the target function.",
                    "label": 1
                },
                {
                    "sent": "And the last step is going to be an L2 regression algorithm run over the candidate monomials that we find in Step 2.",
                    "label": 0
                },
                {
                    "sent": "So the key point here is going to be to run Step 2 using local queries.",
                    "label": 0
                },
                {
                    "sent": "So let me go describe the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step now, so the claim is that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the function F of X.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the function F of G of X, which represents the low degree components of F components, monomers of size at most D for D equals Logn over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Then XD is a good approximation to F. Or in other words, the higher degree monomials of F have very less contribution.",
                    "label": 0
                },
                {
                    "sent": "And this follows from the claim that if you look at any monomial of size S under a log Lipschitz distribution, the probability that this model is one is going to be is going to depend exponentially in the size of this.",
                    "label": 0
                },
                {
                    "sent": "For example, if you set Alpha equals one, you get 1 / 2 to the size office, which is what you would expect for the uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to go into the second part, which is identifying the candidate low degree monomials.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's assume for now that we are in the uniform distribution setting and some of you might be thinking a very natural thing to do here is to run the famous levels Mansour algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a very powerful algorithm which given query access to a sparse function F outputs all the heavy coefficients of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "By heavy I mean all the monomials whose coefficients are at least Theta.",
                    "label": 0
                },
                {
                    "sent": "Ann, let me briefly describe how this algorithm works because it's.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to be very relevant to our final algorithm, so let's as an example, let's take this function of three variables.",
                    "label": 0
                },
                {
                    "sent": "You can view the Kushilevitz Mansour algorithm or the Cam algorithm is doing some sort of a depth first search.",
                    "label": 0
                },
                {
                    "sent": "Exploration of this tree.",
                    "label": 0
                },
                {
                    "sent": "So let's pick variables in some order an let's branch on them.",
                    "label": 0
                },
                {
                    "sent": "So let's branch on X1 first, so we can decompose the function F into two parts F1 and F, sub minus one.",
                    "label": 0
                },
                {
                    "sent": "X of 1 represents the part of the function that contains X one and F sub.",
                    "label": 0
                },
                {
                    "sent": "Minus one represents a part of the function that does not contain X1.",
                    "label": 0
                },
                {
                    "sent": "Then the algorithm tries to gauge the importance of these two terms by running a test on them, which I will call us the L2 test.",
                    "label": 0
                },
                {
                    "sent": "And the test asks is expected value of F sub one squared large?",
                    "label": 0
                },
                {
                    "sent": "And if the answer is yes, then these.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nodes become green nodes and then you recursively branch on other variables.",
                    "label": 0
                },
                {
                    "sent": "So this procedure is continued until you reach the leaves of the tree.",
                    "label": 0
                },
                {
                    "sent": "Which are going.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correspond to meals.",
                    "label": 0
                },
                {
                    "sent": "And all the monomials at the leaf switch pass the test.",
                    "label": 0
                },
                {
                    "sent": "Are going to be the important monomials that the algorithm is going to output, so this is a very savvy algorithm in the sense that it outputs all the important monomials of the function and it outputs only the important monomials of the functions.",
                    "label": 0
                },
                {
                    "sent": "So it's not wasteful.",
                    "label": 0
                },
                {
                    "sent": "Another nice property of this algorithm is that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At each step, the active nodes in the tree partition the polynomial.",
                    "label": 1
                },
                {
                    "sent": "For example, at step one F1 and F sub minus one partition the polynomial into two disjoint sets.",
                    "label": 0
                },
                {
                    "sent": "And this property leads to a very elegant procedure which bounds the width of this tree at each level.",
                    "label": 0
                },
                {
                    "sent": "So it's a very nice algorithm.",
                    "label": 0
                },
                {
                    "sent": "The problem is that it's not a local algorithm, and during the course of its execution it might actually generate queries by flipping up to order N bits from the given training data.",
                    "label": 0
                },
                {
                    "sent": "So we want to make it local and natural approach or obvious idea is to do.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, search instead of depth first search, so that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "So again, let's start with this function F. And we're going to brunch on all the three variables at the same time.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to look at functions F1F2 and F3, which captured the part of the function that depends on X1X2 and X3 respectively.",
                    "label": 0
                },
                {
                    "sent": "Notice that it's not true anymore that these nodes partition the function, so F1 and F2 might have mono meals which are in common to both of them.",
                    "label": 0
                },
                {
                    "sent": "And again, we're going to run some tests to find out the importance of these nodes.",
                    "label": 0
                },
                {
                    "sent": "And we're going to slightly different tests, which I'm going to call as the non zero test which asks is probability of F are not equal to 0 noticeably.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that we want our algorithm to work for log Lipschitz distributions, and this test turns out to be easier to analyze in that case.",
                    "label": 0
                },
                {
                    "sent": "And again, let.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say F1 and F3 pass the test, so then we are going to recursively branch.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On other variables to go one level down.",
                    "label": 0
                },
                {
                    "sent": "And we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Build this tree to a bounded depth, so we're going to go up to the depth log of an over epsilon, and then we're going to stop.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, how do we extract the monomials that this procedure is going to output?",
                    "label": 0
                },
                {
                    "sent": "So the point here is that we're not going to run this tree up till completion and instead of.",
                    "label": 0
                },
                {
                    "sent": "Doing a depth first search instead of choosing an order over variables and then.",
                    "label": 0
                },
                {
                    "sent": "Going in that order, for example, if the Cam algorithm, if sub one passes the test, then you choose another variable.",
                    "label": 0
                },
                {
                    "sent": "To branch on and then you continue, we're going to do this these things in parallel, so we're going to branch in all the variables simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So it makes the analysis a bit trickier.",
                    "label": 0
                },
                {
                    "sent": "But as I'm going to show later, it ensures that we can implement these steps using local queries.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to stop at depth.",
                    "label": 0
                },
                {
                    "sent": "Log in over epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As candidate monomials, we're going to output any set S IF service was artist.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case F1F3F3, two or service.",
                    "label": 0
                },
                {
                    "sent": "So we're going to add monomials X one X3 and X3, two over set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Forget one other.",
                    "label": 0
                },
                {
                    "sent": "It's useful algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let me at least try to convince you that it's a local algorithm, and in order to do that, I have to tell you how to implement the test.",
                    "label": 0
                },
                {
                    "sent": "The non zero test.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to do that by using the example of single monomial, let's say X1.",
                    "label": 0
                },
                {
                    "sent": "So again, I can write F as X 1 * F sub one and plus F sub minus one.",
                    "label": 0
                },
                {
                    "sent": "And I want to figure out whether probability of F1 not equal to 0 is noticeable.",
                    "label": 1
                },
                {
                    "sent": "And the claim is that this test can be implemented using local queries.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way to do it is very simple.",
                    "label": 0
                },
                {
                    "sent": "You just try to factor out X one.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take take an input and substitute X one equals one and then substitute X 1 = 0 and notice that the value of F sub minus one does not change in these two settings.",
                    "label": 0
                },
                {
                    "sent": "So if I just subtract out these two equations, I get the value of F1 of X given the value of F of X on these two inputs.",
                    "label": 0
                },
                {
                    "sent": "So the expression is going to look something like this, so here we are summing up over values of X1.",
                    "label": 0
                },
                {
                    "sent": "So in fact this test can be implemented using one local query.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, if you have a set S, the expression for F service of X is going to look like this, so we are factoring out S variables.",
                    "label": 0
                },
                {
                    "sent": "So it's going to use size of South local queries.",
                    "label": 0
                },
                {
                    "sent": "Just that.",
                    "label": 0
                },
                {
                    "sent": "It's the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Depth of the tree exactly.",
                    "label": 0
                },
                {
                    "sent": "So since we are stopping the tree at depth Logn over epsilon, we're going to use log in local queries.",
                    "label": 0
                },
                {
                    "sent": "So that's fine, but as a result what happens is we lose a lot of the nice properties of the Cam algorithm.",
                    "label": 0
                },
                {
                    "sent": "For example, this algorithm is going to be wasteful.",
                    "label": 0
                },
                {
                    "sent": "It's going to add potentially at a lot of monomials which are not really important.",
                    "label": 0
                },
                {
                    "sent": "Also, if you look at as I mentioned before, the nodes in the tree might have overlaps in them, so it's you have to be a bit careful in arguing that we don't explore the search space in this case.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you have to trust me on that.",
                    "label": 0
                },
                {
                    "sent": "That can be done and the final step is going to be an L2 regression over all the terms that we added.",
                    "label": 0
                },
                {
                    "sent": "I am not going to go into the details of the analysis, but it relies on the properties of log Lipschitz distributions.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it works.",
                    "label": 0
                },
                {
                    "sent": "So let.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just called the conclusions directly so we study this local membership query model an we show that you can do quite a few interesting things using local membership queries.",
                    "label": 0
                },
                {
                    "sent": "Of the interesting there a lot of interesting open problems we would like to understand the limitations of such local membership queries.",
                    "label": 0
                },
                {
                    "sent": "For example, as Nina asked, is Logn locality necessary for learning these classes?",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "Let me mention some.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pacific open problems.",
                    "label": 0
                },
                {
                    "sent": "Under log Lipschitz distributions we gave algorithms to learn log depth decision trees.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to be able to learn polynomial size decision trees using local algorithms.",
                    "label": 1
                },
                {
                    "sent": "As I mentioned before, our DNF learning algorithm is not polynomial time, so that will be nice too.",
                    "label": 0
                },
                {
                    "sent": "And it will also be interesting to study local membership query algorithms for agnostic learning.",
                    "label": 1
                },
                {
                    "sent": "In particular, if one could agnostically learn parities using local algorithms, that would be a way to resolve open problem 2.",
                    "label": 0
                },
                {
                    "sent": "And we have some reason to believe that this might be hard, so we can.",
                    "label": 0
                },
                {
                    "sent": "For example, we can show that if you want to do agnostic learning, then a constant local queries are not going to be enough.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}