{
    "id": "k7atnxfvddzsmio3kgmcj3ovyhbghoai",
    "title": "A Structured Learning Approach to Attributed Graph Embedding",
    "info": {
        "author": [
            "Antonio Robles-Kelly, National ICT Australia"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_robles_kelly_sla/",
    "segmentation": [
        [
            "I'll be talking about the structure learning approach to attrib."
        ],
        [
            "Graduating, this is some work we have been doing.",
            "LinkedIn camera.",
            "It's collaboration with- shower and doing well.",
            "So the problem?",
            "The problem conceptually is very simple, so one would like to learn a linear mapping.",
            "That embeds attributes of the graph vertices into metric space an that learned linear mapping is embedded is optimal up to some cost function, and actually the way we have defined the problem there includes the requirement that."
        ],
        [
            "The basis of the mapping is an optimal transformation of a linear map from the vertex to the space, and that's basically because here you don't have vectors.",
            "You have graphs, so you have an edge map and then you have a vertex set.",
            "So you would like to represent that edge space an vertex it using a linear map from one to the other.",
            "So that's just a technicality.",
            "In reality, what you want to do is to learn an operator which reflects this structure of the edge space of the graph.",
            "But it's also based on the attributes it is actually computed on the basis of the on the basis of the attributed.",
            "And this operator is optimal with respect to some cost function.",
            "Actually, you would like this cost function to make sense from the machine intelligence point of view, so."
        ],
        [
            "We have a set of.",
            "Interesting questions there.",
            "Now.",
            "This is a very current problem.",
            "The motivation is that embedding methods can be used to transform relational matching problems into point pattern matching ones which which are much simpler to solve.",
            "Also, you would like to be able to compare graphs using metrics in diabetic space.",
            "Actually, the way the problem is defined in terms of linear operators allows you to define algebras over graphs, which is very, very interesting and actually quite powerful.",
            "The way we solved it from the theoretical point of view, it also provides a link between spectral graphs and linear operators, which is not really surprising if one thing.",
            "So finding values and eigenvectors as linear operators anyway."
        ],
        [
            "So.",
            "This is the main question always, so even for very simple patterns.",
            "You have.",
            "Very many realizations of the graph.",
            "So.",
            "That's a very simple pattern, just a lattice.",
            "If you want wants to embed the weighted.",
            "The weight matrix.",
            "Well, the graph.",
            "Into a hyperbolic curvature space.",
            "That's what you get if it's positive constant curvature, namely sphere.",
            "That's what you get.",
            "So now the question is.",
            "What do you choose and?",
            "Quiet.",
            "What is it good for?",
            "And how do you and this is just a very simple example and both are metric spaces.",
            "They have well defined metrics.",
            "They are linear operators actually, because of the way you compute them.",
            "But still I haven't sold the main problem.",
            "I want to an embedding who which is good for recognition and matching.",
            "And I also want an embedding that makes sense from the learning POV.",
            "Actually what I I haven't considered here is that acting edge spaces from a set of graphs will reflect the common graph topology for the graphs under study.",
            "So if I have a learning set and I assume they share some structure in common, then I can use that structure to drive."
        ],
        [
            "The embedding process.",
            "So.",
            "I just considered this.",
            "MRF it's just pretty standard.",
            "That's your unary potential.",
            "That's the binary potential.",
            "The X is are the hidden variables and that is the normalization factor or partition function.",
            "And let's say that our embedding your satisfies this very simple constraint, which is.",
            "It's a linear mapping from the attributes.",
            "That's my hidden variable.",
            "And the Matrix A is just the row stack set of attributes in the graph.",
            "So the command the common approach here is to use a well take the log probability function."
        ],
        [
            "And solve it.",
            "And that's what you get.",
            "Now the problem here is that this function is not convex and it's actually quite simple to show that it's not convex Hessian.",
            "An analysis of the case and just immediately yields non convexity.",
            "And.",
            "In the literature, what people have done in the past is just to find the continuous relaxation relaxation solution.",
            "What we did was to note something.",
            "It's actually quite simple, but it simplifies the problem a lot and it actually gives a convex cost function.",
            "These two terms."
        ],
        [
            "Are actually correlation terms.",
            "So one could say well, correlations are actually similarities.",
            "And therefore they are inverse distances.",
            "So if I substitute them with the norm, well in this case the L2 and then I just introduce unary.",
            "This is what is representing the unary term, and this is what is represented.",
            "Substituting the binary term and now we have a mutation instead of a Max because it's the inverse problem I want to.",
            "Minimize distance now, which is similarity.",
            "So this equation well discussed function is actually convex.",
            "That's that's a vector, and that's actually matrix.",
            "Well, the interest of metrics."
        ],
        [
            "And if one looks at this equation, is actually quite quite rewarding, up to a point, because what we have here is the C acts as a target of the embedding.",
            "And that is.",
            "Intuitive if I substitute.",
            "The equation I used showed for the hidden marvels into the cost function, so this is like a target of the embedding.",
            "We want to take these attributes.",
            "Into this space.",
            "And that is a regularization here, so it's basically forcing your embedding to this path.",
            "And this is intuitively correct.",
            "This is actually what you want in this convex.",
            "It can be solved in a number of different ways.",
            "One could use some definite programming or coding programming.",
            "In this case we did something different because we wanted something that was, well, solution that was sufficient.",
            "So what we did was to."
        ],
        [
            "Introduce a set of LaGrange multipliers.",
            "That's because we want the rows of the transformation matrix.",
            "This the actual operator to add up to unity.",
            "So now it's straightforward to compute the partials, equate them to zero, and solve the linear equation.",
            "So I have also we have preserved the complexity, the complexity of the problem.",
            "It's it's got a unique solution and it's easy to compute.",
            "So now I still have a problem because I have I still have assumed that I know this.",
            "These terms.",
            "So to recover the embedding.",
            "So for every graph, we compute the corresponding incident mapping I'm going.",
            "I'm just going to explain what that is at the bottom using the young householder decomposition of the graph Laplacian and there's a reason for that.",
            "I will just go."
        ],
        [
            "That one shortly, and we compute the vectors CI.",
            "By a PCA on the incidence mapping, so that also has a has a good reason.",
            "Then we just compute the linear mapping by minimizing the cost function so.",
            "Wait, what is the incidence mapping?",
            "Well, it has a number of nice theoretical properties.",
            "They are actually in the paper, but if one goes into if 11 rewrites the Laplacian in terms of incidence mappings these eyes.",
            "One can always express the Laplacian also in terms of the eigenvalue matrix and eigenvector matrix.",
            "And this.",
            "Is actually young householder decomposition formula passing of the Laplacian so it's easy to compute.",
            "It reflects this structure of the of the actual graph in terms of the weight matrix.",
            "It also has a number of nice properties because the incident mapping is actually a mapping from edges to nodes.",
            "And again, the theoretical properties are in the paper, but it does have.",
            "A number of nice properties and that interest into quite the PCA because the PCA actually gives it gives us the edge to node scatter.",
            "Ann again, that's unintuitive result because if only one is using, if one is using the PCA of an embedding of a mapping, that is actually the edge to node mapping, then what the PCA is giving you is discovered.",
            "So.",
            "It it it is a nice choice of.",
            "Target back to see.",
            "Again, one could use define any other way."
        ],
        [
            "Of computing them.",
            "So on the experiments.",
            "We use the MPEG 7 C shape one database which is about which is 1400.",
            "Elements.",
            "70 categories 20 shapes per category and we just did something very simple to represent the graph.",
            "We took the contours of the binary shapes and we sampled at regular intervals.",
            "Each pixel becomes the node and the attribute is becomes the histogram of pairwise distances between the node and neighbors, and in this case we used all the other given.",
            "Selected pixels on the control as the neighbors.",
            "So what we have is a connected graph pairwise distances determining the attributes.",
            "And the embedding space is 70 dimensional.",
            "It's also interesting to note, though, that.",
            "An embedding doesn't have to be a subspace projection.",
            "So.",
            "Because of the way we have defined the operator, you could also do what is called unfolding, which is.",
            "Lift the graph to a higher dimensional space, and that's also quite useful because in many cases higher dimensional spaces are linear, are better, are easier to linearize.",
            "And that is just that, the basis.",
            "The basis of the embedding is less correlated.",
            "So.",
            "You could do multi dimensional scaling, but you could also do unfolding with this method.",
            "So just grab matching very simple."
        ],
        [
            "We computed the embedding and then we just embedded two graphs.",
            "And looked at the nearest neighbor of each of the embedded points.",
            "Anne."
        ],
        [
            "That's that's what you get with our method.",
            "That's what you get with software sign we here you can see.",
            "One of the inherent advantages of operators, which is their permutation invariant.",
            "And because of the way we have computed the attributes in this case they will be scale invariant.",
            "We have normalized the pairwise distances to be between zero and one and it will be also rotation invariant because we have used distances.",
            "So for shape categorisation we embedded the graphs and then.",
            "We computed codewords in a very similar way to what bag of features does which is used in this case we took the histogram of distances in the embedding space.",
            "The codebook was recovered using K means on the training data.",
            "Well, the the pairwise distances and embedding space for the training data, and then the testing was done using the code book and a linear SVM.",
            "And it's a very simple method.",
            "It used is based on distances for the attributes of the graphs is based on distances on the image lattice.",
            "For the actual code book is based on distances in the building.",
            "Space is a very very simple method used regular sampling on the contour of the binary shape and actually does better than very specialized methods like is skeletal context or shape contexts.",
            "It actually does better than.",
            "This structure learning method.",
            "This is a very recent method on learning on the metrics that you get.",
            "Out of matching processes."
        ],
        [
            "So.",
            "Conclusions.",
            "We can propose on a better method which is based upon the graph attributes and at the same time is related to the graph topology.",
            "It's linear operator.",
            "It's very simple conceptually, it's.",
            "It's a mapping recovered by a learning process and the cost function is convincing.",
            "Complex in nature can be solved efficiently.",
            "So yeah, that's basically it.",
            "Any questions, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be talking about the structure learning approach to attrib.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graduating, this is some work we have been doing.",
                    "label": 0
                },
                {
                    "sent": "LinkedIn camera.",
                    "label": 0
                },
                {
                    "sent": "It's collaboration with- shower and doing well.",
                    "label": 0
                },
                {
                    "sent": "So the problem?",
                    "label": 0
                },
                {
                    "sent": "The problem conceptually is very simple, so one would like to learn a linear mapping.",
                    "label": 0
                },
                {
                    "sent": "That embeds attributes of the graph vertices into metric space an that learned linear mapping is embedded is optimal up to some cost function, and actually the way we have defined the problem there includes the requirement that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basis of the mapping is an optimal transformation of a linear map from the vertex to the space, and that's basically because here you don't have vectors.",
                    "label": 1
                },
                {
                    "sent": "You have graphs, so you have an edge map and then you have a vertex set.",
                    "label": 0
                },
                {
                    "sent": "So you would like to represent that edge space an vertex it using a linear map from one to the other.",
                    "label": 0
                },
                {
                    "sent": "So that's just a technicality.",
                    "label": 1
                },
                {
                    "sent": "In reality, what you want to do is to learn an operator which reflects this structure of the edge space of the graph.",
                    "label": 1
                },
                {
                    "sent": "But it's also based on the attributes it is actually computed on the basis of the on the basis of the attributed.",
                    "label": 0
                },
                {
                    "sent": "And this operator is optimal with respect to some cost function.",
                    "label": 0
                },
                {
                    "sent": "Actually, you would like this cost function to make sense from the machine intelligence point of view, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a set of.",
                    "label": 0
                },
                {
                    "sent": "Interesting questions there.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is a very current problem.",
                    "label": 0
                },
                {
                    "sent": "The motivation is that embedding methods can be used to transform relational matching problems into point pattern matching ones which which are much simpler to solve.",
                    "label": 1
                },
                {
                    "sent": "Also, you would like to be able to compare graphs using metrics in diabetic space.",
                    "label": 0
                },
                {
                    "sent": "Actually, the way the problem is defined in terms of linear operators allows you to define algebras over graphs, which is very, very interesting and actually quite powerful.",
                    "label": 0
                },
                {
                    "sent": "The way we solved it from the theoretical point of view, it also provides a link between spectral graphs and linear operators, which is not really surprising if one thing.",
                    "label": 1
                },
                {
                    "sent": "So finding values and eigenvectors as linear operators anyway.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the main question always, so even for very simple patterns.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "Very many realizations of the graph.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple pattern, just a lattice.",
                    "label": 0
                },
                {
                    "sent": "If you want wants to embed the weighted.",
                    "label": 0
                },
                {
                    "sent": "The weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Well, the graph.",
                    "label": 0
                },
                {
                    "sent": "Into a hyperbolic curvature space.",
                    "label": 0
                },
                {
                    "sent": "That's what you get if it's positive constant curvature, namely sphere.",
                    "label": 0
                },
                {
                    "sent": "That's what you get.",
                    "label": 0
                },
                {
                    "sent": "So now the question is.",
                    "label": 0
                },
                {
                    "sent": "What do you choose and?",
                    "label": 0
                },
                {
                    "sent": "Quiet.",
                    "label": 0
                },
                {
                    "sent": "What is it good for?",
                    "label": 0
                },
                {
                    "sent": "And how do you and this is just a very simple example and both are metric spaces.",
                    "label": 0
                },
                {
                    "sent": "They have well defined metrics.",
                    "label": 0
                },
                {
                    "sent": "They are linear operators actually, because of the way you compute them.",
                    "label": 0
                },
                {
                    "sent": "But still I haven't sold the main problem.",
                    "label": 0
                },
                {
                    "sent": "I want to an embedding who which is good for recognition and matching.",
                    "label": 0
                },
                {
                    "sent": "And I also want an embedding that makes sense from the learning POV.",
                    "label": 0
                },
                {
                    "sent": "Actually what I I haven't considered here is that acting edge spaces from a set of graphs will reflect the common graph topology for the graphs under study.",
                    "label": 1
                },
                {
                    "sent": "So if I have a learning set and I assume they share some structure in common, then I can use that structure to drive.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The embedding process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I just considered this.",
                    "label": 0
                },
                {
                    "sent": "MRF it's just pretty standard.",
                    "label": 0
                },
                {
                    "sent": "That's your unary potential.",
                    "label": 0
                },
                {
                    "sent": "That's the binary potential.",
                    "label": 0
                },
                {
                    "sent": "The X is are the hidden variables and that is the normalization factor or partition function.",
                    "label": 1
                },
                {
                    "sent": "And let's say that our embedding your satisfies this very simple constraint, which is.",
                    "label": 0
                },
                {
                    "sent": "It's a linear mapping from the attributes.",
                    "label": 0
                },
                {
                    "sent": "That's my hidden variable.",
                    "label": 1
                },
                {
                    "sent": "And the Matrix A is just the row stack set of attributes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So the command the common approach here is to use a well take the log probability function.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And solve it.",
                    "label": 0
                },
                {
                    "sent": "And that's what you get.",
                    "label": 0
                },
                {
                    "sent": "Now the problem here is that this function is not convex and it's actually quite simple to show that it's not convex Hessian.",
                    "label": 1
                },
                {
                    "sent": "An analysis of the case and just immediately yields non convexity.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "In the literature, what people have done in the past is just to find the continuous relaxation relaxation solution.",
                    "label": 0
                },
                {
                    "sent": "What we did was to note something.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite simple, but it simplifies the problem a lot and it actually gives a convex cost function.",
                    "label": 0
                },
                {
                    "sent": "These two terms.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are actually correlation terms.",
                    "label": 0
                },
                {
                    "sent": "So one could say well, correlations are actually similarities.",
                    "label": 0
                },
                {
                    "sent": "And therefore they are inverse distances.",
                    "label": 0
                },
                {
                    "sent": "So if I substitute them with the norm, well in this case the L2 and then I just introduce unary.",
                    "label": 0
                },
                {
                    "sent": "This is what is representing the unary term, and this is what is represented.",
                    "label": 0
                },
                {
                    "sent": "Substituting the binary term and now we have a mutation instead of a Max because it's the inverse problem I want to.",
                    "label": 0
                },
                {
                    "sent": "Minimize distance now, which is similarity.",
                    "label": 0
                },
                {
                    "sent": "So this equation well discussed function is actually convex.",
                    "label": 0
                },
                {
                    "sent": "That's that's a vector, and that's actually matrix.",
                    "label": 0
                },
                {
                    "sent": "Well, the interest of metrics.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if one looks at this equation, is actually quite quite rewarding, up to a point, because what we have here is the C acts as a target of the embedding.",
                    "label": 0
                },
                {
                    "sent": "And that is.",
                    "label": 0
                },
                {
                    "sent": "Intuitive if I substitute.",
                    "label": 0
                },
                {
                    "sent": "The equation I used showed for the hidden marvels into the cost function, so this is like a target of the embedding.",
                    "label": 0
                },
                {
                    "sent": "We want to take these attributes.",
                    "label": 0
                },
                {
                    "sent": "Into this space.",
                    "label": 0
                },
                {
                    "sent": "And that is a regularization here, so it's basically forcing your embedding to this path.",
                    "label": 0
                },
                {
                    "sent": "And this is intuitively correct.",
                    "label": 0
                },
                {
                    "sent": "This is actually what you want in this convex.",
                    "label": 0
                },
                {
                    "sent": "It can be solved in a number of different ways.",
                    "label": 0
                },
                {
                    "sent": "One could use some definite programming or coding programming.",
                    "label": 0
                },
                {
                    "sent": "In this case we did something different because we wanted something that was, well, solution that was sufficient.",
                    "label": 0
                },
                {
                    "sent": "So what we did was to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduce a set of LaGrange multipliers.",
                    "label": 1
                },
                {
                    "sent": "That's because we want the rows of the transformation matrix.",
                    "label": 1
                },
                {
                    "sent": "This the actual operator to add up to unity.",
                    "label": 0
                },
                {
                    "sent": "So now it's straightforward to compute the partials, equate them to zero, and solve the linear equation.",
                    "label": 0
                },
                {
                    "sent": "So I have also we have preserved the complexity, the complexity of the problem.",
                    "label": 0
                },
                {
                    "sent": "It's it's got a unique solution and it's easy to compute.",
                    "label": 0
                },
                {
                    "sent": "So now I still have a problem because I have I still have assumed that I know this.",
                    "label": 0
                },
                {
                    "sent": "These terms.",
                    "label": 0
                },
                {
                    "sent": "So to recover the embedding.",
                    "label": 0
                },
                {
                    "sent": "So for every graph, we compute the corresponding incident mapping I'm going.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to explain what that is at the bottom using the young householder decomposition of the graph Laplacian and there's a reason for that.",
                    "label": 0
                },
                {
                    "sent": "I will just go.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That one shortly, and we compute the vectors CI.",
                    "label": 0
                },
                {
                    "sent": "By a PCA on the incidence mapping, so that also has a has a good reason.",
                    "label": 1
                },
                {
                    "sent": "Then we just compute the linear mapping by minimizing the cost function so.",
                    "label": 1
                },
                {
                    "sent": "Wait, what is the incidence mapping?",
                    "label": 0
                },
                {
                    "sent": "Well, it has a number of nice theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "They are actually in the paper, but if one goes into if 11 rewrites the Laplacian in terms of incidence mappings these eyes.",
                    "label": 1
                },
                {
                    "sent": "One can always express the Laplacian also in terms of the eigenvalue matrix and eigenvector matrix.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "Is actually young householder decomposition formula passing of the Laplacian so it's easy to compute.",
                    "label": 0
                },
                {
                    "sent": "It reflects this structure of the of the actual graph in terms of the weight matrix.",
                    "label": 0
                },
                {
                    "sent": "It also has a number of nice properties because the incident mapping is actually a mapping from edges to nodes.",
                    "label": 0
                },
                {
                    "sent": "And again, the theoretical properties are in the paper, but it does have.",
                    "label": 0
                },
                {
                    "sent": "A number of nice properties and that interest into quite the PCA because the PCA actually gives it gives us the edge to node scatter.",
                    "label": 0
                },
                {
                    "sent": "Ann again, that's unintuitive result because if only one is using, if one is using the PCA of an embedding of a mapping, that is actually the edge to node mapping, then what the PCA is giving you is discovered.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It it it is a nice choice of.",
                    "label": 0
                },
                {
                    "sent": "Target back to see.",
                    "label": 0
                },
                {
                    "sent": "Again, one could use define any other way.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of computing them.",
                    "label": 0
                },
                {
                    "sent": "So on the experiments.",
                    "label": 0
                },
                {
                    "sent": "We use the MPEG 7 C shape one database which is about which is 1400.",
                    "label": 1
                },
                {
                    "sent": "Elements.",
                    "label": 0
                },
                {
                    "sent": "70 categories 20 shapes per category and we just did something very simple to represent the graph.",
                    "label": 0
                },
                {
                    "sent": "We took the contours of the binary shapes and we sampled at regular intervals.",
                    "label": 0
                },
                {
                    "sent": "Each pixel becomes the node and the attribute is becomes the histogram of pairwise distances between the node and neighbors, and in this case we used all the other given.",
                    "label": 0
                },
                {
                    "sent": "Selected pixels on the control as the neighbors.",
                    "label": 0
                },
                {
                    "sent": "So what we have is a connected graph pairwise distances determining the attributes.",
                    "label": 1
                },
                {
                    "sent": "And the embedding space is 70 dimensional.",
                    "label": 1
                },
                {
                    "sent": "It's also interesting to note, though, that.",
                    "label": 0
                },
                {
                    "sent": "An embedding doesn't have to be a subspace projection.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because of the way we have defined the operator, you could also do what is called unfolding, which is.",
                    "label": 0
                },
                {
                    "sent": "Lift the graph to a higher dimensional space, and that's also quite useful because in many cases higher dimensional spaces are linear, are better, are easier to linearize.",
                    "label": 0
                },
                {
                    "sent": "And that is just that, the basis.",
                    "label": 0
                },
                {
                    "sent": "The basis of the embedding is less correlated.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You could do multi dimensional scaling, but you could also do unfolding with this method.",
                    "label": 0
                },
                {
                    "sent": "So just grab matching very simple.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We computed the embedding and then we just embedded two graphs.",
                    "label": 0
                },
                {
                    "sent": "And looked at the nearest neighbor of each of the embedded points.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's that's what you get with our method.",
                    "label": 0
                },
                {
                    "sent": "That's what you get with software sign we here you can see.",
                    "label": 0
                },
                {
                    "sent": "One of the inherent advantages of operators, which is their permutation invariant.",
                    "label": 0
                },
                {
                    "sent": "And because of the way we have computed the attributes in this case they will be scale invariant.",
                    "label": 0
                },
                {
                    "sent": "We have normalized the pairwise distances to be between zero and one and it will be also rotation invariant because we have used distances.",
                    "label": 0
                },
                {
                    "sent": "So for shape categorisation we embedded the graphs and then.",
                    "label": 1
                },
                {
                    "sent": "We computed codewords in a very similar way to what bag of features does which is used in this case we took the histogram of distances in the embedding space.",
                    "label": 1
                },
                {
                    "sent": "The codebook was recovered using K means on the training data.",
                    "label": 1
                },
                {
                    "sent": "Well, the the pairwise distances and embedding space for the training data, and then the testing was done using the code book and a linear SVM.",
                    "label": 1
                },
                {
                    "sent": "And it's a very simple method.",
                    "label": 0
                },
                {
                    "sent": "It used is based on distances for the attributes of the graphs is based on distances on the image lattice.",
                    "label": 0
                },
                {
                    "sent": "For the actual code book is based on distances in the building.",
                    "label": 0
                },
                {
                    "sent": "Space is a very very simple method used regular sampling on the contour of the binary shape and actually does better than very specialized methods like is skeletal context or shape contexts.",
                    "label": 0
                },
                {
                    "sent": "It actually does better than.",
                    "label": 0
                },
                {
                    "sent": "This structure learning method.",
                    "label": 0
                },
                {
                    "sent": "This is a very recent method on learning on the metrics that you get.",
                    "label": 0
                },
                {
                    "sent": "Out of matching processes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "We can propose on a better method which is based upon the graph attributes and at the same time is related to the graph topology.",
                    "label": 1
                },
                {
                    "sent": "It's linear operator.",
                    "label": 0
                },
                {
                    "sent": "It's very simple conceptually, it's.",
                    "label": 1
                },
                {
                    "sent": "It's a mapping recovered by a learning process and the cost function is convincing.",
                    "label": 0
                },
                {
                    "sent": "Complex in nature can be solved efficiently.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's basically it.",
                    "label": 0
                },
                {
                    "sent": "Any questions, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}