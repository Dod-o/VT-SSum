{
    "id": "mb5j5o2xvqbr6mofzlxclbqjaaqqzmo6",
    "title": "A Theory of Multiclass Boosting",
    "info": {
        "author": [
            "Indraneel Mukherjee, Computer Science Department, Princeton University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/nips2010_mukherjee_tmb/",
    "segmentation": [
        [
            "Thanks for coming.",
            "I'll be talking about some new results on multiclass boosting.",
            "This is John work with Robert Peerie."
        ],
        [
            "Right, So what is boosting?",
            "Boosting is a technique for classification problems and it works by combining a lot of simple rules into one powerful predictor.",
            "So here's an example of a classification problem.",
            "Categorized news headlines into the sports business or politics Asim."
        ],
        [
            "The rule could, depending on the presence or absence of the word liquid ITI in the text of the news headline classified business or something else."
        ],
        [
            "These are called weak classifiers because typically they achieve low accuracy."
        ],
        [
            "And boosting combines several weak classifiers to find one highly accurate classifier."
        ],
        [
            "An important."
        ],
        [
            "All of boosting is to be able to boost the simplest weak classifiers in boost."
        ],
        [
            "In parlance, this means requiring the weak classifiers to satisfy the right week learning condition using simple weak classifiers would result in similar final models and which may help prevent overfitting.",
            "At the same time, if the weak classifiers are not powerful enough.",
            "This may lead to Underfitting because no amount of boosting can combine them to form a highly accurate classifier."
        ],
        [
            "Therefore, the right week learning condition is important for generalization, and while the theory is well understood for binary class."
        ],
        [
            "Application many basic questions remain unanswered for multiclass classification, and multiclass is encountered more frequently."
        ],
        [
            "In practice, so in this work we pray."
        ],
        [
            "Then a theory for multiclass boosting we."
        ],
        [
            "Showed that."
        ],
        [
            "Existing frameworks for studying boosting often lead to weak learning condition."
        ],
        [
            "Is that are either too weak or stronger than necessary for boosting?",
            "To fix this, we"
        ],
        [
            "There's a new framework in which we can find."
        ],
        [
            "Only make precise the minimal weak learning condition under which boosting is still possible."
        ],
        [
            "Create boosting algorithms relying only on these minimal assumptions, which can never."
        ],
        [
            "Less probably drive down training error efficiently."
        ],
        [
            "And finally we present some experiments to back our theory."
        ],
        [
            "So we start by looking at the existing framework which was developed for binary classification.",
            "The binary boosting framework is best thought of as a game."
        ],
        [
            "There is a fixed training set and there are two players, booster, an weak learner, which we represent as a fixed space of weak classifiers.",
            "The game proceeds through Raw."
        ],
        [
            "In each round, the booster sends a distribution over the training set to the weak and receives a weak classifier belonging to the fixed space of weak classifiers."
        ],
        [
            "The binary weak learning condition requires the weak classifier to achieve better accuracy than random guessing."
        ],
        [
            "And."
        ],
        [
            "The final model."
        ],
        [
            "Is a weighted majority word combination of the weak classifiers received through various rounds on a new example, the.",
            "Model outputs the label which was predicted a majority of the times by the weak.",
            "Classifiers are reasonable.",
            "Booster strategy is and in any round is to put more weight on the examples that have been misclassified more often so far, and when the binary weak learning condition is satisfied, this strategy can find a majority vote combination which is perfect accuracy after very few rounds."
        ],
        [
            "So the binary week learning condition satisfies a number of interesting useful properties."
        ],
        [
            "For instance, the task posed to the weak classifier in each round is easy.",
            "Namely, just beat random."
        ],
        [
            "Yet satisfying just this."
        ],
        [
            "And ensures that we classifier spaces boostable, that is it contains its classifiers.",
            "Can be combined to form a perfectly accurate weighted majority vote classifier."
        ],
        [
            "At the same time, these conditions are necessary because any boostable space automatically satisfies the binary weak learning condition."
        ],
        [
            "And finally, these conditions are effective because boosting algorithms relying on these assumptions can efficiently drive down training error.",
            "So we want similar week learning conditions for multi in the multiclass setting."
        ],
        [
            "But directly extending, this framework runs into problems."
        ],
        [
            "So to see that, let's stick to this framework where the booster continues to send a distribution over training over the training set and receive a weak classifier which now Maps examples to single multiclass labels.",
            "That's the only difference from the binary framework we are considering, and so here is an attempt at.",
            "Multi Class week learn."
        ],
        [
            "Condition require better performance than random guessing and this has been used by this in the Sammy algorithm by Zoet."
        ],
        [
            "It turns out that this condition is too weak for boosting to be possible."
        ],
        [
            "Another attempt is to continue to require more than 50% accuracy, and this has been used in the Adaboost M1 algorithm, which is a direct multi class extension of Adaboost."
        ],
        [
            "Turns out this this condition is too strong, by which we mean that there are boostable spaces which fail to satisfy this condition."
        ],
        [
            "Other approaches have relied on reductions here.",
            "A multiclass problem is reduced to a bunch of artificially created binary classification problems.",
            "The created binary problems are individually solved using binary boosting and then the predictors are combined to form a classifier for the original problem."
        ],
        [
            "These have these methods have been shown to be practical, but it's not clear that this is the best approach.",
            "For instance."
        ],
        [
            "The weak learning condition being implicitly assumed here is that the artificial binary problems satisfies the binary weak learning condition and be sure that this can sometimes be stronger than necessary."
        ],
        [
            "For instance.",
            "The one against all reductions used in the Adaboost image algorithm may sometimes create binary problems which are harder than the original problem which a boostable space might fail to satisfy."
        ],
        [
            "To resolve these issues, we present a new framework which is 3 key difference."
        ],
        [
            "Firstly, in every round the booster does not send a distribution but a cost matrix and this cost matrix has as many rows as examples and columns."
        ],
        [
            "These labels the eye, Ellet entry of the cost matrix, specifies the cost of predicting label L. On example I.",
            "So that a week class and the cost of a weak classifier is the sum of its prediction costs.",
            "Over all the examples and is given by this expression."
        ],
        [
            "Secondly, there we introduce a fixed baseline predictor with which the weak learner must compete.",
            "The baseline makes random predictions on the examples."
        ],
        [
            "On example BI, L is the probability with which the baseline predicts label L on example I.",
            "And the cost of the baseline is the sum of its expected prediction costs on all its on all the examples and is given by its express."
        ],
        [
            "Finally, we require that in each round the weak classifier suffer equal or less cost than the fixed baseline with respect to the cost matrix that was sent by the booster in that."
        ],
        [
            "Rd putting it all together."
        ],
        [
            "Here.",
            "Here is the framework.",
            "There is a fixed baseline which is a."
        ],
        [
            "Diameter of the framework and then in each round the booster sends a cost matrix and it receives a weak classifier."
        ],
        [
            "Which suffers equal or less cost compared to the baseline.",
            "As an exam."
        ],
        [
            "But we can exactly recover the binary boosting framework by appropriately choosing the baseline, the baseline on each example should predict the correct label with probability half plus gamma.",
            "And predict the incorrect label with probability half minus gamma.",
            "Now this can be generalized to the multiclass setting by considering more general baselines."
        ],
        [
            "We call Edgewood."
        ],
        [
            "Random baseline.",
            "This Q should be a BM.",
            "Sorry bout that."
        ],
        [
            "So an edge of a random baseline.",
            "Will on each example make the right prediction with slightly more probability than it makes any incorrect prediction?"
        ],
        [
            "Notice that there is a unique edge over random baseline.",
            "In the binary classification case, but when there are three or more labels, there are infinitely many possible edges were random."
        ],
        [
            "Baselines a weak learning condition which."
        ],
        [
            "Employs an edge over random baseline will be called an edge over random weak learning condition, and so there are infinitely many edge over random week learning conditions these."
        ],
        [
            "Conditions have many of the nice properties of the binary weak learning condition, for instance, and every round we require."
        ],
        [
            "Performing slightly better than random guys."
        ],
        [
            "Turns out that if if a weak classifiers space satisfies any fixed edge over random condition, then it is also boostable, so it is sufficient."
        ],
        [
            "Moreover, we have been able to create boosting algorithms for any fixed edge over random condition that can drive down training error efficiently.",
            "So these these are effective."
        ],
        [
            "However, unfortunately these are not necessary because for any fixed edge over random condition, there are theoretical constructions of boostable spaces which fails to satisfy this condition.",
            "However."
        ],
        [
            "These satisfy as a whole.",
            "They are necessary.",
            "By which we mean that for any boostable space there is some edge over random condition which that space satisfies and therefore by combine."
        ],
        [
            "Bring all of them.",
            "We can create one single minimal week learning can do."
        ],
        [
            "Which is both necessary and sufficient for boost ability, and this is the right week learning condition in the multiclass setting."
        ],
        [
            "So in the next turn to algorithms using these conditions."
        ],
        [
            "For any fixed edgeware random condition we have derived boosting algorithms that are optimally efficient in a certain sense."
        ],
        [
            "Since.",
            "In the binary setting, similar a similar algorithm called Boost by majority was derived by Freud.",
            "So the issue with all these."
        ],
        [
            "Boredoms, despite their optimality, is that they are non adaptive.",
            "They require advanced knowledge of.",
            "The parameter gamma, which you probably remember from some of the earlier slides and this makes them impact."
        ],
        [
            "But based on the ideas behind these algorithms, we were able to derive adaptive algorithms which assume the minimum week learning."
        ],
        [
            "Addition.",
            "These algorithms are.",
            "Based on cheap computational multiplicative updates.",
            "Computationally cheap multiplicative updates, and sorry very much in the spirit of Adam."
        ],
        [
            "Boost.",
            "And although they are not optimal, but they're still probably extremely efficient, like they drive down training error exponentially passed with the number of rounds."
        ],
        [
            "So here is a high level description of the adaptive adaptive algorithm."
        ],
        [
            "In each row."
        ],
        [
            "Only tend to cost matrix receives of."
        ],
        [
            "Classifier suffering some with having some edge Delta T."
        ],
        [
            "For random and based on this edge, it assigns a weight to this week classifier and its final prediction will be the weighted majority vote combination of these weak classifiers which is updated and this weekend majority vote is updated.",
            "Yeah, as shown here.",
            "Alright, so this is the high level.",
            "The details of how to compute the edge and the cost."
        ],
        [
            "Matrix in each round is not much more complicated.",
            "It's given by these simple expressions, but I won't have time to go describe them.",
            "More detail here, but they are simple."
        ],
        [
            "OK, we are in order to test empirically.",
            "Tests are adaptive algorithm.",
            "We compared our method with existing algorithms on benchmark datasets.",
            "The weak classifiers we used were decision trees and we control their complexity by restricting the size of the decision trees."
        ],
        [
            "So we wanted to test whether our method, which relies on theoretically minimal assumptions, can in practice also be more effective than other algorithms when used with very weak learners.",
            "So this and here are the results.",
            "So the six graphs correspond to six datasets.",
            "The three curves correspond to three algorithms.",
            "The red curve is our new method.",
            "Black Curve is Adaboost M1A direct, the multiclass extension of Adaboost, and the blue curve is Adaboost image, which is based on one against all reductions.",
            "The X axis is the weak classifier complexity measured by the maximum size of the decision.",
            "Cheese we've allowed, and the Y axis is the test error after 500 rounds of boosting.",
            "In each experiment, our algorithm achieves the lowest test error when used with weaker classifiers.",
            "And to understand this phenomenon better phenomenon better, we examine more carefully what happens when using extremely weak classifiers, which are decision trees with at most 5 nodes."
        ],
        [
            "Ncura ARD results.",
            "The X axis now is the number of rounds of boosting and the Y axis is the test error.",
            "Of the majority vote combination, each curve therefore depicts how the weighted majority combinations test error falls with the number of rounds of boosting.",
            "As you can see Adaboost dot MH and an one initially aggressively reduced test error.",
            "But then they flatten out because their aggressive demands can no longer be satisfied by the extremely weak weak learner.",
            "And at that point, these algorithms essentially terminate.",
            "However, our method makes much more gentler, much more gentle demands on the weak classifiers, and are able to drive down error for all 500 rounds."
        ],
        [
            "Right, so before concluding, I would like to talk about what we are currently doing in this."
        ],
        [
            "Object so.",
            "We've been working on extending this theory for weak classifiers, which do not, which have been which not make single multiclass predictions but multilabel multiclass prediction, and these have been used a lot in practice.",
            "So this needs to be studied."
        ],
        [
            "Another line of research is to explore what happens when even the boostability assumption fails to hold, which is what happens most of the times in practice.",
            "Can we still show meaningful behavior?",
            "For instance, consistency results, saying that our algorithm will converge to the optimal combination.",
            "Even though that optimal combination cannot achieve 0 train."
        ],
        [
            "Never.",
            "And finally, we would like to understand the minimal assumptions necessary for.",
            "Boosting algorithms for solving the ranking problem."
        ],
        [
            "Right, thank you.",
            "We have some time for questions.",
            "So let me maybe get started with the question so in the binary setting there is a really nice connection between boost ability and linear separation, so there's going to geometric intuition.",
            "Do you think such a geometric intuition is possible in the multiclass case?",
            "Yes, so the results which was necessary and sufficient is based on Minimax theorem, which shows that if the week learning condition is satisfied, then there exists geometric linear separator, so it is very much like the binary case.",
            "The question so for the binary boosting you can interpret it as minimizing the exponential loss.",
            "Is there such an interpretation for your multiclass boosting?",
            "Yes, so Adaboost minimize this exponential loss, although the ideal goal is to minimize 01 error, so the optimal nonadaptive algorithms that I described the minimize 01 error, but the adaptive algorithm with which the experiments were done minimize exponential loss.",
            "And this loss is the following.",
            "It is a sum of exponential terms and the let's say the correct label is 1.",
            "The first term is E to the number of words for label 2 minus number of words for label one plus the second term will be E to the number of votes were labeled 3 minus number of words for the correct label one and so on, so that this exponential loss actually upper bounds error.",
            "And that is what we try to minimize in the algorithm.",
            "Yeah, in the binary case.",
            "There is a dual problem so.",
            "In the distribution domain you minimize, maximize the minimum edge and then in the tool you maximize the minimum margin.",
            "Do you know what?",
            "What Max maximize the minimum margin corresponds to in the multiclass case.",
            "Yeah, so as I said, this is the.",
            "If so, if you can correctly classify with an edge.",
            "Then that means there is a combination which gets every example right within the same edge.",
            "So it is very much the same minimax argument happening here.",
            "It's.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks for coming.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about some new results on multiclass boosting.",
                    "label": 1
                },
                {
                    "sent": "This is John work with Robert Peerie.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what is boosting?",
                    "label": 0
                },
                {
                    "sent": "Boosting is a technique for classification problems and it works by combining a lot of simple rules into one powerful predictor.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of a classification problem.",
                    "label": 0
                },
                {
                    "sent": "Categorized news headlines into the sports business or politics Asim.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The rule could, depending on the presence or absence of the word liquid ITI in the text of the news headline classified business or something else.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are called weak classifiers because typically they achieve low accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And boosting combines several weak classifiers to find one highly accurate classifier.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An important.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of boosting is to be able to boost the simplest weak classifiers in boost.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In parlance, this means requiring the weak classifiers to satisfy the right week learning condition using simple weak classifiers would result in similar final models and which may help prevent overfitting.",
                    "label": 1
                },
                {
                    "sent": "At the same time, if the weak classifiers are not powerful enough.",
                    "label": 1
                },
                {
                    "sent": "This may lead to Underfitting because no amount of boosting can combine them to form a highly accurate classifier.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, the right week learning condition is important for generalization, and while the theory is well understood for binary class.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Application many basic questions remain unanswered for multiclass classification, and multiclass is encountered more frequently.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, so in this work we pray.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then a theory for multiclass boosting we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Showed that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Existing frameworks for studying boosting often lead to weak learning condition.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that are either too weak or stronger than necessary for boosting?",
                    "label": 0
                },
                {
                    "sent": "To fix this, we",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a new framework in which we can find.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only make precise the minimal weak learning condition under which boosting is still possible.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Create boosting algorithms relying only on these minimal assumptions, which can never.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Less probably drive down training error efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally we present some experiments to back our theory.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we start by looking at the existing framework which was developed for binary classification.",
                    "label": 0
                },
                {
                    "sent": "The binary boosting framework is best thought of as a game.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a fixed training set and there are two players, booster, an weak learner, which we represent as a fixed space of weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "The game proceeds through Raw.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In each round, the booster sends a distribution over the training set to the weak and receives a weak classifier belonging to the fixed space of weak classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The binary weak learning condition requires the weak classifier to achieve better accuracy than random guessing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The final model.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a weighted majority word combination of the weak classifiers received through various rounds on a new example, the.",
                    "label": 0
                },
                {
                    "sent": "Model outputs the label which was predicted a majority of the times by the weak.",
                    "label": 0
                },
                {
                    "sent": "Classifiers are reasonable.",
                    "label": 0
                },
                {
                    "sent": "Booster strategy is and in any round is to put more weight on the examples that have been misclassified more often so far, and when the binary weak learning condition is satisfied, this strategy can find a majority vote combination which is perfect accuracy after very few rounds.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the binary week learning condition satisfies a number of interesting useful properties.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance, the task posed to the weak classifier in each round is easy.",
                    "label": 0
                },
                {
                    "sent": "Namely, just beat random.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet satisfying just this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And ensures that we classifier spaces boostable, that is it contains its classifiers.",
                    "label": 0
                },
                {
                    "sent": "Can be combined to form a perfectly accurate weighted majority vote classifier.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the same time, these conditions are necessary because any boostable space automatically satisfies the binary weak learning condition.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, these conditions are effective because boosting algorithms relying on these assumptions can efficiently drive down training error.",
                    "label": 0
                },
                {
                    "sent": "So we want similar week learning conditions for multi in the multiclass setting.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But directly extending, this framework runs into problems.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to see that, let's stick to this framework where the booster continues to send a distribution over training over the training set and receive a weak classifier which now Maps examples to single multiclass labels.",
                    "label": 0
                },
                {
                    "sent": "That's the only difference from the binary framework we are considering, and so here is an attempt at.",
                    "label": 0
                },
                {
                    "sent": "Multi Class week learn.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Condition require better performance than random guessing and this has been used by this in the Sammy algorithm by Zoet.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that this condition is too weak for boosting to be possible.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another attempt is to continue to require more than 50% accuracy, and this has been used in the Adaboost M1 algorithm, which is a direct multi class extension of Adaboost.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turns out this this condition is too strong, by which we mean that there are boostable spaces which fail to satisfy this condition.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other approaches have relied on reductions here.",
                    "label": 0
                },
                {
                    "sent": "A multiclass problem is reduced to a bunch of artificially created binary classification problems.",
                    "label": 0
                },
                {
                    "sent": "The created binary problems are individually solved using binary boosting and then the predictors are combined to form a classifier for the original problem.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These have these methods have been shown to be practical, but it's not clear that this is the best approach.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The weak learning condition being implicitly assumed here is that the artificial binary problems satisfies the binary weak learning condition and be sure that this can sometimes be stronger than necessary.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "The one against all reductions used in the Adaboost image algorithm may sometimes create binary problems which are harder than the original problem which a boostable space might fail to satisfy.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To resolve these issues, we present a new framework which is 3 key difference.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Firstly, in every round the booster does not send a distribution but a cost matrix and this cost matrix has as many rows as examples and columns.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These labels the eye, Ellet entry of the cost matrix, specifies the cost of predicting label L. On example I.",
                    "label": 1
                },
                {
                    "sent": "So that a week class and the cost of a weak classifier is the sum of its prediction costs.",
                    "label": 0
                },
                {
                    "sent": "Over all the examples and is given by this expression.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Secondly, there we introduce a fixed baseline predictor with which the weak learner must compete.",
                    "label": 0
                },
                {
                    "sent": "The baseline makes random predictions on the examples.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On example BI, L is the probability with which the baseline predicts label L on example I.",
                    "label": 0
                },
                {
                    "sent": "And the cost of the baseline is the sum of its expected prediction costs on all its on all the examples and is given by its express.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we require that in each round the weak classifier suffer equal or less cost than the fixed baseline with respect to the cost matrix that was sent by the booster in that.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rd putting it all together.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Here is the framework.",
                    "label": 0
                },
                {
                    "sent": "There is a fixed baseline which is a.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diameter of the framework and then in each round the booster sends a cost matrix and it receives a weak classifier.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which suffers equal or less cost compared to the baseline.",
                    "label": 0
                },
                {
                    "sent": "As an exam.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can exactly recover the binary boosting framework by appropriately choosing the baseline, the baseline on each example should predict the correct label with probability half plus gamma.",
                    "label": 0
                },
                {
                    "sent": "And predict the incorrect label with probability half minus gamma.",
                    "label": 0
                },
                {
                    "sent": "Now this can be generalized to the multiclass setting by considering more general baselines.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We call Edgewood.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random baseline.",
                    "label": 0
                },
                {
                    "sent": "This Q should be a BM.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an edge of a random baseline.",
                    "label": 0
                },
                {
                    "sent": "Will on each example make the right prediction with slightly more probability than it makes any incorrect prediction?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice that there is a unique edge over random baseline.",
                    "label": 0
                },
                {
                    "sent": "In the binary classification case, but when there are three or more labels, there are infinitely many possible edges were random.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Baselines a weak learning condition which.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Employs an edge over random baseline will be called an edge over random weak learning condition, and so there are infinitely many edge over random week learning conditions these.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conditions have many of the nice properties of the binary weak learning condition, for instance, and every round we require.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performing slightly better than random guys.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turns out that if if a weak classifiers space satisfies any fixed edge over random condition, then it is also boostable, so it is sufficient.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moreover, we have been able to create boosting algorithms for any fixed edge over random condition that can drive down training error efficiently.",
                    "label": 0
                },
                {
                    "sent": "So these these are effective.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, unfortunately these are not necessary because for any fixed edge over random condition, there are theoretical constructions of boostable spaces which fails to satisfy this condition.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These satisfy as a whole.",
                    "label": 0
                },
                {
                    "sent": "They are necessary.",
                    "label": 0
                },
                {
                    "sent": "By which we mean that for any boostable space there is some edge over random condition which that space satisfies and therefore by combine.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring all of them.",
                    "label": 0
                },
                {
                    "sent": "We can create one single minimal week learning can do.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is both necessary and sufficient for boost ability, and this is the right week learning condition in the multiclass setting.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the next turn to algorithms using these conditions.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For any fixed edgeware random condition we have derived boosting algorithms that are optimally efficient in a certain sense.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since.",
                    "label": 0
                },
                {
                    "sent": "In the binary setting, similar a similar algorithm called Boost by majority was derived by Freud.",
                    "label": 0
                },
                {
                    "sent": "So the issue with all these.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Boredoms, despite their optimality, is that they are non adaptive.",
                    "label": 0
                },
                {
                    "sent": "They require advanced knowledge of.",
                    "label": 1
                },
                {
                    "sent": "The parameter gamma, which you probably remember from some of the earlier slides and this makes them impact.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But based on the ideas behind these algorithms, we were able to derive adaptive algorithms which assume the minimum week learning.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addition.",
                    "label": 0
                },
                {
                    "sent": "These algorithms are.",
                    "label": 0
                },
                {
                    "sent": "Based on cheap computational multiplicative updates.",
                    "label": 1
                },
                {
                    "sent": "Computationally cheap multiplicative updates, and sorry very much in the spirit of Adam.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Boost.",
                    "label": 0
                },
                {
                    "sent": "And although they are not optimal, but they're still probably extremely efficient, like they drive down training error exponentially passed with the number of rounds.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a high level description of the adaptive adaptive algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In each row.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only tend to cost matrix receives of.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classifier suffering some with having some edge Delta T.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For random and based on this edge, it assigns a weight to this week classifier and its final prediction will be the weighted majority vote combination of these weak classifiers which is updated and this weekend majority vote is updated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as shown here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is the high level.",
                    "label": 0
                },
                {
                    "sent": "The details of how to compute the edge and the cost.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix in each round is not much more complicated.",
                    "label": 1
                },
                {
                    "sent": "It's given by these simple expressions, but I won't have time to go describe them.",
                    "label": 0
                },
                {
                    "sent": "More detail here, but they are simple.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we are in order to test empirically.",
                    "label": 0
                },
                {
                    "sent": "Tests are adaptive algorithm.",
                    "label": 0
                },
                {
                    "sent": "We compared our method with existing algorithms on benchmark datasets.",
                    "label": 1
                },
                {
                    "sent": "The weak classifiers we used were decision trees and we control their complexity by restricting the size of the decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we wanted to test whether our method, which relies on theoretically minimal assumptions, can in practice also be more effective than other algorithms when used with very weak learners.",
                    "label": 0
                },
                {
                    "sent": "So this and here are the results.",
                    "label": 0
                },
                {
                    "sent": "So the six graphs correspond to six datasets.",
                    "label": 0
                },
                {
                    "sent": "The three curves correspond to three algorithms.",
                    "label": 0
                },
                {
                    "sent": "The red curve is our new method.",
                    "label": 0
                },
                {
                    "sent": "Black Curve is Adaboost M1A direct, the multiclass extension of Adaboost, and the blue curve is Adaboost image, which is based on one against all reductions.",
                    "label": 0
                },
                {
                    "sent": "The X axis is the weak classifier complexity measured by the maximum size of the decision.",
                    "label": 0
                },
                {
                    "sent": "Cheese we've allowed, and the Y axis is the test error after 500 rounds of boosting.",
                    "label": 0
                },
                {
                    "sent": "In each experiment, our algorithm achieves the lowest test error when used with weaker classifiers.",
                    "label": 0
                },
                {
                    "sent": "And to understand this phenomenon better phenomenon better, we examine more carefully what happens when using extremely weak classifiers, which are decision trees with at most 5 nodes.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ncura ARD results.",
                    "label": 0
                },
                {
                    "sent": "The X axis now is the number of rounds of boosting and the Y axis is the test error.",
                    "label": 0
                },
                {
                    "sent": "Of the majority vote combination, each curve therefore depicts how the weighted majority combinations test error falls with the number of rounds of boosting.",
                    "label": 0
                },
                {
                    "sent": "As you can see Adaboost dot MH and an one initially aggressively reduced test error.",
                    "label": 0
                },
                {
                    "sent": "But then they flatten out because their aggressive demands can no longer be satisfied by the extremely weak weak learner.",
                    "label": 0
                },
                {
                    "sent": "And at that point, these algorithms essentially terminate.",
                    "label": 0
                },
                {
                    "sent": "However, our method makes much more gentler, much more gentle demands on the weak classifiers, and are able to drive down error for all 500 rounds.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so before concluding, I would like to talk about what we are currently doing in this.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Object so.",
                    "label": 0
                },
                {
                    "sent": "We've been working on extending this theory for weak classifiers, which do not, which have been which not make single multiclass predictions but multilabel multiclass prediction, and these have been used a lot in practice.",
                    "label": 0
                },
                {
                    "sent": "So this needs to be studied.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another line of research is to explore what happens when even the boostability assumption fails to hold, which is what happens most of the times in practice.",
                    "label": 1
                },
                {
                    "sent": "Can we still show meaningful behavior?",
                    "label": 0
                },
                {
                    "sent": "For instance, consistency results, saying that our algorithm will converge to the optimal combination.",
                    "label": 0
                },
                {
                    "sent": "Even though that optimal combination cannot achieve 0 train.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Never.",
                    "label": 0
                },
                {
                    "sent": "And finally, we would like to understand the minimal assumptions necessary for.",
                    "label": 0
                },
                {
                    "sent": "Boosting algorithms for solving the ranking problem.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, thank you.",
                    "label": 0
                },
                {
                    "sent": "We have some time for questions.",
                    "label": 0
                },
                {
                    "sent": "So let me maybe get started with the question so in the binary setting there is a really nice connection between boost ability and linear separation, so there's going to geometric intuition.",
                    "label": 0
                },
                {
                    "sent": "Do you think such a geometric intuition is possible in the multiclass case?",
                    "label": 0
                },
                {
                    "sent": "Yes, so the results which was necessary and sufficient is based on Minimax theorem, which shows that if the week learning condition is satisfied, then there exists geometric linear separator, so it is very much like the binary case.",
                    "label": 0
                },
                {
                    "sent": "The question so for the binary boosting you can interpret it as minimizing the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "Is there such an interpretation for your multiclass boosting?",
                    "label": 0
                },
                {
                    "sent": "Yes, so Adaboost minimize this exponential loss, although the ideal goal is to minimize 01 error, so the optimal nonadaptive algorithms that I described the minimize 01 error, but the adaptive algorithm with which the experiments were done minimize exponential loss.",
                    "label": 0
                },
                {
                    "sent": "And this loss is the following.",
                    "label": 0
                },
                {
                    "sent": "It is a sum of exponential terms and the let's say the correct label is 1.",
                    "label": 0
                },
                {
                    "sent": "The first term is E to the number of words for label 2 minus number of words for label one plus the second term will be E to the number of votes were labeled 3 minus number of words for the correct label one and so on, so that this exponential loss actually upper bounds error.",
                    "label": 0
                },
                {
                    "sent": "And that is what we try to minimize in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the binary case.",
                    "label": 0
                },
                {
                    "sent": "There is a dual problem so.",
                    "label": 0
                },
                {
                    "sent": "In the distribution domain you minimize, maximize the minimum edge and then in the tool you maximize the minimum margin.",
                    "label": 0
                },
                {
                    "sent": "Do you know what?",
                    "label": 0
                },
                {
                    "sent": "What Max maximize the minimum margin corresponds to in the multiclass case.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so as I said, this is the.",
                    "label": 0
                },
                {
                    "sent": "If so, if you can correctly classify with an edge.",
                    "label": 0
                },
                {
                    "sent": "Then that means there is a combination which gets every example right within the same edge.",
                    "label": 0
                },
                {
                    "sent": "So it is very much the same minimax argument happening here.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}