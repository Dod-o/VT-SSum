{
    "id": "q6kusnfswqmuxh6hsqshz2oupysh276o",
    "title": "Sparse Methods for Under-determined Inverse Problems",
    "info": {
        "author": [
            "R\u00e9mi Gribonval, INRIA Rennes - Bretagne Atlantique"
        ],
        "published": "Oct. 12, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Digital Signal Processing"
        ]
    },
    "url": "http://videolectures.net/mlss2011_gribonval_sparsemethods/",
    "segmentation": [
        [
            "The other this tutorial is organized in three sessions an in this first session what I want to do is to give you a general digital picture, give you another view, and the next two sessions will be getting in more details in some more specific topics.",
            "So for this first session of today, I'm going to introduce the concept of sparsity, why it is important in signal processing and machine learning.",
            "Where it comes from and here I would just like to mention that well, I come from a single processing background an I understand the decision machine learning summer school.",
            "So sometimes they are well they are concept that have been developed in in both communities.",
            "When necessary I'll try to help you identify the names that may be slightly different from between machine learning and signal processing, but in the main.",
            "The flow of what I will be presenting an I apologize for that.",
            "I will probably mostly use the single processing terminology."
        ],
        [
            "So today.",
            "I want to start by.",
            "The notion of sparsity and why and how it will naturally related to the problem of compressing high dimensional data in the second step, I will show that it's not only useful a useful concept for our compression, it's also something that has emerged more recently.",
            "It's a concept that is useful to solve problems, inverse problems where a priority, a notion of sparsity.",
            "We could have seen irrelevant.",
            "Then I will give you a global picture of the main algorithms that I used to that that have been developed to exploit sparsity of high dimensional data, and I will conclude with a brief discussion of the nature of the theory backing the use of this.",
            "These algorithms and its application to compress sensing a keyword that you may have.",
            "Here, here in the."
        ],
        [
            "So let's begin by the beginning.",
            "The notion of sparsity."
        ],
        [
            "So well, and here you will.",
            "This will sound like something you've heard a zillion times.",
            "Well, today we're dealing with massive data data and the data.",
            "The numbers I'm giving here are not so massive, but just give you an example.",
            "If you simply take one second of CD quality stereo audio, it's already more than one.",
            "Megabits and if you take like a standard, the picture out of a digital digital camera, it's already 240 megabits, so dealing with large data collections of large data well, we know that we have limited energy resources and computing resources in more general.",
            "So we need concise or presentations of these data to handle them both, or transmitting them, storing them, but also.",
            "Simply extract the information and manipulated."
        ],
        [
            "So as the standard tools have been established to manipulate different types of data, and for example, if you if you manipulate audio audio is a time can be represented as a time series.",
            "So high dimensional data mean time series with many samples.",
            "If you represent these data in the rules in roll form, every sample matters.",
            "If you remove a few samples that the nature of the data is corrupted, but it is known that if you go in the time frequency domain and this is a time frequency representation with the time here frequency, he ran color codes the intensity at a given time and frequency.",
            "You generally get this type of feature where many areas are black and this means that there's little energy and just a few time frequency points are associated with.",
            "Red or yellow?",
            "High energy content.",
            "This means there are many zeros in this transform representation of audio.",
            "If you replace audio by images here a picture if you analyze it with the wavelet transform with a different color code, you observe that there's Gray at most places and in few places there is white or black, or which means there's more intensity.",
            "So these data when you transform them in the appropriate domain can be represented by.",
            "Set off by vector by sets of coefficients that have few significant coefficients, and this fact turns out to be very useful to compress this data because.",
            "Because they can be very well approximating by setting to zero exactly these values that have that have."
        ],
        [
            "Small significance, so mathematically this means that the data that can, or the signals and images that can be considered as high dimensional vectors.",
            "So each pixel value is a coefficient in this vector.",
            "These high dimensional vectors that can be well approximated as linear combinations of few elements from some set of basic signals or basic images.",
            "So for example for sound.",
            "This means that we can take sounds we can take the short line free transform.",
            "There are many zeros, so many many small coefficients.",
            "We set them to zero and there's an inverse short time for it transform that reconstructs the data.",
            "So if we don't do any, change any settings.",
            "If we don't set anything to zero, we can reconstruct the data exactly.",
            "If we set to 0, the small coefficients with it's only a small perturbation and we reconstruct almost exactly the data, and this inverse transform.",
            "Neither associated to such linear reconstruction, so the data is going to be a combination of few few elements, few time frequency atoms.",
            "It's the same for images for images such as the one I showed, they would be expressed as good as they would be well approximated as a linear combination of few wavelets.",
            "So the fact that there are few elements and that this data is well approximated will be measured by the error between the original data and this approximated reconstruction.",
            "Hopefully we're getting a small error and the fact that we can reconstruct it with few elements is measured by the.",
            "So called L zero norm of the coefficient vector.",
            "So this just counts the number of nonzero elements in this vector.",
            "So I'm sure you have all seen this notion of as your own, but this is the beginning.",
            "So this is the the way the the most standard way of measuring sparsity, counting the number of nonzero elements in."
        ],
        [
            "This representation.",
            "So overall, and this is a type of a type of feature that I will do several times in this in this talk, what we will represent data as vectors, so as column vectors.",
            "So the original data is a high dimensional column vector and I filled it with color to indicate that most coefficients are non zero and the fact that there's a sparse representation in some domain means that when we choose the right.",
            "The right matrix five that combines the atoms from which it can be built.",
            "It is possible to find a coefficient vector to represent this this data vector with only a few significant coefficients.",
            "So this is this is a property that is that has been observed on many signals.",
            "So audio signals with short time free transform and time frequency at homes here in the dictionary.",
            "For images this would be with wavelets.",
            "More generally, the fact that we can finding such an expression is only possible if we choose appropriately the dictionary.",
            "So maybe I forgot to say this matrix is called a dictionary because.",
            "Each element gives like a word and combining a few words you can get a representation of the of the whole signal.",
            "So when the dictionary is well chosen.",
            "You get this type of sparse approximation of the data.",
            "I mean there you get the existence of such a representation.",
            "If you don't choose the dictionary appropriately well.",
            "This raises other questions and I will mention them at the end of this lecture.",
            "How can we choose a dictionary?",
            "So now let's see how this impacts a potential compression of data vectors.",
            "So the initial vector is has entries and Annette Natural way of storing it would be by storing N floats.",
            "But if you look at the vector of the sparse vector on the right hand side, there are only there's a much smaller number of nonzero entries.",
            "So to code this vector you need to code K floats, let's say.",
            "So if K is much smaller than NA preore you get you get some compression.",
            "Unfortunately, the story doesn't end here.",
            "If you just go to the value of these coefficients.",
            "The vector is not coded yet because you still need to know where these coefficients are and this is actually what costs you most, because if you don't have any priority idea of where this K coefficients among N allocated, there are K choose well an choose K possibilities and the number of bits you know you need to encode.",
            "This is roughly speaking this this log which can be well approximated as.",
            "K The number of non zero limits times log N / K which comes from the combinatorial nature nature of this set of K elements.",
            "So, provided that you know the dictionary that you know, transform a domain where your data is case bounce or well approximated as K sparse.",
            "You can hope to get coding of this data with only K log, N / K bits.",
            "I mean this is order of magnitude."
        ],
        [
            "So this this notion of sparsity, I hope to have conveyed the idea that it's very natural in the coding context, 'cause you basically you want to find the representation with few nonzero elements, because the fewer nonzero limits the most compressed your representation.",
            "What I will try to show you now is that in a completely different context, the fact that some data is sparse can help solve the problem and in the.",
            "Inverse and inverse problem."
        ],
        [
            "So what is an inverse problem?",
            "This is a problem where you don't observe the data that you wish to manipulate directly, but only indirectly.",
            "So there's a.",
            "There's some level of missing information that you would like to create compensates, so here is a prototype example which is an example of inpainting problem in.",
            "In painting you can think that there's an image that is you wish to manipulate too.",
            "Alter auto reconstruct, but some pieces of this image are missing, and here these parrots you would like to see the whole parrots.",
            "But there are bars in front of the power.",
            "So you can think of this image as the full image of the Parrot, but only observed on where this mask is black.",
            "So you only observe this image at certain locations.",
            "The rest is just not observed.",
            "So how can you reconstruct the original image from the partially observed one apriori?",
            "You have a problem.",
            "Why do you have a problem?",
            "Because the original image has, let's say, a million pixels an you perhaps observed only 800,000 pixels.",
            "So somehow you have 800,000 OPS observed.",
            "You have eight, you you need to recover vector in Minion dimension from only 800,000 equations.",
            "You are missing equations.",
            "So you have a problem you cannot solve.",
            "So to solve it well, or two to estimate the solution, you need to introduce some prior knowledge on your image.",
            "You need to introduce some some knowledge that will help you figure out among the many possible solutions compatible with what you've observed.",
            "Which one is most likely and will, I mean, will fit your prior and fit your observation.",
            "And if you do so and this can be done using sparse sparse representations in.",
            "In wavelet domain here you can reconstruct an image that visually looks like the parrot, free from his bar.",
            "So if you look carefully close enough where the where the bars where, you will probably notice that.",
            "Well there are some for the areas, but globally this looks like you've solved.",
            "You've extrapolated the parrot where you didn't observe and this has been done using a sparse sparse representation and an algorithm to exploit sparsity."
        ],
        [
            "Another example is audio separation.",
            "So before giving the example, just let me just.",
            "Explain briefly what is source separation?",
            "Well, it's not easy today because you're all very quiet, so you're not making much noise.",
            "But if you were talking why I'm trying to convey this message about sparsity, UOL, each of you would be hearing not only what I'm saying, but combination of the sound coming from many people, and probably the camera there in the back would be capturing this mixture.",
            "And well, if in the end you want to focus on what I'm saying, there's no need to identify and remove the sound from the the.",
            "The disturbing disturbing sources well.",
            "If there is no not so many, well, even in a different context, you may be interested in all sources.",
            "Well, what I was describing was there's the important message, and there's the noise.",
            "But here this would be well if you listen to a CD.",
            "If you listen to music, or if you listen to MP3, well there are different instruments playing together an you may want to focus on separately on each instrument.",
            "For example, you may want to.",
            "Listen to your favorite guitar player to know what he's playing and reproduce it.",
            "Or you may want to remove it.",
            "This sound from the from the recording so that you can try to replace him.",
            "So here if you take a recording and let's let's listen to this recording.",
            "So you listen to it once.",
            "I didn't tell you what to listen to or specifically.",
            "Now I can tell you or try to tell me, can you try to tell me how many instruments?",
            "Are playing on this recording.",
            "So 2, three, OK, let's make a pool.",
            "Who are things?",
            "There are three instruments.",
            "OK, there's a number of hands who think there are two instruments.",
            "Now roughly equal, who thinks there's one instrument?",
            "Somebody in the back of zero instruments.",
            "OK, at least there's activity there.",
            "Sound activities, so the zero is is out, but the number of instrument is, well, is something that human beings.",
            "Well with only one listening to the to the to the sound without being your won't have may have difficulty solving well actually well, we'll see now how many instruments they are.",
            "I mean I'll detected by the source separation algorithm that I've run on it.",
            "There are two the bass and guitar, but you'll see when you listen to them that those who raise their hands for three instruments are not completely well, didn't may have heard something that are present in there.",
            "So if you run a certain algorithm that exploits sparsity of the of these sounds, you figure you can figure out that there are two instruments and.",
            "Extract the sound of these instruments.",
            "So this was the base, but you may have heard there there were some noise actually, as far as I know this recording was done live in a bar or in a restaurant I think, and some people are eating and the Forks and knives and which may sound a bit like like drums like a third instrument and the guitar now.",
            "So this is just to show you are well the results.",
            "And what I can tell you this is done using sparsity.",
            "So OK, maybe so far this is just magic you're.",
            "So let's look a bit more closely how sparsity is exploited to deal with."
        ],
        [
            "This this problem?",
            "Well, but before let me just put it in context so these two examples that I showed there are examples of inverse problems.",
            "Inverse problems arise when we have when we observe indirectly some data and this observation comes from a process that has lost some information.",
            "And well here I'm focusing on linear inverse problems.",
            "The observation like blurring or mixing different sources are including.",
            "Something in an image this this loses some information and yet we wish to reconstruct the original data, so we have fewer equations than unknowns and would like to solve this problem without prior knowledge.",
            "It's impossible, and sparsity is precisely a type of knowledge that says this data.",
            "This data is high dimensional, but you know there's few parameters to describe it, so if they are sufficiently few parameters, maybe.",
            "Maybe this matches well the number of observations that you have, and maybe you can exploit this small number of observation and is even smaller number of parameters to actually solve your inverse problem.",
            "And this is this is what happens.",
            "Not only it's possible, it's in some cases it's a well posed problem, but also there are algorithms that's probably performed this reconstruction."
        ],
        [
            "So for this blue and blind source separation problem that I just mentioned, what is happening?",
            "And this is this?",
            "Is this is a toy model that's valid for studio recordings.",
            "In reverberant rooms like this one where the reflections on the walls and it's a bit more complex, but this time model is valid when you used studio mixing.",
            "So what happens you have?",
            "Recall with two microphones, just like your two years, so on the right channel you have some time series that's recorded and simultaneously on the left channel there's another time series that's recorded.",
            "And let's say this is a recording that comes from the mixture of resources.",
            "If you know nothing about the sources, well, too bad.",
            "You don't know how you don't have enough equations to solve the problem, but let's look at very simplified case.",
            "Suppose that.",
            "I'm speaking then you're speaking, then you're speaking.",
            "So the sources are not active simultaneously.",
            "What happens is that at a given time.",
            "There is only one source that is active.",
            "You don't know which one I preordered.",
            "You don't know where the source is, but you know that at a given time there can only be one active source.",
            "So what happens, I suppose suppose, possibly speaking before before wine.",
            "Anne Francoise is on my right, so the sound that I receive on my right ear is stronger than the sound I receive on my left ear and the ratio the amplitude difference between what I receive on both ears is constant is characterized by where is located.",
            "This is what I can exploit and here.",
            "If you look at the time instance where only the first source is active and you look at the value observed on the left channel versus the value of certain right channel is a line on some line that specific of the difference of amplitude between the I mean of the ratio of completed between the two channels for this location.",
            "So in this setting if I display the the amplitude on the left and on the right, well for all these time instance it's on this line for this time in sense, so it's on this other line, etc.",
            "So from this scatter plot.",
            "You can perform some clustering, identify well all the active sources.",
            "What are the directions of the active sources an?",
            "What are the time instance where each source is active?",
            "So of course this is.",
            "This is a bit and idealistic case.",
            "In practice, you're not very often interested in separating sound sources that are not active at the same time.",
            "And the example I showed was with sources that are active simultaneously."
        ],
        [
            "So if they are all active simultaneously and you do exactly what I showed, the scatterplot is not so nice and well for sure, I wouldn't be able to separate the sources and locate them from this scatter plot.",
            "I don't know if maybe some some modern clustering techniques could do something, but there's a simple trick that drastically changes the scatter plots.",
            "So here in time, when you look at the times, here is the source is there.",
            "All active at all times."
        ],
        [
            "But if you go to the time frequency domain and I showed you.",
            "Sometime frequency representation at the beginning of this lecture.",
            "If you go to the time frequency domain so your represents the time frequency representation of the left channel and the right channel.",
            "And as I told you, for each source.",
            "Is not active most of the time at most time frequency points it's only active at few times.",
            "Frequency points.",
            "So what happens is that at a given time frequency point, most likely there's no source active, and sometimes there's one source active.",
            "In very few cases, there are two active sources.",
            "So in most of the cases there's zero or one active source at a given time frequency point an using this property you can do the same type of scatter plot which I displayed here, and I've already called it because this scatter plot again displays these kind of lines.",
            "It's possible to cluster it, possible to estimate the source locations and to separate the sources."
        ],
        [
            "So the overall message here is that in a context that is quite different from compression, sparsity can be exploited to solve a problem that apriori is imposed.",
            "So it is well known that, well, it's a well known result from Algebra 1, one that if you have fewer equations than unknowns there, you cannot see that there's an infinite number of solutions to your linear system.",
            "And in particular, if you have two vectors that provide the same observation, you cannot conclude that these vectors are identical.",
            "But what is novel and it's now been known for about 10 years, is that if?",
            "Your two vectors are sparse.",
            "Then an match.",
            "Provide the same observation if they are sufficiently sparse, then they must be equal, so there's a under the sparsity assumption.",
            "Your inverse problem becomes well posed.",
            "There's a unique solution.",
            "So it will post.",
            "That's good, but it could still be combinatorial or intractable to find the solution.",
            "The second good news is that, again, if it is sufficiently sparse, your unique solution is not only unique, but it can be identified with some practical algorithms.",
            "And then depending on the.",
            "The strengths of the of the properties of the of this data that you're assuming or that you're entitled to assume well, you'll be entitled to use different types of algorithms to recover it and this.",
            "This will be discussed in more details in tomorrow's lecture.",
            "So the main algorithm that will study our LP minimization with particular focus on L1 minimization, which is a convex problem and greedy algorithms, and in particular the so-called matching pursuit algorithm."
        ],
        [
            "So, um.",
            "Today, well in this lecture I just want to give you a brief overview of these algorithms so that you get first level of.",
            "Reading of this, the what tools are involved in sparse sparse?"
        ],
        [
            "Presentations, and I think this is it is time to spend some time in 220 introduce.",
            "It's not only the single processing vocabulary, but some connection with machine learning.",
            "So the object I've chosen to use basic linear algebra notation all along this this talk.",
            "So what is observed?",
            "The observation, the data, the absurd vector, the right hand side the new equation X = B is well, it can be can be something that is measured on the sensor over can be the multi channel signal that we listen to can be the mask image inpainting.",
            "It is related to some unknown and which can be the unknown.",
            "X is either a sparse representation of.",
            "Yeah, of a vector of the observation or could be the unknown sources that you want to recover.",
            "It is so both are related through linear system, that is that we considered as known and this linear system, so it can be the dictionary that used to model to relate your vector and which is false representation can be the mixing matrix that relates the observed mixtures with the unknown sources can also be what is called the sensing system in compressed sensing.",
            "So when X is known a, there's a forward model that predicts what is observed, but the whole problem is to go backward."
        ],
        [
            "So in machine learning, I think it's somewhat standard.",
            "Notation would rather be Y equals exhibit exhibit A.",
            "So X plays the role of the mixing matrix or the dictionary XI think is the design matrix related to some some problems where you observe a number of wise and number of axes and you would like to predict how?",
            "Why is can be predicted from from these axes?",
            "So better will be the regression parameter that tells you how important are each of these.",
            "Each columns of the design matrix, so I apologize if this connection to machine learning is a bit unclear, and if you want to discuss this further, I welcome you to come and check during the post."
        ],
        [
            "OK.",
            "So.",
            "From now on I'm going to use BA XI.",
            "Will try to stick with these names occasionally.",
            "Will use dictionary mixing matrix and so on when it's hopefully appropriate.",
            "So what is the inverse problem that we are considering?",
            "It's a problem where there are things we know there are things we don't know.",
            "Things we know are the matrix A, so it's a matrix that lowers dimension that projects an input vector.",
            "Of high dimension N and I provides a low dimensional vector B of so smaller dimension an what we would like is to recover.",
            "So the input vector X.",
            "We know we need to use some some properties and I've told you that if it is sparse we can hope to recover it.",
            "So we'll be looking for some somehow the sparsest solution to the system or the passes approximate solution to the system.",
            "For this, well, there are many ways of formulating the problem which are.",
            "Not fully equivalent, but are often interchanged in this in this game.",
            "So the first one consists in simply saying that we're looking for this partial solution and there are approximation constraints.",
            "The second one, consistent thing that we're looking for the best approximation given the sparsity constraints.",
            "And then there's also, there are also cases where we just want to find the solution that satisfies both a an approximation constraint.",
            "And the sparsity constraints."
        ],
        [
            "So this is the kind of idealized problem that we would like to solve.",
            "And I think that's to go further.",
            "It's important to develop some kind of geometric view of what is going on.",
            "So.",
            "The vector that we are looking for is X vector.",
            "It's a sparse vector, so it has many zero entries.",
            "What does it mean?",
            "What does such a vector look like?",
            "So I mean, you can write this vector, there are zeros and non zero entries.",
            "This is not very geometric, but let's look in where the highest dimension I can picture.",
            "In my mind 3D.",
            "Ants are let's look at vectors that have one non zero and three.",
            "What is a vector that has one non zero entry?",
            "It's a vector that align with one of the coordinate axes, so it's something on one of these black lines vector that has two non zero entries is a vector that's a line on one of the of these planes defined by the coordinator access, so I'm sorry if this is a bit slow, but maybe, but this geometry viewpoints.",
            "Will turn out to be useful in understanding the behavior of this algorithm and the definition of this algorithm.",
            "OK, so this is the nature of the vectors that we want to recover, but remember we're not observing directly these vectors.",
            "We observing their low dimensional projection, so we are observing these vectors multiply by this matrix A.",
            "So there's this collection Sigma K of all vectors with at most K nonzero entries, which is a collection of K dimensional subspaces, and there's N choose K of them.",
            "When we project it down to the observation domain, the domain of B.",
            "It's also a union of the same number of subspaces.",
            "But they will be tilted, so we're it's.",
            "It's a bit harder to want to display, so they.",
            "Which what you will observe is will live in this union of combinatorially many low dimensional subspaces.",
            "Yes.",
            "Cute.",
            "I think that's the binomial is the is the number of K dimensional subspaces, but this is the number of K dimensional.",
            "Yeah this is you have to choose.",
            "Where I mean each of space corresponds to a set of K. Non zero entries.",
            "And I think you have if you count the ways to choose this space is that is the way of the number of ways you have to.",
            "You can choose the location of these K nonzero entries among N on their entries.",
            "But you're welcome to discuss it further.",
            "Maybe there's something I missed here.",
            "So where's it could be natural number of such spaces?",
            "And so, and again, when when you go to the projected lower dimensional subspace space M there still potentially a combinatorial number of spaces and our problem of ideal sparse approximation consists in having a vector B.",
            "And finding the best approximation to this vector be from this Union of low dimensional subspaces.",
            "So priore to combinatorial problem because the best naive way we can solve this problem is by going through all possible subspaces, computing the distance, finding the minimum distance, and this gives the solution.",
            "So priority, it's combinatorial search and it can be shown that in a formalized way, this is actually a intractable problem.",
            "It's an NP hard.",
            "It's even an NP complete problem.",
            "So much we do.",
            "Should we just resign and do go go somewhere else?",
            "Of course not this otherwise I wouldn't be giving this."
        ],
        [
            "Toyota, so Fortunately there are cases where the problem, even though in general the problem is intractable, there are very difficult instances of the problem where it turns out that under specific assumptions on the Matrix A and the sparsity of X, it is possible to find it with efficient algorithms.",
            "So now let's talk."
        ],
        [
            "Algorithms?",
            "Well, these algorithms again they will generally try to make a compromise between two quantities.",
            "On the one hand, they want to minimize the approximation error, so they want to have the best approximation quality.",
            "Find an X that gives a good approximation quality.",
            "On the other hand, they like to find an X that is very sparse.",
            "So we've already seen that the sparsity of X can be measured with the O Norma the number of non zero entries, and it turns out that many algorithm exploits, so called relaxed versions of this sparsity, measures the so called LP norms, especially for P between zero and one.",
            "Which expression I've recalled here, you're probably familiar with with these LP norms anyway.",
            "So the goal is to make a compromise between these two.",
            "Many algorithms.",
            "Address tradeoff between these."
        ],
        [
            "Quantities.",
            "So at this stage I think, well, maybe you you feel familiar or not with LP norms, but maybe just a quick reminder on norms, maybe, maybe worth it.",
            "So what is the norm in general A?",
            "It's a it's a quantity that measures the size of vector and that satisfies three basic properties.",
            "I mean, who is not familiar with norms?",
            "OK so I can go really quickly with this.",
            "The basic properties are the zero vector has zero norm that if the norm of a vector multiplied by a scalar is the normal to play by the absolute value of this camera and the triangle inequality.",
            "So all the standard all the LP norms for P between one and Infinity are normal.",
            "The satisfied is properties.",
            "However, the case that we are going to consider prominently when we're looking at sparsity is the case for P between zero and one.",
            "And for this case, the name Norm is a bit.",
            "The well is not appropriate, but will use it anyway just for brevity.",
            "So these are more formally called quasi norms.",
            "So because they are, they do not satisfy the triangle inequality these LP norms for peace more than one they instead of the triangle inequality the triangle inequality says that the norm of the sum of two vectors is more than the sum of the norms.",
            "Well they satisfy the quasi triangle inequality and four P gnome.",
            "The most convenient way of writing that Coke was a triangle inequality is this one.",
            "When you take the P norm to the peace power for peace more than one.",
            "When you apply this to the sum of two vectors, you get something that's more than the sum of the P norm to the peace tower.",
            "This is an essential property of DSP norms, and there are many computations that can be done with the L1 norm that carry two LP norms.",
            "Piece more than one.",
            "Because of this basic property.",
            "So so these four P strictly bigger than zero.",
            "We have quasi norms and four P = 0.",
            "Oh well, it's not even a quasi normal, but let's call them let's call it up sudo gnome if we wish because if you multiply a vector by scalar you don't change it L zero norm unless this color is 0.",
            "So the only thing that remains the only property is this quasi quasi triangle inequality for the L0 norm.",
            "And again it's something that is going to be very useful for the.",
            "For establishing inequalities and properties of sparse recovery.",
            "Maybe I can make your shot pose, here is just.",
            "To let you capture what this what this means, what what is this question?",
            "This quasi triangle inequality for the L0 norm?",
            "It just captures the fact that when you take a two vectors, one vector that has a.",
            "Key non zero coordinates and you add it to a vector that has L nonzero coordinates.",
            "Well, you cannot create nonzero coordinates at locations where there was no nonzero coordinates, so the number of nonzero coordinates of the sum of the vectors is just that most the sum of the number of their coordinates."
        ],
        [
            "OK, so let's go back to the problem.",
            "We want to to the to solve and the algorithm we want to design to solve this problem.",
            "So we want to achieve a tradeoff between approximation quality and sparsity so.",
            "There are three ways to address it, either by trying to get the best approximation with a given sparsity budget if you wish.",
            "You can try to find the sparsest solution with a given error budgets or try to find the tradeoff between both and so standard way to achieve this.",
            "Well to look for this tradeoff is using regularization, regularised specification.",
            "So here.",
            "If, when when the Pinot here is.",
            "And Norma.",
            "For example, deal one normal.",
            "These three formulations are completely exchangeable, you can.",
            "Because of convexity.",
            "The solution of this problem with a certain value of two.",
            "Can be expressed as a solution of this problem with an appropriate choice of epsilon and can also be expressed as the solution of this problem with an appropriate value of Lambda and vice versa.",
            "Things that will be different when the norm is with the smaller than smaller than one.",
            "Then the problems are not necessarily equivalent.",
            "There are solutions of this problem that cannot be achieved that cannot be reached.",
            "Solutions of these other problems.",
            "This is.",
            "Something I wanted to mention in passing, because very often I mean there the equivalence between these problems is known and he's done routinely for convex optimization, but in the cases that we are going to consider is not, well, routine doesn't necessarily work."
        ],
        [
            "So.",
            "The picture that you may have seen before, it's why.",
            "Why I told you we want to use LP gnomes with P smaller than one?",
            "Two perform to compute sparse solutions to a linear system of equation.",
            "So here's the picture.",
            "Let's look on the on the left.",
            "So here we are looking at the problem with, well, two unknowns and one observation, but let's think it's a general description of a problem with fewer equations than unknowns.",
            "There is an affine set of solutions.",
            "The set of solutions of the problem.",
            "It's a particular solution plus an elements of the null space of the Matrix A.",
            "So this is the set of possible solutions an if we look for the minimum L2 norm solution.",
            "Among all these solutions, it means we're looking for this point.",
            "'cause if so, to see why this is the minimum L2 norm solution.",
            "Think of.",
            "Little balls of increasing radius, so can you find a solution on the blue line that has a very small L2 norm?",
            "Can you find a solution on that?",
            "Has a little to normal smaller than this L2 norm?",
            "No, because there's no intersection.",
            "So when you blow this bowl.",
            "At some point, it's the two the two match an you get unique intersection, which is the minimum two norm solution and what you observe here is that well, this L2 norm solution.",
            "It has two nonzero entries.",
            "In the middle, do the same thing with the L1 norm, so the minimum and one norm solution is the intersection between this line and the smallest L1 bowl.",
            "Here the smaller square that.",
            "With the that intersects this and here you see that.",
            "It's in the corner of the of their one goal you get, so you get a solution with one nonzero entry, not two and four piece more than one.",
            "You get nonconvex bulls, but the same phenomenon.",
            "There are corners in the ball, and so the minimum LP norm solution is sparse.",
            "So here, sparsity means that well, there are zeros.",
            "There are many zeros in the solution.",
            "It doesn't mean that it's the sparsest.",
            "It means that.",
            "It's a sparse solution."
        ],
        [
            "So these Lt minimization principles.",
            "Well, they can be written for a variety of LP norms and for variety of in a variety of former and they lead to different types of of pros and cons.",
            "So first of all, if you look if you trying to minimize this type of objective function for P. Let's say above one.",
            "What is nice is that you get a convex optimization problem.",
            "So convexity and you'll see that with the in the cost of living vandeberg has many nice properties.",
            "There are efficient algorithms to solve the problem.",
            "There's a unique.",
            "Well, there's a unique convex set of solution, and very often a unique global minimum to your problem.",
            "So many things are very well defined in this regime.",
            "The problem is that when P is strictly above one, you are not getting a spot solution in general.",
            "So if we're looking for sparsity.",
            "Huh, maybe it's better to have a smaller value of P. However, if you go to pee smaller than one, you get a nonconvex problem, so the global minimizer is going to be sparse.",
            "But the problem is not convex.",
            "There are local minima everywhere, so it's highly the algorithms that you can derive to solve it well will necessarily depend on how you initialize them so that which raises several several issues.",
            "In practice, this can inspire a number of.",
            "Rythms but this raises difficulties and the most extreme cases when you take P = 0 as we've seen solving the problem for P = 0 in general is combinatorial and so very difficult.",
            "So this is the reason why the most considered cases the case of the minimization with their one.",
            "Now because it combines the best of both world, it's specifying and convex.",
            "So with the ability of having algorithms of bounded complexity.",
            "That provide a sparse solution.",
            "So I mentioned here a number of persons that contributed to establishing these.",
            "These algorithms would like to focus in particular on that one minimization it was introduced.",
            "In machine learning by tips, Yanni in 1996 and don't know, and his coauthors introduced it under the name Basis, pursuit in signal processing.",
            "But in machine learning, you're probably more familiar with the lawsuit, the name of the zoo.",
            "The the at at the time when it was introduced, it was essentially exploiting the convexity of the objective function too.",
            "This convexity was exploited together with generic algorithm coming from linear programming or quadratic programming.",
            "To solve this problem.",
            "In this course, we'll see that they are now more specific algorithms that are fully targeted to this type of.",
            "At one minimization problem that have more specific convergence properties, and that are more efficient than the standard linear programming.",
            "Generic linear programming solutions."
        ],
        [
            "So this was the so another view of what type of principles can be used to.",
            "To find this path solution on the basis of a criterion that you want to minimize.",
            "So minimizing this criterion that combines approximation with the sparsity penalty.",
            "Leads to a number of problems, optimization problems and then can fit it to the optimization community to try to find an algorithm that solves your optimization problem.",
            "That's one major approach to finding solutions.",
            "There's another set of approaches that are more directly algorithmic.",
            "They do not rely on a global on on a cost function that you wish to minimize, but somehow in the relay on a more geometric.",
            "Viewpoint and more algorithmic directly algorithmic perspective."
        ],
        [
            "And these are the so-called greedy algorithms.",
            "So these greedy algorithms?",
            "Well, they have many interpretations, but one way to look at them is to look at the optimization problems we're considering in a case which is very well behaved so.",
            "The problem that we're going to look at is a problem where we wish to find the best approximation.",
            "B -- A X India to Norma and their sparsity constraints.",
            "So X should have at most K nonzero entries an we're going to assume that the Matrix A.",
            "Is often normal.",
            "The fact that this matrix is also normal means that the.",
            "Means that the norm of a X or the norm of a U for general U is the same as the norm of you when you measure in the two norm.",
            "So it means that this left hand side here that you want to minimize is just the same as the norm of A transpose B -- X.",
            "So already somehow the A is completely moved.",
            "You can take your data B.",
            "Multiply by a transpose and the only thing that remains to be done is to understand which vector X that's case parse is closest to a transpose X.",
            "Now, if you write this this problem, you can observe that it can be written separated coordinate by coordinate.",
            "So you want to find the vector X so that the L2 norm here, which is the sum of all coordinates of X of these squared.",
            "Called Innate, you want to minimize this under the constraint that Axis case bars.",
            "So what happens?",
            "Well, if you choose.",
            "If you put 207 coordinate.",
            "Then you get an error.",
            "That's the magnitude of this inner product.",
            "If you don't put it to zero, well, then since the magnitude of the coefficient doesn't matter in your sparsity constraint, you'd better choose it to minimize this so it will be equal.",
            "It will set this error term A to 0.",
            "So what you see is that for each coordinate, either you set it to the value of N transpose B or to 0.",
            "So in other words, what you have to do is just to allocate where the zeros are, and the best way to do it is to minimize discussed and this can be done by simply computing all these coordinates, sorting them in decreasing order, keeping the K largest or a set of K largest if there are.",
            "If there is a time and these give you the optimal solution.",
            "So what you can see is that here whether this optimization problem was.",
            "Combinatorial in general, here under the assumption that the Matrix A is orthogonal, this becomes a quite simple problem.",
            "Compute coefficients in the basis A transpose, solve them.",
            "Keep the K largest and you're done.",
            "Another way of uh and algorithmic, another algorithmic way of looking at this procedure, is to simply say that you compute the correlation between your B vector an all the columns of your matrix A.",
            "You find the largest one.",
            "You set it to 0.",
            "You find the second largest one you set it to zero, etc, until you found K called K-90 coordinates.",
            "Well, this is precisely what's the motivation for.",
            "Behind that, the definition of greedy algorithms."
        ],
        [
            "So the greedy algorithms and here and describing matching pursuits they will work with more general.",
            "A that we work in the following way so they are given B the vector that here they want to approximate.",
            "And they are given the matrix the matrix A.",
            "And the first thing they do is they compute well they will update the residual.",
            "But the first thing they do is they take their residual which at the beginning it's just the vector B.",
            "They compute the inner product between these residual an each column of the Matrix A.",
            "They find the largest one.",
            "And then they do the project it out.",
            "They remove the contribution of this column of the matrix from the from the residual to get the next residual.",
            "Once this is done, they iterate so.",
            "Every at every step they need to find the most correlated column of the matrix and then remove it from the residual to get the next best one and stop when they've reached the number of non zero.",
            "The number of countries that design this is what is called matching pursuits."
        ],
        [
            "So this so this was the the definition of matching pursuits was valid for the normal matrix is valid for an arbitrary matrix, we just just do these steps, find the best correlated Atom, remove it from the residual, find the best correlated to the new residual, etc.",
            "So this has been introduced under the name matching pursuits by Stephen Miller and Defense.",
            "You're processing, but this is also known as projection pursuits that was defined, I think, by Friedman and Schoesler.",
            "It's also known as under the name clean in astronomy, and I'm sure it has been invented several, many time.",
            "Many times under other names.",
            "What are some of the important properties of this algorithm?",
            "Is that well, every time it removes a column from the residual?",
            "Well, it removes it in a way that preserves from all the energy of the residual.",
            "So the energy is there to Norma and at a given time you have seen.",
            "So you decompose the residual into the next residual and the projection on the selected Atom.",
            "By Pythagoras Theorem A you preserve this L2 norm.",
            "Azar"
        ],
        [
            "So we'll see more on the properties of this algorithm on in the next session, but now let me give you a more global overview of the algorithm that we've discussed so far.",
            "So there are essentially two categories.",
            "One category of algorithm is based on a principle where you want to optimize.",
            "Global cost function.",
            "So this is this is a principle.",
            "It needs to be turned into a practical algorithm.",
            "And the principal A depends.",
            "Well can be true as several tuning parameters, so there's a parameter.",
            "This regularization factor that choose the tradeoff that you're achieving between approximation and sparsity.",
            "If you choose a large value of this penalty factor Lambda, you're forcing to have a very sparse solution at the price of perhaps a very crude approximation, an when this parameter Lambda goes to zero, you're essentially forcing your solution to be exact in terms of approximation, so it will be less sparse.",
            "In the limit where Lambda goes to 0.",
            "The optimum corresponds to minimizing this penama and their quality constraints.",
            "On the other side, you have the family of greedy algorithms, so they are not based on the global optimization principle.",
            "There based on this iterative procedure, so they are more directly defined in terms of an algorithm, and this algorithm is based on computing iteratively some residuals.",
            "So.",
            "Your little by little, finding the columns of a that must take part in your sparse representation, you're incorporating them progressively until you've reached the number of atoms or the number of columns that are designed.",
            "So the desired sparsity level, so here.",
            "The tradeoff between sparsity an approximation is tuned differently tuned by the stopping criterion for your algorithm.",
            "It is tuned by choosing how many nonzero entries you want, or by tuning the approximation error that you wish to reach.",
            "Now all these algorithms have several valiance, so regarding global optimization, the violence on in particular determined by what is the choice of the sparsity measure, but also, since these are only.",
            "Optimization principles the precise are the output of an algorithm is completely determined by.",
            "I would say that the guts of the algorithm that the precise optimization technique that you're implementing to attempt to minimize this discuss function, and this can have a huge effect on the results.",
            "In particular, when you optimization the initialization of your algorithm is very.",
            "Has a huge impact on the other side.",
            "Therefore greedy algorithms.",
            "What will have an impact is the way the violence can choose to select such or such column of the matrix.",
            "There are variant values, accelerating techniques.",
            "There are techniques where you you're not necessarily looking for the best correlated at home, because this is very costly, so you might want to select.",
            "A column of the matrix that is well correlated, if not the best.",
            "And there may also be technically cases where you are too fast to make things faster.",
            "You will select several nonzero coordinates at a time, so these are called stagewise algorithms.",
            "Last difference source of variation is that the way you update the residual is not always the one that I described in matching pursuit and will see in the other session the details of other strategies that can provide better performance."
        ],
        [
            "Yes.",
            "Can you repeat the question?",
            "I simply didn't hear it.",
            "That's a good question, so.",
            "A When A is often or more.",
            "So it's a square matrix.",
            "Right, so if you have a problem where you have the same number of unknowns as the number of equations, but maybe it's simply very badly behaved.",
            "Well, if it's an invertible matrix.",
            "Uh, somehow well you can.",
            "You can do some sort of preconditioning or.",
            "I mean, it's not necessarily going to make a offer normal in the sense that the L2 norm of the the approximation error measured between B.",
            "My B&AX.",
            "Is not going to be the approximation error between A-B a -- 1 B&X, but.",
            "Of course it's it's better behaved if a square if a is not square.",
            "I.",
            "No no.",
            "OK, so this was maybe longer review of algorithms and will have a longer detailed presentation of algorithms after the pose, but after the break now.",
            "Yeah, we'll discuss the properties that are the major properties of these algorithms and which are essentially the reasons why they are becoming popular in machine learning and signal processing to solve inverse problems.",
            "And the reason why a number of algorithms are based on sparsity are becoming popular is because we're not only they work to solve some practical problems, but they are also associated to theoretical guarantees.",
            "That somehow indicates regimes where we know there they are guaranteed to work."
        ],
        [
            "So the problem that these sparse reconstruction algorithms are trying to solve, there are inverse problems where you're observing.",
            "A low dimensional projection of objects.",
            "Data vectors that live in high dimensions.",
            "These problems would not be.",
            "It will not be possible to solve them.",
            "Without a model on the original data that we want to reconstruct, and somehow the model that have these data is displayed, that is by this cross cross shaped.",
            "Is this is this the set?",
            "Of the original data that we believe our true original data belongs to.",
            "So essentially think about vision.",
            "You are when we're looking at the scene with a single I. Preferably we only see a 2D projection on the retina of this scene, and if if an object is completely in line with the with the optical center, we will completely confuse what's the what's in the back was in the front.",
            "There's no way we can reconstruct it.",
            "So there's a lot of.",
            "There's a lot of information, but if we know that the object that we are willing to reconstruct do not have this bad behavior and rather oblique compared to the the direction of projection will be able to exploit this property.",
            "Well here the sparsity assumption on the object we want to reconstruct together with some properties of the projection matrix.",
            "Here's what will guarantee that we can do the inverse reconstruction."
        ],
        [
            "Anne.",
            "I tried to display it in a more abstract way and so now we've seen there's a number of algorithms we can use to try to solve this problem.",
            "So suppose suppose Francois has chosen an X vector.",
            "And the only thing I can observe is the projection B of this X vector.",
            "And now I wish to use a certain algorithm, let's call it algorithm one.",
            "This algorithm, one sometimes it will succeed in recovering my unknown vector.",
            "Sometimes it won't succeed.",
            "So among all possible choices, that password could have matter.",
            "There's a number of choices that will lead the algorithm to succeed.",
            "This is this green region.",
            "So this green regions indicates symbolizes the input vectors that password can choose an so that if I try to reconstruct using algorithm one, I will exactly find solve the puzzle.",
            "So this is the shape.",
            "This is the abstract shape of this for algorithm one.",
            "Now I would like to understand better what's all these vectors?",
            "What is the shape of this?",
            "And you see it has a complicated shape.",
            "It's so Frio, recharacterizing.",
            "Finally, which vector are recovered by algorithm one and which are not.",
            "It's a daunting task, and maybe it's not so interesting because in the end it will just tell me well.",
            "Well, these are the vectors are the best characterization is these are the vectors are there within, one can recover.",
            "So we want to do something that can be better."
        ],
        [
            "Interpreted and the way we want to do it is by exploiting the sparsity of X.",
            "So let's say we want to figure out how sparse vector needs to be to be in this set to be recovered by algorithm one."
        ],
        [
            "So what we can do is look at from how the level sets of the L0 norm.",
            "So first of all we can.",
            "So in the on the left its representation in the usual Euclidean space.",
            "So I'm looking at one sparse vectors.",
            "They are aligned with the coordinate axes an on the right.",
            "It's a symbolic description the.",
            "So this is the bold L 0 ball vectors that have only one non zero coordinate.",
            "Do they all belong?",
            "Today set, are they all recovered by this algorithm?",
            "One yes, because this blue ball is included in the in the green sets."
        ],
        [
            "Then I can look at two sparse vectors etc.",
            "And what I can wander is how big can I make my LO bowl and still be guaranteed that these vectors are recovered by algorithm one?",
            "And here you see that so well.",
            "OK, for this algorithm sparsity K is recovered because this this Blue Bowl is included.",
            "But if I were to enlarge this bowl.",
            "The I would meet this this corner so there would be vectors that are K plus one sparse and that are not recovered.",
            "So the whole game in getting recovery guarantees for algorithms is 2 characterized how sparse vector.",
            "Is this possible that is sufficient to guarantee that the algorithm can?"
        ],
        [
            "Cover it."
        ],
        [
            "Now, if you take a different."
        ],
        [
            "Algorithm.",
            "Well, you get the different sets of vectors that can be recovered with this algorithm.",
            "Anne, this different sets.",
            "As you can see what I try to display in this picture is that well, it just has a different shape."
        ],
        [
            "So you can play the same game plan for this algorithm.",
            "You observe that you can make.",
            "You can find guarantees with less sparse vectors for the recovery with the second algorithm, but it doesn't mean that this second algorithm is universal.",
            "Universally better than the first one, because there are vectors that the first one can recover and that the second one cannot recover and vice versa.",
            "So the game that we're going to look at now is how can we characterize these sparsity levels that guarantee that certain specific algorithms specific of re algorithms can recover?"
        ],
        [
            "So there's a number of results in the literature and there's an ever growing number of theorems in this field, but most of them share this common structure, so this is the meta theorem on sparse recovery.",
            "If you wish.",
            "This is a theorem that says well, and that's the theorem that I expressed here in a noiseless setting, but it's just for simplicity.",
            "There are equivalents where they are version of this theorem where you allow.",
            "Our approximation here, not exact representation.",
            "This theorem established the existence of sparsity levels.",
            "So that if a vector is sufficiently sparse in terms of L 0 Norma, then if I.",
            "Project it down and reconstruct it with here.",
            "Now the first thing the first thing is the following.",
            "Sorry, they're vector that is so sparse that if I project them down, there's no other representation that is as sparse, so they are the unique spouses solutions.",
            "So the problem is just well posed.",
            "And this is this sufficiently sparse.",
            "Is a function of the projection matrix A?",
            "When does a US so this is the identifiability of the back of sufficiently sparse vectors.",
            "Now there's a second level of results in the theorem that are related to practical algorithm that can recover them in here.",
            "This is for one minimization.",
            "The minimizer of deal one norm under this exact reconstruction constraints is guaranteed to recover X0, provided that X0 has a sufficiently small L zero norm.",
            "This is the general shape of this of the theorems, and if I kept it to be generate, it would be essentially a useless theorem, because if I put KO and K1 to be 0, just means that the zero vector is the sparsest solution of the problem.",
            "0 equals AX.",
            "This is not a surprise even if I put it to to be one.",
            "It's essentially an antiserum, so the whole strength of the theorems.",
            "Will be in characterizing how big.",
            "Can these dispersity levels be?",
            "To stand still ensure recover the recovery, the uniqueness and the recovery of these.",
            "So there's been a number of results exploiting values properties of of the matrix A.",
            "The first one we are due to do no one rule.",
            "In 2001.",
            "They were exploiting the notion of coherence with very specific matrices, a that where the union of two bases and since then the the assumptions have been progressively relaxed.",
            "An extended from recovery for L12 extra recovery with violence of matching pursuits.",
            "And so on."
        ],
        [
            "So.",
            "The fact that.",
            "It is possible.",
            "To lower the dimension.",
            "Drastically to take a very high dimensional object to project it in low dimension an if it is sparse enough, it's still possible to reconstruct the object has led to the notion of compressed sensing, which is a new way.",
            "Well, not so new now, but it's a.",
            "It's a, it's a new trend in how to measure how to capture data.",
            "So when we do machine learning, we most I would say we we have data, it's been acquired.",
            "It's somewhere in the database and we want to extract information.",
            "But somewhere before data exists, it has to be generated and when these data comes from the physical world, there's been an acquisition process.",
            "This acquisition process we've all learned about Shannon sampling, which is the standard way of acquiring data."
        ],
        [
            "And Shannon sampling, yeah performs.",
            "Summer performs in a way compressing will.",
            "Completely completely changed the way we capture data.",
            "For, provided that the data satisfies certain properties.",
            "So as the basic example, here is the thing, something taken from the Seminole Paper by Countess come back on to where they were considering.",
            "Well here a toy problem of demography so.",
            "It's it's and this is an inverse problem.",
            "First, why is it an inverse problem?",
            "Because there is there is data and here this is supposed to be the brain of a slice of the brain of somebody.",
            "This is the shape Logan Phantom.",
            "I hope it is not my brain to except that with the heat here, maybe it's becoming the shape of my brain.",
            "But OK, suppose this.",
            "This is the sum physical object living in the physical world that has.",
            "The density at certain places that follow this that correspond to this picture and this object is acquired by magnetic magnetic resonance imaging.",
            "And actually OK here it's not my forget magnetic resonance imaging.",
            "It's acquired by tomography and the way it's acquired is associated.",
            "The physical process can be mathematically interpreted as follows.",
            "You do before you transform of your of your image and you only keep the value of this free transform along lines in the in the free domain.",
            "This is the.",
            "This is the mathematical interpretation of the physical process acquisition process.",
            "In tomography, So what happens is, well, if you knew the whole free transform of the object, you would just have to do the full inversion and you would reconstruct.",
            "Your object, but you don't know the whole free transform is just like the inpainting problem that I showed before.",
            "You know, the free transform along these white lines, but you don't know the free transform in the black areas, so you have missing values.",
            "How do you reconstruct the data?",
            "First technique would be well.",
            "Neglect the fact that you don't know the values.",
            "Put zeros where you don't have values.",
            "Do inverse Fourier transform and this is what you would get.",
            "Well, obviously this is not a very good solution and way before compressed sensing, people working in MRI and computed tomography have been developing much better ways of doing reconstruction, but yet there's.",
            "It's been a surprise the to have all the types of reconstruction based on their one minimization exploiting the principles of sparsity that are associated to guarantees on the algorithms.",
            "So why can we exploit sparsity at all?",
            "To solve this inverse problem.",
            "Oh, I showed a picture, but any idea in the audience whi WHI can we say that the subject is sparse?",
            "Yeah, it is so this is this is an object that is piecewise constants and the boundaries between the piecewise constant areas are regular.",
            "So it can be described with fair with very few parameters.",
            "There's a, so this is the first informal way of saying it is.",
            "It is sparse because it can be described with very few parameters.",
            "So it's sufficient to identify these very few parameters to reconstruct the object, and here you could say.",
            "I mean this is a superposition of ellipsoids.",
            "So if you know the.",
            "Axis and all the parameters of each of these apes.",
            "Read if you know the color that you put in his sleep.",
            "So read the object can be reconstructed.",
            "So of course you do not want to go in such a parametric model finding.",
            "I mean not in this case because, well, you could adapt your model to reconstruct this particular image, but what would you get for real brain?",
            "So the technique is to be a bit more general, and here the fact that this object is regular away from irregular boundaries implies that when you do that, if you were to do the wavelet transform of the subject.",
            "You would get something that is very sparse, so you would get zero with it coefficients away from the boundaries and significant wavelet coefficients only along the boundaries.",
            "Yes, there's a question here.",
            "Why would?",
            "I don't know if the real brain is sparse.",
            "This is definitely a toy example.",
            "1st and then the source.",
            "This is also true question in the sense that there are many things I will in this tutorial where I will for get in the first 2 lectures I will.",
            "I will not talk about approximation to the model.",
            "It will not talk about noise, but in the third lecture I will come back to this.",
            "The issue I I want to keep the flow of the presentation based on the noiseless Toyota model, but we'll see that many of the results can be extended in a generic way when when the model is not exactly sparse.",
            "So let me just take the time to answer a bit further question the brainer.",
            "It's certainly not sparse.",
            "If you do, even if you do, the wavelet transform of what you could imagine as a full resolution brain image, nothing is here.",
            "There may still be some significance that are significantly larger than others, but maybe for diagnosis the particular structure of the small coefficients maybe maybe useful.",
            "What happens is that if the small coefficients are sufficiently small.",
            "Types of techniques will still be guaranteed to provide the.",
            "An accurate approximation of the image you are willing to reconstruct.",
            "Basically, don't care about it.",
            "Go back.",
            "Well, I was rather talking about the wavelet transform, but the wavelet domain rather free domain.",
            "Now saying that there are cases where anyway if your physical acquisition process is like this, huge undersampling.",
            "I know you're probably might not be to reconstruct exactly the image, because I mean you know you'll need to put a model.",
            "Your model will never be 100%.",
            "So I create what you want to do to get is something that's that's as good as it can, and if the if a real images that I mean the data that you want to acquire is not exactly sparse but sufficiently close to sparse, then there are guarantees for this reconstruction algorithm that we have.",
            "The quality will depend on this how close you are to being exactly sparse.",
            "So here it's really a toy model an for this time model.",
            "So.",
            "Remember, this is the analog domain you you don't get to.",
            "You don't know it, but you model it and for this is a social experiment.",
            "Suppose you had this one you and you were able to do with the transform.",
            "You would get very few significant coefficients and most of them would be 0 here on this toy model.",
            "So what?",
            "How can you relate this to what you've observed?",
            "What you will serve as a product of this linear undersampling for you under sampling?",
            "Measurement system by this, but this is the product of the forward wavelet transform by these coefficients.",
            "So there's a linear relation between these unknown wavelet coefficients.",
            "That's your model as being sparse.",
            "And these observation.",
            "So what you can do is, given this observation, find the sparsest X that's compatible with this linear observation system.",
            "An hope that it will be close to the original X, and if you do so and you can do it by solving this optimization problem, finding the minimum and one normal solution to your reconstruction problem.",
            "This is what Countess Hambergen Tao showed in their Seminole paper.",
            "You get the reconstruction that exact well for this toy example is exactly the original image are the SNR is something like 300 DB.",
            "So this well this process.",
            "I mean, this reconstruction is used when you have this physical acquisition process that's.",
            "That is negatively reducing the dimension.",
            "So in this case you have no choice.",
            "This is the way a tomography proceeds.",
            "You get a number of slices and you have to reconstruct from from them.",
            "But this has brought up the idea that maybe if you know that the data you want to capture is actually sparse in a domain that you know, maybe it's possible to design an acquisition system that will not try to not spend much efforts capturing a higher definition image, but simply.",
            "Capture something a much coarser that will still capture the a sense.",
            "The information that you need so that in the end you can reconstruct the whole object by solving this type of optimization problem."
        ],
        [
            "So.",
            "So this is the so called.",
            "The circle padding of compressed sensing and this is schematic comparison with classical signals that as a classical sampling in classical Shannon sampling, what could say first you sample high resolution and then you think how you can reduce the volume and compress.",
            "So you have a high dimensional analogue object.",
            "You quite a high resolution and once you have the high resolution.",
            "Uh."
        ],
        [
            "Acquisition, you think?",
            "Well, maybe this is.",
            "Maybe maybe the volume.",
            "Maybe the number of terabytes I need to store this is too big.",
            "So let me think what I know about this original object.",
            "Well, I know that if I do it all the time, frequency transform or wavelet transform it is sparse.",
            "So actually there is a sparse representation of my object.",
            "So let me now compute this pulse representation of Z and.",
            "And code it just as I described at the very beginning of this."
        ],
        [
            "Lecture.",
            "So.",
            "If I if you do this well, there's an encoding step, which is somewhat costly because you have to do a transform of your high definition image or sound and you get this lower bitrate representation, and it's generally designed so that the decoding is very cheap, because if.",
            "I mean, you acquire once and probably you're going to to decode many times on several several devices."
        ],
        [
            "The the way compressed sensing proposes to perform acquisition is different well from the beginning, you know, yeah, you have to know that the data you want to acquire is sparse in some domain that you can that you know.",
            "So this data that you want to acquire.",
            "Maybe it's not worth acquiring it full resolution.",
            "Maybe there's a way of making a low dimensional projection of the subject that will.",
            "Exploit the sparsity and preserve the main pro."
        ],
        [
            "What is the object so that from this low dimensional projection it's possible to reconstruct this vector?",
            "And if you're able to reconstruct this vector then you are able to reconstruct this."
        ],
        [
            "So in this case you trying to design an acquisition process that is less expensive in terms of resources, number of sensors, etc.",
            "And this comes at a cost, because when you really want to reconstruct the object, you have to run other types of reconstruction algorithms, and we'll see in the next session that, well, these one minimization or greedy algorithm that can remain quite complex."
        ],
        [
            "OK, for this paradigm for this compressed sensing to work, there are a few necessary hypothesis.",
            "The first one is that you have to know that your data is sparse in some domain, sufficiently sparse an.",
            "Whether the brain is part in the sufficiently sparse in some domain, it's a good question.",
            "You also have to have certain properties of your projection matrix so that the recovery is actually guaranteed for not trivial response vectors, but moderately sparse vectors, and this generally comes from uncertainty principle between the domain where the data is sparse and the domain where it is measured, such as Heisenberg uncertainty principle between time and frequency.",
            "But other types of uncertainty principles.",
            "One of these uncertainties.",
            "Suppose is reached through the design of sensing devices through using some level of randomness randomness brings.",
            "Some uncertainty principle that are desirable.",
            "And the last point that is needed is that, well, well, you need your data to be sparse in some domain.",
            "And you want to reduce the dimension, but you cannot reduce the dimension as much as you as you wish.",
            "Somehow you must still preserve the amount of information that in your original data, and this turns.",
            "This can be written in terms of the numbers of measures that you need.",
            "This number of measures is typically expressed like that in the recovery guarantees, and you'll perhaps recognizer the term that I showed at the beginning of this.",
            "This Class A, which is essentially the coding costs of a vector that is case sparse in dimension N. So essentially the number of measures you need is given by this.",
            "It is necessary if you want to have robust reconstruction, you must have this number of of measures an it is sufficient with certain types of random measurement systems."
        ],
        [
            "So as I said, I mean there's a log factor.",
            "It's so so this this K log N / K. Sometimes it appears as.",
            "As.",
            "Magic, but it's just comes from the fact that the vectors that you want to encode they have a certain entropy.",
            "TI mean there's a number of bit necessary to code them, so you cannot go."
        ],
        [
            "Below this, so just to summarize this first part.",
            "So the goal was to show you first the notion of sparsity.",
            "How it naturally emerged in the context of data compression, where it's naturally related to the notion of economy in terms of number of bits, number of flops to perform computation, and so on, and how in the last 10 years it's been.",
            "Shown and realized that this is deeply connected with inverse problems and sparsity, or is an hour a it's a property.",
            "When an object is sparse, it enables a number of.",
            "Applications related to inverse problems and I'll stop here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other this tutorial is organized in three sessions an in this first session what I want to do is to give you a general digital picture, give you another view, and the next two sessions will be getting in more details in some more specific topics.",
                    "label": 0
                },
                {
                    "sent": "So for this first session of today, I'm going to introduce the concept of sparsity, why it is important in signal processing and machine learning.",
                    "label": 0
                },
                {
                    "sent": "Where it comes from and here I would just like to mention that well, I come from a single processing background an I understand the decision machine learning summer school.",
                    "label": 0
                },
                {
                    "sent": "So sometimes they are well they are concept that have been developed in in both communities.",
                    "label": 0
                },
                {
                    "sent": "When necessary I'll try to help you identify the names that may be slightly different from between machine learning and signal processing, but in the main.",
                    "label": 0
                },
                {
                    "sent": "The flow of what I will be presenting an I apologize for that.",
                    "label": 0
                },
                {
                    "sent": "I will probably mostly use the single processing terminology.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today.",
                    "label": 0
                },
                {
                    "sent": "I want to start by.",
                    "label": 0
                },
                {
                    "sent": "The notion of sparsity and why and how it will naturally related to the problem of compressing high dimensional data in the second step, I will show that it's not only useful a useful concept for our compression, it's also something that has emerged more recently.",
                    "label": 0
                },
                {
                    "sent": "It's a concept that is useful to solve problems, inverse problems where a priority, a notion of sparsity.",
                    "label": 0
                },
                {
                    "sent": "We could have seen irrelevant.",
                    "label": 0
                },
                {
                    "sent": "Then I will give you a global picture of the main algorithms that I used to that that have been developed to exploit sparsity of high dimensional data, and I will conclude with a brief discussion of the nature of the theory backing the use of this.",
                    "label": 0
                },
                {
                    "sent": "These algorithms and its application to compress sensing a keyword that you may have.",
                    "label": 0
                },
                {
                    "sent": "Here, here in the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's begin by the beginning.",
                    "label": 0
                },
                {
                    "sent": "The notion of sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, and here you will.",
                    "label": 0
                },
                {
                    "sent": "This will sound like something you've heard a zillion times.",
                    "label": 0
                },
                {
                    "sent": "Well, today we're dealing with massive data data and the data.",
                    "label": 0
                },
                {
                    "sent": "The numbers I'm giving here are not so massive, but just give you an example.",
                    "label": 0
                },
                {
                    "sent": "If you simply take one second of CD quality stereo audio, it's already more than one.",
                    "label": 1
                },
                {
                    "sent": "Megabits and if you take like a standard, the picture out of a digital digital camera, it's already 240 megabits, so dealing with large data collections of large data well, we know that we have limited energy resources and computing resources in more general.",
                    "label": 0
                },
                {
                    "sent": "So we need concise or presentations of these data to handle them both, or transmitting them, storing them, but also.",
                    "label": 0
                },
                {
                    "sent": "Simply extract the information and manipulated.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as the standard tools have been established to manipulate different types of data, and for example, if you if you manipulate audio audio is a time can be represented as a time series.",
                    "label": 0
                },
                {
                    "sent": "So high dimensional data mean time series with many samples.",
                    "label": 0
                },
                {
                    "sent": "If you represent these data in the rules in roll form, every sample matters.",
                    "label": 0
                },
                {
                    "sent": "If you remove a few samples that the nature of the data is corrupted, but it is known that if you go in the time frequency domain and this is a time frequency representation with the time here frequency, he ran color codes the intensity at a given time and frequency.",
                    "label": 0
                },
                {
                    "sent": "You generally get this type of feature where many areas are black and this means that there's little energy and just a few time frequency points are associated with.",
                    "label": 0
                },
                {
                    "sent": "Red or yellow?",
                    "label": 0
                },
                {
                    "sent": "High energy content.",
                    "label": 0
                },
                {
                    "sent": "This means there are many zeros in this transform representation of audio.",
                    "label": 0
                },
                {
                    "sent": "If you replace audio by images here a picture if you analyze it with the wavelet transform with a different color code, you observe that there's Gray at most places and in few places there is white or black, or which means there's more intensity.",
                    "label": 0
                },
                {
                    "sent": "So these data when you transform them in the appropriate domain can be represented by.",
                    "label": 0
                },
                {
                    "sent": "Set off by vector by sets of coefficients that have few significant coefficients, and this fact turns out to be very useful to compress this data because.",
                    "label": 0
                },
                {
                    "sent": "Because they can be very well approximating by setting to zero exactly these values that have that have.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Small significance, so mathematically this means that the data that can, or the signals and images that can be considered as high dimensional vectors.",
                    "label": 1
                },
                {
                    "sent": "So each pixel value is a coefficient in this vector.",
                    "label": 0
                },
                {
                    "sent": "These high dimensional vectors that can be well approximated as linear combinations of few elements from some set of basic signals or basic images.",
                    "label": 0
                },
                {
                    "sent": "So for example for sound.",
                    "label": 0
                },
                {
                    "sent": "This means that we can take sounds we can take the short line free transform.",
                    "label": 0
                },
                {
                    "sent": "There are many zeros, so many many small coefficients.",
                    "label": 0
                },
                {
                    "sent": "We set them to zero and there's an inverse short time for it transform that reconstructs the data.",
                    "label": 0
                },
                {
                    "sent": "So if we don't do any, change any settings.",
                    "label": 0
                },
                {
                    "sent": "If we don't set anything to zero, we can reconstruct the data exactly.",
                    "label": 0
                },
                {
                    "sent": "If we set to 0, the small coefficients with it's only a small perturbation and we reconstruct almost exactly the data, and this inverse transform.",
                    "label": 0
                },
                {
                    "sent": "Neither associated to such linear reconstruction, so the data is going to be a combination of few few elements, few time frequency atoms.",
                    "label": 0
                },
                {
                    "sent": "It's the same for images for images such as the one I showed, they would be expressed as good as they would be well approximated as a linear combination of few wavelets.",
                    "label": 1
                },
                {
                    "sent": "So the fact that there are few elements and that this data is well approximated will be measured by the error between the original data and this approximated reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Hopefully we're getting a small error and the fact that we can reconstruct it with few elements is measured by the.",
                    "label": 0
                },
                {
                    "sent": "So called L zero norm of the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "So this just counts the number of nonzero elements in this vector.",
                    "label": 0
                },
                {
                    "sent": "So I'm sure you have all seen this notion of as your own, but this is the beginning.",
                    "label": 0
                },
                {
                    "sent": "So this is the the way the the most standard way of measuring sparsity, counting the number of nonzero elements in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This representation.",
                    "label": 0
                },
                {
                    "sent": "So overall, and this is a type of a type of feature that I will do several times in this in this talk, what we will represent data as vectors, so as column vectors.",
                    "label": 0
                },
                {
                    "sent": "So the original data is a high dimensional column vector and I filled it with color to indicate that most coefficients are non zero and the fact that there's a sparse representation in some domain means that when we choose the right.",
                    "label": 0
                },
                {
                    "sent": "The right matrix five that combines the atoms from which it can be built.",
                    "label": 0
                },
                {
                    "sent": "It is possible to find a coefficient vector to represent this this data vector with only a few significant coefficients.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a property that is that has been observed on many signals.",
                    "label": 0
                },
                {
                    "sent": "So audio signals with short time free transform and time frequency at homes here in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "For images this would be with wavelets.",
                    "label": 0
                },
                {
                    "sent": "More generally, the fact that we can finding such an expression is only possible if we choose appropriately the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So maybe I forgot to say this matrix is called a dictionary because.",
                    "label": 0
                },
                {
                    "sent": "Each element gives like a word and combining a few words you can get a representation of the of the whole signal.",
                    "label": 0
                },
                {
                    "sent": "So when the dictionary is well chosen.",
                    "label": 0
                },
                {
                    "sent": "You get this type of sparse approximation of the data.",
                    "label": 0
                },
                {
                    "sent": "I mean there you get the existence of such a representation.",
                    "label": 0
                },
                {
                    "sent": "If you don't choose the dictionary appropriately well.",
                    "label": 0
                },
                {
                    "sent": "This raises other questions and I will mention them at the end of this lecture.",
                    "label": 0
                },
                {
                    "sent": "How can we choose a dictionary?",
                    "label": 0
                },
                {
                    "sent": "So now let's see how this impacts a potential compression of data vectors.",
                    "label": 0
                },
                {
                    "sent": "So the initial vector is has entries and Annette Natural way of storing it would be by storing N floats.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the vector of the sparse vector on the right hand side, there are only there's a much smaller number of nonzero entries.",
                    "label": 1
                },
                {
                    "sent": "So to code this vector you need to code K floats, let's say.",
                    "label": 0
                },
                {
                    "sent": "So if K is much smaller than NA preore you get you get some compression.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the story doesn't end here.",
                    "label": 0
                },
                {
                    "sent": "If you just go to the value of these coefficients.",
                    "label": 0
                },
                {
                    "sent": "The vector is not coded yet because you still need to know where these coefficients are and this is actually what costs you most, because if you don't have any priority idea of where this K coefficients among N allocated, there are K choose well an choose K possibilities and the number of bits you know you need to encode.",
                    "label": 0
                },
                {
                    "sent": "This is roughly speaking this this log which can be well approximated as.",
                    "label": 1
                },
                {
                    "sent": "K The number of non zero limits times log N / K which comes from the combinatorial nature nature of this set of K elements.",
                    "label": 0
                },
                {
                    "sent": "So, provided that you know the dictionary that you know, transform a domain where your data is case bounce or well approximated as K sparse.",
                    "label": 0
                },
                {
                    "sent": "You can hope to get coding of this data with only K log, N / K bits.",
                    "label": 0
                },
                {
                    "sent": "I mean this is order of magnitude.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this notion of sparsity, I hope to have conveyed the idea that it's very natural in the coding context, 'cause you basically you want to find the representation with few nonzero elements, because the fewer nonzero limits the most compressed your representation.",
                    "label": 0
                },
                {
                    "sent": "What I will try to show you now is that in a completely different context, the fact that some data is sparse can help solve the problem and in the.",
                    "label": 0
                },
                {
                    "sent": "Inverse and inverse problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is an inverse problem?",
                    "label": 0
                },
                {
                    "sent": "This is a problem where you don't observe the data that you wish to manipulate directly, but only indirectly.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "There's some level of missing information that you would like to create compensates, so here is a prototype example which is an example of inpainting problem in.",
                    "label": 0
                },
                {
                    "sent": "In painting you can think that there's an image that is you wish to manipulate too.",
                    "label": 0
                },
                {
                    "sent": "Alter auto reconstruct, but some pieces of this image are missing, and here these parrots you would like to see the whole parrots.",
                    "label": 0
                },
                {
                    "sent": "But there are bars in front of the power.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this image as the full image of the Parrot, but only observed on where this mask is black.",
                    "label": 0
                },
                {
                    "sent": "So you only observe this image at certain locations.",
                    "label": 0
                },
                {
                    "sent": "The rest is just not observed.",
                    "label": 0
                },
                {
                    "sent": "So how can you reconstruct the original image from the partially observed one apriori?",
                    "label": 0
                },
                {
                    "sent": "You have a problem.",
                    "label": 0
                },
                {
                    "sent": "Why do you have a problem?",
                    "label": 0
                },
                {
                    "sent": "Because the original image has, let's say, a million pixels an you perhaps observed only 800,000 pixels.",
                    "label": 0
                },
                {
                    "sent": "So somehow you have 800,000 OPS observed.",
                    "label": 0
                },
                {
                    "sent": "You have eight, you you need to recover vector in Minion dimension from only 800,000 equations.",
                    "label": 0
                },
                {
                    "sent": "You are missing equations.",
                    "label": 0
                },
                {
                    "sent": "So you have a problem you cannot solve.",
                    "label": 0
                },
                {
                    "sent": "So to solve it well, or two to estimate the solution, you need to introduce some prior knowledge on your image.",
                    "label": 0
                },
                {
                    "sent": "You need to introduce some some knowledge that will help you figure out among the many possible solutions compatible with what you've observed.",
                    "label": 0
                },
                {
                    "sent": "Which one is most likely and will, I mean, will fit your prior and fit your observation.",
                    "label": 0
                },
                {
                    "sent": "And if you do so and this can be done using sparse sparse representations in.",
                    "label": 0
                },
                {
                    "sent": "In wavelet domain here you can reconstruct an image that visually looks like the parrot, free from his bar.",
                    "label": 0
                },
                {
                    "sent": "So if you look carefully close enough where the where the bars where, you will probably notice that.",
                    "label": 0
                },
                {
                    "sent": "Well there are some for the areas, but globally this looks like you've solved.",
                    "label": 0
                },
                {
                    "sent": "You've extrapolated the parrot where you didn't observe and this has been done using a sparse sparse representation and an algorithm to exploit sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example is audio separation.",
                    "label": 0
                },
                {
                    "sent": "So before giving the example, just let me just.",
                    "label": 0
                },
                {
                    "sent": "Explain briefly what is source separation?",
                    "label": 1
                },
                {
                    "sent": "Well, it's not easy today because you're all very quiet, so you're not making much noise.",
                    "label": 0
                },
                {
                    "sent": "But if you were talking why I'm trying to convey this message about sparsity, UOL, each of you would be hearing not only what I'm saying, but combination of the sound coming from many people, and probably the camera there in the back would be capturing this mixture.",
                    "label": 0
                },
                {
                    "sent": "And well, if in the end you want to focus on what I'm saying, there's no need to identify and remove the sound from the the.",
                    "label": 0
                },
                {
                    "sent": "The disturbing disturbing sources well.",
                    "label": 1
                },
                {
                    "sent": "If there is no not so many, well, even in a different context, you may be interested in all sources.",
                    "label": 0
                },
                {
                    "sent": "Well, what I was describing was there's the important message, and there's the noise.",
                    "label": 0
                },
                {
                    "sent": "But here this would be well if you listen to a CD.",
                    "label": 0
                },
                {
                    "sent": "If you listen to music, or if you listen to MP3, well there are different instruments playing together an you may want to focus on separately on each instrument.",
                    "label": 0
                },
                {
                    "sent": "For example, you may want to.",
                    "label": 0
                },
                {
                    "sent": "Listen to your favorite guitar player to know what he's playing and reproduce it.",
                    "label": 0
                },
                {
                    "sent": "Or you may want to remove it.",
                    "label": 0
                },
                {
                    "sent": "This sound from the from the recording so that you can try to replace him.",
                    "label": 0
                },
                {
                    "sent": "So here if you take a recording and let's let's listen to this recording.",
                    "label": 0
                },
                {
                    "sent": "So you listen to it once.",
                    "label": 0
                },
                {
                    "sent": "I didn't tell you what to listen to or specifically.",
                    "label": 0
                },
                {
                    "sent": "Now I can tell you or try to tell me, can you try to tell me how many instruments?",
                    "label": 0
                },
                {
                    "sent": "Are playing on this recording.",
                    "label": 0
                },
                {
                    "sent": "So 2, three, OK, let's make a pool.",
                    "label": 0
                },
                {
                    "sent": "Who are things?",
                    "label": 0
                },
                {
                    "sent": "There are three instruments.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a number of hands who think there are two instruments.",
                    "label": 0
                },
                {
                    "sent": "Now roughly equal, who thinks there's one instrument?",
                    "label": 0
                },
                {
                    "sent": "Somebody in the back of zero instruments.",
                    "label": 0
                },
                {
                    "sent": "OK, at least there's activity there.",
                    "label": 0
                },
                {
                    "sent": "Sound activities, so the zero is is out, but the number of instrument is, well, is something that human beings.",
                    "label": 0
                },
                {
                    "sent": "Well with only one listening to the to the to the sound without being your won't have may have difficulty solving well actually well, we'll see now how many instruments they are.",
                    "label": 0
                },
                {
                    "sent": "I mean I'll detected by the source separation algorithm that I've run on it.",
                    "label": 0
                },
                {
                    "sent": "There are two the bass and guitar, but you'll see when you listen to them that those who raise their hands for three instruments are not completely well, didn't may have heard something that are present in there.",
                    "label": 0
                },
                {
                    "sent": "So if you run a certain algorithm that exploits sparsity of the of these sounds, you figure you can figure out that there are two instruments and.",
                    "label": 0
                },
                {
                    "sent": "Extract the sound of these instruments.",
                    "label": 0
                },
                {
                    "sent": "So this was the base, but you may have heard there there were some noise actually, as far as I know this recording was done live in a bar or in a restaurant I think, and some people are eating and the Forks and knives and which may sound a bit like like drums like a third instrument and the guitar now.",
                    "label": 0
                },
                {
                    "sent": "So this is just to show you are well the results.",
                    "label": 0
                },
                {
                    "sent": "And what I can tell you this is done using sparsity.",
                    "label": 0
                },
                {
                    "sent": "So OK, maybe so far this is just magic you're.",
                    "label": 0
                },
                {
                    "sent": "So let's look a bit more closely how sparsity is exploited to deal with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this problem?",
                    "label": 0
                },
                {
                    "sent": "Well, but before let me just put it in context so these two examples that I showed there are examples of inverse problems.",
                    "label": 0
                },
                {
                    "sent": "Inverse problems arise when we have when we observe indirectly some data and this observation comes from a process that has lost some information.",
                    "label": 0
                },
                {
                    "sent": "And well here I'm focusing on linear inverse problems.",
                    "label": 0
                },
                {
                    "sent": "The observation like blurring or mixing different sources are including.",
                    "label": 0
                },
                {
                    "sent": "Something in an image this this loses some information and yet we wish to reconstruct the original data, so we have fewer equations than unknowns and would like to solve this problem without prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "It's impossible, and sparsity is precisely a type of knowledge that says this data.",
                    "label": 0
                },
                {
                    "sent": "This data is high dimensional, but you know there's few parameters to describe it, so if they are sufficiently few parameters, maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe this matches well the number of observations that you have, and maybe you can exploit this small number of observation and is even smaller number of parameters to actually solve your inverse problem.",
                    "label": 0
                },
                {
                    "sent": "And this is this is what happens.",
                    "label": 0
                },
                {
                    "sent": "Not only it's possible, it's in some cases it's a well posed problem, but also there are algorithms that's probably performed this reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this blue and blind source separation problem that I just mentioned, what is happening?",
                    "label": 1
                },
                {
                    "sent": "And this is this?",
                    "label": 0
                },
                {
                    "sent": "Is this is a toy model that's valid for studio recordings.",
                    "label": 0
                },
                {
                    "sent": "In reverberant rooms like this one where the reflections on the walls and it's a bit more complex, but this time model is valid when you used studio mixing.",
                    "label": 0
                },
                {
                    "sent": "So what happens you have?",
                    "label": 0
                },
                {
                    "sent": "Recall with two microphones, just like your two years, so on the right channel you have some time series that's recorded and simultaneously on the left channel there's another time series that's recorded.",
                    "label": 0
                },
                {
                    "sent": "And let's say this is a recording that comes from the mixture of resources.",
                    "label": 0
                },
                {
                    "sent": "If you know nothing about the sources, well, too bad.",
                    "label": 0
                },
                {
                    "sent": "You don't know how you don't have enough equations to solve the problem, but let's look at very simplified case.",
                    "label": 0
                },
                {
                    "sent": "Suppose that.",
                    "label": 0
                },
                {
                    "sent": "I'm speaking then you're speaking, then you're speaking.",
                    "label": 0
                },
                {
                    "sent": "So the sources are not active simultaneously.",
                    "label": 0
                },
                {
                    "sent": "What happens is that at a given time.",
                    "label": 0
                },
                {
                    "sent": "There is only one source that is active.",
                    "label": 0
                },
                {
                    "sent": "You don't know which one I preordered.",
                    "label": 0
                },
                {
                    "sent": "You don't know where the source is, but you know that at a given time there can only be one active source.",
                    "label": 0
                },
                {
                    "sent": "So what happens, I suppose suppose, possibly speaking before before wine.",
                    "label": 0
                },
                {
                    "sent": "Anne Francoise is on my right, so the sound that I receive on my right ear is stronger than the sound I receive on my left ear and the ratio the amplitude difference between what I receive on both ears is constant is characterized by where is located.",
                    "label": 0
                },
                {
                    "sent": "This is what I can exploit and here.",
                    "label": 0
                },
                {
                    "sent": "If you look at the time instance where only the first source is active and you look at the value observed on the left channel versus the value of certain right channel is a line on some line that specific of the difference of amplitude between the I mean of the ratio of completed between the two channels for this location.",
                    "label": 0
                },
                {
                    "sent": "So in this setting if I display the the amplitude on the left and on the right, well for all these time instance it's on this line for this time in sense, so it's on this other line, etc.",
                    "label": 0
                },
                {
                    "sent": "So from this scatter plot.",
                    "label": 0
                },
                {
                    "sent": "You can perform some clustering, identify well all the active sources.",
                    "label": 1
                },
                {
                    "sent": "What are the directions of the active sources an?",
                    "label": 0
                },
                {
                    "sent": "What are the time instance where each source is active?",
                    "label": 0
                },
                {
                    "sent": "So of course this is.",
                    "label": 0
                },
                {
                    "sent": "This is a bit and idealistic case.",
                    "label": 0
                },
                {
                    "sent": "In practice, you're not very often interested in separating sound sources that are not active at the same time.",
                    "label": 0
                },
                {
                    "sent": "And the example I showed was with sources that are active simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if they are all active simultaneously and you do exactly what I showed, the scatterplot is not so nice and well for sure, I wouldn't be able to separate the sources and locate them from this scatter plot.",
                    "label": 0
                },
                {
                    "sent": "I don't know if maybe some some modern clustering techniques could do something, but there's a simple trick that drastically changes the scatter plots.",
                    "label": 0
                },
                {
                    "sent": "So here in time, when you look at the times, here is the source is there.",
                    "label": 0
                },
                {
                    "sent": "All active at all times.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you go to the time frequency domain and I showed you.",
                    "label": 0
                },
                {
                    "sent": "Sometime frequency representation at the beginning of this lecture.",
                    "label": 0
                },
                {
                    "sent": "If you go to the time frequency domain so your represents the time frequency representation of the left channel and the right channel.",
                    "label": 0
                },
                {
                    "sent": "And as I told you, for each source.",
                    "label": 0
                },
                {
                    "sent": "Is not active most of the time at most time frequency points it's only active at few times.",
                    "label": 0
                },
                {
                    "sent": "Frequency points.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that at a given time frequency point, most likely there's no source active, and sometimes there's one source active.",
                    "label": 0
                },
                {
                    "sent": "In very few cases, there are two active sources.",
                    "label": 0
                },
                {
                    "sent": "So in most of the cases there's zero or one active source at a given time frequency point an using this property you can do the same type of scatter plot which I displayed here, and I've already called it because this scatter plot again displays these kind of lines.",
                    "label": 0
                },
                {
                    "sent": "It's possible to cluster it, possible to estimate the source locations and to separate the sources.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the overall message here is that in a context that is quite different from compression, sparsity can be exploited to solve a problem that apriori is imposed.",
                    "label": 0
                },
                {
                    "sent": "So it is well known that, well, it's a well known result from Algebra 1, one that if you have fewer equations than unknowns there, you cannot see that there's an infinite number of solutions to your linear system.",
                    "label": 1
                },
                {
                    "sent": "And in particular, if you have two vectors that provide the same observation, you cannot conclude that these vectors are identical.",
                    "label": 0
                },
                {
                    "sent": "But what is novel and it's now been known for about 10 years, is that if?",
                    "label": 0
                },
                {
                    "sent": "Your two vectors are sparse.",
                    "label": 0
                },
                {
                    "sent": "Then an match.",
                    "label": 0
                },
                {
                    "sent": "Provide the same observation if they are sufficiently sparse, then they must be equal, so there's a under the sparsity assumption.",
                    "label": 1
                },
                {
                    "sent": "Your inverse problem becomes well posed.",
                    "label": 0
                },
                {
                    "sent": "There's a unique solution.",
                    "label": 0
                },
                {
                    "sent": "So it will post.",
                    "label": 0
                },
                {
                    "sent": "That's good, but it could still be combinatorial or intractable to find the solution.",
                    "label": 0
                },
                {
                    "sent": "The second good news is that, again, if it is sufficiently sparse, your unique solution is not only unique, but it can be identified with some practical algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then depending on the.",
                    "label": 0
                },
                {
                    "sent": "The strengths of the of the properties of the of this data that you're assuming or that you're entitled to assume well, you'll be entitled to use different types of algorithms to recover it and this.",
                    "label": 0
                },
                {
                    "sent": "This will be discussed in more details in tomorrow's lecture.",
                    "label": 0
                },
                {
                    "sent": "So the main algorithm that will study our LP minimization with particular focus on L1 minimization, which is a convex problem and greedy algorithms, and in particular the so-called matching pursuit algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Today, well in this lecture I just want to give you a brief overview of these algorithms so that you get first level of.",
                    "label": 0
                },
                {
                    "sent": "Reading of this, the what tools are involved in sparse sparse?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Presentations, and I think this is it is time to spend some time in 220 introduce.",
                    "label": 0
                },
                {
                    "sent": "It's not only the single processing vocabulary, but some connection with machine learning.",
                    "label": 1
                },
                {
                    "sent": "So the object I've chosen to use basic linear algebra notation all along this this talk.",
                    "label": 0
                },
                {
                    "sent": "So what is observed?",
                    "label": 0
                },
                {
                    "sent": "The observation, the data, the absurd vector, the right hand side the new equation X = B is well, it can be can be something that is measured on the sensor over can be the multi channel signal that we listen to can be the mask image inpainting.",
                    "label": 0
                },
                {
                    "sent": "It is related to some unknown and which can be the unknown.",
                    "label": 0
                },
                {
                    "sent": "X is either a sparse representation of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of a vector of the observation or could be the unknown sources that you want to recover.",
                    "label": 0
                },
                {
                    "sent": "It is so both are related through linear system, that is that we considered as known and this linear system, so it can be the dictionary that used to model to relate your vector and which is false representation can be the mixing matrix that relates the observed mixtures with the unknown sources can also be what is called the sensing system in compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "So when X is known a, there's a forward model that predicts what is observed, but the whole problem is to go backward.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in machine learning, I think it's somewhat standard.",
                    "label": 0
                },
                {
                    "sent": "Notation would rather be Y equals exhibit exhibit A.",
                    "label": 0
                },
                {
                    "sent": "So X plays the role of the mixing matrix or the dictionary XI think is the design matrix related to some some problems where you observe a number of wise and number of axes and you would like to predict how?",
                    "label": 0
                },
                {
                    "sent": "Why is can be predicted from from these axes?",
                    "label": 0
                },
                {
                    "sent": "So better will be the regression parameter that tells you how important are each of these.",
                    "label": 0
                },
                {
                    "sent": "Each columns of the design matrix, so I apologize if this connection to machine learning is a bit unclear, and if you want to discuss this further, I welcome you to come and check during the post.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "From now on I'm going to use BA XI.",
                    "label": 0
                },
                {
                    "sent": "Will try to stick with these names occasionally.",
                    "label": 0
                },
                {
                    "sent": "Will use dictionary mixing matrix and so on when it's hopefully appropriate.",
                    "label": 0
                },
                {
                    "sent": "So what is the inverse problem that we are considering?",
                    "label": 0
                },
                {
                    "sent": "It's a problem where there are things we know there are things we don't know.",
                    "label": 0
                },
                {
                    "sent": "Things we know are the matrix A, so it's a matrix that lowers dimension that projects an input vector.",
                    "label": 0
                },
                {
                    "sent": "Of high dimension N and I provides a low dimensional vector B of so smaller dimension an what we would like is to recover.",
                    "label": 0
                },
                {
                    "sent": "So the input vector X.",
                    "label": 0
                },
                {
                    "sent": "We know we need to use some some properties and I've told you that if it is sparse we can hope to recover it.",
                    "label": 0
                },
                {
                    "sent": "So we'll be looking for some somehow the sparsest solution to the system or the passes approximate solution to the system.",
                    "label": 0
                },
                {
                    "sent": "For this, well, there are many ways of formulating the problem which are.",
                    "label": 0
                },
                {
                    "sent": "Not fully equivalent, but are often interchanged in this in this game.",
                    "label": 0
                },
                {
                    "sent": "So the first one consists in simply saying that we're looking for this partial solution and there are approximation constraints.",
                    "label": 0
                },
                {
                    "sent": "The second one, consistent thing that we're looking for the best approximation given the sparsity constraints.",
                    "label": 0
                },
                {
                    "sent": "And then there's also, there are also cases where we just want to find the solution that satisfies both a an approximation constraint.",
                    "label": 0
                },
                {
                    "sent": "And the sparsity constraints.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the kind of idealized problem that we would like to solve.",
                    "label": 0
                },
                {
                    "sent": "And I think that's to go further.",
                    "label": 0
                },
                {
                    "sent": "It's important to develop some kind of geometric view of what is going on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The vector that we are looking for is X vector.",
                    "label": 0
                },
                {
                    "sent": "It's a sparse vector, so it has many zero entries.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "What does such a vector look like?",
                    "label": 0
                },
                {
                    "sent": "So I mean, you can write this vector, there are zeros and non zero entries.",
                    "label": 0
                },
                {
                    "sent": "This is not very geometric, but let's look in where the highest dimension I can picture.",
                    "label": 0
                },
                {
                    "sent": "In my mind 3D.",
                    "label": 0
                },
                {
                    "sent": "Ants are let's look at vectors that have one non zero and three.",
                    "label": 0
                },
                {
                    "sent": "What is a vector that has one non zero entry?",
                    "label": 0
                },
                {
                    "sent": "It's a vector that align with one of the coordinate axes, so it's something on one of these black lines vector that has two non zero entries is a vector that's a line on one of the of these planes defined by the coordinator access, so I'm sorry if this is a bit slow, but maybe, but this geometry viewpoints.",
                    "label": 0
                },
                {
                    "sent": "Will turn out to be useful in understanding the behavior of this algorithm and the definition of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the nature of the vectors that we want to recover, but remember we're not observing directly these vectors.",
                    "label": 0
                },
                {
                    "sent": "We observing their low dimensional projection, so we are observing these vectors multiply by this matrix A.",
                    "label": 0
                },
                {
                    "sent": "So there's this collection Sigma K of all vectors with at most K nonzero entries, which is a collection of K dimensional subspaces, and there's N choose K of them.",
                    "label": 0
                },
                {
                    "sent": "When we project it down to the observation domain, the domain of B.",
                    "label": 0
                },
                {
                    "sent": "It's also a union of the same number of subspaces.",
                    "label": 0
                },
                {
                    "sent": "But they will be tilted, so we're it's.",
                    "label": 0
                },
                {
                    "sent": "It's a bit harder to want to display, so they.",
                    "label": 0
                },
                {
                    "sent": "Which what you will observe is will live in this union of combinatorially many low dimensional subspaces.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Cute.",
                    "label": 0
                },
                {
                    "sent": "I think that's the binomial is the is the number of K dimensional subspaces, but this is the number of K dimensional.",
                    "label": 0
                },
                {
                    "sent": "Yeah this is you have to choose.",
                    "label": 0
                },
                {
                    "sent": "Where I mean each of space corresponds to a set of K. Non zero entries.",
                    "label": 1
                },
                {
                    "sent": "And I think you have if you count the ways to choose this space is that is the way of the number of ways you have to.",
                    "label": 0
                },
                {
                    "sent": "You can choose the location of these K nonzero entries among N on their entries.",
                    "label": 0
                },
                {
                    "sent": "But you're welcome to discuss it further.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's something I missed here.",
                    "label": 0
                },
                {
                    "sent": "So where's it could be natural number of such spaces?",
                    "label": 0
                },
                {
                    "sent": "And so, and again, when when you go to the projected lower dimensional subspace space M there still potentially a combinatorial number of spaces and our problem of ideal sparse approximation consists in having a vector B.",
                    "label": 1
                },
                {
                    "sent": "And finding the best approximation to this vector be from this Union of low dimensional subspaces.",
                    "label": 1
                },
                {
                    "sent": "So priore to combinatorial problem because the best naive way we can solve this problem is by going through all possible subspaces, computing the distance, finding the minimum distance, and this gives the solution.",
                    "label": 0
                },
                {
                    "sent": "So priority, it's combinatorial search and it can be shown that in a formalized way, this is actually a intractable problem.",
                    "label": 0
                },
                {
                    "sent": "It's an NP hard.",
                    "label": 0
                },
                {
                    "sent": "It's even an NP complete problem.",
                    "label": 0
                },
                {
                    "sent": "So much we do.",
                    "label": 0
                },
                {
                    "sent": "Should we just resign and do go go somewhere else?",
                    "label": 0
                },
                {
                    "sent": "Of course not this otherwise I wouldn't be giving this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Toyota, so Fortunately there are cases where the problem, even though in general the problem is intractable, there are very difficult instances of the problem where it turns out that under specific assumptions on the Matrix A and the sparsity of X, it is possible to find it with efficient algorithms.",
                    "label": 0
                },
                {
                    "sent": "So now let's talk.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "Well, these algorithms again they will generally try to make a compromise between two quantities.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, they want to minimize the approximation error, so they want to have the best approximation quality.",
                    "label": 0
                },
                {
                    "sent": "Find an X that gives a good approximation quality.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, they like to find an X that is very sparse.",
                    "label": 0
                },
                {
                    "sent": "So we've already seen that the sparsity of X can be measured with the O Norma the number of non zero entries, and it turns out that many algorithm exploits, so called relaxed versions of this sparsity, measures the so called LP norms, especially for P between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Which expression I've recalled here, you're probably familiar with with these LP norms anyway.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to make a compromise between these two.",
                    "label": 0
                },
                {
                    "sent": "Many algorithms.",
                    "label": 0
                },
                {
                    "sent": "Address tradeoff between these.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quantities.",
                    "label": 0
                },
                {
                    "sent": "So at this stage I think, well, maybe you you feel familiar or not with LP norms, but maybe just a quick reminder on norms, maybe, maybe worth it.",
                    "label": 0
                },
                {
                    "sent": "So what is the norm in general A?",
                    "label": 0
                },
                {
                    "sent": "It's a it's a quantity that measures the size of vector and that satisfies three basic properties.",
                    "label": 0
                },
                {
                    "sent": "I mean, who is not familiar with norms?",
                    "label": 0
                },
                {
                    "sent": "OK so I can go really quickly with this.",
                    "label": 0
                },
                {
                    "sent": "The basic properties are the zero vector has zero norm that if the norm of a vector multiplied by a scalar is the normal to play by the absolute value of this camera and the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "So all the standard all the LP norms for P between one and Infinity are normal.",
                    "label": 0
                },
                {
                    "sent": "The satisfied is properties.",
                    "label": 0
                },
                {
                    "sent": "However, the case that we are going to consider prominently when we're looking at sparsity is the case for P between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And for this case, the name Norm is a bit.",
                    "label": 0
                },
                {
                    "sent": "The well is not appropriate, but will use it anyway just for brevity.",
                    "label": 0
                },
                {
                    "sent": "So these are more formally called quasi norms.",
                    "label": 0
                },
                {
                    "sent": "So because they are, they do not satisfy the triangle inequality these LP norms for peace more than one they instead of the triangle inequality the triangle inequality says that the norm of the sum of two vectors is more than the sum of the norms.",
                    "label": 0
                },
                {
                    "sent": "Well they satisfy the quasi triangle inequality and four P gnome.",
                    "label": 0
                },
                {
                    "sent": "The most convenient way of writing that Coke was a triangle inequality is this one.",
                    "label": 0
                },
                {
                    "sent": "When you take the P norm to the peace power for peace more than one.",
                    "label": 0
                },
                {
                    "sent": "When you apply this to the sum of two vectors, you get something that's more than the sum of the P norm to the peace tower.",
                    "label": 0
                },
                {
                    "sent": "This is an essential property of DSP norms, and there are many computations that can be done with the L1 norm that carry two LP norms.",
                    "label": 0
                },
                {
                    "sent": "Piece more than one.",
                    "label": 0
                },
                {
                    "sent": "Because of this basic property.",
                    "label": 0
                },
                {
                    "sent": "So so these four P strictly bigger than zero.",
                    "label": 0
                },
                {
                    "sent": "We have quasi norms and four P = 0.",
                    "label": 0
                },
                {
                    "sent": "Oh well, it's not even a quasi normal, but let's call them let's call it up sudo gnome if we wish because if you multiply a vector by scalar you don't change it L zero norm unless this color is 0.",
                    "label": 0
                },
                {
                    "sent": "So the only thing that remains the only property is this quasi quasi triangle inequality for the L0 norm.",
                    "label": 0
                },
                {
                    "sent": "And again it's something that is going to be very useful for the.",
                    "label": 0
                },
                {
                    "sent": "For establishing inequalities and properties of sparse recovery.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can make your shot pose, here is just.",
                    "label": 0
                },
                {
                    "sent": "To let you capture what this what this means, what what is this question?",
                    "label": 0
                },
                {
                    "sent": "This quasi triangle inequality for the L0 norm?",
                    "label": 0
                },
                {
                    "sent": "It just captures the fact that when you take a two vectors, one vector that has a.",
                    "label": 0
                },
                {
                    "sent": "Key non zero coordinates and you add it to a vector that has L nonzero coordinates.",
                    "label": 0
                },
                {
                    "sent": "Well, you cannot create nonzero coordinates at locations where there was no nonzero coordinates, so the number of nonzero coordinates of the sum of the vectors is just that most the sum of the number of their coordinates.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go back to the problem.",
                    "label": 0
                },
                {
                    "sent": "We want to to the to solve and the algorithm we want to design to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So we want to achieve a tradeoff between approximation quality and sparsity so.",
                    "label": 0
                },
                {
                    "sent": "There are three ways to address it, either by trying to get the best approximation with a given sparsity budget if you wish.",
                    "label": 0
                },
                {
                    "sent": "You can try to find the sparsest solution with a given error budgets or try to find the tradeoff between both and so standard way to achieve this.",
                    "label": 0
                },
                {
                    "sent": "Well to look for this tradeoff is using regularization, regularised specification.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "If, when when the Pinot here is.",
                    "label": 0
                },
                {
                    "sent": "And Norma.",
                    "label": 0
                },
                {
                    "sent": "For example, deal one normal.",
                    "label": 0
                },
                {
                    "sent": "These three formulations are completely exchangeable, you can.",
                    "label": 0
                },
                {
                    "sent": "Because of convexity.",
                    "label": 0
                },
                {
                    "sent": "The solution of this problem with a certain value of two.",
                    "label": 0
                },
                {
                    "sent": "Can be expressed as a solution of this problem with an appropriate choice of epsilon and can also be expressed as the solution of this problem with an appropriate value of Lambda and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Things that will be different when the norm is with the smaller than smaller than one.",
                    "label": 0
                },
                {
                    "sent": "Then the problems are not necessarily equivalent.",
                    "label": 0
                },
                {
                    "sent": "There are solutions of this problem that cannot be achieved that cannot be reached.",
                    "label": 0
                },
                {
                    "sent": "Solutions of these other problems.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Something I wanted to mention in passing, because very often I mean there the equivalence between these problems is known and he's done routinely for convex optimization, but in the cases that we are going to consider is not, well, routine doesn't necessarily work.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The picture that you may have seen before, it's why.",
                    "label": 0
                },
                {
                    "sent": "Why I told you we want to use LP gnomes with P smaller than one?",
                    "label": 0
                },
                {
                    "sent": "Two perform to compute sparse solutions to a linear system of equation.",
                    "label": 0
                },
                {
                    "sent": "So here's the picture.",
                    "label": 0
                },
                {
                    "sent": "Let's look on the on the left.",
                    "label": 0
                },
                {
                    "sent": "So here we are looking at the problem with, well, two unknowns and one observation, but let's think it's a general description of a problem with fewer equations than unknowns.",
                    "label": 0
                },
                {
                    "sent": "There is an affine set of solutions.",
                    "label": 0
                },
                {
                    "sent": "The set of solutions of the problem.",
                    "label": 0
                },
                {
                    "sent": "It's a particular solution plus an elements of the null space of the Matrix A.",
                    "label": 0
                },
                {
                    "sent": "So this is the set of possible solutions an if we look for the minimum L2 norm solution.",
                    "label": 0
                },
                {
                    "sent": "Among all these solutions, it means we're looking for this point.",
                    "label": 0
                },
                {
                    "sent": "'cause if so, to see why this is the minimum L2 norm solution.",
                    "label": 0
                },
                {
                    "sent": "Think of.",
                    "label": 0
                },
                {
                    "sent": "Little balls of increasing radius, so can you find a solution on the blue line that has a very small L2 norm?",
                    "label": 0
                },
                {
                    "sent": "Can you find a solution on that?",
                    "label": 0
                },
                {
                    "sent": "Has a little to normal smaller than this L2 norm?",
                    "label": 0
                },
                {
                    "sent": "No, because there's no intersection.",
                    "label": 0
                },
                {
                    "sent": "So when you blow this bowl.",
                    "label": 0
                },
                {
                    "sent": "At some point, it's the two the two match an you get unique intersection, which is the minimum two norm solution and what you observe here is that well, this L2 norm solution.",
                    "label": 0
                },
                {
                    "sent": "It has two nonzero entries.",
                    "label": 0
                },
                {
                    "sent": "In the middle, do the same thing with the L1 norm, so the minimum and one norm solution is the intersection between this line and the smallest L1 bowl.",
                    "label": 0
                },
                {
                    "sent": "Here the smaller square that.",
                    "label": 0
                },
                {
                    "sent": "With the that intersects this and here you see that.",
                    "label": 0
                },
                {
                    "sent": "It's in the corner of the of their one goal you get, so you get a solution with one nonzero entry, not two and four piece more than one.",
                    "label": 0
                },
                {
                    "sent": "You get nonconvex bulls, but the same phenomenon.",
                    "label": 0
                },
                {
                    "sent": "There are corners in the ball, and so the minimum LP norm solution is sparse.",
                    "label": 0
                },
                {
                    "sent": "So here, sparsity means that well, there are zeros.",
                    "label": 0
                },
                {
                    "sent": "There are many zeros in the solution.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that it's the sparsest.",
                    "label": 0
                },
                {
                    "sent": "It means that.",
                    "label": 0
                },
                {
                    "sent": "It's a sparse solution.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these Lt minimization principles.",
                    "label": 0
                },
                {
                    "sent": "Well, they can be written for a variety of LP norms and for variety of in a variety of former and they lead to different types of of pros and cons.",
                    "label": 0
                },
                {
                    "sent": "So first of all, if you look if you trying to minimize this type of objective function for P. Let's say above one.",
                    "label": 0
                },
                {
                    "sent": "What is nice is that you get a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So convexity and you'll see that with the in the cost of living vandeberg has many nice properties.",
                    "label": 0
                },
                {
                    "sent": "There are efficient algorithms to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "There's a unique.",
                    "label": 0
                },
                {
                    "sent": "Well, there's a unique convex set of solution, and very often a unique global minimum to your problem.",
                    "label": 0
                },
                {
                    "sent": "So many things are very well defined in this regime.",
                    "label": 0
                },
                {
                    "sent": "The problem is that when P is strictly above one, you are not getting a spot solution in general.",
                    "label": 0
                },
                {
                    "sent": "So if we're looking for sparsity.",
                    "label": 0
                },
                {
                    "sent": "Huh, maybe it's better to have a smaller value of P. However, if you go to pee smaller than one, you get a nonconvex problem, so the global minimizer is going to be sparse.",
                    "label": 0
                },
                {
                    "sent": "But the problem is not convex.",
                    "label": 0
                },
                {
                    "sent": "There are local minima everywhere, so it's highly the algorithms that you can derive to solve it well will necessarily depend on how you initialize them so that which raises several several issues.",
                    "label": 0
                },
                {
                    "sent": "In practice, this can inspire a number of.",
                    "label": 0
                },
                {
                    "sent": "Rythms but this raises difficulties and the most extreme cases when you take P = 0 as we've seen solving the problem for P = 0 in general is combinatorial and so very difficult.",
                    "label": 0
                },
                {
                    "sent": "So this is the reason why the most considered cases the case of the minimization with their one.",
                    "label": 0
                },
                {
                    "sent": "Now because it combines the best of both world, it's specifying and convex.",
                    "label": 0
                },
                {
                    "sent": "So with the ability of having algorithms of bounded complexity.",
                    "label": 0
                },
                {
                    "sent": "That provide a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned here a number of persons that contributed to establishing these.",
                    "label": 0
                },
                {
                    "sent": "These algorithms would like to focus in particular on that one minimization it was introduced.",
                    "label": 0
                },
                {
                    "sent": "In machine learning by tips, Yanni in 1996 and don't know, and his coauthors introduced it under the name Basis, pursuit in signal processing.",
                    "label": 0
                },
                {
                    "sent": "But in machine learning, you're probably more familiar with the lawsuit, the name of the zoo.",
                    "label": 0
                },
                {
                    "sent": "The the at at the time when it was introduced, it was essentially exploiting the convexity of the objective function too.",
                    "label": 0
                },
                {
                    "sent": "This convexity was exploited together with generic algorithm coming from linear programming or quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem.",
                    "label": 0
                },
                {
                    "sent": "In this course, we'll see that they are now more specific algorithms that are fully targeted to this type of.",
                    "label": 0
                },
                {
                    "sent": "At one minimization problem that have more specific convergence properties, and that are more efficient than the standard linear programming.",
                    "label": 0
                },
                {
                    "sent": "Generic linear programming solutions.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was the so another view of what type of principles can be used to.",
                    "label": 0
                },
                {
                    "sent": "To find this path solution on the basis of a criterion that you want to minimize.",
                    "label": 0
                },
                {
                    "sent": "So minimizing this criterion that combines approximation with the sparsity penalty.",
                    "label": 0
                },
                {
                    "sent": "Leads to a number of problems, optimization problems and then can fit it to the optimization community to try to find an algorithm that solves your optimization problem.",
                    "label": 0
                },
                {
                    "sent": "That's one major approach to finding solutions.",
                    "label": 0
                },
                {
                    "sent": "There's another set of approaches that are more directly algorithmic.",
                    "label": 0
                },
                {
                    "sent": "They do not rely on a global on on a cost function that you wish to minimize, but somehow in the relay on a more geometric.",
                    "label": 0
                },
                {
                    "sent": "Viewpoint and more algorithmic directly algorithmic perspective.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are the so-called greedy algorithms.",
                    "label": 0
                },
                {
                    "sent": "So these greedy algorithms?",
                    "label": 0
                },
                {
                    "sent": "Well, they have many interpretations, but one way to look at them is to look at the optimization problems we're considering in a case which is very well behaved so.",
                    "label": 0
                },
                {
                    "sent": "The problem that we're going to look at is a problem where we wish to find the best approximation.",
                    "label": 0
                },
                {
                    "sent": "B -- A X India to Norma and their sparsity constraints.",
                    "label": 0
                },
                {
                    "sent": "So X should have at most K nonzero entries an we're going to assume that the Matrix A.",
                    "label": 0
                },
                {
                    "sent": "Is often normal.",
                    "label": 0
                },
                {
                    "sent": "The fact that this matrix is also normal means that the.",
                    "label": 0
                },
                {
                    "sent": "Means that the norm of a X or the norm of a U for general U is the same as the norm of you when you measure in the two norm.",
                    "label": 0
                },
                {
                    "sent": "So it means that this left hand side here that you want to minimize is just the same as the norm of A transpose B -- X.",
                    "label": 0
                },
                {
                    "sent": "So already somehow the A is completely moved.",
                    "label": 0
                },
                {
                    "sent": "You can take your data B.",
                    "label": 0
                },
                {
                    "sent": "Multiply by a transpose and the only thing that remains to be done is to understand which vector X that's case parse is closest to a transpose X.",
                    "label": 0
                },
                {
                    "sent": "Now, if you write this this problem, you can observe that it can be written separated coordinate by coordinate.",
                    "label": 0
                },
                {
                    "sent": "So you want to find the vector X so that the L2 norm here, which is the sum of all coordinates of X of these squared.",
                    "label": 0
                },
                {
                    "sent": "Called Innate, you want to minimize this under the constraint that Axis case bars.",
                    "label": 0
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "Well, if you choose.",
                    "label": 0
                },
                {
                    "sent": "If you put 207 coordinate.",
                    "label": 0
                },
                {
                    "sent": "Then you get an error.",
                    "label": 0
                },
                {
                    "sent": "That's the magnitude of this inner product.",
                    "label": 0
                },
                {
                    "sent": "If you don't put it to zero, well, then since the magnitude of the coefficient doesn't matter in your sparsity constraint, you'd better choose it to minimize this so it will be equal.",
                    "label": 0
                },
                {
                    "sent": "It will set this error term A to 0.",
                    "label": 0
                },
                {
                    "sent": "So what you see is that for each coordinate, either you set it to the value of N transpose B or to 0.",
                    "label": 0
                },
                {
                    "sent": "So in other words, what you have to do is just to allocate where the zeros are, and the best way to do it is to minimize discussed and this can be done by simply computing all these coordinates, sorting them in decreasing order, keeping the K largest or a set of K largest if there are.",
                    "label": 0
                },
                {
                    "sent": "If there is a time and these give you the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "So what you can see is that here whether this optimization problem was.",
                    "label": 0
                },
                {
                    "sent": "Combinatorial in general, here under the assumption that the Matrix A is orthogonal, this becomes a quite simple problem.",
                    "label": 0
                },
                {
                    "sent": "Compute coefficients in the basis A transpose, solve them.",
                    "label": 0
                },
                {
                    "sent": "Keep the K largest and you're done.",
                    "label": 1
                },
                {
                    "sent": "Another way of uh and algorithmic, another algorithmic way of looking at this procedure, is to simply say that you compute the correlation between your B vector an all the columns of your matrix A.",
                    "label": 0
                },
                {
                    "sent": "You find the largest one.",
                    "label": 0
                },
                {
                    "sent": "You set it to 0.",
                    "label": 0
                },
                {
                    "sent": "You find the second largest one you set it to zero, etc, until you found K called K-90 coordinates.",
                    "label": 0
                },
                {
                    "sent": "Well, this is precisely what's the motivation for.",
                    "label": 0
                },
                {
                    "sent": "Behind that, the definition of greedy algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the greedy algorithms and here and describing matching pursuits they will work with more general.",
                    "label": 1
                },
                {
                    "sent": "A that we work in the following way so they are given B the vector that here they want to approximate.",
                    "label": 0
                },
                {
                    "sent": "And they are given the matrix the matrix A.",
                    "label": 0
                },
                {
                    "sent": "And the first thing they do is they compute well they will update the residual.",
                    "label": 0
                },
                {
                    "sent": "But the first thing they do is they take their residual which at the beginning it's just the vector B.",
                    "label": 0
                },
                {
                    "sent": "They compute the inner product between these residual an each column of the Matrix A.",
                    "label": 0
                },
                {
                    "sent": "They find the largest one.",
                    "label": 1
                },
                {
                    "sent": "And then they do the project it out.",
                    "label": 0
                },
                {
                    "sent": "They remove the contribution of this column of the matrix from the from the residual to get the next residual.",
                    "label": 0
                },
                {
                    "sent": "Once this is done, they iterate so.",
                    "label": 0
                },
                {
                    "sent": "Every at every step they need to find the most correlated column of the matrix and then remove it from the residual to get the next best one and stop when they've reached the number of non zero.",
                    "label": 0
                },
                {
                    "sent": "The number of countries that design this is what is called matching pursuits.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this so this was the the definition of matching pursuits was valid for the normal matrix is valid for an arbitrary matrix, we just just do these steps, find the best correlated Atom, remove it from the residual, find the best correlated to the new residual, etc.",
                    "label": 0
                },
                {
                    "sent": "So this has been introduced under the name matching pursuits by Stephen Miller and Defense.",
                    "label": 0
                },
                {
                    "sent": "You're processing, but this is also known as projection pursuits that was defined, I think, by Friedman and Schoesler.",
                    "label": 0
                },
                {
                    "sent": "It's also known as under the name clean in astronomy, and I'm sure it has been invented several, many time.",
                    "label": 0
                },
                {
                    "sent": "Many times under other names.",
                    "label": 0
                },
                {
                    "sent": "What are some of the important properties of this algorithm?",
                    "label": 0
                },
                {
                    "sent": "Is that well, every time it removes a column from the residual?",
                    "label": 0
                },
                {
                    "sent": "Well, it removes it in a way that preserves from all the energy of the residual.",
                    "label": 0
                },
                {
                    "sent": "So the energy is there to Norma and at a given time you have seen.",
                    "label": 0
                },
                {
                    "sent": "So you decompose the residual into the next residual and the projection on the selected Atom.",
                    "label": 0
                },
                {
                    "sent": "By Pythagoras Theorem A you preserve this L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Azar",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll see more on the properties of this algorithm on in the next session, but now let me give you a more global overview of the algorithm that we've discussed so far.",
                    "label": 0
                },
                {
                    "sent": "So there are essentially two categories.",
                    "label": 0
                },
                {
                    "sent": "One category of algorithm is based on a principle where you want to optimize.",
                    "label": 0
                },
                {
                    "sent": "Global cost function.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a principle.",
                    "label": 0
                },
                {
                    "sent": "It needs to be turned into a practical algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the principal A depends.",
                    "label": 0
                },
                {
                    "sent": "Well can be true as several tuning parameters, so there's a parameter.",
                    "label": 0
                },
                {
                    "sent": "This regularization factor that choose the tradeoff that you're achieving between approximation and sparsity.",
                    "label": 0
                },
                {
                    "sent": "If you choose a large value of this penalty factor Lambda, you're forcing to have a very sparse solution at the price of perhaps a very crude approximation, an when this parameter Lambda goes to zero, you're essentially forcing your solution to be exact in terms of approximation, so it will be less sparse.",
                    "label": 0
                },
                {
                    "sent": "In the limit where Lambda goes to 0.",
                    "label": 0
                },
                {
                    "sent": "The optimum corresponds to minimizing this penama and their quality constraints.",
                    "label": 0
                },
                {
                    "sent": "On the other side, you have the family of greedy algorithms, so they are not based on the global optimization principle.",
                    "label": 1
                },
                {
                    "sent": "There based on this iterative procedure, so they are more directly defined in terms of an algorithm, and this algorithm is based on computing iteratively some residuals.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Your little by little, finding the columns of a that must take part in your sparse representation, you're incorporating them progressively until you've reached the number of atoms or the number of columns that are designed.",
                    "label": 0
                },
                {
                    "sent": "So the desired sparsity level, so here.",
                    "label": 0
                },
                {
                    "sent": "The tradeoff between sparsity an approximation is tuned differently tuned by the stopping criterion for your algorithm.",
                    "label": 1
                },
                {
                    "sent": "It is tuned by choosing how many nonzero entries you want, or by tuning the approximation error that you wish to reach.",
                    "label": 0
                },
                {
                    "sent": "Now all these algorithms have several valiance, so regarding global optimization, the violence on in particular determined by what is the choice of the sparsity measure, but also, since these are only.",
                    "label": 1
                },
                {
                    "sent": "Optimization principles the precise are the output of an algorithm is completely determined by.",
                    "label": 0
                },
                {
                    "sent": "I would say that the guts of the algorithm that the precise optimization technique that you're implementing to attempt to minimize this discuss function, and this can have a huge effect on the results.",
                    "label": 0
                },
                {
                    "sent": "In particular, when you optimization the initialization of your algorithm is very.",
                    "label": 0
                },
                {
                    "sent": "Has a huge impact on the other side.",
                    "label": 0
                },
                {
                    "sent": "Therefore greedy algorithms.",
                    "label": 0
                },
                {
                    "sent": "What will have an impact is the way the violence can choose to select such or such column of the matrix.",
                    "label": 0
                },
                {
                    "sent": "There are variant values, accelerating techniques.",
                    "label": 0
                },
                {
                    "sent": "There are techniques where you you're not necessarily looking for the best correlated at home, because this is very costly, so you might want to select.",
                    "label": 0
                },
                {
                    "sent": "A column of the matrix that is well correlated, if not the best.",
                    "label": 0
                },
                {
                    "sent": "And there may also be technically cases where you are too fast to make things faster.",
                    "label": 0
                },
                {
                    "sent": "You will select several nonzero coordinates at a time, so these are called stagewise algorithms.",
                    "label": 0
                },
                {
                    "sent": "Last difference source of variation is that the way you update the residual is not always the one that I described in matching pursuit and will see in the other session the details of other strategies that can provide better performance.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat the question?",
                    "label": 0
                },
                {
                    "sent": "I simply didn't hear it.",
                    "label": 0
                },
                {
                    "sent": "That's a good question, so.",
                    "label": 0
                },
                {
                    "sent": "A When A is often or more.",
                    "label": 0
                },
                {
                    "sent": "So it's a square matrix.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you have a problem where you have the same number of unknowns as the number of equations, but maybe it's simply very badly behaved.",
                    "label": 0
                },
                {
                    "sent": "Well, if it's an invertible matrix.",
                    "label": 0
                },
                {
                    "sent": "Uh, somehow well you can.",
                    "label": 0
                },
                {
                    "sent": "You can do some sort of preconditioning or.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not necessarily going to make a offer normal in the sense that the L2 norm of the the approximation error measured between B.",
                    "label": 0
                },
                {
                    "sent": "My B&AX.",
                    "label": 0
                },
                {
                    "sent": "Is not going to be the approximation error between A-B a -- 1 B&X, but.",
                    "label": 0
                },
                {
                    "sent": "Of course it's it's better behaved if a square if a is not square.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "No no.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was maybe longer review of algorithms and will have a longer detailed presentation of algorithms after the pose, but after the break now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we'll discuss the properties that are the major properties of these algorithms and which are essentially the reasons why they are becoming popular in machine learning and signal processing to solve inverse problems.",
                    "label": 0
                },
                {
                    "sent": "And the reason why a number of algorithms are based on sparsity are becoming popular is because we're not only they work to solve some practical problems, but they are also associated to theoretical guarantees.",
                    "label": 0
                },
                {
                    "sent": "That somehow indicates regimes where we know there they are guaranteed to work.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem that these sparse reconstruction algorithms are trying to solve, there are inverse problems where you're observing.",
                    "label": 0
                },
                {
                    "sent": "A low dimensional projection of objects.",
                    "label": 0
                },
                {
                    "sent": "Data vectors that live in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "These problems would not be.",
                    "label": 0
                },
                {
                    "sent": "It will not be possible to solve them.",
                    "label": 0
                },
                {
                    "sent": "Without a model on the original data that we want to reconstruct, and somehow the model that have these data is displayed, that is by this cross cross shaped.",
                    "label": 0
                },
                {
                    "sent": "Is this is this the set?",
                    "label": 0
                },
                {
                    "sent": "Of the original data that we believe our true original data belongs to.",
                    "label": 0
                },
                {
                    "sent": "So essentially think about vision.",
                    "label": 0
                },
                {
                    "sent": "You are when we're looking at the scene with a single I. Preferably we only see a 2D projection on the retina of this scene, and if if an object is completely in line with the with the optical center, we will completely confuse what's the what's in the back was in the front.",
                    "label": 0
                },
                {
                    "sent": "There's no way we can reconstruct it.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of information, but if we know that the object that we are willing to reconstruct do not have this bad behavior and rather oblique compared to the the direction of projection will be able to exploit this property.",
                    "label": 0
                },
                {
                    "sent": "Well here the sparsity assumption on the object we want to reconstruct together with some properties of the projection matrix.",
                    "label": 0
                },
                {
                    "sent": "Here's what will guarantee that we can do the inverse reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I tried to display it in a more abstract way and so now we've seen there's a number of algorithms we can use to try to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So suppose suppose Francois has chosen an X vector.",
                    "label": 0
                },
                {
                    "sent": "And the only thing I can observe is the projection B of this X vector.",
                    "label": 0
                },
                {
                    "sent": "And now I wish to use a certain algorithm, let's call it algorithm one.",
                    "label": 0
                },
                {
                    "sent": "This algorithm, one sometimes it will succeed in recovering my unknown vector.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it won't succeed.",
                    "label": 0
                },
                {
                    "sent": "So among all possible choices, that password could have matter.",
                    "label": 0
                },
                {
                    "sent": "There's a number of choices that will lead the algorithm to succeed.",
                    "label": 0
                },
                {
                    "sent": "This is this green region.",
                    "label": 0
                },
                {
                    "sent": "So this green regions indicates symbolizes the input vectors that password can choose an so that if I try to reconstruct using algorithm one, I will exactly find solve the puzzle.",
                    "label": 0
                },
                {
                    "sent": "So this is the shape.",
                    "label": 0
                },
                {
                    "sent": "This is the abstract shape of this for algorithm one.",
                    "label": 0
                },
                {
                    "sent": "Now I would like to understand better what's all these vectors?",
                    "label": 0
                },
                {
                    "sent": "What is the shape of this?",
                    "label": 0
                },
                {
                    "sent": "And you see it has a complicated shape.",
                    "label": 0
                },
                {
                    "sent": "It's so Frio, recharacterizing.",
                    "label": 0
                },
                {
                    "sent": "Finally, which vector are recovered by algorithm one and which are not.",
                    "label": 0
                },
                {
                    "sent": "It's a daunting task, and maybe it's not so interesting because in the end it will just tell me well.",
                    "label": 0
                },
                {
                    "sent": "Well, these are the vectors are the best characterization is these are the vectors are there within, one can recover.",
                    "label": 0
                },
                {
                    "sent": "So we want to do something that can be better.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpreted and the way we want to do it is by exploiting the sparsity of X.",
                    "label": 0
                },
                {
                    "sent": "So let's say we want to figure out how sparse vector needs to be to be in this set to be recovered by algorithm one.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we can do is look at from how the level sets of the L0 norm.",
                    "label": 1
                },
                {
                    "sent": "So first of all we can.",
                    "label": 0
                },
                {
                    "sent": "So in the on the left its representation in the usual Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So I'm looking at one sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "They are aligned with the coordinate axes an on the right.",
                    "label": 0
                },
                {
                    "sent": "It's a symbolic description the.",
                    "label": 0
                },
                {
                    "sent": "So this is the bold L 0 ball vectors that have only one non zero coordinate.",
                    "label": 0
                },
                {
                    "sent": "Do they all belong?",
                    "label": 0
                },
                {
                    "sent": "Today set, are they all recovered by this algorithm?",
                    "label": 0
                },
                {
                    "sent": "One yes, because this blue ball is included in the in the green sets.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I can look at two sparse vectors etc.",
                    "label": 0
                },
                {
                    "sent": "And what I can wander is how big can I make my LO bowl and still be guaranteed that these vectors are recovered by algorithm one?",
                    "label": 0
                },
                {
                    "sent": "And here you see that so well.",
                    "label": 0
                },
                {
                    "sent": "OK, for this algorithm sparsity K is recovered because this this Blue Bowl is included.",
                    "label": 0
                },
                {
                    "sent": "But if I were to enlarge this bowl.",
                    "label": 0
                },
                {
                    "sent": "The I would meet this this corner so there would be vectors that are K plus one sparse and that are not recovered.",
                    "label": 0
                },
                {
                    "sent": "So the whole game in getting recovery guarantees for algorithms is 2 characterized how sparse vector.",
                    "label": 0
                },
                {
                    "sent": "Is this possible that is sufficient to guarantee that the algorithm can?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cover it.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, if you take a different.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, you get the different sets of vectors that can be recovered with this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Anne, this different sets.",
                    "label": 0
                },
                {
                    "sent": "As you can see what I try to display in this picture is that well, it just has a different shape.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can play the same game plan for this algorithm.",
                    "label": 0
                },
                {
                    "sent": "You observe that you can make.",
                    "label": 0
                },
                {
                    "sent": "You can find guarantees with less sparse vectors for the recovery with the second algorithm, but it doesn't mean that this second algorithm is universal.",
                    "label": 0
                },
                {
                    "sent": "Universally better than the first one, because there are vectors that the first one can recover and that the second one cannot recover and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So the game that we're going to look at now is how can we characterize these sparsity levels that guarantee that certain specific algorithms specific of re algorithms can recover?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a number of results in the literature and there's an ever growing number of theorems in this field, but most of them share this common structure, so this is the meta theorem on sparse recovery.",
                    "label": 0
                },
                {
                    "sent": "If you wish.",
                    "label": 0
                },
                {
                    "sent": "This is a theorem that says well, and that's the theorem that I expressed here in a noiseless setting, but it's just for simplicity.",
                    "label": 0
                },
                {
                    "sent": "There are equivalents where they are version of this theorem where you allow.",
                    "label": 0
                },
                {
                    "sent": "Our approximation here, not exact representation.",
                    "label": 0
                },
                {
                    "sent": "This theorem established the existence of sparsity levels.",
                    "label": 0
                },
                {
                    "sent": "So that if a vector is sufficiently sparse in terms of L 0 Norma, then if I.",
                    "label": 0
                },
                {
                    "sent": "Project it down and reconstruct it with here.",
                    "label": 0
                },
                {
                    "sent": "Now the first thing the first thing is the following.",
                    "label": 0
                },
                {
                    "sent": "Sorry, they're vector that is so sparse that if I project them down, there's no other representation that is as sparse, so they are the unique spouses solutions.",
                    "label": 0
                },
                {
                    "sent": "So the problem is just well posed.",
                    "label": 0
                },
                {
                    "sent": "And this is this sufficiently sparse.",
                    "label": 0
                },
                {
                    "sent": "Is a function of the projection matrix A?",
                    "label": 0
                },
                {
                    "sent": "When does a US so this is the identifiability of the back of sufficiently sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "Now there's a second level of results in the theorem that are related to practical algorithm that can recover them in here.",
                    "label": 0
                },
                {
                    "sent": "This is for one minimization.",
                    "label": 0
                },
                {
                    "sent": "The minimizer of deal one norm under this exact reconstruction constraints is guaranteed to recover X0, provided that X0 has a sufficiently small L zero norm.",
                    "label": 0
                },
                {
                    "sent": "This is the general shape of this of the theorems, and if I kept it to be generate, it would be essentially a useless theorem, because if I put KO and K1 to be 0, just means that the zero vector is the sparsest solution of the problem.",
                    "label": 0
                },
                {
                    "sent": "0 equals AX.",
                    "label": 0
                },
                {
                    "sent": "This is not a surprise even if I put it to to be one.",
                    "label": 0
                },
                {
                    "sent": "It's essentially an antiserum, so the whole strength of the theorems.",
                    "label": 0
                },
                {
                    "sent": "Will be in characterizing how big.",
                    "label": 0
                },
                {
                    "sent": "Can these dispersity levels be?",
                    "label": 0
                },
                {
                    "sent": "To stand still ensure recover the recovery, the uniqueness and the recovery of these.",
                    "label": 0
                },
                {
                    "sent": "So there's been a number of results exploiting values properties of of the matrix A.",
                    "label": 0
                },
                {
                    "sent": "The first one we are due to do no one rule.",
                    "label": 0
                },
                {
                    "sent": "In 2001.",
                    "label": 0
                },
                {
                    "sent": "They were exploiting the notion of coherence with very specific matrices, a that where the union of two bases and since then the the assumptions have been progressively relaxed.",
                    "label": 0
                },
                {
                    "sent": "An extended from recovery for L12 extra recovery with violence of matching pursuits.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The fact that.",
                    "label": 0
                },
                {
                    "sent": "It is possible.",
                    "label": 0
                },
                {
                    "sent": "To lower the dimension.",
                    "label": 0
                },
                {
                    "sent": "Drastically to take a very high dimensional object to project it in low dimension an if it is sparse enough, it's still possible to reconstruct the object has led to the notion of compressed sensing, which is a new way.",
                    "label": 0
                },
                {
                    "sent": "Well, not so new now, but it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a new trend in how to measure how to capture data.",
                    "label": 0
                },
                {
                    "sent": "So when we do machine learning, we most I would say we we have data, it's been acquired.",
                    "label": 0
                },
                {
                    "sent": "It's somewhere in the database and we want to extract information.",
                    "label": 0
                },
                {
                    "sent": "But somewhere before data exists, it has to be generated and when these data comes from the physical world, there's been an acquisition process.",
                    "label": 0
                },
                {
                    "sent": "This acquisition process we've all learned about Shannon sampling, which is the standard way of acquiring data.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Shannon sampling, yeah performs.",
                    "label": 0
                },
                {
                    "sent": "Summer performs in a way compressing will.",
                    "label": 0
                },
                {
                    "sent": "Completely completely changed the way we capture data.",
                    "label": 0
                },
                {
                    "sent": "For, provided that the data satisfies certain properties.",
                    "label": 0
                },
                {
                    "sent": "So as the basic example, here is the thing, something taken from the Seminole Paper by Countess come back on to where they were considering.",
                    "label": 0
                },
                {
                    "sent": "Well here a toy problem of demography so.",
                    "label": 0
                },
                {
                    "sent": "It's it's and this is an inverse problem.",
                    "label": 0
                },
                {
                    "sent": "First, why is it an inverse problem?",
                    "label": 0
                },
                {
                    "sent": "Because there is there is data and here this is supposed to be the brain of a slice of the brain of somebody.",
                    "label": 0
                },
                {
                    "sent": "This is the shape Logan Phantom.",
                    "label": 0
                },
                {
                    "sent": "I hope it is not my brain to except that with the heat here, maybe it's becoming the shape of my brain.",
                    "label": 0
                },
                {
                    "sent": "But OK, suppose this.",
                    "label": 0
                },
                {
                    "sent": "This is the sum physical object living in the physical world that has.",
                    "label": 0
                },
                {
                    "sent": "The density at certain places that follow this that correspond to this picture and this object is acquired by magnetic magnetic resonance imaging.",
                    "label": 0
                },
                {
                    "sent": "And actually OK here it's not my forget magnetic resonance imaging.",
                    "label": 0
                },
                {
                    "sent": "It's acquired by tomography and the way it's acquired is associated.",
                    "label": 0
                },
                {
                    "sent": "The physical process can be mathematically interpreted as follows.",
                    "label": 0
                },
                {
                    "sent": "You do before you transform of your of your image and you only keep the value of this free transform along lines in the in the free domain.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the mathematical interpretation of the physical process acquisition process.",
                    "label": 0
                },
                {
                    "sent": "In tomography, So what happens is, well, if you knew the whole free transform of the object, you would just have to do the full inversion and you would reconstruct.",
                    "label": 0
                },
                {
                    "sent": "Your object, but you don't know the whole free transform is just like the inpainting problem that I showed before.",
                    "label": 1
                },
                {
                    "sent": "You know, the free transform along these white lines, but you don't know the free transform in the black areas, so you have missing values.",
                    "label": 0
                },
                {
                    "sent": "How do you reconstruct the data?",
                    "label": 0
                },
                {
                    "sent": "First technique would be well.",
                    "label": 0
                },
                {
                    "sent": "Neglect the fact that you don't know the values.",
                    "label": 0
                },
                {
                    "sent": "Put zeros where you don't have values.",
                    "label": 0
                },
                {
                    "sent": "Do inverse Fourier transform and this is what you would get.",
                    "label": 0
                },
                {
                    "sent": "Well, obviously this is not a very good solution and way before compressed sensing, people working in MRI and computed tomography have been developing much better ways of doing reconstruction, but yet there's.",
                    "label": 0
                },
                {
                    "sent": "It's been a surprise the to have all the types of reconstruction based on their one minimization exploiting the principles of sparsity that are associated to guarantees on the algorithms.",
                    "label": 0
                },
                {
                    "sent": "So why can we exploit sparsity at all?",
                    "label": 0
                },
                {
                    "sent": "To solve this inverse problem.",
                    "label": 0
                },
                {
                    "sent": "Oh, I showed a picture, but any idea in the audience whi WHI can we say that the subject is sparse?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is so this is this is an object that is piecewise constants and the boundaries between the piecewise constant areas are regular.",
                    "label": 0
                },
                {
                    "sent": "So it can be described with fair with very few parameters.",
                    "label": 0
                },
                {
                    "sent": "There's a, so this is the first informal way of saying it is.",
                    "label": 0
                },
                {
                    "sent": "It is sparse because it can be described with very few parameters.",
                    "label": 0
                },
                {
                    "sent": "So it's sufficient to identify these very few parameters to reconstruct the object, and here you could say.",
                    "label": 0
                },
                {
                    "sent": "I mean this is a superposition of ellipsoids.",
                    "label": 0
                },
                {
                    "sent": "So if you know the.",
                    "label": 0
                },
                {
                    "sent": "Axis and all the parameters of each of these apes.",
                    "label": 0
                },
                {
                    "sent": "Read if you know the color that you put in his sleep.",
                    "label": 0
                },
                {
                    "sent": "So read the object can be reconstructed.",
                    "label": 0
                },
                {
                    "sent": "So of course you do not want to go in such a parametric model finding.",
                    "label": 0
                },
                {
                    "sent": "I mean not in this case because, well, you could adapt your model to reconstruct this particular image, but what would you get for real brain?",
                    "label": 0
                },
                {
                    "sent": "So the technique is to be a bit more general, and here the fact that this object is regular away from irregular boundaries implies that when you do that, if you were to do the wavelet transform of the subject.",
                    "label": 1
                },
                {
                    "sent": "You would get something that is very sparse, so you would get zero with it coefficients away from the boundaries and significant wavelet coefficients only along the boundaries.",
                    "label": 0
                },
                {
                    "sent": "Yes, there's a question here.",
                    "label": 0
                },
                {
                    "sent": "Why would?",
                    "label": 1
                },
                {
                    "sent": "I don't know if the real brain is sparse.",
                    "label": 0
                },
                {
                    "sent": "This is definitely a toy example.",
                    "label": 0
                },
                {
                    "sent": "1st and then the source.",
                    "label": 0
                },
                {
                    "sent": "This is also true question in the sense that there are many things I will in this tutorial where I will for get in the first 2 lectures I will.",
                    "label": 0
                },
                {
                    "sent": "I will not talk about approximation to the model.",
                    "label": 0
                },
                {
                    "sent": "It will not talk about noise, but in the third lecture I will come back to this.",
                    "label": 0
                },
                {
                    "sent": "The issue I I want to keep the flow of the presentation based on the noiseless Toyota model, but we'll see that many of the results can be extended in a generic way when when the model is not exactly sparse.",
                    "label": 0
                },
                {
                    "sent": "So let me just take the time to answer a bit further question the brainer.",
                    "label": 0
                },
                {
                    "sent": "It's certainly not sparse.",
                    "label": 0
                },
                {
                    "sent": "If you do, even if you do, the wavelet transform of what you could imagine as a full resolution brain image, nothing is here.",
                    "label": 0
                },
                {
                    "sent": "There may still be some significance that are significantly larger than others, but maybe for diagnosis the particular structure of the small coefficients maybe maybe useful.",
                    "label": 0
                },
                {
                    "sent": "What happens is that if the small coefficients are sufficiently small.",
                    "label": 0
                },
                {
                    "sent": "Types of techniques will still be guaranteed to provide the.",
                    "label": 0
                },
                {
                    "sent": "An accurate approximation of the image you are willing to reconstruct.",
                    "label": 0
                },
                {
                    "sent": "Basically, don't care about it.",
                    "label": 0
                },
                {
                    "sent": "Go back.",
                    "label": 0
                },
                {
                    "sent": "Well, I was rather talking about the wavelet transform, but the wavelet domain rather free domain.",
                    "label": 0
                },
                {
                    "sent": "Now saying that there are cases where anyway if your physical acquisition process is like this, huge undersampling.",
                    "label": 0
                },
                {
                    "sent": "I know you're probably might not be to reconstruct exactly the image, because I mean you know you'll need to put a model.",
                    "label": 0
                },
                {
                    "sent": "Your model will never be 100%.",
                    "label": 0
                },
                {
                    "sent": "So I create what you want to do to get is something that's that's as good as it can, and if the if a real images that I mean the data that you want to acquire is not exactly sparse but sufficiently close to sparse, then there are guarantees for this reconstruction algorithm that we have.",
                    "label": 0
                },
                {
                    "sent": "The quality will depend on this how close you are to being exactly sparse.",
                    "label": 0
                },
                {
                    "sent": "So here it's really a toy model an for this time model.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Remember, this is the analog domain you you don't get to.",
                    "label": 0
                },
                {
                    "sent": "You don't know it, but you model it and for this is a social experiment.",
                    "label": 0
                },
                {
                    "sent": "Suppose you had this one you and you were able to do with the transform.",
                    "label": 0
                },
                {
                    "sent": "You would get very few significant coefficients and most of them would be 0 here on this toy model.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "How can you relate this to what you've observed?",
                    "label": 0
                },
                {
                    "sent": "What you will serve as a product of this linear undersampling for you under sampling?",
                    "label": 1
                },
                {
                    "sent": "Measurement system by this, but this is the product of the forward wavelet transform by these coefficients.",
                    "label": 0
                },
                {
                    "sent": "So there's a linear relation between these unknown wavelet coefficients.",
                    "label": 0
                },
                {
                    "sent": "That's your model as being sparse.",
                    "label": 0
                },
                {
                    "sent": "And these observation.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is, given this observation, find the sparsest X that's compatible with this linear observation system.",
                    "label": 0
                },
                {
                    "sent": "An hope that it will be close to the original X, and if you do so and you can do it by solving this optimization problem, finding the minimum and one normal solution to your reconstruction problem.",
                    "label": 0
                },
                {
                    "sent": "This is what Countess Hambergen Tao showed in their Seminole paper.",
                    "label": 0
                },
                {
                    "sent": "You get the reconstruction that exact well for this toy example is exactly the original image are the SNR is something like 300 DB.",
                    "label": 0
                },
                {
                    "sent": "So this well this process.",
                    "label": 0
                },
                {
                    "sent": "I mean, this reconstruction is used when you have this physical acquisition process that's.",
                    "label": 0
                },
                {
                    "sent": "That is negatively reducing the dimension.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have no choice.",
                    "label": 0
                },
                {
                    "sent": "This is the way a tomography proceeds.",
                    "label": 0
                },
                {
                    "sent": "You get a number of slices and you have to reconstruct from from them.",
                    "label": 0
                },
                {
                    "sent": "But this has brought up the idea that maybe if you know that the data you want to capture is actually sparse in a domain that you know, maybe it's possible to design an acquisition system that will not try to not spend much efforts capturing a higher definition image, but simply.",
                    "label": 0
                },
                {
                    "sent": "Capture something a much coarser that will still capture the a sense.",
                    "label": 0
                },
                {
                    "sent": "The information that you need so that in the end you can reconstruct the whole object by solving this type of optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is the so called.",
                    "label": 0
                },
                {
                    "sent": "The circle padding of compressed sensing and this is schematic comparison with classical signals that as a classical sampling in classical Shannon sampling, what could say first you sample high resolution and then you think how you can reduce the volume and compress.",
                    "label": 1
                },
                {
                    "sent": "So you have a high dimensional analogue object.",
                    "label": 0
                },
                {
                    "sent": "You quite a high resolution and once you have the high resolution.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acquisition, you think?",
                    "label": 0
                },
                {
                    "sent": "Well, maybe this is.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe the volume.",
                    "label": 0
                },
                {
                    "sent": "Maybe the number of terabytes I need to store this is too big.",
                    "label": 0
                },
                {
                    "sent": "So let me think what I know about this original object.",
                    "label": 0
                },
                {
                    "sent": "Well, I know that if I do it all the time, frequency transform or wavelet transform it is sparse.",
                    "label": 0
                },
                {
                    "sent": "So actually there is a sparse representation of my object.",
                    "label": 0
                },
                {
                    "sent": "So let me now compute this pulse representation of Z and.",
                    "label": 0
                },
                {
                    "sent": "And code it just as I described at the very beginning of this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lecture.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I if you do this well, there's an encoding step, which is somewhat costly because you have to do a transform of your high definition image or sound and you get this lower bitrate representation, and it's generally designed so that the decoding is very cheap, because if.",
                    "label": 0
                },
                {
                    "sent": "I mean, you acquire once and probably you're going to to decode many times on several several devices.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the way compressed sensing proposes to perform acquisition is different well from the beginning, you know, yeah, you have to know that the data you want to acquire is sparse in some domain that you can that you know.",
                    "label": 1
                },
                {
                    "sent": "So this data that you want to acquire.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not worth acquiring it full resolution.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a way of making a low dimensional projection of the subject that will.",
                    "label": 0
                },
                {
                    "sent": "Exploit the sparsity and preserve the main pro.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the object so that from this low dimensional projection it's possible to reconstruct this vector?",
                    "label": 0
                },
                {
                    "sent": "And if you're able to reconstruct this vector then you are able to reconstruct this.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case you trying to design an acquisition process that is less expensive in terms of resources, number of sensors, etc.",
                    "label": 0
                },
                {
                    "sent": "And this comes at a cost, because when you really want to reconstruct the object, you have to run other types of reconstruction algorithms, and we'll see in the next session that, well, these one minimization or greedy algorithm that can remain quite complex.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for this paradigm for this compressed sensing to work, there are a few necessary hypothesis.",
                    "label": 1
                },
                {
                    "sent": "The first one is that you have to know that your data is sparse in some domain, sufficiently sparse an.",
                    "label": 0
                },
                {
                    "sent": "Whether the brain is part in the sufficiently sparse in some domain, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "You also have to have certain properties of your projection matrix so that the recovery is actually guaranteed for not trivial response vectors, but moderately sparse vectors, and this generally comes from uncertainty principle between the domain where the data is sparse and the domain where it is measured, such as Heisenberg uncertainty principle between time and frequency.",
                    "label": 1
                },
                {
                    "sent": "But other types of uncertainty principles.",
                    "label": 0
                },
                {
                    "sent": "One of these uncertainties.",
                    "label": 0
                },
                {
                    "sent": "Suppose is reached through the design of sensing devices through using some level of randomness randomness brings.",
                    "label": 0
                },
                {
                    "sent": "Some uncertainty principle that are desirable.",
                    "label": 0
                },
                {
                    "sent": "And the last point that is needed is that, well, well, you need your data to be sparse in some domain.",
                    "label": 0
                },
                {
                    "sent": "And you want to reduce the dimension, but you cannot reduce the dimension as much as you as you wish.",
                    "label": 0
                },
                {
                    "sent": "Somehow you must still preserve the amount of information that in your original data, and this turns.",
                    "label": 0
                },
                {
                    "sent": "This can be written in terms of the numbers of measures that you need.",
                    "label": 0
                },
                {
                    "sent": "This number of measures is typically expressed like that in the recovery guarantees, and you'll perhaps recognizer the term that I showed at the beginning of this.",
                    "label": 0
                },
                {
                    "sent": "This Class A, which is essentially the coding costs of a vector that is case sparse in dimension N. So essentially the number of measures you need is given by this.",
                    "label": 0
                },
                {
                    "sent": "It is necessary if you want to have robust reconstruction, you must have this number of of measures an it is sufficient with certain types of random measurement systems.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, I mean there's a log factor.",
                    "label": 0
                },
                {
                    "sent": "It's so so this this K log N / K. Sometimes it appears as.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "Magic, but it's just comes from the fact that the vectors that you want to encode they have a certain entropy.",
                    "label": 0
                },
                {
                    "sent": "TI mean there's a number of bit necessary to code them, so you cannot go.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Below this, so just to summarize this first part.",
                    "label": 0
                },
                {
                    "sent": "So the goal was to show you first the notion of sparsity.",
                    "label": 1
                },
                {
                    "sent": "How it naturally emerged in the context of data compression, where it's naturally related to the notion of economy in terms of number of bits, number of flops to perform computation, and so on, and how in the last 10 years it's been.",
                    "label": 0
                },
                {
                    "sent": "Shown and realized that this is deeply connected with inverse problems and sparsity, or is an hour a it's a property.",
                    "label": 0
                },
                {
                    "sent": "When an object is sparse, it enables a number of.",
                    "label": 1
                },
                {
                    "sent": "Applications related to inverse problems and I'll stop here.",
                    "label": 0
                }
            ]
        }
    }
}