{
    "id": "wu7ag4gfwllkji6fe6azklos2egb5vh5",
    "title": "Language Modeling with Tree Substitution Grammars",
    "info": {
        "author": [
            "Matt Post, University of Rochester"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Programming Languages"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_post_lmtsg/",
    "segmentation": [
        [
            "Hi, my name is Matt and this is joint work with my advisor Daniel Day at the University of Rochester."
        ],
        [
            "OK, so a quick summary of this talk is going to parts in a quick summary of it is.",
            "The first part is that using certain learned tree substitution grammars have lower perplexity and test data, which is something we should expect, but they do so with roughly the same size grammars as treebank as the standard treatment context free grammar.",
            "So."
        ],
        [
            "Just as a quick review, I guess there's a bunch of overlap here with Mark stock, But a standard CFG.",
            "This is just a picture of a rule from the tree Bank, which is about 50,000 sentences that have been manually annotated bilingual.",
            "It's like this and."
        ],
        [
            "So you can see just one example right there in."
        ],
        [
            "Tree substitution grammar is essentially a context free grammar, except that nonterminals can rewrite a sub."
        ],
        [
            "Trees of arbitrary size.",
            "So here the boxed nodes represent boundaries on these rules, so you can see the taskbar, its frontiers on NP, and then the lexical word should and then VP.",
            "So TST is our context free grammars.",
            "They have the same distribution over strings, but we can do different things with them with language modeling.",
            "So really, a big."
        ],
        [
            "Album with tree substitution grammars is how to learn them, so in the in the tree before the treebank annotations, the context free grammar, the standard treatment grammar is just."
        ],
        [
            "Implicit in the parse trees.",
            "But with trees, tree substitution grammars, we have to find away basically to find hidden biscuit, cut up the tree and find out where the boundaries between the rules are to find the derivation that we think produced that.",
            "So 11 approach popular approach is done pretty well as the data Orient Orient parsing approach where you essentially take all subtrees as your as your grammar.",
            "And actually I mean this goes way back before 2001, but there's a long line of work by run spot in doing that and another."
        ],
        [
            "Approach could do is EM where you just kind of guess the derivations and literally iteratively guest the derivations and account subtrees.",
            "But so problems common to both of these approaches is 1.",
            "Is that we?",
            "It's over fitted to the training data, so we have just grammars that don't generalize as well as we'd like to, and also we just have a representation problem.",
            "There's an exponential number of subtrees in the tree Bank, so actually representing them in memory is a problem so we like."
        ],
        [
            "Better way to do that, so a number of groups did that this year and.",
            "We had a paper at ACL where we adapted Sharon Goldwater's work for segmentation doing final logical segmentation to this task, which is essentially a segmentation task as well so we can address the overfitting problem by using dear slave process prior that discourages large subtrees.",
            "So here we just access the non terminal and we sample the rewrites for each.",
            "Non terminal.",
            "Using a base, measure that.",
            "The first outside the product is a geometric distribution on the number of rules that constitute each subtree, and then the real properties themselves.",
            "We get from maximum likelihood estimate from the tree bank.",
            "And so there's other ways you could do that, but that's the."
        ],
        [
            "We did it and we address space efficiency.",
            "Is this approach if use collapsed Gibbs sampling that as Mark mentioned, that that addresses this problem because we have to maintain counts for rules that are exist in derivations from the current state of of the corpus."
        ],
        [
            "So here's just a picture of sampling from two different initialization initialization points.",
            "The top is just the treebank initialization and the bottom is spinal spinal grammar, which I'll discuss in a second, and you can see just going through the first 100 iterations.",
            "How this how these were visiting every node in turn and sampling whether we should make that a boundary point or not, and.",
            "So that this is just doing 100 iterations and you can see that it hasn't.",
            "Really these trees are actually quite different, so that's there's a couple issues related to this, which I'll discuss, but one of them is that you get.",
            "You do get different grammars depending on."
        ],
        [
            "Where you stop and whether you take a single point estimate or sum over them."
        ],
        [
            "So, but one interesting aspect from those grammars is so here's a picture of if you parse the 23rd section of the Wall Street Journal and then read out all the rules that you used in parsing it and then plot a histogram by subtree size where size is the number of lexical items in each rule.",
            "These are two plots that you get.",
            "It's a log scale on the Y axis, so one is the DOP grammar and the circles are the sampled grammar.",
            "So this is kind of a.",
            "It distribution you might expect, but then if you look."
        ],
        [
            "The actual grammars themselves, the DLP grammar, just taking that as representative of a previous approach has just this huge grammar, which is basically flat across the whole.",
            "Yet you have all these items with 12 lexical items on them which don't generalize very well whereas."
        ],
        [
            "The picture for the sampled grammar is much more natural and efficient to represent."
        ],
        [
            "OK, so the experiments for this.",
            "For this paper, um, so we just we took three days."
        ],
        [
            "Grammar is one.",
            "Is the treatment grammar and then we used a spinal grammar which is just taking each lexical word an maximally, projecting it up.",
            "So you get.",
            "Us every subtree is 1 lexical item on them and it goes up to the highest internal that has that word as as its head."
        ],
        [
            "And then we also have a sample grammar and these are just three different pictures of.",
            "What does?",
            "What the derivation state for a particular subpar stream?",
            "The training data look like?"
        ],
        [
            "And so, here we've graphed the size of these grammars versus the perplexity in the test data, and the major point here is that the treebank grammar.",
            "Has you can see that as the square on the top and the sampled grammar, it has much lower perplexity, and it hasn't lost much in terms or hasn't hasn't grown much in terms of size.",
            "And then we also have variations with the spinal in the treebank grammar.",
            "In the sampled grammar we also add in all accounts from the tree bank rules.",
            "No, those are the whole the whole on binary string subtrees.",
            "I'm sorry.",
            "Would know this is actually just parsing OK, so when I parsed the rules were binary idea, but then I.",
            "You know unflattened them into the whole rules.",
            "So, but some of these rules do have like skeletons on them, but the model is not a lexicalized model.",
            "And so the spinal we had to add the treebank that'll I'll talk about that more.",
            "In second we had the tree bank accounts in just because it was too brittle to build up to parse the entire Test Corp."
        ],
        [
            "With.",
            "So we don't have spirit.",
            "Be nice to you know.",
            "Perplexities, kind of stand in for actual language, model performance on a task like machine translation and we were able to do that for here.",
            "So instead we generated some pseudo random text and then look at the perplexity scores on what is essentially grammatical garbage so.",
            "Here's an example sentence.",
            "The way we generated the pseudorandom text.",
            "Is we took NP's and just randomly permuted them and that seems kind of arbitrary, but it's just the first thing we tried and we wanted to do something syntactic in contrast to somebody else who did this for training discriminative language models where negative examples were sampled from a trigram model."
        ],
        [
            "So if you parse, if you then get scores on this here better is in the upper left hand corner and this is basically just rotated on as a mirror image.",
            "So you want worse perplexity scores, ungrammatical text, worse being higher, and so we basically get the same thing that repair the sample grammar isn't much bigger, but it has much.",
            "Thanks, this ungrammatical sentences are worse.",
            "But this is again basically just flipped or mirrored image over a horizontal axis."
        ],
        [
            "OK, so just as a preface to talking with the second part of the paper.",
            "So if you look at these tree substitution grammars that that you learn, I said earlier.",
            "Basically, context free grammars because you can take these and just the internal nodes of a subtree are irrelevant when you're doing parsing language modeling.",
            "They don't contribute anything, so you can just remove them, and that's actually what we do when we parse with it.",
            "And if you want to test parse accuracy, you just maintain that mapping and put him back into score against the gold standard trees.",
            "But you don't need to do that when you're doing.",
            "Beginning perplexity measures, so you can kind of look at this as.",
            "Initial structural learning is that we've stripped out the tree substitution grammar that was learned, stripped out nodes that it didn't think were contributed to this.",
            "So here you can see just any node.",
            "That snap box has been removed and we have a slightly different tree, and that's the one that we parse with after after finalizing it."
        ],
        [
            "OK, so with that observation, what we were kind of curious is that we know that by lexical parsing models are just much better parsers than a standard PC FG model.",
            "So the question is if this is learning slightly different structures, we wonder if training those more complicated by lexical grammars on these newer structures would give us further increase in.",
            "Or decrease in perplexity, which is an improvement.",
            "And they do."
        ],
        [
            "So that's there's some caveats to that, but I'll, I'll get to that.",
            "So just a quick review.",
            "CFG parsing."
        ],
        [
            "The generative models you just replace each non terminal with all of his children in one act and that has a probability associated with it.",
            "And then you just recurse and take it down to the leaves with by lexical."
        ],
        [
            "Parsing hits a bit more complicated, so you generate one particular model which counts Model 1 so you have an internal you generate it's.",
            "Its head tag and head word."
        ],
        [
            "And then from there you generate the head child and then you."
        ],
        [
            "2.",
            "Then you generate the sibling label."
        ],
        [
            "It had tags and you do all those independently and then each of these is conditioned on a lot of the history."
        ],
        [
            "And then finally generate the headwords of each of these head tags and labels and it's called by lexical parsing.",
            "Because in this last probability.",
            "Distribution here we have or conditioning the probability of word on the headward.",
            "So we have this.",
            "Lexical dependency, which is computed computed over these structures."
        ],
        [
            "And then you just recurse.",
            "So these parsing models do much better than a standard model and parsing."
        ],
        [
            "You see, and also have lower perplexity.",
            "So one point before we show we train these accounts.",
            "Model training procedures.",
            "Expect expect free terminals above all lexical items.",
            "So in order to do that, when you have rules that have lexical items in them, you have to introduce this deterministic pre terminal above lexical items so you can see how we've done that in this grammar transformation.",
            "We take this rule here.",
            "Anta so basically any any any word that doesn't have a pre terminal above it.",
            "We introduced a deterministic pre terminal that expands directly to that lexical item."
        ],
        [
            "And then we can train the counts model over it.",
            "So in this is a."
        ],
        [
            "Is showing a problem that arises when you do that.",
            "So this is basically a problem that's inherent to trying to use these tree substitution grammars with words on the frontiers which we have shown are better for perplexity, but we can't adapt this better model to take advantage of them as much as we'd like, and part of the reason."
        ],
        [
            "That is because of the way the back off structure is defined in the columns model.",
            "So there's this three."
        ],
        [
            "Level interpolation, where when you generate.",
            "The label and head tag of a sibling child.",
            "Its condition first."
        ],
        [
            "Done the parent label in the head label Head Tag and had word and then whether it's left or right of the head, and then you interpolate that with a more limited estimate.",
            "So normally when you do this you get less sparse statistics, but when you have lexical items on the frontier, you this interpolation structure doesn't give you less sparse statistics because we still have this even in the third level of this interpolation we still have this by lexical dependency because of those.",
            "Deterministic pre terminals that were introduced so because that we couldn't parse."
        ],
        [
            "With the spinal grammar at all and what we had to do, and we've had problems with some of the sample grammars and we had to especially just take all the subtrees that we learned.",
            "And instead of extracting the lexicalized subtree we as a work around to this, we just detached the."
        ],
        [
            "Lexical items, so they became separate rules.",
            "So before we had this sbar rule that although we went all the way down to should.",
            "And now it's a bigger rule, but it only goes down to the NP and then the model tag and the VP.",
            "And so then we have a separate rule of MD goes to should, so we're losing what we would suspect is the part of the advantage of this model, which is the lexicalization."
        ],
        [
            "We still get some improvement in perplexity scores using this parsing model trained on these sampled grammars, so the D denotes this detachment, so again, we have the treatment grammar, and we have a sampled in the sample plus tree Bank, and so basically.",
            "Hi perplexity.",
            "Actually, oh, I'm sorry.",
            "So here is plotted the scores on the grammatical versus the ungrammatical.",
            "So you want low perplexity on the grammatical and high perplexity on the ungrammatical pseudo pseudo text, so upper left is better than we see.",
            "Getting real counts from the sample plus spinal does the best here.",
            "And the it's it's harder to analyze exactly what's going on to speak, partly because the model is more complicated.",
            "But there's definitely just what I went over as a clear problem.",
            "It's something that we could probably fix to be able to generalize these.",
            "More complicated parsing Model 2."
        ],
        [
            "These different structures that are learned with tree substitution grammars.",
            "So that's all.",
            "The absolute values of the complexities seem on the high side and how does it compare to like standard track record?",
            "So it's all about the in the paper I have the trigram bigram, so it's well above that and I didn't put in the talk, so people have used parsers.",
            "As tried, evaluated them for Plex it before turning at a paper where you did by lexical parsing and get improved over the baseline so.",
            "I think there's I mean, I think there's improvement in there to be had so.",
            "So in here we had well above the.",
            "The trigram baseline, but.",
            "I mean, you could get something better.",
            "Yeah, yeah, and it's hard to compare with their previous work 'cause they had a different size vocabulary which makes it hard to compare perplexities and things like that so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Matt and this is joint work with my advisor Daniel Day at the University of Rochester.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so a quick summary of this talk is going to parts in a quick summary of it is.",
                    "label": 0
                },
                {
                    "sent": "The first part is that using certain learned tree substitution grammars have lower perplexity and test data, which is something we should expect, but they do so with roughly the same size grammars as treebank as the standard treatment context free grammar.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just as a quick review, I guess there's a bunch of overlap here with Mark stock, But a standard CFG.",
                    "label": 1
                },
                {
                    "sent": "This is just a picture of a rule from the tree Bank, which is about 50,000 sentences that have been manually annotated bilingual.",
                    "label": 0
                },
                {
                    "sent": "It's like this and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can see just one example right there in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree substitution grammar is essentially a context free grammar, except that nonterminals can rewrite a sub.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trees of arbitrary size.",
                    "label": 0
                },
                {
                    "sent": "So here the boxed nodes represent boundaries on these rules, so you can see the taskbar, its frontiers on NP, and then the lexical word should and then VP.",
                    "label": 0
                },
                {
                    "sent": "So TST is our context free grammars.",
                    "label": 0
                },
                {
                    "sent": "They have the same distribution over strings, but we can do different things with them with language modeling.",
                    "label": 0
                },
                {
                    "sent": "So really, a big.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Album with tree substitution grammars is how to learn them, so in the in the tree before the treebank annotations, the context free grammar, the standard treatment grammar is just.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Implicit in the parse trees.",
                    "label": 1
                },
                {
                    "sent": "But with trees, tree substitution grammars, we have to find away basically to find hidden biscuit, cut up the tree and find out where the boundaries between the rules are to find the derivation that we think produced that.",
                    "label": 0
                },
                {
                    "sent": "So 11 approach popular approach is done pretty well as the data Orient Orient parsing approach where you essentially take all subtrees as your as your grammar.",
                    "label": 0
                },
                {
                    "sent": "And actually I mean this goes way back before 2001, but there's a long line of work by run spot in doing that and another.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach could do is EM where you just kind of guess the derivations and literally iteratively guest the derivations and account subtrees.",
                    "label": 1
                },
                {
                    "sent": "But so problems common to both of these approaches is 1.",
                    "label": 0
                },
                {
                    "sent": "Is that we?",
                    "label": 0
                },
                {
                    "sent": "It's over fitted to the training data, so we have just grammars that don't generalize as well as we'd like to, and also we just have a representation problem.",
                    "label": 1
                },
                {
                    "sent": "There's an exponential number of subtrees in the tree Bank, so actually representing them in memory is a problem so we like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better way to do that, so a number of groups did that this year and.",
                    "label": 0
                },
                {
                    "sent": "We had a paper at ACL where we adapted Sharon Goldwater's work for segmentation doing final logical segmentation to this task, which is essentially a segmentation task as well so we can address the overfitting problem by using dear slave process prior that discourages large subtrees.",
                    "label": 1
                },
                {
                    "sent": "So here we just access the non terminal and we sample the rewrites for each.",
                    "label": 0
                },
                {
                    "sent": "Non terminal.",
                    "label": 0
                },
                {
                    "sent": "Using a base, measure that.",
                    "label": 0
                },
                {
                    "sent": "The first outside the product is a geometric distribution on the number of rules that constitute each subtree, and then the real properties themselves.",
                    "label": 0
                },
                {
                    "sent": "We get from maximum likelihood estimate from the tree bank.",
                    "label": 0
                },
                {
                    "sent": "And so there's other ways you could do that, but that's the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did it and we address space efficiency.",
                    "label": 0
                },
                {
                    "sent": "Is this approach if use collapsed Gibbs sampling that as Mark mentioned, that that addresses this problem because we have to maintain counts for rules that are exist in derivations from the current state of of the corpus.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's just a picture of sampling from two different initialization initialization points.",
                    "label": 0
                },
                {
                    "sent": "The top is just the treebank initialization and the bottom is spinal spinal grammar, which I'll discuss in a second, and you can see just going through the first 100 iterations.",
                    "label": 1
                },
                {
                    "sent": "How this how these were visiting every node in turn and sampling whether we should make that a boundary point or not, and.",
                    "label": 0
                },
                {
                    "sent": "So that this is just doing 100 iterations and you can see that it hasn't.",
                    "label": 0
                },
                {
                    "sent": "Really these trees are actually quite different, so that's there's a couple issues related to this, which I'll discuss, but one of them is that you get.",
                    "label": 0
                },
                {
                    "sent": "You do get different grammars depending on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where you stop and whether you take a single point estimate or sum over them.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but one interesting aspect from those grammars is so here's a picture of if you parse the 23rd section of the Wall Street Journal and then read out all the rules that you used in parsing it and then plot a histogram by subtree size where size is the number of lexical items in each rule.",
                    "label": 0
                },
                {
                    "sent": "These are two plots that you get.",
                    "label": 0
                },
                {
                    "sent": "It's a log scale on the Y axis, so one is the DOP grammar and the circles are the sampled grammar.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of a.",
                    "label": 0
                },
                {
                    "sent": "It distribution you might expect, but then if you look.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual grammars themselves, the DLP grammar, just taking that as representative of a previous approach has just this huge grammar, which is basically flat across the whole.",
                    "label": 0
                },
                {
                    "sent": "Yet you have all these items with 12 lexical items on them which don't generalize very well whereas.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The picture for the sampled grammar is much more natural and efficient to represent.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the experiments for this.",
                    "label": 0
                },
                {
                    "sent": "For this paper, um, so we just we took three days.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grammar is one.",
                    "label": 0
                },
                {
                    "sent": "Is the treatment grammar and then we used a spinal grammar which is just taking each lexical word an maximally, projecting it up.",
                    "label": 1
                },
                {
                    "sent": "So you get.",
                    "label": 0
                },
                {
                    "sent": "Us every subtree is 1 lexical item on them and it goes up to the highest internal that has that word as as its head.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we also have a sample grammar and these are just three different pictures of.",
                    "label": 0
                },
                {
                    "sent": "What does?",
                    "label": 0
                },
                {
                    "sent": "What the derivation state for a particular subpar stream?",
                    "label": 0
                },
                {
                    "sent": "The training data look like?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, here we've graphed the size of these grammars versus the perplexity in the test data, and the major point here is that the treebank grammar.",
                    "label": 0
                },
                {
                    "sent": "Has you can see that as the square on the top and the sampled grammar, it has much lower perplexity, and it hasn't lost much in terms or hasn't hasn't grown much in terms of size.",
                    "label": 0
                },
                {
                    "sent": "And then we also have variations with the spinal in the treebank grammar.",
                    "label": 0
                },
                {
                    "sent": "In the sampled grammar we also add in all accounts from the tree bank rules.",
                    "label": 0
                },
                {
                    "sent": "No, those are the whole the whole on binary string subtrees.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Would know this is actually just parsing OK, so when I parsed the rules were binary idea, but then I.",
                    "label": 0
                },
                {
                    "sent": "You know unflattened them into the whole rules.",
                    "label": 0
                },
                {
                    "sent": "So, but some of these rules do have like skeletons on them, but the model is not a lexicalized model.",
                    "label": 0
                },
                {
                    "sent": "And so the spinal we had to add the treebank that'll I'll talk about that more.",
                    "label": 0
                },
                {
                    "sent": "In second we had the tree bank accounts in just because it was too brittle to build up to parse the entire Test Corp.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "So we don't have spirit.",
                    "label": 0
                },
                {
                    "sent": "Be nice to you know.",
                    "label": 0
                },
                {
                    "sent": "Perplexities, kind of stand in for actual language, model performance on a task like machine translation and we were able to do that for here.",
                    "label": 0
                },
                {
                    "sent": "So instead we generated some pseudo random text and then look at the perplexity scores on what is essentially grammatical garbage so.",
                    "label": 0
                },
                {
                    "sent": "Here's an example sentence.",
                    "label": 0
                },
                {
                    "sent": "The way we generated the pseudorandom text.",
                    "label": 1
                },
                {
                    "sent": "Is we took NP's and just randomly permuted them and that seems kind of arbitrary, but it's just the first thing we tried and we wanted to do something syntactic in contrast to somebody else who did this for training discriminative language models where negative examples were sampled from a trigram model.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you parse, if you then get scores on this here better is in the upper left hand corner and this is basically just rotated on as a mirror image.",
                    "label": 0
                },
                {
                    "sent": "So you want worse perplexity scores, ungrammatical text, worse being higher, and so we basically get the same thing that repair the sample grammar isn't much bigger, but it has much.",
                    "label": 0
                },
                {
                    "sent": "Thanks, this ungrammatical sentences are worse.",
                    "label": 0
                },
                {
                    "sent": "But this is again basically just flipped or mirrored image over a horizontal axis.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just as a preface to talking with the second part of the paper.",
                    "label": 0
                },
                {
                    "sent": "So if you look at these tree substitution grammars that that you learn, I said earlier.",
                    "label": 0
                },
                {
                    "sent": "Basically, context free grammars because you can take these and just the internal nodes of a subtree are irrelevant when you're doing parsing language modeling.",
                    "label": 0
                },
                {
                    "sent": "They don't contribute anything, so you can just remove them, and that's actually what we do when we parse with it.",
                    "label": 0
                },
                {
                    "sent": "And if you want to test parse accuracy, you just maintain that mapping and put him back into score against the gold standard trees.",
                    "label": 0
                },
                {
                    "sent": "But you don't need to do that when you're doing.",
                    "label": 0
                },
                {
                    "sent": "Beginning perplexity measures, so you can kind of look at this as.",
                    "label": 0
                },
                {
                    "sent": "Initial structural learning is that we've stripped out the tree substitution grammar that was learned, stripped out nodes that it didn't think were contributed to this.",
                    "label": 0
                },
                {
                    "sent": "So here you can see just any node.",
                    "label": 0
                },
                {
                    "sent": "That snap box has been removed and we have a slightly different tree, and that's the one that we parse with after after finalizing it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so with that observation, what we were kind of curious is that we know that by lexical parsing models are just much better parsers than a standard PC FG model.",
                    "label": 0
                },
                {
                    "sent": "So the question is if this is learning slightly different structures, we wonder if training those more complicated by lexical grammars on these newer structures would give us further increase in.",
                    "label": 0
                },
                {
                    "sent": "Or decrease in perplexity, which is an improvement.",
                    "label": 0
                },
                {
                    "sent": "And they do.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's there's some caveats to that, but I'll, I'll get to that.",
                    "label": 0
                },
                {
                    "sent": "So just a quick review.",
                    "label": 0
                },
                {
                    "sent": "CFG parsing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The generative models you just replace each non terminal with all of his children in one act and that has a probability associated with it.",
                    "label": 0
                },
                {
                    "sent": "And then you just recurse and take it down to the leaves with by lexical.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parsing hits a bit more complicated, so you generate one particular model which counts Model 1 so you have an internal you generate it's.",
                    "label": 0
                },
                {
                    "sent": "Its head tag and head word.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then from there you generate the head child and then you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Then you generate the sibling label.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It had tags and you do all those independently and then each of these is conditioned on a lot of the history.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then finally generate the headwords of each of these head tags and labels and it's called by lexical parsing.",
                    "label": 1
                },
                {
                    "sent": "Because in this last probability.",
                    "label": 0
                },
                {
                    "sent": "Distribution here we have or conditioning the probability of word on the headward.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "Lexical dependency, which is computed computed over these structures.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you just recurse.",
                    "label": 0
                },
                {
                    "sent": "So these parsing models do much better than a standard model and parsing.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see, and also have lower perplexity.",
                    "label": 0
                },
                {
                    "sent": "So one point before we show we train these accounts.",
                    "label": 0
                },
                {
                    "sent": "Model training procedures.",
                    "label": 0
                },
                {
                    "sent": "Expect expect free terminals above all lexical items.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that, when you have rules that have lexical items in them, you have to introduce this deterministic pre terminal above lexical items so you can see how we've done that in this grammar transformation.",
                    "label": 0
                },
                {
                    "sent": "We take this rule here.",
                    "label": 0
                },
                {
                    "sent": "Anta so basically any any any word that doesn't have a pre terminal above it.",
                    "label": 0
                },
                {
                    "sent": "We introduced a deterministic pre terminal that expands directly to that lexical item.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can train the counts model over it.",
                    "label": 0
                },
                {
                    "sent": "So in this is a.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is showing a problem that arises when you do that.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a problem that's inherent to trying to use these tree substitution grammars with words on the frontiers which we have shown are better for perplexity, but we can't adapt this better model to take advantage of them as much as we'd like, and part of the reason.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is because of the way the back off structure is defined in the columns model.",
                    "label": 0
                },
                {
                    "sent": "So there's this three.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Level interpolation, where when you generate.",
                    "label": 0
                },
                {
                    "sent": "The label and head tag of a sibling child.",
                    "label": 0
                },
                {
                    "sent": "Its condition first.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done the parent label in the head label Head Tag and had word and then whether it's left or right of the head, and then you interpolate that with a more limited estimate.",
                    "label": 0
                },
                {
                    "sent": "So normally when you do this you get less sparse statistics, but when you have lexical items on the frontier, you this interpolation structure doesn't give you less sparse statistics because we still have this even in the third level of this interpolation we still have this by lexical dependency because of those.",
                    "label": 0
                },
                {
                    "sent": "Deterministic pre terminals that were introduced so because that we couldn't parse.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the spinal grammar at all and what we had to do, and we've had problems with some of the sample grammars and we had to especially just take all the subtrees that we learned.",
                    "label": 0
                },
                {
                    "sent": "And instead of extracting the lexicalized subtree we as a work around to this, we just detached the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lexical items, so they became separate rules.",
                    "label": 0
                },
                {
                    "sent": "So before we had this sbar rule that although we went all the way down to should.",
                    "label": 0
                },
                {
                    "sent": "And now it's a bigger rule, but it only goes down to the NP and then the model tag and the VP.",
                    "label": 1
                },
                {
                    "sent": "And so then we have a separate rule of MD goes to should, so we're losing what we would suspect is the part of the advantage of this model, which is the lexicalization.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We still get some improvement in perplexity scores using this parsing model trained on these sampled grammars, so the D denotes this detachment, so again, we have the treatment grammar, and we have a sampled in the sample plus tree Bank, and so basically.",
                    "label": 0
                },
                {
                    "sent": "Hi perplexity.",
                    "label": 0
                },
                {
                    "sent": "Actually, oh, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So here is plotted the scores on the grammatical versus the ungrammatical.",
                    "label": 0
                },
                {
                    "sent": "So you want low perplexity on the grammatical and high perplexity on the ungrammatical pseudo pseudo text, so upper left is better than we see.",
                    "label": 0
                },
                {
                    "sent": "Getting real counts from the sample plus spinal does the best here.",
                    "label": 0
                },
                {
                    "sent": "And the it's it's harder to analyze exactly what's going on to speak, partly because the model is more complicated.",
                    "label": 0
                },
                {
                    "sent": "But there's definitely just what I went over as a clear problem.",
                    "label": 0
                },
                {
                    "sent": "It's something that we could probably fix to be able to generalize these.",
                    "label": 0
                },
                {
                    "sent": "More complicated parsing Model 2.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These different structures that are learned with tree substitution grammars.",
                    "label": 0
                },
                {
                    "sent": "So that's all.",
                    "label": 0
                },
                {
                    "sent": "The absolute values of the complexities seem on the high side and how does it compare to like standard track record?",
                    "label": 0
                },
                {
                    "sent": "So it's all about the in the paper I have the trigram bigram, so it's well above that and I didn't put in the talk, so people have used parsers.",
                    "label": 0
                },
                {
                    "sent": "As tried, evaluated them for Plex it before turning at a paper where you did by lexical parsing and get improved over the baseline so.",
                    "label": 0
                },
                {
                    "sent": "I think there's I mean, I think there's improvement in there to be had so.",
                    "label": 0
                },
                {
                    "sent": "So in here we had well above the.",
                    "label": 0
                },
                {
                    "sent": "The trigram baseline, but.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could get something better.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, and it's hard to compare with their previous work 'cause they had a different size vocabulary which makes it hard to compare perplexities and things like that so.",
                    "label": 0
                }
            ]
        }
    }
}