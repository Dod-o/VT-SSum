{
    "id": "eokzsgcgkizogrxwl5lvfqtslbxkxfvz",
    "title": "Approximate Inference in Natural Language Processing",
    "info": {
        "author": [
            "Noah Smith, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_smith_ainlp/",
    "segmentation": [
        [
            "Talk about.",
            "Approximate inference in natural language processing and the work that I'm going to be presenting is joint with my students Kevin Gimpel and Andre Martine and my colleague at CMU Eric Xing, who will be talking about other work later in."
        ],
        [
            "Workshop this is kind of bad form, but I think the organizers made a mistake.",
            "When they invited me to give a talk in this workshop, but the mistake is actually helpful to illustrate, so I'm actually not known so much for approximate inference algorithms and that kind of thing that differ for unsupervised and like variable."
        ],
        [
            "Parsing.",
            "So I think they actually confused me with this guy.",
            "David Smith, who I've collaborated with several times.",
            "We've been doing really nice work with loopy belief propagation to solve NLP problems.",
            "So that was kind of my first instinct.",
            "And then I thought, you know, going to Whistler might not be so bad.",
            "So I decided to come anyway, but I feel I feel obligated to point out that that David's been doing really nice work in this area.",
            "You should.",
            "You should definitely read this paper.",
            "I'm not going to talk about any of that stuff.",
            "Instead, I'm going to promote my own work and work for my group.",
            "But the point is that if you don't like the talk, it's not my fault.",
            "Organizers fault and it's due to faulty natural language processing in humans."
        ],
        [
            "OK, so I'm going to start out talking about natural language processing, and in particular the problem of parsing, which I believe is sort of the the prototypical natural language processing problem, and I'm going to give it to you in an idealized form, and we're going to pretend that the world is perfect and we can use exact inference.",
            "And then I'm going to break that down into different ways and talk about a connection between dynamic programming with semirings and approximate inference, and then finally parsing with integer linear programming and relaxation, which I think is a familiar topic to this crowd."
        ],
        [
            "OK, so this is this is an unfortunately brief introduction to a very rich field.",
            "Parsing has to do with linguistically analyzing sentences, and so at the bottom of the slide I have a picture of a sentence in English, and these these little arrows represent syntactic dependencies among the words.",
            "And if you ask two different linguists, you'll get three different opinions about exactly how you should best draw these arrows.",
            "But for now, we'll assume that this is a sort of solid, well understood representation.",
            "And it sort of gets at the syntax of natural language.",
            "Notice that chuckled as an argument professor because it's the subject and we is another argument of chuckled because it has to do with the way that the chuckling is happening.",
            "And so on.",
            "And so the point is that these these kinds of representations are now being used very broadly in many, many problems of natural language.",
            "Processing any applications in particular translation, which I'll come back to later in the talk."
        ],
        [
            "So so to 1st approximation we can represent the dependency parsing problem as parsing with a context free grammar.",
            "So here I've I've introduced nonterminals one for word and written a little Chomsky normal form context free grammar in which non terminals rewrite and you can see all the dependencies come out OK.",
            "I won't do that again.",
            "All the dependencies come out in in the binary rules, and then if you put if you have unary rules and you put probabilities on these things, you can model geometric balance distributions.",
            "Over how many?",
            "How many kids each word is going to?"
        ],
        [
            "Have and so on.",
            "And so once you go to this context, free grammar representation parsing can be done using probabilistic CKY, which I'm representing here with instead of recursive equations.",
            "If you're not familiar with the CKY algorithm, you should basically just think of it as a sort of dynamic programming approach to solving weighted parsing problems.",
            "Uh, the you know the runtime is cubic, which in NLP we consider to be pretty good.",
            "And then we use bottom up dynamic programming techniques.",
            "Solve this and then there are specialized versions that I'm not going to get into in too much detail for dependencies and you can make them cubic with strong independence assumptions and so on."
        ],
        [
            "A nice generalization of this.",
            "This sort of Max product algorithm is to replace the Max and the product with semiring.",
            "Operations are similar in class in the semiring times and this gives us an abstraction that let's us talk about finding modes, finding marginals, finding entropies, doing loss, augmented inference, and it's all still using bottom up dynamic programming.",
            "So this is mainly building on work by Joshua Goodman about 10 years ago that I think has been has been really important to understanding how we can.",
            "Think very generically about the kinds of algorithms we need in natural language processing, and there's been recent work by Soto connecting the CKY algorithm, in particular to belief propagation."
        ],
        [
            "And so you know, this is this is a graphical models.",
            "What we what we like to do these days is replace those probabilities on the rules with little factor functions that look at little pieces of structure in the parse tree, and I circled these in red and these are little exponentials of linear functions, so this is this is essentially becoming a log linear model or an exponential model, or a MAXENT model, or a global linear model.",
            "Or you know these things have a million different names and we're not using them pretty widely for parsing problems in NLP, and there are a lot of papers in the past few years that have.",
            "Sort of adapted this for the parsing framework.",
            "So while I'm not going to represent the problem as a graphical model, it's a very similar parsing, has very similar field linear representations of our of a probability distribution, and we want to do some kind."
        ],
        [
            "Reasoning for learning in person.",
            "So I'm going to give you a quick picture of how that.",
            "Just because we're going to get into these algorithms a little bit of how context free dependency parsing works.",
            "So we we like to use these graphical representations so, so you have the words at the bottom and we start out by drawing little half triangles over each word, and our goal is to get a really big half triangle that covers the whole sentence and that that will mean that we have.",
            "We have shown that there's a partial."
        ],
        [
            "Under grammar.",
            "And then we have these inference rules that tell us how to put together the little triangles to build these trapezoid things, and each trap is."
        ],
        [
            "You can think of as a possible attachment, one word to another.",
            "OK, and we're once.",
            "Once you've built the trapezoid, you sort of forget about everything that's underneath, and that's not building on the mark of independence assumptions that are inherent in your car."
        ],
        [
            "And there's another rule that lets you put together trapezoid triangles to make other triangle."
        ],
        [
            "If you do this as you do this, what you're what you're actually doing is you're multiplying together the two things that you that you're combining, and then this gives you an update to the value of the thing that you constructed.",
            "So if I'm putting A&B together, I semiring multiplied together their weights.",
            "And then I semiring add that that result for the value of."
        ],
        [
            "So if I keep."
        ],
        [
            "Going, I'm just going to."
        ],
        [
            "Quickly go through."
        ],
        [
            "If I did everything right, then I end up with a an entire structure that covers the entire sentence and each of my."
        ],
        [
            "So it's first ones exactly one of the ways, and so there's a one to one correspondence between entire."
        ],
        [
            "Chrzan parsers.",
            "And so inference for many definitions of inference involves finding a semiring some, and we can do this in cubic time."
        ],
        [
            "OK, so that's sort of the core algorithm, and once you have that ability to to solve inference, then this is how you build a parser.",
            "You first get a few $1,000,000 and you pay some smart people who know something about linguistics to sit down and come to consensus about the correct annotations of say, a million sentence is this is done at Upenn about 10 or 15 years ago 15 years ago, and then you use you train some kind of statistical model from those training examples and we have tons of ways of doing this.",
            "Is huge literature on generative and discriminative.",
            "Approaches to training these parsing models, and of course, all of that requires a thrust said earlier inference you've gotta, you gotta do inference on inside most training algorithms.",
            "And then at the end you get your model and you parse using your favorite introduction.",
            "So that's where that's where this dynamic programming algorithm will come in again, and then you measure accuracy against against the gold standard is a very typical machine learning training path scenario and widely accepted."
        ],
        [
            "Is it?",
            "OK, so just to make this kind of college terrible picture.",
            "So this is a very long sentence for Moby Dick.",
            "That one of my graduate students wasted an hour parsing by hand and to give you a sense of I am sorry I should have put the text, John, but you know, it's some long rambly."
        ],
        [
            "Sentence.",
            "Run this through, you know, really good automatic parser that came out of Stanford.",
            "It took about 10 minutes or half gigabytes of memory, and it it got about 1/3 of the words attached incorrectly.",
            "This is the problem is not."
        ],
        [
            "OK so so far I have given you this idealized picture where you think dynamic programming with the exact inference, But this is very unsatisfying.",
            "Computational linguists like me are never satisfied with with the model.",
            "We really like to be dealing with richer features.",
            "They can look at larger parts of the substructure through richer formalisms, and breaking some of the really strong independence assumptions that let us do dynamic programming in the first place.",
            "And we're also interested, you know, parsing again is just one problem.",
            "We're really interested in other problems as well.",
            "So if you look at other languages, you have word structure.",
            "We'd like to integrate morphology would like to start representing meaning in more rich ways.",
            "Movie on syntax, and so all all kinds of linguistic structures might."
        ],
        [
            "Be considered here and context free grammars are not going to be the end of the story.",
            "So so I have to give this card.",
            "I'm being a bit brazen by telling you that most of NLP can be thought of as parsing problem more and more problems in natural language processing are looking like parsing, and I think that's the right direction for a long time.",
            "People in machine learning when they thought about structure prediction, they thought about sequence labeling and I think I think now it's helpful to have sort of a few more harder problems.",
            "And parsing is sort of the next thing in the sequence in LP, but there's there's considerably more we could do in terms of other structures."
        ],
        [
            "Very clear.",
            "OK, so now I'm going to get to the new stuff.",
            "So I'm going to take this dynamic programming framework and this is work by my student Kevin Gimpel and we're going to try and generalize this to include nonlocal features of the structures that look at larger regions."
        ],
        [
            "Love the tree.",
            "OK, so so with with dynamic programming it's really easy to incorporate any feature of our of our Heart Street that looks only at one edge at a time.",
            "So we called.",
            "These are factored or highly local features.",
            "So knowing what the parents of a particular our relationship is or the parent and the child or how far apart they are, how far apart they are.",
            "Or there's the word context of either of them.",
            "This is all really easy.",
            "We can fit this."
        ],
        [
            "To the model without any pain.",
            "If we want to look at multiple edges and have these second order interactions, then things become more tricky, and dynamic programming becomes considerably more expensive.",
            "So knowing that chuckled and Gleann with our triple, that should all kind of go together and having a feature that can look at that entire substructure much more."
        ],
        [
            "Spent.",
            "Here's a an example of a sibling if you want to have a feature that knows that castigation is primarily a relationship between professors and students, you need a second order feature that model sibling interactions.",
            "That was supposed."
        ],
        [
            "Joe.",
            "Of and then you can.",
            "You can actually get there.",
            "There's recent recent examples in the literature of highly nonlocal features that are there are very extreme.",
            "You look at two adjacent words like the start symbol here and the first word of the sentence, and you want it.",
            "You want to know the entire tree path between the two, and this is this is been seen as a helpful feature for parsing, but it's incredibly expensive to do that because there could be arbitrary distance three wise between two adjacent words."
        ],
        [
            "OK, so the more nonlocal features are the higher the polynomial order of your dynamic programming algorithm, and in the limit, if you have arbitrarily nonlocal features, you can't do dynamic programming is it becomes intractable.",
            "So I'm going to start by talking a little bit about a solution for machine translation, called Cube pruning developed by David Chang and being clogged.",
            "The.",
            "The idea is that you're going to keep an approximate K best list.",
            "Of the complete structures for each of these little pieces you're building in your dynamic programming algorithm, and I'm not actually going to pruning.",
            "I'm going to present more than I even simpler version of it called Cube decoding."
        ],
        [
            "So remember when we put together 2 items and we still see.",
            "A bigger a bigger item.",
            "What we have to do is semiring multiply the values of A&B together.",
            "And then we're going to semiring, add that value to C. So we're going to.",
            "We're going to let the values instead of being just a real number.",
            "They're going to be hailing vectors of scored partial structures, so we now have to keep track of a vector of numbers and a structure."
        ],
        [
            "Each one, and we're going to consider the cross product.",
            "So for example, if I'm putting together Andy and I have three different ways that K = 3 of constructing each one, I'm going to have kept track of the entire structure all the way down to the little triangles at the bottom of the screen, and then in the example earlier on.",
            "And I'm going to take the cross product of these two things and I just take the cross product of the values of each of the structures and this tells me if I were to put together this particular trapezoid, but that particular triangle and everything below this would be the product of those two structures, and then I have to multiply."
        ],
        [
            "Ryan well, if that were it then.",
            "Then I would take the three highest scoring.",
            "The three high scoring things in the grid and that would give me my K best list and my set of structure for this year is a valid city ring value because I have three different structures intended valuable.",
            "But I actually."
        ],
        [
            "Multiply in the the feature rate, so here I'm multiplying in the non local and the local feature weights for each of these complete structure.",
            "So the local feature weights are actually the same.",
            "They can't look below the top level item of A or B, they can't look into the into the substructure, but the nonlocal features can, so that's kind of the point.",
            "You can look all the way down as far."
        ],
        [
            "And so once I've I've rescored everything based on the nonlocal features that the game changes, and now the winner is actually here.",
            "There and this one has made its way into the fold."
        ],
        [
            "So there's a different best list because the nonlocal features have weighed in and told me that I should prefer a different data structures, and so I get a different answer."
        ],
        [
            "OK, so so what's nice about this is that I'm keeping the logical structure of the dynamic programming algorithm.",
            "My equations haven't changed, so I can think about the algorithm the same way that I did before, but this is approximate and I have I unfortunately don't have any formal guarantees if I'm using nonlocal features.",
            "All I can say is this very week thing that as I increase K, I'm going to get better and better approximation.",
            "So if someone in the room has ideas about how to formalize this and get some stronger guarantee or some some.",
            "Under some under some conditions, get a stronger guarantee.",
            "I'd be really interested to hear about that.",
            "But then you know.",
            "So this is only part of the story.",
            "This gives us sort of an approximate way of getting paid best inference, but if I'm really interested in something or getting all the somehow accounting for all the exponentially many other structures, then this doesn't really help me.",
            "So our extension to this is called cubes."
        ],
        [
            "Thing and it it adds an additional firm.",
            "So the idea is that in addition to my pay best list, I'm going to keep a value that is my approximate sum of all the other ways of building this trapezoid and all the other ways of building that triangle."
        ],
        [
            "OK, and so once I have I do the cross product as before and then I take the case that's set of things in the inside just."
        ],
        [
            "I didn't understand the coding.",
            "And then I collapse everything else from the K ^2 -- K ways of putting these things together, and that's going to be part of the new residual.",
            "And then they want to account for all the ways of putting everything else together.",
            "That's not accounted for by this these 99 different.",
            "Combination.",
            "So I send out the cross."
        ],
        [
            "Product and I can think of all of these as sort of trying to account for all the other parents."
        ],
        [
            "And then at this point I can actually put in any nonlocal features because I can't see down below.",
            "This is abstracted and I don't know what the subscriptions are, but I can multiply in the non local local fees so I do that."
        ],
        [
            "And that gives me a sum over over everything else that's approximate.",
            "So so for these guys I've taken into account the nonlocal features and that's going to individual term, and for these guys I've only taken."
        ],
        [
            "Your account the local 1."
        ],
        [
            "I add them together and that gives me like my result.",
            "Yeah.",
            "It's it's on a different scale because we failed to take into account all the factors, that's right.",
            "We're not going to worry about it yet."
        ],
        [
            "OK, so.",
            "So again, if I only have local features and you can show that all of this is exactly you have a semiring, the residual is exact.",
            "Everything is fine.",
            "As soon as you introduce non local feature is not a semiring anymore, you're you're you lose the associativity and distributivity properties that you represent that you need to have a semiring again if he goes to Infinity residual, it is zero and the K best list is exactly.",
            "So this is kind of a generalization of the sum product in the K best semirings.",
            "One other nice thing about this is that if you need first derivatives, you can get them through a relatively simple transformation, and they're going to be exact with respect to the approximation that you've done, so there's a lot more detail in the in the ACL paper from last winter.",
            "So right so this is sort of come back to this point.",
            "There are a lot of theoretical questions here.",
            "I don't know how I don't have any guarantees apart from is K is is arbitrarily large, then this, then this is correct I there's more, there's more for more work formal."
        ],
        [
            "To be done here, but I'm going to give you is an empirical argument that there's something good going on.",
            "So, so at a very high level in one slide I can't do this topic justice.",
            "There's been a lot of work and a lot of interest in the idea of using statistical models to translate sentences from one language to another, and the two.",
            "The two main ways of doing that involve translating it sort of a continuous phrase level and sort of thinking of translation is translating phrases and reordering them, and the other the other approaches to use syntactic structure.",
            "On one or both sides and transform one string into a tree or a tree into a string or a tree into a tree.",
            "And so these these two ideas for awhile were sort of competition and now people are realizing that they're actually quite similar.",
            "What we wanted to do is build a single model that was a log linear model that took into account both kinds of both kinds of structure phrases and also grammar rules.",
            "And just think of them both.",
            "Those kinds of features.",
            "So we have a dynamic programming backbone that based on lattice dependency parsing.",
            "So in this model the syntactic features are local.",
            "The phrase ones in the language model aren't and then it's a log linear model with hidden variables.",
            "We trained with pseudo likelihood, there's a."
        ],
        [
            "More to say you should see the MLP paper if you're interested.",
            "This is the.",
            "This is the graph that that I think makes makes a compelling case.",
            "What we have here is 4 different models trained with some of the features turned on or off.",
            "So the full model is the purple one at the top and then the other three are using different subsets of the features and what you see is that as you as you increase K for decoding you get improved.",
            "Performance up is good, the blue score is a standard metric for measuring performance machine translation.",
            "Performance automatically.",
            "But the important I think an important thing to notice is that you don't get as much gain increasing pay for the syntactic model because it's already local.",
            "It doesn't.",
            "You don't actually need the non local approximations, be just just because most most of the features are local.",
            "All of this was trained with people to 10 and do something.",
            "It remains to be seen if actually rather expensive, as pay increases because runtime is going to be quadratic in K. How how things will change as you very fast.",
            "So this there's no."
        ],
        [
            "But I think that the punchline here, about about translation, at least, is that having having these rich nonlocal features is way more important than getting exact inference with those features.",
            "So just including them in the model and doing something approximate usually benefits you, and this is, I think it's fair to say that this has been widely understood as as NLP research continues to want more and more nonlocal features, we find it's better to throw them in somehow and approximate."
        ],
        [
            "OK, so the the last part of the talk.",
            "Is going to come back to parsing.",
            "And we're going to.",
            "We're going to completely ditch the dynamic programming formulation and think of parsing as integer linear programming and use relaxation.",
            "This is work by Andrea Martinez."
        ],
        [
            "And also urging.",
            "So.",
            "Going back to the idea that I said, I think in the second slide that you could think of parsing the first approximation as being.",
            "Context free parsing for natural languages.",
            "This when you when you decide you're going to use context free grammar, you are implicitly making an assumption that when you draw your parts tree, you're only getting consider parse trees where the edges going cross.",
            "We call that projectivity constraint.",
            "But non projective structures where the edges cross like in this example are not too hard to find in English.",
            "It's not too common, but in some other languages it is.",
            "It is considerably more common, and so 11 sort of important development in the field has been the consideration of these non protective parsing algorithms that can second come up with."
        ],
        [
            "With non protected part.",
            "And so some really nice work.",
            "Over the past few years has developed exact inference algorithms for non projective parsing when the edges are conditionally independent.",
            "So again when the features are all arc factor.",
            "So if it's pointed out by Ryan McDonald and colleagues in 2005 that the Max inference problem here is essentially can be solved using a maximum directed spanning tree algorithm like choosing Edmonds in quadratic time and then summing inferences found out concurrently by three different groups that the matrix tree theorem would, with straightforwardly solve the summing problem.",
            "Again, this is all assuming that the agents are conditionally independent given the structure is a tree."
        ],
        [
            "OK, so those tricks only work when the features are arc local.",
            "An result by McDonald's and Sada shows that as soon as we try to do anything more fancy, save with second order features, non protective parsing becomes NP hard.",
            "Exactly.",
            "So our goal here is to get efficient non projective parsing with arbitrary features and I'm only going to focus on Max imprints and by extension loss augmented, McAfee active."
        ],
        [
            "So.",
            "This this idea of solving something parsing problems with integer linear programming is actually telling Clark in 2006, and they pose.",
            "The problem isn't as initial linear program with exponentially many constraints and you're cutting planes.",
            "And that's the crux of this is that you have binary variables for each of the possible attachments.",
            "So so if you have lengthened sentence, you have N squared of these variables or order and squared.",
            "These variables that turn on when word I attaches to work day.",
            "So pretty pretty simple.",
            "And then you you have constraints that force this to be a tree and that's the tricky part.",
            "So recent development from Andre and myself and our exciting is that you can.",
            "You can actually get a concise IO P with a polynomial number of constraints by replacing constraints, trying to impose that the graph is acyclic.",
            "With a constraint that it's connected and this is, this reduces to something called single commodity flow in integer, integer linear programming literature.",
            "Because our loss function, which is usually attachment accuracy, also factors very well.",
            "The mass and the Max loss augmented inference problem is also easily solved.",
            "In this framework you reduce the whole problem.",
            "Dial P and there are all kinds of cool extensions you can do, so I don't have time to go into this and I'm guessing this this audience isn't quite as interested in these tricks that you can use multi commodity flow to have hard constraints or features.",
            "For Projectivity, you can prefer projective parses but allow non projective ones.",
            "You can get higher order.",
            "Features using linear linearization tricks, you can have grandchild and sibling and balanced features, and all of these things tend to give you."
        ],
        [
            "Games in your in your parking performance.",
            "So OK, so you represent the problem as an integer program and then the standard thing that people do once they once they do this is to use an LP relaxation.",
            "So I'm going to skip vegan represent the convex Hull of the set of vertices each.",
            "Each vertex is a valid part.",
            "And then I'm going to represent the outer polytope by the bar.",
            "And the idea is that this is our concise representation, but it might introduce some additional fractional parts vertices that are not good solutions that those are.",
            "Those are the dangerous things we want to stay away from."
        ],
        [
            "So to parse the overall infra saga rhythms as first, you solve the relax Lt and if you get an integral solution that you should be happy you're done, you've got that you've got the solution you want.",
            "If not, you have to.",
            "You have to project and get a nearby approximate best free and in that case we we end up using the two Lu Edmonds algorithm again in solving a spanning tree problem.",
            "This is the more expensive case you want to avoid.",
            "I'm not sure it might be the case that in the arc factor case.",
            "I think that well in in the architecture case I I'm not actually thought about that in the art factory case.",
            "It might be that that all your solutions say integral, but I'm not sure.",
            "I wouldn't exactly.",
            "I mean you if you are factored, you don't need this right?",
            "You just yeah, I'm not sure."
        ],
        [
            "OK, so so here's the we're going to do this in a in a Max margin framework, so we want to minimize some.",
            "You know, quadratic regularization term plus.",
            "An average across the training examples, and I'm not.",
            "I'm not going to go into details this.",
            "Our sub TFW is just the teeth example.",
            "Then you have to solve the inverse problem right?",
            "And for from that you're going to get a subgradient and you're going to take a step.",
            "We're going to do it online setting.",
            "But our our point here is that we really want to avoid this fractional vertices, so the hypothesis is that by by trying to.",
            "If you look at the relaxation gap or the difference between your approximate inference in your true inference scores, the arts of P and the R bar, where were you?"
        ],
        [
            "Franklin"
        ],
        [
            "In the outer polytope.",
            "Minimizing that should diminish your computational costs.",
            "That's kind of the key idea.",
            "So what we do is we enter."
        ],
        [
            "Is there a penalty term in the objective function?",
            "For the relaxation gap.",
            "So this is, this is the score of influences that the infant store under approximately printing this season for under exact inference, and if you do a little bit of algebraic.",
            "Rearranging affirms what you got is a combination between exact and approximate inference.",
            "Instead of a 1 -- 8 here and eight here, and this leaves very nicely to a stochastic online algorithm, building building on the online subgradient method of Ratliff at all from 2006.",
            "We're going to flip a coin when every time we look at an example and with probability ADA we're going to solve the approximate inference inference problem and the probability 1 minus ETA.",
            "We're going to solve the exact login problem.",
            "And so we're going to spend a little bit more energy doing exact inference from time to time by solved by getting getting the exact solution to the to the IOP.",
            "But in the in the long run, most of the time because it is going to be close to one, we're mostly going to be exactly."
        ],
        [
            "And so there's sort of a geometric interpretation here.",
            "If in expectation what you're actually doing is you're maximizing over a polytope that the linear combination of points between Z&Z bar.",
            "So we'll call this detail data and you can see it sort of.",
            "Intuitively, there's been an integral vertices util dot A dot R, sort of closer to the integer vertices infusion.",
            "Trying to push yourself closer to good solution.",
            "You might still get fractional solutions, but hopefully they're going to be closer to the integral one."
        ],
        [
            "OK, so skipping over a lot of details, it works pretty well.",
            "This is these are our scores on seven different languages.",
            "This gives you some idea where the state of the art of parsing is.",
            "We're sort of in the low 90s, high 80s range partitions pretty hard.",
            "Add.",
            "But the the the baselines are pretty solid work from the past McDonald's 2nd order model that builds on the MST work and previous work for my group using stacking.",
            "And what we find is that exact inference.",
            "4 cases out of seven does slightly better, significantly better than earlier work, and if we use approximate inference, we lose very, very little.",
            "Maybe maybe a 10th of a point on the accuracy score.",
            "This is with the.",
            "I believe this is A to equal to 1.",
            "All approximate."
        ],
        [
            "So right so then you want to know what happens when you when you change Ada and what we did was we measured the relaxation gap on the test data for one of these languages, right?",
            "I'm sorry I don't know which one and what we find is that as a double scored one as you're doing more approximate inference, you get a tighter relaxation gap."
        ],
        [
            "Kind of.",
            "Good to see and then also sort of you know this is the part that gets natural language processing people excited as you increase ADA, you actually see faster test inference even when you're doing when you're doing exactly for the test.",
            "I'm having trained the model with this additional relaxation gap term.",
            "If you considerable speedups at Test time, and you can't really see it, but there's a green line at the bottom.",
            "There's a similar is a similar pattern, but it's much closer to 0 because approximate inference is so fast for this problem, it's a half a second or something percent."
        ],
        [
            "And so you know, just to give you some information about what's getting fixed.",
            "So here's another sentence, and the little guy gets frightened.",
            "The big guys hurt badly, and you know most most people who understand these structures would say that this.",
            "This attachment, this little phrase, when the little guy gets pregnant, is really attached here to work her modifier the are factored model gets it wrong.",
            "The richer featured model gets it right."
        ],
        [
            "And another one kind of a nice example.",
            "The art factored model thinks of this sentence.",
            "We learned a lesson in 1987 about volatility as being kind of like.",
            "We learned a lesson in economics about volatility, where the.",
            "Temporal modifier attaches to lessen as opposed to to the verb, and it should catch the verb in the.",
            "This is the model that has sibling features can get this right.",
            "OK, so I think.",
            "Leave a little bit of time for questions.",
            "I'm going to wrap up so, so proud."
        ],
        [
            "Ballistic models and linguistic structures and trying to do automatic linguistic analysis have a very natural affinity.",
            "This is very hard problem, and probabilistic reasoning has been incredibly helpful in making advances on it.",
            "I'm actually working on a short book, since this is lecture lecture that should be coming out next year on linguistic structure predictions.",
            "If you want to know more about this general set of problems.",
            "Watch for that.",
            "Today's models are very rich in terms of their features and they require approximate inference.",
            "We've really been doing approximate inference in natural language processing for as long as we've been using probabilistic models, but now we're starting to realize that there are these connections in that this is a well studied topic in machine learning, and hopefully we'll start giving something back and using more easy to understand and easy to analyze techniques.",
            "Up one thing that's that's important to remember when you look at work in natural language processing is that are.",
            "Target variable that we're trying to predict is often somewhat controversial.",
            "Like I said, any two computational linguists we're going to have three different opinions about the right representation, and so when you get annotated data, that's linguistic data.",
            "It's often very useful, and you can do interesting things with it, but it didn't come from God.",
            "It came from linguists.",
            "So computational representations are always evolving, something to be aware of.",
            "I think there's a lot more to be done in this area of developing generic declarative frameworks for approximating hard NLP problems, and so I look forward to hearing your thoughts, and I guess we'll hear a lot more about this today, so thanks.",
            "Thank my."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference in natural language processing and the work that I'm going to be presenting is joint with my students Kevin Gimpel and Andre Martine and my colleague at CMU Eric Xing, who will be talking about other work later in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Workshop this is kind of bad form, but I think the organizers made a mistake.",
                    "label": 0
                },
                {
                    "sent": "When they invited me to give a talk in this workshop, but the mistake is actually helpful to illustrate, so I'm actually not known so much for approximate inference algorithms and that kind of thing that differ for unsupervised and like variable.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parsing.",
                    "label": 0
                },
                {
                    "sent": "So I think they actually confused me with this guy.",
                    "label": 1
                },
                {
                    "sent": "David Smith, who I've collaborated with several times.",
                    "label": 0
                },
                {
                    "sent": "We've been doing really nice work with loopy belief propagation to solve NLP problems.",
                    "label": 1
                },
                {
                    "sent": "So that was kind of my first instinct.",
                    "label": 0
                },
                {
                    "sent": "And then I thought, you know, going to Whistler might not be so bad.",
                    "label": 0
                },
                {
                    "sent": "So I decided to come anyway, but I feel I feel obligated to point out that that David's been doing really nice work in this area.",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "You should definitely read this paper.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to talk about any of that stuff.",
                    "label": 0
                },
                {
                    "sent": "Instead, I'm going to promote my own work and work for my group.",
                    "label": 0
                },
                {
                    "sent": "But the point is that if you don't like the talk, it's not my fault.",
                    "label": 0
                },
                {
                    "sent": "Organizers fault and it's due to faulty natural language processing in humans.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to start out talking about natural language processing, and in particular the problem of parsing, which I believe is sort of the the prototypical natural language processing problem, and I'm going to give it to you in an idealized form, and we're going to pretend that the world is perfect and we can use exact inference.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to break that down into different ways and talk about a connection between dynamic programming with semirings and approximate inference, and then finally parsing with integer linear programming and relaxation, which I think is a familiar topic to this crowd.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is this is an unfortunately brief introduction to a very rich field.",
                    "label": 0
                },
                {
                    "sent": "Parsing has to do with linguistically analyzing sentences, and so at the bottom of the slide I have a picture of a sentence in English, and these these little arrows represent syntactic dependencies among the words.",
                    "label": 0
                },
                {
                    "sent": "And if you ask two different linguists, you'll get three different opinions about exactly how you should best draw these arrows.",
                    "label": 0
                },
                {
                    "sent": "But for now, we'll assume that this is a sort of solid, well understood representation.",
                    "label": 0
                },
                {
                    "sent": "And it sort of gets at the syntax of natural language.",
                    "label": 1
                },
                {
                    "sent": "Notice that chuckled as an argument professor because it's the subject and we is another argument of chuckled because it has to do with the way that the chuckling is happening.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And so the point is that these these kinds of representations are now being used very broadly in many, many problems of natural language.",
                    "label": 1
                },
                {
                    "sent": "Processing any applications in particular translation, which I'll come back to later in the talk.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so to 1st approximation we can represent the dependency parsing problem as parsing with a context free grammar.",
                    "label": 1
                },
                {
                    "sent": "So here I've I've introduced nonterminals one for word and written a little Chomsky normal form context free grammar in which non terminals rewrite and you can see all the dependencies come out OK.",
                    "label": 0
                },
                {
                    "sent": "I won't do that again.",
                    "label": 0
                },
                {
                    "sent": "All the dependencies come out in in the binary rules, and then if you put if you have unary rules and you put probabilities on these things, you can model geometric balance distributions.",
                    "label": 1
                },
                {
                    "sent": "Over how many?",
                    "label": 0
                },
                {
                    "sent": "How many kids each word is going to?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have and so on.",
                    "label": 0
                },
                {
                    "sent": "And so once you go to this context, free grammar representation parsing can be done using probabilistic CKY, which I'm representing here with instead of recursive equations.",
                    "label": 1
                },
                {
                    "sent": "If you're not familiar with the CKY algorithm, you should basically just think of it as a sort of dynamic programming approach to solving weighted parsing problems.",
                    "label": 0
                },
                {
                    "sent": "Uh, the you know the runtime is cubic, which in NLP we consider to be pretty good.",
                    "label": 0
                },
                {
                    "sent": "And then we use bottom up dynamic programming techniques.",
                    "label": 1
                },
                {
                    "sent": "Solve this and then there are specialized versions that I'm not going to get into in too much detail for dependencies and you can make them cubic with strong independence assumptions and so on.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A nice generalization of this.",
                    "label": 0
                },
                {
                    "sent": "This sort of Max product algorithm is to replace the Max and the product with semiring.",
                    "label": 0
                },
                {
                    "sent": "Operations are similar in class in the semiring times and this gives us an abstraction that let's us talk about finding modes, finding marginals, finding entropies, doing loss, augmented inference, and it's all still using bottom up dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So this is mainly building on work by Joshua Goodman about 10 years ago that I think has been has been really important to understanding how we can.",
                    "label": 0
                },
                {
                    "sent": "Think very generically about the kinds of algorithms we need in natural language processing, and there's been recent work by Soto connecting the CKY algorithm, in particular to belief propagation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you know, this is this is a graphical models.",
                    "label": 0
                },
                {
                    "sent": "What we what we like to do these days is replace those probabilities on the rules with little factor functions that look at little pieces of structure in the parse tree, and I circled these in red and these are little exponentials of linear functions, so this is this is essentially becoming a log linear model or an exponential model, or a MAXENT model, or a global linear model.",
                    "label": 0
                },
                {
                    "sent": "Or you know these things have a million different names and we're not using them pretty widely for parsing problems in NLP, and there are a lot of papers in the past few years that have.",
                    "label": 0
                },
                {
                    "sent": "Sort of adapted this for the parsing framework.",
                    "label": 0
                },
                {
                    "sent": "So while I'm not going to represent the problem as a graphical model, it's a very similar parsing, has very similar field linear representations of our of a probability distribution, and we want to do some kind.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reasoning for learning in person.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give you a quick picture of how that.",
                    "label": 0
                },
                {
                    "sent": "Just because we're going to get into these algorithms a little bit of how context free dependency parsing works.",
                    "label": 1
                },
                {
                    "sent": "So we we like to use these graphical representations so, so you have the words at the bottom and we start out by drawing little half triangles over each word, and our goal is to get a really big half triangle that covers the whole sentence and that that will mean that we have.",
                    "label": 0
                },
                {
                    "sent": "We have shown that there's a partial.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under grammar.",
                    "label": 0
                },
                {
                    "sent": "And then we have these inference rules that tell us how to put together the little triangles to build these trapezoid things, and each trap is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can think of as a possible attachment, one word to another.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're once.",
                    "label": 0
                },
                {
                    "sent": "Once you've built the trapezoid, you sort of forget about everything that's underneath, and that's not building on the mark of independence assumptions that are inherent in your car.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's another rule that lets you put together trapezoid triangles to make other triangle.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do this as you do this, what you're what you're actually doing is you're multiplying together the two things that you that you're combining, and then this gives you an update to the value of the thing that you constructed.",
                    "label": 0
                },
                {
                    "sent": "So if I'm putting A&B together, I semiring multiplied together their weights.",
                    "label": 0
                },
                {
                    "sent": "And then I semiring add that that result for the value of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I keep.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going, I'm just going to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly go through.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I did everything right, then I end up with a an entire structure that covers the entire sentence and each of my.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's first ones exactly one of the ways, and so there's a one to one correspondence between entire.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chrzan parsers.",
                    "label": 0
                },
                {
                    "sent": "And so inference for many definitions of inference involves finding a semiring some, and we can do this in cubic time.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's sort of the core algorithm, and once you have that ability to to solve inference, then this is how you build a parser.",
                    "label": 0
                },
                {
                    "sent": "You first get a few $1,000,000 and you pay some smart people who know something about linguistics to sit down and come to consensus about the correct annotations of say, a million sentence is this is done at Upenn about 10 or 15 years ago 15 years ago, and then you use you train some kind of statistical model from those training examples and we have tons of ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "Is huge literature on generative and discriminative.",
                    "label": 0
                },
                {
                    "sent": "Approaches to training these parsing models, and of course, all of that requires a thrust said earlier inference you've gotta, you gotta do inference on inside most training algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then at the end you get your model and you parse using your favorite introduction.",
                    "label": 0
                },
                {
                    "sent": "So that's where that's where this dynamic programming algorithm will come in again, and then you measure accuracy against against the gold standard is a very typical machine learning training path scenario and widely accepted.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "OK, so just to make this kind of college terrible picture.",
                    "label": 0
                },
                {
                    "sent": "So this is a very long sentence for Moby Dick.",
                    "label": 0
                },
                {
                    "sent": "That one of my graduate students wasted an hour parsing by hand and to give you a sense of I am sorry I should have put the text, John, but you know, it's some long rambly.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sentence.",
                    "label": 0
                },
                {
                    "sent": "Run this through, you know, really good automatic parser that came out of Stanford.",
                    "label": 0
                },
                {
                    "sent": "It took about 10 minutes or half gigabytes of memory, and it it got about 1/3 of the words attached incorrectly.",
                    "label": 0
                },
                {
                    "sent": "This is the problem is not.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so so far I have given you this idealized picture where you think dynamic programming with the exact inference, But this is very unsatisfying.",
                    "label": 0
                },
                {
                    "sent": "Computational linguists like me are never satisfied with with the model.",
                    "label": 0
                },
                {
                    "sent": "We really like to be dealing with richer features.",
                    "label": 0
                },
                {
                    "sent": "They can look at larger parts of the substructure through richer formalisms, and breaking some of the really strong independence assumptions that let us do dynamic programming in the first place.",
                    "label": 0
                },
                {
                    "sent": "And we're also interested, you know, parsing again is just one problem.",
                    "label": 0
                },
                {
                    "sent": "We're really interested in other problems as well.",
                    "label": 0
                },
                {
                    "sent": "So if you look at other languages, you have word structure.",
                    "label": 0
                },
                {
                    "sent": "We'd like to integrate morphology would like to start representing meaning in more rich ways.",
                    "label": 0
                },
                {
                    "sent": "Movie on syntax, and so all all kinds of linguistic structures might.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be considered here and context free grammars are not going to be the end of the story.",
                    "label": 0
                },
                {
                    "sent": "So so I have to give this card.",
                    "label": 0
                },
                {
                    "sent": "I'm being a bit brazen by telling you that most of NLP can be thought of as parsing problem more and more problems in natural language processing are looking like parsing, and I think that's the right direction for a long time.",
                    "label": 0
                },
                {
                    "sent": "People in machine learning when they thought about structure prediction, they thought about sequence labeling and I think I think now it's helpful to have sort of a few more harder problems.",
                    "label": 0
                },
                {
                    "sent": "And parsing is sort of the next thing in the sequence in LP, but there's there's considerably more we could do in terms of other structures.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to get to the new stuff.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take this dynamic programming framework and this is work by my student Kevin Gimpel and we're going to try and generalize this to include nonlocal features of the structures that look at larger regions.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so so with with dynamic programming it's really easy to incorporate any feature of our of our Heart Street that looks only at one edge at a time.",
                    "label": 0
                },
                {
                    "sent": "So we called.",
                    "label": 0
                },
                {
                    "sent": "These are factored or highly local features.",
                    "label": 0
                },
                {
                    "sent": "So knowing what the parents of a particular our relationship is or the parent and the child or how far apart they are, how far apart they are.",
                    "label": 0
                },
                {
                    "sent": "Or there's the word context of either of them.",
                    "label": 0
                },
                {
                    "sent": "This is all really easy.",
                    "label": 0
                },
                {
                    "sent": "We can fit this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the model without any pain.",
                    "label": 0
                },
                {
                    "sent": "If we want to look at multiple edges and have these second order interactions, then things become more tricky, and dynamic programming becomes considerably more expensive.",
                    "label": 0
                },
                {
                    "sent": "So knowing that chuckled and Gleann with our triple, that should all kind of go together and having a feature that can look at that entire substructure much more.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spent.",
                    "label": 0
                },
                {
                    "sent": "Here's a an example of a sibling if you want to have a feature that knows that castigation is primarily a relationship between professors and students, you need a second order feature that model sibling interactions.",
                    "label": 0
                },
                {
                    "sent": "That was supposed.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joe.",
                    "label": 0
                },
                {
                    "sent": "Of and then you can.",
                    "label": 0
                },
                {
                    "sent": "You can actually get there.",
                    "label": 0
                },
                {
                    "sent": "There's recent recent examples in the literature of highly nonlocal features that are there are very extreme.",
                    "label": 1
                },
                {
                    "sent": "You look at two adjacent words like the start symbol here and the first word of the sentence, and you want it.",
                    "label": 0
                },
                {
                    "sent": "You want to know the entire tree path between the two, and this is this is been seen as a helpful feature for parsing, but it's incredibly expensive to do that because there could be arbitrary distance three wise between two adjacent words.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the more nonlocal features are the higher the polynomial order of your dynamic programming algorithm, and in the limit, if you have arbitrarily nonlocal features, you can't do dynamic programming is it becomes intractable.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start by talking a little bit about a solution for machine translation, called Cube pruning developed by David Chang and being clogged.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you're going to keep an approximate K best list.",
                    "label": 0
                },
                {
                    "sent": "Of the complete structures for each of these little pieces you're building in your dynamic programming algorithm, and I'm not actually going to pruning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present more than I even simpler version of it called Cube decoding.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So remember when we put together 2 items and we still see.",
                    "label": 0
                },
                {
                    "sent": "A bigger a bigger item.",
                    "label": 0
                },
                {
                    "sent": "What we have to do is semiring multiply the values of A&B together.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to semiring, add that value to C. So we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to let the values instead of being just a real number.",
                    "label": 0
                },
                {
                    "sent": "They're going to be hailing vectors of scored partial structures, so we now have to keep track of a vector of numbers and a structure.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each one, and we're going to consider the cross product.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I'm putting together Andy and I have three different ways that K = 3 of constructing each one, I'm going to have kept track of the entire structure all the way down to the little triangles at the bottom of the screen, and then in the example earlier on.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to take the cross product of these two things and I just take the cross product of the values of each of the structures and this tells me if I were to put together this particular trapezoid, but that particular triangle and everything below this would be the product of those two structures, and then I have to multiply.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ryan well, if that were it then.",
                    "label": 0
                },
                {
                    "sent": "Then I would take the three highest scoring.",
                    "label": 0
                },
                {
                    "sent": "The three high scoring things in the grid and that would give me my K best list and my set of structure for this year is a valid city ring value because I have three different structures intended valuable.",
                    "label": 0
                },
                {
                    "sent": "But I actually.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiply in the the feature rate, so here I'm multiplying in the non local and the local feature weights for each of these complete structure.",
                    "label": 0
                },
                {
                    "sent": "So the local feature weights are actually the same.",
                    "label": 0
                },
                {
                    "sent": "They can't look below the top level item of A or B, they can't look into the into the substructure, but the nonlocal features can, so that's kind of the point.",
                    "label": 0
                },
                {
                    "sent": "You can look all the way down as far.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so once I've I've rescored everything based on the nonlocal features that the game changes, and now the winner is actually here.",
                    "label": 0
                },
                {
                    "sent": "There and this one has made its way into the fold.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a different best list because the nonlocal features have weighed in and told me that I should prefer a different data structures, and so I get a different answer.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so what's nice about this is that I'm keeping the logical structure of the dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "My equations haven't changed, so I can think about the algorithm the same way that I did before, but this is approximate and I have I unfortunately don't have any formal guarantees if I'm using nonlocal features.",
                    "label": 0
                },
                {
                    "sent": "All I can say is this very week thing that as I increase K, I'm going to get better and better approximation.",
                    "label": 0
                },
                {
                    "sent": "So if someone in the room has ideas about how to formalize this and get some stronger guarantee or some some.",
                    "label": 0
                },
                {
                    "sent": "Under some under some conditions, get a stronger guarantee.",
                    "label": 0
                },
                {
                    "sent": "I'd be really interested to hear about that.",
                    "label": 0
                },
                {
                    "sent": "But then you know.",
                    "label": 0
                },
                {
                    "sent": "So this is only part of the story.",
                    "label": 0
                },
                {
                    "sent": "This gives us sort of an approximate way of getting paid best inference, but if I'm really interested in something or getting all the somehow accounting for all the exponentially many other structures, then this doesn't really help me.",
                    "label": 0
                },
                {
                    "sent": "So our extension to this is called cubes.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing and it it adds an additional firm.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that in addition to my pay best list, I'm going to keep a value that is my approximate sum of all the other ways of building this trapezoid and all the other ways of building that triangle.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so once I have I do the cross product as before and then I take the case that's set of things in the inside just.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I didn't understand the coding.",
                    "label": 0
                },
                {
                    "sent": "And then I collapse everything else from the K ^2 -- K ways of putting these things together, and that's going to be part of the new residual.",
                    "label": 0
                },
                {
                    "sent": "And then they want to account for all the ways of putting everything else together.",
                    "label": 0
                },
                {
                    "sent": "That's not accounted for by this these 99 different.",
                    "label": 0
                },
                {
                    "sent": "Combination.",
                    "label": 0
                },
                {
                    "sent": "So I send out the cross.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Product and I can think of all of these as sort of trying to account for all the other parents.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then at this point I can actually put in any nonlocal features because I can't see down below.",
                    "label": 0
                },
                {
                    "sent": "This is abstracted and I don't know what the subscriptions are, but I can multiply in the non local local fees so I do that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that gives me a sum over over everything else that's approximate.",
                    "label": 0
                },
                {
                    "sent": "So so for these guys I've taken into account the nonlocal features and that's going to individual term, and for these guys I've only taken.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your account the local 1.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I add them together and that gives me like my result.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's it's on a different scale because we failed to take into account all the factors, that's right.",
                    "label": 0
                },
                {
                    "sent": "We're not going to worry about it yet.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So again, if I only have local features and you can show that all of this is exactly you have a semiring, the residual is exact.",
                    "label": 0
                },
                {
                    "sent": "Everything is fine.",
                    "label": 0
                },
                {
                    "sent": "As soon as you introduce non local feature is not a semiring anymore, you're you're you lose the associativity and distributivity properties that you represent that you need to have a semiring again if he goes to Infinity residual, it is zero and the K best list is exactly.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a generalization of the sum product in the K best semirings.",
                    "label": 0
                },
                {
                    "sent": "One other nice thing about this is that if you need first derivatives, you can get them through a relatively simple transformation, and they're going to be exact with respect to the approximation that you've done, so there's a lot more detail in the in the ACL paper from last winter.",
                    "label": 0
                },
                {
                    "sent": "So right so this is sort of come back to this point.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of theoretical questions here.",
                    "label": 0
                },
                {
                    "sent": "I don't know how I don't have any guarantees apart from is K is is arbitrarily large, then this, then this is correct I there's more, there's more for more work formal.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be done here, but I'm going to give you is an empirical argument that there's something good going on.",
                    "label": 0
                },
                {
                    "sent": "So, so at a very high level in one slide I can't do this topic justice.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work and a lot of interest in the idea of using statistical models to translate sentences from one language to another, and the two.",
                    "label": 0
                },
                {
                    "sent": "The two main ways of doing that involve translating it sort of a continuous phrase level and sort of thinking of translation is translating phrases and reordering them, and the other the other approaches to use syntactic structure.",
                    "label": 0
                },
                {
                    "sent": "On one or both sides and transform one string into a tree or a tree into a string or a tree into a tree.",
                    "label": 0
                },
                {
                    "sent": "And so these these two ideas for awhile were sort of competition and now people are realizing that they're actually quite similar.",
                    "label": 0
                },
                {
                    "sent": "What we wanted to do is build a single model that was a log linear model that took into account both kinds of both kinds of structure phrases and also grammar rules.",
                    "label": 0
                },
                {
                    "sent": "And just think of them both.",
                    "label": 0
                },
                {
                    "sent": "Those kinds of features.",
                    "label": 0
                },
                {
                    "sent": "So we have a dynamic programming backbone that based on lattice dependency parsing.",
                    "label": 0
                },
                {
                    "sent": "So in this model the syntactic features are local.",
                    "label": 0
                },
                {
                    "sent": "The phrase ones in the language model aren't and then it's a log linear model with hidden variables.",
                    "label": 0
                },
                {
                    "sent": "We trained with pseudo likelihood, there's a.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More to say you should see the MLP paper if you're interested.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the graph that that I think makes makes a compelling case.",
                    "label": 0
                },
                {
                    "sent": "What we have here is 4 different models trained with some of the features turned on or off.",
                    "label": 0
                },
                {
                    "sent": "So the full model is the purple one at the top and then the other three are using different subsets of the features and what you see is that as you as you increase K for decoding you get improved.",
                    "label": 0
                },
                {
                    "sent": "Performance up is good, the blue score is a standard metric for measuring performance machine translation.",
                    "label": 0
                },
                {
                    "sent": "Performance automatically.",
                    "label": 0
                },
                {
                    "sent": "But the important I think an important thing to notice is that you don't get as much gain increasing pay for the syntactic model because it's already local.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "You don't actually need the non local approximations, be just just because most most of the features are local.",
                    "label": 0
                },
                {
                    "sent": "All of this was trained with people to 10 and do something.",
                    "label": 0
                },
                {
                    "sent": "It remains to be seen if actually rather expensive, as pay increases because runtime is going to be quadratic in K. How how things will change as you very fast.",
                    "label": 0
                },
                {
                    "sent": "So this there's no.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I think that the punchline here, about about translation, at least, is that having having these rich nonlocal features is way more important than getting exact inference with those features.",
                    "label": 0
                },
                {
                    "sent": "So just including them in the model and doing something approximate usually benefits you, and this is, I think it's fair to say that this has been widely understood as as NLP research continues to want more and more nonlocal features, we find it's better to throw them in somehow and approximate.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the the last part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Is going to come back to parsing.",
                    "label": 0
                },
                {
                    "sent": "And we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to completely ditch the dynamic programming formulation and think of parsing as integer linear programming and use relaxation.",
                    "label": 0
                },
                {
                    "sent": "This is work by Andrea Martinez.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also urging.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Going back to the idea that I said, I think in the second slide that you could think of parsing the first approximation as being.",
                    "label": 0
                },
                {
                    "sent": "Context free parsing for natural languages.",
                    "label": 0
                },
                {
                    "sent": "This when you when you decide you're going to use context free grammar, you are implicitly making an assumption that when you draw your parts tree, you're only getting consider parse trees where the edges going cross.",
                    "label": 0
                },
                {
                    "sent": "We call that projectivity constraint.",
                    "label": 0
                },
                {
                    "sent": "But non projective structures where the edges cross like in this example are not too hard to find in English.",
                    "label": 0
                },
                {
                    "sent": "It's not too common, but in some other languages it is.",
                    "label": 0
                },
                {
                    "sent": "It is considerably more common, and so 11 sort of important development in the field has been the consideration of these non protective parsing algorithms that can second come up with.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With non protected part.",
                    "label": 0
                },
                {
                    "sent": "And so some really nice work.",
                    "label": 0
                },
                {
                    "sent": "Over the past few years has developed exact inference algorithms for non projective parsing when the edges are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "So again when the features are all arc factor.",
                    "label": 0
                },
                {
                    "sent": "So if it's pointed out by Ryan McDonald and colleagues in 2005 that the Max inference problem here is essentially can be solved using a maximum directed spanning tree algorithm like choosing Edmonds in quadratic time and then summing inferences found out concurrently by three different groups that the matrix tree theorem would, with straightforwardly solve the summing problem.",
                    "label": 0
                },
                {
                    "sent": "Again, this is all assuming that the agents are conditionally independent given the structure is a tree.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so those tricks only work when the features are arc local.",
                    "label": 0
                },
                {
                    "sent": "An result by McDonald's and Sada shows that as soon as we try to do anything more fancy, save with second order features, non protective parsing becomes NP hard.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "So our goal here is to get efficient non projective parsing with arbitrary features and I'm only going to focus on Max imprints and by extension loss augmented, McAfee active.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This this idea of solving something parsing problems with integer linear programming is actually telling Clark in 2006, and they pose.",
                    "label": 0
                },
                {
                    "sent": "The problem isn't as initial linear program with exponentially many constraints and you're cutting planes.",
                    "label": 0
                },
                {
                    "sent": "And that's the crux of this is that you have binary variables for each of the possible attachments.",
                    "label": 0
                },
                {
                    "sent": "So so if you have lengthened sentence, you have N squared of these variables or order and squared.",
                    "label": 0
                },
                {
                    "sent": "These variables that turn on when word I attaches to work day.",
                    "label": 0
                },
                {
                    "sent": "So pretty pretty simple.",
                    "label": 0
                },
                {
                    "sent": "And then you you have constraints that force this to be a tree and that's the tricky part.",
                    "label": 1
                },
                {
                    "sent": "So recent development from Andre and myself and our exciting is that you can.",
                    "label": 0
                },
                {
                    "sent": "You can actually get a concise IO P with a polynomial number of constraints by replacing constraints, trying to impose that the graph is acyclic.",
                    "label": 0
                },
                {
                    "sent": "With a constraint that it's connected and this is, this reduces to something called single commodity flow in integer, integer linear programming literature.",
                    "label": 0
                },
                {
                    "sent": "Because our loss function, which is usually attachment accuracy, also factors very well.",
                    "label": 1
                },
                {
                    "sent": "The mass and the Max loss augmented inference problem is also easily solved.",
                    "label": 0
                },
                {
                    "sent": "In this framework you reduce the whole problem.",
                    "label": 0
                },
                {
                    "sent": "Dial P and there are all kinds of cool extensions you can do, so I don't have time to go into this and I'm guessing this this audience isn't quite as interested in these tricks that you can use multi commodity flow to have hard constraints or features.",
                    "label": 0
                },
                {
                    "sent": "For Projectivity, you can prefer projective parses but allow non projective ones.",
                    "label": 0
                },
                {
                    "sent": "You can get higher order.",
                    "label": 0
                },
                {
                    "sent": "Features using linear linearization tricks, you can have grandchild and sibling and balanced features, and all of these things tend to give you.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Games in your in your parking performance.",
                    "label": 0
                },
                {
                    "sent": "So OK, so you represent the problem as an integer program and then the standard thing that people do once they once they do this is to use an LP relaxation.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to skip vegan represent the convex Hull of the set of vertices each.",
                    "label": 0
                },
                {
                    "sent": "Each vertex is a valid part.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to represent the outer polytope by the bar.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that this is our concise representation, but it might introduce some additional fractional parts vertices that are not good solutions that those are.",
                    "label": 0
                },
                {
                    "sent": "Those are the dangerous things we want to stay away from.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to parse the overall infra saga rhythms as first, you solve the relax Lt and if you get an integral solution that you should be happy you're done, you've got that you've got the solution you want.",
                    "label": 0
                },
                {
                    "sent": "If not, you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to project and get a nearby approximate best free and in that case we we end up using the two Lu Edmonds algorithm again in solving a spanning tree problem.",
                    "label": 0
                },
                {
                    "sent": "This is the more expensive case you want to avoid.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure it might be the case that in the arc factor case.",
                    "label": 0
                },
                {
                    "sent": "I think that well in in the architecture case I I'm not actually thought about that in the art factory case.",
                    "label": 0
                },
                {
                    "sent": "It might be that that all your solutions say integral, but I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't exactly.",
                    "label": 0
                },
                {
                    "sent": "I mean you if you are factored, you don't need this right?",
                    "label": 0
                },
                {
                    "sent": "You just yeah, I'm not sure.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so here's the we're going to do this in a in a Max margin framework, so we want to minimize some.",
                    "label": 0
                },
                {
                    "sent": "You know, quadratic regularization term plus.",
                    "label": 0
                },
                {
                    "sent": "An average across the training examples, and I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into details this.",
                    "label": 0
                },
                {
                    "sent": "Our sub TFW is just the teeth example.",
                    "label": 0
                },
                {
                    "sent": "Then you have to solve the inverse problem right?",
                    "label": 0
                },
                {
                    "sent": "And for from that you're going to get a subgradient and you're going to take a step.",
                    "label": 0
                },
                {
                    "sent": "We're going to do it online setting.",
                    "label": 0
                },
                {
                    "sent": "But our our point here is that we really want to avoid this fractional vertices, so the hypothesis is that by by trying to.",
                    "label": 0
                },
                {
                    "sent": "If you look at the relaxation gap or the difference between your approximate inference in your true inference scores, the arts of P and the R bar, where were you?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Franklin",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the outer polytope.",
                    "label": 0
                },
                {
                    "sent": "Minimizing that should diminish your computational costs.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the key idea.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we enter.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is there a penalty term in the objective function?",
                    "label": 0
                },
                {
                    "sent": "For the relaxation gap.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is the score of influences that the infant store under approximately printing this season for under exact inference, and if you do a little bit of algebraic.",
                    "label": 0
                },
                {
                    "sent": "Rearranging affirms what you got is a combination between exact and approximate inference.",
                    "label": 0
                },
                {
                    "sent": "Instead of a 1 -- 8 here and eight here, and this leaves very nicely to a stochastic online algorithm, building building on the online subgradient method of Ratliff at all from 2006.",
                    "label": 1
                },
                {
                    "sent": "We're going to flip a coin when every time we look at an example and with probability ADA we're going to solve the approximate inference inference problem and the probability 1 minus ETA.",
                    "label": 0
                },
                {
                    "sent": "We're going to solve the exact login problem.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to spend a little bit more energy doing exact inference from time to time by solved by getting getting the exact solution to the to the IOP.",
                    "label": 0
                },
                {
                    "sent": "But in the in the long run, most of the time because it is going to be close to one, we're mostly going to be exactly.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so there's sort of a geometric interpretation here.",
                    "label": 1
                },
                {
                    "sent": "If in expectation what you're actually doing is you're maximizing over a polytope that the linear combination of points between Z&Z bar.",
                    "label": 0
                },
                {
                    "sent": "So we'll call this detail data and you can see it sort of.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, there's been an integral vertices util dot A dot R, sort of closer to the integer vertices infusion.",
                    "label": 0
                },
                {
                    "sent": "Trying to push yourself closer to good solution.",
                    "label": 0
                },
                {
                    "sent": "You might still get fractional solutions, but hopefully they're going to be closer to the integral one.",
                    "label": 1
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so skipping over a lot of details, it works pretty well.",
                    "label": 0
                },
                {
                    "sent": "This is these are our scores on seven different languages.",
                    "label": 0
                },
                {
                    "sent": "This gives you some idea where the state of the art of parsing is.",
                    "label": 0
                },
                {
                    "sent": "We're sort of in the low 90s, high 80s range partitions pretty hard.",
                    "label": 0
                },
                {
                    "sent": "Add.",
                    "label": 0
                },
                {
                    "sent": "But the the the baselines are pretty solid work from the past McDonald's 2nd order model that builds on the MST work and previous work for my group using stacking.",
                    "label": 0
                },
                {
                    "sent": "And what we find is that exact inference.",
                    "label": 0
                },
                {
                    "sent": "4 cases out of seven does slightly better, significantly better than earlier work, and if we use approximate inference, we lose very, very little.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe a 10th of a point on the accuracy score.",
                    "label": 0
                },
                {
                    "sent": "This is with the.",
                    "label": 0
                },
                {
                    "sent": "I believe this is A to equal to 1.",
                    "label": 0
                },
                {
                    "sent": "All approximate.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So right so then you want to know what happens when you when you change Ada and what we did was we measured the relaxation gap on the test data for one of these languages, right?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I don't know which one and what we find is that as a double scored one as you're doing more approximate inference, you get a tighter relaxation gap.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Good to see and then also sort of you know this is the part that gets natural language processing people excited as you increase ADA, you actually see faster test inference even when you're doing when you're doing exactly for the test.",
                    "label": 0
                },
                {
                    "sent": "I'm having trained the model with this additional relaxation gap term.",
                    "label": 0
                },
                {
                    "sent": "If you considerable speedups at Test time, and you can't really see it, but there's a green line at the bottom.",
                    "label": 0
                },
                {
                    "sent": "There's a similar is a similar pattern, but it's much closer to 0 because approximate inference is so fast for this problem, it's a half a second or something percent.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you know, just to give you some information about what's getting fixed.",
                    "label": 0
                },
                {
                    "sent": "So here's another sentence, and the little guy gets frightened.",
                    "label": 0
                },
                {
                    "sent": "The big guys hurt badly, and you know most most people who understand these structures would say that this.",
                    "label": 0
                },
                {
                    "sent": "This attachment, this little phrase, when the little guy gets pregnant, is really attached here to work her modifier the are factored model gets it wrong.",
                    "label": 0
                },
                {
                    "sent": "The richer featured model gets it right.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another one kind of a nice example.",
                    "label": 0
                },
                {
                    "sent": "The art factored model thinks of this sentence.",
                    "label": 0
                },
                {
                    "sent": "We learned a lesson in 1987 about volatility as being kind of like.",
                    "label": 0
                },
                {
                    "sent": "We learned a lesson in economics about volatility, where the.",
                    "label": 0
                },
                {
                    "sent": "Temporal modifier attaches to lessen as opposed to to the verb, and it should catch the verb in the.",
                    "label": 0
                },
                {
                    "sent": "This is the model that has sibling features can get this right.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think.",
                    "label": 0
                },
                {
                    "sent": "Leave a little bit of time for questions.",
                    "label": 0
                },
                {
                    "sent": "I'm going to wrap up so, so proud.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ballistic models and linguistic structures and trying to do automatic linguistic analysis have a very natural affinity.",
                    "label": 0
                },
                {
                    "sent": "This is very hard problem, and probabilistic reasoning has been incredibly helpful in making advances on it.",
                    "label": 0
                },
                {
                    "sent": "I'm actually working on a short book, since this is lecture lecture that should be coming out next year on linguistic structure predictions.",
                    "label": 0
                },
                {
                    "sent": "If you want to know more about this general set of problems.",
                    "label": 0
                },
                {
                    "sent": "Watch for that.",
                    "label": 0
                },
                {
                    "sent": "Today's models are very rich in terms of their features and they require approximate inference.",
                    "label": 0
                },
                {
                    "sent": "We've really been doing approximate inference in natural language processing for as long as we've been using probabilistic models, but now we're starting to realize that there are these connections in that this is a well studied topic in machine learning, and hopefully we'll start giving something back and using more easy to understand and easy to analyze techniques.",
                    "label": 0
                },
                {
                    "sent": "Up one thing that's that's important to remember when you look at work in natural language processing is that are.",
                    "label": 0
                },
                {
                    "sent": "Target variable that we're trying to predict is often somewhat controversial.",
                    "label": 0
                },
                {
                    "sent": "Like I said, any two computational linguists we're going to have three different opinions about the right representation, and so when you get annotated data, that's linguistic data.",
                    "label": 0
                },
                {
                    "sent": "It's often very useful, and you can do interesting things with it, but it didn't come from God.",
                    "label": 0
                },
                {
                    "sent": "It came from linguists.",
                    "label": 0
                },
                {
                    "sent": "So computational representations are always evolving, something to be aware of.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot more to be done in this area of developing generic declarative frameworks for approximating hard NLP problems, and so I look forward to hearing your thoughts, and I guess we'll hear a lot more about this today, so thanks.",
                    "label": 0
                },
                {
                    "sent": "Thank my.",
                    "label": 0
                }
            ]
        }
    }
}