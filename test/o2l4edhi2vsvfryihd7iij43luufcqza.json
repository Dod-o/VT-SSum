{
    "id": "o2l4edhi2vsvfryihd7iij43luufcqza",
    "title": "Nonnegative Matrix Factorization via Rank-One Downdate",
    "info": {
        "author": [
            "Ali Ghodsi, University of Waterloo"
        ],
        "published": "July 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_ghodsi_nmf/",
    "segmentation": [
        [
            "KA?",
            "The topic of this talk is about non negative matrix factorization and I see from University of Waterloo May Co authors are Michael Dickson.",
            "Stephen was dispersed from University of what?"
        ],
        [
            "OK, let me first briefly tell you what the problem of non negative matrix factorization is.",
            "And then I will explain in the algorithm and some theoretical result that we have for non negative matrix factorization.",
            "So the problem is that if you have a matrix a such that the entries of this matrix are non negative or either positive or zero, you want to decompose the matrix to two matrices W&H such that both of these two matrices are also non negative.",
            "OK, so why do we even care about this problem?",
            "Why?"
        ],
        [
            "We want to do that.",
            "Let me give you a very.",
            "Basically, brief view of the composition of matrices.",
            "And maybe a can explain intuitively that why it could be useful.",
            "Things to suppose I have sort of image."
        ],
        [
            "Is.",
            "I can make a matrix out of these images such that I can put each image in a column of this matrix.",
            "So here for example, each image has 560 pixels, so I have 560 rows and I have 1965 images, so I make a matrix.",
            "If I decompose this matrix to two matrices, this space, so so far it's not negative.",
            "Matrix factorization is just the composition of me.",
            "If I decompose this matrix this way.",
            "Well, I can reconstruct each column of the original matrix.",
            "By basically.",
            "Using these two columns.",
            "And using corresponding basically numbers that they have in this to mix.",
            "Basically I can multiply this column by this coefficient plus this column by this coefficient, so.",
            "You can think of this problem that you know these are some bases and.",
            "You know you have enough and this matrix basically gives you the coefficient of this now, but by this now we can change the intensity of these two bases and then you can add them.",
            "But we can subtract them and reconstruct one of these images.",
            "OK, that's what we usually do with singular value decomposition or PCA.",
            "Let's suppose that you know all interests of these two are positive or basically non negative.",
            "If they are non negative.",
            "The first thing is that these two bases are interpretable images, so you don't have any negative values that can be interpreted as intensity, so they're valid images.",
            "And this coefficients also are positive, so any column, any image, any original image can be the constructor just by adding the basis.",
            "So you're not allowed to subtract basis so we can reconstruct an image is by adding some bases, so we can intuitively think that one way to do this to reconstruct an image just by.",
            "I think some other images is that this bases are segments of the original image.",
            "Part of original image, so hopefully this technique can be used to segment images and can be used in.",
            "Text retrieval as you're going to see, so this is very."
        ],
        [
            "General view of matrix decomposition and as I mentioned, you know we can decompose matrices, for example with singular value decomposition, but there is no guarantee that the entries of the matrix are non negative in this case.",
            "So with singular value decomposition so it cannot be interpreted as images.",
            "Also since we have negative values.",
            "There is no probabilistic interpretation correspond."
        ],
        [
            "Link to this.",
            "Let me give you a very brief history of this problem.",
            "This problem non negative matrix factorization started from.",
            "Kind of.",
            "Old problems started from 1993 and."
        ],
        [
            "In 1999, this problem became popular by the work of Lee and Seung with the paper that we published in Nature.",
            "They showed that they can do basically a segmentation of images."
        ],
        [
            "It has been shown that many applications can use this technique.",
            "For example, it has been shown that in DNA microarray.",
            "They can do some sort of biclustering."
        ],
        [
            "Enema also a paper in 2003.",
            "Basically, extract nodes from a music."
        ],
        [
            "NMS in terms of the algorithm that previous work queues are basically the major trend.",
            "Is that their?",
            "Start with some gas?"
        ],
        [
            "For W&H and they try to minimize this objective function.",
            "You can assume that if one of these two W or H is known.",
            "It's a list of square problem but constrained least square problem.",
            "So if W is down you can solve a list of square problem but with some constraint the entries of H is positive and vice versa.",
            "So most algorithms are basically local search with assumption that one of these two matrices are fixed and they're trying to find the other one."
        ],
        [
            "Uh.",
            "And there are.",
            "Variation of this?",
            "OK, the approach that we are going to take to solve the negative matrix factorization is somehow different actually.",
            "This is based on a very simple observation.",
            "The observation is that of if you have a non negative matrix.",
            "The leading singular vectors of a non negative matrix is non negative or non negative.",
            "This is a consequence of provenance, nor business theorem, but it also can be seen."
        ],
        [
            "Based on the power method, you know you can find the singular vectors of a matrix using power method, very simple.",
            "Iteration method and using power method you can see that there is no way that the leading eigenvector singular vectors of a non negative matrix.",
            "Are negative because in power method you know what you do.",
            "Basically, you know you want to find EU, Sigma and V such that U Sigma V transpose is A is the original matrix or low rank approximation of the original matrix.",
            "What you do is that you start with a random guess for V and then you multiply this by a an normalize.",
            "It is going to be your U use this U multiplied by a transpose.",
            "Normalize it is going to be.",
            "Your V and you keep going.",
            "You know it will converge its power method.",
            "It would converge to leading singular vectors of the matrix, so it's not going to be negative because a is non negative, V is non negative, so there's no way that you generate negative numbers in living eigen vectors or letting singular vectors.",
            "Basically.",
            "So the first observation is that leading singular vectors are non negative.",
            "So based on this observation, it's going to be trivial to have a rank one non negative matrix factorization.",
            "We can just use SVD, singular value decomposition and have a rank one NMF."
        ],
        [
            "But rank one NF is not useful.",
            "We usually wants a higher order rank.",
            "So a very naive approach based on this observation, is that OK, when you find the leading eigenvectors and you have.",
            "Rank one reconstruction of your matrix.",
            "Just cancel out.",
            "This may fix the rank one approximation of your matrix.",
            "Cancel it out from the matrix.",
            "But the UA.",
            "When you cancel out this rank, one matrix is not going to be non negative anymore.",
            "You generate negative entries very very diverse approach and also a bad approach actually.",
            "Is that when you generate some negative values?",
            "You can force those non non negative values to busy.",
            "And then keep going.",
            "You know you have a.",
            "You do you find leading singular vectors?",
            "It's rank 1F XNM approximation.",
            "You cancel out this part from the Matrix.",
            "You subtract this bird.",
            "You generate some negative values and you set them to zero.",
            "It's very nice man.",
            "It's a bad approach.",
            "But based on this intuition, actually we can do much bigger."
        ],
        [
            "So suppose that in this to the finding the leading eigen singular vectors of the original matrix A, we find a submatrix of a.",
            "Such that this submatrix of a is.",
            "Close to rank one or is rank one.",
            "And then find leading eigenvector leading singular vectors of this submatrix.",
            "So affects data rank one submatrix of a.",
            "This notation.",
            "Basically I am an.",
            "It's like Matlab notation, M is subset of Roseanne is subset of columns, so is submatrix of your matrix and if this is a rank one submatrix when you do.",
            "Rank one reconstruction of this matrix you can basically set this part equal to 0, because it's a good approximation of this sub matrix and then you can continue your algorithm."
        ],
        [
            "OK, based on this intuition we have this objective function.",
            "I'm looking for a submatrix of my matrix.",
            "I want to maximize this objective function which is.",
            "As large as possible.",
            "But I want to penalize myself.",
            "If this submatrix is not rank one.",
            "As we get far from being rank one, I would penalize the object.",
            "This is the objective function which we want to maximize to find the submatrix of the matrix.",
            "This is an NP hard problem, so we cannot solve this exact, but there's a good heuristic and actually have it."
        ],
        [
            "Puristic to solve this heuristic is that OK this this objective function depends on subset of rows, subset of columns and singular vectors.",
            "And given subset of rules and such stuff column, you know it's independent of EU Sigma NV because you can give an M and an you can find you Sigma and be using SVD singular ready the composition.",
            "So we can show that if M. And you is given.",
            "So if you have the optimal value for MU.",
            "The problem is separable.",
            "Basically, if subset of rows.",
            "And singular vectors corresponding to that is given.",
            "The problem is decomposable.",
            "Or is separable under on the columns of the matrix.",
            "And vice versa if the subset of columns is known, the problem is separable on the rows.",
            "Basically, if if subset of roses given you don't need to solve a combinatorial problem to find, you know the optimal subset of columns, you can check columns 1 by 1.",
            "To see if they improve the objective or if they don't and if they do, you can put it in the set of optimum subset of columns and you can do.",
            "Desk for Rose as well.",
            "So basically you can have kind of iterative algorithm.",
            "Assume that subset of columns is now find the optimal subset of rows.",
            "And then assume this subset of rows is optimal.",
            "Find the subset of columns.",
            "And you can iterate and it will come."
        ],
        [
            "We can prove that it will come with to give you a demo of what's going on.",
            "Do you have 10 minutes or 5 minutes?",
            "That phone so to give you a demo.",
            "You know, assume that this is your original matrix, that you have two sub matrices here."
        ],
        [
            "And at the beginning, you know, based on some initial guess, you find one of these singular vectors, and this problem is separable on."
        ],
        [
            "Columns as I said so we can find optimal."
        ],
        [
            "Set of columns so.",
            "In the next iteration, I will assume that I don't have five columns.",
            "I only have these three columns.",
            "These are my optimal column and then I will ask my algorithm.",
            "Where would be the optimal optimal subset of rule."
        ],
        [
            "And."
        ],
        [
            "Will find it."
        ],
        [
            "And given the optimal Subs."
        ],
        [
            "Water froze again, I will."
        ],
        [
            "Find the optimal subset of columns and it will converge.",
            "You know it gives me a submatrix a 0."
        ],
        [
            "Note this submatrix basically remove this submatrix from the."
        ],
        [
            "Matrix and continue.",
            "It leads to an algorithm pretty similar to power method.",
            "Basically, you know the main loop is power method, except that after each part that we find one of these singular vectors in the singular vectors, not the singular vector of the whole matrix is singular vector of a submatrix, and we do have a test to make sure that you know change the subset of optimal subset."
        ],
        [
            "So I will go pretty quickly, just you know we have two theorem in this theory in this paper, and it's not even in ICML paper, because the proof is pretty lengthy.",
            "It's an archive if you want to look at the proof completely, we can prove that if there's some assumption for a text corpus.",
            "Quickly if you have T topics and this the topics comes from distribution and each topic can be represented by a distribution from a bag of word.",
            "Uh, in epsilon separable means that term belongs to topic one.",
            "For example, the chance that this terms belongs to another topic is the probability is less than."
        ],
        [
            "Epsilon.",
            "We can prove that if the number of documents go to Infinity, then with probability one this model finds the right terms and right topics in the corpus."
        ],
        [
            "And the little experiment, an very large data set, consists of use.",
            "Hopefully news agency don't produce their own use using this bag of word and you know random random generation of terms, but apparently it works in.",
            "This is latent semantic indexing.",
            "If you look at the words in each column, you can see that there is kind of mix.",
            "There's Simpson and there's Clinton, and there's a police.",
            "There's, you know both."
        ],
        [
            "Yeah, for example in LSI.",
            "But if you look at our algorithm, which is R1D, it's pretty consistent.",
            "Simpson George Jewelry defense trial, Los Angeles, for example, the place that the court was held in."
        ],
        [
            "We also have a theorem about images that under certain condition you know this.",
            "Method can basically find the right features in a bitmap image.",
            "Image consists only black and white zero and one."
        ],
        [
            "And this is another simple experiment that you know if you have this.",
            "Data set which each image is a combination of three features of the five."
        ],
        [
            "Then this is what you get from LSI.",
            "The first column is the basis that.",
            "LS I will find, and each of these rows in front of the first entry is, you know, entries of H. This is W. This is H, so this supposed to cluster similar image.",
            "Is this supposed to find the base?"
        ],
        [
            "This is what LSI does.",
            "This is what some another."
        ],
        [
            "NFL and this is with R1D, does you know we can see that you know this column is just features exactly and these rules shows that they have been clustered exactly correctly.",
            "You know all of these images have this feature.",
            "Or all of these images have this feature?"
        ],
        [
            "On another data set.",
            "This is like a semantic indexing and you know this is I can face this an these supposed to cluster images again.",
            "You can see that in this clustering there some inconsistency.",
            "There like for example, you know.",
            "This has left right side view left side view, but this is right side view."
        ],
        [
            "Also, if you apply this in other animals, you know these are the features that.",
            "One of the implementation of NF previous information of any other point.",
            "Again, you can system inconsistency in the clustering.",
            "Here you know, for example, this one compared with the rest of the."
        ],
        [
            "True, but our wind is pretty consistent.",
            "So our when defines these features as leading features of this data set.",
            "And this is the clusters.",
            "That you have to consider the clusters are pretty consistent.",
            "And the segments actually are sort of representative of this cluster.",
            "K."
        ],
        [
            "Think that's it?",
            "Question.",
            "Hello.",
            "Transformation.",
            "Sorry, can you repeat the question loudly?",
            "I can't hear you well.",
            "Quality of the approximation also.",
            "Play.",
            "So.",
            "It's normal.",
            "Open personal property.",
            "Oh yes, I I see what you mean.",
            "So we have different objective function.",
            "We can prove that our technique basically maximized our objective function.",
            "But if you mean the for vinius norm of the original matrix minus W * H, No, we don't know if it's going to minimize that.",
            "Yes, it's possible because you know.",
            "There are many reasons for that.",
            "Actually, you know if you look at I mean there is a very naive intuition about segmentation of images based on non negative matrix factorization.",
            "As I say you know to generate an image you can add some basis but that's not the only way to do that right?",
            "I mean you can add some images you can.",
            "Avoid subtraction, but it doesn't basically oblige you.",
            "That bases are.",
            "Features if bases are not as sparse.",
            "You know, you can imagine that a part of a feature comes in one base, another part comes, another in another way.",
            "If you look at the original objective function of.",
            "R. NMF it doesn't guarantee a sparsity.",
            "Right, so there is no guarantee that the spaces are sparse, so capturing a part of basically feature.",
            "And there are versions that they want to add a sparsity by adding L1 norm organization and so here actually you know the sparsity is guaranteed because when you find the submatrix in leading eigenvector vector corresponding to the whole matrix, the rest of the I mean entries correspond to the rest of the matrix are going to be 0, so it's pretty sparse.",
            "And that's one reason that you know it captures the.",
            "Parts this is basically intuitively, but as I said, theoretically the proof that you know maximizing this objective function will find the right terms.",
            "An right topics and write features under the assumption that we have for the model of effort, model of text and image.",
            "Carryover for college.",
            "Automatically.",
            "I haven't tried that on color images, I don't know.",
            "The theorem doesn't extend to color image, the term is just for bitmap images, and the term that we have experimentally I don't know.",
            "Or the top into colorless.",
            "Order one hopefully or.",
            "Virginia acquisition just go over it in someone else is not working on lower right, said overlap.",
            "So what we have, what country?",
            "But at your ever applied for another computer or Internet, or also overlap between basis vectors question, I think you're so you're basically taking chromium agriculture companies operating station.",
            "Yep.",
            "What image then multiple colors?",
            "Yeah, I. Yeah.",
            "Well, you know apart if you know part of assumption in the theory.",
            "As I said, is that the Corp is epsilon separable by epsilon separable.",
            "Basically naively it means that yeah, we have a kind of block matrix.",
            "I mean terms coming from one topic are not in another topic ideally, but.",
            "By epsilon.",
            "Separable basically means that the chance the probability that terms from one topic appear in another topic.",
            "It's possible, but the chance is less than epsilon.",
            "And we showed that the proof that you know the algorithm works perfectly if there are.",
            "If this epsilon is pretty small, you know.",
            "If there are huge overlap between these blocks.",
            "There's a good chance that this algorithm doesn't work well, but in practice actually on all kinds of data set that we try.",
            "It's better than other.",
            "NMF algorithm at something that I forgot to mention is that this is much faster than those you know in this, like data, text data set, some of the previous implementation of North is not even.",
            "You can't even run them.",
            "You know it takes a couple of days throughout, but this takes less than 3 minutes too.",
            "Sure.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "KA?",
                    "label": 0
                },
                {
                    "sent": "The topic of this talk is about non negative matrix factorization and I see from University of Waterloo May Co authors are Michael Dickson.",
                    "label": 1
                },
                {
                    "sent": "Stephen was dispersed from University of what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me first briefly tell you what the problem of non negative matrix factorization is.",
                    "label": 0
                },
                {
                    "sent": "And then I will explain in the algorithm and some theoretical result that we have for non negative matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that if you have a matrix a such that the entries of this matrix are non negative or either positive or zero, you want to decompose the matrix to two matrices W&H such that both of these two matrices are also non negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so why do we even care about this problem?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to do that.",
                    "label": 0
                },
                {
                    "sent": "Let me give you a very.",
                    "label": 0
                },
                {
                    "sent": "Basically, brief view of the composition of matrices.",
                    "label": 0
                },
                {
                    "sent": "And maybe a can explain intuitively that why it could be useful.",
                    "label": 0
                },
                {
                    "sent": "Things to suppose I have sort of image.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "I can make a matrix out of these images such that I can put each image in a column of this matrix.",
                    "label": 0
                },
                {
                    "sent": "So here for example, each image has 560 pixels, so I have 560 rows and I have 1965 images, so I make a matrix.",
                    "label": 0
                },
                {
                    "sent": "If I decompose this matrix to two matrices, this space, so so far it's not negative.",
                    "label": 0
                },
                {
                    "sent": "Matrix factorization is just the composition of me.",
                    "label": 0
                },
                {
                    "sent": "If I decompose this matrix this way.",
                    "label": 0
                },
                {
                    "sent": "Well, I can reconstruct each column of the original matrix.",
                    "label": 0
                },
                {
                    "sent": "By basically.",
                    "label": 0
                },
                {
                    "sent": "Using these two columns.",
                    "label": 0
                },
                {
                    "sent": "And using corresponding basically numbers that they have in this to mix.",
                    "label": 0
                },
                {
                    "sent": "Basically I can multiply this column by this coefficient plus this column by this coefficient, so.",
                    "label": 0
                },
                {
                    "sent": "You can think of this problem that you know these are some bases and.",
                    "label": 0
                },
                {
                    "sent": "You know you have enough and this matrix basically gives you the coefficient of this now, but by this now we can change the intensity of these two bases and then you can add them.",
                    "label": 0
                },
                {
                    "sent": "But we can subtract them and reconstruct one of these images.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what we usually do with singular value decomposition or PCA.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose that you know all interests of these two are positive or basically non negative.",
                    "label": 0
                },
                {
                    "sent": "If they are non negative.",
                    "label": 0
                },
                {
                    "sent": "The first thing is that these two bases are interpretable images, so you don't have any negative values that can be interpreted as intensity, so they're valid images.",
                    "label": 0
                },
                {
                    "sent": "And this coefficients also are positive, so any column, any image, any original image can be the constructor just by adding the basis.",
                    "label": 0
                },
                {
                    "sent": "So you're not allowed to subtract basis so we can reconstruct an image is by adding some bases, so we can intuitively think that one way to do this to reconstruct an image just by.",
                    "label": 0
                },
                {
                    "sent": "I think some other images is that this bases are segments of the original image.",
                    "label": 0
                },
                {
                    "sent": "Part of original image, so hopefully this technique can be used to segment images and can be used in.",
                    "label": 0
                },
                {
                    "sent": "Text retrieval as you're going to see, so this is very.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "General view of matrix decomposition and as I mentioned, you know we can decompose matrices, for example with singular value decomposition, but there is no guarantee that the entries of the matrix are non negative in this case.",
                    "label": 0
                },
                {
                    "sent": "So with singular value decomposition so it cannot be interpreted as images.",
                    "label": 1
                },
                {
                    "sent": "Also since we have negative values.",
                    "label": 0
                },
                {
                    "sent": "There is no probabilistic interpretation correspond.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Link to this.",
                    "label": 0
                },
                {
                    "sent": "Let me give you a very brief history of this problem.",
                    "label": 0
                },
                {
                    "sent": "This problem non negative matrix factorization started from.",
                    "label": 0
                },
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Old problems started from 1993 and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In 1999, this problem became popular by the work of Lee and Seung with the paper that we published in Nature.",
                    "label": 0
                },
                {
                    "sent": "They showed that they can do basically a segmentation of images.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has been shown that many applications can use this technique.",
                    "label": 0
                },
                {
                    "sent": "For example, it has been shown that in DNA microarray.",
                    "label": 0
                },
                {
                    "sent": "They can do some sort of biclustering.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enema also a paper in 2003.",
                    "label": 0
                },
                {
                    "sent": "Basically, extract nodes from a music.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NMS in terms of the algorithm that previous work queues are basically the major trend.",
                    "label": 0
                },
                {
                    "sent": "Is that their?",
                    "label": 0
                },
                {
                    "sent": "Start with some gas?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For W&H and they try to minimize this objective function.",
                    "label": 0
                },
                {
                    "sent": "You can assume that if one of these two W or H is known.",
                    "label": 0
                },
                {
                    "sent": "It's a list of square problem but constrained least square problem.",
                    "label": 0
                },
                {
                    "sent": "So if W is down you can solve a list of square problem but with some constraint the entries of H is positive and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So most algorithms are basically local search with assumption that one of these two matrices are fixed and they're trying to find the other one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "And there are.",
                    "label": 0
                },
                {
                    "sent": "Variation of this?",
                    "label": 0
                },
                {
                    "sent": "OK, the approach that we are going to take to solve the negative matrix factorization is somehow different actually.",
                    "label": 0
                },
                {
                    "sent": "This is based on a very simple observation.",
                    "label": 0
                },
                {
                    "sent": "The observation is that of if you have a non negative matrix.",
                    "label": 0
                },
                {
                    "sent": "The leading singular vectors of a non negative matrix is non negative or non negative.",
                    "label": 0
                },
                {
                    "sent": "This is a consequence of provenance, nor business theorem, but it also can be seen.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based on the power method, you know you can find the singular vectors of a matrix using power method, very simple.",
                    "label": 1
                },
                {
                    "sent": "Iteration method and using power method you can see that there is no way that the leading eigenvector singular vectors of a non negative matrix.",
                    "label": 0
                },
                {
                    "sent": "Are negative because in power method you know what you do.",
                    "label": 0
                },
                {
                    "sent": "Basically, you know you want to find EU, Sigma and V such that U Sigma V transpose is A is the original matrix or low rank approximation of the original matrix.",
                    "label": 0
                },
                {
                    "sent": "What you do is that you start with a random guess for V and then you multiply this by a an normalize.",
                    "label": 0
                },
                {
                    "sent": "It is going to be your U use this U multiplied by a transpose.",
                    "label": 0
                },
                {
                    "sent": "Normalize it is going to be.",
                    "label": 0
                },
                {
                    "sent": "Your V and you keep going.",
                    "label": 0
                },
                {
                    "sent": "You know it will converge its power method.",
                    "label": 0
                },
                {
                    "sent": "It would converge to leading singular vectors of the matrix, so it's not going to be negative because a is non negative, V is non negative, so there's no way that you generate negative numbers in living eigen vectors or letting singular vectors.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 1
                },
                {
                    "sent": "So the first observation is that leading singular vectors are non negative.",
                    "label": 0
                },
                {
                    "sent": "So based on this observation, it's going to be trivial to have a rank one non negative matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "We can just use SVD, singular value decomposition and have a rank one NMF.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But rank one NF is not useful.",
                    "label": 0
                },
                {
                    "sent": "We usually wants a higher order rank.",
                    "label": 0
                },
                {
                    "sent": "So a very naive approach based on this observation, is that OK, when you find the leading eigenvectors and you have.",
                    "label": 0
                },
                {
                    "sent": "Rank one reconstruction of your matrix.",
                    "label": 0
                },
                {
                    "sent": "Just cancel out.",
                    "label": 0
                },
                {
                    "sent": "This may fix the rank one approximation of your matrix.",
                    "label": 0
                },
                {
                    "sent": "Cancel it out from the matrix.",
                    "label": 0
                },
                {
                    "sent": "But the UA.",
                    "label": 0
                },
                {
                    "sent": "When you cancel out this rank, one matrix is not going to be non negative anymore.",
                    "label": 0
                },
                {
                    "sent": "You generate negative entries very very diverse approach and also a bad approach actually.",
                    "label": 0
                },
                {
                    "sent": "Is that when you generate some negative values?",
                    "label": 0
                },
                {
                    "sent": "You can force those non non negative values to busy.",
                    "label": 0
                },
                {
                    "sent": "And then keep going.",
                    "label": 0
                },
                {
                    "sent": "You know you have a.",
                    "label": 0
                },
                {
                    "sent": "You do you find leading singular vectors?",
                    "label": 0
                },
                {
                    "sent": "It's rank 1F XNM approximation.",
                    "label": 0
                },
                {
                    "sent": "You cancel out this part from the Matrix.",
                    "label": 0
                },
                {
                    "sent": "You subtract this bird.",
                    "label": 0
                },
                {
                    "sent": "You generate some negative values and you set them to zero.",
                    "label": 0
                },
                {
                    "sent": "It's very nice man.",
                    "label": 0
                },
                {
                    "sent": "It's a bad approach.",
                    "label": 0
                },
                {
                    "sent": "But based on this intuition, actually we can do much bigger.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose that in this to the finding the leading eigen singular vectors of the original matrix A, we find a submatrix of a.",
                    "label": 0
                },
                {
                    "sent": "Such that this submatrix of a is.",
                    "label": 0
                },
                {
                    "sent": "Close to rank one or is rank one.",
                    "label": 0
                },
                {
                    "sent": "And then find leading eigenvector leading singular vectors of this submatrix.",
                    "label": 0
                },
                {
                    "sent": "So affects data rank one submatrix of a.",
                    "label": 0
                },
                {
                    "sent": "This notation.",
                    "label": 0
                },
                {
                    "sent": "Basically I am an.",
                    "label": 0
                },
                {
                    "sent": "It's like Matlab notation, M is subset of Roseanne is subset of columns, so is submatrix of your matrix and if this is a rank one submatrix when you do.",
                    "label": 0
                },
                {
                    "sent": "Rank one reconstruction of this matrix you can basically set this part equal to 0, because it's a good approximation of this sub matrix and then you can continue your algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, based on this intuition we have this objective function.",
                    "label": 0
                },
                {
                    "sent": "I'm looking for a submatrix of my matrix.",
                    "label": 0
                },
                {
                    "sent": "I want to maximize this objective function which is.",
                    "label": 0
                },
                {
                    "sent": "As large as possible.",
                    "label": 0
                },
                {
                    "sent": "But I want to penalize myself.",
                    "label": 0
                },
                {
                    "sent": "If this submatrix is not rank one.",
                    "label": 0
                },
                {
                    "sent": "As we get far from being rank one, I would penalize the object.",
                    "label": 0
                },
                {
                    "sent": "This is the objective function which we want to maximize to find the submatrix of the matrix.",
                    "label": 0
                },
                {
                    "sent": "This is an NP hard problem, so we cannot solve this exact, but there's a good heuristic and actually have it.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Puristic to solve this heuristic is that OK this this objective function depends on subset of rows, subset of columns and singular vectors.",
                    "label": 0
                },
                {
                    "sent": "And given subset of rules and such stuff column, you know it's independent of EU Sigma NV because you can give an M and an you can find you Sigma and be using SVD singular ready the composition.",
                    "label": 0
                },
                {
                    "sent": "So we can show that if M. And you is given.",
                    "label": 0
                },
                {
                    "sent": "So if you have the optimal value for MU.",
                    "label": 0
                },
                {
                    "sent": "The problem is separable.",
                    "label": 0
                },
                {
                    "sent": "Basically, if subset of rows.",
                    "label": 0
                },
                {
                    "sent": "And singular vectors corresponding to that is given.",
                    "label": 0
                },
                {
                    "sent": "The problem is decomposable.",
                    "label": 0
                },
                {
                    "sent": "Or is separable under on the columns of the matrix.",
                    "label": 0
                },
                {
                    "sent": "And vice versa if the subset of columns is known, the problem is separable on the rows.",
                    "label": 0
                },
                {
                    "sent": "Basically, if if subset of roses given you don't need to solve a combinatorial problem to find, you know the optimal subset of columns, you can check columns 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "To see if they improve the objective or if they don't and if they do, you can put it in the set of optimum subset of columns and you can do.",
                    "label": 0
                },
                {
                    "sent": "Desk for Rose as well.",
                    "label": 0
                },
                {
                    "sent": "So basically you can have kind of iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "Assume that subset of columns is now find the optimal subset of rows.",
                    "label": 0
                },
                {
                    "sent": "And then assume this subset of rows is optimal.",
                    "label": 0
                },
                {
                    "sent": "Find the subset of columns.",
                    "label": 0
                },
                {
                    "sent": "And you can iterate and it will come.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can prove that it will come with to give you a demo of what's going on.",
                    "label": 0
                },
                {
                    "sent": "Do you have 10 minutes or 5 minutes?",
                    "label": 0
                },
                {
                    "sent": "That phone so to give you a demo.",
                    "label": 0
                },
                {
                    "sent": "You know, assume that this is your original matrix, that you have two sub matrices here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at the beginning, you know, based on some initial guess, you find one of these singular vectors, and this problem is separable on.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Columns as I said so we can find optimal.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set of columns so.",
                    "label": 0
                },
                {
                    "sent": "In the next iteration, I will assume that I don't have five columns.",
                    "label": 0
                },
                {
                    "sent": "I only have these three columns.",
                    "label": 0
                },
                {
                    "sent": "These are my optimal column and then I will ask my algorithm.",
                    "label": 0
                },
                {
                    "sent": "Where would be the optimal optimal subset of rule.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will find it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And given the optimal Subs.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Water froze again, I will.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the optimal subset of columns and it will converge.",
                    "label": 0
                },
                {
                    "sent": "You know it gives me a submatrix a 0.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note this submatrix basically remove this submatrix from the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix and continue.",
                    "label": 0
                },
                {
                    "sent": "It leads to an algorithm pretty similar to power method.",
                    "label": 0
                },
                {
                    "sent": "Basically, you know the main loop is power method, except that after each part that we find one of these singular vectors in the singular vectors, not the singular vector of the whole matrix is singular vector of a submatrix, and we do have a test to make sure that you know change the subset of optimal subset.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will go pretty quickly, just you know we have two theorem in this theory in this paper, and it's not even in ICML paper, because the proof is pretty lengthy.",
                    "label": 0
                },
                {
                    "sent": "It's an archive if you want to look at the proof completely, we can prove that if there's some assumption for a text corpus.",
                    "label": 0
                },
                {
                    "sent": "Quickly if you have T topics and this the topics comes from distribution and each topic can be represented by a distribution from a bag of word.",
                    "label": 0
                },
                {
                    "sent": "Uh, in epsilon separable means that term belongs to topic one.",
                    "label": 0
                },
                {
                    "sent": "For example, the chance that this terms belongs to another topic is the probability is less than.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Epsilon.",
                    "label": 0
                },
                {
                    "sent": "We can prove that if the number of documents go to Infinity, then with probability one this model finds the right terms and right topics in the corpus.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the little experiment, an very large data set, consists of use.",
                    "label": 0
                },
                {
                    "sent": "Hopefully news agency don't produce their own use using this bag of word and you know random random generation of terms, but apparently it works in.",
                    "label": 0
                },
                {
                    "sent": "This is latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "If you look at the words in each column, you can see that there is kind of mix.",
                    "label": 0
                },
                {
                    "sent": "There's Simpson and there's Clinton, and there's a police.",
                    "label": 0
                },
                {
                    "sent": "There's, you know both.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, for example in LSI.",
                    "label": 0
                },
                {
                    "sent": "But if you look at our algorithm, which is R1D, it's pretty consistent.",
                    "label": 0
                },
                {
                    "sent": "Simpson George Jewelry defense trial, Los Angeles, for example, the place that the court was held in.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have a theorem about images that under certain condition you know this.",
                    "label": 0
                },
                {
                    "sent": "Method can basically find the right features in a bitmap image.",
                    "label": 0
                },
                {
                    "sent": "Image consists only black and white zero and one.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is another simple experiment that you know if you have this.",
                    "label": 0
                },
                {
                    "sent": "Data set which each image is a combination of three features of the five.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this is what you get from LSI.",
                    "label": 0
                },
                {
                    "sent": "The first column is the basis that.",
                    "label": 0
                },
                {
                    "sent": "LS I will find, and each of these rows in front of the first entry is, you know, entries of H. This is W. This is H, so this supposed to cluster similar image.",
                    "label": 0
                },
                {
                    "sent": "Is this supposed to find the base?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what LSI does.",
                    "label": 0
                },
                {
                    "sent": "This is what some another.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NFL and this is with R1D, does you know we can see that you know this column is just features exactly and these rules shows that they have been clustered exactly correctly.",
                    "label": 0
                },
                {
                    "sent": "You know all of these images have this feature.",
                    "label": 0
                },
                {
                    "sent": "Or all of these images have this feature?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On another data set.",
                    "label": 0
                },
                {
                    "sent": "This is like a semantic indexing and you know this is I can face this an these supposed to cluster images again.",
                    "label": 0
                },
                {
                    "sent": "You can see that in this clustering there some inconsistency.",
                    "label": 0
                },
                {
                    "sent": "There like for example, you know.",
                    "label": 0
                },
                {
                    "sent": "This has left right side view left side view, but this is right side view.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, if you apply this in other animals, you know these are the features that.",
                    "label": 0
                },
                {
                    "sent": "One of the implementation of NF previous information of any other point.",
                    "label": 0
                },
                {
                    "sent": "Again, you can system inconsistency in the clustering.",
                    "label": 0
                },
                {
                    "sent": "Here you know, for example, this one compared with the rest of the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True, but our wind is pretty consistent.",
                    "label": 0
                },
                {
                    "sent": "So our when defines these features as leading features of this data set.",
                    "label": 0
                },
                {
                    "sent": "And this is the clusters.",
                    "label": 0
                },
                {
                    "sent": "That you have to consider the clusters are pretty consistent.",
                    "label": 0
                },
                {
                    "sent": "And the segments actually are sort of representative of this cluster.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think that's it?",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Transformation.",
                    "label": 0
                },
                {
                    "sent": "Sorry, can you repeat the question loudly?",
                    "label": 0
                },
                {
                    "sent": "I can't hear you well.",
                    "label": 0
                },
                {
                    "sent": "Quality of the approximation also.",
                    "label": 0
                },
                {
                    "sent": "Play.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's normal.",
                    "label": 0
                },
                {
                    "sent": "Open personal property.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, I I see what you mean.",
                    "label": 0
                },
                {
                    "sent": "So we have different objective function.",
                    "label": 0
                },
                {
                    "sent": "We can prove that our technique basically maximized our objective function.",
                    "label": 0
                },
                {
                    "sent": "But if you mean the for vinius norm of the original matrix minus W * H, No, we don't know if it's going to minimize that.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's possible because you know.",
                    "label": 0
                },
                {
                    "sent": "There are many reasons for that.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know if you look at I mean there is a very naive intuition about segmentation of images based on non negative matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "As I say you know to generate an image you can add some basis but that's not the only way to do that right?",
                    "label": 0
                },
                {
                    "sent": "I mean you can add some images you can.",
                    "label": 0
                },
                {
                    "sent": "Avoid subtraction, but it doesn't basically oblige you.",
                    "label": 0
                },
                {
                    "sent": "That bases are.",
                    "label": 0
                },
                {
                    "sent": "Features if bases are not as sparse.",
                    "label": 0
                },
                {
                    "sent": "You know, you can imagine that a part of a feature comes in one base, another part comes, another in another way.",
                    "label": 0
                },
                {
                    "sent": "If you look at the original objective function of.",
                    "label": 0
                },
                {
                    "sent": "R. NMF it doesn't guarantee a sparsity.",
                    "label": 0
                },
                {
                    "sent": "Right, so there is no guarantee that the spaces are sparse, so capturing a part of basically feature.",
                    "label": 0
                },
                {
                    "sent": "And there are versions that they want to add a sparsity by adding L1 norm organization and so here actually you know the sparsity is guaranteed because when you find the submatrix in leading eigenvector vector corresponding to the whole matrix, the rest of the I mean entries correspond to the rest of the matrix are going to be 0, so it's pretty sparse.",
                    "label": 0
                },
                {
                    "sent": "And that's one reason that you know it captures the.",
                    "label": 0
                },
                {
                    "sent": "Parts this is basically intuitively, but as I said, theoretically the proof that you know maximizing this objective function will find the right terms.",
                    "label": 0
                },
                {
                    "sent": "An right topics and write features under the assumption that we have for the model of effort, model of text and image.",
                    "label": 0
                },
                {
                    "sent": "Carryover for college.",
                    "label": 0
                },
                {
                    "sent": "Automatically.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried that on color images, I don't know.",
                    "label": 0
                },
                {
                    "sent": "The theorem doesn't extend to color image, the term is just for bitmap images, and the term that we have experimentally I don't know.",
                    "label": 0
                },
                {
                    "sent": "Or the top into colorless.",
                    "label": 0
                },
                {
                    "sent": "Order one hopefully or.",
                    "label": 0
                },
                {
                    "sent": "Virginia acquisition just go over it in someone else is not working on lower right, said overlap.",
                    "label": 0
                },
                {
                    "sent": "So what we have, what country?",
                    "label": 0
                },
                {
                    "sent": "But at your ever applied for another computer or Internet, or also overlap between basis vectors question, I think you're so you're basically taking chromium agriculture companies operating station.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "What image then multiple colors?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, you know apart if you know part of assumption in the theory.",
                    "label": 0
                },
                {
                    "sent": "As I said, is that the Corp is epsilon separable by epsilon separable.",
                    "label": 0
                },
                {
                    "sent": "Basically naively it means that yeah, we have a kind of block matrix.",
                    "label": 0
                },
                {
                    "sent": "I mean terms coming from one topic are not in another topic ideally, but.",
                    "label": 0
                },
                {
                    "sent": "By epsilon.",
                    "label": 0
                },
                {
                    "sent": "Separable basically means that the chance the probability that terms from one topic appear in another topic.",
                    "label": 0
                },
                {
                    "sent": "It's possible, but the chance is less than epsilon.",
                    "label": 0
                },
                {
                    "sent": "And we showed that the proof that you know the algorithm works perfectly if there are.",
                    "label": 0
                },
                {
                    "sent": "If this epsilon is pretty small, you know.",
                    "label": 0
                },
                {
                    "sent": "If there are huge overlap between these blocks.",
                    "label": 0
                },
                {
                    "sent": "There's a good chance that this algorithm doesn't work well, but in practice actually on all kinds of data set that we try.",
                    "label": 0
                },
                {
                    "sent": "It's better than other.",
                    "label": 0
                },
                {
                    "sent": "NMF algorithm at something that I forgot to mention is that this is much faster than those you know in this, like data, text data set, some of the previous implementation of North is not even.",
                    "label": 0
                },
                {
                    "sent": "You can't even run them.",
                    "label": 0
                },
                {
                    "sent": "You know it takes a couple of days throughout, but this takes less than 3 minutes too.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}