{
    "id": "e3lovsokq4airxchkjsceuv7tjzo7ktn",
    "title": "Advanced Topics in RL",
    "info": {
        "author": [
            "Joelle Pineau, McGill University"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_pineau_advanced_topics/",
    "segmentation": [
        [
            "I am going to finish at the spot where we usually begin, which is how do we collect the data and the reason I finish with this I guess is because in some sense I unfortunately I don't have as many insightful things as I wish I did in the sense that it is very much still an open problem and so I still want to give you a little bit of a taste of what are the techniques that are out there for exploration.",
            "For figuring out how to collect the data.",
            "But certainly expect to come out with few answers to your problems."
        ],
        [
            "So this is really this problem of trading off between exploration and exploitation, right when we choose our actions, we can gather reward, but we can also gather information and so how do we trade off between picking actions that will return more information versus picking actions that we know have a proven track record of yielding good reward."
        ],
        [
            "One of the biggest applications of this on the market right now turns out there's many people who care a lot about this.",
            "Explore, exploit problem an and they are interested in Internet advertising.",
            "How much when you visit a webpage should they choose ads to place on it that are designed to get better information regarding which add you're likely to click on or how much should just just project the ads that that you think have a higher probability of being clicked on an.",
            "This is a I don't want to.",
            "Put a number dollar, but it's a very very big industry right now."
        ],
        [
            "There's a lot of people also dealing with.",
            "Computing infrastructure trying to figure out when you have jobs coming in.",
            "How should we be dispatching these jobs across several servers?",
            "In that case, you know something comes in the pipeline.",
            "You have a family of different servers, all with different loads.",
            "That job has different characteristics.",
            "How do you decide how to do that and how do you use all exploration to figure out a better model of how to do this better in?"
        ],
        [
            "Of my own interests, I'm actually interested in cases that have a much different characteristic, in that we are trying to explore the choice of treatment.",
            "In this case, this is drawn from a study worth doing, trying to optimize the choice of treatments for cancer.",
            "So we're doing this study with animals right now, and we have four different treatments.",
            "Were interested in trying out, and we want to look at the sequence of these treatment.",
            "Do we give treatment a treatment?",
            "BA&B together nothing at all.",
            "Over a sequence of time steps and we have to figure out which of these four treatments is better as a function of this feature space.",
            "In this particular study, literally we are going to get about we're doing this study with mice about 10 animals with which to do the full study, and so we have to be very, very careful about exploration in this case and trying to make the most out of the information we can get.",
            "But at the same time, trying to make your animals live longer so that we can get more information from them."
        ],
        [
            "So all of these problems are typically cast in the framework of multi arm bandits.",
            "That's the basic framework for this.",
            "There is a really rich literature on bandits and my job today is really just to make you aware of that literature.",
            "Give you a taste for it, and if you're interested, there's been a number of tutorials on bandits in recent years at NIPS that I smell similar venues, and they're all on line, so there's incredible resources.",
            "For those of you who are interested in learning a little bit more so in the framework, sometimes called multi arm Bandit or KR bed that you think of it up slot machine and there's a bunch of different arms.",
            "You want to figure out which arm to pull, except this machine is a little bit different, maybe than your standard slot machine or what they tell you about these standard slot machine in that it's presumed that the arms have different expectations, so different expected return.",
            "So your job is to figure out jointly you know.",
            "Should I pull this arm which gave me money the last time around, or should I try this other arm which might give me more money, but I don't know much about so that really crystallizes that tradeoff.",
            "We usually use mu of A to denote the.",
            "Expected utility of a particular arm, so the expected reward you'll get for pulling that ARM and in the next 5 minutes or so, I'm going to move away from the MDP states framework where I really emphasize we were looking at the sequence of action, and I'm going to assume that I'm looking just at a single decision, so the bandit framework is essentially the one step M DPS and MVP with the trajectory of 1, so it's sort of simplifies some the other issues."
        ],
        [
            "So in the multi Arm Bandit, the most important notion in terms of our optimization criteria is this notion of a regret.",
            "And the regret really measures the difference between how well you would have done if over your whole data exploration you already knew everything there was to know about your arm, so you had perfect information about the expected value of your different arms.",
            "Then what would you do right?",
            "Maybe you would have a deterministic strategy to always put the pull the arm with the highest expected reward, so the regret is the gap summed up over the length of your experiment.",
            "Between this optimal value mustar and knew of a so that expected value of pulling a particular arm at the time where you pulled it.",
            "So the bandits essentially are looking for an adaptive strategy.",
            "That's going to minimize the regret.",
            "So when we think of loss when I talked about MDP SAR, optimization notion was really this idea of maximizing the sum of rewards over the lifetime of the agent.",
            "In this framework, I'm interested in minimizing the regret.",
            "Over the learning."
        ],
        [
            "And so this is an example of a very simple strategy, right?",
            "We could decide that 10% of the time we're going to choose a random action and another 90% of the time we're going to choose the action, which so far has the best expected utility, and so it's adaptive in the sense of that choice of the best arm changes.",
            "Overtime, the strategy itself is fixed, but you're going to pick different arms based on what you've observed so far.",
            "This is really close to what people are doing.",
            "Practice in many."
        ],
        [
            "Cases and it's called epsilon greedy action selection, so this is one of the most common way to do exploration.",
            "It's remarkably robust.",
            "We've tried to do many things that are much, much more sophisticated, but in many cases this still turns out to be hard to beat, so we set some epsilon, usually to some small amount, and with that probability epsilon we're going to pick a random arm, and with probability 1 minus epsilon, we're actually going to pull the best arm we have so far.",
            "And we can put epsilon on a schedule where we decrease the exploration rate overtime.",
            "This is often what's done.",
            "The advantage of this, of course, is that it's very simple to implement.",
            "Anyone can run with this.",
            "It applies very easily to very large problems.",
            "We don't worry about the scalability of this, it applies in continuous state spaces, function approximation.",
            "There's no problem with that.",
            "It turns out that it has some pretty weak theoretical properties compared to some of the smarter methods in particular with respect to the regret.",
            "And I'm not going to go today into any of these theoretical properties, but if you start looking at this bandit literature, what you'll find is that most of them are in fact quite theoretical.",
            "Papers that are interested in proving results with respect to the regret.",
            "In particular, the sample complexity to achieve a particular regret.",
            "So many of the results will look up the form you need, see so many samples to make sure that your regret is bounded by particular quantity, and now you start looking at what are the things that show up in that quantity in terms of the properties of your domain.",
            "The second really simple."
        ],
        [
            "Approach is softmax action selection.",
            "Sometimes it's called the Boltzmann exploration strategy.",
            "The idea in this case it's an adaptation of epsilon, greedy and you're really looking at making your action probability function of the magnitude of that Q value.",
            "So in the previous epsilon greedy strategy."
        ],
        [
            "Is just you pick the best one with a particular probability and you pick a random one with another probably."
        ],
        [
            "This looks as a distribution and it forms a distribution using the Q function using the Boltzmann expression.",
            "So it's going to be proportional.",
            "The higher the Q value, the more likely you're going to pick that action.",
            "I'm missing a normalization factor in there, but it's going to be essentially proportional to Q function and what you see is you hav E to the Q divided by Tao and Tao acts really as a parameter to control how much you're being favorable to the high Q function.",
            "It's very similar to the temperature parameter you see.",
            "For example, in simulated annealing, and so the problem we're seeing here is not quite so different from the type of problems we would use simulated annealing that we're really trying to optimize a function."
        ],
        [
            "A slightly more sophisticated method that's been proposed.",
            "This one's been around for a long time 1933 by a fellow called Thompson.",
            "Scott Thompson sampling and it turns out to be quite hard to beat it sub Asian strategy.",
            "In that case we're actually going to keep a posterior over the expected value of the arm, so we're going to say maybe this variable is a characterized by an Bernoulli distribution.",
            "We have a posterior over that variable and we're going to sample according to that posterior and play the arm.",
            "That's the best with respect to that sampled utility.",
            "And in many cases for medical treatment design, this is the type of approach that's chosen because it gives you some better characterization of your uncertainty.",
            "So when you're picking one treatment versus another, you're able to better characterize tradeoffs.",
            "And when you stop your trial, you also have a better characterization in terms of your performance.",
            "In this particular case.",
            "Thompson sampling is very simple to implement, but in terms of analyzing its properties and deriving results for bounds on the regret, it was surprisingly resistant to analysis, and it's only in the last two or three years that we've started to have sample complexity results for the Thompson sampling case.",
            "But practically, it was very, very good."
        ],
        [
            "And the last family of approaches I'll mention in my whirlwind tour of exploration method is a family of methods that are based on this notion of an upper confidence bound.",
            "And the notion here is really that you again preserve an estimate of your expected value for each action.",
            "But your estimate is in the form of a confidence interval, and so you're always going to sample the arm that has the highest upper bound, and when you sample that arm that has the highest upper bound, essentially, either you're going to find that that ARM is really that good, and that will be great.",
            "You'll have exploited, essentially without intending to, or you'll find that that ARM is not that good, and then your upper bound will shrink down very quickly, and you'll learn that it doesn't do so well, and so this framework is the one that has had the most.",
            "Results in terms of theoretical analysis of its properties.",
            "Yes.",
            "And it's based on this concept of optimism in the face of uncertainty.",
            "Yes, that's kind of the fundamental principle in this case."
        ],
        [
            "I won't tell you I don't want to right now."
        ],
        [
            "Your parade, but empirically.",
            "That one would be my last theoretically beautiful results, so we're going to have models beside you.",
            "OK, alright?"
        ],
        [
            "Everything I've said so far is for bandits.",
            "I said there was a horizon of one.",
            "I also didn't tell you, but probably you observed there was no state really.",
            "There's no notion of a state, and so the extension of the bandit literata the case with state is called contextual Bandit.",
            "It turns out that many of the methods I presented have extensions that are quite straightforward to the case of contextual bandits, so you can still maintain a posterior probability if you want to do Thompson sampling.",
            "Or a UCB bound that is conditioned on state.",
            "The theoretical analysis changes quite a bit, but in terms of practical implementation, it's very, very similar.",
            "I will."
        ],
        [
            "Finish by just telling you one last piece about going to Markov decision process.",
            "And that is quite often the standard approach that people are using to this day is either form of epsilon, greedy, or Bozeman exploration that seems to do quite well in the case where you have very large spaces to explore, what seemed to be most effective, and we're seeing that in some of the policy optimization method, this is what we saw in Alpha go as well is to actually start with a phase of imitation learning.",
            "So get an expert to give you some trajectory's that are drawn from expert trajectories.",
            "After you've done augmentation learning, you have a pretty good policy right in the case of the Alpha go system, they actually trained on neural network to predict that policy, then do local exploration around that policy so you can use your epsilon greedy, but around that really good initial policy in the case where you're doing with very small sample spaces, there's a large number of extensions of the Bayesian methods.",
            "Past Thompson sampling, there's several different directions of doing that.",
            "We've recently released the foundations and trends manuscript on beige and Reinforcement learning a survey.",
            "So if you want the 150 page version of this slide, I encourage you to access it."
        ],
        [
            "I will finish with like 3 sort of simple comments.",
            "One of them is essentially if you can't control your data collection, I absolve you from worry and just make sure that you worry about correcting the policy if necessary.",
            "In the case where you can afford to collect lots of data, then after then maybe these kinds of greedy methods might do just as well, starting usually with an optimistic initialization of your Q function.",
            "And having an optimistic initialization, this is this notion of optimism in the face of uncertainty is a good enough driver of exploration, and if you can't afford to collect data, you only can afford very limited amounts of data, then I think looking to the beige and methods that give you an explicit tradeoff between exploration and exploitation.",
            "By characterizing your uncertainty are quite promising.",
            "My personal favorite these days is a new method called Bessel that's been developed by Shimon or another few of his students.",
            "And the advantage of best over something like the Thompson sampling is that you don't have to characterize the distribution nearly as much, and they have a very cute way to characterize your uncertainty based on your data."
        ],
        [
            "I will leave it at that for now.",
            "I don't want to stand in the way of your coffee much longer, but at least you have a little taste of what you might think about.",
            "If you need to control your data collection, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am going to finish at the spot where we usually begin, which is how do we collect the data and the reason I finish with this I guess is because in some sense I unfortunately I don't have as many insightful things as I wish I did in the sense that it is very much still an open problem and so I still want to give you a little bit of a taste of what are the techniques that are out there for exploration.",
                    "label": 0
                },
                {
                    "sent": "For figuring out how to collect the data.",
                    "label": 0
                },
                {
                    "sent": "But certainly expect to come out with few answers to your problems.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is really this problem of trading off between exploration and exploitation, right when we choose our actions, we can gather reward, but we can also gather information and so how do we trade off between picking actions that will return more information versus picking actions that we know have a proven track record of yielding good reward.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the biggest applications of this on the market right now turns out there's many people who care a lot about this.",
                    "label": 1
                },
                {
                    "sent": "Explore, exploit problem an and they are interested in Internet advertising.",
                    "label": 1
                },
                {
                    "sent": "How much when you visit a webpage should they choose ads to place on it that are designed to get better information regarding which add you're likely to click on or how much should just just project the ads that that you think have a higher probability of being clicked on an.",
                    "label": 0
                },
                {
                    "sent": "This is a I don't want to.",
                    "label": 0
                },
                {
                    "sent": "Put a number dollar, but it's a very very big industry right now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a lot of people also dealing with.",
                    "label": 0
                },
                {
                    "sent": "Computing infrastructure trying to figure out when you have jobs coming in.",
                    "label": 0
                },
                {
                    "sent": "How should we be dispatching these jobs across several servers?",
                    "label": 0
                },
                {
                    "sent": "In that case, you know something comes in the pipeline.",
                    "label": 0
                },
                {
                    "sent": "You have a family of different servers, all with different loads.",
                    "label": 0
                },
                {
                    "sent": "That job has different characteristics.",
                    "label": 0
                },
                {
                    "sent": "How do you decide how to do that and how do you use all exploration to figure out a better model of how to do this better in?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of my own interests, I'm actually interested in cases that have a much different characteristic, in that we are trying to explore the choice of treatment.",
                    "label": 0
                },
                {
                    "sent": "In this case, this is drawn from a study worth doing, trying to optimize the choice of treatments for cancer.",
                    "label": 0
                },
                {
                    "sent": "So we're doing this study with animals right now, and we have four different treatments.",
                    "label": 0
                },
                {
                    "sent": "Were interested in trying out, and we want to look at the sequence of these treatment.",
                    "label": 0
                },
                {
                    "sent": "Do we give treatment a treatment?",
                    "label": 0
                },
                {
                    "sent": "BA&B together nothing at all.",
                    "label": 0
                },
                {
                    "sent": "Over a sequence of time steps and we have to figure out which of these four treatments is better as a function of this feature space.",
                    "label": 0
                },
                {
                    "sent": "In this particular study, literally we are going to get about we're doing this study with mice about 10 animals with which to do the full study, and so we have to be very, very careful about exploration in this case and trying to make the most out of the information we can get.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, trying to make your animals live longer so that we can get more information from them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all of these problems are typically cast in the framework of multi arm bandits.",
                    "label": 0
                },
                {
                    "sent": "That's the basic framework for this.",
                    "label": 0
                },
                {
                    "sent": "There is a really rich literature on bandits and my job today is really just to make you aware of that literature.",
                    "label": 0
                },
                {
                    "sent": "Give you a taste for it, and if you're interested, there's been a number of tutorials on bandits in recent years at NIPS that I smell similar venues, and they're all on line, so there's incredible resources.",
                    "label": 0
                },
                {
                    "sent": "For those of you who are interested in learning a little bit more so in the framework, sometimes called multi arm Bandit or KR bed that you think of it up slot machine and there's a bunch of different arms.",
                    "label": 0
                },
                {
                    "sent": "You want to figure out which arm to pull, except this machine is a little bit different, maybe than your standard slot machine or what they tell you about these standard slot machine in that it's presumed that the arms have different expectations, so different expected return.",
                    "label": 0
                },
                {
                    "sent": "So your job is to figure out jointly you know.",
                    "label": 0
                },
                {
                    "sent": "Should I pull this arm which gave me money the last time around, or should I try this other arm which might give me more money, but I don't know much about so that really crystallizes that tradeoff.",
                    "label": 0
                },
                {
                    "sent": "We usually use mu of A to denote the.",
                    "label": 0
                },
                {
                    "sent": "Expected utility of a particular arm, so the expected reward you'll get for pulling that ARM and in the next 5 minutes or so, I'm going to move away from the MDP states framework where I really emphasize we were looking at the sequence of action, and I'm going to assume that I'm looking just at a single decision, so the bandit framework is essentially the one step M DPS and MVP with the trajectory of 1, so it's sort of simplifies some the other issues.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the multi Arm Bandit, the most important notion in terms of our optimization criteria is this notion of a regret.",
                    "label": 0
                },
                {
                    "sent": "And the regret really measures the difference between how well you would have done if over your whole data exploration you already knew everything there was to know about your arm, so you had perfect information about the expected value of your different arms.",
                    "label": 0
                },
                {
                    "sent": "Then what would you do right?",
                    "label": 0
                },
                {
                    "sent": "Maybe you would have a deterministic strategy to always put the pull the arm with the highest expected reward, so the regret is the gap summed up over the length of your experiment.",
                    "label": 0
                },
                {
                    "sent": "Between this optimal value mustar and knew of a so that expected value of pulling a particular arm at the time where you pulled it.",
                    "label": 0
                },
                {
                    "sent": "So the bandits essentially are looking for an adaptive strategy.",
                    "label": 1
                },
                {
                    "sent": "That's going to minimize the regret.",
                    "label": 0
                },
                {
                    "sent": "So when we think of loss when I talked about MDP SAR, optimization notion was really this idea of maximizing the sum of rewards over the lifetime of the agent.",
                    "label": 0
                },
                {
                    "sent": "In this framework, I'm interested in minimizing the regret.",
                    "label": 0
                },
                {
                    "sent": "Over the learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so this is an example of a very simple strategy, right?",
                    "label": 0
                },
                {
                    "sent": "We could decide that 10% of the time we're going to choose a random action and another 90% of the time we're going to choose the action, which so far has the best expected utility, and so it's adaptive in the sense of that choice of the best arm changes.",
                    "label": 1
                },
                {
                    "sent": "Overtime, the strategy itself is fixed, but you're going to pick different arms based on what you've observed so far.",
                    "label": 0
                },
                {
                    "sent": "This is really close to what people are doing.",
                    "label": 0
                },
                {
                    "sent": "Practice in many.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cases and it's called epsilon greedy action selection, so this is one of the most common way to do exploration.",
                    "label": 0
                },
                {
                    "sent": "It's remarkably robust.",
                    "label": 0
                },
                {
                    "sent": "We've tried to do many things that are much, much more sophisticated, but in many cases this still turns out to be hard to beat, so we set some epsilon, usually to some small amount, and with that probability epsilon we're going to pick a random arm, and with probability 1 minus epsilon, we're actually going to pull the best arm we have so far.",
                    "label": 0
                },
                {
                    "sent": "And we can put epsilon on a schedule where we decrease the exploration rate overtime.",
                    "label": 0
                },
                {
                    "sent": "This is often what's done.",
                    "label": 0
                },
                {
                    "sent": "The advantage of this, of course, is that it's very simple to implement.",
                    "label": 0
                },
                {
                    "sent": "Anyone can run with this.",
                    "label": 0
                },
                {
                    "sent": "It applies very easily to very large problems.",
                    "label": 0
                },
                {
                    "sent": "We don't worry about the scalability of this, it applies in continuous state spaces, function approximation.",
                    "label": 0
                },
                {
                    "sent": "There's no problem with that.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it has some pretty weak theoretical properties compared to some of the smarter methods in particular with respect to the regret.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to go today into any of these theoretical properties, but if you start looking at this bandit literature, what you'll find is that most of them are in fact quite theoretical.",
                    "label": 0
                },
                {
                    "sent": "Papers that are interested in proving results with respect to the regret.",
                    "label": 0
                },
                {
                    "sent": "In particular, the sample complexity to achieve a particular regret.",
                    "label": 0
                },
                {
                    "sent": "So many of the results will look up the form you need, see so many samples to make sure that your regret is bounded by particular quantity, and now you start looking at what are the things that show up in that quantity in terms of the properties of your domain.",
                    "label": 0
                },
                {
                    "sent": "The second really simple.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach is softmax action selection.",
                    "label": 1
                },
                {
                    "sent": "Sometimes it's called the Boltzmann exploration strategy.",
                    "label": 1
                },
                {
                    "sent": "The idea in this case it's an adaptation of epsilon, greedy and you're really looking at making your action probability function of the magnitude of that Q value.",
                    "label": 0
                },
                {
                    "sent": "So in the previous epsilon greedy strategy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is just you pick the best one with a particular probability and you pick a random one with another probably.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This looks as a distribution and it forms a distribution using the Q function using the Boltzmann expression.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be proportional.",
                    "label": 0
                },
                {
                    "sent": "The higher the Q value, the more likely you're going to pick that action.",
                    "label": 0
                },
                {
                    "sent": "I'm missing a normalization factor in there, but it's going to be essentially proportional to Q function and what you see is you hav E to the Q divided by Tao and Tao acts really as a parameter to control how much you're being favorable to the high Q function.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to the temperature parameter you see.",
                    "label": 1
                },
                {
                    "sent": "For example, in simulated annealing, and so the problem we're seeing here is not quite so different from the type of problems we would use simulated annealing that we're really trying to optimize a function.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A slightly more sophisticated method that's been proposed.",
                    "label": 0
                },
                {
                    "sent": "This one's been around for a long time 1933 by a fellow called Thompson.",
                    "label": 0
                },
                {
                    "sent": "Scott Thompson sampling and it turns out to be quite hard to beat it sub Asian strategy.",
                    "label": 1
                },
                {
                    "sent": "In that case we're actually going to keep a posterior over the expected value of the arm, so we're going to say maybe this variable is a characterized by an Bernoulli distribution.",
                    "label": 1
                },
                {
                    "sent": "We have a posterior over that variable and we're going to sample according to that posterior and play the arm.",
                    "label": 0
                },
                {
                    "sent": "That's the best with respect to that sampled utility.",
                    "label": 0
                },
                {
                    "sent": "And in many cases for medical treatment design, this is the type of approach that's chosen because it gives you some better characterization of your uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So when you're picking one treatment versus another, you're able to better characterize tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "And when you stop your trial, you also have a better characterization in terms of your performance.",
                    "label": 0
                },
                {
                    "sent": "In this particular case.",
                    "label": 0
                },
                {
                    "sent": "Thompson sampling is very simple to implement, but in terms of analyzing its properties and deriving results for bounds on the regret, it was surprisingly resistant to analysis, and it's only in the last two or three years that we've started to have sample complexity results for the Thompson sampling case.",
                    "label": 0
                },
                {
                    "sent": "But practically, it was very, very good.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last family of approaches I'll mention in my whirlwind tour of exploration method is a family of methods that are based on this notion of an upper confidence bound.",
                    "label": 1
                },
                {
                    "sent": "And the notion here is really that you again preserve an estimate of your expected value for each action.",
                    "label": 1
                },
                {
                    "sent": "But your estimate is in the form of a confidence interval, and so you're always going to sample the arm that has the highest upper bound, and when you sample that arm that has the highest upper bound, essentially, either you're going to find that that ARM is really that good, and that will be great.",
                    "label": 0
                },
                {
                    "sent": "You'll have exploited, essentially without intending to, or you'll find that that ARM is not that good, and then your upper bound will shrink down very quickly, and you'll learn that it doesn't do so well, and so this framework is the one that has had the most.",
                    "label": 0
                },
                {
                    "sent": "Results in terms of theoretical analysis of its properties.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And it's based on this concept of optimism in the face of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's kind of the fundamental principle in this case.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I won't tell you I don't want to right now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your parade, but empirically.",
                    "label": 0
                },
                {
                    "sent": "That one would be my last theoretically beautiful results, so we're going to have models beside you.",
                    "label": 0
                },
                {
                    "sent": "OK, alright?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything I've said so far is for bandits.",
                    "label": 0
                },
                {
                    "sent": "I said there was a horizon of one.",
                    "label": 0
                },
                {
                    "sent": "I also didn't tell you, but probably you observed there was no state really.",
                    "label": 0
                },
                {
                    "sent": "There's no notion of a state, and so the extension of the bandit literata the case with state is called contextual Bandit.",
                    "label": 1
                },
                {
                    "sent": "It turns out that many of the methods I presented have extensions that are quite straightforward to the case of contextual bandits, so you can still maintain a posterior probability if you want to do Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "Or a UCB bound that is conditioned on state.",
                    "label": 0
                },
                {
                    "sent": "The theoretical analysis changes quite a bit, but in terms of practical implementation, it's very, very similar.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finish by just telling you one last piece about going to Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "And that is quite often the standard approach that people are using to this day is either form of epsilon, greedy, or Bozeman exploration that seems to do quite well in the case where you have very large spaces to explore, what seemed to be most effective, and we're seeing that in some of the policy optimization method, this is what we saw in Alpha go as well is to actually start with a phase of imitation learning.",
                    "label": 0
                },
                {
                    "sent": "So get an expert to give you some trajectory's that are drawn from expert trajectories.",
                    "label": 0
                },
                {
                    "sent": "After you've done augmentation learning, you have a pretty good policy right in the case of the Alpha go system, they actually trained on neural network to predict that policy, then do local exploration around that policy so you can use your epsilon greedy, but around that really good initial policy in the case where you're doing with very small sample spaces, there's a large number of extensions of the Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "Past Thompson sampling, there's several different directions of doing that.",
                    "label": 0
                },
                {
                    "sent": "We've recently released the foundations and trends manuscript on beige and Reinforcement learning a survey.",
                    "label": 0
                },
                {
                    "sent": "So if you want the 150 page version of this slide, I encourage you to access it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will finish with like 3 sort of simple comments.",
                    "label": 0
                },
                {
                    "sent": "One of them is essentially if you can't control your data collection, I absolve you from worry and just make sure that you worry about correcting the policy if necessary.",
                    "label": 1
                },
                {
                    "sent": "In the case where you can afford to collect lots of data, then after then maybe these kinds of greedy methods might do just as well, starting usually with an optimistic initialization of your Q function.",
                    "label": 1
                },
                {
                    "sent": "And having an optimistic initialization, this is this notion of optimism in the face of uncertainty is a good enough driver of exploration, and if you can't afford to collect data, you only can afford very limited amounts of data, then I think looking to the beige and methods that give you an explicit tradeoff between exploration and exploitation.",
                    "label": 0
                },
                {
                    "sent": "By characterizing your uncertainty are quite promising.",
                    "label": 0
                },
                {
                    "sent": "My personal favorite these days is a new method called Bessel that's been developed by Shimon or another few of his students.",
                    "label": 0
                },
                {
                    "sent": "And the advantage of best over something like the Thompson sampling is that you don't have to characterize the distribution nearly as much, and they have a very cute way to characterize your uncertainty based on your data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will leave it at that for now.",
                    "label": 0
                },
                {
                    "sent": "I don't want to stand in the way of your coffee much longer, but at least you have a little taste of what you might think about.",
                    "label": 0
                },
                {
                    "sent": "If you need to control your data collection, thanks.",
                    "label": 0
                }
            ]
        }
    }
}