{
    "id": "rdjl2vs3jwi5x6ncpldw6ws4kewiuqjv",
    "title": "Learning to Combine Discriminative Classifiers",
    "info": {
        "author": [
            "Chi-Hoon Lee, Yahoo! Research Silicon Valley"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Artificial Intelligence",
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd2010_lee_lcd/",
    "segmentation": [
        [
            "Thank you very much.",
            "My name is June and the the paper title I'm going to present is the learning to combine this classifier.",
            "As you can see from the title, this talk is pretty much focused on example framework, so trying to combine multiple responses in order to produce single response."
        ],
        [
            "So this is slightly showing the overview of today's talk in the introduction.",
            "I'm going to particularly talk about the main idea and the motivation of this work, and then an background I'm going to call a little bit deeper so that we can try to address the difference between this work and the previous work, and then in foundation I'm going to primarily focused on two major aspects, example, the framework and example is the.",
            "As I mentioned before it trying to combine multiple responses so that there are two major.",
            "Principles one is the combination rule that always waiting over individual classifier.",
            "I'm going to actually cover this and then in the experiments I'm going to show you some experimental result and then in conclusion I'm going to summarize and then showing some future work."
        ],
        [
            "So classification task is a very popular in industry and academia research community.",
            "So if I summarize in one sentence, given the testing instance we want to label it as a one of the predefined categories.",
            "Example of the application is each transaction fraud which is very popular in sponsor search and as a query have shopping intent which is very important in in search engine.",
            "If we know users query intent is shopping.",
            "You can just search engine just your focus on subset of indexed document.",
            "So that we can actually increase the users the experience.",
            "And the other example is, does this patient require specific test?",
            "An another example is either twist spam, which is very popular in social networking.",
            "But the challenge here in the here is that the how to develop an effective classifier for the classification task due to the different data assumption about data distribution or some kind of characteristic, we can appeal the one single mega super power of the classifier.",
            "So we have to explore other possibilities."
        ],
        [
            "So in order to explore and to develop such a effective classifier, which is also known as the decision function, typically we require require training instance of the setup of the patient and its corresponding class label.",
            "And using this training data, we are going to fit a model parameter which is typically not as a data.",
            "And to improve the effectiveness of this classifier, actually many technologies have been discussed to going from generative versus discriminative approach.",
            "So this is the assumption about the relationship between observation and its corresponding class label.",
            "And the other actually technology discussion is that the assumption about if weather data are IID or non IID.",
            "So this is pretty much the based on single.",
            "For example like the live page versus hidden Markov model, assuming if there is structure.",
            "And the other actually discussion could be a single response versus multiple responses, which is the primary topic of Twitter.",
            "So example is the actual main topic of this talk, and then in this talk I'm going to provide to the principle to how to combine responses and then how we are going to wait of individual responses, so specifically the weighting scheme of this work is the actual confidence based, which means whenever a an individual classifier is confident on.",
            "A testing instance, it can actually loudly speak out, and then that loudness is considered as the weight, so that actually this weight is not actually static statically.",
            "The bounding is dynamically changes depending on the testing instance."
        ],
        [
            "So now I'm going to talk about."
        ],
        [
            "Background.",
            "So, example learning and theoretically empirically proven to be effective in many different application and data domain.",
            "So the challenges two major challenges associated with an example learning is the how to combine responses and how to generate the weight.",
            "So example of the combined responses is the we can just simply taking average which is the begging is one of the example an or we can just add it if the aggregation by their ways boosting and each variant for example like.",
            "OK, stick reading posting.",
            "And in terms of the weight, how to generate the weight?",
            "The weight can be actually based on error made on training samples so that basically we can optimize sort of the optimize optimize certain objective function during the training and then use this fixed weight during the testing so it is statically divided during the testing and the other example is based on confidence made on testing example so they can actually formulate the certainty.",
            "For this measure, whenever they receive the testing instance."
        ],
        [
            "So if you I really called differ for the way how to wait computer weight of responses in the related work.",
            "So I mentioned the bagging which is the uniform weight every individual instance has the same weight and the other one is a non uniform weight which is actually different classifier has different weight.",
            "So one example is a forward stepwise.",
            "So we can use actually wait to the training sample based on previous decision function.",
            "Example of this work is a double stand stochastic gradient boosting.",
            "As we can see this, the objective function displayed this objective function is not actually the final objective function.",
            "This is the objective function for to find the weight of individual classifier.",
            "So beta M is in M iteration.",
            "We're trying to find the object the weight of.",
            "An individual classifier that minimizing this objective function.",
            "An the other actually example for the non uniform weight is.",
            "Accordingly we can consider correlations among responses.",
            "So one of the examples are trying to minimize estimated actually generalization error.",
            "Actually that involves a variance term defined as ambiguity.",
            "This is example from the neural network example of the neural network.",
            "So they define the values as actual difference between a response from an individual classifier and the certain target function over the expected expected response from the all of the response from the K different classifier."
        ],
        [
            "So this is actually since our work is confident space and this confidence is computed by variance.",
            "So I'm going to briefly talk about some kind of work that also the talk about the variance.",
            "So 1993 and 1994 Perron and Crow actually day also defined the variance.",
            "And in the example framework in neural network.",
            "So the various inapparent defined by the.",
            "By showing this slide and the majorly, the M is the difference between the target function and the each individual classifier.",
            "So they must use the A train example to optimize this, and then a Crow actually defined the variance as a difference between a individual classifier and the expected value.",
            "Expected responses from the individual classifier.",
            "So one of the input here, the weight of each network, is it to learn between the training and test during the testing the weight it's actually fixed.",
            "And 2006 actually actually defined the variance from the from the response of testing instance.",
            "Actually, this work is prevented quite similar to the current work, but the this work is each individual classifier is from the Bayesian network, while our work is based on discrete approach.",
            "And 2007 and 1992 in active learning research community.",
            "They also use the variance, which is quite a similar, but the similar to our work.",
            "But this actually the variance it used to quantify the benefit of the laboring and you sample.",
            "And the variance is also used in the mixture of expert, the research area."
        ],
        [
            "So now I'm going to move to talk about the foundation of this work.",
            "Pretty much focus on combination rule and weight of individual classifier."
        ],
        [
            "So.",
            "I wasn't trying to actually make a pedestal sink and summarize the slide to explain the principle of this work, so pretty much we need to develop the combination rule and weight rule for example framework.",
            "So assumption here I made is the we assume that correct answer Lambda exist.",
            "And then we view a response.",
            "Response from each individual classifier among a different view classifier.",
            "As an estimate of Lambda with a random error which is absolute I so that each response is going to be represented as a sum of this correct answer plus random noise which is actually similar to the same as the linear regression formulation.",
            "But nice thing about this formulation is that if we try to do maximum likelihood estimate from the distance formula and this the Lambda out, the maximum likelihood is actually expressed in terms of this number of the terms.",
            "And this actually maximum likely estimate as to provide us the nice way of combination rule as well as the waiting room, which is actually the inverse of this variance is going to show the how we are going to wait or response of the the response from each individual classifier and then this actually red circle one is actually showing us the combination how we can actually combine things which is additive."
        ],
        [
            "So now actually we need to define how what kind of response we're going to use.",
            "And how we can actually derive the variance from the response?",
            "So we have some choice at the freedom of choice based upon the previous this framework.",
            "So here I'm taking actually disconnected approaches.",
            "The reason is that this approach, actually empirical and theoretical, proven to be.",
            "To produce very effective declassification result and.",
            "And it is a actually popular for the many different application domain.",
            "So the example of this class approaches a support vector machine and large deviation.",
            "So here I'm going to use the logical relation for the Aditya response modeling because the reason is that which actually loads the relation of maximum likelihood estimator parameter formula stimulation actually allow us to efficiently compute the variance which is approximation."
        ],
        [
            "So this slide actually showing a very simple summary of the logical relation.",
            "It's probabilistic model and it's proven to be effective and the one thing we have to pay attention a little bit is that the model parameter is can be actually learned by maximizing this low likelihood and this slow like maximize the lower likelihood maximization estimate estimate actually provide us very efficient way to compute the variance."
        ],
        [
            "So now I'm going to talk about various Octa response, how we can compute.",
            "But if before, before we talk about variance of the response, we need to actually look at how we're going to derive the variance of the parameter first.",
            "And in order to compute to derive this variance of the parameter, we actually look at 2 nice property in the maximum likelihood estimate.",
            "So which is the consistency and a simpleton normality?",
            "So as you can see from this slide, as we have the larger number of the data, actually the mean of the difference between the optimal parameter and the estimate maximum likelihood parameter is going to be 0, and then we're going to have this the.",
            "The variance so that this variance we're going to actually use sufficient information, which is the actually expected value of the squared first derivative of low light with function which is showing this slide.",
            "Actually this sufficient information actually provide us a very nice property.",
            "Actually we can talk about in the authorization.",
            "So."
        ],
        [
            "The role of sufficient information is that by using actually a claim roll roll bound, or we can actually approximate the variance variance of the parameter which is showing the second example of here.",
            "Showing at this the formula.",
            "So based on using this, the camera roll around and we can actually approximate this.",
            "The variance as the variance of the response as the three different actually turned the product of this term.",
            "So as we can see from the first term and the last time, it's clear it's clear that this variance is based on instant speed so.",
            "And they wanting nice thing about this variance of the responses that it also proven to be useful in quantifying the benefits of this, letting you sample to label in active learning in the 2007 and 1992.",
            "So aggregating this all."
        ],
        [
            "Together I put I had some kind of city job to experience."
        ],
        [
            "So first round of experience I used the UCI data set I did 10 times of iteration, and each iteration and a three fold cross validation.",
            "The reason for the reason why I did a 10 times iteration is that UCI data set it typically has a very small number of.",
            "Some number of dirty data.",
            "So each year we each cross.",
            "A valid of each folder cross validation.",
            "I wasn't trying to minimize the impact of the skewed cluster distribution, so each time of the iteration I randomly to shuffle the data instances and then did 3 for the cross validation and then I used the 20 generated 20 individual classifiers in order to combine them together and each the individual classifier actually are randomly selected to actually.",
            "50% of the training sample and then they are trained on.",
            "Upon this the 50% of the data.",
            "So the reason why I selected a 50% is that this is not telling, no.",
            "Theoretical or the explanation, but I according to my holdout data set experiment, it shows the best as the to differentiate among the 20 classifiers.",
            "And then I compared our model which is I named NLR which is symbolized local likelihood padding and ADA boost.",
            "And then this is it."
        ],
        [
            "City showing the experimental result and also available in front of paper.",
            "So if you look at one thing, I'd like to briefly mention from this graph is that typically over in error is showing much better than single logistic regression SLR here.",
            "But some of the data instances I mean for this amount of data instances the improvement improvement is very marginal.",
            "So I plotted the number of instances.",
            "As actually axis and the number of feature as a Y axis is A2 dimensions, so I can see some kind of pattern here."
        ],
        [
            "That which is showing here.",
            "If we have a pretty number of reasonable enough number of training instance, I mean number of instances we can train.",
            "Considering the number of features, then we can actually see noticeability improvement, but if we have a.",
            "We have if we don't have enough number of number of instances considering number of the features that we are seeing, actually very marginal improvement.",
            "As we can see from the SRX.",
            "Cleave actually cleave and the heart or those kind of data set."
        ],
        [
            "And this is actually showing the comparing with the BEGGING and ADA boost."
        ],
        [
            "And I also did another actually experiment using your actual problem using actually Yahoo data set, which is the user issue.",
            "Search search query search engine need to actually decide whether the search engine results search engine results page includes a specific model such as the news product or image.",
            "Because we are trying to encourage users engagement."
        ],
        [
            "So I formulated this as the focus one of the example as an image trigger task and then formulated as an image of the classification problem so generated.",
            "So whole bunch of the query level, feature length, coding length, number of tokens and clear information.",
            "And as you can see from the slide you know we if you put it like a star Fox which is one of the game we want to show some kind of picture about this game so that people can encourage the people's engagement.",
            "So."
        ],
        [
            "So I did the same experiment and.",
            "Think of because of short.",
            "At the time I can just we can talk about in the poster session.",
            "More detail.",
            "Our model shows the better performance and."
        ],
        [
            "The future work you know.",
            "I do like to actually extended this work to multi classes or extension two other descriptive approach, another different sampling."
        ],
        [
            "Yeah, and you wanting?",
            "I'd like to actually prefer mention is that I checked the preceding version and then the figure one is very is loose lost.",
            "The advantage from the figure one is broken and I updated the one version in.",
            "And this URL.",
            "So please go and if you want to use Facebook, feel free to download."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "My name is June and the the paper title I'm going to present is the learning to combine this classifier.",
                    "label": 1
                },
                {
                    "sent": "As you can see from the title, this talk is pretty much focused on example framework, so trying to combine multiple responses in order to produce single response.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is slightly showing the overview of today's talk in the introduction.",
                    "label": 0
                },
                {
                    "sent": "I'm going to particularly talk about the main idea and the motivation of this work, and then an background I'm going to call a little bit deeper so that we can try to address the difference between this work and the previous work, and then in foundation I'm going to primarily focused on two major aspects, example, the framework and example is the.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before it trying to combine multiple responses so that there are two major.",
                    "label": 0
                },
                {
                    "sent": "Principles one is the combination rule that always waiting over individual classifier.",
                    "label": 1
                },
                {
                    "sent": "I'm going to actually cover this and then in the experiments I'm going to show you some experimental result and then in conclusion I'm going to summarize and then showing some future work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So classification task is a very popular in industry and academia research community.",
                    "label": 0
                },
                {
                    "sent": "So if I summarize in one sentence, given the testing instance we want to label it as a one of the predefined categories.",
                    "label": 1
                },
                {
                    "sent": "Example of the application is each transaction fraud which is very popular in sponsor search and as a query have shopping intent which is very important in in search engine.",
                    "label": 0
                },
                {
                    "sent": "If we know users query intent is shopping.",
                    "label": 0
                },
                {
                    "sent": "You can just search engine just your focus on subset of indexed document.",
                    "label": 0
                },
                {
                    "sent": "So that we can actually increase the users the experience.",
                    "label": 1
                },
                {
                    "sent": "And the other example is, does this patient require specific test?",
                    "label": 1
                },
                {
                    "sent": "An another example is either twist spam, which is very popular in social networking.",
                    "label": 0
                },
                {
                    "sent": "But the challenge here in the here is that the how to develop an effective classifier for the classification task due to the different data assumption about data distribution or some kind of characteristic, we can appeal the one single mega super power of the classifier.",
                    "label": 0
                },
                {
                    "sent": "So we have to explore other possibilities.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to explore and to develop such a effective classifier, which is also known as the decision function, typically we require require training instance of the setup of the patient and its corresponding class label.",
                    "label": 0
                },
                {
                    "sent": "And using this training data, we are going to fit a model parameter which is typically not as a data.",
                    "label": 1
                },
                {
                    "sent": "And to improve the effectiveness of this classifier, actually many technologies have been discussed to going from generative versus discriminative approach.",
                    "label": 1
                },
                {
                    "sent": "So this is the assumption about the relationship between observation and its corresponding class label.",
                    "label": 0
                },
                {
                    "sent": "And the other actually technology discussion is that the assumption about if weather data are IID or non IID.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty much the based on single.",
                    "label": 0
                },
                {
                    "sent": "For example like the live page versus hidden Markov model, assuming if there is structure.",
                    "label": 0
                },
                {
                    "sent": "And the other actually discussion could be a single response versus multiple responses, which is the primary topic of Twitter.",
                    "label": 0
                },
                {
                    "sent": "So example is the actual main topic of this talk, and then in this talk I'm going to provide to the principle to how to combine responses and then how we are going to wait of individual responses, so specifically the weighting scheme of this work is the actual confidence based, which means whenever a an individual classifier is confident on.",
                    "label": 0
                },
                {
                    "sent": "A testing instance, it can actually loudly speak out, and then that loudness is considered as the weight, so that actually this weight is not actually static statically.",
                    "label": 0
                },
                {
                    "sent": "The bounding is dynamically changes depending on the testing instance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Background.",
                    "label": 0
                },
                {
                    "sent": "So, example learning and theoretically empirically proven to be effective in many different application and data domain.",
                    "label": 1
                },
                {
                    "sent": "So the challenges two major challenges associated with an example learning is the how to combine responses and how to generate the weight.",
                    "label": 1
                },
                {
                    "sent": "So example of the combined responses is the we can just simply taking average which is the begging is one of the example an or we can just add it if the aggregation by their ways boosting and each variant for example like.",
                    "label": 0
                },
                {
                    "sent": "OK, stick reading posting.",
                    "label": 0
                },
                {
                    "sent": "And in terms of the weight, how to generate the weight?",
                    "label": 0
                },
                {
                    "sent": "The weight can be actually based on error made on training samples so that basically we can optimize sort of the optimize optimize certain objective function during the training and then use this fixed weight during the testing so it is statically divided during the testing and the other example is based on confidence made on testing example so they can actually formulate the certainty.",
                    "label": 1
                },
                {
                    "sent": "For this measure, whenever they receive the testing instance.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you I really called differ for the way how to wait computer weight of responses in the related work.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned the bagging which is the uniform weight every individual instance has the same weight and the other one is a non uniform weight which is actually different classifier has different weight.",
                    "label": 0
                },
                {
                    "sent": "So one example is a forward stepwise.",
                    "label": 0
                },
                {
                    "sent": "So we can use actually wait to the training sample based on previous decision function.",
                    "label": 1
                },
                {
                    "sent": "Example of this work is a double stand stochastic gradient boosting.",
                    "label": 0
                },
                {
                    "sent": "As we can see this, the objective function displayed this objective function is not actually the final objective function.",
                    "label": 0
                },
                {
                    "sent": "This is the objective function for to find the weight of individual classifier.",
                    "label": 0
                },
                {
                    "sent": "So beta M is in M iteration.",
                    "label": 0
                },
                {
                    "sent": "We're trying to find the object the weight of.",
                    "label": 0
                },
                {
                    "sent": "An individual classifier that minimizing this objective function.",
                    "label": 0
                },
                {
                    "sent": "An the other actually example for the non uniform weight is.",
                    "label": 0
                },
                {
                    "sent": "Accordingly we can consider correlations among responses.",
                    "label": 1
                },
                {
                    "sent": "So one of the examples are trying to minimize estimated actually generalization error.",
                    "label": 0
                },
                {
                    "sent": "Actually that involves a variance term defined as ambiguity.",
                    "label": 1
                },
                {
                    "sent": "This is example from the neural network example of the neural network.",
                    "label": 0
                },
                {
                    "sent": "So they define the values as actual difference between a response from an individual classifier and the certain target function over the expected expected response from the all of the response from the K different classifier.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is actually since our work is confident space and this confidence is computed by variance.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to briefly talk about some kind of work that also the talk about the variance.",
                    "label": 0
                },
                {
                    "sent": "So 1993 and 1994 Perron and Crow actually day also defined the variance.",
                    "label": 0
                },
                {
                    "sent": "And in the example framework in neural network.",
                    "label": 0
                },
                {
                    "sent": "So the various inapparent defined by the.",
                    "label": 0
                },
                {
                    "sent": "By showing this slide and the majorly, the M is the difference between the target function and the each individual classifier.",
                    "label": 0
                },
                {
                    "sent": "So they must use the A train example to optimize this, and then a Crow actually defined the variance as a difference between a individual classifier and the expected value.",
                    "label": 0
                },
                {
                    "sent": "Expected responses from the individual classifier.",
                    "label": 0
                },
                {
                    "sent": "So one of the input here, the weight of each network, is it to learn between the training and test during the testing the weight it's actually fixed.",
                    "label": 1
                },
                {
                    "sent": "And 2006 actually actually defined the variance from the from the response of testing instance.",
                    "label": 0
                },
                {
                    "sent": "Actually, this work is prevented quite similar to the current work, but the this work is each individual classifier is from the Bayesian network, while our work is based on discrete approach.",
                    "label": 1
                },
                {
                    "sent": "And 2007 and 1992 in active learning research community.",
                    "label": 0
                },
                {
                    "sent": "They also use the variance, which is quite a similar, but the similar to our work.",
                    "label": 1
                },
                {
                    "sent": "But this actually the variance it used to quantify the benefit of the laboring and you sample.",
                    "label": 0
                },
                {
                    "sent": "And the variance is also used in the mixture of expert, the research area.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to move to talk about the foundation of this work.",
                    "label": 0
                },
                {
                    "sent": "Pretty much focus on combination rule and weight of individual classifier.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I wasn't trying to actually make a pedestal sink and summarize the slide to explain the principle of this work, so pretty much we need to develop the combination rule and weight rule for example framework.",
                    "label": 0
                },
                {
                    "sent": "So assumption here I made is the we assume that correct answer Lambda exist.",
                    "label": 0
                },
                {
                    "sent": "And then we view a response.",
                    "label": 1
                },
                {
                    "sent": "Response from each individual classifier among a different view classifier.",
                    "label": 0
                },
                {
                    "sent": "As an estimate of Lambda with a random error which is absolute I so that each response is going to be represented as a sum of this correct answer plus random noise which is actually similar to the same as the linear regression formulation.",
                    "label": 1
                },
                {
                    "sent": "But nice thing about this formulation is that if we try to do maximum likelihood estimate from the distance formula and this the Lambda out, the maximum likelihood is actually expressed in terms of this number of the terms.",
                    "label": 0
                },
                {
                    "sent": "And this actually maximum likely estimate as to provide us the nice way of combination rule as well as the waiting room, which is actually the inverse of this variance is going to show the how we are going to wait or response of the the response from each individual classifier and then this actually red circle one is actually showing us the combination how we can actually combine things which is additive.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now actually we need to define how what kind of response we're going to use.",
                    "label": 0
                },
                {
                    "sent": "And how we can actually derive the variance from the response?",
                    "label": 0
                },
                {
                    "sent": "So we have some choice at the freedom of choice based upon the previous this framework.",
                    "label": 1
                },
                {
                    "sent": "So here I'm taking actually disconnected approaches.",
                    "label": 0
                },
                {
                    "sent": "The reason is that this approach, actually empirical and theoretical, proven to be.",
                    "label": 0
                },
                {
                    "sent": "To produce very effective declassification result and.",
                    "label": 0
                },
                {
                    "sent": "And it is a actually popular for the many different application domain.",
                    "label": 1
                },
                {
                    "sent": "So the example of this class approaches a support vector machine and large deviation.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to use the logical relation for the Aditya response modeling because the reason is that which actually loads the relation of maximum likelihood estimator parameter formula stimulation actually allow us to efficiently compute the variance which is approximation.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this slide actually showing a very simple summary of the logical relation.",
                    "label": 0
                },
                {
                    "sent": "It's probabilistic model and it's proven to be effective and the one thing we have to pay attention a little bit is that the model parameter is can be actually learned by maximizing this low likelihood and this slow like maximize the lower likelihood maximization estimate estimate actually provide us very efficient way to compute the variance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about various Octa response, how we can compute.",
                    "label": 0
                },
                {
                    "sent": "But if before, before we talk about variance of the response, we need to actually look at how we're going to derive the variance of the parameter first.",
                    "label": 1
                },
                {
                    "sent": "And in order to compute to derive this variance of the parameter, we actually look at 2 nice property in the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "So which is the consistency and a simpleton normality?",
                    "label": 0
                },
                {
                    "sent": "So as you can see from this slide, as we have the larger number of the data, actually the mean of the difference between the optimal parameter and the estimate maximum likelihood parameter is going to be 0, and then we're going to have this the.",
                    "label": 1
                },
                {
                    "sent": "The variance so that this variance we're going to actually use sufficient information, which is the actually expected value of the squared first derivative of low light with function which is showing this slide.",
                    "label": 0
                },
                {
                    "sent": "Actually this sufficient information actually provide us a very nice property.",
                    "label": 0
                },
                {
                    "sent": "Actually we can talk about in the authorization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The role of sufficient information is that by using actually a claim roll roll bound, or we can actually approximate the variance variance of the parameter which is showing the second example of here.",
                    "label": 0
                },
                {
                    "sent": "Showing at this the formula.",
                    "label": 0
                },
                {
                    "sent": "So based on using this, the camera roll around and we can actually approximate this.",
                    "label": 0
                },
                {
                    "sent": "The variance as the variance of the response as the three different actually turned the product of this term.",
                    "label": 0
                },
                {
                    "sent": "So as we can see from the first term and the last time, it's clear it's clear that this variance is based on instant speed so.",
                    "label": 0
                },
                {
                    "sent": "And they wanting nice thing about this variance of the responses that it also proven to be useful in quantifying the benefits of this, letting you sample to label in active learning in the 2007 and 1992.",
                    "label": 1
                },
                {
                    "sent": "So aggregating this all.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together I put I had some kind of city job to experience.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first round of experience I used the UCI data set I did 10 times of iteration, and each iteration and a three fold cross validation.",
                    "label": 1
                },
                {
                    "sent": "The reason for the reason why I did a 10 times iteration is that UCI data set it typically has a very small number of.",
                    "label": 0
                },
                {
                    "sent": "Some number of dirty data.",
                    "label": 0
                },
                {
                    "sent": "So each year we each cross.",
                    "label": 0
                },
                {
                    "sent": "A valid of each folder cross validation.",
                    "label": 0
                },
                {
                    "sent": "I wasn't trying to minimize the impact of the skewed cluster distribution, so each time of the iteration I randomly to shuffle the data instances and then did 3 for the cross validation and then I used the 20 generated 20 individual classifiers in order to combine them together and each the individual classifier actually are randomly selected to actually.",
                    "label": 0
                },
                {
                    "sent": "50% of the training sample and then they are trained on.",
                    "label": 0
                },
                {
                    "sent": "Upon this the 50% of the data.",
                    "label": 0
                },
                {
                    "sent": "So the reason why I selected a 50% is that this is not telling, no.",
                    "label": 0
                },
                {
                    "sent": "Theoretical or the explanation, but I according to my holdout data set experiment, it shows the best as the to differentiate among the 20 classifiers.",
                    "label": 0
                },
                {
                    "sent": "And then I compared our model which is I named NLR which is symbolized local likelihood padding and ADA boost.",
                    "label": 0
                },
                {
                    "sent": "And then this is it.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City showing the experimental result and also available in front of paper.",
                    "label": 0
                },
                {
                    "sent": "So if you look at one thing, I'd like to briefly mention from this graph is that typically over in error is showing much better than single logistic regression SLR here.",
                    "label": 0
                },
                {
                    "sent": "But some of the data instances I mean for this amount of data instances the improvement improvement is very marginal.",
                    "label": 0
                },
                {
                    "sent": "So I plotted the number of instances.",
                    "label": 0
                },
                {
                    "sent": "As actually axis and the number of feature as a Y axis is A2 dimensions, so I can see some kind of pattern here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That which is showing here.",
                    "label": 0
                },
                {
                    "sent": "If we have a pretty number of reasonable enough number of training instance, I mean number of instances we can train.",
                    "label": 0
                },
                {
                    "sent": "Considering the number of features, then we can actually see noticeability improvement, but if we have a.",
                    "label": 0
                },
                {
                    "sent": "We have if we don't have enough number of number of instances considering number of the features that we are seeing, actually very marginal improvement.",
                    "label": 0
                },
                {
                    "sent": "As we can see from the SRX.",
                    "label": 0
                },
                {
                    "sent": "Cleave actually cleave and the heart or those kind of data set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is actually showing the comparing with the BEGGING and ADA boost.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I also did another actually experiment using your actual problem using actually Yahoo data set, which is the user issue.",
                    "label": 0
                },
                {
                    "sent": "Search search query search engine need to actually decide whether the search engine results search engine results page includes a specific model such as the news product or image.",
                    "label": 1
                },
                {
                    "sent": "Because we are trying to encourage users engagement.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I formulated this as the focus one of the example as an image trigger task and then formulated as an image of the classification problem so generated.",
                    "label": 0
                },
                {
                    "sent": "So whole bunch of the query level, feature length, coding length, number of tokens and clear information.",
                    "label": 1
                },
                {
                    "sent": "And as you can see from the slide you know we if you put it like a star Fox which is one of the game we want to show some kind of picture about this game so that people can encourage the people's engagement.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I did the same experiment and.",
                    "label": 0
                },
                {
                    "sent": "Think of because of short.",
                    "label": 0
                },
                {
                    "sent": "At the time I can just we can talk about in the poster session.",
                    "label": 0
                },
                {
                    "sent": "More detail.",
                    "label": 0
                },
                {
                    "sent": "Our model shows the better performance and.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The future work you know.",
                    "label": 0
                },
                {
                    "sent": "I do like to actually extended this work to multi classes or extension two other descriptive approach, another different sampling.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and you wanting?",
                    "label": 0
                },
                {
                    "sent": "I'd like to actually prefer mention is that I checked the preceding version and then the figure one is very is loose lost.",
                    "label": 0
                },
                {
                    "sent": "The advantage from the figure one is broken and I updated the one version in.",
                    "label": 0
                },
                {
                    "sent": "And this URL.",
                    "label": 0
                },
                {
                    "sent": "So please go and if you want to use Facebook, feel free to download.",
                    "label": 0
                }
            ]
        }
    }
}