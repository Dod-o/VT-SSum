{
    "id": "a2a37qfa4ruynt5a6nyyxkyxuqtjt56e",
    "title": "Multimodal nonlinear filtering using Gauss-Hermite Quadrature",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Nicolas Heess, University of Edinburgh"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_heess_quadrature/",
    "segmentation": [
        [
            "So this works right before I start.",
            "I just want to point out that really most of the work in this presentation has been done by Hannes, who is unfortunately able to attend.",
            "So I'm presenting on his behalf."
        ],
        [
            "Basically, what we're interested in is the filtering problem, where we have a sequence of latent States and a corresponding sequence of observations, and we have been interested given the observations up to time step T to compute the posterior distribution over the state at that point in time.",
            "The solution to this is usually formulated in a recursive manner and it gives rise to the well known filtering equation which is."
        ],
        [
            "Turn here, but what I'm going to consider in this talk is really somewhat simplified scenario where we basically assume that this data is fixed and we deal with the observation app update only.",
            "In this situation we get a very simple recursive rule for updating our posterior.",
            "So if we want to compute the posterior times that given all the observation up to times 30, we simply take the posterior up 2 * 15 -- 1 and combine it with the likelihood of the new observation.",
            "And now the solution to this problem is simple.",
            "An exact instance.",
            "Very specific situations, but in the general case has been especially if we have a nonlinear likelihood, then the posterior can have a very complex form and for instance be multimodal.",
            "So this is just a simple example where we start off with a unimodal prior over this state.",
            "But as we observe several observations, the posture becomes strongly bimodal.",
            "Now the question that we are interested in this talk is twofold.",
            "So first the if you consider friends to the posterior here, then clearly this past year is unlikely.",
            "Have to have an exact parametric representation, so we need if we want to work with it, we need to choose some approximate form in the mind of the talk.",
            "We're going to basically use the notation QT to the to indicate the approximate posterior times 15.",
            "The second question, once we've chosen that some representation.",
            "Is how should we update this representation when we observe a new observation and basically in most cases the exact recursive update which is given here is still intractable.",
            "So we need to choose some approximation."
        ],
        [
            "So this problem is obviously received a lot of attention in the literature sample based approaches are nice because they're very flexible and don't make any assumptions as to the shape of the posterior distribution.",
            "They cannot be rather computationally expensive if we move to higher dimensions in particular, and there are certain applications, where would we like to have a very compact parametric representation of the posterior?",
            "Really, what motivated this work here is an active learning scenario that tennis was working on.",
            "Where basically a sample based representation just becomes computationally infeasible.",
            "What I'm going to focus on, therefore, are deterministic approximations where we choose some parametric distribution to represent our posterior approximately and the most obvious choice.",
            "There is of course the Gaussian distribution, which has been widely used in the literature and gives rise to very efficient filtering solution, such As for instance the extended common filter.",
            "Now, if you."
        ],
        [
            "Apply this star simple toy example.",
            "Then we can see that by its very nature, the Gaussian is not give us a very good approximation to a complex posterior shape.",
            "As an alternative, several authors in the literature have proposed to use a mixture of Gaussians approximately represent the posterior.",
            "Now this is a potentially very flexible way of representing the posterior and should work reasonably well for our problem here, but unfortunately the updates that they have been using and specifically.",
            "Update which treats the individual mixture components in the mixture in dependently and basically upstate updates them as if one was dealing with a large number of single Gaussian filters.",
            "Independent Gaussian filters.",
            "Hence the Bank of independent filters terms makes this so.",
            "Basically this update makes the approximation rather inefficient."
        ],
        [
            "As we can see, if we apply it to our toy example here, although we are allowing for two mixture components, the approximate the overall approximation that we get is barely better than the one with a single Gaussian."
        ],
        [
            "So basically what we're going to do in this talk is we are going to describe alternative way of representing the state state posterior approximately using a mixture of Gaussians by using a more efficient recursive update which jointly optimizes the parameters of all mixture components and will tanias Lee and basically what I'm going to show is that this gives a better approximate, much better approximation to the posterior even if we use a much lower number of mixture components."
        ],
        [
            "And this is just kind of a preview as Paul is going to work."
        ],
        [
            "OK, so basically the mix filter that filtering approach which are also going to refer to as the variational mixture filter makes very similar assumptions to existing work in the literature.",
            "We assume that we have a prior of the state which is a gas mixture.",
            "We have a nonlinear likelihood, which is effectively a Gaussian with a mean that not linearly depends on the state X and this nonlinear optimization efforts are not what I'm going to refer.",
            "Just an observation function within an approximate hour.",
            "Posterior with a Gaussian mixture at each time step T. Now the main difference is that we jointly optimize the mixture parameters at each time step.",
            "Directly via minimization as the callback liberal divergences between our approximating mixture and the exact recursive update, so basically.",
            "Like this here, just exactly present update.",
            "We keep this computationally tractable in two ways.",
            "Firstly, we choose a radial basis function representation for our nonlinear like for nonlinear observation function, which on the one hand is very flexible, so it can represent any nonlinearity.",
            "But on the other hand will make certain expectations or integrals computation analytically tractable, and certainly for the remaining intractable integrals in our minimization optimization scheme.",
            "And we use an efficient deterministic sampling approach."
        ],
        [
            "OK, and since basically we did this optimization, minimum minimizations of the cobra divergences, the core our approach I want to spend a few minutes explaining on explaining how it works in some more detail.",
            "So basically what we need to do at each time step is we need to minimize the code back.",
            "Liberal divergences between QTR approximating mixture and the exact recursive update.",
            "This correctly better versions is shown here and it decomposes into three sounds over expectations.",
            "These expectations are all taken with respect to the mixture components of our approximating distribution and the third set of expectations which, over the likelihood, remains analytically tractable.",
            "Because our particular choice of the nonlinear observation function, the other two."
        ],
        [
            "However, which are expectations over log sums, so these are both mixtures here, right?",
            "These are intractable and we need to approximate them in some way."
        ],
        [
            "We do this using Gauss Hermite quadrature, which is the deterministic sampling approach that allows us to approximate expectations over of nonlinear functions with respect to Gaussian distributions.",
            "So G axis Justin learning a function here and the way it works, is that it evaluates the nonlinear function at a small number of sample points and then computes a weighted sound level, the sample points and the weights are chosen deterministically based on the parameters of this Gaussian here."
        ],
        [
            "Now in our case we are interested in computing expectations of log sums, so expectations of the log of mixtures, right?",
            "In this case, this approximation has some unpleasant properties and some care must be taken for it to work well.",
            "To give you an example of what can happen, let's consider this simple Gaussian mixture down here, which has two components and one and two.",
            "One of them is very wide and one the others reynero an if we want to compute the expectation of the log of this mixture which is shown in dark blue here.",
            "So this is the log of this mic.",
            "With respect to the wider component, then the approximation that it goes her mind will compute is basically a weighted sum of those growing sample points.",
            "Here now implicitly, or one way to think about what's going on for this approximation is that implicitly guys had fits a second order polynomial.",
            "Then compute the expectation exactly with respect to that polynomial.",
            "Now clearly this second order polynomial, which is shown in science here, doesn't really capture, for instance, this bulge.",
            "And what we found is that this can lead to very nasty effects during the minimization.",
            "Basically, it diverges and doesn't recover anymore.",
            "Now."
        ],
        [
            "To address this issue.",
            "We take 2 steps.",
            "The first one is that we simply that we rewrite our log sum so the sum over several Gaston components as a sum over basically several log terms.",
            "OK. And then this has the advantage that we can then compute the expectation with each of those lock terms separately, and we can optimize the approximation in each case individually.",
            "Basically, basically what we can do is if we can swap around the distribution with respect to which we compute the expectation.",
            "So if we consider for instance the this term here and we compute the expectation with respect to mixture component one as shown here.",
            "Then we can Alternatively write this as an expectation.",
            "Over mixture component to now the effect that this."
        ],
        [
            "When you compute the approximation of those expectations is shown here in the two rightmost panels.",
            "So basically, if we consider the left hand side of the equation, then this term is here shown in blue, right an hour waiting distribution mixture component, one in red.",
            "And clearly the approximation that will be computed by those turn right with a small number of data points is going to be very poor if you consider.",
            "However, in the right hand side of the equation then this is our our nonlinear function and this is the weighting distribution.",
            "Here the approximation is going to be much better than the.",
            "Resolve a lot more accurate and we found that this makes a rather dramatic difference, and especially fixes the divergent issues that we encounter."
        ],
        [
            "OK, so basically to show you some results of what our approach does when we apply it to simple 1 dimensional problems, we chose a bunch of observation functions and a bunch of observation sequences, and then we approximate the true posterior distribution using mixtures of Gaussians that were fitted either with the variational mixture filtering approach or with the initially described Bank of independent filters.",
            "If we consider, for instance the first example, then this is our prior distribution and each row we obtain an additional observation.",
            "The black curve shows in all cases the true posterior, which was computed numerically and we can see it first becomes bimodal and then converges to a single node.",
            "Anne.",
            "The mixture of Gaussian that is fitted with the web in a way where each component is optimized independently clearly doesn't give us a very good approximation of this posterior.",
            "The variational mixer, if it makes your filtering approach, however, captures the posterior fairly well.",
            "These are some additional examples where we use either more mixture components or have all the shapes of the posterior and the picture is effectively the same number cases."
        ],
        [
            "And to obtain a somewhat more quantitative evaluation in more complex scenarios and also in higher dimensions, we again generated a large number of different scenarios by randomly generating nonlinear observation functions and choosing observation sequences that lead to complex shape posteriors.",
            "We then tracked those those exactly, then track those posterior distributions using our variational filtering approach.",
            "With four mixture components in the representation and several and several versions of the Bank of Independent filtering approach which were using different number of mixture components in their representation for each of us for each scenario and for each object for each Bank of independent filtering of version, we then computed the difference in the basically approximation error given by the kullback Leibler divergent.",
            "So we compute the difference between the kullback Leibler divergent between the.",
            "Bank of independent filters say with four components and the true posterior and the variation which are filtered and the true posterior.",
            "The reason for taking this difference is that we can evaluate this only up to a constant.",
            "And this KL divergences difference is plotted here as a function of the number of observations of taint and each curve basically corresponds to an average over many different scenarios considered.",
            "Basically, if this difference is larger, that means that the approximation error here is larger than for the variation mixture filter, and if it's smaller than 0.",
            "Lower so basically what we find is that in all cases, even if we allow many more.",
            "Mixture components for the sorry lack of independent filter approach.",
            "The approximation will be worse."
        ],
        [
            "Now all this gave us some confidence that the approximation of the posterior is indeed better.",
            "What we're really interested in was when that would help us in.",
            "For instance, in an active learning scenario where we use the.",
            "The current at each timestep the current posterior directly to choose how a new observation is obtained.",
            "To test the effect of the improved approximation in this context, we consider a simple localization task where basically the system had to identify static target X star by moving around a pro position represented by its moving around the probe represented by its observation functions and at each time step it would then choose the position of that probe in a way such as to obtain an observation that has a high probability of being informative.",
            "Respect to the true position of the target OK?",
            "And we implemented this active learning scenario with our variational Mr Filtering approach and with a end with this path with the Bank of or with the.",
            "Sort of with the back of independence for the approach, and then what we're plotting here is basically the convergence towards the target, and we plotted in terms of the log likelihood of the true target position under the posterior.",
            "So larger is better.",
            "And what we find is where's the variational filter tends to converge towards the target as the number of observations increases.",
            "For instance, the back of independent filters effectively get stuck after a small number of observations.",
            "Suggesting that really having an accurate representation of the posterior distribution is important in an active learning scenario."
        ],
        [
            "In a second set of experiments, we wanted to see whether we want to make a fair comparison between the two approaches.",
            "Because the variational filter is a lot more considering more computationally expensive, that amount of independent filters.",
            "So basically we gave both methods a fixed time budget during which they could do inference in active learning.",
            "And basically what we expected then is that obviously the Bank of it that the Bank of Independent filters would process more observations given in a fixed unit of time.",
            "Or the question was would that would that lead to a better approximation of the target position?",
            "And what we found is that indeed the back then you can filters process is more observations and a fixed amount of time.",
            "But it doesn't lead to better approximation of the true target position, so it can't really make use of having more observations due to the property of the approximate approximation of the posterior.",
            "OK."
        ],
        [
            "So let me quickly conclude basically what we have proposed is a new approach to nonlinear filtering that is basically a more efficient way of fitting a Gaussian mixture representation to the posterior.",
            "The result of this is a compact representation of the posterior, which is also very accurate or quite accurate.",
            "We have implemented efficient rating based approach for optimizing the code, likely by the vergence at each point in time.",
            "We found in experiments that we do get indeed better fit through the posterior and faster convergence, and that really does help in the context of the active learning scenarios that will considering.",
            "There are some directions.",
            "I guess.",
            "Quick question if anybody has one.",
            "You didn't say much about the FDF training.",
            "There was wondering which kind of samples you used.",
            "There is a well ACP.",
            "We assume that we have and in this case we assume that we have an RBF representation of our non linearity.",
            "This is not.",
            "Basically that's not the.",
            "Not what we are interested in, how we would get this nonlinearity.",
            "So these are experiments.",
            "You assume that you have discovered.",
            "Yeah, the RBF is given that it was given for all approach.",
            "Basically for all for all approaches and so has in other experiments that he has done an.",
            "So instead of the RBF you can also use for instance Gaussian process and there he has indeed done some learning of the nonlinearity.",
            "Basically in the standard way how you would learn English learning with some.",
            "Estimated values that they fix is estimated that case.",
            "You mean the nonlinearity will be estimated?",
            "Yeah, well, OK, but that will be true in any case, yeah, yeah yeah.",
            "I mean, I don't know what would be the best way to do that, but I mean there are many, many ways of fitting these networks, yeah?",
            "Any other questions?",
            "OK, well I think it's copy side down, so thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this works right before I start.",
                    "label": 0
                },
                {
                    "sent": "I just want to point out that really most of the work in this presentation has been done by Hannes, who is unfortunately able to attend.",
                    "label": 0
                },
                {
                    "sent": "So I'm presenting on his behalf.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, what we're interested in is the filtering problem, where we have a sequence of latent States and a corresponding sequence of observations, and we have been interested given the observations up to time step T to compute the posterior distribution over the state at that point in time.",
                    "label": 0
                },
                {
                    "sent": "The solution to this is usually formulated in a recursive manner and it gives rise to the well known filtering equation which is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn here, but what I'm going to consider in this talk is really somewhat simplified scenario where we basically assume that this data is fixed and we deal with the observation app update only.",
                    "label": 0
                },
                {
                    "sent": "In this situation we get a very simple recursive rule for updating our posterior.",
                    "label": 0
                },
                {
                    "sent": "So if we want to compute the posterior times that given all the observation up to times 30, we simply take the posterior up 2 * 15 -- 1 and combine it with the likelihood of the new observation.",
                    "label": 0
                },
                {
                    "sent": "And now the solution to this problem is simple.",
                    "label": 0
                },
                {
                    "sent": "An exact instance.",
                    "label": 0
                },
                {
                    "sent": "Very specific situations, but in the general case has been especially if we have a nonlinear likelihood, then the posterior can have a very complex form and for instance be multimodal.",
                    "label": 0
                },
                {
                    "sent": "So this is just a simple example where we start off with a unimodal prior over this state.",
                    "label": 0
                },
                {
                    "sent": "But as we observe several observations, the posture becomes strongly bimodal.",
                    "label": 0
                },
                {
                    "sent": "Now the question that we are interested in this talk is twofold.",
                    "label": 0
                },
                {
                    "sent": "So first the if you consider friends to the posterior here, then clearly this past year is unlikely.",
                    "label": 0
                },
                {
                    "sent": "Have to have an exact parametric representation, so we need if we want to work with it, we need to choose some approximate form in the mind of the talk.",
                    "label": 0
                },
                {
                    "sent": "We're going to basically use the notation QT to the to indicate the approximate posterior times 15.",
                    "label": 0
                },
                {
                    "sent": "The second question, once we've chosen that some representation.",
                    "label": 0
                },
                {
                    "sent": "Is how should we update this representation when we observe a new observation and basically in most cases the exact recursive update which is given here is still intractable.",
                    "label": 0
                },
                {
                    "sent": "So we need to choose some approximation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this problem is obviously received a lot of attention in the literature sample based approaches are nice because they're very flexible and don't make any assumptions as to the shape of the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "They cannot be rather computationally expensive if we move to higher dimensions in particular, and there are certain applications, where would we like to have a very compact parametric representation of the posterior?",
                    "label": 1
                },
                {
                    "sent": "Really, what motivated this work here is an active learning scenario that tennis was working on.",
                    "label": 0
                },
                {
                    "sent": "Where basically a sample based representation just becomes computationally infeasible.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to focus on, therefore, are deterministic approximations where we choose some parametric distribution to represent our posterior approximately and the most obvious choice.",
                    "label": 0
                },
                {
                    "sent": "There is of course the Gaussian distribution, which has been widely used in the literature and gives rise to very efficient filtering solution, such As for instance the extended common filter.",
                    "label": 0
                },
                {
                    "sent": "Now, if you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apply this star simple toy example.",
                    "label": 0
                },
                {
                    "sent": "Then we can see that by its very nature, the Gaussian is not give us a very good approximation to a complex posterior shape.",
                    "label": 0
                },
                {
                    "sent": "As an alternative, several authors in the literature have proposed to use a mixture of Gaussians approximately represent the posterior.",
                    "label": 1
                },
                {
                    "sent": "Now this is a potentially very flexible way of representing the posterior and should work reasonably well for our problem here, but unfortunately the updates that they have been using and specifically.",
                    "label": 0
                },
                {
                    "sent": "Update which treats the individual mixture components in the mixture in dependently and basically upstate updates them as if one was dealing with a large number of single Gaussian filters.",
                    "label": 0
                },
                {
                    "sent": "Independent Gaussian filters.",
                    "label": 0
                },
                {
                    "sent": "Hence the Bank of independent filters terms makes this so.",
                    "label": 0
                },
                {
                    "sent": "Basically this update makes the approximation rather inefficient.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we can see, if we apply it to our toy example here, although we are allowing for two mixture components, the approximate the overall approximation that we get is barely better than the one with a single Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what we're going to do in this talk is we are going to describe alternative way of representing the state state posterior approximately using a mixture of Gaussians by using a more efficient recursive update which jointly optimizes the parameters of all mixture components and will tanias Lee and basically what I'm going to show is that this gives a better approximate, much better approximation to the posterior even if we use a much lower number of mixture components.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just kind of a preview as Paul is going to work.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically the mix filter that filtering approach which are also going to refer to as the variational mixture filter makes very similar assumptions to existing work in the literature.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have a prior of the state which is a gas mixture.",
                    "label": 0
                },
                {
                    "sent": "We have a nonlinear likelihood, which is effectively a Gaussian with a mean that not linearly depends on the state X and this nonlinear optimization efforts are not what I'm going to refer.",
                    "label": 0
                },
                {
                    "sent": "Just an observation function within an approximate hour.",
                    "label": 0
                },
                {
                    "sent": "Posterior with a Gaussian mixture at each time step T. Now the main difference is that we jointly optimize the mixture parameters at each time step.",
                    "label": 0
                },
                {
                    "sent": "Directly via minimization as the callback liberal divergences between our approximating mixture and the exact recursive update, so basically.",
                    "label": 0
                },
                {
                    "sent": "Like this here, just exactly present update.",
                    "label": 0
                },
                {
                    "sent": "We keep this computationally tractable in two ways.",
                    "label": 0
                },
                {
                    "sent": "Firstly, we choose a radial basis function representation for our nonlinear like for nonlinear observation function, which on the one hand is very flexible, so it can represent any nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand will make certain expectations or integrals computation analytically tractable, and certainly for the remaining intractable integrals in our minimization optimization scheme.",
                    "label": 0
                },
                {
                    "sent": "And we use an efficient deterministic sampling approach.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and since basically we did this optimization, minimum minimizations of the cobra divergences, the core our approach I want to spend a few minutes explaining on explaining how it works in some more detail.",
                    "label": 0
                },
                {
                    "sent": "So basically what we need to do at each time step is we need to minimize the code back.",
                    "label": 0
                },
                {
                    "sent": "Liberal divergences between QTR approximating mixture and the exact recursive update.",
                    "label": 0
                },
                {
                    "sent": "This correctly better versions is shown here and it decomposes into three sounds over expectations.",
                    "label": 0
                },
                {
                    "sent": "These expectations are all taken with respect to the mixture components of our approximating distribution and the third set of expectations which, over the likelihood, remains analytically tractable.",
                    "label": 0
                },
                {
                    "sent": "Because our particular choice of the nonlinear observation function, the other two.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, which are expectations over log sums, so these are both mixtures here, right?",
                    "label": 0
                },
                {
                    "sent": "These are intractable and we need to approximate them in some way.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do this using Gauss Hermite quadrature, which is the deterministic sampling approach that allows us to approximate expectations over of nonlinear functions with respect to Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "So G axis Justin learning a function here and the way it works, is that it evaluates the nonlinear function at a small number of sample points and then computes a weighted sound level, the sample points and the weights are chosen deterministically based on the parameters of this Gaussian here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in our case we are interested in computing expectations of log sums, so expectations of the log of mixtures, right?",
                    "label": 1
                },
                {
                    "sent": "In this case, this approximation has some unpleasant properties and some care must be taken for it to work well.",
                    "label": 0
                },
                {
                    "sent": "To give you an example of what can happen, let's consider this simple Gaussian mixture down here, which has two components and one and two.",
                    "label": 0
                },
                {
                    "sent": "One of them is very wide and one the others reynero an if we want to compute the expectation of the log of this mixture which is shown in dark blue here.",
                    "label": 0
                },
                {
                    "sent": "So this is the log of this mic.",
                    "label": 0
                },
                {
                    "sent": "With respect to the wider component, then the approximation that it goes her mind will compute is basically a weighted sum of those growing sample points.",
                    "label": 0
                },
                {
                    "sent": "Here now implicitly, or one way to think about what's going on for this approximation is that implicitly guys had fits a second order polynomial.",
                    "label": 0
                },
                {
                    "sent": "Then compute the expectation exactly with respect to that polynomial.",
                    "label": 0
                },
                {
                    "sent": "Now clearly this second order polynomial, which is shown in science here, doesn't really capture, for instance, this bulge.",
                    "label": 0
                },
                {
                    "sent": "And what we found is that this can lead to very nasty effects during the minimization.",
                    "label": 0
                },
                {
                    "sent": "Basically, it diverges and doesn't recover anymore.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To address this issue.",
                    "label": 0
                },
                {
                    "sent": "We take 2 steps.",
                    "label": 0
                },
                {
                    "sent": "The first one is that we simply that we rewrite our log sum so the sum over several Gaston components as a sum over basically several log terms.",
                    "label": 0
                },
                {
                    "sent": "OK. And then this has the advantage that we can then compute the expectation with each of those lock terms separately, and we can optimize the approximation in each case individually.",
                    "label": 1
                },
                {
                    "sent": "Basically, basically what we can do is if we can swap around the distribution with respect to which we compute the expectation.",
                    "label": 0
                },
                {
                    "sent": "So if we consider for instance the this term here and we compute the expectation with respect to mixture component one as shown here.",
                    "label": 0
                },
                {
                    "sent": "Then we can Alternatively write this as an expectation.",
                    "label": 0
                },
                {
                    "sent": "Over mixture component to now the effect that this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you compute the approximation of those expectations is shown here in the two rightmost panels.",
                    "label": 1
                },
                {
                    "sent": "So basically, if we consider the left hand side of the equation, then this term is here shown in blue, right an hour waiting distribution mixture component, one in red.",
                    "label": 0
                },
                {
                    "sent": "And clearly the approximation that will be computed by those turn right with a small number of data points is going to be very poor if you consider.",
                    "label": 0
                },
                {
                    "sent": "However, in the right hand side of the equation then this is our our nonlinear function and this is the weighting distribution.",
                    "label": 1
                },
                {
                    "sent": "Here the approximation is going to be much better than the.",
                    "label": 0
                },
                {
                    "sent": "Resolve a lot more accurate and we found that this makes a rather dramatic difference, and especially fixes the divergent issues that we encounter.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so basically to show you some results of what our approach does when we apply it to simple 1 dimensional problems, we chose a bunch of observation functions and a bunch of observation sequences, and then we approximate the true posterior distribution using mixtures of Gaussians that were fitted either with the variational mixture filtering approach or with the initially described Bank of independent filters.",
                    "label": 1
                },
                {
                    "sent": "If we consider, for instance the first example, then this is our prior distribution and each row we obtain an additional observation.",
                    "label": 0
                },
                {
                    "sent": "The black curve shows in all cases the true posterior, which was computed numerically and we can see it first becomes bimodal and then converges to a single node.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The mixture of Gaussian that is fitted with the web in a way where each component is optimized independently clearly doesn't give us a very good approximation of this posterior.",
                    "label": 0
                },
                {
                    "sent": "The variational mixer, if it makes your filtering approach, however, captures the posterior fairly well.",
                    "label": 0
                },
                {
                    "sent": "These are some additional examples where we use either more mixture components or have all the shapes of the posterior and the picture is effectively the same number cases.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to obtain a somewhat more quantitative evaluation in more complex scenarios and also in higher dimensions, we again generated a large number of different scenarios by randomly generating nonlinear observation functions and choosing observation sequences that lead to complex shape posteriors.",
                    "label": 0
                },
                {
                    "sent": "We then tracked those those exactly, then track those posterior distributions using our variational filtering approach.",
                    "label": 0
                },
                {
                    "sent": "With four mixture components in the representation and several and several versions of the Bank of Independent filtering approach which were using different number of mixture components in their representation for each of us for each scenario and for each object for each Bank of independent filtering of version, we then computed the difference in the basically approximation error given by the kullback Leibler divergent.",
                    "label": 0
                },
                {
                    "sent": "So we compute the difference between the kullback Leibler divergent between the.",
                    "label": 0
                },
                {
                    "sent": "Bank of independent filters say with four components and the true posterior and the variation which are filtered and the true posterior.",
                    "label": 1
                },
                {
                    "sent": "The reason for taking this difference is that we can evaluate this only up to a constant.",
                    "label": 0
                },
                {
                    "sent": "And this KL divergences difference is plotted here as a function of the number of observations of taint and each curve basically corresponds to an average over many different scenarios considered.",
                    "label": 1
                },
                {
                    "sent": "Basically, if this difference is larger, that means that the approximation error here is larger than for the variation mixture filter, and if it's smaller than 0.",
                    "label": 0
                },
                {
                    "sent": "Lower so basically what we find is that in all cases, even if we allow many more.",
                    "label": 0
                },
                {
                    "sent": "Mixture components for the sorry lack of independent filter approach.",
                    "label": 0
                },
                {
                    "sent": "The approximation will be worse.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now all this gave us some confidence that the approximation of the posterior is indeed better.",
                    "label": 0
                },
                {
                    "sent": "What we're really interested in was when that would help us in.",
                    "label": 0
                },
                {
                    "sent": "For instance, in an active learning scenario where we use the.",
                    "label": 0
                },
                {
                    "sent": "The current at each timestep the current posterior directly to choose how a new observation is obtained.",
                    "label": 0
                },
                {
                    "sent": "To test the effect of the improved approximation in this context, we consider a simple localization task where basically the system had to identify static target X star by moving around a pro position represented by its moving around the probe represented by its observation functions and at each time step it would then choose the position of that probe in a way such as to obtain an observation that has a high probability of being informative.",
                    "label": 0
                },
                {
                    "sent": "Respect to the true position of the target OK?",
                    "label": 0
                },
                {
                    "sent": "And we implemented this active learning scenario with our variational Mr Filtering approach and with a end with this path with the Bank of or with the.",
                    "label": 1
                },
                {
                    "sent": "Sort of with the back of independence for the approach, and then what we're plotting here is basically the convergence towards the target, and we plotted in terms of the log likelihood of the true target position under the posterior.",
                    "label": 0
                },
                {
                    "sent": "So larger is better.",
                    "label": 0
                },
                {
                    "sent": "And what we find is where's the variational filter tends to converge towards the target as the number of observations increases.",
                    "label": 1
                },
                {
                    "sent": "For instance, the back of independent filters effectively get stuck after a small number of observations.",
                    "label": 0
                },
                {
                    "sent": "Suggesting that really having an accurate representation of the posterior distribution is important in an active learning scenario.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a second set of experiments, we wanted to see whether we want to make a fair comparison between the two approaches.",
                    "label": 0
                },
                {
                    "sent": "Because the variational filter is a lot more considering more computationally expensive, that amount of independent filters.",
                    "label": 0
                },
                {
                    "sent": "So basically we gave both methods a fixed time budget during which they could do inference in active learning.",
                    "label": 1
                },
                {
                    "sent": "And basically what we expected then is that obviously the Bank of it that the Bank of Independent filters would process more observations given in a fixed unit of time.",
                    "label": 0
                },
                {
                    "sent": "Or the question was would that would that lead to a better approximation of the target position?",
                    "label": 0
                },
                {
                    "sent": "And what we found is that indeed the back then you can filters process is more observations and a fixed amount of time.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't lead to better approximation of the true target position, so it can't really make use of having more observations due to the property of the approximate approximation of the posterior.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly conclude basically what we have proposed is a new approach to nonlinear filtering that is basically a more efficient way of fitting a Gaussian mixture representation to the posterior.",
                    "label": 1
                },
                {
                    "sent": "The result of this is a compact representation of the posterior, which is also very accurate or quite accurate.",
                    "label": 0
                },
                {
                    "sent": "We have implemented efficient rating based approach for optimizing the code, likely by the vergence at each point in time.",
                    "label": 1
                },
                {
                    "sent": "We found in experiments that we do get indeed better fit through the posterior and faster convergence, and that really does help in the context of the active learning scenarios that will considering.",
                    "label": 0
                },
                {
                    "sent": "There are some directions.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Quick question if anybody has one.",
                    "label": 0
                },
                {
                    "sent": "You didn't say much about the FDF training.",
                    "label": 0
                },
                {
                    "sent": "There was wondering which kind of samples you used.",
                    "label": 0
                },
                {
                    "sent": "There is a well ACP.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have and in this case we assume that we have an RBF representation of our non linearity.",
                    "label": 0
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "Basically that's not the.",
                    "label": 0
                },
                {
                    "sent": "Not what we are interested in, how we would get this nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "So these are experiments.",
                    "label": 0
                },
                {
                    "sent": "You assume that you have discovered.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the RBF is given that it was given for all approach.",
                    "label": 0
                },
                {
                    "sent": "Basically for all for all approaches and so has in other experiments that he has done an.",
                    "label": 0
                },
                {
                    "sent": "So instead of the RBF you can also use for instance Gaussian process and there he has indeed done some learning of the nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "Basically in the standard way how you would learn English learning with some.",
                    "label": 0
                },
                {
                    "sent": "Estimated values that they fix is estimated that case.",
                    "label": 0
                },
                {
                    "sent": "You mean the nonlinearity will be estimated?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, OK, but that will be true in any case, yeah, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't know what would be the best way to do that, but I mean there are many, many ways of fitting these networks, yeah?",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, well I think it's copy side down, so thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}