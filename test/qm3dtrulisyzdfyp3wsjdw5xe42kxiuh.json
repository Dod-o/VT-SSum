{
    "id": "qm3dtrulisyzdfyp3wsjdw5xe42kxiuh",
    "title": "Distribution-Independent Reliable Learning",
    "info": {
        "author": [
            "Varun Kanade, \u00c9cole normale sup\u00e9rieure Paris"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_kanade_distribution/",
    "segmentation": [
        [
            "And this is joint work with Justin, who's in the picture.",
            "If you can see him.",
            "Who's at Yahoo labs?"
        ],
        [
            "So let me begin with some learning scenarios, so all of us use email an we get a lot of spam, and of course it's annoying to deal with spam messages, so we don't want them.",
            "But on the other hand it's it's much, much more costly if actually an important email gets into spam, and so this is a setting where."
        ],
        [
            "False positives can be much worse than false negatives as far as learning is concerned.",
            "There could be other situations.",
            "Let's say if you're trying to detect failures in some electrical network or something.",
            "If you fail to detect, that's very costly.",
            "If you detect unnecessarily annoying, but maybe not so bad and get another situations, you actually want to make no errors.",
            "So let's say you were running a medical diagnostic test and you just want to decide whether or not a patient needs an operation and you don't want to make an error on that.",
            "Maybe you can allow your classifier to say, don't know, and you want to go and do another test.",
            "So, so we call these kinds of problems reliable learning."
        ],
        [
            "And there's a lot of prior work on this, and I'm not going to be able to talk about most of it, but in general, one could always right asymmetric loss function, where false positives and false negatives are weighted differently, maybe, and try and minimize these, and there's been a lot of work in statistics actually starting from Neyman Pearson, that gives actually very good properties of what this.",
            "How these classifiers should look?",
            "But the question that we are interested in, what is the computational complexity of finding these kinds of classifiers so these last functions are almost never going to be convex?",
            "And how do we solve this minimization problem efficiently in some cases?",
            "So that's the problem we're interested in."
        ],
        [
            "So let me first now talk about the exact model that we're going to work in, and this is going to be in the agnostic learning framework."
        ],
        [
            "So before I talk about diagnostic learning framework, let me quickly go over the PAC Learning framework which was introduced by Les Valiant and I'll be quick here.",
            "So in the past learning we have points X one through XM that come from distribution and here awhile assume the distribution is always over minus one one to the inserts over the Boolean cube and these points are labeled and here the assumption is that the labels actually come from some function from a class like linear separator save and so there's no noise in any of the labels that you see, and so the goal of the learning algorithm is to output.",
            "The hypothesis that does almost as well so it has error at most epsilon, so it gets almost all examples correctly.",
            "And when we want the learning algorithm to succeed for all distributions in this talk, I'm going to look for algorithms that have to work for all distributions.",
            "And.",
            "And agnostic learning is framework is a generalization of this pack framework, so this was introduced by."
        ],
        [
            "Housler and contribution silly in the early 90s.",
            "And here there's no longer an assumption that data is labeled according to some fixed functions.",
            "With the labels may be completely arbitrary, so they come from some joint distribution over minus one, one to the N N -- 1 one, and now the goal has to be different.",
            "So what's the goal here?",
            "So let's say we fix some class of functions.",
            "Halfspaces say you look at the best half space that explains the data right?",
            "Then it will have some error and what you want your learning algorithm to do is output hypothesis that has error.",
            "That's almost.",
            "As good as this best one from this class.",
            "So that's the standard agnostic model.",
            "And now let's say what we want in the context of these reliable settings.",
            "So we're going to work in diagnostics."
        ],
        [
            "Think again so.",
            "This the way we get the data is the same X1X1Y one through XMYM come from some arbitrary joint distribution.",
            "Anne.",
            "And now again we want to compare ourselves with classifiers from a class.",
            "But let's say we've picked some class F and we now we look at classifiers that of the following form.",
            "So they make no false positive errors, let's say.",
            "And subject to this, they minimize their false negative errors.",
            "And.",
            "Again, the game is going to be Wonder algorithm too.",
            "I'll put some hypothesis that makes almost no false positive errors, and subject to this minimizes its false negative error rate, right?",
            "And so this was a model actually introduced in at Coulter few years ago with Adam Kalai.",
            "Any shame on, Sir?",
            "And this captures examples like spam classification, where you really don't want to make false positives.",
            "And you can define negative reliable learning analogously.",
            "So just flip positives and negatives.",
            "And I'm going to talk about one last model and then I'll."
        ],
        [
            "I'm going to say something about the results, so this is the fully reliable learning setting which is also in this paper.",
            "And here again, the setup is the same.",
            "You get some data.",
            "But now we want to look at classifiers that can predict positive, negative or?",
            "Right?",
            "So they can also abstain from making a prediction and.",
            "And in this way we would define it as you look at a class of functions F and you define a partial classifier using pairs of classifiers from this class.",
            "So take the two functions F plus an F minus in some class and you define a new function G that predicts the same as F + F minus if F lesson F minus both agree on it, and if they disagree you predict?",
            "So this defines a partial classifier and you look at all the virtual classifiers which make no error and subject to that you look at the one that makes predict?",
            "Least often, so that's your best partial classifier.",
            "And.",
            "And you want your algorithm again to output something that had almost an error and subject to this, it predicts question Marks and not much more often than the best one.",
            "So there's any questions about the model itself.",
            "Take them right now, maybe.",
            "Otherwise I'll move on to describing the results.",
            "OK. OK, so."
        ],
        [
            "So some private results in this model.",
            "So in this paper what we showed was that if a class of functions is agnostically learnable, then it's also positive and negative, reliably learnable.",
            "And in fact we should something stronger that in fact disjunctions of functions in F are also positive, reliably learnability.",
            "And an easy observation that we also made was if F is both positive and negative reliably available, then it's also fully reliably learnable.",
            "So what this shows?",
            "Since that reliable learning is in some sense stop harder than agnostic learning, and in fact also gave some evidence that positive and negative reliable learning might be a bit easier.",
            "It wasn't clear, but the fully reliable learning is strictly is in an agnostic learning or not, and in this talk I'm going to suggest that maybe it will.",
            "It is strictly easier than agnostic learning.",
            "OK.",
            "So.",
            "Talk about the results now so."
        ],
        [
            "So the general approach to designing algorithms that work for learning is like you have.",
            "Function that minimizes some appropriate loss on your training data set and so impact learning.",
            "The problem is easy, you just want to find a consistent classifier.",
            "If you can find that, then you're done in agnostic learning.",
            "Just want to minimize the error on your data set and similarly on these setups.",
            "In reliable learning you would.",
            "Want OK, what would you want to on all the negative examples?",
            "Your classifier is is consistent and on the positive examples you just want to minimize the errors.",
            "So if you could find some classifier like this then it would.",
            "Solve your problem.",
            "The difficulty, of course, is that most of the times these problems would not be convex, and so that computationally hard.",
            "Right, so so of course these problems will not often be convex, and So what we would want to do is find some larger class such that for any function in my original class F, There's some function in H that approximates F in some sense and minimize a different loss function on H, which will hopefully be convex and then also give me something about.",
            "The original problem.",
            "And one of the tools that have been most powerful in learning as being polynomial approximations of various different kinds.",
            "And again I just want to re emphasize that we're focusing on distribution independent learning here.",
            "So.",
            "So what kind of polynom?"
        ],
        [
            "Approximations that we look at.",
            "So let's take a function to a Boolean function F and I want to find a polynomial that agrees with F in sign everywhere.",
            "So this is one kind of polynomial approximation.",
            "So here there would be a simple.",
            "It's just a degree one polynomial that would do the trick."
        ],
        [
            "Had this somewhat more complicated function I would maybe require degree two polynomial and of course my function is defined on many variables of.",
            "The polynomials are motivated and it could be much more complicated, but as long as you have such a polynomial approximation that always agrees in sign.",
            "This is sufficient for PAC learning, so you can now just solve the problem by linear programming.",
            "So I find a polynomial that says that PXIY is always positive and this technique actually gives some of the best known algorithms for PAC learning.",
            "So in a famous paper, cabins in Servidio showed that you could learn DNF in time 2 to the order end to the one third, and basically what they showed is that the threshold degree of DNS is into the 1/3 and this is what gives this result.",
            "Right?",
            "OK, so."
        ],
        [
            "It's also fine today, so the degree of the polynomial will control the running time of this and the sample complexity is related to the weight of this polynomial and I'll come back to this point, maybe in a bit.",
            "OK, but you could look at other approximations an there is just.",
            "So let's say for pointwise approximation.",
            "So now I don't want to find a function that disagrees inside, but I want a polynomial is actually close to the function everywhere.",
            "I.",
            "And.",
            "And this can be much.",
            "This may require much more degree.",
            "But if we have such an approximation but collide Kliment sponsor enter video showed.",
            "Is that this actually gives you agnostic learning algorithms, not just back learning algorithms that they showed that if you just do L1 regression once you have such polynomial approximations, then that will automatically give you.",
            "And ignostic learning algorithm for the class that has these polynomial approximations.",
            "And in fact this is really the only tool we have for.",
            "Distribution independent agnostic learning.",
            "So it's so maybe this even kind of approximation is required for agnostic learning."
        ],
        [
            "So now I'm going to talk about the approximation that we use in this paper, which is a 1 sided approximation.",
            "So now we want a polynomial.",
            "This we have some function which is a pointwise approximation on when the function is negative.",
            "So when F is minus one, we want something to be very close to F. But if F is plus one, we just want the polynomial to take a value that's larger than one.",
            "This somewhere intermediate between pointwise approximation in threshold approximations, and so we call this positive 1 sided approximation polynomial.",
            "And.",
            "OK. And So what will show is that this actually suffices for positive, reliable learning.",
            "And again, the point is that 1 sided approximate degree can be much lower than approximate degree and this notion was actually introduced fairly recently, just last year to prove lower bounds in complexity theory.",
            "OK, so now we say what I mean.",
            "Result is and try and schedule proof of it."
        ],
        [
            "What we show is there any class function after functions if that has positive 1 sided polynomial approximations of degree D, an weight W. So the weight of a polynomial is going to be defined as they want all the coefficients to be integers and the weight is just the sum of absolute values of the coefficients.",
            "And so then you can learn a positive reliably this class in time that send to the authority and the sample complexity is polynomial in NWN one over epsilon.",
            "And you can have an analogous result for negative reliable learning just by flipping the approximation.",
            "Alright, so.",
            "And the algorithm to do is is just as a convex program, so I'm going to find a polynomial of the correct degree that minimizes this hinge loss on all the positive examples.",
            "So this is 1 -- P X.",
            "If this is positive, otherwise it's zero subject to the constraint set on all the negative examples, the polynomial agrees and is actually less than minus one plus epsilon, so this is.",
            "Going to be giving us constraints with the program.",
            "OK, so the."
        ],
        [
            "And there was that the on the positive examples we have a hinge loss function which is convex, so that's not a problem.",
            "And then for the negative examples we actually have to use a 01 loss which is not convex, but their poses constraints with a linear program.",
            "So the overall the program still remains a convex program and can be solved efficiently.",
            "OK, so now that we know that these kinds of approximations give learning algorithms, what what classes of functions have these kind of polynomial approximations?",
            "And so we look at the class of.",
            "My threshold, so to get functions which is a sign of some linear threshold linear function, which is where these W's are integers and W is the total weight of this.",
            "It can be shown that the the degree the 1 sided approximation degree of this is square root of the weight.",
            "So the proof of this is just using Chebyshev polynomials that uses standard techniques, but we know that if it's just looking at approximate degree where we want the pointwise approximation, even the majority function requires degree that's linear in N. So what this shows is that the class of majorities can actually be done in positive, reliable, negatively and also fully reliable setting in time that's two to the order square root of N. So which is the best known agnostic learning algorithm which uses this point?",
            "Worse approximation takes time that's due to the.",
            "Omega fenders so we also prove a few others."
        ],
        [
            "I also want one of the things is a composition results if a class of functions can be has these positive 1 sided polynomial approximations of degree D, then if you look at disjunctions from this class it also has polynomial approximation of degree D and wait.",
            "It's multiplied by the number of functions that we take disjunctions over and so we get.",
            "These results for learning disjunctions of classes as well for any class that has admits these approximations, and we can also get a tradeoff between the running time and the sample complexity by getting a tradeoff between the weight of the polynomial and the degree of the polynomial.",
            "So if you allow the degree to be larger than is required, then we can reduce the weight.",
            "OK."
        ],
        [
            "So so just to conclude, so this is a picture that sort of Morales summarizes what I said, so we have these settings through the back learning diagnostic learning which most of you are familiar with, and if we look at reliable learning framework.",
            "We have different kinds of polynomial approximations that seem to be learning learning algorithms.",
            "In all of these settings, and depending on which approximation you need, it will have different decreases and these kinds of tools seem to give the most powerful algorithms in all of these settings and."
        ],
        [
            "There's some open questions, so we showed that for low rate threshold essentially it has degree that can be sub linear and end.",
            "What can be said about general half spaces.",
            "So actually if the weight of ahoskie spend we really large.",
            "So if it's like 2 to the Omega and offend then we can show that it requires linear degree for even for the 1 sided approximations.",
            "But for smaller wait it's not clear what happens.",
            "And maybe what are some other applications in learning or elsewhere of these one sided polynomial approximations?",
            "So stop here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is joint work with Justin, who's in the picture.",
                    "label": 0
                },
                {
                    "sent": "If you can see him.",
                    "label": 0
                },
                {
                    "sent": "Who's at Yahoo labs?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me begin with some learning scenarios, so all of us use email an we get a lot of spam, and of course it's annoying to deal with spam messages, so we don't want them.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand it's it's much, much more costly if actually an important email gets into spam, and so this is a setting where.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "False positives can be much worse than false negatives as far as learning is concerned.",
                    "label": 1
                },
                {
                    "sent": "There could be other situations.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you're trying to detect failures in some electrical network or something.",
                    "label": 1
                },
                {
                    "sent": "If you fail to detect, that's very costly.",
                    "label": 0
                },
                {
                    "sent": "If you detect unnecessarily annoying, but maybe not so bad and get another situations, you actually want to make no errors.",
                    "label": 0
                },
                {
                    "sent": "So let's say you were running a medical diagnostic test and you just want to decide whether or not a patient needs an operation and you don't want to make an error on that.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can allow your classifier to say, don't know, and you want to go and do another test.",
                    "label": 1
                },
                {
                    "sent": "So, so we call these kinds of problems reliable learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a lot of prior work on this, and I'm not going to be able to talk about most of it, but in general, one could always right asymmetric loss function, where false positives and false negatives are weighted differently, maybe, and try and minimize these, and there's been a lot of work in statistics actually starting from Neyman Pearson, that gives actually very good properties of what this.",
                    "label": 0
                },
                {
                    "sent": "How these classifiers should look?",
                    "label": 0
                },
                {
                    "sent": "But the question that we are interested in, what is the computational complexity of finding these kinds of classifiers so these last functions are almost never going to be convex?",
                    "label": 1
                },
                {
                    "sent": "And how do we solve this minimization problem efficiently in some cases?",
                    "label": 0
                },
                {
                    "sent": "So that's the problem we're interested in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me first now talk about the exact model that we're going to work in, and this is going to be in the agnostic learning framework.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I talk about diagnostic learning framework, let me quickly go over the PAC Learning framework which was introduced by Les Valiant and I'll be quick here.",
                    "label": 1
                },
                {
                    "sent": "So in the past learning we have points X one through XM that come from distribution and here awhile assume the distribution is always over minus one one to the inserts over the Boolean cube and these points are labeled and here the assumption is that the labels actually come from some function from a class like linear separator save and so there's no noise in any of the labels that you see, and so the goal of the learning algorithm is to output.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis that does almost as well so it has error at most epsilon, so it gets almost all examples correctly.",
                    "label": 0
                },
                {
                    "sent": "And when we want the learning algorithm to succeed for all distributions in this talk, I'm going to look for algorithms that have to work for all distributions.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And agnostic learning is framework is a generalization of this pack framework, so this was introduced by.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Housler and contribution silly in the early 90s.",
                    "label": 0
                },
                {
                    "sent": "And here there's no longer an assumption that data is labeled according to some fixed functions.",
                    "label": 0
                },
                {
                    "sent": "With the labels may be completely arbitrary, so they come from some joint distribution over minus one, one to the N N -- 1 one, and now the goal has to be different.",
                    "label": 0
                },
                {
                    "sent": "So what's the goal here?",
                    "label": 0
                },
                {
                    "sent": "So let's say we fix some class of functions.",
                    "label": 0
                },
                {
                    "sent": "Halfspaces say you look at the best half space that explains the data right?",
                    "label": 0
                },
                {
                    "sent": "Then it will have some error and what you want your learning algorithm to do is output hypothesis that has error.",
                    "label": 0
                },
                {
                    "sent": "That's almost.",
                    "label": 0
                },
                {
                    "sent": "As good as this best one from this class.",
                    "label": 0
                },
                {
                    "sent": "So that's the standard agnostic model.",
                    "label": 0
                },
                {
                    "sent": "And now let's say what we want in the context of these reliable settings.",
                    "label": 0
                },
                {
                    "sent": "So we're going to work in diagnostics.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think again so.",
                    "label": 0
                },
                {
                    "sent": "This the way we get the data is the same X1X1Y one through XMYM come from some arbitrary joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And now again we want to compare ourselves with classifiers from a class.",
                    "label": 0
                },
                {
                    "sent": "But let's say we've picked some class F and we now we look at classifiers that of the following form.",
                    "label": 1
                },
                {
                    "sent": "So they make no false positive errors, let's say.",
                    "label": 0
                },
                {
                    "sent": "And subject to this, they minimize their false negative errors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Again, the game is going to be Wonder algorithm too.",
                    "label": 0
                },
                {
                    "sent": "I'll put some hypothesis that makes almost no false positive errors, and subject to this minimizes its false negative error rate, right?",
                    "label": 0
                },
                {
                    "sent": "And so this was a model actually introduced in at Coulter few years ago with Adam Kalai.",
                    "label": 0
                },
                {
                    "sent": "Any shame on, Sir?",
                    "label": 0
                },
                {
                    "sent": "And this captures examples like spam classification, where you really don't want to make false positives.",
                    "label": 1
                },
                {
                    "sent": "And you can define negative reliable learning analogously.",
                    "label": 0
                },
                {
                    "sent": "So just flip positives and negatives.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about one last model and then I'll.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to say something about the results, so this is the fully reliable learning setting which is also in this paper.",
                    "label": 0
                },
                {
                    "sent": "And here again, the setup is the same.",
                    "label": 0
                },
                {
                    "sent": "You get some data.",
                    "label": 0
                },
                {
                    "sent": "But now we want to look at classifiers that can predict positive, negative or?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So they can also abstain from making a prediction and.",
                    "label": 0
                },
                {
                    "sent": "And in this way we would define it as you look at a class of functions F and you define a partial classifier using pairs of classifiers from this class.",
                    "label": 0
                },
                {
                    "sent": "So take the two functions F plus an F minus in some class and you define a new function G that predicts the same as F + F minus if F lesson F minus both agree on it, and if they disagree you predict?",
                    "label": 0
                },
                {
                    "sent": "So this defines a partial classifier and you look at all the virtual classifiers which make no error and subject to that you look at the one that makes predict?",
                    "label": 0
                },
                {
                    "sent": "Least often, so that's your best partial classifier.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And you want your algorithm again to output something that had almost an error and subject to this, it predicts question Marks and not much more often than the best one.",
                    "label": 0
                },
                {
                    "sent": "So there's any questions about the model itself.",
                    "label": 0
                },
                {
                    "sent": "Take them right now, maybe.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I'll move on to describing the results.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some private results in this model.",
                    "label": 0
                },
                {
                    "sent": "So in this paper what we showed was that if a class of functions is agnostically learnable, then it's also positive and negative, reliably learnable.",
                    "label": 1
                },
                {
                    "sent": "And in fact we should something stronger that in fact disjunctions of functions in F are also positive, reliably learnability.",
                    "label": 0
                },
                {
                    "sent": "And an easy observation that we also made was if F is both positive and negative reliably available, then it's also fully reliably learnable.",
                    "label": 0
                },
                {
                    "sent": "So what this shows?",
                    "label": 0
                },
                {
                    "sent": "Since that reliable learning is in some sense stop harder than agnostic learning, and in fact also gave some evidence that positive and negative reliable learning might be a bit easier.",
                    "label": 1
                },
                {
                    "sent": "It wasn't clear, but the fully reliable learning is strictly is in an agnostic learning or not, and in this talk I'm going to suggest that maybe it will.",
                    "label": 0
                },
                {
                    "sent": "It is strictly easier than agnostic learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Talk about the results now so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general approach to designing algorithms that work for learning is like you have.",
                    "label": 1
                },
                {
                    "sent": "Function that minimizes some appropriate loss on your training data set and so impact learning.",
                    "label": 0
                },
                {
                    "sent": "The problem is easy, you just want to find a consistent classifier.",
                    "label": 1
                },
                {
                    "sent": "If you can find that, then you're done in agnostic learning.",
                    "label": 1
                },
                {
                    "sent": "Just want to minimize the error on your data set and similarly on these setups.",
                    "label": 0
                },
                {
                    "sent": "In reliable learning you would.",
                    "label": 1
                },
                {
                    "sent": "Want OK, what would you want to on all the negative examples?",
                    "label": 0
                },
                {
                    "sent": "Your classifier is is consistent and on the positive examples you just want to minimize the errors.",
                    "label": 0
                },
                {
                    "sent": "So if you could find some classifier like this then it would.",
                    "label": 1
                },
                {
                    "sent": "Solve your problem.",
                    "label": 0
                },
                {
                    "sent": "The difficulty, of course, is that most of the times these problems would not be convex, and so that computationally hard.",
                    "label": 0
                },
                {
                    "sent": "Right, so so of course these problems will not often be convex, and So what we would want to do is find some larger class such that for any function in my original class F, There's some function in H that approximates F in some sense and minimize a different loss function on H, which will hopefully be convex and then also give me something about.",
                    "label": 0
                },
                {
                    "sent": "The original problem.",
                    "label": 0
                },
                {
                    "sent": "And one of the tools that have been most powerful in learning as being polynomial approximations of various different kinds.",
                    "label": 0
                },
                {
                    "sent": "And again I just want to re emphasize that we're focusing on distribution independent learning here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what kind of polynom?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximations that we look at.",
                    "label": 0
                },
                {
                    "sent": "So let's take a function to a Boolean function F and I want to find a polynomial that agrees with F in sign everywhere.",
                    "label": 0
                },
                {
                    "sent": "So this is one kind of polynomial approximation.",
                    "label": 0
                },
                {
                    "sent": "So here there would be a simple.",
                    "label": 0
                },
                {
                    "sent": "It's just a degree one polynomial that would do the trick.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Had this somewhat more complicated function I would maybe require degree two polynomial and of course my function is defined on many variables of.",
                    "label": 0
                },
                {
                    "sent": "The polynomials are motivated and it could be much more complicated, but as long as you have such a polynomial approximation that always agrees in sign.",
                    "label": 0
                },
                {
                    "sent": "This is sufficient for PAC learning, so you can now just solve the problem by linear programming.",
                    "label": 1
                },
                {
                    "sent": "So I find a polynomial that says that PXIY is always positive and this technique actually gives some of the best known algorithms for PAC learning.",
                    "label": 1
                },
                {
                    "sent": "So in a famous paper, cabins in Servidio showed that you could learn DNF in time 2 to the order end to the one third, and basically what they showed is that the threshold degree of DNS is into the 1/3 and this is what gives this result.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's also fine today, so the degree of the polynomial will control the running time of this and the sample complexity is related to the weight of this polynomial and I'll come back to this point, maybe in a bit.",
                    "label": 1
                },
                {
                    "sent": "OK, but you could look at other approximations an there is just.",
                    "label": 0
                },
                {
                    "sent": "So let's say for pointwise approximation.",
                    "label": 0
                },
                {
                    "sent": "So now I don't want to find a function that disagrees inside, but I want a polynomial is actually close to the function everywhere.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And this can be much.",
                    "label": 0
                },
                {
                    "sent": "This may require much more degree.",
                    "label": 0
                },
                {
                    "sent": "But if we have such an approximation but collide Kliment sponsor enter video showed.",
                    "label": 0
                },
                {
                    "sent": "Is that this actually gives you agnostic learning algorithms, not just back learning algorithms that they showed that if you just do L1 regression once you have such polynomial approximations, then that will automatically give you.",
                    "label": 1
                },
                {
                    "sent": "And ignostic learning algorithm for the class that has these polynomial approximations.",
                    "label": 0
                },
                {
                    "sent": "And in fact this is really the only tool we have for.",
                    "label": 0
                },
                {
                    "sent": "Distribution independent agnostic learning.",
                    "label": 1
                },
                {
                    "sent": "So it's so maybe this even kind of approximation is required for agnostic learning.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about the approximation that we use in this paper, which is a 1 sided approximation.",
                    "label": 0
                },
                {
                    "sent": "So now we want a polynomial.",
                    "label": 0
                },
                {
                    "sent": "This we have some function which is a pointwise approximation on when the function is negative.",
                    "label": 0
                },
                {
                    "sent": "So when F is minus one, we want something to be very close to F. But if F is plus one, we just want the polynomial to take a value that's larger than one.",
                    "label": 0
                },
                {
                    "sent": "This somewhere intermediate between pointwise approximation in threshold approximations, and so we call this positive 1 sided approximation polynomial.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK. And So what will show is that this actually suffices for positive, reliable learning.",
                    "label": 0
                },
                {
                    "sent": "And again, the point is that 1 sided approximate degree can be much lower than approximate degree and this notion was actually introduced fairly recently, just last year to prove lower bounds in complexity theory.",
                    "label": 1
                },
                {
                    "sent": "OK, so now we say what I mean.",
                    "label": 0
                },
                {
                    "sent": "Result is and try and schedule proof of it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we show is there any class function after functions if that has positive 1 sided polynomial approximations of degree D, an weight W. So the weight of a polynomial is going to be defined as they want all the coefficients to be integers and the weight is just the sum of absolute values of the coefficients.",
                    "label": 1
                },
                {
                    "sent": "And so then you can learn a positive reliably this class in time that send to the authority and the sample complexity is polynomial in NWN one over epsilon.",
                    "label": 1
                },
                {
                    "sent": "And you can have an analogous result for negative reliable learning just by flipping the approximation.",
                    "label": 1
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm to do is is just as a convex program, so I'm going to find a polynomial of the correct degree that minimizes this hinge loss on all the positive examples.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 -- P X.",
                    "label": 0
                },
                {
                    "sent": "If this is positive, otherwise it's zero subject to the constraint set on all the negative examples, the polynomial agrees and is actually less than minus one plus epsilon, so this is.",
                    "label": 0
                },
                {
                    "sent": "Going to be giving us constraints with the program.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there was that the on the positive examples we have a hinge loss function which is convex, so that's not a problem.",
                    "label": 0
                },
                {
                    "sent": "And then for the negative examples we actually have to use a 01 loss which is not convex, but their poses constraints with a linear program.",
                    "label": 0
                },
                {
                    "sent": "So the overall the program still remains a convex program and can be solved efficiently.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that we know that these kinds of approximations give learning algorithms, what what classes of functions have these kind of polynomial approximations?",
                    "label": 0
                },
                {
                    "sent": "And so we look at the class of.",
                    "label": 1
                },
                {
                    "sent": "My threshold, so to get functions which is a sign of some linear threshold linear function, which is where these W's are integers and W is the total weight of this.",
                    "label": 0
                },
                {
                    "sent": "It can be shown that the the degree the 1 sided approximation degree of this is square root of the weight.",
                    "label": 1
                },
                {
                    "sent": "So the proof of this is just using Chebyshev polynomials that uses standard techniques, but we know that if it's just looking at approximate degree where we want the pointwise approximation, even the majority function requires degree that's linear in N. So what this shows is that the class of majorities can actually be done in positive, reliable, negatively and also fully reliable setting in time that's two to the order square root of N. So which is the best known agnostic learning algorithm which uses this point?",
                    "label": 1
                },
                {
                    "sent": "Worse approximation takes time that's due to the.",
                    "label": 0
                },
                {
                    "sent": "Omega fenders so we also prove a few others.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I also want one of the things is a composition results if a class of functions can be has these positive 1 sided polynomial approximations of degree D, then if you look at disjunctions from this class it also has polynomial approximation of degree D and wait.",
                    "label": 1
                },
                {
                    "sent": "It's multiplied by the number of functions that we take disjunctions over and so we get.",
                    "label": 0
                },
                {
                    "sent": "These results for learning disjunctions of classes as well for any class that has admits these approximations, and we can also get a tradeoff between the running time and the sample complexity by getting a tradeoff between the weight of the polynomial and the degree of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "So if you allow the degree to be larger than is required, then we can reduce the weight.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so just to conclude, so this is a picture that sort of Morales summarizes what I said, so we have these settings through the back learning diagnostic learning which most of you are familiar with, and if we look at reliable learning framework.",
                    "label": 0
                },
                {
                    "sent": "We have different kinds of polynomial approximations that seem to be learning learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "In all of these settings, and depending on which approximation you need, it will have different decreases and these kinds of tools seem to give the most powerful algorithms in all of these settings and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's some open questions, so we showed that for low rate threshold essentially it has degree that can be sub linear and end.",
                    "label": 0
                },
                {
                    "sent": "What can be said about general half spaces.",
                    "label": 1
                },
                {
                    "sent": "So actually if the weight of ahoskie spend we really large.",
                    "label": 0
                },
                {
                    "sent": "So if it's like 2 to the Omega and offend then we can show that it requires linear degree for even for the 1 sided approximations.",
                    "label": 0
                },
                {
                    "sent": "But for smaller wait it's not clear what happens.",
                    "label": 0
                },
                {
                    "sent": "And maybe what are some other applications in learning or elsewhere of these one sided polynomial approximations?",
                    "label": 0
                },
                {
                    "sent": "So stop here.",
                    "label": 0
                }
            ]
        }
    }
}