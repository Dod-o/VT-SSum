{
    "id": "w3t26tx3ivpr26bwjwh3czxzitih6jie",
    "title": "From kernels to causal inference",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nips2011_scholkopf_inference/",
    "segmentation": [
        [
            "Thank you very much.",
            "So this is somewhat intimidating room and I had time to reflect on that during the two or three minutes that it took me to.",
            "Well from the top to the bottom, but nevertheless I'm very happy to be here and to speak at this first European news conference.",
            "I would like to thank the organizers, not just for inviting me, but also or especially for the additional work they had by running the conference in Europe in this new place in which is a wonderful place, I think.",
            "So I will talk about mainly about causal inference, which is a contentious topic, and if you know something about it, it's quite possible that we will find my choice or my view of things.",
            "My treatment of the topics are a little irritating, so that's the disclaimer at the start.",
            "I'm interested in inference from data in a broad sense, so not just in statistical inference, and I think causal inference is something that's also not just statistical."
        ],
        [
            "So we all know there's a difference between dependence and causation, and so whether there's a strong correlation, for instance between the occurrence of stocks in the birth rate in different European countries, we wouldn't try to increase the birth rate by increasing the number of storks.",
            "That would obviously constitute a form of a cargo cult.",
            "Now this came up in the discussion with never the other day, and I Googled for cargo cover cargo cultism is world for this.",
            "It was coined by Feynman for religious practices in the Pacific tribes around World War Two, trying to obtain wealth cargo by building mock landing strips or waving flags and things like that.",
            "So I googled it for cargo cult cult in causality in that led me to another quote, which is a little scary, so I'll read it to you.",
            "The term cargo cult is also idiomatically used in the words of Wikipedia to mean any group of people who imitate the superficial exterior of a process or system without having any understanding of the underlying substance.",
            "So question is the statistical machine learning a cargo cult."
        ],
        [
            "Translation buffer, philosopher and physicist argued that statistics in causality are not the same, but they are linked by what he called the common cost principle.",
            "So the common cause principle says if you observe two statistically dependent quantities, then there must be another quantity which causally influences both.",
            "So there's no statistical dependence without causality, and this is the picture of the common cause.",
            "Of course, as a special case this common cause could coincide with X or Y.",
            "In which case we would get one of these two pictures.",
            "Moreover, Reichenbach said or postulated that this third variable zed should screen X&Y from each other in the sense that conditioned on zed, X&Y should become statistically independent, so this is a link between statistics and causality which I think was quite profound and it's effectively still at the heart of most causal inference methods.",
            "Nevertheless, personally I prefer to view something else as the most logical starting point for thinking of causal methods and its functional causal model."
        ],
        [
            "It's.",
            "Non functional causal models.",
            "We have a set of observables X one through XN.",
            "We think of these observables as vertices on a DAG.",
            "This semantics of the graph is that parents in the graph correspond to direct causes, and at each vertex we have a function FI which takes as an input all the parents of that vertex and a noise variable, and so we have noise variable per vertex, and these noise variables are assumed to be jointly independent.",
            "Which one can also justify by the light in both principle.",
            "So if if they were not independent, they would have another common cause and then the graph wouldn't be complete.",
            "So we can think of this as a causal sufficiency.",
            "Now we can show that this model.",
            "So obviously if these noises have some distributions then this will induce a joint distribution of the vertices XI and this joint distribution can be shown to satisfy the Titan back principle.",
            "And we can ask some questions about this joint distribution.",
            "One of those when we what kind of properties does it have in the second one?",
            "Is it possible, given complete knowledge of this joint distribution, to recover the graph?",
            "So is it possible to recover the structure of what is the direct cause of what?"
        ],
        [
            "So regarding the first question, what can we say about this distribution?",
            "There's a number of beautiful results this has been studied in great detail by the causality community, and it turns out that the existence of such a functional causal model is equivalent to the so called local causal Markov condition, which people probably know from graphical models only.",
            "Here it has a causal semantics at the local cause.",
            "Markov condition says that variable X here should be statistically independent of its.",
            "Non decendants when conditioned on its parents on the red ones.",
            "So in other words every information exchange between a variable and its non descendants has to involve its parents.",
            "It turns out that this local conditions with this gives us tells us certain local conditional independence properties.",
            "They directly imply a set of global conditional independence properties, which can be characterized in terms of the separation.",
            "I'm not going to go into detail on that.",
            "Moreover, all these things are also equivalent to a certain factorization of the joint distribution in terms of conditional probabilities, sometimes called Markov kernels of variables, given their parents.",
            "So this conditional is market kernels can be thought of as the causal mechanisms that actually generate statistical dependence."
        ],
        [
            "So.",
            "Regarding the second question.",
            "So given the joint distribution complete knowledge which is hypothetical, are we able to infer the graph structure?",
            "The answer to this question is it's impossible if we don't have additional information.",
            "It's possible if we have interventions or randomized control trials.",
            "And Moreover, if we assume something called faithfulness, which is an assumption that looks somewhat innocuous, but it isn't that innocuous.",
            "And also if we have certain assumptions that may conditionally independent testing feasible, or if we have a good good enough conditionally independent testing method, then we can estimate a so called Markov equivalence class of graphs that contains the correct graph by running conditional independence test.",
            "So remember the Markov assumption was saying that certain conditional independence properties hold, so given the distribution we can in principle test all possible conditional independence statements.",
            "This constrains which graphs.",
            "Fit together with it and we typically end up with this equivalence class, which usually contains more than one graph, so usually we don't get a single graph, and in particular in the very simplest case where we have graphs with just two nodes, so just two nodes, one could be caused, one could be effect, or vice versa.",
            "This is going to be useless because of course every conditional independence statement already involves three variables, so there's no nontrivial conditionally independent statements that we can make for two variables.",
            "So, so we're stuck.",
            "So what can we do?",
            "What kind of assumptions could we make in order to make the two variable case solvable?",
            "So that's the main problem that we looked at over the last years, and I'm going to tell you a little bit about the kind of methods that we came up with, sorted in order of increasing strangeness.",
            "So I'll start with."
        ],
        [
            "Maybe the most plausible one which is to restrict the functional model.",
            "So remember our functional model that I showed you before the general one said that each variable is a function of its parents and annoys variable, and these noises were joined to independent.",
            "Now let's assume a particular relatively simple noise that just takes discrete values.",
            "So suppose that noise can only take 10 different values.",
            "In that case, we could still imagine that this noise, which is random switches, are functions between different mechanisms, so in different functions, not just of the parents.",
            "Of course, it's additional restriction that the noise has to be discrete.",
            "The noise could also be a continuous variable, in which case in principle we could switch between a non countable number of different dependencies.",
            "With such a mechanism.",
            "So this is a very general model and I think you will follow me that intuitively it seems very difficult to identify this kind of model if we don't make additional restrictions.",
            "In fact, it's even surprising.",
            "Of course these models still implies certain conditional independence properties, the Markov properties.",
            "So it has some implications and sometimes it allows us to identify something which is sort of surprising, but I think as a machine learner you will.",
            "You will say we all know there is no free lunch.",
            "We have to make assumptions about the function classes.",
            "So let's make an assumption and let's make an assumption that will somehow limit the complexity with which the noise can act on our functions.",
            "And this assumption in my case is an additivity assumption.",
            "So I'll assume the noise acts only additively.",
            "You could say this is a little bit like saying we we.",
            "Assume the noise is small that the function is relatively smooth, and maybe the inputs are concentrated.",
            "But in any case it's an assumption it's strong assumption."
        ],
        [
            "It turns out it's strong enough to make the two variable case solvable.",
            "So let's try to get an intuition for that.",
            "So two variable case means we have a model.",
            "It looks like this.",
            "So why is a function of X plus noise?",
            "And this noise is independent of the input X and we cannot ask the question.",
            "Will it be possible to write an inverse model of the same form?",
            "So if we assume additivity?",
            "Is it?",
            "Does it become?",
            "Is it still symmetric problem that we can view either way or does it become a symmetric and it turns out it becomes a symmetric and to see this, just imagine that the noise is like this so it is an additive noise of bounded range Sigma.",
            "So if we now have our function F of X we add this noise.",
            "We get this kind of tube around this regression function or this function connecting X&Y and of course Now if we look at the backward direction you will see that if this function is nonlinear.",
            "Then the width of this tube in the backward direction will vary with why.",
            "In other words, the noise will no longer be independent of why it would be a heteroscedastic noise.",
            "So we had this intuition relatively fast, but it still took us half a year to prove a theorem about it."
        ],
        [
            "About the two variable case, and I'm not going to go through all the details here, I'll just tell you the main message of this theorem.",
            "So the theorem says that.",
            "If we assume there's a density and the joint density has this form, so it's a product of the noise density.",
            "Sorry, the noise density here for an additive noise, so it's just the density for the difference between Y and FX, and the input density here.",
            "M. If we assume this model is true, and if we assume, Moreover, that in the backward direction we have a model of the same form, then we can derive a complicated differential equation which connects the logarithm of the noise density, the logarithm of the input density, and the function.",
            "F in this complicated way, and when can show that.",
            "Under some assumptions, this differential equation is only a 3 dimensional solution space out of a space of possibly infinitely many functions, which means that only if the noises and the nonlinearity the function are somehow tuned to each other in a very special way will this differential equation be satisfied.",
            "So in the general generic cases will be violated.",
            "Therefore we won't be able to find a backward model of the same form.",
            "One case where this differential equation is satisfied is of course the case of linear functions in Gaussian noise.",
            "So this was very already known that this case is not identifiable."
        ],
        [
            "Now we can try to use this to suggest a causal inference method, and I think it is obvious how we're going to do it.",
            "We will compute a regression of X&Y.",
            "We compute the residual error term and then we check whether this error and the input are statistically independent and we're going to try this in both directions X to Y or Y to X and see in which direction we have independence or we have at least more independence in the other direction or less dependence."
        ],
        [
            "And we can then try to apply this.",
            "For instance.",
            "This is one data set where we have the altitude and the average temperature of different places in Germany.",
            "And it's clear that the temperature we can review with the temperature as the cause of an added to the altitude of a city, but vice versa.",
            "The altitude certainly will have a causal inference on the temperature.",
            "So if we do it."
        ],
        [
            "And we do it in both directions, and we look at the residual noise terms.",
            "We we find relatively independent noise here.",
            "The correct direction and highly dependent noise dependent on the input in the wrong direction.",
            "So so that's good, and I'll show you more results later."
        ],
        [
            "I should also mention that this work has been generalized to models that are not just with additive noise, but have a post nonlinearity applied to the sum of F of X and noise.",
            "By signing into varied and it has been recently generalized by our group to graphs with more than two vertices.",
            "So in that case basically the story is as long as we have a function class for which the two variable cases identifiable.",
            "We can prove identifiability in the invariable case and under an assumption which is called causal minimality, which it turns out is a little little bit weaker than this assumption of faithfulness that I mentioned before, so that's nice.",
            "And we started working.",
            "Or especially you always my who has a poster here at NIPS on the case where we have looks so no longer directed acyclic graphs."
        ],
        [
            "Now the regression.",
            "We don't method I told you is is somehow a little bit broken for this problem or a bit misplaced in the sense that.",
            "Usually regression methods assume something about noise distributions, and if these assumptions are not correct, we might get a solution where the residuals are dependent even though in reality they shouldn't be.",
            "So we can address this by trying to minimize the resident.",
            "The dependence of the residuals rather than some error metric, or rather than maximizing the likelihood of the data.",
            "And we can do this by taking some dependence measure should be at Amanda's measure, not just a correlation measure.",
            "And since we come from the kernel world, we use the kernel measure.",
            "There is a distance between certain kernel mean embeddings and I'll tell you a little bit about this.",
            "On the next slide."
        ],
        [
            "So this builds on these methods of kernel independence testing, and these methods can be derived by viewing, looking at the kernel mean map.",
            "So what is the kernel mean map?",
            "Do we have some probability measure P and we map it into?",
            "We take the feature map, so this is one way to write the feature, map the map into the feature space for support vector machine.",
            "We take this map and take its expectation with respect to this probability measure.",
            "So this gives us an element of the reproducing kernel Hilbert space, and it turns out that this element or this this mapping is invertible or injective under certain conditions on the kernel, for instance universality.",
            "So for instance, the Gaussian kernel turns out will have an injective kernel mean map, which means that this we don't lose any information if we represent a probability measure by this quantity here.",
            "Which in turn implies that if we wanted to do independent testing, what we can do is we can map the joint joint distribution into our reputation, Kerbal Space.",
            "We can also map the product of two marginal distributions and these two quantities of course will be, well.",
            "These two distributions will be the same if and only if the variables are independent.",
            "And if we are injective then also this quantity and this quantity will be the same if and only if we have independence.",
            "So we can take this length of this difference vector as a test statistic or an estimate of this as a test statistics for independence, and we can use this kind of quantity as a criterion in our regression that I showed you in the last slide, so I don't have time to talk about this much more.",
            "Maybe I should just mention that this is actually a generalization of what statisticians call the moment generating function.",
            "OK.",
            "So, since we're already in the world of kernels, I wanted to say I think this is a.",
            "This is a nice example of the new generation of kernel methods, which is quite different from from the past.",
            "At the beginning of the kernel methods we were thinking the whole field will just be about kernel Ising multiple algorithms coming up with nonlinear versions of algorithms and just to relieve this past for a minute."
        ],
        [
            "A slide from my first NIPS talk, which was in 1998 and I wanted to point out that this algorithm still hasn't been developed yet."
        ],
        [
            "So, back to causality.",
            "So we have this additive noise assumption, which is a strong assumption, but it allows us to do certain things and we also looked at this this problem of detecting confounders, which is a notorious problem in causality, because usually if you get 2 variables it's not the case that one of them is a course of the other one.",
            "Usually there's something something else that comes into play, and so ideally we would like a method that given observations of X&Y are given the joint distribution can decide whether either one of.",
            "Is a reason of the other one, or there's a confounder that we haven't observed, so we make an additive noise assumption?",
            "Also in this case, so we assume X is a function of the confounder plus noise, same for Y, and these noises in the confounder should be jointly independent, so there's the corresponding assumption and you can think of this as some kind of dimensionality reduction problem, so we try to find the manifold parameterized by T, which somehow explains X&Y.",
            "Up to independent noises so it's almost like a dimensionality reduction only.",
            "Again, the criterion is a little different is not minimizing sum squared error or something like that, but minimizing dependence of the residuals.",
            "And we can do this.",
            "I should say we don't really have good algorithms for this yet, but we can do the toy examples and we have an identifiability result for small noise."
        ],
        [
            "So everything I told you about so far was using properties of the noise to do causal inference so it it would seem that maybe this is the crucial thing we have to look at how the noise behaves, But it turns out they are further as symmetries that we can use for causal inference.",
            "In I'll show you one such as symmetry.",
            "And to show you that it doesn't build a noise, we apply it or we study in the case of deterministic relationships.",
            "So here we have Y = F of X with an invertible function of X.",
            "So we can write it either way.",
            "It's completely symmetric, there's no noise.",
            "Nevertheless, we still want to decide his extra cause of why or his way the course of X.",
            "So if you want, you could think about this for a second.",
            "That seems like a completely strange problem.",
            "Wouldn't make sense to study this at all.",
            "So here our assumption or idea that we use is that if X causes Y, then it seems to be reasonable to assume that this function so X is the cause.",
            "Why is the effect this function and the density over X?",
            "So the thing that we feed into the function should be independent in some sense.",
            "They don't know if you travel, the mechanism doesn't know what we feed into the mechanism, so of course this could be wrong if we have feedback loops and things like that.",
            "But we're considering simple case, so let's assume that this independence holds.",
            "Then it should be the case that the structure of this density in particular doesn't correlate with the structure of this function.",
            "However, in the backward direction, whenever this this function here was flat, we suddenly have a lot of mass in the output distribution of the output density.",
            "Therefore, the output density should have large mass wherever this function is flared, or wherever the inverse function is very steep.",
            "So the peaks of the output distribution should correlate with the slopes.",
            "The slope of the inverse function.",
            "So let's try to formalize this a little bit."
        ],
        [
            "And see where it takes us.",
            "So we assume F for simplicity's objection of 01 we're going to consider the slope of F or actually, will you actually, the logarithm of the slope of F as one random variable and as the second random variable, we will consider the value of the density, so that's a little strange.",
            "But of course we can do that.",
            "We consider it as a random variable on this probability space with lebec measure, and then we can write down the covariance of these two random variables.",
            "And we will stipulate that in particular, this covariance should be 0.",
            "Suddenly these two objects are independent.",
            "Their covariance should be 0.",
            "And it turns out that under this assumption, we can prove that in the backward direction, the covariance first of all is lower bounded by zero, and we have equality if and only if the identity mapping.",
            "So in the non trivial case it will be the case that in the correct in the forward direction covariance is zero in the backward direction these covariance will be strictly positive.",
            "It is clear that this kind of symmetry allows us to operate on the estimator that we can use in practice for inference.",
            "But I want to 1st tell you a little bit more about."
        ],
        [
            "A."
        ],
        [
            "This kind of postulate.",
            "We can rewrite this."
        ],
        [
            "Postulate in various ways and and maybe I don't have time to go through all the details, but I should say we can write it as an activity condition for entropies.",
            "We can write it as an information, geometric orthogonality in information space and also here.",
            "Maybe I'll say a little bit about this.",
            "We can write it as this condition, which says that here we have the relative entropy between the output distribution and UY, which is a uniform density on the output.",
            "So that you can think of as the area irregularity of the observed output, the observed distribution over Y.",
            "And so this is now equal to the relative entropy between PX and UX.",
            "So the distance between the input distribution observed and a uniform input distribution, and plus you're the third term, which is the distance between VY, which is the density on why which would be induced if we feed.",
            "If we fed a uniform distribution over X into the function.",
            "So this is somehow the effect that the function has.",
            "On a uniform distribution, and here we measure the distance between that and the uniform output.",
            "So this is only the irregularity induced by the output.",
            "So we have this kind of interpretation, which I think is quite nice, and now we have several methods and let's."
        ],
        [
            "Get some datasets so we have collected a number of cause effect datasets where we think the ground truth is reasonably clear is not always easy, and if you have more data sets, please send them to us that these datasets are on the web.",
            "Yeah, you can look at them."
        ],
        [
            "So here's some examples of datasets, altitude, temperature I mentioned before, but there's also other things like age of a person and their wage.",
            "It's clear that the wage is not a cause of the age.",
            "Vice versa.",
            "There's certainly some costly effect, so we mark these different datasets with the ground truth that we think are reasonable and then."
        ],
        [
            "Different algorithms?",
            "And here's the result.",
            "So what I show is the fraction of correct decisions for different algorithms, distinguishing cause and effects.",
            "And of course the chance level would be 50%.",
            "And on the X axis we have the decision rate.",
            "By this I mean all these algorithms have some some continuous quantity that we used to decide and we could threshold it and allow the algorithms not to decide in cases where there's no no clear difference.",
            "So if we decide then if we force him to decide always then we can see the performances go closer to chance level, closer to 50%.",
            "If we allow them to not take decisions in half of the cases we get higher performances.",
            "The Gray area is a significant region or insignificance.",
            "Basically, if you did random guessing, then in 95% of the cases you should end up in this Gray area.",
            "So let's look where are we outside of this Gray area?",
            "And you can see different methods.",
            "There's just the deterministic method that I just told you about.",
            "The Red One is our additive noise methods that I mentioned before this took was one is additive noise with post nonlinearity.",
            "This one here is a Gaussian process based method that we had a nips last year in the green one is the previous method, which is a sense for linear non Gaussian model linear, non Gaussian acyclic model.",
            "It's basically assuming linear relationships in non Gaussian noise, so I don't want to interpret the differences in great detail, but I think the main message here is that we can solve this problem better than chance and with performance better than chance, and I think that's already quite encouraging because this problem used to be considered insolvable.",
            "As far as I know."
        ],
        [
            "OK, so.",
            "What's the point of estimating cause and effects?",
            "So let me try to connect things a little bit more to machine learning and mode.",
            "Maybe motivate why why we should be interested in this kind of causal inference problem.",
            "To do this, I want to discuss 2 machine learning problems.",
            "The first one is a problem from bioinformatics, so suppose we have here the Messenger RNA sequence and you'd like to predict from this sequence something about the protein structure.",
            "Maybe the sequence of amino acids, or maybe something in between.",
            "Something simpler but you want to predict something about the protein from this, and of course in reality we have this Messenger RNA.",
            "There's a mechanism called the ribosome, which travels along this sequence and somehow it translates it into amino acids, builds proteins.",
            "So in reality, there is a causal mechanism called the ribosome, which is believed to be reasonably invariant across different organisms or different species.",
            "The input sequence might differ, of course, otherwise we wouldn't look different, but the mechanism is more or less stable.",
            "It's a causal mechanism that produces a certain output, so if we try to solve this prediction problem from data, we are learning a prediction in the same direction as the causal mechanism, which I have denoted by fire.",
            "Here.",
            "Now let's look another provide another problem that we may be more familiar with.",
            "So that's optical character recognition.",
            "This is the in this set in this case.",
            "And maybe this is a trivial note to many of you, but for me when I first notice it was actually quite a revelation.",
            "In this case, we're actually learning learning in the anti causal direction, so it's not the case that this image is the cause of the label.",
            "In fact, the way that images are produced is this some persons it's done that decides to write the digit 3 and then produces an image of the digit 3.",
            "So that causes the label and of course the images the effect.",
            "Nevertheless we want to learn the mapping from.",
            "Image to label.",
            "Which means that we're still learning from X to Y, but now X is the effect.",
            "And why is the course?",
            "So that's kind of surprising, and it has some interesting."
        ],
        [
            "Implications.",
            "And and I want to specifically look at the implications for covariate shift in semi supervised learning.",
            "So again, we make this assumption that the probability distribution or the density of the course and the density of the mechanism so probability of effect given course in dependently chosen by nature.",
            "So you could argue with that, but I think it's a reasonable assumption and our goal is always to learn X, learn the mapping from X to Y.",
            "So we want to estimate some properties of the conditional of my given X.",
            "So let's look at the formal direction.",
            "So causal causal direction.",
            "So in this case.",
            "What happens if we have covariate shift?",
            "So if the input distribution changes, then of course the mechanism P of Y given X is not affected.",
            "I think that's sort of clear to everybody working in covariate shift and covariate shift adaptation is either trivial or at least possible.",
            "There are some ramifications, but if we have a model that's underspecified, maybe we have to rewrite the samples or things like that.",
            "But in principle it's possible because the mechanism is not affected.",
            "How about semi supervised learning?",
            "Well, we have made this assumption that probability of course, and the probability of effect given costs are independent.",
            "So this means in our case, in this case here where fire and the function are aligned that the distribution of X doesn't contain any information about the condition of Y given X.",
            "So if we are given additional data from P of X and we could maybe get a better estimate of P of X, this shouldn't contain information about the conditional.",
            "So we should not expect to benefit from this and semi supervised learning should be impossible, or at least very difficult to do.",
            "Let's look at the other direction.",
            "So now the causal function, the causal dependencies backwards.",
            "But we still want to predict in this direction, so that was the MNIST case in this case, covariate shift.",
            "Is more difficult because we need to decide if a changes due to a change in the mechanism or due to a change in the course distribution.",
            "That's nontrivial.",
            "I think we have some ideas how to do it, but it's not trivial.",
            "And Secondly, semi supervised learning in this case actually should be possible, because now P of X does contain information about POY given X.",
            "So remember, before in the deterministic case we have results showing that in the forward direction, if we have independence forward direction, we should expect.",
            "Dependence in the backward direction.",
            "So here we have the information.",
            "We have dependence and I think actually, as far as I know, all the semi supervised learning methods are using this in one way or another.",
            "So for instance, if you make the cluster assumption.",
            "It simply means that within areas of high density, high values of this, so within certain areas that are specified by this distribution, the conditional should be constant.",
            "The label doesn't change within clusters.",
            "So that's a way of transferring information from here to here, and I think other assumptions like the low density separation or manifold assumptions and so on fit into this picture as well.",
            "So.",
            "I want to give you some more sophisticated examples of what we can do if we somehow take into account the cost structure specifically.",
            "If we now assume so.",
            "I'm still in this picture that the noise X is additive, so there's again our favorite assumption.",
            "Then of course, the distribution that we observe for this quantity will have the form of a convolution.",
            "It will be a convolution of this noise density and the density of Y transformed by fire.",
            "And of course, then the learning the mapping from X to Y would require deconvolution.",
            "Fact deconvolution in signal processing is, I think, a beautiful example of a prediction problem where one can do a lot better by taking into account the causal direction, namely by estimating effectively this mapping fire and inverting."
        ],
        [
            "So here we have an example where we have to do the convolution.",
            "We built a lens with just a single piece of glass.",
            "This is a really crappy lens, but it's still a causal mechanism that transforms some image are there in the world to a measurement.",
            "So of course into an effect.",
            "And what we can do is we can measure the point spread function of this lens in various places, which means we measure XY examples for a very simple distribution distribution where the the causes are just.",
            "Points and the effects are these images of points you could see."
        ],
        [
            "Now we want to be given another X which is much more much more complex.",
            "For instance from the distribution of blood real world images.",
            "Our goal is to reconstruct the corresponding why so we want to make an anti causal prediction.",
            "Now we can try to do this by training a learning machine on XY pairs, for instance from the class of images like that and I have to admit that I naively tried to do this when I first got into the field of deconvolution and about five years ago and very much failed.",
            "So this is not going to work this problem we cannot solve by it by not taking into Honda causal structure.",
            "But if you do it correctly so if you view it as a decoy."
        ],
        [
            "Pollution problem then you can get."
        ],
        [
            "Very nice results, so this was the result obtained by Christine Schuler."
        ],
        [
            "PhD student, so for these really crappy lens where the image is quite bad in the corners, you can get a reconstruction like that and you can do similar things also in the cases where you don't know the convolution, so this is it's a non blind case.",
            "We have measured the point spread function."
        ],
        [
            "He's a blind case where we have camera motion, so we have some objects out there in the world."
        ],
        [
            "We keep the shutter open, move the camera.",
            "We have some trajectory in this prioritization of the camera position and we can."
        ],
        [
            "Take this into account and then for instance take an input image."
        ],
        [
            "Like this and."
        ],
        [
            "Structed, like that."
        ],
        [
            "And if we take pictures through turbulence, this has a similar but more complex effect than camera motion, and we can also look at this kind of problems and then reconstruct so sort of deconvolution problem for this kind of images and we could get a reconstruction which is actually very close to the ground truth in this case."
        ],
        [
            "And very briefly, just want to mention that the question when did convolution is possible is actually closely related to kernels, so I'll use this in another excuse for a very short digression and for a shift invariant kernel, the kernel mean map that I told you about before.",
            "So this is a shift invariant cuddle.",
            "In this case this is nothing but a convolution and turns out if the kernel is the inverse Fourier transform of an aperture.",
            "I convolved with itself.",
            "So this is the aperture convolved with itself.",
            "That's the kernel.",
            "Free transform the kernel.",
            "So if we take the inverse of that then we can still give a simple physical realization of the kernel mean map in terms of fan hopeful diffraction.",
            "And we can then ask the question.",
            "When is this process invertible or when is found her for imaging invertible and it turns out first of all, if the aperture is finitely large and we have no other assumption, it's not possible, and there's well known in optics that CRB resolution limit.",
            "But if we restrict the class of input distributions here for instance to distributions, so even images that have compact support and then this process becomes invertible even if this is just a very small aperture, as long as it's larger than zero this problem."
        ],
        [
            "Is invertible.",
            "So the last two minutes very briefly.",
            "And for a very different kind of approach to causal inference so far, have used an assumption that the common cause principle that all the causal Markov assumption to connect causality in statistics and it's not clear that statistics should be the only domain where causality leaves a footprint thing.",
            "In real life we often draw causal conclusions from individual objects.",
            "So here we have some individual objects and if you look at them you will probably be quite willing to say that there's some kind of causal link in the form of a common history.",
            "So these things have been produced or designed by the same factory.",
            "So you probably wouldn't say that these two T shirts have been designed by the same factory, so so these two objects share a certain amount of complexity, which makes us willing to do this cause the inference that these two objects don't share."
        ],
        [
            "So let's try to formalize this a little bit.",
            "So remember the causal Markov condition local, the local form, which said that an observable is statistically independent of its non descendants given the parents.",
            "So we can reformulate this more intuitively by saying given all direct causes of an observable, it's non effects give us no additional statistical information on it, and the stress here is in statistical.",
            "So it's a link between causality in statistics."
        ],
        [
            "What if we delete this world statistics?",
            "So then we have a more general principle which we could instantiate in different domains with measures of information that we consider useful, and one such measure could be a combo of complexity or algorithmic complexity.",
            "So this gives us an algorithmic condition, so we just fill in the word algorithmic here."
        ],
        [
            "And let's see where this takes us.",
            "So I think you all have heard about Kolmogorov complexity of a binary string.",
            "Is the length of the shortest program with output X on a Turing machine.",
            "And it's not computable.",
            "We don't worry about this right now.",
            "We cannot solve all problems in the world, but I think it's a.",
            "It's a beautiful."
        ],
        [
            "Concept and it has been generalized in various ways.",
            "So when can define conditional of complexities.",
            "Shortest program that generates WHI from the shortest description of X."
        ],
        [
            "And.",
            "One can define algorithmic mutual information, so information of X about Y and vice versa in terms of Kolmogorov complexity.",
            "It's the number of bits that we save when compressing two objects jointly rather than independently.",
            "We can then use this mutual information to define a notion of algorithmic independence."
        ],
        [
            "All this has been done, so this is not our work.",
            "One can define conditional algorithmic mutual information.",
            "I think you can guess where I'm getting.",
            "So we can define conditional algorithmic independence."
        ],
        [
            "And."
        ],
        [
            "We can then write down all possible.",
            "It's the local algorithmic market conditions, so we have a set of observations formalized as strings, and then given its direct causes, every variable.",
            "Every object is conditionally, algorithmically independent of its not effects, so let's just."
        ],
        [
            "Stipulate this.",
            "First of all, just like in the statistical case, this can be written in three equivalent forms, so we can prove that our local condition is equivalent to a global market condition in terms of the separation, and it's also equivalent to a certain decomposition.",
            "Now is now it's no longer effective decomposition or multiplicative decomposition of the joint probability distribution.",
            "It's a additive decomposition of the joint kernel of complexity.",
            "And."
        ],
        [
            "Also, it was very nice is we can define a certain algorithmic model of causality which we can show to imply the algorithmic Markov condition.",
            "So this algorithmic model says that we have a graph structure.",
            "Each node computes a program of all his parents.",
            "The program in this case is basically identical with the noise in the sense of Kolmogorov complexity.",
            "There's no noise, noise is just high complexity, so this just means we have a complex program.",
            "Here we assume all these programs are jointly independent and their programs are the causal mechanisms, and this model then implies the algorithmic market conditions.",
            "So if you think this is a good model of a certain type of causal structure in terms of programs, then you have to believe that this has certain.",
            "Implications in terms of algorithmic independence that we can in principle maybe not for global complexity but for something else.",
            "Check on data.",
            "So this brings me."
        ],
        [
            "To the end, I should briefly mention we have tried to make this a little bit more practical in different ways."
        ],
        [
            "And with that I would like to acknowledge my Co workers in the field of causality, field of image deconvolution and in this field of kernel means which we've been working on already for a longer time.",
            "And thank you for your attention.",
            "OK, I think the question is isn't it.",
            "Shouldn't it be possible to increase the birth rate by increasing the number of storks in some complicated race?",
            "So I agree with this, so I think it's imaginable that maybe that's what you're suggesting.",
            "If we have more socks.",
            "People like to live there and think this is a nice place, and young families will settle there and produce more babies, so this was a very simplistic model with the storks is probably wrong, probably everything that I've said is wrong in some sense.",
            "If you look closely enough.",
            "So he's saying it optical image and there are no methods for doing super resolution and it's contradiction.",
            "Actually, I don't think it's a contradiction, and I even think.",
            "I mean, OK, I'm saying that if you don't have any assumptions then this process is not invertible, which was the received wisdom for a long time.",
            "But I think what I'm saying is that even going a bit further than this method, it just it says we don't even have to do something fancy by this fluorescent tricks or something like that, but only if if we know for sure that all our images have bounded support.",
            "It doesn't matter how large, but it has to be bounded support, then we can.",
            "We can mathematically invert the problem and I think that the reason or maybe one reason I'm speculating this is not my home field.",
            "My reason by people thought this wasn't possible is that they they always look at this.",
            "Modulation transfer function.",
            "Things like that which are basically descriptions of an optical system, does if you feed an infinitely long sine wave into it.",
            "What kind of frequencies can it transform on?",
            "These are infinitely large objects, but if you object that finally lot turns out, you can use.",
            "You don't lose information, so I think I don't know.",
            "I think this might have interesting implications that fit together with these new developments.",
            "Question is, what happens if we have hundreds of nodes, so I should say we focused on the case of two nodes only because that was still an open problem.",
            "Case we have you have many notes is very expensive with the traditional methods.",
            "If you have hundreds, I think it's basically not doable if we don't make additional assumptions, we have briefly mentioned we have this one generalization where we use assumptions about the functional, the model number class to also tackle the invariable case, and I think that can also be very interesting for larger graphs.",
            "It doesn't require the faithfulness assumption.",
            "It's a little bit difficult to tell.",
            "I think it might actually be also possible to do it more efficiently than the standard methods, because it doesn't rely on this conditional independence testing, which immediately gives you this kind of combinatorial problem so.",
            "We haven't done it yet, but I think that's an interesting direction here.",
            "Yeah, so the question is, can we also handle binary data?",
            "Yes, I think so.",
            "So we have.",
            "We have generalized some of this or we or specialized some of this through the case of categorical or binary variables, which is often important in causal inference.",
            "So I think the answer is yes, but so we have only.",
            "I've only had a finite amount of time and also we haven't looked at all these problems yet, but I think someone can do that.",
            "So the question is, it's interesting because I was tempted to ask this question to the audience before Christmas.",
            "What is easier cause learning or anti causal learning?",
            "I don't know.",
            "I think it depends on the situation.",
            "So for simply supervised learning, anti causal learning is easier for.",
            "Robust learning or comparative store extrapolation into completely different domains.",
            "Causal learning seems to be easier, but.",
            "I think there's still a lot of space to think about these problems.",
            "So that's a deep question.",
            "How so is cause and effect?",
            "Something in the real world?",
            "Or is it just in our brains?",
            "I don't know, I I. I think when we talk about cause and effect, of course.",
            "It's something in our brains.",
            "So I'd like to start with the definition of course structures in terms of functional models, because that's just a mathematical object which for me correlates what with what I have in my brain when I talk about cause and effect, but I don't know.",
            "OK, so let's think about over his great talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So this is somewhat intimidating room and I had time to reflect on that during the two or three minutes that it took me to.",
                    "label": 0
                },
                {
                    "sent": "Well from the top to the bottom, but nevertheless I'm very happy to be here and to speak at this first European news conference.",
                    "label": 0
                },
                {
                    "sent": "I would like to thank the organizers, not just for inviting me, but also or especially for the additional work they had by running the conference in Europe in this new place in which is a wonderful place, I think.",
                    "label": 0
                },
                {
                    "sent": "So I will talk about mainly about causal inference, which is a contentious topic, and if you know something about it, it's quite possible that we will find my choice or my view of things.",
                    "label": 0
                },
                {
                    "sent": "My treatment of the topics are a little irritating, so that's the disclaimer at the start.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in inference from data in a broad sense, so not just in statistical inference, and I think causal inference is something that's also not just statistical.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we all know there's a difference between dependence and causation, and so whether there's a strong correlation, for instance between the occurrence of stocks in the birth rate in different European countries, we wouldn't try to increase the birth rate by increasing the number of storks.",
                    "label": 0
                },
                {
                    "sent": "That would obviously constitute a form of a cargo cult.",
                    "label": 0
                },
                {
                    "sent": "Now this came up in the discussion with never the other day, and I Googled for cargo cover cargo cultism is world for this.",
                    "label": 0
                },
                {
                    "sent": "It was coined by Feynman for religious practices in the Pacific tribes around World War Two, trying to obtain wealth cargo by building mock landing strips or waving flags and things like that.",
                    "label": 0
                },
                {
                    "sent": "So I googled it for cargo cult cult in causality in that led me to another quote, which is a little scary, so I'll read it to you.",
                    "label": 0
                },
                {
                    "sent": "The term cargo cult is also idiomatically used in the words of Wikipedia to mean any group of people who imitate the superficial exterior of a process or system without having any understanding of the underlying substance.",
                    "label": 1
                },
                {
                    "sent": "So question is the statistical machine learning a cargo cult.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Translation buffer, philosopher and physicist argued that statistics in causality are not the same, but they are linked by what he called the common cost principle.",
                    "label": 0
                },
                {
                    "sent": "So the common cause principle says if you observe two statistically dependent quantities, then there must be another quantity which causally influences both.",
                    "label": 0
                },
                {
                    "sent": "So there's no statistical dependence without causality, and this is the picture of the common cause.",
                    "label": 0
                },
                {
                    "sent": "Of course, as a special case this common cause could coincide with X or Y.",
                    "label": 0
                },
                {
                    "sent": "In which case we would get one of these two pictures.",
                    "label": 0
                },
                {
                    "sent": "Moreover, Reichenbach said or postulated that this third variable zed should screen X&Y from each other in the sense that conditioned on zed, X&Y should become statistically independent, so this is a link between statistics and causality which I think was quite profound and it's effectively still at the heart of most causal inference methods.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, personally I prefer to view something else as the most logical starting point for thinking of causal methods and its functional causal model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Non functional causal models.",
                    "label": 0
                },
                {
                    "sent": "We have a set of observables X one through XN.",
                    "label": 0
                },
                {
                    "sent": "We think of these observables as vertices on a DAG.",
                    "label": 0
                },
                {
                    "sent": "This semantics of the graph is that parents in the graph correspond to direct causes, and at each vertex we have a function FI which takes as an input all the parents of that vertex and a noise variable, and so we have noise variable per vertex, and these noise variables are assumed to be jointly independent.",
                    "label": 0
                },
                {
                    "sent": "Which one can also justify by the light in both principle.",
                    "label": 0
                },
                {
                    "sent": "So if if they were not independent, they would have another common cause and then the graph wouldn't be complete.",
                    "label": 0
                },
                {
                    "sent": "So we can think of this as a causal sufficiency.",
                    "label": 0
                },
                {
                    "sent": "Now we can show that this model.",
                    "label": 0
                },
                {
                    "sent": "So obviously if these noises have some distributions then this will induce a joint distribution of the vertices XI and this joint distribution can be shown to satisfy the Titan back principle.",
                    "label": 0
                },
                {
                    "sent": "And we can ask some questions about this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "One of those when we what kind of properties does it have in the second one?",
                    "label": 0
                },
                {
                    "sent": "Is it possible, given complete knowledge of this joint distribution, to recover the graph?",
                    "label": 0
                },
                {
                    "sent": "So is it possible to recover the structure of what is the direct cause of what?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So regarding the first question, what can we say about this distribution?",
                    "label": 0
                },
                {
                    "sent": "There's a number of beautiful results this has been studied in great detail by the causality community, and it turns out that the existence of such a functional causal model is equivalent to the so called local causal Markov condition, which people probably know from graphical models only.",
                    "label": 0
                },
                {
                    "sent": "Here it has a causal semantics at the local cause.",
                    "label": 0
                },
                {
                    "sent": "Markov condition says that variable X here should be statistically independent of its.",
                    "label": 0
                },
                {
                    "sent": "Non decendants when conditioned on its parents on the red ones.",
                    "label": 0
                },
                {
                    "sent": "So in other words every information exchange between a variable and its non descendants has to involve its parents.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this local conditions with this gives us tells us certain local conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "They directly imply a set of global conditional independence properties, which can be characterized in terms of the separation.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into detail on that.",
                    "label": 0
                },
                {
                    "sent": "Moreover, all these things are also equivalent to a certain factorization of the joint distribution in terms of conditional probabilities, sometimes called Markov kernels of variables, given their parents.",
                    "label": 0
                },
                {
                    "sent": "So this conditional is market kernels can be thought of as the causal mechanisms that actually generate statistical dependence.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Regarding the second question.",
                    "label": 0
                },
                {
                    "sent": "So given the joint distribution complete knowledge which is hypothetical, are we able to infer the graph structure?",
                    "label": 0
                },
                {
                    "sent": "The answer to this question is it's impossible if we don't have additional information.",
                    "label": 0
                },
                {
                    "sent": "It's possible if we have interventions or randomized control trials.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, if we assume something called faithfulness, which is an assumption that looks somewhat innocuous, but it isn't that innocuous.",
                    "label": 0
                },
                {
                    "sent": "And also if we have certain assumptions that may conditionally independent testing feasible, or if we have a good good enough conditionally independent testing method, then we can estimate a so called Markov equivalence class of graphs that contains the correct graph by running conditional independence test.",
                    "label": 1
                },
                {
                    "sent": "So remember the Markov assumption was saying that certain conditional independence properties hold, so given the distribution we can in principle test all possible conditional independence statements.",
                    "label": 0
                },
                {
                    "sent": "This constrains which graphs.",
                    "label": 0
                },
                {
                    "sent": "Fit together with it and we typically end up with this equivalence class, which usually contains more than one graph, so usually we don't get a single graph, and in particular in the very simplest case where we have graphs with just two nodes, so just two nodes, one could be caused, one could be effect, or vice versa.",
                    "label": 0
                },
                {
                    "sent": "This is going to be useless because of course every conditional independence statement already involves three variables, so there's no nontrivial conditionally independent statements that we can make for two variables.",
                    "label": 0
                },
                {
                    "sent": "So, so we're stuck.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 1
                },
                {
                    "sent": "What kind of assumptions could we make in order to make the two variable case solvable?",
                    "label": 0
                },
                {
                    "sent": "So that's the main problem that we looked at over the last years, and I'm going to tell you a little bit about the kind of methods that we came up with, sorted in order of increasing strangeness.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe the most plausible one which is to restrict the functional model.",
                    "label": 1
                },
                {
                    "sent": "So remember our functional model that I showed you before the general one said that each variable is a function of its parents and annoys variable, and these noises were joined to independent.",
                    "label": 0
                },
                {
                    "sent": "Now let's assume a particular relatively simple noise that just takes discrete values.",
                    "label": 0
                },
                {
                    "sent": "So suppose that noise can only take 10 different values.",
                    "label": 0
                },
                {
                    "sent": "In that case, we could still imagine that this noise, which is random switches, are functions between different mechanisms, so in different functions, not just of the parents.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's additional restriction that the noise has to be discrete.",
                    "label": 0
                },
                {
                    "sent": "The noise could also be a continuous variable, in which case in principle we could switch between a non countable number of different dependencies.",
                    "label": 0
                },
                {
                    "sent": "With such a mechanism.",
                    "label": 0
                },
                {
                    "sent": "So this is a very general model and I think you will follow me that intuitively it seems very difficult to identify this kind of model if we don't make additional restrictions.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's even surprising.",
                    "label": 0
                },
                {
                    "sent": "Of course these models still implies certain conditional independence properties, the Markov properties.",
                    "label": 0
                },
                {
                    "sent": "So it has some implications and sometimes it allows us to identify something which is sort of surprising, but I think as a machine learner you will.",
                    "label": 0
                },
                {
                    "sent": "You will say we all know there is no free lunch.",
                    "label": 0
                },
                {
                    "sent": "We have to make assumptions about the function classes.",
                    "label": 0
                },
                {
                    "sent": "So let's make an assumption and let's make an assumption that will somehow limit the complexity with which the noise can act on our functions.",
                    "label": 0
                },
                {
                    "sent": "And this assumption in my case is an additivity assumption.",
                    "label": 0
                },
                {
                    "sent": "So I'll assume the noise acts only additively.",
                    "label": 0
                },
                {
                    "sent": "You could say this is a little bit like saying we we.",
                    "label": 0
                },
                {
                    "sent": "Assume the noise is small that the function is relatively smooth, and maybe the inputs are concentrated.",
                    "label": 0
                },
                {
                    "sent": "But in any case it's an assumption it's strong assumption.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out it's strong enough to make the two variable case solvable.",
                    "label": 0
                },
                {
                    "sent": "So let's try to get an intuition for that.",
                    "label": 0
                },
                {
                    "sent": "So two variable case means we have a model.",
                    "label": 0
                },
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "So why is a function of X plus noise?",
                    "label": 0
                },
                {
                    "sent": "And this noise is independent of the input X and we cannot ask the question.",
                    "label": 0
                },
                {
                    "sent": "Will it be possible to write an inverse model of the same form?",
                    "label": 0
                },
                {
                    "sent": "So if we assume additivity?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Does it become?",
                    "label": 0
                },
                {
                    "sent": "Is it still symmetric problem that we can view either way or does it become a symmetric and it turns out it becomes a symmetric and to see this, just imagine that the noise is like this so it is an additive noise of bounded range Sigma.",
                    "label": 0
                },
                {
                    "sent": "So if we now have our function F of X we add this noise.",
                    "label": 0
                },
                {
                    "sent": "We get this kind of tube around this regression function or this function connecting X&Y and of course Now if we look at the backward direction you will see that if this function is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Then the width of this tube in the backward direction will vary with why.",
                    "label": 0
                },
                {
                    "sent": "In other words, the noise will no longer be independent of why it would be a heteroscedastic noise.",
                    "label": 0
                },
                {
                    "sent": "So we had this intuition relatively fast, but it still took us half a year to prove a theorem about it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the two variable case, and I'm not going to go through all the details here, I'll just tell you the main message of this theorem.",
                    "label": 0
                },
                {
                    "sent": "So the theorem says that.",
                    "label": 0
                },
                {
                    "sent": "If we assume there's a density and the joint density has this form, so it's a product of the noise density.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the noise density here for an additive noise, so it's just the density for the difference between Y and FX, and the input density here.",
                    "label": 0
                },
                {
                    "sent": "M. If we assume this model is true, and if we assume, Moreover, that in the backward direction we have a model of the same form, then we can derive a complicated differential equation which connects the logarithm of the noise density, the logarithm of the input density, and the function.",
                    "label": 0
                },
                {
                    "sent": "F in this complicated way, and when can show that.",
                    "label": 0
                },
                {
                    "sent": "Under some assumptions, this differential equation is only a 3 dimensional solution space out of a space of possibly infinitely many functions, which means that only if the noises and the nonlinearity the function are somehow tuned to each other in a very special way will this differential equation be satisfied.",
                    "label": 0
                },
                {
                    "sent": "So in the general generic cases will be violated.",
                    "label": 0
                },
                {
                    "sent": "Therefore we won't be able to find a backward model of the same form.",
                    "label": 0
                },
                {
                    "sent": "One case where this differential equation is satisfied is of course the case of linear functions in Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So this was very already known that this case is not identifiable.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can try to use this to suggest a causal inference method, and I think it is obvious how we're going to do it.",
                    "label": 1
                },
                {
                    "sent": "We will compute a regression of X&Y.",
                    "label": 0
                },
                {
                    "sent": "We compute the residual error term and then we check whether this error and the input are statistically independent and we're going to try this in both directions X to Y or Y to X and see in which direction we have independence or we have at least more independence in the other direction or less dependence.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can then try to apply this.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "This is one data set where we have the altitude and the average temperature of different places in Germany.",
                    "label": 1
                },
                {
                    "sent": "And it's clear that the temperature we can review with the temperature as the cause of an added to the altitude of a city, but vice versa.",
                    "label": 0
                },
                {
                    "sent": "The altitude certainly will have a causal inference on the temperature.",
                    "label": 0
                },
                {
                    "sent": "So if we do it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do it in both directions, and we look at the residual noise terms.",
                    "label": 0
                },
                {
                    "sent": "We we find relatively independent noise here.",
                    "label": 0
                },
                {
                    "sent": "The correct direction and highly dependent noise dependent on the input in the wrong direction.",
                    "label": 0
                },
                {
                    "sent": "So so that's good, and I'll show you more results later.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should also mention that this work has been generalized to models that are not just with additive noise, but have a post nonlinearity applied to the sum of F of X and noise.",
                    "label": 0
                },
                {
                    "sent": "By signing into varied and it has been recently generalized by our group to graphs with more than two vertices.",
                    "label": 0
                },
                {
                    "sent": "So in that case basically the story is as long as we have a function class for which the two variable cases identifiable.",
                    "label": 0
                },
                {
                    "sent": "We can prove identifiability in the invariable case and under an assumption which is called causal minimality, which it turns out is a little little bit weaker than this assumption of faithfulness that I mentioned before, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "And we started working.",
                    "label": 0
                },
                {
                    "sent": "Or especially you always my who has a poster here at NIPS on the case where we have looks so no longer directed acyclic graphs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the regression.",
                    "label": 0
                },
                {
                    "sent": "We don't method I told you is is somehow a little bit broken for this problem or a bit misplaced in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Usually regression methods assume something about noise distributions, and if these assumptions are not correct, we might get a solution where the residuals are dependent even though in reality they shouldn't be.",
                    "label": 0
                },
                {
                    "sent": "So we can address this by trying to minimize the resident.",
                    "label": 0
                },
                {
                    "sent": "The dependence of the residuals rather than some error metric, or rather than maximizing the likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "And we can do this by taking some dependence measure should be at Amanda's measure, not just a correlation measure.",
                    "label": 0
                },
                {
                    "sent": "And since we come from the kernel world, we use the kernel measure.",
                    "label": 0
                },
                {
                    "sent": "There is a distance between certain kernel mean embeddings and I'll tell you a little bit about this.",
                    "label": 0
                },
                {
                    "sent": "On the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this builds on these methods of kernel independence testing, and these methods can be derived by viewing, looking at the kernel mean map.",
                    "label": 1
                },
                {
                    "sent": "So what is the kernel mean map?",
                    "label": 0
                },
                {
                    "sent": "Do we have some probability measure P and we map it into?",
                    "label": 0
                },
                {
                    "sent": "We take the feature map, so this is one way to write the feature, map the map into the feature space for support vector machine.",
                    "label": 0
                },
                {
                    "sent": "We take this map and take its expectation with respect to this probability measure.",
                    "label": 0
                },
                {
                    "sent": "So this gives us an element of the reproducing kernel Hilbert space, and it turns out that this element or this this mapping is invertible or injective under certain conditions on the kernel, for instance universality.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the Gaussian kernel turns out will have an injective kernel mean map, which means that this we don't lose any information if we represent a probability measure by this quantity here.",
                    "label": 0
                },
                {
                    "sent": "Which in turn implies that if we wanted to do independent testing, what we can do is we can map the joint joint distribution into our reputation, Kerbal Space.",
                    "label": 0
                },
                {
                    "sent": "We can also map the product of two marginal distributions and these two quantities of course will be, well.",
                    "label": 0
                },
                {
                    "sent": "These two distributions will be the same if and only if the variables are independent.",
                    "label": 0
                },
                {
                    "sent": "And if we are injective then also this quantity and this quantity will be the same if and only if we have independence.",
                    "label": 0
                },
                {
                    "sent": "So we can take this length of this difference vector as a test statistic or an estimate of this as a test statistics for independence, and we can use this kind of quantity as a criterion in our regression that I showed you in the last slide, so I don't have time to talk about this much more.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should just mention that this is actually a generalization of what statisticians call the moment generating function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, since we're already in the world of kernels, I wanted to say I think this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a nice example of the new generation of kernel methods, which is quite different from from the past.",
                    "label": 0
                },
                {
                    "sent": "At the beginning of the kernel methods we were thinking the whole field will just be about kernel Ising multiple algorithms coming up with nonlinear versions of algorithms and just to relieve this past for a minute.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A slide from my first NIPS talk, which was in 1998 and I wanted to point out that this algorithm still hasn't been developed yet.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, back to causality.",
                    "label": 0
                },
                {
                    "sent": "So we have this additive noise assumption, which is a strong assumption, but it allows us to do certain things and we also looked at this this problem of detecting confounders, which is a notorious problem in causality, because usually if you get 2 variables it's not the case that one of them is a course of the other one.",
                    "label": 0
                },
                {
                    "sent": "Usually there's something something else that comes into play, and so ideally we would like a method that given observations of X&Y are given the joint distribution can decide whether either one of.",
                    "label": 0
                },
                {
                    "sent": "Is a reason of the other one, or there's a confounder that we haven't observed, so we make an additive noise assumption?",
                    "label": 0
                },
                {
                    "sent": "Also in this case, so we assume X is a function of the confounder plus noise, same for Y, and these noises in the confounder should be jointly independent, so there's the corresponding assumption and you can think of this as some kind of dimensionality reduction problem, so we try to find the manifold parameterized by T, which somehow explains X&Y.",
                    "label": 0
                },
                {
                    "sent": "Up to independent noises so it's almost like a dimensionality reduction only.",
                    "label": 0
                },
                {
                    "sent": "Again, the criterion is a little different is not minimizing sum squared error or something like that, but minimizing dependence of the residuals.",
                    "label": 0
                },
                {
                    "sent": "And we can do this.",
                    "label": 0
                },
                {
                    "sent": "I should say we don't really have good algorithms for this yet, but we can do the toy examples and we have an identifiability result for small noise.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So everything I told you about so far was using properties of the noise to do causal inference so it it would seem that maybe this is the crucial thing we have to look at how the noise behaves, But it turns out they are further as symmetries that we can use for causal inference.",
                    "label": 0
                },
                {
                    "sent": "In I'll show you one such as symmetry.",
                    "label": 0
                },
                {
                    "sent": "And to show you that it doesn't build a noise, we apply it or we study in the case of deterministic relationships.",
                    "label": 0
                },
                {
                    "sent": "So here we have Y = F of X with an invertible function of X.",
                    "label": 0
                },
                {
                    "sent": "So we can write it either way.",
                    "label": 0
                },
                {
                    "sent": "It's completely symmetric, there's no noise.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, we still want to decide his extra cause of why or his way the course of X.",
                    "label": 0
                },
                {
                    "sent": "So if you want, you could think about this for a second.",
                    "label": 0
                },
                {
                    "sent": "That seems like a completely strange problem.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't make sense to study this at all.",
                    "label": 0
                },
                {
                    "sent": "So here our assumption or idea that we use is that if X causes Y, then it seems to be reasonable to assume that this function so X is the cause.",
                    "label": 0
                },
                {
                    "sent": "Why is the effect this function and the density over X?",
                    "label": 0
                },
                {
                    "sent": "So the thing that we feed into the function should be independent in some sense.",
                    "label": 0
                },
                {
                    "sent": "They don't know if you travel, the mechanism doesn't know what we feed into the mechanism, so of course this could be wrong if we have feedback loops and things like that.",
                    "label": 0
                },
                {
                    "sent": "But we're considering simple case, so let's assume that this independence holds.",
                    "label": 0
                },
                {
                    "sent": "Then it should be the case that the structure of this density in particular doesn't correlate with the structure of this function.",
                    "label": 0
                },
                {
                    "sent": "However, in the backward direction, whenever this this function here was flat, we suddenly have a lot of mass in the output distribution of the output density.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the output density should have large mass wherever this function is flared, or wherever the inverse function is very steep.",
                    "label": 0
                },
                {
                    "sent": "So the peaks of the output distribution should correlate with the slopes.",
                    "label": 0
                },
                {
                    "sent": "The slope of the inverse function.",
                    "label": 0
                },
                {
                    "sent": "So let's try to formalize this a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see where it takes us.",
                    "label": 0
                },
                {
                    "sent": "So we assume F for simplicity's objection of 01 we're going to consider the slope of F or actually, will you actually, the logarithm of the slope of F as one random variable and as the second random variable, we will consider the value of the density, so that's a little strange.",
                    "label": 0
                },
                {
                    "sent": "But of course we can do that.",
                    "label": 0
                },
                {
                    "sent": "We consider it as a random variable on this probability space with lebec measure, and then we can write down the covariance of these two random variables.",
                    "label": 0
                },
                {
                    "sent": "And we will stipulate that in particular, this covariance should be 0.",
                    "label": 0
                },
                {
                    "sent": "Suddenly these two objects are independent.",
                    "label": 0
                },
                {
                    "sent": "Their covariance should be 0.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that under this assumption, we can prove that in the backward direction, the covariance first of all is lower bounded by zero, and we have equality if and only if the identity mapping.",
                    "label": 0
                },
                {
                    "sent": "So in the non trivial case it will be the case that in the correct in the forward direction covariance is zero in the backward direction these covariance will be strictly positive.",
                    "label": 0
                },
                {
                    "sent": "It is clear that this kind of symmetry allows us to operate on the estimator that we can use in practice for inference.",
                    "label": 0
                },
                {
                    "sent": "But I want to 1st tell you a little bit more about.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of postulate.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Postulate in various ways and and maybe I don't have time to go through all the details, but I should say we can write it as an activity condition for entropies.",
                    "label": 0
                },
                {
                    "sent": "We can write it as an information, geometric orthogonality in information space and also here.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll say a little bit about this.",
                    "label": 0
                },
                {
                    "sent": "We can write it as this condition, which says that here we have the relative entropy between the output distribution and UY, which is a uniform density on the output.",
                    "label": 0
                },
                {
                    "sent": "So that you can think of as the area irregularity of the observed output, the observed distribution over Y.",
                    "label": 0
                },
                {
                    "sent": "And so this is now equal to the relative entropy between PX and UX.",
                    "label": 0
                },
                {
                    "sent": "So the distance between the input distribution observed and a uniform input distribution, and plus you're the third term, which is the distance between VY, which is the density on why which would be induced if we feed.",
                    "label": 0
                },
                {
                    "sent": "If we fed a uniform distribution over X into the function.",
                    "label": 0
                },
                {
                    "sent": "So this is somehow the effect that the function has.",
                    "label": 0
                },
                {
                    "sent": "On a uniform distribution, and here we measure the distance between that and the uniform output.",
                    "label": 0
                },
                {
                    "sent": "So this is only the irregularity induced by the output.",
                    "label": 0
                },
                {
                    "sent": "So we have this kind of interpretation, which I think is quite nice, and now we have several methods and let's.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get some datasets so we have collected a number of cause effect datasets where we think the ground truth is reasonably clear is not always easy, and if you have more data sets, please send them to us that these datasets are on the web.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can look at them.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's some examples of datasets, altitude, temperature I mentioned before, but there's also other things like age of a person and their wage.",
                    "label": 0
                },
                {
                    "sent": "It's clear that the wage is not a cause of the age.",
                    "label": 0
                },
                {
                    "sent": "Vice versa.",
                    "label": 0
                },
                {
                    "sent": "There's certainly some costly effect, so we mark these different datasets with the ground truth that we think are reasonable and then.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different algorithms?",
                    "label": 0
                },
                {
                    "sent": "And here's the result.",
                    "label": 0
                },
                {
                    "sent": "So what I show is the fraction of correct decisions for different algorithms, distinguishing cause and effects.",
                    "label": 0
                },
                {
                    "sent": "And of course the chance level would be 50%.",
                    "label": 0
                },
                {
                    "sent": "And on the X axis we have the decision rate.",
                    "label": 0
                },
                {
                    "sent": "By this I mean all these algorithms have some some continuous quantity that we used to decide and we could threshold it and allow the algorithms not to decide in cases where there's no no clear difference.",
                    "label": 0
                },
                {
                    "sent": "So if we decide then if we force him to decide always then we can see the performances go closer to chance level, closer to 50%.",
                    "label": 0
                },
                {
                    "sent": "If we allow them to not take decisions in half of the cases we get higher performances.",
                    "label": 0
                },
                {
                    "sent": "The Gray area is a significant region or insignificance.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you did random guessing, then in 95% of the cases you should end up in this Gray area.",
                    "label": 0
                },
                {
                    "sent": "So let's look where are we outside of this Gray area?",
                    "label": 0
                },
                {
                    "sent": "And you can see different methods.",
                    "label": 0
                },
                {
                    "sent": "There's just the deterministic method that I just told you about.",
                    "label": 1
                },
                {
                    "sent": "The Red One is our additive noise methods that I mentioned before this took was one is additive noise with post nonlinearity.",
                    "label": 1
                },
                {
                    "sent": "This one here is a Gaussian process based method that we had a nips last year in the green one is the previous method, which is a sense for linear non Gaussian model linear, non Gaussian acyclic model.",
                    "label": 0
                },
                {
                    "sent": "It's basically assuming linear relationships in non Gaussian noise, so I don't want to interpret the differences in great detail, but I think the main message here is that we can solve this problem better than chance and with performance better than chance, and I think that's already quite encouraging because this problem used to be considered insolvable.",
                    "label": 0
                },
                {
                    "sent": "As far as I know.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What's the point of estimating cause and effects?",
                    "label": 0
                },
                {
                    "sent": "So let me try to connect things a little bit more to machine learning and mode.",
                    "label": 1
                },
                {
                    "sent": "Maybe motivate why why we should be interested in this kind of causal inference problem.",
                    "label": 0
                },
                {
                    "sent": "To do this, I want to discuss 2 machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "The first one is a problem from bioinformatics, so suppose we have here the Messenger RNA sequence and you'd like to predict from this sequence something about the protein structure.",
                    "label": 0
                },
                {
                    "sent": "Maybe the sequence of amino acids, or maybe something in between.",
                    "label": 0
                },
                {
                    "sent": "Something simpler but you want to predict something about the protein from this, and of course in reality we have this Messenger RNA.",
                    "label": 0
                },
                {
                    "sent": "There's a mechanism called the ribosome, which travels along this sequence and somehow it translates it into amino acids, builds proteins.",
                    "label": 0
                },
                {
                    "sent": "So in reality, there is a causal mechanism called the ribosome, which is believed to be reasonably invariant across different organisms or different species.",
                    "label": 0
                },
                {
                    "sent": "The input sequence might differ, of course, otherwise we wouldn't look different, but the mechanism is more or less stable.",
                    "label": 0
                },
                {
                    "sent": "It's a causal mechanism that produces a certain output, so if we try to solve this prediction problem from data, we are learning a prediction in the same direction as the causal mechanism, which I have denoted by fire.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Now let's look another provide another problem that we may be more familiar with.",
                    "label": 0
                },
                {
                    "sent": "So that's optical character recognition.",
                    "label": 0
                },
                {
                    "sent": "This is the in this set in this case.",
                    "label": 0
                },
                {
                    "sent": "And maybe this is a trivial note to many of you, but for me when I first notice it was actually quite a revelation.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're actually learning learning in the anti causal direction, so it's not the case that this image is the cause of the label.",
                    "label": 0
                },
                {
                    "sent": "In fact, the way that images are produced is this some persons it's done that decides to write the digit 3 and then produces an image of the digit 3.",
                    "label": 0
                },
                {
                    "sent": "So that causes the label and of course the images the effect.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless we want to learn the mapping from.",
                    "label": 0
                },
                {
                    "sent": "Image to label.",
                    "label": 0
                },
                {
                    "sent": "Which means that we're still learning from X to Y, but now X is the effect.",
                    "label": 0
                },
                {
                    "sent": "And why is the course?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of surprising, and it has some interesting.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Implications.",
                    "label": 0
                },
                {
                    "sent": "And and I want to specifically look at the implications for covariate shift in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So again, we make this assumption that the probability distribution or the density of the course and the density of the mechanism so probability of effect given course in dependently chosen by nature.",
                    "label": 0
                },
                {
                    "sent": "So you could argue with that, but I think it's a reasonable assumption and our goal is always to learn X, learn the mapping from X to Y.",
                    "label": 0
                },
                {
                    "sent": "So we want to estimate some properties of the conditional of my given X.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the formal direction.",
                    "label": 0
                },
                {
                    "sent": "So causal causal direction.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "What happens if we have covariate shift?",
                    "label": 0
                },
                {
                    "sent": "So if the input distribution changes, then of course the mechanism P of Y given X is not affected.",
                    "label": 0
                },
                {
                    "sent": "I think that's sort of clear to everybody working in covariate shift and covariate shift adaptation is either trivial or at least possible.",
                    "label": 1
                },
                {
                    "sent": "There are some ramifications, but if we have a model that's underspecified, maybe we have to rewrite the samples or things like that.",
                    "label": 0
                },
                {
                    "sent": "But in principle it's possible because the mechanism is not affected.",
                    "label": 0
                },
                {
                    "sent": "How about semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "Well, we have made this assumption that probability of course, and the probability of effect given costs are independent.",
                    "label": 0
                },
                {
                    "sent": "So this means in our case, in this case here where fire and the function are aligned that the distribution of X doesn't contain any information about the condition of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So if we are given additional data from P of X and we could maybe get a better estimate of P of X, this shouldn't contain information about the conditional.",
                    "label": 0
                },
                {
                    "sent": "So we should not expect to benefit from this and semi supervised learning should be impossible, or at least very difficult to do.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the other direction.",
                    "label": 0
                },
                {
                    "sent": "So now the causal function, the causal dependencies backwards.",
                    "label": 0
                },
                {
                    "sent": "But we still want to predict in this direction, so that was the MNIST case in this case, covariate shift.",
                    "label": 0
                },
                {
                    "sent": "Is more difficult because we need to decide if a changes due to a change in the mechanism or due to a change in the course distribution.",
                    "label": 0
                },
                {
                    "sent": "That's nontrivial.",
                    "label": 0
                },
                {
                    "sent": "I think we have some ideas how to do it, but it's not trivial.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, semi supervised learning in this case actually should be possible, because now P of X does contain information about POY given X.",
                    "label": 0
                },
                {
                    "sent": "So remember, before in the deterministic case we have results showing that in the forward direction, if we have independence forward direction, we should expect.",
                    "label": 0
                },
                {
                    "sent": "Dependence in the backward direction.",
                    "label": 0
                },
                {
                    "sent": "So here we have the information.",
                    "label": 0
                },
                {
                    "sent": "We have dependence and I think actually, as far as I know, all the semi supervised learning methods are using this in one way or another.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you make the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "It simply means that within areas of high density, high values of this, so within certain areas that are specified by this distribution, the conditional should be constant.",
                    "label": 0
                },
                {
                    "sent": "The label doesn't change within clusters.",
                    "label": 0
                },
                {
                    "sent": "So that's a way of transferring information from here to here, and I think other assumptions like the low density separation or manifold assumptions and so on fit into this picture as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I want to give you some more sophisticated examples of what we can do if we somehow take into account the cost structure specifically.",
                    "label": 0
                },
                {
                    "sent": "If we now assume so.",
                    "label": 0
                },
                {
                    "sent": "I'm still in this picture that the noise X is additive, so there's again our favorite assumption.",
                    "label": 0
                },
                {
                    "sent": "Then of course, the distribution that we observe for this quantity will have the form of a convolution.",
                    "label": 0
                },
                {
                    "sent": "It will be a convolution of this noise density and the density of Y transformed by fire.",
                    "label": 0
                },
                {
                    "sent": "And of course, then the learning the mapping from X to Y would require deconvolution.",
                    "label": 0
                },
                {
                    "sent": "Fact deconvolution in signal processing is, I think, a beautiful example of a prediction problem where one can do a lot better by taking into account the causal direction, namely by estimating effectively this mapping fire and inverting.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have an example where we have to do the convolution.",
                    "label": 0
                },
                {
                    "sent": "We built a lens with just a single piece of glass.",
                    "label": 0
                },
                {
                    "sent": "This is a really crappy lens, but it's still a causal mechanism that transforms some image are there in the world to a measurement.",
                    "label": 0
                },
                {
                    "sent": "So of course into an effect.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is we can measure the point spread function of this lens in various places, which means we measure XY examples for a very simple distribution distribution where the the causes are just.",
                    "label": 0
                },
                {
                    "sent": "Points and the effects are these images of points you could see.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we want to be given another X which is much more much more complex.",
                    "label": 0
                },
                {
                    "sent": "For instance from the distribution of blood real world images.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to reconstruct the corresponding why so we want to make an anti causal prediction.",
                    "label": 0
                },
                {
                    "sent": "Now we can try to do this by training a learning machine on XY pairs, for instance from the class of images like that and I have to admit that I naively tried to do this when I first got into the field of deconvolution and about five years ago and very much failed.",
                    "label": 0
                },
                {
                    "sent": "So this is not going to work this problem we cannot solve by it by not taking into Honda causal structure.",
                    "label": 0
                },
                {
                    "sent": "But if you do it correctly so if you view it as a decoy.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pollution problem then you can get.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very nice results, so this was the result obtained by Christine Schuler.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "PhD student, so for these really crappy lens where the image is quite bad in the corners, you can get a reconstruction like that and you can do similar things also in the cases where you don't know the convolution, so this is it's a non blind case.",
                    "label": 0
                },
                {
                    "sent": "We have measured the point spread function.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He's a blind case where we have camera motion, so we have some objects out there in the world.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We keep the shutter open, move the camera.",
                    "label": 0
                },
                {
                    "sent": "We have some trajectory in this prioritization of the camera position and we can.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take this into account and then for instance take an input image.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structed, like that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we take pictures through turbulence, this has a similar but more complex effect than camera motion, and we can also look at this kind of problems and then reconstruct so sort of deconvolution problem for this kind of images and we could get a reconstruction which is actually very close to the ground truth in this case.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And very briefly, just want to mention that the question when did convolution is possible is actually closely related to kernels, so I'll use this in another excuse for a very short digression and for a shift invariant kernel, the kernel mean map that I told you about before.",
                    "label": 0
                },
                {
                    "sent": "So this is a shift invariant cuddle.",
                    "label": 1
                },
                {
                    "sent": "In this case this is nothing but a convolution and turns out if the kernel is the inverse Fourier transform of an aperture.",
                    "label": 0
                },
                {
                    "sent": "I convolved with itself.",
                    "label": 0
                },
                {
                    "sent": "So this is the aperture convolved with itself.",
                    "label": 0
                },
                {
                    "sent": "That's the kernel.",
                    "label": 0
                },
                {
                    "sent": "Free transform the kernel.",
                    "label": 0
                },
                {
                    "sent": "So if we take the inverse of that then we can still give a simple physical realization of the kernel mean map in terms of fan hopeful diffraction.",
                    "label": 0
                },
                {
                    "sent": "And we can then ask the question.",
                    "label": 0
                },
                {
                    "sent": "When is this process invertible or when is found her for imaging invertible and it turns out first of all, if the aperture is finitely large and we have no other assumption, it's not possible, and there's well known in optics that CRB resolution limit.",
                    "label": 0
                },
                {
                    "sent": "But if we restrict the class of input distributions here for instance to distributions, so even images that have compact support and then this process becomes invertible even if this is just a very small aperture, as long as it's larger than zero this problem.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is invertible.",
                    "label": 0
                },
                {
                    "sent": "So the last two minutes very briefly.",
                    "label": 0
                },
                {
                    "sent": "And for a very different kind of approach to causal inference so far, have used an assumption that the common cause principle that all the causal Markov assumption to connect causality in statistics and it's not clear that statistics should be the only domain where causality leaves a footprint thing.",
                    "label": 0
                },
                {
                    "sent": "In real life we often draw causal conclusions from individual objects.",
                    "label": 1
                },
                {
                    "sent": "So here we have some individual objects and if you look at them you will probably be quite willing to say that there's some kind of causal link in the form of a common history.",
                    "label": 0
                },
                {
                    "sent": "So these things have been produced or designed by the same factory.",
                    "label": 0
                },
                {
                    "sent": "So you probably wouldn't say that these two T shirts have been designed by the same factory, so so these two objects share a certain amount of complexity, which makes us willing to do this cause the inference that these two objects don't share.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's try to formalize this a little bit.",
                    "label": 0
                },
                {
                    "sent": "So remember the causal Markov condition local, the local form, which said that an observable is statistically independent of its non descendants given the parents.",
                    "label": 1
                },
                {
                    "sent": "So we can reformulate this more intuitively by saying given all direct causes of an observable, it's non effects give us no additional statistical information on it, and the stress here is in statistical.",
                    "label": 0
                },
                {
                    "sent": "So it's a link between causality in statistics.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What if we delete this world statistics?",
                    "label": 0
                },
                {
                    "sent": "So then we have a more general principle which we could instantiate in different domains with measures of information that we consider useful, and one such measure could be a combo of complexity or algorithmic complexity.",
                    "label": 0
                },
                {
                    "sent": "So this gives us an algorithmic condition, so we just fill in the word algorithmic here.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let's see where this takes us.",
                    "label": 0
                },
                {
                    "sent": "So I think you all have heard about Kolmogorov complexity of a binary string.",
                    "label": 1
                },
                {
                    "sent": "Is the length of the shortest program with output X on a Turing machine.",
                    "label": 0
                },
                {
                    "sent": "And it's not computable.",
                    "label": 0
                },
                {
                    "sent": "We don't worry about this right now.",
                    "label": 0
                },
                {
                    "sent": "We cannot solve all problems in the world, but I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a beautiful.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concept and it has been generalized in various ways.",
                    "label": 0
                },
                {
                    "sent": "So when can define conditional of complexities.",
                    "label": 0
                },
                {
                    "sent": "Shortest program that generates WHI from the shortest description of X.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "One can define algorithmic mutual information, so information of X about Y and vice versa in terms of Kolmogorov complexity.",
                    "label": 1
                },
                {
                    "sent": "It's the number of bits that we save when compressing two objects jointly rather than independently.",
                    "label": 0
                },
                {
                    "sent": "We can then use this mutual information to define a notion of algorithmic independence.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All this has been done, so this is not our work.",
                    "label": 0
                },
                {
                    "sent": "One can define conditional algorithmic mutual information.",
                    "label": 1
                },
                {
                    "sent": "I think you can guess where I'm getting.",
                    "label": 0
                },
                {
                    "sent": "So we can define conditional algorithmic independence.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can then write down all possible.",
                    "label": 0
                },
                {
                    "sent": "It's the local algorithmic market conditions, so we have a set of observations formalized as strings, and then given its direct causes, every variable.",
                    "label": 1
                },
                {
                    "sent": "Every object is conditionally, algorithmically independent of its not effects, so let's just.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stipulate this.",
                    "label": 0
                },
                {
                    "sent": "First of all, just like in the statistical case, this can be written in three equivalent forms, so we can prove that our local condition is equivalent to a global market condition in terms of the separation, and it's also equivalent to a certain decomposition.",
                    "label": 0
                },
                {
                    "sent": "Now is now it's no longer effective decomposition or multiplicative decomposition of the joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a additive decomposition of the joint kernel of complexity.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, it was very nice is we can define a certain algorithmic model of causality which we can show to imply the algorithmic Markov condition.",
                    "label": 1
                },
                {
                    "sent": "So this algorithmic model says that we have a graph structure.",
                    "label": 0
                },
                {
                    "sent": "Each node computes a program of all his parents.",
                    "label": 0
                },
                {
                    "sent": "The program in this case is basically identical with the noise in the sense of Kolmogorov complexity.",
                    "label": 0
                },
                {
                    "sent": "There's no noise, noise is just high complexity, so this just means we have a complex program.",
                    "label": 0
                },
                {
                    "sent": "Here we assume all these programs are jointly independent and their programs are the causal mechanisms, and this model then implies the algorithmic market conditions.",
                    "label": 0
                },
                {
                    "sent": "So if you think this is a good model of a certain type of causal structure in terms of programs, then you have to believe that this has certain.",
                    "label": 0
                },
                {
                    "sent": "Implications in terms of algorithmic independence that we can in principle maybe not for global complexity but for something else.",
                    "label": 0
                },
                {
                    "sent": "Check on data.",
                    "label": 0
                },
                {
                    "sent": "So this brings me.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the end, I should briefly mention we have tried to make this a little bit more practical in different ways.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And with that I would like to acknowledge my Co workers in the field of causality, field of image deconvolution and in this field of kernel means which we've been working on already for a longer time.",
                    "label": 1
                },
                {
                    "sent": "And thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "OK, I think the question is isn't it.",
                    "label": 0
                },
                {
                    "sent": "Shouldn't it be possible to increase the birth rate by increasing the number of storks in some complicated race?",
                    "label": 0
                },
                {
                    "sent": "So I agree with this, so I think it's imaginable that maybe that's what you're suggesting.",
                    "label": 0
                },
                {
                    "sent": "If we have more socks.",
                    "label": 0
                },
                {
                    "sent": "People like to live there and think this is a nice place, and young families will settle there and produce more babies, so this was a very simplistic model with the storks is probably wrong, probably everything that I've said is wrong in some sense.",
                    "label": 1
                },
                {
                    "sent": "If you look closely enough.",
                    "label": 0
                },
                {
                    "sent": "So he's saying it optical image and there are no methods for doing super resolution and it's contradiction.",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't think it's a contradiction, and I even think.",
                    "label": 0
                },
                {
                    "sent": "I mean, OK, I'm saying that if you don't have any assumptions then this process is not invertible, which was the received wisdom for a long time.",
                    "label": 0
                },
                {
                    "sent": "But I think what I'm saying is that even going a bit further than this method, it just it says we don't even have to do something fancy by this fluorescent tricks or something like that, but only if if we know for sure that all our images have bounded support.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter how large, but it has to be bounded support, then we can.",
                    "label": 0
                },
                {
                    "sent": "We can mathematically invert the problem and I think that the reason or maybe one reason I'm speculating this is not my home field.",
                    "label": 0
                },
                {
                    "sent": "My reason by people thought this wasn't possible is that they they always look at this.",
                    "label": 0
                },
                {
                    "sent": "Modulation transfer function.",
                    "label": 0
                },
                {
                    "sent": "Things like that which are basically descriptions of an optical system, does if you feed an infinitely long sine wave into it.",
                    "label": 0
                },
                {
                    "sent": "What kind of frequencies can it transform on?",
                    "label": 0
                },
                {
                    "sent": "These are infinitely large objects, but if you object that finally lot turns out, you can use.",
                    "label": 0
                },
                {
                    "sent": "You don't lose information, so I think I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think this might have interesting implications that fit together with these new developments.",
                    "label": 0
                },
                {
                    "sent": "Question is, what happens if we have hundreds of nodes, so I should say we focused on the case of two nodes only because that was still an open problem.",
                    "label": 0
                },
                {
                    "sent": "Case we have you have many notes is very expensive with the traditional methods.",
                    "label": 0
                },
                {
                    "sent": "If you have hundreds, I think it's basically not doable if we don't make additional assumptions, we have briefly mentioned we have this one generalization where we use assumptions about the functional, the model number class to also tackle the invariable case, and I think that can also be very interesting for larger graphs.",
                    "label": 0
                },
                {
                    "sent": "It doesn't require the faithfulness assumption.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit difficult to tell.",
                    "label": 0
                },
                {
                    "sent": "I think it might actually be also possible to do it more efficiently than the standard methods, because it doesn't rely on this conditional independence testing, which immediately gives you this kind of combinatorial problem so.",
                    "label": 0
                },
                {
                    "sent": "We haven't done it yet, but I think that's an interesting direction here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the question is, can we also handle binary data?",
                    "label": 0
                },
                {
                    "sent": "Yes, I think so.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We have generalized some of this or we or specialized some of this through the case of categorical or binary variables, which is often important in causal inference.",
                    "label": 0
                },
                {
                    "sent": "So I think the answer is yes, but so we have only.",
                    "label": 0
                },
                {
                    "sent": "I've only had a finite amount of time and also we haven't looked at all these problems yet, but I think someone can do that.",
                    "label": 0
                },
                {
                    "sent": "So the question is, it's interesting because I was tempted to ask this question to the audience before Christmas.",
                    "label": 0
                },
                {
                    "sent": "What is easier cause learning or anti causal learning?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think it depends on the situation.",
                    "label": 0
                },
                {
                    "sent": "So for simply supervised learning, anti causal learning is easier for.",
                    "label": 0
                },
                {
                    "sent": "Robust learning or comparative store extrapolation into completely different domains.",
                    "label": 0
                },
                {
                    "sent": "Causal learning seems to be easier, but.",
                    "label": 0
                },
                {
                    "sent": "I think there's still a lot of space to think about these problems.",
                    "label": 0
                },
                {
                    "sent": "So that's a deep question.",
                    "label": 0
                },
                {
                    "sent": "How so is cause and effect?",
                    "label": 0
                },
                {
                    "sent": "Something in the real world?",
                    "label": 0
                },
                {
                    "sent": "Or is it just in our brains?",
                    "label": 0
                },
                {
                    "sent": "I don't know, I I. I think when we talk about cause and effect, of course.",
                    "label": 0
                },
                {
                    "sent": "It's something in our brains.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to start with the definition of course structures in terms of functional models, because that's just a mathematical object which for me correlates what with what I have in my brain when I talk about cause and effect, but I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's think about over his great talk.",
                    "label": 0
                }
            ]
        }
    }
}