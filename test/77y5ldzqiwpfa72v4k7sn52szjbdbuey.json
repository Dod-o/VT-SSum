{
    "id": "77y5ldzqiwpfa72v4k7sn52szjbdbuey",
    "title": "CLAMS: Computational Linguistic Applications for Multimedia Services Image Archival Data",
    "info": {
        "author": [
            "James Pustejovsky, Brandeis University"
        ],
        "published": "Jan. 14, 2019",
        "recorded": "October 2018",
        "category": [
            "Top->Humanities",
            "Top->Social Sciences"
        ]
    },
    "url": "http://videolectures.net/clarinannualconference2018_pustejovsky_data/",
    "segmentation": [
        [
            "Express my gratitude for the organizers and anyone responsible for suggesting my giving a talk here and also for humoring me to not necessarily talk about something computational semantic related.",
            "The I should perhaps give a little.",
            "Path regarding what would prompt me to talk about.",
            "Computational linguistic applications for multimedia services."
        ],
        [
            "This is a reporting on collaborative work that we've been doing for about six months with WG BH Archives, whose director is Karen Cariani.",
            "Who is actually giving a talk on Wednesday here about the GBH WGH archives?",
            "What I?",
            "Would like to do is to."
        ],
        [
            "Talk about the role that.",
            "Semantic and syntactic.",
            "Processing of media.",
            "Which is.",
            "The core of Clarence Mission really can provide as.",
            "Discover ability resources there is that term again.",
            "Example is favorite term language resources, but the tools associated with the resources to enable discover ability over media of variety of sorts.",
            "Multimedia in this case.",
            "My path to this is not.",
            "Really unusual, it actually is fairly directed, so I've been working on computational semantics over text for more years, and I'd like to really admit she already did admit so 35 years so I won't hide that over the last 10 years I've been working on multimodal.",
            "Data and the semantics associated with.",
            "Visual data.",
            "Gesture.",
            "As well as text and speech.",
            "Through that work which involves images, videos, but also the recognition and expression of gestures and speech ensemble together in peer to peer communication.",
            "Which is one of my primary areas of research.",
            "I was able to actually.",
            "Merge this with another area of research that I've been very committed to.",
            "For at least 15 to 20 years, which is the development of standards within ISO but also.",
            "Also, standards which are.",
            "Not necessarily, I so related, such as space and I so time L as well as the development of platforms for annotation of various types of data.",
            "And I've tried to promote various.",
            "Platforms for such things.",
            "Annotation environments with many of my colleagues and students.",
            "Recently the.",
            "Work on multimodal semantics has led me to find a home in actually trying to annotate and extract information from video and images, so I have a couple of grants that have led me to trying to try to extract information through semi supervised annotation of images and video, as well as linking that to text.",
            "Now this happens to relate to.",
            "Metadata to two archives that I've been familiar with from being a Bostonian for many years, namely W GBH and the corpora associated with W GBH, which is news and Frontline and Nova, and masterpiece theater and other things that we are Co. Co edited and Co directed by the BBC and in discussion with Karen Cariani and others at WGH.",
            "Became interested in trying to provide.",
            "Tools to enhance the metadata.",
            "After discovering what problems there were.",
            "With the archival assets that they have, so I'm going to describe this initial work that we've been doing, and.",
            "Try to relate that to some of the themes which are so dear to declare and work.",
            "So I'll start with the laps work that we've.",
            "We've been working on for about close to 10 years, actually, and then I'll introduce what is this project called Clams and then go through a couple of demo scenarios of how this can actually be deployed in the context."
        ],
        [
            "Of the American Archive of Public Broadcasting, which is APB."
        ],
        [
            "You'll hear more about this on Wednesday when.",
            "Karen Cariani gives her presentation briefly though.",
            "The American Archive of Public Broadcasting."
        ],
        [
            "Is close to 100,000 assets comprising about 50,000 hours of.",
            "Video an audio that go back to the 40s, nineteen 40s.",
            "Over public television and public radio that is chaired by the Library of Congress and W GBH and it's a historical treasure, it should be treated as kind of a national treasure as much as any sort of national archive, really.",
            "The problem with the."
        ],
        [
            "Assets as currently.",
            "Deployed which you can go to americanarchive.org and examine is that the Searchability discover ability and the general problems that you hear in claren over the past.",
            "Whatever would be 10 years actually then to 2008 have been trying to address how to leverage computational tools.",
            "To improve the experience of searching.",
            "Anne browsing and then utilizing an, then exploiting in your own research.",
            "So we are computational linguists, most of us.",
            "But the real.",
            "Customers are.",
            "The Economist, the historians, the digital humanities researchers, the linguists, historical linguists, and other archivists as well.",
            "Who would love to have access to such data?",
            "Many of you here have already deployed successful platforms for such tools, so I would like to discuss some of the initial steps that we've been making towards this goal."
        ],
        [
            "The first element of this that I'd like to discuss that.",
            "That began the investigation into.",
            "Putting tools together in a common platform is something called the language application grid."
        ],
        [
            "This.",
            "Where is started as a collaborative effort.",
            "Between Brandeis and Vassar, NASA, Nancy and Chris, Siri, who's here from LDC and Eric Nyberg at CMU.",
            "We have been funded by the National Science Foundation for.",
            "A number of number of rounds very successfully.",
            "This builds on work that really started back in 2005, actually initially with the unified linguistic annotation framework, which was also with Chris and Martha Palmer and others, and then something that was called silt.",
            "That was Nancy and myself, but it also is ground on the language grid panacea, lingua grid, and other web service networks that have been worked on and develop within the NLP community."
        ],
        [
            "So.",
            "Very much like the efforts within Claire and the lapse grid.",
            "In really a smaller funding profile and environment.",
            "Is a framework to create a service of discovery, composition and reuse for tools.",
            "And the goal here is, as with.",
            "The overall goals of Claren is to promote sustainability and usability.",
            "An also importantly interoperability, and so I'll focus today on interoperability of NLP components.",
            "But not just NLP, but as we move into other media assets and other media type, we have to look at things which are going to be interoperability with other sorts of media, aren't we?",
            "So it's based on the service oriented architecture, the SOA where we have web wrapped pipeline architectured applications which allow us to do loosely coupled analysis and pipelines."
        ],
        [
            "So I'll just zip through this somewhat quickly because I believe this is going to be fairly shared.",
            "Goals for the community here.",
            "The language application Grid, which is laps.",
            "Is going to.",
            "Support and has supported in small form not not anywhere near the clear and development of integrated NLP applications.",
            "So.",
            "One thing that has been facilitated by having LDC's contribution is to be able to navigate through licensing issues and a really very very nice harbor.",
            "Kind of painless way."
        ],
        [
            "The NLP processing tools that we have and you can go to too.",
            "Laps grid and laps galaxy.org to see what we have we have.",
            "All the basic NLP processing tools that you could imagine at 1st at first glance and mono and multi lingual corpora in lexicons available, and what we enable is a kind of black box composite servicing."
        ],
        [
            "So the.",
            "Current frameworks just to motivate some of this, and I think it's important to.",
            "Discuss the role of lapse as a open architecture that does not force you to adopt A specific.",
            "Type of of pipelining or workflow.",
            "Composition or architecture, such as in UIMA or gate.",
            "The tools for which, by the way we actually have.",
            "Within laps, so they actually are tools themselves within the services themselves within the market."
        ],
        [
            "Alright we as I mentioned, we are based on the open service grid which is developed by an ICT in Japan and were linked to the the Kyoto in other Asian grids as well."
        ],
        [
            "The architecture.",
            "Is as diploid through Galaxy which initially started.",
            "In the bioinformatics community, and I believe some within Claire and have started.",
            "Using an adopting some of the workflow structure that Galaxy provides the very nice environment.",
            "To essentially be able to take the services we have your web browser interface user."
        ],
        [
            "And essentially able to.",
            "Communicate with a lot of different manager nodes through the Galaxy architecture itself.",
            "Pipelining in a fairly seamless.",
            "And very simple structure in architecture."
        ],
        [
            "Now the.",
            "Evaluation services that we allow provide for comparing various workflow structures dynamically so you can best configure and optimize the application of 1 tool over another.",
            "For example, in a complicated pipeline to test out, for example, which dependency parser would be best suited for your particular data set, or what part of speech.",
            "Or named entity recognizer and so on and so forth would be the best for the style you have, and that's a very nice tool that's been provided by the CMU."
        ],
        [
            "So.",
            "The the Galaxy project.org you'll see the the workflow engine for the Galaxy and then as deployed within laps."
        ],
        [
            "So that the basic idea is you pick a tool and we have type compliance.",
            "Of the IO of the tool to allow you to.",
            "Select only the appropriate.",
            "Tools to be new dled as the output too.",
            "The tool that you've connected to, so there's a type compliance.",
            "And this is not actually part of Galaxy.",
            "This is part of our own introduction of the the.",
            "The exchange the interchange format that allows you to essentially do kind of type checking on the tools before you actually do a run.",
            "And that's really quite nice."
        ],
        [
            "So.",
            "The interoperability that's enabled is through soap and whistle.",
            "By developing this interchange format the LAF Soul if this allows us to Rep a tool and to essentially not worry about the compliance of the IO to another tool and this is what within the text.",
            "Based domain of whatever we're dealing with in terms of NLP application, this allows us syntactic and semantic interoperability, so the syntactic.",
            "Is handled by Jason LD and the Semantic interoperability is enhanced by using the link data.",
            "Component Jason LD.",
            "So."
        ],
        [
            "So why Jason LD?",
            "Well, I don't want to go into this in too much detail because I believe this is speaks for itself.",
            "It's lightweight.",
            "It's based on RDF and it allows for us to have very very simple mappings to other graph based formats.",
            "An will also enable reference categories mapped in other repository's that are not component parts of the lapse create itself."
        ],
        [
            "OK. Now.",
            "Lapse uses Jason LD is a standard format for the payload, so the converters to and from Jason LD deliver in other formats, and some are wrapped to produce and consume Jason LD."
        ],
        [
            "So the logic flow is very straightforward in terms of client server communication, which is not represented here.",
            "But I have data I've got.",
            "For example, a gate service.",
            "I want to basically be able to map that to a UIMA service and maybe then push that to something that comes.",
            "From open NLP, perhaps something provided from Stanford.",
            "All of these are going to be essentially completely compliant and play together in the same in the same environment, the same.",
            "In the same general toolkit, and that is one of the advantages of this.",
            "This is enabled through the exchange vocabulary that."
        ],
        [
            "As part of the web service.",
            "Web service protocol.",
            "The WCV has this terminology for any particular linguistic object that we're particularly interested in for an application and that is consuming or being consumed by a tool."
        ],
        [
            "So the exchange vocabulary has a type hierarchy with inheritance that here.",
            "For example, we can see the different annotation types.",
            "I've got paragraph sentences, noun chunks, verb chunks, etc etc.",
            "Everything that we would be familiar with from the literature as consuming and generating data types.",
            "You can.",
            "Browse this if you want.",
            "Add vocab lapse grid."
        ],
        [
            "OK.",
            "I will skip over that now."
        ],
        [
            "What we've done in the context of a Mellon grant that we had for.",
            "Looking at some of the Hathitrust data with Indiana and Illinois over the last three years, we deployed the lapse grid in the context of."
        ],
        [
            "They hardly trust.",
            "Data capsule.",
            "Now the.",
            "Kathy Trust Research Center.",
            "Created a data capsule.",
            "So that researchers could explore the in Copyright component of the Hathitrust data.",
            "Anne.",
            "That meant we had to deploy the.",
            "Laps as.",
            "Something which would have secure access and stay within the data capsule.",
            "So this was actually a very interesting challenge, so I'm sure you're familiar with the trust.",
            "It's 50 million volumes that were the scanned digitized content from Google that has been part of the research library.",
            "Of Illinois and Indiana.",
            "Another preserves and the.",
            "The challenge from Indiana, Illinois was to say, OK, we have a lot of tools for doing topicalization, topic spotting and some basic OCR, but what about named entity recognition without parsing?",
            "What about some more sophisticated tools so they came to us to see whether or not we could actually deploy that in the context of a secure environment within a virtual virtual machine and that that project just concluded quite successfully with.",
            "With us showing that these tools within that data capsule are deployed."
        ],
        [
            "So the structure for that was deployed as Docker images in the data capsule that were dedicated and did not communicate with anything outside of the data capsule itself.",
            "Any tool that we had that was requested by the researchers were any tool was actually native to the server that was directly dedicated to the data capsule.",
            "And that was actually really very interesting project."
        ],
        [
            "The.",
            "We removed the Galaxy front end as essentially something that wasn't necessary because we didn't need.",
            "They didn't really need to have sophisticated pipelining and workflow structure.",
            "As I said, that was deployed as a Docker swarm on the physical machine.",
            "That allowed the in Copyright data to be secure and licensing issues were never really compromised."
        ],
        [
            "So the.",
            "What this allowed to the researchers was enhanced search and discovery.",
            "And this was complementing actually quite nicely.",
            "The traditional volume level bibliographic metadata which was being created by the Hathitrust in terms of what they were contributing to the metadata for search.",
            "They're all they were adding to that named entities events as well as other multi word expressions and things which might be more CL related data types from any R for example."
        ],
        [
            "The next.",
            "Effort we had, which goes towards creating this interest in the multimodal, and video data was.",
            "Started by this.",
            "Collaboration that Nancy and I had with Earhart in Tubingen Jan in Prague, which was essentially away.",
            "Of testing whether the authentication and other aspects of interoperability between Claire, Indian, Linda, Claire, and.",
            "Could be linked to the laps tools and the laps architecture."
        ],
        [
            "The first thing that was on the plate in terms of.",
            "What we needed to address was the authentication an authorization.",
            "So the.",
            "The first the first challenge was essentially addressed by prog and then tubing and associates at Vassar.",
            "To try to find the best means of.",
            "Allowing for institutional.",
            "Access from the current claren.",
            "And the current laps users and user base to make sure that we could utilize and access seamlessly each others applications."
        ],
        [
            "Part of the.",
            "Step that was required after the authentication and.",
            "The authentication infrastructure was in place is to essentially ensure syntactic and semantic interoperability.",
            "And this is going to be the key for moving into multimodal data as well, so.",
            "Web licked uses a TCF text corpus format, while laps has the laps interchange format.",
            "The The Protocols are different between both domains, so we needed to be able to map 1 to the other through converters."
        ],
        [
            "Which were created.",
            "The vocabulary from lapse and the schema set from TCF.",
            "Found their ways.",
            "Into a concept mapping index which allows for this type of transfer and conversion quite quite successfully.",
            "So the semantic interoperability allowed for.",
            "The mapping between the basic core applications that we've looked at so far, much more work needs to be done to look at more sophisticated and broader range of applications.",
            "That's current work that we're actually pursuing right now."
        ],
        [
            "The obvious difference between our being soap and.",
            "Rabbits being restful was.",
            "The requirement of a converter between those and the metadata converter was.",
            "Constructed and successfully deployed now."
        ],
        [
            "Gives rise to a very nice integration between web list and the laps.",
            "Grid, which is summarized here where I have converters for the formats, the interchange formats.",
            "That allow me to essentially go in through either one.",
            "I can go in through web based or through apps deploy and call and actually pipeline a tool from either location.",
            "Which is really, really nice.",
            "There are so many other so many resources that are available through Claire and that we have no access to.",
            "We just don't have in our toolkit, but we can actually have access to from the marriage of these two.",
            "So this integration, interoperability that to go back to Antonio Examplea and early work from the early calls from Nicoletta from 25 years ago about interoperability.",
            "Are partially achieved by this type of careful mapping of the protocols, the interchange formats and the infrastructures."
        ],
        [
            "That leads me to.",
            "The interest that is at hand now, which is to try to bring some of these tools.",
            "2 Morse to Richard.",
            "Data types that involve more than just language multimedia."
        ],
        [
            "So the clams architecture is really to be.",
            "In the service of what is called an eye, a moving image archives.",
            "And this is an entire area that I'm just becoming familiar with.",
            "So Mia and the associated.",
            "Datasets of video which many of you here in the audience and the Claron workshop have been dealing with, for I believe for many years.",
            "Require different interchange formats because you're dealing with video you're dealing with with images.",
            "You're dealing with time based content and indexing.",
            "That are just not part of the problem of text analysis, so.",
            "This is a kind of general archivist, researcher to a web browser has a front end.",
            "Workflow engine essentially able to load up.",
            "Some data and perform some analysis."
        ],
        [
            "Now the interchange formats for laps and for many of the applications within Claire and Dr. And Linda are text based.",
            "Or language based.",
            "The annotations within laps are in lift and they are always anchored to a character offset.",
            "So we did show that we have successful integration with bibliographic data in the potty, but we need to."
        ],
        [
            "Move.",
            "Two image archives and moving image archive.",
            "So there's two different types of archival sets.",
            "There's images as in.",
            "Flickr 30 K An visual genome and just other sorts of CPR based datasets, which are of course used in the vision community, but also moving image archives.",
            "So within MI Amia you're going beyond text data.",
            "Time based media, video and audio and we need to do an analysis of time based media.",
            "So it's not based on characters, obviously.",
            "So, but of course, as you know, the results of such mihn will be subject to linguistic analysis.",
            "If I want to do character analysis or content analysis of text within an image, and so audios can be transcribed text data as we know with with alignment and videos have text inside this caption, logo, subtitles, etc etc.",
            "Now this is an area of quite considerable research, but there's still very little diploid work on this.",
            "That is actually available to a working archivist.",
            "So what we need are analytic tools for multimodality."
        ],
        [
            "So we've been working on a multimedia interchange format in myth which handle video, audio, text on image and transcripts.",
            "This creates an alignment between these different media and annotation types and then subsequently allows for multimodal analysis.",
            "So we have the initial.",
            "Initial draft of this and it's deployed.",
            "I'll show you in a demo if the servers up.",
            "And this has easy IO.",
            "Believe it should be no significant difficulty in mapping this to lift and then there by two TCF in Vebb licked for use.",
            "Any tool that would be available in.",
            "Claire and D for example.",
            "So."
        ],
        [
            "Basically, the way this would work is I have an MF encoded video.",
            "I do a demux on it, I get my audio on my video track.",
            "And I able to.",
            "Essentially I need to have pointers to the external files.",
            "I need to have pointers to the internal content, internal regions, etc etc.",
            "As you would have for region analysis and then doing analysis of subregions of text."
        ],
        [
            "So the type hierarchy for clams is.",
            "I have in terms of the media type I've got time based data, video and audio character based data can be conventional linguistic data.",
            "And then I have region based data which is bounding boxes on still images.",
            "Mutation types are going to be down to the media type obviously, and then I'll have special multimodal alignments such as a forced alignment and character based things like that."
        ],
        [
            "So I'll have character start end for text for named entity tokenization classic things like that for video and audio.",
            "I'm going to have time stamps, frame numbers etc etc.",
            "I'm going to want to do for example Ching.",
            "I'm going to want to story segmentation sound classification to recognize things that are not not speech but are trains and whistles and dogs.",
            "And then for image annotation I've got bounding box, classic bounding box annotation.",
            "So."
        ],
        [
            "The conventional time timestamp boxes I'm going to have essentially one index to a particular time, and I'm going to have ID block that I'm going to have from frame to frame in order to be able to do Co reference."
        ],
        [
            "This classic classic."
        ],
        [
            "Rotation of that."
        ],
        [
            "So I'll show you in a second, the actually.",
            "Maybe this is time to do it the platform prototype.",
            "So.",
            "Let me go."
        ],
        [
            "Through the workflows.",
            "So we have news hours and video from the the 90,000 assets that WGH American Archive has.",
            "You're able to select an asset video.",
            "You're able to select maybe a nice transcript if you want to do forced alignment.",
            "We are able to perform some filtering such as bar and tone detection.",
            "For example, I'll show you some of these and then do some noise segment filtering and."
        ],
        [
            "Then I'll show you another workflow, which is to do a forced alignment with and without a bar and tone filtering."
        ],
        [
            "So let me show you now some of the.",
            "I can.",
            "To this.",
            "OK, hopefully this will be working.",
            "Alright, so.",
            "I'll select a Connecticut public television.",
            "Text.",
            "So this is you see that this is an example.",
            "This is an example of the video for the 1st.",
            "30 to 60 seconds I have bars and tone.",
            "Then I have this late.",
            "OK, so this is what I want to do is this late?",
            "I'd like to be able to just pause that for a second that's late.",
            "Is something that we'd like to do text segment, text region analysis.",
            "So there's a nice text region analysis algorithm that was just published last year at CPR, which uses neural net architecture which will recognize the text segment and then we perform tesseract to do OCR on that and we get a about a 90% accuracy.",
            "The time expression if I perform.",
            "If I do the OCR and then I can run that through one of the lapse applications such as Heidel time which is wrapped heidel time will recognize that.",
            "Convert that to ISO time L which then allows me to create the metadata which then goes back to the metadata format addition of the asset, which then could hopefully eventually be added to something like vlog for search.",
            "So that's the idea.",
            "So that's one thing that we need to do.",
            "There's graphics on the page.",
            "This is the all these things are quite difficult to recognize.",
            "I'm sure many of you know this.",
            "And.",
            "Little stylized and then there's another thing which is really quite difficult is what's called the Chiron, the lower 1/3, which I'm sure you're familiar with when you have the lower 1/3.",
            "Video.",
            "Well this is very slow.",
            "OK let me just stop.",
            "OK so maybe just so.",
            "So let's just say stop this guy.",
            "So I have.",
            "I want to basically show I'm going to add the service of East detection.",
            "That's the frame.",
            "And then I'm going to add the service of doing an OCR.",
            "OK, and then then I'll perform Heidel time.",
            "OK. And then I deploy the workflow.",
            "And.",
            "I download it and I got a sample myth output which basically is this wrapped data that I'm not going to go through and examine, But basically is something which would be compliant with the ISO time L. Format which I mentioned before another example would be to take.",
            "News hour.",
            "And let's say I want to forced alignment.",
            "So I take an audio and video and I take a video and I take a clean transcription that has no trend that has no alignment.",
            "Ann, I want to apply.",
            "The.",
            "The MFA, the Montreal 4 cyliner.",
            "And.",
            "I could actually I could do bars and tone detection which actually cleans it up a little bit.",
            "So because the bars and tone will actually take out some of the confusion that the Montreal forced aligner actually runs into, and then I deploy it and the same thing it'll output and I'll get the forced alignment output that I don't need to show you.",
            "So that's the idea.",
            "There one of the other tools that we've been deploying is something called Elf, which is the event loop."
        ],
        [
            "Station Finder.",
            "So basically it's given A-frame in a video.",
            "What's the most likely seen type?",
            "So this is scene recognition.",
            "Now this has worked."
        ],
        [
            "We've been doing for a number of years in the context of multimodal lexecon construction, something that started in coling 2016 whenever how you ever had a workshop on on basically resources that we were interested in terms of multimodal resources and creating a lexecon where you have information associated with a number of modalities, not just a semantic type, but also the video or visual image of something associated with it, as well as its acoustics or sound.",
            "So in the context of this work, we have seen types and they specify where events happen, so seem recognition is of course a very very steady problem in CPR and vision, but it's not necessarily adapted to the type of data that we have here.",
            "An we are benchmarking our algorithm against the algorithms that have not been trained on the data for within GBH and of course by.",
            "Virtue of our training we're performing better than, say, the places to places.",
            "Two algorithms from MIT, so."
        ],
        [
            "Let's just let me just walk you through a little bit of how this works.",
            "Do I have like 5 minutes?",
            "What do I have?",
            "Thank thank you.",
            "So first we started, we created an event localization corpus which was founded on the Flickr 30 K. You're familiar with 30K, is having essentially annotations describing each image so.",
            "You"
        ],
        [
            "Classic sort of five annotations for a picture, but what we do so this is these are crowdsourced annotations that Flickr provided.",
            "For images, and so a man and two kids on a boat, two children wearing lifejackets.",
            "What we did."
        ],
        [
            "Well, we took those and we crowdsourced annotations of where it was happening.",
            "According to the places to hierarchy which."
        ],
        [
            "Was essentially.",
            "Either indoor or outdoor weather was man made.",
            "Indoor, or whether if it was outdoor, whether it was a natural outdoor scene or an artificial outdoor scene such as a sports field or something like that.",
            "These are some of the classic hierarchical hierarchical classifications used in vision and."
        ],
        [
            "So.",
            "What we did was we adapted this by training it on some of the news hour data.",
            "So the question is, given an image, what's the most likely location type?",
            "So we did a pre trained places 2 model RESNET 5050 layer convolutional neural net Anna text model which was also a 2G R or NN and."
        ],
        [
            "That gave good results for the Flickr.",
            "Now then we took that model and adapted it.",
            "We trained it on the news hour data specifically, so we took one video and then we extracted the frames, one frame per second and annotated with the scenes such as this is indoor.",
            "This is a this is a studio shot and we train the model on the annotation frames.",
            "Now what we did."
        ],
        [
            "Do with the change the ontology to adapt to essentially.",
            "TV news, as it were, so we had bars and tones, blank screens, graphics on screen, guest in a studio guest out of studio ahead with the graphic.",
            "Things like this so they."
        ],
        [
            "Basically, these are the kinds of scene ontologies barn tone I've got.",
            "Text in a picture I've got just graphics that is on the record.",
            "I've got the classic talking head with a graphic, then I've got a talking head outside and then natural a person.",
            "In front of a natural in front of an artificial outdoor scene and then so on and so forth."
        ],
        [
            "And so these were.",
            "Then"
        ],
        [
            "So we took that and did transfer learning.",
            "We train the last layer to classify according to this new ontology."
        ],
        [
            "And without some of the merging of categories, we got a 73% accuracy by category.",
            "You can look at the results here, but there."
        ],
        [
            "Some interesting things to point out, so this was really hard to point out because effectively this is a Reporter at a desk versus a guest in a studio, and what you really what the model actually had to do was to identify an individual such as layer or mcniell, whatever some individual versus some studio guest.",
            "So this is actually we combine this talking head category and we achieved over 86% F measure."
        ],
        [
            "Another thing which was difficult, which is graphics on a scene versus on the left versus this other thing which was a video it was it was a show about an art.",
            "And art show.",
            "And there are other things which were very confusing, such as online shopping, which were web pages and things like that.",
            "So if we if we combine those, we essentially can merge the results of that and get actually something close to 87%."
        ],
        [
            "The."
        ],
        [
            "OCR within a within a box.",
            "OCR in characters in various patterns does not work well, and so this is because."
        ],
        [
            "Is this is a digital slate?",
            "OK, now in the early copies early video as you are aware.",
            "If you have access to.",
            "Two shows before the 80s.",
            "You have slates which are handwritten, and those are quite difficult, but they are at least on a clear background on a steady background.",
            "So what's interesting about this is check this out.",
            "This is a this is a slate.",
            "Notice the date which we've recognized with Heidel time through this various MMIFF mapping to lift too.",
            "Heidel time OK, but notice the mention of the director and the producer."
        ],
        [
            "OK, so East, which is this neural net text box recognizer recognizes those we do."
        ],
        [
            "OCR on that, here's the bottom third, which is more difficult to recognize because of the noise behind it.",
            "Notice the white of the shirt, so this this comes out quite difficult to understand, however.",
            "By virtue of doing some sort of entity coreference we have referenced of Unger in Douglas as as information that allows us to say this could be Douglas because I'm not really reading that."
        ],
        [
            "And then you've got the credit roll, which also has additional information that we can use to create additional support for a particular hypothesis in our model, that maybe that is Douglas.",
            "And maybe that is Unger, for example."
        ],
        [
            "So.",
            "Finally."
        ],
        [
            "What we've done is forced alignment example that I showed you.",
            "We are currently using the Montreal 4 cyliner.",
            "We'd like to use assets and tools that are part of the web link resource kit and we're talking to Ehrhardt about that and but this essentially is really critical for multimodal analysis, which allows us to do.",
            "Eventual text tiling and then segmentation in a Ching format so."
        ],
        [
            "So the forced alignment is is still very limited in terms of just the transcription based forced alignment, but we've been experimenting with is to use some of the tech scene.",
            "The scene classification to give us kind of prior to the idea of where is this Ching actually starting and also the bars and tone identification also greatly enhances.",
            "Some of the segment boundary identification."
        ],
        [
            "OK, so let me just close up here."
        ],
        [
            "I hope I've given you some not totally incoherent summary of what we're trying to do.",
            "This is really early work.",
            "A lot of it's really green, and we're probably repeating mistakes and false starts that many of you have done and overcome.",
            "Maybe perhaps many years ago, but.",
            "We were having fun doing it the.",
            "Future work is to continue on with the chaptering.",
            "Some of the scene recognition is work that we're continuing anyway with our multimodal work.",
            "And I think the multimodal entity Coreference which I mentioned just now.",
            "He's actually quite promising to help do some of the disambiguation as well as to help enhance some of the information coming from the sparse models that we get for OCR.",
            "Obviously, much of this relates to the content based video retrieval work that's gone on for years, and I think some.",
            "Interesting shared work and shared challenges could be developed around these types of multimodal tasks that are not just the kind that have been promoted by by NIST, but in fact maybe something that's closer to the hearts of the Claron community, and we could perhaps talk about that.",
            "I think I'll just close there.",
            "Thank you.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Express my gratitude for the organizers and anyone responsible for suggesting my giving a talk here and also for humoring me to not necessarily talk about something computational semantic related.",
                    "label": 0
                },
                {
                    "sent": "The I should perhaps give a little.",
                    "label": 0
                },
                {
                    "sent": "Path regarding what would prompt me to talk about.",
                    "label": 0
                },
                {
                    "sent": "Computational linguistic applications for multimedia services.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a reporting on collaborative work that we've been doing for about six months with WG BH Archives, whose director is Karen Cariani.",
                    "label": 0
                },
                {
                    "sent": "Who is actually giving a talk on Wednesday here about the GBH WGH archives?",
                    "label": 0
                },
                {
                    "sent": "What I?",
                    "label": 0
                },
                {
                    "sent": "Would like to do is to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about the role that.",
                    "label": 0
                },
                {
                    "sent": "Semantic and syntactic.",
                    "label": 0
                },
                {
                    "sent": "Processing of media.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "The core of Clarence Mission really can provide as.",
                    "label": 0
                },
                {
                    "sent": "Discover ability resources there is that term again.",
                    "label": 0
                },
                {
                    "sent": "Example is favorite term language resources, but the tools associated with the resources to enable discover ability over media of variety of sorts.",
                    "label": 0
                },
                {
                    "sent": "Multimedia in this case.",
                    "label": 0
                },
                {
                    "sent": "My path to this is not.",
                    "label": 0
                },
                {
                    "sent": "Really unusual, it actually is fairly directed, so I've been working on computational semantics over text for more years, and I'd like to really admit she already did admit so 35 years so I won't hide that over the last 10 years I've been working on multimodal.",
                    "label": 0
                },
                {
                    "sent": "Data and the semantics associated with.",
                    "label": 0
                },
                {
                    "sent": "Visual data.",
                    "label": 0
                },
                {
                    "sent": "Gesture.",
                    "label": 0
                },
                {
                    "sent": "As well as text and speech.",
                    "label": 0
                },
                {
                    "sent": "Through that work which involves images, videos, but also the recognition and expression of gestures and speech ensemble together in peer to peer communication.",
                    "label": 0
                },
                {
                    "sent": "Which is one of my primary areas of research.",
                    "label": 0
                },
                {
                    "sent": "I was able to actually.",
                    "label": 0
                },
                {
                    "sent": "Merge this with another area of research that I've been very committed to.",
                    "label": 0
                },
                {
                    "sent": "For at least 15 to 20 years, which is the development of standards within ISO but also.",
                    "label": 0
                },
                {
                    "sent": "Also, standards which are.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily, I so related, such as space and I so time L as well as the development of platforms for annotation of various types of data.",
                    "label": 0
                },
                {
                    "sent": "And I've tried to promote various.",
                    "label": 0
                },
                {
                    "sent": "Platforms for such things.",
                    "label": 0
                },
                {
                    "sent": "Annotation environments with many of my colleagues and students.",
                    "label": 0
                },
                {
                    "sent": "Recently the.",
                    "label": 0
                },
                {
                    "sent": "Work on multimodal semantics has led me to find a home in actually trying to annotate and extract information from video and images, so I have a couple of grants that have led me to trying to try to extract information through semi supervised annotation of images and video, as well as linking that to text.",
                    "label": 0
                },
                {
                    "sent": "Now this happens to relate to.",
                    "label": 0
                },
                {
                    "sent": "Metadata to two archives that I've been familiar with from being a Bostonian for many years, namely W GBH and the corpora associated with W GBH, which is news and Frontline and Nova, and masterpiece theater and other things that we are Co. Co edited and Co directed by the BBC and in discussion with Karen Cariani and others at WGH.",
                    "label": 0
                },
                {
                    "sent": "Became interested in trying to provide.",
                    "label": 0
                },
                {
                    "sent": "Tools to enhance the metadata.",
                    "label": 0
                },
                {
                    "sent": "After discovering what problems there were.",
                    "label": 0
                },
                {
                    "sent": "With the archival assets that they have, so I'm going to describe this initial work that we've been doing, and.",
                    "label": 0
                },
                {
                    "sent": "Try to relate that to some of the themes which are so dear to declare and work.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with the laps work that we've.",
                    "label": 0
                },
                {
                    "sent": "We've been working on for about close to 10 years, actually, and then I'll introduce what is this project called Clams and then go through a couple of demo scenarios of how this can actually be deployed in the context.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the American Archive of Public Broadcasting, which is APB.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You'll hear more about this on Wednesday when.",
                    "label": 0
                },
                {
                    "sent": "Karen Cariani gives her presentation briefly though.",
                    "label": 0
                },
                {
                    "sent": "The American Archive of Public Broadcasting.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is close to 100,000 assets comprising about 50,000 hours of.",
                    "label": 0
                },
                {
                    "sent": "Video an audio that go back to the 40s, nineteen 40s.",
                    "label": 0
                },
                {
                    "sent": "Over public television and public radio that is chaired by the Library of Congress and W GBH and it's a historical treasure, it should be treated as kind of a national treasure as much as any sort of national archive, really.",
                    "label": 0
                },
                {
                    "sent": "The problem with the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assets as currently.",
                    "label": 0
                },
                {
                    "sent": "Deployed which you can go to americanarchive.org and examine is that the Searchability discover ability and the general problems that you hear in claren over the past.",
                    "label": 0
                },
                {
                    "sent": "Whatever would be 10 years actually then to 2008 have been trying to address how to leverage computational tools.",
                    "label": 0
                },
                {
                    "sent": "To improve the experience of searching.",
                    "label": 0
                },
                {
                    "sent": "Anne browsing and then utilizing an, then exploiting in your own research.",
                    "label": 0
                },
                {
                    "sent": "So we are computational linguists, most of us.",
                    "label": 0
                },
                {
                    "sent": "But the real.",
                    "label": 0
                },
                {
                    "sent": "Customers are.",
                    "label": 0
                },
                {
                    "sent": "The Economist, the historians, the digital humanities researchers, the linguists, historical linguists, and other archivists as well.",
                    "label": 0
                },
                {
                    "sent": "Who would love to have access to such data?",
                    "label": 0
                },
                {
                    "sent": "Many of you here have already deployed successful platforms for such tools, so I would like to discuss some of the initial steps that we've been making towards this goal.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first element of this that I'd like to discuss that.",
                    "label": 0
                },
                {
                    "sent": "That began the investigation into.",
                    "label": 0
                },
                {
                    "sent": "Putting tools together in a common platform is something called the language application grid.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Where is started as a collaborative effort.",
                    "label": 0
                },
                {
                    "sent": "Between Brandeis and Vassar, NASA, Nancy and Chris, Siri, who's here from LDC and Eric Nyberg at CMU.",
                    "label": 0
                },
                {
                    "sent": "We have been funded by the National Science Foundation for.",
                    "label": 1
                },
                {
                    "sent": "A number of number of rounds very successfully.",
                    "label": 0
                },
                {
                    "sent": "This builds on work that really started back in 2005, actually initially with the unified linguistic annotation framework, which was also with Chris and Martha Palmer and others, and then something that was called silt.",
                    "label": 0
                },
                {
                    "sent": "That was Nancy and myself, but it also is ground on the language grid panacea, lingua grid, and other web service networks that have been worked on and develop within the NLP community.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Very much like the efforts within Claire and the lapse grid.",
                    "label": 0
                },
                {
                    "sent": "In really a smaller funding profile and environment.",
                    "label": 0
                },
                {
                    "sent": "Is a framework to create a service of discovery, composition and reuse for tools.",
                    "label": 1
                },
                {
                    "sent": "And the goal here is, as with.",
                    "label": 0
                },
                {
                    "sent": "The overall goals of Claren is to promote sustainability and usability.",
                    "label": 1
                },
                {
                    "sent": "An also importantly interoperability, and so I'll focus today on interoperability of NLP components.",
                    "label": 0
                },
                {
                    "sent": "But not just NLP, but as we move into other media assets and other media type, we have to look at things which are going to be interoperability with other sorts of media, aren't we?",
                    "label": 0
                },
                {
                    "sent": "So it's based on the service oriented architecture, the SOA where we have web wrapped pipeline architectured applications which allow us to do loosely coupled analysis and pipelines.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just zip through this somewhat quickly because I believe this is going to be fairly shared.",
                    "label": 0
                },
                {
                    "sent": "Goals for the community here.",
                    "label": 0
                },
                {
                    "sent": "The language application Grid, which is laps.",
                    "label": 1
                },
                {
                    "sent": "Is going to.",
                    "label": 1
                },
                {
                    "sent": "Support and has supported in small form not not anywhere near the clear and development of integrated NLP applications.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "One thing that has been facilitated by having LDC's contribution is to be able to navigate through licensing issues and a really very very nice harbor.",
                    "label": 0
                },
                {
                    "sent": "Kind of painless way.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The NLP processing tools that we have and you can go to too.",
                    "label": 0
                },
                {
                    "sent": "Laps grid and laps galaxy.org to see what we have we have.",
                    "label": 0
                },
                {
                    "sent": "All the basic NLP processing tools that you could imagine at 1st at first glance and mono and multi lingual corpora in lexicons available, and what we enable is a kind of black box composite servicing.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Current frameworks just to motivate some of this, and I think it's important to.",
                    "label": 0
                },
                {
                    "sent": "Discuss the role of lapse as a open architecture that does not force you to adopt A specific.",
                    "label": 0
                },
                {
                    "sent": "Type of of pipelining or workflow.",
                    "label": 0
                },
                {
                    "sent": "Composition or architecture, such as in UIMA or gate.",
                    "label": 0
                },
                {
                    "sent": "The tools for which, by the way we actually have.",
                    "label": 0
                },
                {
                    "sent": "Within laps, so they actually are tools themselves within the services themselves within the market.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright we as I mentioned, we are based on the open service grid which is developed by an ICT in Japan and were linked to the the Kyoto in other Asian grids as well.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The architecture.",
                    "label": 0
                },
                {
                    "sent": "Is as diploid through Galaxy which initially started.",
                    "label": 0
                },
                {
                    "sent": "In the bioinformatics community, and I believe some within Claire and have started.",
                    "label": 0
                },
                {
                    "sent": "Using an adopting some of the workflow structure that Galaxy provides the very nice environment.",
                    "label": 0
                },
                {
                    "sent": "To essentially be able to take the services we have your web browser interface user.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And essentially able to.",
                    "label": 0
                },
                {
                    "sent": "Communicate with a lot of different manager nodes through the Galaxy architecture itself.",
                    "label": 0
                },
                {
                    "sent": "Pipelining in a fairly seamless.",
                    "label": 0
                },
                {
                    "sent": "And very simple structure in architecture.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "Evaluation services that we allow provide for comparing various workflow structures dynamically so you can best configure and optimize the application of 1 tool over another.",
                    "label": 0
                },
                {
                    "sent": "For example, in a complicated pipeline to test out, for example, which dependency parser would be best suited for your particular data set, or what part of speech.",
                    "label": 0
                },
                {
                    "sent": "Or named entity recognizer and so on and so forth would be the best for the style you have, and that's a very nice tool that's been provided by the CMU.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The the Galaxy project.org you'll see the the workflow engine for the Galaxy and then as deployed within laps.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that the basic idea is you pick a tool and we have type compliance.",
                    "label": 0
                },
                {
                    "sent": "Of the IO of the tool to allow you to.",
                    "label": 0
                },
                {
                    "sent": "Select only the appropriate.",
                    "label": 0
                },
                {
                    "sent": "Tools to be new dled as the output too.",
                    "label": 0
                },
                {
                    "sent": "The tool that you've connected to, so there's a type compliance.",
                    "label": 0
                },
                {
                    "sent": "And this is not actually part of Galaxy.",
                    "label": 0
                },
                {
                    "sent": "This is part of our own introduction of the the.",
                    "label": 0
                },
                {
                    "sent": "The exchange the interchange format that allows you to essentially do kind of type checking on the tools before you actually do a run.",
                    "label": 0
                },
                {
                    "sent": "And that's really quite nice.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The interoperability that's enabled is through soap and whistle.",
                    "label": 0
                },
                {
                    "sent": "By developing this interchange format the LAF Soul if this allows us to Rep a tool and to essentially not worry about the compliance of the IO to another tool and this is what within the text.",
                    "label": 0
                },
                {
                    "sent": "Based domain of whatever we're dealing with in terms of NLP application, this allows us syntactic and semantic interoperability, so the syntactic.",
                    "label": 0
                },
                {
                    "sent": "Is handled by Jason LD and the Semantic interoperability is enhanced by using the link data.",
                    "label": 1
                },
                {
                    "sent": "Component Jason LD.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why Jason LD?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't want to go into this in too much detail because I believe this is speaks for itself.",
                    "label": 0
                },
                {
                    "sent": "It's lightweight.",
                    "label": 0
                },
                {
                    "sent": "It's based on RDF and it allows for us to have very very simple mappings to other graph based formats.",
                    "label": 0
                },
                {
                    "sent": "An will also enable reference categories mapped in other repository's that are not component parts of the lapse create itself.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "Lapse uses Jason LD is a standard format for the payload, so the converters to and from Jason LD deliver in other formats, and some are wrapped to produce and consume Jason LD.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the logic flow is very straightforward in terms of client server communication, which is not represented here.",
                    "label": 0
                },
                {
                    "sent": "But I have data I've got.",
                    "label": 0
                },
                {
                    "sent": "For example, a gate service.",
                    "label": 0
                },
                {
                    "sent": "I want to basically be able to map that to a UIMA service and maybe then push that to something that comes.",
                    "label": 0
                },
                {
                    "sent": "From open NLP, perhaps something provided from Stanford.",
                    "label": 0
                },
                {
                    "sent": "All of these are going to be essentially completely compliant and play together in the same in the same environment, the same.",
                    "label": 0
                },
                {
                    "sent": "In the same general toolkit, and that is one of the advantages of this.",
                    "label": 0
                },
                {
                    "sent": "This is enabled through the exchange vocabulary that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As part of the web service.",
                    "label": 0
                },
                {
                    "sent": "Web service protocol.",
                    "label": 0
                },
                {
                    "sent": "The WCV has this terminology for any particular linguistic object that we're particularly interested in for an application and that is consuming or being consumed by a tool.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the exchange vocabulary has a type hierarchy with inheritance that here.",
                    "label": 1
                },
                {
                    "sent": "For example, we can see the different annotation types.",
                    "label": 0
                },
                {
                    "sent": "I've got paragraph sentences, noun chunks, verb chunks, etc etc.",
                    "label": 0
                },
                {
                    "sent": "Everything that we would be familiar with from the literature as consuming and generating data types.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Browse this if you want.",
                    "label": 0
                },
                {
                    "sent": "Add vocab lapse grid.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I will skip over that now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we've done in the context of a Mellon grant that we had for.",
                    "label": 0
                },
                {
                    "sent": "Looking at some of the Hathitrust data with Indiana and Illinois over the last three years, we deployed the lapse grid in the context of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They hardly trust.",
                    "label": 0
                },
                {
                    "sent": "Data capsule.",
                    "label": 0
                },
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "Kathy Trust Research Center.",
                    "label": 0
                },
                {
                    "sent": "Created a data capsule.",
                    "label": 0
                },
                {
                    "sent": "So that researchers could explore the in Copyright component of the Hathitrust data.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That meant we had to deploy the.",
                    "label": 0
                },
                {
                    "sent": "Laps as.",
                    "label": 0
                },
                {
                    "sent": "Something which would have secure access and stay within the data capsule.",
                    "label": 0
                },
                {
                    "sent": "So this was actually a very interesting challenge, so I'm sure you're familiar with the trust.",
                    "label": 0
                },
                {
                    "sent": "It's 50 million volumes that were the scanned digitized content from Google that has been part of the research library.",
                    "label": 0
                },
                {
                    "sent": "Of Illinois and Indiana.",
                    "label": 0
                },
                {
                    "sent": "Another preserves and the.",
                    "label": 0
                },
                {
                    "sent": "The challenge from Indiana, Illinois was to say, OK, we have a lot of tools for doing topicalization, topic spotting and some basic OCR, but what about named entity recognition without parsing?",
                    "label": 0
                },
                {
                    "sent": "What about some more sophisticated tools so they came to us to see whether or not we could actually deploy that in the context of a secure environment within a virtual virtual machine and that that project just concluded quite successfully with.",
                    "label": 0
                },
                {
                    "sent": "With us showing that these tools within that data capsule are deployed.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the structure for that was deployed as Docker images in the data capsule that were dedicated and did not communicate with anything outside of the data capsule itself.",
                    "label": 0
                },
                {
                    "sent": "Any tool that we had that was requested by the researchers were any tool was actually native to the server that was directly dedicated to the data capsule.",
                    "label": 0
                },
                {
                    "sent": "And that was actually really very interesting project.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "We removed the Galaxy front end as essentially something that wasn't necessary because we didn't need.",
                    "label": 0
                },
                {
                    "sent": "They didn't really need to have sophisticated pipelining and workflow structure.",
                    "label": 0
                },
                {
                    "sent": "As I said, that was deployed as a Docker swarm on the physical machine.",
                    "label": 1
                },
                {
                    "sent": "That allowed the in Copyright data to be secure and licensing issues were never really compromised.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "What this allowed to the researchers was enhanced search and discovery.",
                    "label": 1
                },
                {
                    "sent": "And this was complementing actually quite nicely.",
                    "label": 1
                },
                {
                    "sent": "The traditional volume level bibliographic metadata which was being created by the Hathitrust in terms of what they were contributing to the metadata for search.",
                    "label": 0
                },
                {
                    "sent": "They're all they were adding to that named entities events as well as other multi word expressions and things which might be more CL related data types from any R for example.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next.",
                    "label": 0
                },
                {
                    "sent": "Effort we had, which goes towards creating this interest in the multimodal, and video data was.",
                    "label": 0
                },
                {
                    "sent": "Started by this.",
                    "label": 0
                },
                {
                    "sent": "Collaboration that Nancy and I had with Earhart in Tubingen Jan in Prague, which was essentially away.",
                    "label": 0
                },
                {
                    "sent": "Of testing whether the authentication and other aspects of interoperability between Claire, Indian, Linda, Claire, and.",
                    "label": 0
                },
                {
                    "sent": "Could be linked to the laps tools and the laps architecture.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first thing that was on the plate in terms of.",
                    "label": 0
                },
                {
                    "sent": "What we needed to address was the authentication an authorization.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The first the first challenge was essentially addressed by prog and then tubing and associates at Vassar.",
                    "label": 0
                },
                {
                    "sent": "To try to find the best means of.",
                    "label": 0
                },
                {
                    "sent": "Allowing for institutional.",
                    "label": 0
                },
                {
                    "sent": "Access from the current claren.",
                    "label": 0
                },
                {
                    "sent": "And the current laps users and user base to make sure that we could utilize and access seamlessly each others applications.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of the.",
                    "label": 0
                },
                {
                    "sent": "Step that was required after the authentication and.",
                    "label": 0
                },
                {
                    "sent": "The authentication infrastructure was in place is to essentially ensure syntactic and semantic interoperability.",
                    "label": 0
                },
                {
                    "sent": "And this is going to be the key for moving into multimodal data as well, so.",
                    "label": 0
                },
                {
                    "sent": "Web licked uses a TCF text corpus format, while laps has the laps interchange format.",
                    "label": 0
                },
                {
                    "sent": "The The Protocols are different between both domains, so we needed to be able to map 1 to the other through converters.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which were created.",
                    "label": 0
                },
                {
                    "sent": "The vocabulary from lapse and the schema set from TCF.",
                    "label": 0
                },
                {
                    "sent": "Found their ways.",
                    "label": 0
                },
                {
                    "sent": "Into a concept mapping index which allows for this type of transfer and conversion quite quite successfully.",
                    "label": 0
                },
                {
                    "sent": "So the semantic interoperability allowed for.",
                    "label": 1
                },
                {
                    "sent": "The mapping between the basic core applications that we've looked at so far, much more work needs to be done to look at more sophisticated and broader range of applications.",
                    "label": 0
                },
                {
                    "sent": "That's current work that we're actually pursuing right now.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The obvious difference between our being soap and.",
                    "label": 0
                },
                {
                    "sent": "Rabbits being restful was.",
                    "label": 0
                },
                {
                    "sent": "The requirement of a converter between those and the metadata converter was.",
                    "label": 0
                },
                {
                    "sent": "Constructed and successfully deployed now.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gives rise to a very nice integration between web list and the laps.",
                    "label": 0
                },
                {
                    "sent": "Grid, which is summarized here where I have converters for the formats, the interchange formats.",
                    "label": 0
                },
                {
                    "sent": "That allow me to essentially go in through either one.",
                    "label": 0
                },
                {
                    "sent": "I can go in through web based or through apps deploy and call and actually pipeline a tool from either location.",
                    "label": 0
                },
                {
                    "sent": "Which is really, really nice.",
                    "label": 0
                },
                {
                    "sent": "There are so many other so many resources that are available through Claire and that we have no access to.",
                    "label": 0
                },
                {
                    "sent": "We just don't have in our toolkit, but we can actually have access to from the marriage of these two.",
                    "label": 0
                },
                {
                    "sent": "So this integration, interoperability that to go back to Antonio Examplea and early work from the early calls from Nicoletta from 25 years ago about interoperability.",
                    "label": 0
                },
                {
                    "sent": "Are partially achieved by this type of careful mapping of the protocols, the interchange formats and the infrastructures.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That leads me to.",
                    "label": 0
                },
                {
                    "sent": "The interest that is at hand now, which is to try to bring some of these tools.",
                    "label": 0
                },
                {
                    "sent": "2 Morse to Richard.",
                    "label": 0
                },
                {
                    "sent": "Data types that involve more than just language multimedia.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the clams architecture is really to be.",
                    "label": 0
                },
                {
                    "sent": "In the service of what is called an eye, a moving image archives.",
                    "label": 0
                },
                {
                    "sent": "And this is an entire area that I'm just becoming familiar with.",
                    "label": 0
                },
                {
                    "sent": "So Mia and the associated.",
                    "label": 0
                },
                {
                    "sent": "Datasets of video which many of you here in the audience and the Claron workshop have been dealing with, for I believe for many years.",
                    "label": 0
                },
                {
                    "sent": "Require different interchange formats because you're dealing with video you're dealing with with images.",
                    "label": 0
                },
                {
                    "sent": "You're dealing with time based content and indexing.",
                    "label": 0
                },
                {
                    "sent": "That are just not part of the problem of text analysis, so.",
                    "label": 0
                },
                {
                    "sent": "This is a kind of general archivist, researcher to a web browser has a front end.",
                    "label": 0
                },
                {
                    "sent": "Workflow engine essentially able to load up.",
                    "label": 0
                },
                {
                    "sent": "Some data and perform some analysis.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the interchange formats for laps and for many of the applications within Claire and Dr. And Linda are text based.",
                    "label": 0
                },
                {
                    "sent": "Or language based.",
                    "label": 0
                },
                {
                    "sent": "The annotations within laps are in lift and they are always anchored to a character offset.",
                    "label": 0
                },
                {
                    "sent": "So we did show that we have successful integration with bibliographic data in the potty, but we need to.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Move.",
                    "label": 0
                },
                {
                    "sent": "Two image archives and moving image archive.",
                    "label": 1
                },
                {
                    "sent": "So there's two different types of archival sets.",
                    "label": 0
                },
                {
                    "sent": "There's images as in.",
                    "label": 0
                },
                {
                    "sent": "Flickr 30 K An visual genome and just other sorts of CPR based datasets, which are of course used in the vision community, but also moving image archives.",
                    "label": 1
                },
                {
                    "sent": "So within MI Amia you're going beyond text data.",
                    "label": 1
                },
                {
                    "sent": "Time based media, video and audio and we need to do an analysis of time based media.",
                    "label": 1
                },
                {
                    "sent": "So it's not based on characters, obviously.",
                    "label": 1
                },
                {
                    "sent": "So, but of course, as you know, the results of such mihn will be subject to linguistic analysis.",
                    "label": 0
                },
                {
                    "sent": "If I want to do character analysis or content analysis of text within an image, and so audios can be transcribed text data as we know with with alignment and videos have text inside this caption, logo, subtitles, etc etc.",
                    "label": 1
                },
                {
                    "sent": "Now this is an area of quite considerable research, but there's still very little diploid work on this.",
                    "label": 0
                },
                {
                    "sent": "That is actually available to a working archivist.",
                    "label": 0
                },
                {
                    "sent": "So what we need are analytic tools for multimodality.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've been working on a multimedia interchange format in myth which handle video, audio, text on image and transcripts.",
                    "label": 1
                },
                {
                    "sent": "This creates an alignment between these different media and annotation types and then subsequently allows for multimodal analysis.",
                    "label": 1
                },
                {
                    "sent": "So we have the initial.",
                    "label": 0
                },
                {
                    "sent": "Initial draft of this and it's deployed.",
                    "label": 1
                },
                {
                    "sent": "I'll show you in a demo if the servers up.",
                    "label": 0
                },
                {
                    "sent": "And this has easy IO.",
                    "label": 0
                },
                {
                    "sent": "Believe it should be no significant difficulty in mapping this to lift and then there by two TCF in Vebb licked for use.",
                    "label": 0
                },
                {
                    "sent": "Any tool that would be available in.",
                    "label": 0
                },
                {
                    "sent": "Claire and D for example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, the way this would work is I have an MF encoded video.",
                    "label": 0
                },
                {
                    "sent": "I do a demux on it, I get my audio on my video track.",
                    "label": 0
                },
                {
                    "sent": "And I able to.",
                    "label": 0
                },
                {
                    "sent": "Essentially I need to have pointers to the external files.",
                    "label": 0
                },
                {
                    "sent": "I need to have pointers to the internal content, internal regions, etc etc.",
                    "label": 0
                },
                {
                    "sent": "As you would have for region analysis and then doing analysis of subregions of text.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the type hierarchy for clams is.",
                    "label": 1
                },
                {
                    "sent": "I have in terms of the media type I've got time based data, video and audio character based data can be conventional linguistic data.",
                    "label": 1
                },
                {
                    "sent": "And then I have region based data which is bounding boxes on still images.",
                    "label": 1
                },
                {
                    "sent": "Mutation types are going to be down to the media type obviously, and then I'll have special multimodal alignments such as a forced alignment and character based things like that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll have character start end for text for named entity tokenization classic things like that for video and audio.",
                    "label": 1
                },
                {
                    "sent": "I'm going to have time stamps, frame numbers etc etc.",
                    "label": 0
                },
                {
                    "sent": "I'm going to want to do for example Ching.",
                    "label": 1
                },
                {
                    "sent": "I'm going to want to story segmentation sound classification to recognize things that are not not speech but are trains and whistles and dogs.",
                    "label": 0
                },
                {
                    "sent": "And then for image annotation I've got bounding box, classic bounding box annotation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The conventional time timestamp boxes I'm going to have essentially one index to a particular time, and I'm going to have ID block that I'm going to have from frame to frame in order to be able to do Co reference.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This classic classic.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rotation of that.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll show you in a second, the actually.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is time to do it the platform prototype.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me go.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through the workflows.",
                    "label": 0
                },
                {
                    "sent": "So we have news hours and video from the the 90,000 assets that WGH American Archive has.",
                    "label": 0
                },
                {
                    "sent": "You're able to select an asset video.",
                    "label": 0
                },
                {
                    "sent": "You're able to select maybe a nice transcript if you want to do forced alignment.",
                    "label": 0
                },
                {
                    "sent": "We are able to perform some filtering such as bar and tone detection.",
                    "label": 0
                },
                {
                    "sent": "For example, I'll show you some of these and then do some noise segment filtering and.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I'll show you another workflow, which is to do a forced alignment with and without a bar and tone filtering.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you now some of the.",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "To this.",
                    "label": 0
                },
                {
                    "sent": "OK, hopefully this will be working.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I'll select a Connecticut public television.",
                    "label": 0
                },
                {
                    "sent": "Text.",
                    "label": 0
                },
                {
                    "sent": "So this is you see that this is an example.",
                    "label": 0
                },
                {
                    "sent": "This is an example of the video for the 1st.",
                    "label": 0
                },
                {
                    "sent": "30 to 60 seconds I have bars and tone.",
                    "label": 0
                },
                {
                    "sent": "Then I have this late.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what I want to do is this late?",
                    "label": 0
                },
                {
                    "sent": "I'd like to be able to just pause that for a second that's late.",
                    "label": 0
                },
                {
                    "sent": "Is something that we'd like to do text segment, text region analysis.",
                    "label": 0
                },
                {
                    "sent": "So there's a nice text region analysis algorithm that was just published last year at CPR, which uses neural net architecture which will recognize the text segment and then we perform tesseract to do OCR on that and we get a about a 90% accuracy.",
                    "label": 0
                },
                {
                    "sent": "The time expression if I perform.",
                    "label": 0
                },
                {
                    "sent": "If I do the OCR and then I can run that through one of the lapse applications such as Heidel time which is wrapped heidel time will recognize that.",
                    "label": 0
                },
                {
                    "sent": "Convert that to ISO time L which then allows me to create the metadata which then goes back to the metadata format addition of the asset, which then could hopefully eventually be added to something like vlog for search.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So that's one thing that we need to do.",
                    "label": 0
                },
                {
                    "sent": "There's graphics on the page.",
                    "label": 0
                },
                {
                    "sent": "This is the all these things are quite difficult to recognize.",
                    "label": 0
                },
                {
                    "sent": "I'm sure many of you know this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Little stylized and then there's another thing which is really quite difficult is what's called the Chiron, the lower 1/3, which I'm sure you're familiar with when you have the lower 1/3.",
                    "label": 0
                },
                {
                    "sent": "Video.",
                    "label": 0
                },
                {
                    "sent": "Well this is very slow.",
                    "label": 0
                },
                {
                    "sent": "OK let me just stop.",
                    "label": 0
                },
                {
                    "sent": "OK so maybe just so.",
                    "label": 0
                },
                {
                    "sent": "So let's just say stop this guy.",
                    "label": 0
                },
                {
                    "sent": "So I have.",
                    "label": 0
                },
                {
                    "sent": "I want to basically show I'm going to add the service of East detection.",
                    "label": 0
                },
                {
                    "sent": "That's the frame.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to add the service of doing an OCR.",
                    "label": 0
                },
                {
                    "sent": "OK, and then then I'll perform Heidel time.",
                    "label": 0
                },
                {
                    "sent": "OK. And then I deploy the workflow.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I download it and I got a sample myth output which basically is this wrapped data that I'm not going to go through and examine, But basically is something which would be compliant with the ISO time L. Format which I mentioned before another example would be to take.",
                    "label": 0
                },
                {
                    "sent": "News hour.",
                    "label": 0
                },
                {
                    "sent": "And let's say I want to forced alignment.",
                    "label": 0
                },
                {
                    "sent": "So I take an audio and video and I take a video and I take a clean transcription that has no trend that has no alignment.",
                    "label": 0
                },
                {
                    "sent": "Ann, I want to apply.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The MFA, the Montreal 4 cyliner.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I could actually I could do bars and tone detection which actually cleans it up a little bit.",
                    "label": 0
                },
                {
                    "sent": "So because the bars and tone will actually take out some of the confusion that the Montreal forced aligner actually runs into, and then I deploy it and the same thing it'll output and I'll get the forced alignment output that I don't need to show you.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "There one of the other tools that we've been deploying is something called Elf, which is the event loop.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station Finder.",
                    "label": 0
                },
                {
                    "sent": "So basically it's given A-frame in a video.",
                    "label": 0
                },
                {
                    "sent": "What's the most likely seen type?",
                    "label": 1
                },
                {
                    "sent": "So this is scene recognition.",
                    "label": 0
                },
                {
                    "sent": "Now this has worked.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've been doing for a number of years in the context of multimodal lexecon construction, something that started in coling 2016 whenever how you ever had a workshop on on basically resources that we were interested in terms of multimodal resources and creating a lexecon where you have information associated with a number of modalities, not just a semantic type, but also the video or visual image of something associated with it, as well as its acoustics or sound.",
                    "label": 0
                },
                {
                    "sent": "So in the context of this work, we have seen types and they specify where events happen, so seem recognition is of course a very very steady problem in CPR and vision, but it's not necessarily adapted to the type of data that we have here.",
                    "label": 1
                },
                {
                    "sent": "An we are benchmarking our algorithm against the algorithms that have not been trained on the data for within GBH and of course by.",
                    "label": 0
                },
                {
                    "sent": "Virtue of our training we're performing better than, say, the places to places.",
                    "label": 0
                },
                {
                    "sent": "Two algorithms from MIT, so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's just let me just walk you through a little bit of how this works.",
                    "label": 0
                },
                {
                    "sent": "Do I have like 5 minutes?",
                    "label": 0
                },
                {
                    "sent": "What do I have?",
                    "label": 0
                },
                {
                    "sent": "Thank thank you.",
                    "label": 0
                },
                {
                    "sent": "So first we started, we created an event localization corpus which was founded on the Flickr 30 K. You're familiar with 30K, is having essentially annotations describing each image so.",
                    "label": 1
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classic sort of five annotations for a picture, but what we do so this is these are crowdsourced annotations that Flickr provided.",
                    "label": 0
                },
                {
                    "sent": "For images, and so a man and two kids on a boat, two children wearing lifejackets.",
                    "label": 1
                },
                {
                    "sent": "What we did.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we took those and we crowdsourced annotations of where it was happening.",
                    "label": 0
                },
                {
                    "sent": "According to the places to hierarchy which.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was essentially.",
                    "label": 0
                },
                {
                    "sent": "Either indoor or outdoor weather was man made.",
                    "label": 0
                },
                {
                    "sent": "Indoor, or whether if it was outdoor, whether it was a natural outdoor scene or an artificial outdoor scene such as a sports field or something like that.",
                    "label": 0
                },
                {
                    "sent": "These are some of the classic hierarchical hierarchical classifications used in vision and.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we did was we adapted this by training it on some of the news hour data.",
                    "label": 0
                },
                {
                    "sent": "So the question is, given an image, what's the most likely location type?",
                    "label": 1
                },
                {
                    "sent": "So we did a pre trained places 2 model RESNET 5050 layer convolutional neural net Anna text model which was also a 2G R or NN and.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That gave good results for the Flickr.",
                    "label": 0
                },
                {
                    "sent": "Now then we took that model and adapted it.",
                    "label": 0
                },
                {
                    "sent": "We trained it on the news hour data specifically, so we took one video and then we extracted the frames, one frame per second and annotated with the scenes such as this is indoor.",
                    "label": 0
                },
                {
                    "sent": "This is a this is a studio shot and we train the model on the annotation frames.",
                    "label": 0
                },
                {
                    "sent": "Now what we did.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do with the change the ontology to adapt to essentially.",
                    "label": 0
                },
                {
                    "sent": "TV news, as it were, so we had bars and tones, blank screens, graphics on screen, guest in a studio guest out of studio ahead with the graphic.",
                    "label": 1
                },
                {
                    "sent": "Things like this so they.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, these are the kinds of scene ontologies barn tone I've got.",
                    "label": 0
                },
                {
                    "sent": "Text in a picture I've got just graphics that is on the record.",
                    "label": 0
                },
                {
                    "sent": "I've got the classic talking head with a graphic, then I've got a talking head outside and then natural a person.",
                    "label": 0
                },
                {
                    "sent": "In front of a natural in front of an artificial outdoor scene and then so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so these were.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we took that and did transfer learning.",
                    "label": 0
                },
                {
                    "sent": "We train the last layer to classify according to this new ontology.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And without some of the merging of categories, we got a 73% accuracy by category.",
                    "label": 0
                },
                {
                    "sent": "You can look at the results here, but there.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some interesting things to point out, so this was really hard to point out because effectively this is a Reporter at a desk versus a guest in a studio, and what you really what the model actually had to do was to identify an individual such as layer or mcniell, whatever some individual versus some studio guest.",
                    "label": 0
                },
                {
                    "sent": "So this is actually we combine this talking head category and we achieved over 86% F measure.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing which was difficult, which is graphics on a scene versus on the left versus this other thing which was a video it was it was a show about an art.",
                    "label": 0
                },
                {
                    "sent": "And art show.",
                    "label": 0
                },
                {
                    "sent": "And there are other things which were very confusing, such as online shopping, which were web pages and things like that.",
                    "label": 0
                },
                {
                    "sent": "So if we if we combine those, we essentially can merge the results of that and get actually something close to 87%.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OCR within a within a box.",
                    "label": 0
                },
                {
                    "sent": "OCR in characters in various patterns does not work well, and so this is because.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this is a digital slate?",
                    "label": 1
                },
                {
                    "sent": "OK, now in the early copies early video as you are aware.",
                    "label": 0
                },
                {
                    "sent": "If you have access to.",
                    "label": 0
                },
                {
                    "sent": "Two shows before the 80s.",
                    "label": 0
                },
                {
                    "sent": "You have slates which are handwritten, and those are quite difficult, but they are at least on a clear background on a steady background.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting about this is check this out.",
                    "label": 0
                },
                {
                    "sent": "This is a this is a slate.",
                    "label": 0
                },
                {
                    "sent": "Notice the date which we've recognized with Heidel time through this various MMIFF mapping to lift too.",
                    "label": 0
                },
                {
                    "sent": "Heidel time OK, but notice the mention of the director and the producer.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so East, which is this neural net text box recognizer recognizes those we do.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OCR on that, here's the bottom third, which is more difficult to recognize because of the noise behind it.",
                    "label": 1
                },
                {
                    "sent": "Notice the white of the shirt, so this this comes out quite difficult to understand, however.",
                    "label": 0
                },
                {
                    "sent": "By virtue of doing some sort of entity coreference we have referenced of Unger in Douglas as as information that allows us to say this could be Douglas because I'm not really reading that.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you've got the credit roll, which also has additional information that we can use to create additional support for a particular hypothesis in our model, that maybe that is Douglas.",
                    "label": 0
                },
                {
                    "sent": "And maybe that is Unger, for example.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we've done is forced alignment example that I showed you.",
                    "label": 1
                },
                {
                    "sent": "We are currently using the Montreal 4 cyliner.",
                    "label": 0
                },
                {
                    "sent": "We'd like to use assets and tools that are part of the web link resource kit and we're talking to Ehrhardt about that and but this essentially is really critical for multimodal analysis, which allows us to do.",
                    "label": 1
                },
                {
                    "sent": "Eventual text tiling and then segmentation in a Ching format so.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the forced alignment is is still very limited in terms of just the transcription based forced alignment, but we've been experimenting with is to use some of the tech scene.",
                    "label": 0
                },
                {
                    "sent": "The scene classification to give us kind of prior to the idea of where is this Ching actually starting and also the bars and tone identification also greatly enhances.",
                    "label": 0
                },
                {
                    "sent": "Some of the segment boundary identification.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just close up here.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I hope I've given you some not totally incoherent summary of what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "This is really early work.",
                    "label": 0
                },
                {
                    "sent": "A lot of it's really green, and we're probably repeating mistakes and false starts that many of you have done and overcome.",
                    "label": 0
                },
                {
                    "sent": "Maybe perhaps many years ago, but.",
                    "label": 0
                },
                {
                    "sent": "We were having fun doing it the.",
                    "label": 0
                },
                {
                    "sent": "Future work is to continue on with the chaptering.",
                    "label": 0
                },
                {
                    "sent": "Some of the scene recognition is work that we're continuing anyway with our multimodal work.",
                    "label": 0
                },
                {
                    "sent": "And I think the multimodal entity Coreference which I mentioned just now.",
                    "label": 1
                },
                {
                    "sent": "He's actually quite promising to help do some of the disambiguation as well as to help enhance some of the information coming from the sparse models that we get for OCR.",
                    "label": 0
                },
                {
                    "sent": "Obviously, much of this relates to the content based video retrieval work that's gone on for years, and I think some.",
                    "label": 0
                },
                {
                    "sent": "Interesting shared work and shared challenges could be developed around these types of multimodal tasks that are not just the kind that have been promoted by by NIST, but in fact maybe something that's closer to the hearts of the Claron community, and we could perhaps talk about that.",
                    "label": 0
                },
                {
                    "sent": "I think I'll just close there.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}