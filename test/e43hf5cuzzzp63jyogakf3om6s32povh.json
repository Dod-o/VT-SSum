{
    "id": "e43hf5cuzzzp63jyogakf3om6s32povh",
    "title": "Clustering - An overview",
    "info": {
        "author": [
            "Marina Meila, Department of Statistics, University of Washington"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_meila_co/",
    "segmentation": [
        [
            "Good morning and welcome to the last day of this summer school.",
            "Today will be a day entirely devoted to clustering.",
            "And believe me, after this day you will still still be at the beginning of clustering.",
            "I'll never be even close to finishing of what is possible to say about clustering.",
            "So let's go on with the.",
            "Lecture 2."
        ],
        [
            "This is the outline of the talk.",
            "What is in Gray is what interesting, so we have covered this area that are now Gray areas.",
            "I hope that they are not Gray areas in your mind though, so we talked about various frameworks of in which we can formulate clustering and we talked about the distinction between parametric versus nonparametric clustering among others.",
            "It is important to make this distinction and to, although I have said that this distinction is slightly blurred and it's artificial in many ways, it is worthwhile remembering that there is a distinction.",
            "Clustering problems are not all alike.",
            "And we'll come back to this today and again later today this morning.",
            "And then again later in the afternoon.",
            "In particular, it's important to know whether in your problem.",
            "The number of clusters is known, and it's a small number.",
            "That will somehow implicitly go with knowing the number of clusters or if.",
            "There is a large number of clusters that you have no idea how many clusters there are.",
            "In fact, there are so many outliers, outliers that actually can be thought of as clusters containing one point, in which case the number of clusters doesn't matter very much because it will fluctuate from 1 sample to the next.",
            "Since the number of outliers can fluctuate.",
            "And we will talk about that.",
            "We'll continue talking about vector data.",
            "We go now to discuss some issues that are fundamental to clustering.",
            "Mostly the parametric clustering because they have to do with selecting the number of clusters, and that's one part that's important in parametric clustering and handling outliers."
        ],
        [
            "So what is the problem of selecting K in practice?",
            "So the algorithm that I have presented either model based or cost based assume that K is given, so they take K as an input, the number of clusters.",
            "But this is a an assumption made for the convenience of the person designing the algorithm.",
            "In practice.",
            "Usually we don't know the number of clusters, that's part of the exploration of the data.",
            "How many groups there?",
            "That's the most frequent case, and so we have to every algorithm that takes K as an input has to also have a way to have has to be coupled with a method that actually determines K. In practice, this is done in the following way.",
            "We assume we tried to run the algorithm with.",
            "Several case you know you know chosen range.",
            "Each of the each of the runs gives US1 clustering.",
            "That's presumably the best we could find for that particular K. And then we have a different algorithm or a different method that chooses of these clusterings.",
            "Which is the best overall clustering?",
            "So we choose K by choosing the best clustering from clustering with different numbers of clusters.",
            "Now, of course, each clustering has a cost, so you could just say take all the clustering and pick the one that has the lowest cost.",
            "But by now, after having seen so much about overfitting, you'll realize that this is not a good approach.",
            "Becausw in practically every method that.",
            "Um?",
            "Cost base or model based that I can think of.",
            "If you add one more cluster then you can model you have one more set of you have more parameters to model the data with and so you are expecting the cost to decrease.",
            "And so if you plot the cost with respect to K, what you're going to see is a curve that goes.",
            "More or less descending from K = 2, three 45 to larger numbers.",
            "And so just comparing control costs is not a good strategy, and again.",
            "Thinking of what is known from other parts of machine learning, in particular supervised learning or estimation.",
            "Cost is penalized with something that.",
            "Accounts for the number of parameters that we have used to fit the data to obtain that cost.",
            "One of one approach comes from statistics is a very general approach coming from a part of statistics that called Bayesian statistics and that is the Bayesian information criterion, the BIC.",
            "So the BIC lets us select.",
            "Model and model being represented by the set of parameters status of K which are the parameters of the clustering with K cluster.",
            "The best clustering with K clusters that we could found fine.",
            "This criterion works for mixture models, so for model based clustering for probabilistic models of is more general than mixture.",
            "It works for general statistical models and the philosophes that follow.",
            "I compute the number that is.",
            "Logarithm of elevator.",
            "This is the likelihood of the data with parameter status of K and so the higher this number the better.",
            "Hope there is a mistake here.",
            "Because this number should get as high as possible.",
            "Yeah, and then I add a penalty, which is the number of parameters in the model times logarithm of N. Log out of the number of data points and so.",
            "This should be with the minus.",
            "Sorry for the typo, there is a minus yes, so if I put more if K is larger than the likelihood of the data increases because I fit the data better.",
            "But the number of parameters also increases and so this term is abstract.",
            "So abstract.",
            "I have two increasing curves that ice abstract, so I hope to get at least one pic.",
            "I hope so.",
            "There's no proof that or there's a proof in the limit that I will yes, and so the second term is meant to balance the first term and it comes from Bayesian theory.",
            "It has to.",
            "It assumes that.",
            "The data themselves come from a distribution.",
            "And the probability of getting the right data is somehow penalized by by the second term.",
            "Now, just to make sure that you understand what is the number of what number, do we put here.",
            "That's because any mixture can be parameterized by in several ways, and they're not.",
            "They may not have the same number of parameters.",
            "Beta K is a number of independent parameters in data in the model, yes.",
            "So for example, if I take a mixture of Gaussians with K groups K components, each of them being a Gaussian with full covariance matrix, then I would have the following.",
            "I would have came in.",
            "Minus one parameters for the parameter pies yes, because the pie sum to one.",
            "So I have K values for the cluster probabilities, but one of them is dependent on the others and then for each mixture I'm going to have the parameters for the mean and D choose two for the entries in the covariance matrix.",
            "Which can all be independent if I choose now a model that has.",
            "Let's say a spherical covariance matrix.",
            "Maybe with one Sigma?",
            "Then for each mixture model I would have one parameter for a covariance matrix and then instead of this D * D -- 1 / 2, I would have one.",
            "Yes, so counting the number of parameters in the model is not hard, but it's something that you need to understand.",
            "And remembering that this is a -- 2.",
            "And so the the philosophy or the.",
            "Technique is to compute this be I see value for every.",
            "Modeling the under consideration and then choose the one that has the highest value.",
            "So the highest penalized likelihood?",
            "And then if we are looking for.",
            "Since you're comparing mixtures of Gaussians, this will be the model that will give us the number of clusters.",
            "Notice that it's it's not quite so simple problem cause I could compare mixtures of Gaussians with spherical clusters with mixtures of Gaussians with.",
            "With less constrained covariance matrices and then the.",
            "The number of clusters would depend on exactly what kind of covariance structure I would have, so maybe the winner among spherical covariance matrices will have a higher K and the winner about over unconstrained covariance matrices will have a lower key.",
            "However, the PC let us compare.",
            "This is not comparing apples and oranges.",
            "They can be compared, and if you're on the overall winner, then is the one with the highest score.",
            "What do you know about this criteria?",
            "It we know that for very large N asymptotically, and we don't know what very large is, it depends on the program.",
            "It will select the true model.",
            "So if the model is truly a mixture of Gaussians.",
            "And with parameters in a compact set that we know, and if we know the range of case which is not too large, then this.",
            "Criterion will select the true model and under similar assumptions, if the true model is not a mixture.",
            "So we have almost infinite data, so we actually can get the true model.",
            "If we want is the true model is not among the ones that we have tried, it will select the one that's closest.",
            "In likelihood, so it's strong in the case.",
            "Where?",
            "We have enough data which is true for about a lot of clusters.",
            "And in the case that we try a model that is somehow related to the data, yes.",
            "This criterion, as I said, works for mixture models.",
            "What do we do for other models?",
            "Well, four other models of clustering we like which are not based on probabilistic.",
            "On probability, the situation is less, it's less sure what to do.",
            "So what exists are only heuristics, and I'm going to give you a few of them that have worked reasonably well in the past.",
            "Or in experiment.",
            "The last one I'm going, I'm not going to present you in detail is a K means algorithm that does search over K. It's not the only one, this is just an example.",
            "Of of such an algorithm is an algorithm where during the optimization that should of came in now and then there are some decision steps where the algorithm asks instead of just recomputing a center and you in the K means algorithm algorithm, ASK itself, shall I?",
            "Split this cluster.",
            "Maybe this cluster could be split into two and that would give us a better model, and in that case it tries to find the best split and attempt some.",
            "Make the decision based on some tests of gaussianity.",
            "If this cluster looks now, there are statistical tests that let you decide whether data could have come from a Gaussian distribution or not, and so it applies a test to the cluster and says well if this data could have come from a Gaussian distribution, I'll leave it as such.",
            "If not, if it seems that the cluster is very oddly shaped, then maybe I'll try to split it and gets to round 1/2.",
            "And there is a similar strategy for deciding whether to merge two clusters.",
            "Again, it is a heuristic, and it's sort of.",
            "It's not clear what it does, because at this point it doesn't even go to a local optimal over cost function.",
            "The other two are also heuristics, but some of.",
            "Both of them have some statistical.",
            "Theory behind.",
            "So what is the gap statistic?"
        ],
        [
            "The guest statistiques of statistic applies to.",
            "Practically any Costel daughters have a favorite cost that they have applied experimented with, but it could apply in theory to any cost.",
            "By the way, statistic means anything that you compute from the data.",
            "Yes.",
            "So any number that you compute from the data like the mean of the data or.",
            "The cost of the clustering or so.",
            "So this idea is based on statistical testing.",
            "And it's based on comparing the clustering that was obtained with a model that has.",
            "With and it's cost with the cost of a clustering with K clusters on on a data set that has no clusters at all.",
            "So what do we expect it?",
            "We have a.",
            "We have a data set.",
            "We obtain a clustering, will compute its cost and it has K clusters.",
            "Then we generate some other datasets, let's say a uniform distribution over us.",
            "In the same range where the our current data lie.",
            "Oil.",
            "Compute what the cost would be if we cluster that with K clusters.",
            "We actually.",
            "Compute an expectation over all possible samples of that distribution if we can anyway compute what the cost would be on that distribution has no clusters, and what do we expect?",
            "Expect that cost will be higher, because if there are clusters in the data, then and exactly K of them, then the data will be more compact around the clusters, and so I'll get a lower cost.",
            "I want to minimize so the well cluster data will have a better cost, and then I compute the gap, which is the cost on the data with no clusters and the custom my data and if the gap is large it means that there is a strong argument for the fact that my data is clustered.",
            "That's the idea.",
            "And this distribution, which has no clusters is called the null distribution, and it's chosen to be Gaussian.",
            "So this is some artificial data that I generate.",
            "This is Gaussian or uniform or what they used and seems to work better is.",
            "If the data is in high dimension, first find the principal subspace and then sample uniformly from that.",
            "So basically you want it to be uniform but of the true dimensionality of the data.",
            "And again this case, you know doesn't have to do with the number of clusters, it's just the real dimensionality of the data.",
            "And then from this artificial data you compute the cost of the clustering.",
            "This is expectation under this null distribution of the cost for endpoints and K clusters.",
            "And ignore the fact that probably you don't know how to compute that.",
            "For very simple distribution you could.",
            "And low dimensions maybe?",
            "And then the gap is this expectation minus the cost that I have, and I'm going to call this expectation LO.",
            "And then the larger the gap, the better the clustering that I have compared to this null model, which is the baseline.",
            "Therefore I choose the K that maximizes the gap.",
            "What's nice is that if I have small gaps for all case, it means that I have no clusters.",
            "Which is another problem with cluster with clustering algorithms that they always return a clustering and if the data really has no clusters, you'd like to know that and the algorithm will not tell you."
        ],
        [
            "OK, that was the theory or the idea in practice.",
            "Typically this quantity cannot be computed.",
            "Well, that's not such a big problem.",
            "What you can do is to just generate a sample, cluster it.",
            "Generate another sample, cluster it again, do it as many times as you have time.",
            "It's fairly expensive.",
            "And then.",
            "Average of the cost that you obtain, and that's an estimate for.",
            "But average cost that you want in for the baseline.",
            "And then use that estimates to compare with your true cost.",
            "With that you have.",
            "Now, of course, if you estimate something from samples, there is some variability.",
            "Just because you are sampling and that is captured by the variance in the cost.",
            "And you don't want to if you want to make a decision to be statistically significant, you don't want to make two things that are within one standard deviation from each other.",
            "Can't consider them different.",
            "They could actually be equal.",
            "And therefore they don't take these choices to take the smallest K such that its gap is greater than the the next gap.",
            "Minus one standard deviation, yes.",
            "So the first gap that is significantly smaller than the others.",
            "Is there strategy?",
            "Again, this is what this is.",
            "This is obtained partly from Thierry, but partly by trial and error.",
            "Why?",
            "Why is the?",
            "First one.",
            "Again, this is this is heuristics.",
            "Because that they favor is very similar to the K means distortion cost.",
            "And in fact, the experiments are many of them are done with mixtures of Gaussian.",
            "So there is a.",
            "An underlying assumption that you're looking for Gaussian clusters or round clusters, although there is not a model based clustering, so the cost is the sum of the squared errors to the cluster means.",
            "Divided by the points in the cluster.",
            "So it's like if you think that this device by the points in the cluster is an estimation of it's an estimate of the variance.",
            "Of that particular cluster cluster, then this LV is like the sum of variances.",
            "It's been used by other researchers and it has some qualities.",
            "For example, it's more or less data independent and weights all clusters equally.",
            "On the other hand, you may not want to weigh all clusters equally, like for example if there is one small cluster, large variance.",
            "That contains outliers, then that would increase the cost very much.",
            "So this is something to take with a grain of salt and maybe put your own favorite criterion there."
        ],
        [
            "And now for another heuristic.",
            "That has the merit that it has worked well compared with others with several other heuristics.",
            "So in some sense it's it's also winner.",
            "This has to do with the same cost, which is the sum of the variances LV.",
            "But the authors thought of it in the following way.",
            "If how does this cost vary with K and the dimension?",
            "And they found their asymptotics, arguments that under the null distribution under some load distribution.",
            "This is very this way like K squared over.",
            "Be.",
            "To the power two 2 / D. Yeah, so each dimension.",
            "And then.",
            "For a good clustering.",
            "What I would want so we can think of what happens to the cost when when there are K clusters.",
            "If I put.",
            "If I try with a smaller K than the true K. Then in the best case, or in some cases I have a cluster that contains two true clusters.",
            "Or more.",
            "Because I must fit all the real clusters in a smaller number of clusters, and as I increase K towards the true value.",
            "I let the cluster spread and occupied by each their own.",
            "Two clusters spread and each being in their own cluster, so that the distortions, distortions like this and variances will decay very fast.",
            "So here is some L, maybe LV, maybe some other yes.",
            "And let's say that.",
            "This is a true number of clusters as I'm trying values with.",
            "Lower number and what I know is that the cost will decrease, but.",
            "There are two different situations when I have two few clusters then.",
            "Suppose these are the true clusters, three clusters.",
            "If I use two clusters, then I'm going to have a cluster with large variance and one is small variance in the best, and then when I jump from two to three, suddenly the variance will jump from this value to this app.",
            "Yes, so I'm going to have large jumps down to the truth through the K star, so the criterion will go like this after that when I try 4 clusters, then typically I'm going to have two centers here and then.",
            "If I put 5 maybe I'll have something like this.",
            "So now what I'm doing is I'm sort of filling the space, filling the space where the data like with more and more centers.",
            "This is called vector quantization.",
            "When I actually just imputing center somewhere to quantize the data.",
            "So it's a very different regime, and then there is actually some work predicting how this cost will decrease asymptotically.",
            "If I add more and more points.",
            "But it's not.",
            "There's never a large jump because I'm just taking a few points from this center and adding another center, and so the decay will be.",
            "Much slower.",
            "And so one intuitive way of looking for the true number of clusters is to look for.",
            "They need the curve.",
            "But of course I have looked at a lot of queries with a lot of needs and.",
            "Is there is a lot of variation so you can see when you're out here, but where exactly is the knee is very hard to describe by by sort of formula so that you can put in a computer program in any case.",
            "This is one way these people did it by looking at the jump so.",
            "A lot of junk means that I'm.",
            "I'm approaching the truckie.",
            "But I'm here in this side.",
            "And then they said I'm going to look for the largest jump, but they probably found that this jumps are larger and then the jumps progressively decrease, so they're going to look for the largest jump relative to the next step.",
            "This part is better motivated.",
            "This part is less less well motivated.",
            "OK, I will stop for quest for a minute for questions here just to like just as a moral, don't take of algorithm at computer at face value yes, think what you could use, what's useful and could be used from an algorithm.",
            "What you could change and improve because sometimes you can make improvements.",
            "That's another reason I presented this.",
            "OK, let's see if there are questions.",
            "Then"
        ],
        [
            "I look at another set of methods for choosing K which are completely different.",
            "And they're justified intuitively, but they also people have tried them, and they seem to work well in experiments.",
            "They're called stability methods.",
            "So what is the idea?",
            "The idea is that if you're going to find structuring data.",
            "Then that if you change a few, only a few points then you should still be able to find the same structure.",
            "So at the high level nothing should change.",
            "And so everything that's not every algorithm not stable to a small perturbation of the data should be suspected.",
            "It will mean it can be used for the problem, or its results can be interpreted.",
            "Because they are too sensitive to changes in data.",
            "And so.",
            "What is this is a very like is a incontestable truth, I think.",
            "These methods use the converse, which may not be true, which is if you find something that stable, it means it reflects structure in data.",
            "So what they do is avoid.",
            "You have a data set.",
            "If for turbot.",
            "You have a clustering on D. You have a clustering on the perturbed data set and then you compare the two.",
            "Yes.",
            "I'm sorry.",
            "I think the microphone is.",
            "OK, so the question was what is the data really changes because it's.",
            "Save.",
            "A stream of data, like a video stream or stream of news or dynamic system and the question is that these are all very simple algorithms that assume that the data always come from the same distribution.",
            "There you have to define, so a way of looking at data that is dynamic is to define a time scale where something is preserved and then try to find structure at that time scale.",
            "But yeah, you can just take these algorithms and apply them to data that changes because they assume data comes from the same distribution.",
            "If there is any distribution at all.",
            "And in particular, here I I perturb the data myself, and at this provision should be small, so that theoretically the deprime could have come from the same distribution SD.",
            "So one way of perturbing the data is to actually sampling from the data with replacement.",
            "Then you get a data set that contains the same points.",
            "But maybe if you are missing, and if you are just a few times there repeated once or twice, and that's actually called the Bootstrap is a very.",
            "While we use technique in statistics.",
            "OK, so if these are dissimilar.",
            "Then we should suspect both of them, because we know that one of them is wrong.",
            "At least one, and in fact people.",
            "But if we repeat this many times and this team over and over and over again, then the two clusterings are similar.",
            "Then maybe there is a reason to believe that we have found something in the data, especially if they are more similar only at 1K and not so more variable at different case.",
            "And so this is the idea was implemented for every K. We resample the data many times and look how close this clustering czar.",
            "For this you have to have a distance to measure clustering and talk about distances later and you actually this clustering because the data set is perturbed may not contain not may not be over the same number of points, so you have to take only the points that are common to the 2 two clusterings, but these are all details.",
            "And in fact, what they did experiments showed that very often what happens is that.",
            "For the 3K.",
            "All the clusterings they get seem to be similar, whereas for the other values of K there is more much more variation.",
            "There is another method, so this can work for any clustering method, provided it's a hard clustering because we don't really know how to compare soft clustering, that's why.",
            "Um?",
            "Another idea which works.",
            "And has been implemented by for model based clustering.",
            "Is the following.",
            "I divide the data set into two halves and the cluster both with the EM algorithm.",
            "So I get both the clustering and the data.",
            "And of course even gives soft clusterings, but you can always transform them into hard clustering for the purpose of comparison.",
            "And then the next thing to do is to take the parameters from the second set and cluster again the first set.",
            "With them that's because if you assume that the datasets both come from the same distribution as they are since you just divide the data.",
            "Then and you have learned the true distribution, then if the data to another one should be very similar.",
            "But you don't compare that data.",
            "If you compare the results of the clustering.",
            "So you just apply data 2 to D1 and get another clustering of the first data set, and now you compare the two clusterings.",
            "And if they are almost the same, if they are similar, then it means that.",
            "You have two independent witnesses.",
            "The first date data set and the second data set on which you have gotten the same, the same structure.",
            "So maybe you have found the true structure.",
            "And then you can repeat this several times to make sure that.",
            "Um?",
            "That you're not.",
            "Basically, to reduce the effects of chance and random fluctuations.",
            "And this also has been tried on datasets and has worked.",
            "In practice, however.",
            "So they have been tried and many other people have tried working on these methods because they are interesting.",
            "However, there is no theory to support them, and in fact there are.",
            "There is a recent theoretical result which say that these methods should not work.",
            "OK, so when you see something like this then sort of go and look there because there is something interesting happening.",
            "So and work is in progress and probably new results will be coming.",
            "Or another model is don't take any of these results is final.",
            "Because probably the problem is not solved yet and there is another yet another way to look at the problem.",
            "And now and then it happens that an algorithm is not supposed to work, does work.",
            "But that's a rare chance, so we'll see the jury still Albanese.",
            "Let me see if we have.",
            "Let me see if you have any questions about the stability methods and then we'll go to outliers.",
            "Yes.",
            "Look at their data.",
            "One that one part is that based on the hard clustering.",
            "Yes, they're hard clustering, and again this is only big cause there is no good method of comparing soft clusterings to date and the results are actually sensitive to what distance they use.",
            "I don't work for some distances.",
            "OK."
        ],
        [
            "So I would like to see the statistical term.",
            "And it means that.",
            "It's a point that's not typical for that particular distribution.",
            "So if, for example, if you have a Gaussian distribution, then it's a point that's far away from the mean.",
            "So it's a point that will have appear very rarely if you sample data from the distribution, or possibly it's a corruption of the data somebody entered the data.",
            "One had a typo and they they typed that particular value, and so you get a number that's not very likely to appear from the distribution.",
            "And so for distributions like the Gaussian distribution, where the probability of an outlier is very small.",
            "Basically, you know that.",
            "99% of the data are within three Sigma from the mean.",
            "So everything away is has a very small chance of appearing.",
            "An outlier should be cause of some concern for an algorithm or an algorithm assumes that the data don't have many outliers because the data are Gaussian, and that's what most people do.",
            "Outline how to concern.",
            "They will not be a concern if you assume that you're the distributions of your clusters are what's called heavy tail, like our allow many points to be away for them from the mean.",
            "And this is important also when you don't have mixtures becausw many other costs implicitly.",
            "As I showed for the single linkage method, can be can be disturbed very much by an outlier sensitive to our lives.",
            "And so we have to think of what to do when the data contains some outliers.",
            "The problem is that all the remedies depend on what there is no general solution.",
            "Actually, the general solution is to go to nonparametric clustering.",
            "But if you want to stay with parametric clustering, then you have to do some ad hoc tweaking for outliers.",
            "One of them if you have mixture models.",
            "Is to put all the outliers in a in a fictitious cluster.",
            "You introduce a cluster with a very large Sigma.",
            "And that thing we don't train.",
            "So I think that as large as a signal the data, basically some sort of uniform distribution overall, where the data could be.",
            "And everything that's an outlier.",
            "That's not very likely to come from the other clusters we put there.",
            "There is another.",
            "Remedy, and that is to estimate the clusters by robust method methods.",
            "So robust methods are a branch of statistics that is meant to deal with.",
            "Specifically with outliers, so the assumption is that you have a distribution like the Gaussian which has few outliers.",
            "But that's corrupted by 2% or 3% of data from another distribution which is not from the distribution.",
            "Um?",
            "And then what do you do?",
            "Because the way of computing the mean of a Gaussian which is to sum up all the data is actually fairly sensitive to.",
            "112 hours.",
            "And there are books written on robust statistics how to make an algorithm that is not.",
            "It's almost optimal when the data comes from the from the two distribution, but it doesn't degrade, but it doesn't degrade badly when the data are corrupted with outliers, and one of the simplest ways of doing it is by what's called a trimmed mean and the trim variance, and that's the following.",
            "Basically, you solve the data in any dimension and then remove the highest 2% and the lowest 2%, or the highest 1% and the lowest 1%.",
            "And of course, in the day to happen to come from a Gaussian then you are.",
            "You're disturbing the result a little, but you can show that in an average that is the error introducing is small.",
            "And the potential error that you are removing if there actually those outliers don't come from the data is the benefit is great when actually you have true errors in the data.",
            "And that's called trim mean because you just trim the top and the bottom of the of the coordinates.",
            "And there are similar methods for the variance and so on.",
            "Very intuitive.",
            "And that can be done at every step of re estimation or in here in the EM algorithm or in the K means algorithm.",
            "Or everywhere where I mean is estimated.",
            "There is another remedy, which is that instead instead of to change the cost function.",
            "Basically the quadratic cost is more sensitive to outliers than the same cost without the square.",
            "And so if I want to end that is called the K Millions algorithm, because this is.",
            "Because the minimizer of that cost is the median of the data.",
            "Yeah, let me make a little drawing to show you.",
            "So if the data are on this line.",
            "Here is the.",
            "This is the mean of the data.",
            "The main minimizes the cost X minus mu squared.",
            "Some over access.",
            "And now I delete this square and I want to minimize sum over X of.",
            "I don't need this.",
            "Just X minus nu.",
            "But minimizer of this cost is the medium.",
            "Which would be 123 here.",
            "Actually, probably.",
            "OK, and that's why the algorithm is called K medians.",
            "Now the median has a nice property with respect to outliers that I can take this point and move it far away out of the room.",
            "The median will not change at all.",
            "So the median is sensitive.",
            "The median is actually, the is not.",
            "If I don't make any change in the middle of the data, the median will not change at all, so it's much more robust to outliers and therefore an algorithm uses.",
            "This cost will automatically be robust.",
            "What I can say about this cost is that we don't know how to minimize, so it's NP hard as well, and it's in some way.",
            "Hi.",
            "No, I shouldn't say that, but there are some computer science algorithms that can give some fairly complicated.",
            "That's why I didn't include them in the in the handouts that can actually give some guarantees, As for example the.",
            "I think a factor of four approximation on the cost.",
            "So this means, again a factor of four approximation means that.",
            "The algorithm guarantees to get.",
            "At most four times the true cost.",
            "So if the data are well clustered and the true cost is small, then that's not too bad.",
            "Or is the data really compact?",
            "Then there's not too bad.",
            "If the data are clustered, but.",
            "Not really compactly, then it's also.",
            "And other methods that are applied to single linkage or.",
            "Other algorithms are really sensitive to outliers.",
            "Hard to eliminate small clusters.",
            "Yes, so if you have a cluster with two or three points then that's considered or one point which can come out in a single link as you just eliminate it, eliminate that point and count your clusters without that one.",
            "But before we quit, the subject of outliers, let's think of the following.",
            "This razor what's an outlier is something sometimes the philosophical problem or it's a problem that.",
            "Statistics alone cannot solve.",
            "You have to think what you really want.",
            "And one of the problems that they raise is that if you have outliers so points that are clusters that contain one point, that could be another definition of outliers.",
            "Then does the number of clusters make sense?",
            "And if there are not too many of them, or if you conclude that all the points that are singletons are some sort of errors, then it does make sense.",
            "But if there are many of them, or if you have clusters with one point but also clusters with two points and clusters with three points and clusters with five points.",
            "Then which would you stop calling them outliers and start calling them clusters?",
            "Again, maybe the if the boundary is not clear, then maybe you should not.",
            "And of course if you have many clusters with three or four points, then the number of classes is not well defined.",
            "Becauses another sample of the data will give you slightly different cluster sizes, or some of them will actually disappear.",
            "And not appear in every sample.",
            "However, you may not want to ignore them because they may be there like there may be something that happens rarely.",
            "A small cluster.",
            "In this situation though, the K, the number of clusters doesn't make any sense.",
            "Because it may be that in any instantiation of the data will not see the same key.",
            "Or you will not see all the games and that is the philosophical problem.",
            "When does it?",
            "When is there a fixed number of clusters and when there isn't and if there isn't, then we should go and look at nonparametric clustering because that is a problem that actually finds as many clusters as there are small or large and doesn't doesn't put a specific premium on this number.",
            "K uses other parameters to describe how densely the data is clustered.",
            "Let me see if there are questions about outliers.",
            "OK."
        ],
        [
            "So.",
            "Now we are starting so we have for now we are leaving parametric clustering and going to another part of clustering.",
            "Another way of doing clustering which is nonparametric."
        ],
        [
            "And here again, there is this.",
            "There is a split between.",
            "In two kinds of methods, one that have a probabilistic model and are based on probabilistic estimation, just like mixture models, and in fact they are generalization of mixtures mixture models.",
            "And other algorithms which are based on a completely different.",
            "Completely different approach and we started those.",
            "So we talk now about algorithms based on nonparametric density estimation.",
            "And let's see how many of you know what nonparametric density estimation is.",
            "Can be nobody.",
            "So let's explain it.",
            "So assume my data is in one dimension and.",
            "And.",
            "This is the data.",
            "Can you see?",
            "Yeah well your hands if you can't see these little dots.",
            "You can't.",
            "Ah, OK.",
            "So do you see any clusters in the data?",
            "That's another question.",
            "In clustering, is that if the data clustering at all and will talk about cluster at all, and I'll talk about it later, but one another question I would I'm not asking, but if you think of it is how many clusters do you see if you see?",
            "Um?",
            "But one way of thinking of clusters is places where the data is close together and another way of calling a place where the data is close together is the data has high density.",
            "So this is a probability density is a function F of X.",
            "That integrates one, and that describes the data density of this along this axis, for example.",
            "Or in the domain where the data like.",
            "And so if I knew that.",
            "If instead of the data I had the data density, so here is the data density.",
            "Then I could define the clusters to be the peaks of this function.",
            "And all the methods that I'm going to talk about are about this finding either the peaks of this distribution and so how many picks do I have?",
            "12345 of these peaks or not?",
            "I guess that's a question.",
            "And each peak would be considered a cluster.",
            "A nice thing about the method is that.",
            "First of all, pics are not that I'm not that easy to find, but there are still matters to find the pics.",
            "Also, another interesting feature of the method is that if you only find one peak, you know that the data that has no clusters.",
            "Also, if there are points which lie under no peak at all, except this is.",
            "Let's say don't call these peaks, they're too small.",
            "Then outliers are.",
            "Taking out the very naturally, we don't have to do anything about outliers, because if they're not under one peak, then.",
            "They are not clustered.",
            "And there is a series of methods that that.",
            "Take this approach.",
            "But to do this first you have to have a smooth function that covers the data.",
            "And that is called the density estimate.",
            "So this if you can draw this F of X from data data density estimate.",
            "And this is the estimation can be nonparametric at itself and the methods that are used here are called typically use what's called kernel density estimation.",
            "Now this Colonel.",
            "Is not like the kernel in kernel methods.",
            "And it's as follows.",
            "So what I mean by current?"
        ],
        [
            "The kernel this estimation is that you choose some sort of.",
            "Shape like a bump that you call the curve.",
            "And then this function is constructed by putting a little bump on every point.",
            "Centered on every point.",
            "And then.",
            "To get this Hill, I basically add up this this little bumps.",
            "And so if I add up these three functions, I get.",
            "Something like this because they overlap.",
            "Here what they will not overlap or overlap only on the tail I will get a separate bump for each data point.",
            "And for example, this is a Gaussian kernel, like a little Gaussian shape on every point.",
            "And in general, K can be any density, but I prefer something that's sent like that symmetric and smooth, and so yes, so I can take any function that is that integrates into a finite number.",
            "But I have to have good reasons to choose something that's not strange and odd shape.",
            "This so the resulting F is called a nonparametric estimate estimator of the density.",
            "However, it does have a parameter and this parameter is H. Which is called the kernel width.",
            "And if you look carefully at this formula, if H is large.",
            "Then this bump the bump.",
            "There will be wide, and then the function will, so if every bump is wide, if you add them up, you're not going to get any sharp peaks in the distribution.",
            "Yes, so this means that the distribution that the resulting density is smooth.",
            "Smooth here doesn't mean whether it has derivatives or not.",
            "It means whether it varies fast or slowly.",
            "Yeah, and if H is very small then I'm going to have very sharp peaks on each point and so the resulting estimate will be non smooth or very jacked.",
            "Yes, so this is the parameter of this function and it's called the smoothness parameter because it controls the smoothness.",
            "After density estimation of the resulting F that I'm computing.",
            "And now I hope it's.",
            "You will not be surprised that if I choose a different age.",
            "Something like this narrow?",
            "I'm going to get a different shape for this.",
            "So with a function like this, I'm going to get something.",
            "More like this.",
            "And so instead of 1 peak here, I'm going to have many peaks.",
            "So choosing age for density estimation affects the the cluster that I get.",
            "So in one case all the points were in one cluster.",
            "In other case, which is in their own cluster or their outliers, yes.",
            "So.",
            "This means that.",
            "Oh, before I go on, let's see if you have questions about density estimation or non per hour kernel density estimation in particular.",
            "No questions.",
            "Alright, so.",
            "Then let's see what we can do with this."
        ],
        [
            "So what kind of things we can do with kernel density estimators for clustering?",
            "This is 1 algorithm that does.",
            "The following is computer kernel density estimator F4H chosen ahead of time and actually that's an issue?",
            "How do you choose H?",
            "You must basically one way to choose the H is to know something about the kind of clusters you want to see, like, how large are the clusters.",
            "So when you do, clusterings generally have to know you have to know what what you're looking for and that you can't escape this.",
            "Even in nonparametric density estimation.",
            "And then after you build this blue function, then you compute for various values.",
            "You basically threshold it with lines at different Heights.",
            "And then at each level you look for, you know how many connected components are above this line, how many peaks are above that line, and that gives you a cluster.",
            "So for example, if I threshold here, I will see one peak 2345.",
            "But if I put the threshold lower like here, I may see no peek at all, which means that at this level the data all in one cluster.",
            "Here I have five clusters plus some outliers, and at this level here I have only one cluster left.",
            "Yes.",
            "And so the algorithm produces not just one clustering, But basically all the possible clusterings when you move this line from zero to.",
            "Above the highest peak of the distribution.",
            "Yes.",
            "Face.",
            "I.",
            "How behavior?",
            "Oh, thank you.",
            "This is a very good question.",
            "In fact I don't know if it's not at the bottom of the page.",
            "This doesn't work.",
            "So you spotted this.",
            "There are various reasons why this can't really work in high dimensions.",
            "And one of them is that actually kernel density estimation doesn't work in high dimensions too well.",
            "Practically, for several reasons, so this will work in in the second problem, so everything that's in green is.",
            "Difficult or difficult in high dimensions is so.",
            "First of all, it's hard to get a higher kernel density estimate in high dimensions.",
            "Can you see why?",
            "Because typical in high dimensions.",
            "Like let's say in 10 dimensions in 10 dimensions.",
            "If you break every direction into two, you have about 2 to the 10th.",
            "About 1000 little cubes.",
            "If you just break the dimension to two left and right, and so if you want to have a data point in each of these, you need 1000 data points.",
            "Of course, if the data is clustered then it will not be spread.",
            "So basically we need.",
            "A lot of data to fill the space, so most of the space is empty and lots of points are very far apart, and so if you put spheres on top of the points, this case will not overlap very much unless the data is really well clustered.",
            "And so it's very hard to construct to adjust the size of the current into constructor, yes, so why can't it be fixed by adjusting that scaling parameter, let it grow with the dimension or something like that.",
            "So there are some methods that do that, like ball trees where you actually put put spheres that cover several.",
            "Several things, but that's not.",
            "So bowl trees are, as far as they are understood, understood as a data structure.",
            "It's not understood what their asymptotic properties are like does it?",
            "Does that like if you're doing density estimation, statisticians want to know whether it converges and so on and.",
            "In high dimension you just make the bump flatter.",
            "Why does that work?",
            "You talked about before.",
            "If you make the product that bumps peaked, then it doesn't really want to mention so in high dimension you just use ladder bumps.",
            "Does that?",
            "Why can't you compensate for dimensionality?",
            "Oh because yeah, because then you won't see peaks.",
            "If you make flatter bumps then then what you want is to see the clusters and so if you make them yes you can make the basically have to make the bump almost.",
            "Tesla as flat as all the data on one dimension.",
            "Anne.",
            "Also, there is another reason.",
            "When the data are in high dimensions very often.",
            "Their true dimension is not a dimension of the space, so they lie along strings in the space and or they lie along lower dimensional curved manifolds, and so you can put spheres the sphere contains.",
            "A lot of empty space.",
            "So that's another reason why.",
            "Why kernel density estimation is in this standard version of most versions don't work in high dimensions.",
            "You want to actually put the bumps.",
            "Make them very elongated in the direction sort of data line so.",
            "This simple kernel density estimation doesn't go far in D and also finding level sets is not easy if you are in more than one dimension.",
            "Yes is damage in two dimensions.",
            "There are methods if you go for level sets in three and four and five dimensions you will see that it's much harder, but there actually this can be fixed.",
            "And they fixed it by actually looking only at the values of the Bay City at the data points.",
            "Which means that if there is a peak between the data, they will not find it.",
            "But then again, a peak between the data means that you didn't have enough data to see the peak.",
            "OK, so if you have higher dimensions then.",
            "You compute the levels.",
            "That's only for the point.",
            "So you compute only what which of the data is still higher than the line or not.",
            "Also, there is a new version of this algorithm where instead of kernel density estimator you they build a spanning tree of the data and the spanning tree.",
            "Because it's based on distance is insensitive to dimension and then they try to look at the density along the edges of the tree, which is more sensible because then you are going to look basically only between data points.",
            "You're not going to wonder in the large empty space around the points and that should work for higher dimensions.",
            "OK, and to find connected components in low dimensions is not that hard in high dimensions.",
            "Basically you look at the density along the segment between the data.",
            "And the idea is that if two points are in the same cluster.",
            "So under the same peak, then the segment connecting them should also be under the peak.",
            "So the density on any point between XI and XJ should be also high, and so they sample points on their segment and see whether the check whether the density is high or not.",
            "And then they put if two points are connected by segment that is inside the level set they are put in the same cluster and then it's easy to find the clusters by this.",
            "So this is a level set method.",
            "Oh this particular method in this particular method, if you care about the algorithm, it may seem at 1st that it's hopelessly.",
            "Expensive to compute all these level sets.",
            "It's not so big cause if you have information about the one level set.",
            "Then it is possible to so basically, if two points are in the same cluster at this level, they will not.",
            "We must know if two points are in different clusters.",
            "At one level they will not be merged in the next level, yes, so there is a lot of information that can be used from one level to the next to make the algorithm efficient.",
            "So fairly efficient algorithms exist that can do all the levels in one pass.",
            "They have also computed confidence intervals, which is.",
            "An important problem and it's related to what do I call it peak?",
            "Like for example, here is this one week or two weeks.",
            "And so their method.",
            "And I'm I'm mentioning it because it's a good and general method and it addresses a general issue in this kind of clustering is to actually resample the data.",
            "This method is called Bootstrap.",
            "In statistics and so re estimate the density the density and see whether one is the peak remains there and 2nd if how deep is the peak?",
            "Yes, so deep.",
            "Or the Valley?",
            "Yes, so small valleys don't count, and what small means depends on the variance of the density with respect to sampling the data, which is the correct thing to do?",
            "Because they have to define in some sense what is small and what is large."
        ],
        [
            "Oh actually, this is a good place to stop, so let me take some questions.",
            "Yes.",
            "You are beautiful.",
            "So what about that chicken recipe?",
            "Space, I don't know why it features this in this case this recipe, so this will use to overcome passcode.",
            "That is true.",
            "So here saying that there is.",
            "Nonparametric methods and density estimation is requires a lot of computation.",
            "That is true.",
            "Um?",
            "There are methods to accelerate the densities.",
            "Kernel density estimation that actually work very well.",
            "And the idea is.",
            "The following so when you compute the density here.",
            "At this point.",
            "You have to use all the data points.",
            "And I haven't said it explicitly, but I'm saying it now to do kernel to do nonparametric density estimation.",
            "You need a lot of data.",
            "Much more data than for parametric density estimation, becausw convergence, the convergence rates are much lower than than for parameter estimation as in the usual sense so.",
            "If you compute the density here, it's a sum over the curls on top of this data in theory, but in practice this point here has no influence on the density there, and there are methods which actually compute how far you need to look.",
            "Basically we need to look as far as the width of the kernel, so you need to look.",
            "You need to look only in this neighborhood and this neighborhood in high dimensions will contain few points.",
            "Basically you want.",
            "Actually, I think there's a small section of the data, hopefully for any good kernel density estimator you want it to be local, like you don't want this point here to influence that point there.",
            "Otherwise there is a little problem with being nonparametric.",
            "It becomes parametrically yes and so there are very efficient methods that exploit this in low and high dimensions they were going to hundreds of dimensions.",
            "That's not a problem computing this.",
            "Another issue is how do we use this estimator in clustering?",
            "We don't know if we have a fast method to compute it, then the methods for finding peaks are also local, meaning they use only information in the local neighborhood, so only a few points so they could be made so that is not they are intensive, but they're not.",
            "So.",
            "Proclivities in the sense that if you can handle.",
            "They are linear in the number of data points or.",
            "But but not quadratic usually.",
            "Yes, and so if you admit that you can handle anything that's proportional to the number of data points, then you can do that.",
            "Are there other questions?",
            "Yes.",
            "Other questions.",
            "OK, then I should stop now and continue after the break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning and welcome to the last day of this summer school.",
                    "label": 0
                },
                {
                    "sent": "Today will be a day entirely devoted to clustering.",
                    "label": 0
                },
                {
                    "sent": "And believe me, after this day you will still still be at the beginning of clustering.",
                    "label": 0
                },
                {
                    "sent": "I'll never be even close to finishing of what is possible to say about clustering.",
                    "label": 0
                },
                {
                    "sent": "So let's go on with the.",
                    "label": 0
                },
                {
                    "sent": "Lecture 2.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "What is in Gray is what interesting, so we have covered this area that are now Gray areas.",
                    "label": 0
                },
                {
                    "sent": "I hope that they are not Gray areas in your mind though, so we talked about various frameworks of in which we can formulate clustering and we talked about the distinction between parametric versus nonparametric clustering among others.",
                    "label": 0
                },
                {
                    "sent": "It is important to make this distinction and to, although I have said that this distinction is slightly blurred and it's artificial in many ways, it is worthwhile remembering that there is a distinction.",
                    "label": 0
                },
                {
                    "sent": "Clustering problems are not all alike.",
                    "label": 0
                },
                {
                    "sent": "And we'll come back to this today and again later today this morning.",
                    "label": 0
                },
                {
                    "sent": "And then again later in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "In particular, it's important to know whether in your problem.",
                    "label": 1
                },
                {
                    "sent": "The number of clusters is known, and it's a small number.",
                    "label": 0
                },
                {
                    "sent": "That will somehow implicitly go with knowing the number of clusters or if.",
                    "label": 0
                },
                {
                    "sent": "There is a large number of clusters that you have no idea how many clusters there are.",
                    "label": 0
                },
                {
                    "sent": "In fact, there are so many outliers, outliers that actually can be thought of as clusters containing one point, in which case the number of clusters doesn't matter very much because it will fluctuate from 1 sample to the next.",
                    "label": 0
                },
                {
                    "sent": "Since the number of outliers can fluctuate.",
                    "label": 0
                },
                {
                    "sent": "And we will talk about that.",
                    "label": 0
                },
                {
                    "sent": "We'll continue talking about vector data.",
                    "label": 1
                },
                {
                    "sent": "We go now to discuss some issues that are fundamental to clustering.",
                    "label": 0
                },
                {
                    "sent": "Mostly the parametric clustering because they have to do with selecting the number of clusters, and that's one part that's important in parametric clustering and handling outliers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the problem of selecting K in practice?",
                    "label": 1
                },
                {
                    "sent": "So the algorithm that I have presented either model based or cost based assume that K is given, so they take K as an input, the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "But this is a an assumption made for the convenience of the person designing the algorithm.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "Usually we don't know the number of clusters, that's part of the exploration of the data.",
                    "label": 0
                },
                {
                    "sent": "How many groups there?",
                    "label": 0
                },
                {
                    "sent": "That's the most frequent case, and so we have to every algorithm that takes K as an input has to also have a way to have has to be coupled with a method that actually determines K. In practice, this is done in the following way.",
                    "label": 0
                },
                {
                    "sent": "We assume we tried to run the algorithm with.",
                    "label": 0
                },
                {
                    "sent": "Several case you know you know chosen range.",
                    "label": 0
                },
                {
                    "sent": "Each of the each of the runs gives US1 clustering.",
                    "label": 0
                },
                {
                    "sent": "That's presumably the best we could find for that particular K. And then we have a different algorithm or a different method that chooses of these clusterings.",
                    "label": 0
                },
                {
                    "sent": "Which is the best overall clustering?",
                    "label": 0
                },
                {
                    "sent": "So we choose K by choosing the best clustering from clustering with different numbers of clusters.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, each clustering has a cost, so you could just say take all the clustering and pick the one that has the lowest cost.",
                    "label": 0
                },
                {
                    "sent": "But by now, after having seen so much about overfitting, you'll realize that this is not a good approach.",
                    "label": 0
                },
                {
                    "sent": "Becausw in practically every method that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Cost base or model based that I can think of.",
                    "label": 0
                },
                {
                    "sent": "If you add one more cluster then you can model you have one more set of you have more parameters to model the data with and so you are expecting the cost to decrease.",
                    "label": 0
                },
                {
                    "sent": "And so if you plot the cost with respect to K, what you're going to see is a curve that goes.",
                    "label": 0
                },
                {
                    "sent": "More or less descending from K = 2, three 45 to larger numbers.",
                    "label": 0
                },
                {
                    "sent": "And so just comparing control costs is not a good strategy, and again.",
                    "label": 0
                },
                {
                    "sent": "Thinking of what is known from other parts of machine learning, in particular supervised learning or estimation.",
                    "label": 0
                },
                {
                    "sent": "Cost is penalized with something that.",
                    "label": 0
                },
                {
                    "sent": "Accounts for the number of parameters that we have used to fit the data to obtain that cost.",
                    "label": 0
                },
                {
                    "sent": "One of one approach comes from statistics is a very general approach coming from a part of statistics that called Bayesian statistics and that is the Bayesian information criterion, the BIC.",
                    "label": 0
                },
                {
                    "sent": "So the BIC lets us select.",
                    "label": 0
                },
                {
                    "sent": "Model and model being represented by the set of parameters status of K which are the parameters of the clustering with K cluster.",
                    "label": 0
                },
                {
                    "sent": "The best clustering with K clusters that we could found fine.",
                    "label": 0
                },
                {
                    "sent": "This criterion works for mixture models, so for model based clustering for probabilistic models of is more general than mixture.",
                    "label": 0
                },
                {
                    "sent": "It works for general statistical models and the philosophes that follow.",
                    "label": 0
                },
                {
                    "sent": "I compute the number that is.",
                    "label": 0
                },
                {
                    "sent": "Logarithm of elevator.",
                    "label": 0
                },
                {
                    "sent": "This is the likelihood of the data with parameter status of K and so the higher this number the better.",
                    "label": 0
                },
                {
                    "sent": "Hope there is a mistake here.",
                    "label": 0
                },
                {
                    "sent": "Because this number should get as high as possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then I add a penalty, which is the number of parameters in the model times logarithm of N. Log out of the number of data points and so.",
                    "label": 0
                },
                {
                    "sent": "This should be with the minus.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the typo, there is a minus yes, so if I put more if K is larger than the likelihood of the data increases because I fit the data better.",
                    "label": 0
                },
                {
                    "sent": "But the number of parameters also increases and so this term is abstract.",
                    "label": 0
                },
                {
                    "sent": "So abstract.",
                    "label": 0
                },
                {
                    "sent": "I have two increasing curves that ice abstract, so I hope to get at least one pic.",
                    "label": 0
                },
                {
                    "sent": "I hope so.",
                    "label": 0
                },
                {
                    "sent": "There's no proof that or there's a proof in the limit that I will yes, and so the second term is meant to balance the first term and it comes from Bayesian theory.",
                    "label": 0
                },
                {
                    "sent": "It has to.",
                    "label": 0
                },
                {
                    "sent": "It assumes that.",
                    "label": 0
                },
                {
                    "sent": "The data themselves come from a distribution.",
                    "label": 0
                },
                {
                    "sent": "And the probability of getting the right data is somehow penalized by by the second term.",
                    "label": 0
                },
                {
                    "sent": "Now, just to make sure that you understand what is the number of what number, do we put here.",
                    "label": 0
                },
                {
                    "sent": "That's because any mixture can be parameterized by in several ways, and they're not.",
                    "label": 0
                },
                {
                    "sent": "They may not have the same number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Beta K is a number of independent parameters in data in the model, yes.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I take a mixture of Gaussians with K groups K components, each of them being a Gaussian with full covariance matrix, then I would have the following.",
                    "label": 1
                },
                {
                    "sent": "I would have came in.",
                    "label": 0
                },
                {
                    "sent": "Minus one parameters for the parameter pies yes, because the pie sum to one.",
                    "label": 0
                },
                {
                    "sent": "So I have K values for the cluster probabilities, but one of them is dependent on the others and then for each mixture I'm going to have the parameters for the mean and D choose two for the entries in the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Which can all be independent if I choose now a model that has.",
                    "label": 0
                },
                {
                    "sent": "Let's say a spherical covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Maybe with one Sigma?",
                    "label": 0
                },
                {
                    "sent": "Then for each mixture model I would have one parameter for a covariance matrix and then instead of this D * D -- 1 / 2, I would have one.",
                    "label": 0
                },
                {
                    "sent": "Yes, so counting the number of parameters in the model is not hard, but it's something that you need to understand.",
                    "label": 0
                },
                {
                    "sent": "And remembering that this is a -- 2.",
                    "label": 0
                },
                {
                    "sent": "And so the the philosophy or the.",
                    "label": 0
                },
                {
                    "sent": "Technique is to compute this be I see value for every.",
                    "label": 0
                },
                {
                    "sent": "Modeling the under consideration and then choose the one that has the highest value.",
                    "label": 0
                },
                {
                    "sent": "So the highest penalized likelihood?",
                    "label": 0
                },
                {
                    "sent": "And then if we are looking for.",
                    "label": 0
                },
                {
                    "sent": "Since you're comparing mixtures of Gaussians, this will be the model that will give us the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Notice that it's it's not quite so simple problem cause I could compare mixtures of Gaussians with spherical clusters with mixtures of Gaussians with.",
                    "label": 0
                },
                {
                    "sent": "With less constrained covariance matrices and then the.",
                    "label": 0
                },
                {
                    "sent": "The number of clusters would depend on exactly what kind of covariance structure I would have, so maybe the winner among spherical covariance matrices will have a higher K and the winner about over unconstrained covariance matrices will have a lower key.",
                    "label": 0
                },
                {
                    "sent": "However, the PC let us compare.",
                    "label": 0
                },
                {
                    "sent": "This is not comparing apples and oranges.",
                    "label": 0
                },
                {
                    "sent": "They can be compared, and if you're on the overall winner, then is the one with the highest score.",
                    "label": 0
                },
                {
                    "sent": "What do you know about this criteria?",
                    "label": 0
                },
                {
                    "sent": "It we know that for very large N asymptotically, and we don't know what very large is, it depends on the program.",
                    "label": 0
                },
                {
                    "sent": "It will select the true model.",
                    "label": 0
                },
                {
                    "sent": "So if the model is truly a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And with parameters in a compact set that we know, and if we know the range of case which is not too large, then this.",
                    "label": 0
                },
                {
                    "sent": "Criterion will select the true model and under similar assumptions, if the true model is not a mixture.",
                    "label": 0
                },
                {
                    "sent": "So we have almost infinite data, so we actually can get the true model.",
                    "label": 0
                },
                {
                    "sent": "If we want is the true model is not among the ones that we have tried, it will select the one that's closest.",
                    "label": 0
                },
                {
                    "sent": "In likelihood, so it's strong in the case.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "We have enough data which is true for about a lot of clusters.",
                    "label": 0
                },
                {
                    "sent": "And in the case that we try a model that is somehow related to the data, yes.",
                    "label": 0
                },
                {
                    "sent": "This criterion, as I said, works for mixture models.",
                    "label": 0
                },
                {
                    "sent": "What do we do for other models?",
                    "label": 0
                },
                {
                    "sent": "Well, four other models of clustering we like which are not based on probabilistic.",
                    "label": 0
                },
                {
                    "sent": "On probability, the situation is less, it's less sure what to do.",
                    "label": 0
                },
                {
                    "sent": "So what exists are only heuristics, and I'm going to give you a few of them that have worked reasonably well in the past.",
                    "label": 0
                },
                {
                    "sent": "Or in experiment.",
                    "label": 0
                },
                {
                    "sent": "The last one I'm going, I'm not going to present you in detail is a K means algorithm that does search over K. It's not the only one, this is just an example.",
                    "label": 0
                },
                {
                    "sent": "Of of such an algorithm is an algorithm where during the optimization that should of came in now and then there are some decision steps where the algorithm asks instead of just recomputing a center and you in the K means algorithm algorithm, ASK itself, shall I?",
                    "label": 0
                },
                {
                    "sent": "Split this cluster.",
                    "label": 0
                },
                {
                    "sent": "Maybe this cluster could be split into two and that would give us a better model, and in that case it tries to find the best split and attempt some.",
                    "label": 1
                },
                {
                    "sent": "Make the decision based on some tests of gaussianity.",
                    "label": 0
                },
                {
                    "sent": "If this cluster looks now, there are statistical tests that let you decide whether data could have come from a Gaussian distribution or not, and so it applies a test to the cluster and says well if this data could have come from a Gaussian distribution, I'll leave it as such.",
                    "label": 0
                },
                {
                    "sent": "If not, if it seems that the cluster is very oddly shaped, then maybe I'll try to split it and gets to round 1/2.",
                    "label": 0
                },
                {
                    "sent": "And there is a similar strategy for deciding whether to merge two clusters.",
                    "label": 0
                },
                {
                    "sent": "Again, it is a heuristic, and it's sort of.",
                    "label": 0
                },
                {
                    "sent": "It's not clear what it does, because at this point it doesn't even go to a local optimal over cost function.",
                    "label": 0
                },
                {
                    "sent": "The other two are also heuristics, but some of.",
                    "label": 0
                },
                {
                    "sent": "Both of them have some statistical.",
                    "label": 0
                },
                {
                    "sent": "Theory behind.",
                    "label": 0
                },
                {
                    "sent": "So what is the gap statistic?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The guest statistiques of statistic applies to.",
                    "label": 0
                },
                {
                    "sent": "Practically any Costel daughters have a favorite cost that they have applied experimented with, but it could apply in theory to any cost.",
                    "label": 0
                },
                {
                    "sent": "By the way, statistic means anything that you compute from the data.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So any number that you compute from the data like the mean of the data or.",
                    "label": 0
                },
                {
                    "sent": "The cost of the clustering or so.",
                    "label": 0
                },
                {
                    "sent": "So this idea is based on statistical testing.",
                    "label": 0
                },
                {
                    "sent": "And it's based on comparing the clustering that was obtained with a model that has.",
                    "label": 0
                },
                {
                    "sent": "With and it's cost with the cost of a clustering with K clusters on on a data set that has no clusters at all.",
                    "label": 1
                },
                {
                    "sent": "So what do we expect it?",
                    "label": 0
                },
                {
                    "sent": "We have a.",
                    "label": 0
                },
                {
                    "sent": "We have a data set.",
                    "label": 0
                },
                {
                    "sent": "We obtain a clustering, will compute its cost and it has K clusters.",
                    "label": 0
                },
                {
                    "sent": "Then we generate some other datasets, let's say a uniform distribution over us.",
                    "label": 0
                },
                {
                    "sent": "In the same range where the our current data lie.",
                    "label": 0
                },
                {
                    "sent": "Oil.",
                    "label": 0
                },
                {
                    "sent": "Compute what the cost would be if we cluster that with K clusters.",
                    "label": 0
                },
                {
                    "sent": "We actually.",
                    "label": 0
                },
                {
                    "sent": "Compute an expectation over all possible samples of that distribution if we can anyway compute what the cost would be on that distribution has no clusters, and what do we expect?",
                    "label": 0
                },
                {
                    "sent": "Expect that cost will be higher, because if there are clusters in the data, then and exactly K of them, then the data will be more compact around the clusters, and so I'll get a lower cost.",
                    "label": 0
                },
                {
                    "sent": "I want to minimize so the well cluster data will have a better cost, and then I compute the gap, which is the cost on the data with no clusters and the custom my data and if the gap is large it means that there is a strong argument for the fact that my data is clustered.",
                    "label": 0
                },
                {
                    "sent": "That's the idea.",
                    "label": 1
                },
                {
                    "sent": "And this distribution, which has no clusters is called the null distribution, and it's chosen to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this is some artificial data that I generate.",
                    "label": 0
                },
                {
                    "sent": "This is Gaussian or uniform or what they used and seems to work better is.",
                    "label": 0
                },
                {
                    "sent": "If the data is in high dimension, first find the principal subspace and then sample uniformly from that.",
                    "label": 0
                },
                {
                    "sent": "So basically you want it to be uniform but of the true dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "And again this case, you know doesn't have to do with the number of clusters, it's just the real dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "And then from this artificial data you compute the cost of the clustering.",
                    "label": 1
                },
                {
                    "sent": "This is expectation under this null distribution of the cost for endpoints and K clusters.",
                    "label": 0
                },
                {
                    "sent": "And ignore the fact that probably you don't know how to compute that.",
                    "label": 0
                },
                {
                    "sent": "For very simple distribution you could.",
                    "label": 0
                },
                {
                    "sent": "And low dimensions maybe?",
                    "label": 1
                },
                {
                    "sent": "And then the gap is this expectation minus the cost that I have, and I'm going to call this expectation LO.",
                    "label": 0
                },
                {
                    "sent": "And then the larger the gap, the better the clustering that I have compared to this null model, which is the baseline.",
                    "label": 0
                },
                {
                    "sent": "Therefore I choose the K that maximizes the gap.",
                    "label": 0
                },
                {
                    "sent": "What's nice is that if I have small gaps for all case, it means that I have no clusters.",
                    "label": 0
                },
                {
                    "sent": "Which is another problem with cluster with clustering algorithms that they always return a clustering and if the data really has no clusters, you'd like to know that and the algorithm will not tell you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that was the theory or the idea in practice.",
                    "label": 0
                },
                {
                    "sent": "Typically this quantity cannot be computed.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not such a big problem.",
                    "label": 0
                },
                {
                    "sent": "What you can do is to just generate a sample, cluster it.",
                    "label": 0
                },
                {
                    "sent": "Generate another sample, cluster it again, do it as many times as you have time.",
                    "label": 0
                },
                {
                    "sent": "It's fairly expensive.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Average of the cost that you obtain, and that's an estimate for.",
                    "label": 0
                },
                {
                    "sent": "But average cost that you want in for the baseline.",
                    "label": 0
                },
                {
                    "sent": "And then use that estimates to compare with your true cost.",
                    "label": 0
                },
                {
                    "sent": "With that you have.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if you estimate something from samples, there is some variability.",
                    "label": 0
                },
                {
                    "sent": "Just because you are sampling and that is captured by the variance in the cost.",
                    "label": 0
                },
                {
                    "sent": "And you don't want to if you want to make a decision to be statistically significant, you don't want to make two things that are within one standard deviation from each other.",
                    "label": 0
                },
                {
                    "sent": "Can't consider them different.",
                    "label": 0
                },
                {
                    "sent": "They could actually be equal.",
                    "label": 0
                },
                {
                    "sent": "And therefore they don't take these choices to take the smallest K such that its gap is greater than the the next gap.",
                    "label": 1
                },
                {
                    "sent": "Minus one standard deviation, yes.",
                    "label": 0
                },
                {
                    "sent": "So the first gap that is significantly smaller than the others.",
                    "label": 0
                },
                {
                    "sent": "Is there strategy?",
                    "label": 0
                },
                {
                    "sent": "Again, this is what this is.",
                    "label": 0
                },
                {
                    "sent": "This is obtained partly from Thierry, but partly by trial and error.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why is the?",
                    "label": 0
                },
                {
                    "sent": "First one.",
                    "label": 0
                },
                {
                    "sent": "Again, this is this is heuristics.",
                    "label": 0
                },
                {
                    "sent": "Because that they favor is very similar to the K means distortion cost.",
                    "label": 0
                },
                {
                    "sent": "And in fact, the experiments are many of them are done with mixtures of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "An underlying assumption that you're looking for Gaussian clusters or round clusters, although there is not a model based clustering, so the cost is the sum of the squared errors to the cluster means.",
                    "label": 0
                },
                {
                    "sent": "Divided by the points in the cluster.",
                    "label": 0
                },
                {
                    "sent": "So it's like if you think that this device by the points in the cluster is an estimation of it's an estimate of the variance.",
                    "label": 0
                },
                {
                    "sent": "Of that particular cluster cluster, then this LV is like the sum of variances.",
                    "label": 0
                },
                {
                    "sent": "It's been used by other researchers and it has some qualities.",
                    "label": 0
                },
                {
                    "sent": "For example, it's more or less data independent and weights all clusters equally.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you may not want to weigh all clusters equally, like for example if there is one small cluster, large variance.",
                    "label": 0
                },
                {
                    "sent": "That contains outliers, then that would increase the cost very much.",
                    "label": 0
                },
                {
                    "sent": "So this is something to take with a grain of salt and maybe put your own favorite criterion there.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now for another heuristic.",
                    "label": 0
                },
                {
                    "sent": "That has the merit that it has worked well compared with others with several other heuristics.",
                    "label": 0
                },
                {
                    "sent": "So in some sense it's it's also winner.",
                    "label": 0
                },
                {
                    "sent": "This has to do with the same cost, which is the sum of the variances LV.",
                    "label": 0
                },
                {
                    "sent": "But the authors thought of it in the following way.",
                    "label": 0
                },
                {
                    "sent": "If how does this cost vary with K and the dimension?",
                    "label": 0
                },
                {
                    "sent": "And they found their asymptotics, arguments that under the null distribution under some load distribution.",
                    "label": 0
                },
                {
                    "sent": "This is very this way like K squared over.",
                    "label": 0
                },
                {
                    "sent": "Be.",
                    "label": 0
                },
                {
                    "sent": "To the power two 2 / D. Yeah, so each dimension.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "For a good clustering.",
                    "label": 0
                },
                {
                    "sent": "What I would want so we can think of what happens to the cost when when there are K clusters.",
                    "label": 0
                },
                {
                    "sent": "If I put.",
                    "label": 0
                },
                {
                    "sent": "If I try with a smaller K than the true K. Then in the best case, or in some cases I have a cluster that contains two true clusters.",
                    "label": 0
                },
                {
                    "sent": "Or more.",
                    "label": 0
                },
                {
                    "sent": "Because I must fit all the real clusters in a smaller number of clusters, and as I increase K towards the true value.",
                    "label": 0
                },
                {
                    "sent": "I let the cluster spread and occupied by each their own.",
                    "label": 0
                },
                {
                    "sent": "Two clusters spread and each being in their own cluster, so that the distortions, distortions like this and variances will decay very fast.",
                    "label": 0
                },
                {
                    "sent": "So here is some L, maybe LV, maybe some other yes.",
                    "label": 0
                },
                {
                    "sent": "And let's say that.",
                    "label": 0
                },
                {
                    "sent": "This is a true number of clusters as I'm trying values with.",
                    "label": 0
                },
                {
                    "sent": "Lower number and what I know is that the cost will decrease, but.",
                    "label": 0
                },
                {
                    "sent": "There are two different situations when I have two few clusters then.",
                    "label": 0
                },
                {
                    "sent": "Suppose these are the true clusters, three clusters.",
                    "label": 0
                },
                {
                    "sent": "If I use two clusters, then I'm going to have a cluster with large variance and one is small variance in the best, and then when I jump from two to three, suddenly the variance will jump from this value to this app.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I'm going to have large jumps down to the truth through the K star, so the criterion will go like this after that when I try 4 clusters, then typically I'm going to have two centers here and then.",
                    "label": 0
                },
                {
                    "sent": "If I put 5 maybe I'll have something like this.",
                    "label": 0
                },
                {
                    "sent": "So now what I'm doing is I'm sort of filling the space, filling the space where the data like with more and more centers.",
                    "label": 0
                },
                {
                    "sent": "This is called vector quantization.",
                    "label": 0
                },
                {
                    "sent": "When I actually just imputing center somewhere to quantize the data.",
                    "label": 0
                },
                {
                    "sent": "So it's a very different regime, and then there is actually some work predicting how this cost will decrease asymptotically.",
                    "label": 0
                },
                {
                    "sent": "If I add more and more points.",
                    "label": 0
                },
                {
                    "sent": "But it's not.",
                    "label": 0
                },
                {
                    "sent": "There's never a large jump because I'm just taking a few points from this center and adding another center, and so the decay will be.",
                    "label": 1
                },
                {
                    "sent": "Much slower.",
                    "label": 0
                },
                {
                    "sent": "And so one intuitive way of looking for the true number of clusters is to look for.",
                    "label": 0
                },
                {
                    "sent": "They need the curve.",
                    "label": 0
                },
                {
                    "sent": "But of course I have looked at a lot of queries with a lot of needs and.",
                    "label": 0
                },
                {
                    "sent": "Is there is a lot of variation so you can see when you're out here, but where exactly is the knee is very hard to describe by by sort of formula so that you can put in a computer program in any case.",
                    "label": 0
                },
                {
                    "sent": "This is one way these people did it by looking at the jump so.",
                    "label": 0
                },
                {
                    "sent": "A lot of junk means that I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm approaching the truckie.",
                    "label": 0
                },
                {
                    "sent": "But I'm here in this side.",
                    "label": 0
                },
                {
                    "sent": "And then they said I'm going to look for the largest jump, but they probably found that this jumps are larger and then the jumps progressively decrease, so they're going to look for the largest jump relative to the next step.",
                    "label": 0
                },
                {
                    "sent": "This part is better motivated.",
                    "label": 1
                },
                {
                    "sent": "This part is less less well motivated.",
                    "label": 0
                },
                {
                    "sent": "OK, I will stop for quest for a minute for questions here just to like just as a moral, don't take of algorithm at computer at face value yes, think what you could use, what's useful and could be used from an algorithm.",
                    "label": 0
                },
                {
                    "sent": "What you could change and improve because sometimes you can make improvements.",
                    "label": 0
                },
                {
                    "sent": "That's another reason I presented this.",
                    "label": 0
                },
                {
                    "sent": "OK, let's see if there are questions.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I look at another set of methods for choosing K which are completely different.",
                    "label": 1
                },
                {
                    "sent": "And they're justified intuitively, but they also people have tried them, and they seem to work well in experiments.",
                    "label": 0
                },
                {
                    "sent": "They're called stability methods.",
                    "label": 0
                },
                {
                    "sent": "So what is the idea?",
                    "label": 0
                },
                {
                    "sent": "The idea is that if you're going to find structuring data.",
                    "label": 0
                },
                {
                    "sent": "Then that if you change a few, only a few points then you should still be able to find the same structure.",
                    "label": 0
                },
                {
                    "sent": "So at the high level nothing should change.",
                    "label": 0
                },
                {
                    "sent": "And so everything that's not every algorithm not stable to a small perturbation of the data should be suspected.",
                    "label": 0
                },
                {
                    "sent": "It will mean it can be used for the problem, or its results can be interpreted.",
                    "label": 0
                },
                {
                    "sent": "Because they are too sensitive to changes in data.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "What is this is a very like is a incontestable truth, I think.",
                    "label": 0
                },
                {
                    "sent": "These methods use the converse, which may not be true, which is if you find something that stable, it means it reflects structure in data.",
                    "label": 0
                },
                {
                    "sent": "So what they do is avoid.",
                    "label": 0
                },
                {
                    "sent": "You have a data set.",
                    "label": 0
                },
                {
                    "sent": "If for turbot.",
                    "label": 0
                },
                {
                    "sent": "You have a clustering on D. You have a clustering on the perturbed data set and then you compare the two.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I think the microphone is.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question was what is the data really changes because it's.",
                    "label": 0
                },
                {
                    "sent": "Save.",
                    "label": 0
                },
                {
                    "sent": "A stream of data, like a video stream or stream of news or dynamic system and the question is that these are all very simple algorithms that assume that the data always come from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "There you have to define, so a way of looking at data that is dynamic is to define a time scale where something is preserved and then try to find structure at that time scale.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you can just take these algorithms and apply them to data that changes because they assume data comes from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "If there is any distribution at all.",
                    "label": 0
                },
                {
                    "sent": "And in particular, here I I perturb the data myself, and at this provision should be small, so that theoretically the deprime could have come from the same distribution SD.",
                    "label": 0
                },
                {
                    "sent": "So one way of perturbing the data is to actually sampling from the data with replacement.",
                    "label": 0
                },
                {
                    "sent": "Then you get a data set that contains the same points.",
                    "label": 0
                },
                {
                    "sent": "But maybe if you are missing, and if you are just a few times there repeated once or twice, and that's actually called the Bootstrap is a very.",
                    "label": 0
                },
                {
                    "sent": "While we use technique in statistics.",
                    "label": 0
                },
                {
                    "sent": "OK, so if these are dissimilar.",
                    "label": 0
                },
                {
                    "sent": "Then we should suspect both of them, because we know that one of them is wrong.",
                    "label": 0
                },
                {
                    "sent": "At least one, and in fact people.",
                    "label": 0
                },
                {
                    "sent": "But if we repeat this many times and this team over and over and over again, then the two clusterings are similar.",
                    "label": 0
                },
                {
                    "sent": "Then maybe there is a reason to believe that we have found something in the data, especially if they are more similar only at 1K and not so more variable at different case.",
                    "label": 0
                },
                {
                    "sent": "And so this is the idea was implemented for every K. We resample the data many times and look how close this clustering czar.",
                    "label": 0
                },
                {
                    "sent": "For this you have to have a distance to measure clustering and talk about distances later and you actually this clustering because the data set is perturbed may not contain not may not be over the same number of points, so you have to take only the points that are common to the 2 two clusterings, but these are all details.",
                    "label": 0
                },
                {
                    "sent": "And in fact, what they did experiments showed that very often what happens is that.",
                    "label": 0
                },
                {
                    "sent": "For the 3K.",
                    "label": 0
                },
                {
                    "sent": "All the clusterings they get seem to be similar, whereas for the other values of K there is more much more variation.",
                    "label": 0
                },
                {
                    "sent": "There is another method, so this can work for any clustering method, provided it's a hard clustering because we don't really know how to compare soft clustering, that's why.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Another idea which works.",
                    "label": 0
                },
                {
                    "sent": "And has been implemented by for model based clustering.",
                    "label": 0
                },
                {
                    "sent": "Is the following.",
                    "label": 0
                },
                {
                    "sent": "I divide the data set into two halves and the cluster both with the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I get both the clustering and the data.",
                    "label": 0
                },
                {
                    "sent": "And of course even gives soft clusterings, but you can always transform them into hard clustering for the purpose of comparison.",
                    "label": 0
                },
                {
                    "sent": "And then the next thing to do is to take the parameters from the second set and cluster again the first set.",
                    "label": 0
                },
                {
                    "sent": "With them that's because if you assume that the datasets both come from the same distribution as they are since you just divide the data.",
                    "label": 0
                },
                {
                    "sent": "Then and you have learned the true distribution, then if the data to another one should be very similar.",
                    "label": 0
                },
                {
                    "sent": "But you don't compare that data.",
                    "label": 0
                },
                {
                    "sent": "If you compare the results of the clustering.",
                    "label": 0
                },
                {
                    "sent": "So you just apply data 2 to D1 and get another clustering of the first data set, and now you compare the two clusterings.",
                    "label": 0
                },
                {
                    "sent": "And if they are almost the same, if they are similar, then it means that.",
                    "label": 0
                },
                {
                    "sent": "You have two independent witnesses.",
                    "label": 0
                },
                {
                    "sent": "The first date data set and the second data set on which you have gotten the same, the same structure.",
                    "label": 0
                },
                {
                    "sent": "So maybe you have found the true structure.",
                    "label": 0
                },
                {
                    "sent": "And then you can repeat this several times to make sure that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That you're not.",
                    "label": 0
                },
                {
                    "sent": "Basically, to reduce the effects of chance and random fluctuations.",
                    "label": 0
                },
                {
                    "sent": "And this also has been tried on datasets and has worked.",
                    "label": 0
                },
                {
                    "sent": "In practice, however.",
                    "label": 0
                },
                {
                    "sent": "So they have been tried and many other people have tried working on these methods because they are interesting.",
                    "label": 0
                },
                {
                    "sent": "However, there is no theory to support them, and in fact there are.",
                    "label": 0
                },
                {
                    "sent": "There is a recent theoretical result which say that these methods should not work.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you see something like this then sort of go and look there because there is something interesting happening.",
                    "label": 0
                },
                {
                    "sent": "So and work is in progress and probably new results will be coming.",
                    "label": 1
                },
                {
                    "sent": "Or another model is don't take any of these results is final.",
                    "label": 0
                },
                {
                    "sent": "Because probably the problem is not solved yet and there is another yet another way to look at the problem.",
                    "label": 0
                },
                {
                    "sent": "And now and then it happens that an algorithm is not supposed to work, does work.",
                    "label": 0
                },
                {
                    "sent": "But that's a rare chance, so we'll see the jury still Albanese.",
                    "label": 0
                },
                {
                    "sent": "Let me see if we have.",
                    "label": 0
                },
                {
                    "sent": "Let me see if you have any questions about the stability methods and then we'll go to outliers.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Look at their data.",
                    "label": 0
                },
                {
                    "sent": "One that one part is that based on the hard clustering.",
                    "label": 0
                },
                {
                    "sent": "Yes, they're hard clustering, and again this is only big cause there is no good method of comparing soft clusterings to date and the results are actually sensitive to what distance they use.",
                    "label": 0
                },
                {
                    "sent": "I don't work for some distances.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to see the statistical term.",
                    "label": 0
                },
                {
                    "sent": "And it means that.",
                    "label": 0
                },
                {
                    "sent": "It's a point that's not typical for that particular distribution.",
                    "label": 0
                },
                {
                    "sent": "So if, for example, if you have a Gaussian distribution, then it's a point that's far away from the mean.",
                    "label": 0
                },
                {
                    "sent": "So it's a point that will have appear very rarely if you sample data from the distribution, or possibly it's a corruption of the data somebody entered the data.",
                    "label": 0
                },
                {
                    "sent": "One had a typo and they they typed that particular value, and so you get a number that's not very likely to appear from the distribution.",
                    "label": 0
                },
                {
                    "sent": "And so for distributions like the Gaussian distribution, where the probability of an outlier is very small.",
                    "label": 0
                },
                {
                    "sent": "Basically, you know that.",
                    "label": 0
                },
                {
                    "sent": "99% of the data are within three Sigma from the mean.",
                    "label": 0
                },
                {
                    "sent": "So everything away is has a very small chance of appearing.",
                    "label": 0
                },
                {
                    "sent": "An outlier should be cause of some concern for an algorithm or an algorithm assumes that the data don't have many outliers because the data are Gaussian, and that's what most people do.",
                    "label": 0
                },
                {
                    "sent": "Outline how to concern.",
                    "label": 0
                },
                {
                    "sent": "They will not be a concern if you assume that you're the distributions of your clusters are what's called heavy tail, like our allow many points to be away for them from the mean.",
                    "label": 0
                },
                {
                    "sent": "And this is important also when you don't have mixtures becausw many other costs implicitly.",
                    "label": 0
                },
                {
                    "sent": "As I showed for the single linkage method, can be can be disturbed very much by an outlier sensitive to our lives.",
                    "label": 0
                },
                {
                    "sent": "And so we have to think of what to do when the data contains some outliers.",
                    "label": 0
                },
                {
                    "sent": "The problem is that all the remedies depend on what there is no general solution.",
                    "label": 0
                },
                {
                    "sent": "Actually, the general solution is to go to nonparametric clustering.",
                    "label": 1
                },
                {
                    "sent": "But if you want to stay with parametric clustering, then you have to do some ad hoc tweaking for outliers.",
                    "label": 0
                },
                {
                    "sent": "One of them if you have mixture models.",
                    "label": 0
                },
                {
                    "sent": "Is to put all the outliers in a in a fictitious cluster.",
                    "label": 0
                },
                {
                    "sent": "You introduce a cluster with a very large Sigma.",
                    "label": 1
                },
                {
                    "sent": "And that thing we don't train.",
                    "label": 0
                },
                {
                    "sent": "So I think that as large as a signal the data, basically some sort of uniform distribution overall, where the data could be.",
                    "label": 0
                },
                {
                    "sent": "And everything that's an outlier.",
                    "label": 0
                },
                {
                    "sent": "That's not very likely to come from the other clusters we put there.",
                    "label": 0
                },
                {
                    "sent": "There is another.",
                    "label": 0
                },
                {
                    "sent": "Remedy, and that is to estimate the clusters by robust method methods.",
                    "label": 1
                },
                {
                    "sent": "So robust methods are a branch of statistics that is meant to deal with.",
                    "label": 0
                },
                {
                    "sent": "Specifically with outliers, so the assumption is that you have a distribution like the Gaussian which has few outliers.",
                    "label": 0
                },
                {
                    "sent": "But that's corrupted by 2% or 3% of data from another distribution which is not from the distribution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then what do you do?",
                    "label": 0
                },
                {
                    "sent": "Because the way of computing the mean of a Gaussian which is to sum up all the data is actually fairly sensitive to.",
                    "label": 0
                },
                {
                    "sent": "112 hours.",
                    "label": 0
                },
                {
                    "sent": "And there are books written on robust statistics how to make an algorithm that is not.",
                    "label": 0
                },
                {
                    "sent": "It's almost optimal when the data comes from the from the two distribution, but it doesn't degrade, but it doesn't degrade badly when the data are corrupted with outliers, and one of the simplest ways of doing it is by what's called a trimmed mean and the trim variance, and that's the following.",
                    "label": 0
                },
                {
                    "sent": "Basically, you solve the data in any dimension and then remove the highest 2% and the lowest 2%, or the highest 1% and the lowest 1%.",
                    "label": 0
                },
                {
                    "sent": "And of course, in the day to happen to come from a Gaussian then you are.",
                    "label": 0
                },
                {
                    "sent": "You're disturbing the result a little, but you can show that in an average that is the error introducing is small.",
                    "label": 0
                },
                {
                    "sent": "And the potential error that you are removing if there actually those outliers don't come from the data is the benefit is great when actually you have true errors in the data.",
                    "label": 0
                },
                {
                    "sent": "And that's called trim mean because you just trim the top and the bottom of the of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "And there are similar methods for the variance and so on.",
                    "label": 0
                },
                {
                    "sent": "Very intuitive.",
                    "label": 0
                },
                {
                    "sent": "And that can be done at every step of re estimation or in here in the EM algorithm or in the K means algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or everywhere where I mean is estimated.",
                    "label": 0
                },
                {
                    "sent": "There is another remedy, which is that instead instead of to change the cost function.",
                    "label": 0
                },
                {
                    "sent": "Basically the quadratic cost is more sensitive to outliers than the same cost without the square.",
                    "label": 0
                },
                {
                    "sent": "And so if I want to end that is called the K Millions algorithm, because this is.",
                    "label": 0
                },
                {
                    "sent": "Because the minimizer of that cost is the median of the data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me make a little drawing to show you.",
                    "label": 0
                },
                {
                    "sent": "So if the data are on this line.",
                    "label": 0
                },
                {
                    "sent": "Here is the.",
                    "label": 0
                },
                {
                    "sent": "This is the mean of the data.",
                    "label": 0
                },
                {
                    "sent": "The main minimizes the cost X minus mu squared.",
                    "label": 0
                },
                {
                    "sent": "Some over access.",
                    "label": 0
                },
                {
                    "sent": "And now I delete this square and I want to minimize sum over X of.",
                    "label": 0
                },
                {
                    "sent": "I don't need this.",
                    "label": 0
                },
                {
                    "sent": "Just X minus nu.",
                    "label": 0
                },
                {
                    "sent": "But minimizer of this cost is the medium.",
                    "label": 0
                },
                {
                    "sent": "Which would be 123 here.",
                    "label": 0
                },
                {
                    "sent": "Actually, probably.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's why the algorithm is called K medians.",
                    "label": 0
                },
                {
                    "sent": "Now the median has a nice property with respect to outliers that I can take this point and move it far away out of the room.",
                    "label": 0
                },
                {
                    "sent": "The median will not change at all.",
                    "label": 0
                },
                {
                    "sent": "So the median is sensitive.",
                    "label": 0
                },
                {
                    "sent": "The median is actually, the is not.",
                    "label": 0
                },
                {
                    "sent": "If I don't make any change in the middle of the data, the median will not change at all, so it's much more robust to outliers and therefore an algorithm uses.",
                    "label": 0
                },
                {
                    "sent": "This cost will automatically be robust.",
                    "label": 0
                },
                {
                    "sent": "What I can say about this cost is that we don't know how to minimize, so it's NP hard as well, and it's in some way.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "No, I shouldn't say that, but there are some computer science algorithms that can give some fairly complicated.",
                    "label": 0
                },
                {
                    "sent": "That's why I didn't include them in the in the handouts that can actually give some guarantees, As for example the.",
                    "label": 0
                },
                {
                    "sent": "I think a factor of four approximation on the cost.",
                    "label": 0
                },
                {
                    "sent": "So this means, again a factor of four approximation means that.",
                    "label": 0
                },
                {
                    "sent": "The algorithm guarantees to get.",
                    "label": 0
                },
                {
                    "sent": "At most four times the true cost.",
                    "label": 0
                },
                {
                    "sent": "So if the data are well clustered and the true cost is small, then that's not too bad.",
                    "label": 0
                },
                {
                    "sent": "Or is the data really compact?",
                    "label": 0
                },
                {
                    "sent": "Then there's not too bad.",
                    "label": 0
                },
                {
                    "sent": "If the data are clustered, but.",
                    "label": 0
                },
                {
                    "sent": "Not really compactly, then it's also.",
                    "label": 0
                },
                {
                    "sent": "And other methods that are applied to single linkage or.",
                    "label": 0
                },
                {
                    "sent": "Other algorithms are really sensitive to outliers.",
                    "label": 0
                },
                {
                    "sent": "Hard to eliminate small clusters.",
                    "label": 1
                },
                {
                    "sent": "Yes, so if you have a cluster with two or three points then that's considered or one point which can come out in a single link as you just eliminate it, eliminate that point and count your clusters without that one.",
                    "label": 0
                },
                {
                    "sent": "But before we quit, the subject of outliers, let's think of the following.",
                    "label": 0
                },
                {
                    "sent": "This razor what's an outlier is something sometimes the philosophical problem or it's a problem that.",
                    "label": 0
                },
                {
                    "sent": "Statistics alone cannot solve.",
                    "label": 0
                },
                {
                    "sent": "You have to think what you really want.",
                    "label": 0
                },
                {
                    "sent": "And one of the problems that they raise is that if you have outliers so points that are clusters that contain one point, that could be another definition of outliers.",
                    "label": 0
                },
                {
                    "sent": "Then does the number of clusters make sense?",
                    "label": 0
                },
                {
                    "sent": "And if there are not too many of them, or if you conclude that all the points that are singletons are some sort of errors, then it does make sense.",
                    "label": 0
                },
                {
                    "sent": "But if there are many of them, or if you have clusters with one point but also clusters with two points and clusters with three points and clusters with five points.",
                    "label": 0
                },
                {
                    "sent": "Then which would you stop calling them outliers and start calling them clusters?",
                    "label": 0
                },
                {
                    "sent": "Again, maybe the if the boundary is not clear, then maybe you should not.",
                    "label": 0
                },
                {
                    "sent": "And of course if you have many clusters with three or four points, then the number of classes is not well defined.",
                    "label": 0
                },
                {
                    "sent": "Becauses another sample of the data will give you slightly different cluster sizes, or some of them will actually disappear.",
                    "label": 0
                },
                {
                    "sent": "And not appear in every sample.",
                    "label": 0
                },
                {
                    "sent": "However, you may not want to ignore them because they may be there like there may be something that happens rarely.",
                    "label": 0
                },
                {
                    "sent": "A small cluster.",
                    "label": 0
                },
                {
                    "sent": "In this situation though, the K, the number of clusters doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "Because it may be that in any instantiation of the data will not see the same key.",
                    "label": 0
                },
                {
                    "sent": "Or you will not see all the games and that is the philosophical problem.",
                    "label": 1
                },
                {
                    "sent": "When does it?",
                    "label": 0
                },
                {
                    "sent": "When is there a fixed number of clusters and when there isn't and if there isn't, then we should go and look at nonparametric clustering because that is a problem that actually finds as many clusters as there are small or large and doesn't doesn't put a specific premium on this number.",
                    "label": 0
                },
                {
                    "sent": "K uses other parameters to describe how densely the data is clustered.",
                    "label": 0
                },
                {
                    "sent": "Let me see if there are questions about outliers.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we are starting so we have for now we are leaving parametric clustering and going to another part of clustering.",
                    "label": 0
                },
                {
                    "sent": "Another way of doing clustering which is nonparametric.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here again, there is this.",
                    "label": 0
                },
                {
                    "sent": "There is a split between.",
                    "label": 1
                },
                {
                    "sent": "In two kinds of methods, one that have a probabilistic model and are based on probabilistic estimation, just like mixture models, and in fact they are generalization of mixtures mixture models.",
                    "label": 0
                },
                {
                    "sent": "And other algorithms which are based on a completely different.",
                    "label": 0
                },
                {
                    "sent": "Completely different approach and we started those.",
                    "label": 0
                },
                {
                    "sent": "So we talk now about algorithms based on nonparametric density estimation.",
                    "label": 1
                },
                {
                    "sent": "And let's see how many of you know what nonparametric density estimation is.",
                    "label": 0
                },
                {
                    "sent": "Can be nobody.",
                    "label": 0
                },
                {
                    "sent": "So let's explain it.",
                    "label": 0
                },
                {
                    "sent": "So assume my data is in one dimension and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is the data.",
                    "label": 0
                },
                {
                    "sent": "Can you see?",
                    "label": 0
                },
                {
                    "sent": "Yeah well your hands if you can't see these little dots.",
                    "label": 0
                },
                {
                    "sent": "You can't.",
                    "label": 0
                },
                {
                    "sent": "Ah, OK.",
                    "label": 0
                },
                {
                    "sent": "So do you see any clusters in the data?",
                    "label": 0
                },
                {
                    "sent": "That's another question.",
                    "label": 0
                },
                {
                    "sent": "In clustering, is that if the data clustering at all and will talk about cluster at all, and I'll talk about it later, but one another question I would I'm not asking, but if you think of it is how many clusters do you see if you see?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But one way of thinking of clusters is places where the data is close together and another way of calling a place where the data is close together is the data has high density.",
                    "label": 0
                },
                {
                    "sent": "So this is a probability density is a function F of X.",
                    "label": 1
                },
                {
                    "sent": "That integrates one, and that describes the data density of this along this axis, for example.",
                    "label": 0
                },
                {
                    "sent": "Or in the domain where the data like.",
                    "label": 1
                },
                {
                    "sent": "And so if I knew that.",
                    "label": 0
                },
                {
                    "sent": "If instead of the data I had the data density, so here is the data density.",
                    "label": 0
                },
                {
                    "sent": "Then I could define the clusters to be the peaks of this function.",
                    "label": 0
                },
                {
                    "sent": "And all the methods that I'm going to talk about are about this finding either the peaks of this distribution and so how many picks do I have?",
                    "label": 0
                },
                {
                    "sent": "12345 of these peaks or not?",
                    "label": 0
                },
                {
                    "sent": "I guess that's a question.",
                    "label": 0
                },
                {
                    "sent": "And each peak would be considered a cluster.",
                    "label": 0
                },
                {
                    "sent": "A nice thing about the method is that.",
                    "label": 0
                },
                {
                    "sent": "First of all, pics are not that I'm not that easy to find, but there are still matters to find the pics.",
                    "label": 0
                },
                {
                    "sent": "Also, another interesting feature of the method is that if you only find one peak, you know that the data that has no clusters.",
                    "label": 0
                },
                {
                    "sent": "Also, if there are points which lie under no peak at all, except this is.",
                    "label": 0
                },
                {
                    "sent": "Let's say don't call these peaks, they're too small.",
                    "label": 0
                },
                {
                    "sent": "Then outliers are.",
                    "label": 0
                },
                {
                    "sent": "Taking out the very naturally, we don't have to do anything about outliers, because if they're not under one peak, then.",
                    "label": 0
                },
                {
                    "sent": "They are not clustered.",
                    "label": 0
                },
                {
                    "sent": "And there is a series of methods that that.",
                    "label": 0
                },
                {
                    "sent": "Take this approach.",
                    "label": 0
                },
                {
                    "sent": "But to do this first you have to have a smooth function that covers the data.",
                    "label": 1
                },
                {
                    "sent": "And that is called the density estimate.",
                    "label": 0
                },
                {
                    "sent": "So this if you can draw this F of X from data data density estimate.",
                    "label": 0
                },
                {
                    "sent": "And this is the estimation can be nonparametric at itself and the methods that are used here are called typically use what's called kernel density estimation.",
                    "label": 0
                },
                {
                    "sent": "Now this Colonel.",
                    "label": 0
                },
                {
                    "sent": "Is not like the kernel in kernel methods.",
                    "label": 0
                },
                {
                    "sent": "And it's as follows.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by current?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The kernel this estimation is that you choose some sort of.",
                    "label": 0
                },
                {
                    "sent": "Shape like a bump that you call the curve.",
                    "label": 0
                },
                {
                    "sent": "And then this function is constructed by putting a little bump on every point.",
                    "label": 0
                },
                {
                    "sent": "Centered on every point.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "To get this Hill, I basically add up this this little bumps.",
                    "label": 0
                },
                {
                    "sent": "And so if I add up these three functions, I get.",
                    "label": 0
                },
                {
                    "sent": "Something like this because they overlap.",
                    "label": 0
                },
                {
                    "sent": "Here what they will not overlap or overlap only on the tail I will get a separate bump for each data point.",
                    "label": 0
                },
                {
                    "sent": "And for example, this is a Gaussian kernel, like a little Gaussian shape on every point.",
                    "label": 0
                },
                {
                    "sent": "And in general, K can be any density, but I prefer something that's sent like that symmetric and smooth, and so yes, so I can take any function that is that integrates into a finite number.",
                    "label": 0
                },
                {
                    "sent": "But I have to have good reasons to choose something that's not strange and odd shape.",
                    "label": 0
                },
                {
                    "sent": "This so the resulting F is called a nonparametric estimate estimator of the density.",
                    "label": 0
                },
                {
                    "sent": "However, it does have a parameter and this parameter is H. Which is called the kernel width.",
                    "label": 0
                },
                {
                    "sent": "And if you look carefully at this formula, if H is large.",
                    "label": 0
                },
                {
                    "sent": "Then this bump the bump.",
                    "label": 0
                },
                {
                    "sent": "There will be wide, and then the function will, so if every bump is wide, if you add them up, you're not going to get any sharp peaks in the distribution.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this means that the distribution that the resulting density is smooth.",
                    "label": 0
                },
                {
                    "sent": "Smooth here doesn't mean whether it has derivatives or not.",
                    "label": 0
                },
                {
                    "sent": "It means whether it varies fast or slowly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and if H is very small then I'm going to have very sharp peaks on each point and so the resulting estimate will be non smooth or very jacked.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is the parameter of this function and it's called the smoothness parameter because it controls the smoothness.",
                    "label": 0
                },
                {
                    "sent": "After density estimation of the resulting F that I'm computing.",
                    "label": 0
                },
                {
                    "sent": "And now I hope it's.",
                    "label": 0
                },
                {
                    "sent": "You will not be surprised that if I choose a different age.",
                    "label": 0
                },
                {
                    "sent": "Something like this narrow?",
                    "label": 0
                },
                {
                    "sent": "I'm going to get a different shape for this.",
                    "label": 0
                },
                {
                    "sent": "So with a function like this, I'm going to get something.",
                    "label": 0
                },
                {
                    "sent": "More like this.",
                    "label": 0
                },
                {
                    "sent": "And so instead of 1 peak here, I'm going to have many peaks.",
                    "label": 0
                },
                {
                    "sent": "So choosing age for density estimation affects the the cluster that I get.",
                    "label": 0
                },
                {
                    "sent": "So in one case all the points were in one cluster.",
                    "label": 0
                },
                {
                    "sent": "In other case, which is in their own cluster or their outliers, yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This means that.",
                    "label": 0
                },
                {
                    "sent": "Oh, before I go on, let's see if you have questions about density estimation or non per hour kernel density estimation in particular.",
                    "label": 0
                },
                {
                    "sent": "No questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Then let's see what we can do with this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what kind of things we can do with kernel density estimators for clustering?",
                    "label": 0
                },
                {
                    "sent": "This is 1 algorithm that does.",
                    "label": 0
                },
                {
                    "sent": "The following is computer kernel density estimator F4H chosen ahead of time and actually that's an issue?",
                    "label": 0
                },
                {
                    "sent": "How do you choose H?",
                    "label": 0
                },
                {
                    "sent": "You must basically one way to choose the H is to know something about the kind of clusters you want to see, like, how large are the clusters.",
                    "label": 0
                },
                {
                    "sent": "So when you do, clusterings generally have to know you have to know what what you're looking for and that you can't escape this.",
                    "label": 0
                },
                {
                    "sent": "Even in nonparametric density estimation.",
                    "label": 0
                },
                {
                    "sent": "And then after you build this blue function, then you compute for various values.",
                    "label": 0
                },
                {
                    "sent": "You basically threshold it with lines at different Heights.",
                    "label": 0
                },
                {
                    "sent": "And then at each level you look for, you know how many connected components are above this line, how many peaks are above that line, and that gives you a cluster.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I threshold here, I will see one peak 2345.",
                    "label": 0
                },
                {
                    "sent": "But if I put the threshold lower like here, I may see no peek at all, which means that at this level the data all in one cluster.",
                    "label": 0
                },
                {
                    "sent": "Here I have five clusters plus some outliers, and at this level here I have only one cluster left.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And so the algorithm produces not just one clustering, But basically all the possible clusterings when you move this line from zero to.",
                    "label": 0
                },
                {
                    "sent": "Above the highest peak of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Face.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "How behavior?",
                    "label": 0
                },
                {
                    "sent": "Oh, thank you.",
                    "label": 0
                },
                {
                    "sent": "This is a very good question.",
                    "label": 0
                },
                {
                    "sent": "In fact I don't know if it's not at the bottom of the page.",
                    "label": 0
                },
                {
                    "sent": "This doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So you spotted this.",
                    "label": 0
                },
                {
                    "sent": "There are various reasons why this can't really work in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "And one of them is that actually kernel density estimation doesn't work in high dimensions too well.",
                    "label": 0
                },
                {
                    "sent": "Practically, for several reasons, so this will work in in the second problem, so everything that's in green is.",
                    "label": 0
                },
                {
                    "sent": "Difficult or difficult in high dimensions is so.",
                    "label": 0
                },
                {
                    "sent": "First of all, it's hard to get a higher kernel density estimate in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Can you see why?",
                    "label": 0
                },
                {
                    "sent": "Because typical in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Like let's say in 10 dimensions in 10 dimensions.",
                    "label": 0
                },
                {
                    "sent": "If you break every direction into two, you have about 2 to the 10th.",
                    "label": 0
                },
                {
                    "sent": "About 1000 little cubes.",
                    "label": 0
                },
                {
                    "sent": "If you just break the dimension to two left and right, and so if you want to have a data point in each of these, you need 1000 data points.",
                    "label": 0
                },
                {
                    "sent": "Of course, if the data is clustered then it will not be spread.",
                    "label": 0
                },
                {
                    "sent": "So basically we need.",
                    "label": 0
                },
                {
                    "sent": "A lot of data to fill the space, so most of the space is empty and lots of points are very far apart, and so if you put spheres on top of the points, this case will not overlap very much unless the data is really well clustered.",
                    "label": 0
                },
                {
                    "sent": "And so it's very hard to construct to adjust the size of the current into constructor, yes, so why can't it be fixed by adjusting that scaling parameter, let it grow with the dimension or something like that.",
                    "label": 0
                },
                {
                    "sent": "So there are some methods that do that, like ball trees where you actually put put spheres that cover several.",
                    "label": 0
                },
                {
                    "sent": "Several things, but that's not.",
                    "label": 0
                },
                {
                    "sent": "So bowl trees are, as far as they are understood, understood as a data structure.",
                    "label": 0
                },
                {
                    "sent": "It's not understood what their asymptotic properties are like does it?",
                    "label": 0
                },
                {
                    "sent": "Does that like if you're doing density estimation, statisticians want to know whether it converges and so on and.",
                    "label": 0
                },
                {
                    "sent": "In high dimension you just make the bump flatter.",
                    "label": 0
                },
                {
                    "sent": "Why does that work?",
                    "label": 0
                },
                {
                    "sent": "You talked about before.",
                    "label": 0
                },
                {
                    "sent": "If you make the product that bumps peaked, then it doesn't really want to mention so in high dimension you just use ladder bumps.",
                    "label": 0
                },
                {
                    "sent": "Does that?",
                    "label": 0
                },
                {
                    "sent": "Why can't you compensate for dimensionality?",
                    "label": 0
                },
                {
                    "sent": "Oh because yeah, because then you won't see peaks.",
                    "label": 0
                },
                {
                    "sent": "If you make flatter bumps then then what you want is to see the clusters and so if you make them yes you can make the basically have to make the bump almost.",
                    "label": 0
                },
                {
                    "sent": "Tesla as flat as all the data on one dimension.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Also, there is another reason.",
                    "label": 0
                },
                {
                    "sent": "When the data are in high dimensions very often.",
                    "label": 0
                },
                {
                    "sent": "Their true dimension is not a dimension of the space, so they lie along strings in the space and or they lie along lower dimensional curved manifolds, and so you can put spheres the sphere contains.",
                    "label": 0
                },
                {
                    "sent": "A lot of empty space.",
                    "label": 0
                },
                {
                    "sent": "So that's another reason why.",
                    "label": 0
                },
                {
                    "sent": "Why kernel density estimation is in this standard version of most versions don't work in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "You want to actually put the bumps.",
                    "label": 0
                },
                {
                    "sent": "Make them very elongated in the direction sort of data line so.",
                    "label": 0
                },
                {
                    "sent": "This simple kernel density estimation doesn't go far in D and also finding level sets is not easy if you are in more than one dimension.",
                    "label": 1
                },
                {
                    "sent": "Yes is damage in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "There are methods if you go for level sets in three and four and five dimensions you will see that it's much harder, but there actually this can be fixed.",
                    "label": 0
                },
                {
                    "sent": "And they fixed it by actually looking only at the values of the Bay City at the data points.",
                    "label": 0
                },
                {
                    "sent": "Which means that if there is a peak between the data, they will not find it.",
                    "label": 0
                },
                {
                    "sent": "But then again, a peak between the data means that you didn't have enough data to see the peak.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have higher dimensions then.",
                    "label": 0
                },
                {
                    "sent": "You compute the levels.",
                    "label": 1
                },
                {
                    "sent": "That's only for the point.",
                    "label": 0
                },
                {
                    "sent": "So you compute only what which of the data is still higher than the line or not.",
                    "label": 0
                },
                {
                    "sent": "Also, there is a new version of this algorithm where instead of kernel density estimator you they build a spanning tree of the data and the spanning tree.",
                    "label": 0
                },
                {
                    "sent": "Because it's based on distance is insensitive to dimension and then they try to look at the density along the edges of the tree, which is more sensible because then you are going to look basically only between data points.",
                    "label": 0
                },
                {
                    "sent": "You're not going to wonder in the large empty space around the points and that should work for higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "OK, and to find connected components in low dimensions is not that hard in high dimensions.",
                    "label": 1
                },
                {
                    "sent": "Basically you look at the density along the segment between the data.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that if two points are in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "So under the same peak, then the segment connecting them should also be under the peak.",
                    "label": 0
                },
                {
                    "sent": "So the density on any point between XI and XJ should be also high, and so they sample points on their segment and see whether the check whether the density is high or not.",
                    "label": 0
                },
                {
                    "sent": "And then they put if two points are connected by segment that is inside the level set they are put in the same cluster and then it's easy to find the clusters by this.",
                    "label": 1
                },
                {
                    "sent": "So this is a level set method.",
                    "label": 0
                },
                {
                    "sent": "Oh this particular method in this particular method, if you care about the algorithm, it may seem at 1st that it's hopelessly.",
                    "label": 0
                },
                {
                    "sent": "Expensive to compute all these level sets.",
                    "label": 0
                },
                {
                    "sent": "It's not so big cause if you have information about the one level set.",
                    "label": 0
                },
                {
                    "sent": "Then it is possible to so basically, if two points are in the same cluster at this level, they will not.",
                    "label": 1
                },
                {
                    "sent": "We must know if two points are in different clusters.",
                    "label": 0
                },
                {
                    "sent": "At one level they will not be merged in the next level, yes, so there is a lot of information that can be used from one level to the next to make the algorithm efficient.",
                    "label": 0
                },
                {
                    "sent": "So fairly efficient algorithms exist that can do all the levels in one pass.",
                    "label": 0
                },
                {
                    "sent": "They have also computed confidence intervals, which is.",
                    "label": 0
                },
                {
                    "sent": "An important problem and it's related to what do I call it peak?",
                    "label": 0
                },
                {
                    "sent": "Like for example, here is this one week or two weeks.",
                    "label": 0
                },
                {
                    "sent": "And so their method.",
                    "label": 0
                },
                {
                    "sent": "And I'm I'm mentioning it because it's a good and general method and it addresses a general issue in this kind of clustering is to actually resample the data.",
                    "label": 0
                },
                {
                    "sent": "This method is called Bootstrap.",
                    "label": 0
                },
                {
                    "sent": "In statistics and so re estimate the density the density and see whether one is the peak remains there and 2nd if how deep is the peak?",
                    "label": 0
                },
                {
                    "sent": "Yes, so deep.",
                    "label": 0
                },
                {
                    "sent": "Or the Valley?",
                    "label": 0
                },
                {
                    "sent": "Yes, so small valleys don't count, and what small means depends on the variance of the density with respect to sampling the data, which is the correct thing to do?",
                    "label": 0
                },
                {
                    "sent": "Because they have to define in some sense what is small and what is large.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh actually, this is a good place to stop, so let me take some questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You are beautiful.",
                    "label": 0
                },
                {
                    "sent": "So what about that chicken recipe?",
                    "label": 0
                },
                {
                    "sent": "Space, I don't know why it features this in this case this recipe, so this will use to overcome passcode.",
                    "label": 0
                },
                {
                    "sent": "That is true.",
                    "label": 0
                },
                {
                    "sent": "So here saying that there is.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric methods and density estimation is requires a lot of computation.",
                    "label": 0
                },
                {
                    "sent": "That is true.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There are methods to accelerate the densities.",
                    "label": 0
                },
                {
                    "sent": "Kernel density estimation that actually work very well.",
                    "label": 0
                },
                {
                    "sent": "And the idea is.",
                    "label": 0
                },
                {
                    "sent": "The following so when you compute the density here.",
                    "label": 0
                },
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "You have to use all the data points.",
                    "label": 0
                },
                {
                    "sent": "And I haven't said it explicitly, but I'm saying it now to do kernel to do nonparametric density estimation.",
                    "label": 0
                },
                {
                    "sent": "You need a lot of data.",
                    "label": 0
                },
                {
                    "sent": "Much more data than for parametric density estimation, becausw convergence, the convergence rates are much lower than than for parameter estimation as in the usual sense so.",
                    "label": 0
                },
                {
                    "sent": "If you compute the density here, it's a sum over the curls on top of this data in theory, but in practice this point here has no influence on the density there, and there are methods which actually compute how far you need to look.",
                    "label": 0
                },
                {
                    "sent": "Basically we need to look as far as the width of the kernel, so you need to look.",
                    "label": 0
                },
                {
                    "sent": "You need to look only in this neighborhood and this neighborhood in high dimensions will contain few points.",
                    "label": 0
                },
                {
                    "sent": "Basically you want.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think there's a small section of the data, hopefully for any good kernel density estimator you want it to be local, like you don't want this point here to influence that point there.",
                    "label": 0
                },
                {
                    "sent": "Otherwise there is a little problem with being nonparametric.",
                    "label": 0
                },
                {
                    "sent": "It becomes parametrically yes and so there are very efficient methods that exploit this in low and high dimensions they were going to hundreds of dimensions.",
                    "label": 0
                },
                {
                    "sent": "That's not a problem computing this.",
                    "label": 0
                },
                {
                    "sent": "Another issue is how do we use this estimator in clustering?",
                    "label": 0
                },
                {
                    "sent": "We don't know if we have a fast method to compute it, then the methods for finding peaks are also local, meaning they use only information in the local neighborhood, so only a few points so they could be made so that is not they are intensive, but they're not.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Proclivities in the sense that if you can handle.",
                    "label": 0
                },
                {
                    "sent": "They are linear in the number of data points or.",
                    "label": 0
                },
                {
                    "sent": "But but not quadratic usually.",
                    "label": 0
                },
                {
                    "sent": "Yes, and so if you admit that you can handle anything that's proportional to the number of data points, then you can do that.",
                    "label": 0
                },
                {
                    "sent": "Are there other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, then I should stop now and continue after the break.",
                    "label": 0
                }
            ]
        }
    }
}