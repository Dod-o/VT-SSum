{
    "id": "fez2kg3unvfnlprsxogjl2wveyzwwz6k",
    "title": "Information-Theoretic Metric Learning",
    "info": {
        "author": [
            "Jason Davis, Stanford University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Multiple Kernel Learning"
        ]
    },
    "url": "http://videolectures.net/lce06_davis_itml/",
    "segmentation": [
        [
            "Thanks so my name is Jason Davis and today I'll be describing some joint work with some colleagues of mine at the University of Texas.",
            "Brian Coolest Super, it's Raw and our advisor, Inderjit Dhillon.",
            "And we propose an information theoretic viewpoint of sort of a standard metric learning problem.",
            "So our goal here is to learn about Holland."
        ],
        [
            "This distance function, subject to a set of linear constraints.",
            "These constraints are assumed to be pairwise constraints between points in our input space.",
            "We adopt an information theoretic viewpoint.",
            "Specifically, we exploit a simple bijection which has already been shown several times today between Gaussian distribution and Mahalanobis distance, and from this we take an information theoretic viewpoint and we can use a natural entropy based objective for our problem.",
            "We next show some interesting connections between our metric learning problem and a recently proposed kernel learning problem.",
            "And from this we can leverage the fast and simple methods used in this kernel learning problem to optimize our problem.",
            "In particular, we use convex optimization procedure proposed by Bregman in the 60s and this.",
            "This method has several advantages over sort of black box software or basically more basic methods.",
            "In particular, no eigenvalue computations are needed.",
            "So first give a brief overview of the problem, so we assume that we're given endpoints X one through XN&D dimensional space."
        ],
        [
            "And we also assume that we're given a set of similarity or dissimilarity constraints.",
            "So two points XI and XJ are said to be similar if they are, if their distance is at most some upper bound, you.",
            "And two points are constrained to be dissimilar if their distance is at least some lower bound L. And of course, the distance function we seek to learn here is a Mahalanobis distance function given here, and we only consider.",
            "Mahalanobis distance functions parameterized by positive definite matrices A so full rank matrix is that we do not consider lowering case as Sam and described in his NCA and some of his other algorithms.",
            "You're fixing a single you into single out for the whole problem, right?",
            "Although this is just our formulation in general to sell the problem, that's not a not necessary and will get it later.",
            "Will get to other extensions of this.",
            "So of course there are many applications here in K means clustering.",
            "You may have some prior knowledge relating pairs of points.",
            "These so-called must links and cannot link constraints, and this is certainly one approach to incorporating that knowledge into the K means algorithm.",
            "And of course there are these neighborhood nearest neighbor searches or cannon classification.",
            "Sort of.",
            "Approaches that you can use or applications rather.",
            "So of course, given a set of our constraints set relating the."
        ],
        [
            "Wise distance between points.",
            "I'm in general there will be many melanomas distance functions to choose.",
            "So I mean this is sort of a question that's been asked several times throughout the day today.",
            "How should we choose the best Mahalanobis distance function?",
            "Well, in the absence of prior knowledge, many algorithms assume just they just use the squared Euclidean distance.",
            "For example, the K means algorithm optimizes squared putting distance and Euclidean distance is a popular metric.",
            "Also popular metric in nearest neighbor searches.",
            "So we propose to regularize the Holy Novas distance by choosing that which is closest to sort of the baseline squared Euclidean distance.",
            "So this is where we use our bijection between the Gaussian animal Innovus distance function.",
            "So we see here, this is just our standard multivariate Gaussian, a parameterized by me by mean M and covariance matrix equal to the inverse of a.",
            "And from this projection we can have a way of comparing two Mahalanobis distance functions through their associated multivariate Gaussians.",
            "And then given our information theoretic viewpoint, we can compare to multivariate Gaussians via the differential relative entropy between their associated Gaussians.",
            "So the differential relative entropy is the continuous analogue of the KL divergent course.",
            "The KL is defined over discrete sets and the differential relative entropy is defined over our set of which are probably probabilities are designed are.",
            "In the domain of our probability function.",
            "So here for the Gaussian, our domain is our D and of course this is the same as KL, except the summation is replaced with an integral.",
            "So August."
        ],
        [
            "Give a in one slide an overview of our problem formulation, so we seek to minimize the relative entropy from our Gaussian parameterized by by matrix A2.",
            "Sort of the standard isotropic Gaussian or to our squared Euclidean distance.",
            "Of course, the constraints here are pairwise similarity and or dissimilarity constraints, and the last this last constraint should actually be a strict inequality this constrains.",
            "A to B positive positive definite."
        ],
        [
            "So next I'll give an overview of how we solve our model.",
            "So the first thing we do is we show that this problem is actually equivalent to a recently proposed low rank kernel learning problem by coolest.",
            "And from this equivalence we can show a few things.",
            "So first is that our problem objective specifically, that of computing the relative entropy between two Gaussians.",
            "This can be actually computed in closed form in its raw form.",
            "It's an integral over in D dimensional space.",
            "It's not apparently obvious that that you can compute it simply, but you can and 2nd this equivalence shows that the problem is in fact convex.",
            "Which of course is good.",
            "And finally, of course."
        ],
        [
            "Besides having sort of being an interesting sort of theoretical equivalence, there is some nice practical value to this as well.",
            "There are some excellent algorithms developed by Coolest to solve the low rank kernel learning problem efficiently, and we use that we use those to solve our problem."
        ],
        [
            "So next, I'll first give a very brief overview of the kernel learning problem.",
            "In some sense it's looks somewhat similar to ours, but we'll see the differences shortly, so again, we're assuming we're given endpoints in D dimensional space, and we define our kernel K not to be the standard inner product.",
            "Kernel X transpose X is an end by end kernel over the points.",
            "And similarly, we also assumed to have a set of similarity and or dissimilarity constraints relating the distance between pairs of points.",
            "Our objective here is to learn a kernel matrix K that minimizes the divergences decay, not subject to our constraints.",
            "So what sort of divergance is used here?",
            "So the problem minimizes a divergent measure called the Berg Divergent."
        ],
        [
            "This is the bird divergences that type of Bregman diversions that Bregman divergences are a wide class of divergent measures.",
            "They generalize distance measures like squared Euclidean distance KL diversions.",
            "I divergents, of course, model Anubis distance, but here the bird diversions is actually a function of two matrices.",
            "We can see here involves a trace in the log of the determinant of the product of the second matrices.",
            "So this problem seeks to minimize this diversions.",
            "Subject to these inequality constraints.",
            "So are you and Al part of the problem statement or are they under your control?",
            "Are you and ell something you're solving for their part of the problem states you and LR are inputs to the problem.",
            "So let me first give an important characterization of this optimal kernel K, which is a solution to."
        ],
        [
            "This kernel learning problem.",
            "So well, so given this case given Section K. It can be shown that K actually has the same range space as canine.",
            "So what does that mean?",
            "Well, if we consider our input vectors X.",
            "The algorithm seeks a linear transformation W that scales or rotates your input points before some form some new set of input points.",
            "This is sort of with Sam and some of the other speakers were discussing and it can be shown that the final matrix K is really just an inner product kernel between these new set of points."
        ],
        [
            "So the our main equivalence theorem between this lowering kernel learning problem in our problem is the following.",
            "So given a kernel K of this form.",
            "We show that if A equals W transpose W which is.",
            "The the product of these transformation matrices, then this is actually also an optimal solution.",
            "Of course, with the same corresponding constraints to our metric learning problem.",
            "Of course, this theorem.",
            "One common problem with these kernel learning algorithms is how do you generalize.",
            "And of course, in this special case of this special type of objective function, you actually can generalize.",
            "So the proof has to let."
        ],
        [
            "As as you'd expect, the first one is stab Lish, is that the objectives are the same.",
            "So the objective for the kernel learning problem, the bird diversions between the two kernel matrices is just a linear function of the KL diversions between our learn Mahalanobis associated Gaussian of our Learn Mahala nobis distance.",
            "A an sort of the squared Euclidean distance.",
            "One sort of surprising fact about this equivalence, is that these kernel matrices K are recalled their end by, and they're over our input points.",
            "However, the Gaussians are there D dimensional Gaussians, parameterized by D by D covariance matrices.",
            "And this also this result also builds on a recent connection between the relative entropy and of two Gaussians, Annaberg divergance.",
            "In particular, it can be shown that the relative entropy between two Gaussians is actually a convex combination of two different Bregman divergences.",
            "One is the burg divergance between the covariances and the other is a Mahalanobis distance between the means.",
            "So the second lemma."
        ],
        [
            "Establishes that the two problems have the same feasible set.",
            "So given OK, which is the solution 2 hour a feasible solution to our kernel learning problem?",
            "If a parameterise is our metric learning problem IIS feasible if and only if K is feasable?"
        ],
        [
            "So.",
            "In light of this equivalence, we can all now describe the algorithms used by the lower in kernel learning problem and how they can be and describe these algorithms that we use for our problem.",
            "So the kernel learning problem uses a method called bregman's method.",
            "It's a dual ascent method so it acts on the dual of the problem.",
            "It can be shown that at each iteration of Bregman's method, the objective function for the dual problem increases.",
            "And the algorithm works by iteratively projecting the current kernel matrix onto one constraint at a time.",
            "So one problem arises quite often with bregman's method.",
            "Is this projection doesn't have a closed form solution?",
            "However, however, it was shown for the kernel learning problem that this is projection can be computed in closed form quite efficiently.",
            "Just in closed form.",
            "So."
        ],
        [
            "Therefore, the running time for one iteration of the algorithm is just order CD squared, so C is the number of constraints and ears is your dimensionality.",
            "Of course, let me in terms of a.",
            "It can't get much faster than that in terms of work per iteration.",
            "By doing this it actually works in the kernel in factored form, so it doesn't never computes the entire kernel matrix.",
            "So that's definitely one thing it does.",
            "And of course this running time is helped greatly by the fact that we had these.",
            "We can use these closed form projections.",
            "And most importantly, the algorithm requires."
        ],
        [
            "No eigenvalue decompositions.",
            "This is a bit surprising because the bird divergance between two matrices can be naturally expressed as a function of the eigenvalues and eigenvectors of the two kernel matrices.",
            "K&K, not.",
            "Yet the algorithm does not need to compute them to actually optimize over it.",
            "And Furthermore, many existing methods, I believe neighborhood components analysis must compute eigenvectors project into the Kona positive semidefinite matrices and some other popular metric learning algorithms.",
            "Some work bising all require eigenvalue decompositions, which of course are cubic in the dimensionality.",
            "I'll discuss a few extensions, so I guess in the discussion early."
        ],
        [
            "Today was brought up how to incorporate prior knowledge into metric learning problem, so I'm sort of made this assumption that your baseline.",
            "So your prior distance metric that you would use is your squared Euclidean distance.",
            "Fact this is actually not a limitation of our algorithm that can be extended to minimize the KL diversions to other matrices.",
            "Perhaps, maybe the inverse of your sample covariance matrix may be a natural choice.",
            "Slack variables or of course."
        ],
        [
            "Very important determining these upper and lower bounds can be quite difficult.",
            "Constraints can be noisy, etc.",
            "In fact, in our preliminary experiments, which I'll show you in a moment, we do incorporate Slack variables.",
            "And the running time, the algorithm is no more expensive.",
            "And finally our frame."
        ],
        [
            "It can easily incorporate different types of linear inequality constraints, not just pairwise inequality constraints.",
            "Some recent work by shots an yokums say they argue that relative pairwise constraints are easier for people at this sort of telesystem.",
            "You know A&B or closer than CMD etc and our framework can be extended to handle such constraints.",
            "So next present a few very preliminary experiment experiments.",
            "So our method methodology is."
        ],
        [
            "As such, so we seek to learn a Mahala Novus distance function which maximizes the accuracy of the resulting cannon classifier.",
            "So our approach is we."
        ],
        [
            "We assume a fully labeled scenario where each input instance has a true label, and we constrain points in the same class to be similar and points in different classes to be dissimilar.",
            "And we determine these upper and lower bounds by computing the empirical distribution of the squared Euclidean distance between our input points and then selecting the upper on Upper 10th percentile mark as lower bound for the similar dissimilarity constraints and the lower 10% tile as an upper bound for our similarity constraints.",
            "And finally, a few sort of simplifications for these initial experiments we sampled only shows 100 such constraints."
        ],
        [
            "By by choosing uniformly at random, and we do know parameter tuning with the slack variables, there's of course a gamma parameter which regulates the tradeoff between the slack.",
            "Very very slack penalty, and the objective function."
        ],
        [
            "And then we evaluate results via A2 fold cross validation."
        ],
        [
            "So we compare our algorithm, which we call here as I TML information theoretic metric learning to four different methods.",
            "The first is actually just your standard squared Euclidean distance.",
            "Your second here is Parameterising Omaha Innovus distance by your inverse covariance.",
            "The third is LDA, which I which was described in previous slides, so I won't discuss.",
            "And finally, using an approach just described earlier by Sam MLE algorithm."
        ],
        [
            "So we see that in general it's.",
            "There's no clear winner, certainly for so the soybean data set.",
            "The MLE algorithm seems to do a bit better.",
            "One thing you'll notice is that since we are regularising against this standard, squared Euclidean distance, sort of the 1st and the 3rd columns behaves similarly.",
            "Course these experiments are bit preliminary."
        ],
        [
            "So in conclusion, we presented an information theoretic formulation to a standard metric learning problem.",
            "We gave an equivalence between this problem and the lower rank and Anna recently proposed low rank kernel learning problem.",
            "And from this equivalence we gave memorable to develop efficient algorithms to solve our problem.",
            "And finally, the experimental results are promising, but definitely much more work is needed here.",
            "Yeah.",
            "So I guess just calling up on what Pablo asked.",
            "It seems a bit aggressive to ask for every point in.",
            "One class to be.",
            "Separated from every other point by you and and the other way around.",
            "Like if you imagine just the boundary right, there's a clean boundary between two classes.",
            "OK, if I'm sort of, you know, on the North End of the boundary and you're on the South end of my right, it doesn't really matter.",
            "That were far away as long as but this algorithm is.",
            "So is there some geometric assumption here?",
            "Like in MCM L where it's unimodality I mean.",
            "At this point I mean, the problem is in its basic form is formulated as you know, these are inputs, but in terms of, you know, unimodality.",
            "I'm really not sure.",
            "The constraints relative to some distance you may have already show me if for some reason you had prior knowledge about the pairwise distances.",
            "Certainly that would be the best way to go, but I guess typically that information is hard to come by.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks so my name is Jason Davis and today I'll be describing some joint work with some colleagues of mine at the University of Texas.",
                    "label": 1
                },
                {
                    "sent": "Brian Coolest Super, it's Raw and our advisor, Inderjit Dhillon.",
                    "label": 0
                },
                {
                    "sent": "And we propose an information theoretic viewpoint of sort of a standard metric learning problem.",
                    "label": 0
                },
                {
                    "sent": "So our goal here is to learn about Holland.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This distance function, subject to a set of linear constraints.",
                    "label": 1
                },
                {
                    "sent": "These constraints are assumed to be pairwise constraints between points in our input space.",
                    "label": 0
                },
                {
                    "sent": "We adopt an information theoretic viewpoint.",
                    "label": 0
                },
                {
                    "sent": "Specifically, we exploit a simple bijection which has already been shown several times today between Gaussian distribution and Mahalanobis distance, and from this we take an information theoretic viewpoint and we can use a natural entropy based objective for our problem.",
                    "label": 0
                },
                {
                    "sent": "We next show some interesting connections between our metric learning problem and a recently proposed kernel learning problem.",
                    "label": 1
                },
                {
                    "sent": "And from this we can leverage the fast and simple methods used in this kernel learning problem to optimize our problem.",
                    "label": 0
                },
                {
                    "sent": "In particular, we use convex optimization procedure proposed by Bregman in the 60s and this.",
                    "label": 0
                },
                {
                    "sent": "This method has several advantages over sort of black box software or basically more basic methods.",
                    "label": 0
                },
                {
                    "sent": "In particular, no eigenvalue computations are needed.",
                    "label": 1
                },
                {
                    "sent": "So first give a brief overview of the problem, so we assume that we're given endpoints X one through XN&D dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also assume that we're given a set of similarity or dissimilarity constraints.",
                    "label": 1
                },
                {
                    "sent": "So two points XI and XJ are said to be similar if they are, if their distance is at most some upper bound, you.",
                    "label": 0
                },
                {
                    "sent": "And two points are constrained to be dissimilar if their distance is at least some lower bound L. And of course, the distance function we seek to learn here is a Mahalanobis distance function given here, and we only consider.",
                    "label": 0
                },
                {
                    "sent": "Mahalanobis distance functions parameterized by positive definite matrices A so full rank matrix is that we do not consider lowering case as Sam and described in his NCA and some of his other algorithms.",
                    "label": 0
                },
                {
                    "sent": "You're fixing a single you into single out for the whole problem, right?",
                    "label": 0
                },
                {
                    "sent": "Although this is just our formulation in general to sell the problem, that's not a not necessary and will get it later.",
                    "label": 0
                },
                {
                    "sent": "Will get to other extensions of this.",
                    "label": 0
                },
                {
                    "sent": "So of course there are many applications here in K means clustering.",
                    "label": 0
                },
                {
                    "sent": "You may have some prior knowledge relating pairs of points.",
                    "label": 1
                },
                {
                    "sent": "These so-called must links and cannot link constraints, and this is certainly one approach to incorporating that knowledge into the K means algorithm.",
                    "label": 1
                },
                {
                    "sent": "And of course there are these neighborhood nearest neighbor searches or cannon classification.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Approaches that you can use or applications rather.",
                    "label": 0
                },
                {
                    "sent": "So of course, given a set of our constraints set relating the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wise distance between points.",
                    "label": 0
                },
                {
                    "sent": "I'm in general there will be many melanomas distance functions to choose.",
                    "label": 0
                },
                {
                    "sent": "So I mean this is sort of a question that's been asked several times throughout the day today.",
                    "label": 0
                },
                {
                    "sent": "How should we choose the best Mahalanobis distance function?",
                    "label": 1
                },
                {
                    "sent": "Well, in the absence of prior knowledge, many algorithms assume just they just use the squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "For example, the K means algorithm optimizes squared putting distance and Euclidean distance is a popular metric.",
                    "label": 0
                },
                {
                    "sent": "Also popular metric in nearest neighbor searches.",
                    "label": 0
                },
                {
                    "sent": "So we propose to regularize the Holy Novas distance by choosing that which is closest to sort of the baseline squared Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "So this is where we use our bijection between the Gaussian animal Innovus distance function.",
                    "label": 0
                },
                {
                    "sent": "So we see here, this is just our standard multivariate Gaussian, a parameterized by me by mean M and covariance matrix equal to the inverse of a.",
                    "label": 1
                },
                {
                    "sent": "And from this projection we can have a way of comparing two Mahalanobis distance functions through their associated multivariate Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And then given our information theoretic viewpoint, we can compare to multivariate Gaussians via the differential relative entropy between their associated Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So the differential relative entropy is the continuous analogue of the KL divergent course.",
                    "label": 0
                },
                {
                    "sent": "The KL is defined over discrete sets and the differential relative entropy is defined over our set of which are probably probabilities are designed are.",
                    "label": 0
                },
                {
                    "sent": "In the domain of our probability function.",
                    "label": 0
                },
                {
                    "sent": "So here for the Gaussian, our domain is our D and of course this is the same as KL, except the summation is replaced with an integral.",
                    "label": 0
                },
                {
                    "sent": "So August.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give a in one slide an overview of our problem formulation, so we seek to minimize the relative entropy from our Gaussian parameterized by by matrix A2.",
                    "label": 1
                },
                {
                    "sent": "Sort of the standard isotropic Gaussian or to our squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "Of course, the constraints here are pairwise similarity and or dissimilarity constraints, and the last this last constraint should actually be a strict inequality this constrains.",
                    "label": 0
                },
                {
                    "sent": "A to B positive positive definite.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next I'll give an overview of how we solve our model.",
                    "label": 1
                },
                {
                    "sent": "So the first thing we do is we show that this problem is actually equivalent to a recently proposed low rank kernel learning problem by coolest.",
                    "label": 1
                },
                {
                    "sent": "And from this equivalence we can show a few things.",
                    "label": 0
                },
                {
                    "sent": "So first is that our problem objective specifically, that of computing the relative entropy between two Gaussians.",
                    "label": 1
                },
                {
                    "sent": "This can be actually computed in closed form in its raw form.",
                    "label": 0
                },
                {
                    "sent": "It's an integral over in D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It's not apparently obvious that that you can compute it simply, but you can and 2nd this equivalence shows that the problem is in fact convex.",
                    "label": 1
                },
                {
                    "sent": "Which of course is good.",
                    "label": 0
                },
                {
                    "sent": "And finally, of course.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Besides having sort of being an interesting sort of theoretical equivalence, there is some nice practical value to this as well.",
                    "label": 0
                },
                {
                    "sent": "There are some excellent algorithms developed by Coolest to solve the low rank kernel learning problem efficiently, and we use that we use those to solve our problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next, I'll first give a very brief overview of the kernel learning problem.",
                    "label": 1
                },
                {
                    "sent": "In some sense it's looks somewhat similar to ours, but we'll see the differences shortly, so again, we're assuming we're given endpoints in D dimensional space, and we define our kernel K not to be the standard inner product.",
                    "label": 0
                },
                {
                    "sent": "Kernel X transpose X is an end by end kernel over the points.",
                    "label": 0
                },
                {
                    "sent": "And similarly, we also assumed to have a set of similarity and or dissimilarity constraints relating the distance between pairs of points.",
                    "label": 1
                },
                {
                    "sent": "Our objective here is to learn a kernel matrix K that minimizes the divergences decay, not subject to our constraints.",
                    "label": 1
                },
                {
                    "sent": "So what sort of divergance is used here?",
                    "label": 0
                },
                {
                    "sent": "So the problem minimizes a divergent measure called the Berg Divergent.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the bird divergences that type of Bregman diversions that Bregman divergences are a wide class of divergent measures.",
                    "label": 0
                },
                {
                    "sent": "They generalize distance measures like squared Euclidean distance KL diversions.",
                    "label": 0
                },
                {
                    "sent": "I divergents, of course, model Anubis distance, but here the bird diversions is actually a function of two matrices.",
                    "label": 0
                },
                {
                    "sent": "We can see here involves a trace in the log of the determinant of the product of the second matrices.",
                    "label": 0
                },
                {
                    "sent": "So this problem seeks to minimize this diversions.",
                    "label": 0
                },
                {
                    "sent": "Subject to these inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "So are you and Al part of the problem statement or are they under your control?",
                    "label": 0
                },
                {
                    "sent": "Are you and ell something you're solving for their part of the problem states you and LR are inputs to the problem.",
                    "label": 0
                },
                {
                    "sent": "So let me first give an important characterization of this optimal kernel K, which is a solution to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kernel learning problem.",
                    "label": 0
                },
                {
                    "sent": "So well, so given this case given Section K. It can be shown that K actually has the same range space as canine.",
                    "label": 1
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Well, if we consider our input vectors X.",
                    "label": 0
                },
                {
                    "sent": "The algorithm seeks a linear transformation W that scales or rotates your input points before some form some new set of input points.",
                    "label": 0
                },
                {
                    "sent": "This is sort of with Sam and some of the other speakers were discussing and it can be shown that the final matrix K is really just an inner product kernel between these new set of points.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the our main equivalence theorem between this lowering kernel learning problem in our problem is the following.",
                    "label": 1
                },
                {
                    "sent": "So given a kernel K of this form.",
                    "label": 0
                },
                {
                    "sent": "We show that if A equals W transpose W which is.",
                    "label": 0
                },
                {
                    "sent": "The the product of these transformation matrices, then this is actually also an optimal solution.",
                    "label": 1
                },
                {
                    "sent": "Of course, with the same corresponding constraints to our metric learning problem.",
                    "label": 1
                },
                {
                    "sent": "Of course, this theorem.",
                    "label": 0
                },
                {
                    "sent": "One common problem with these kernel learning algorithms is how do you generalize.",
                    "label": 0
                },
                {
                    "sent": "And of course, in this special case of this special type of objective function, you actually can generalize.",
                    "label": 0
                },
                {
                    "sent": "So the proof has to let.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As as you'd expect, the first one is stab Lish, is that the objectives are the same.",
                    "label": 1
                },
                {
                    "sent": "So the objective for the kernel learning problem, the bird diversions between the two kernel matrices is just a linear function of the KL diversions between our learn Mahalanobis associated Gaussian of our Learn Mahala nobis distance.",
                    "label": 0
                },
                {
                    "sent": "A an sort of the squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "One sort of surprising fact about this equivalence, is that these kernel matrices K are recalled their end by, and they're over our input points.",
                    "label": 0
                },
                {
                    "sent": "However, the Gaussians are there D dimensional Gaussians, parameterized by D by D covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "And this also this result also builds on a recent connection between the relative entropy and of two Gaussians, Annaberg divergance.",
                    "label": 1
                },
                {
                    "sent": "In particular, it can be shown that the relative entropy between two Gaussians is actually a convex combination of two different Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "One is the burg divergance between the covariances and the other is a Mahalanobis distance between the means.",
                    "label": 0
                },
                {
                    "sent": "So the second lemma.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Establishes that the two problems have the same feasible set.",
                    "label": 1
                },
                {
                    "sent": "So given OK, which is the solution 2 hour a feasible solution to our kernel learning problem?",
                    "label": 0
                },
                {
                    "sent": "If a parameterise is our metric learning problem IIS feasible if and only if K is feasable?",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In light of this equivalence, we can all now describe the algorithms used by the lower in kernel learning problem and how they can be and describe these algorithms that we use for our problem.",
                    "label": 0
                },
                {
                    "sent": "So the kernel learning problem uses a method called bregman's method.",
                    "label": 1
                },
                {
                    "sent": "It's a dual ascent method so it acts on the dual of the problem.",
                    "label": 1
                },
                {
                    "sent": "It can be shown that at each iteration of Bregman's method, the objective function for the dual problem increases.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm works by iteratively projecting the current kernel matrix onto one constraint at a time.",
                    "label": 1
                },
                {
                    "sent": "So one problem arises quite often with bregman's method.",
                    "label": 0
                },
                {
                    "sent": "Is this projection doesn't have a closed form solution?",
                    "label": 0
                },
                {
                    "sent": "However, however, it was shown for the kernel learning problem that this is projection can be computed in closed form quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "Just in closed form.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, the running time for one iteration of the algorithm is just order CD squared, so C is the number of constraints and ears is your dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Of course, let me in terms of a.",
                    "label": 0
                },
                {
                    "sent": "It can't get much faster than that in terms of work per iteration.",
                    "label": 1
                },
                {
                    "sent": "By doing this it actually works in the kernel in factored form, so it doesn't never computes the entire kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "So that's definitely one thing it does.",
                    "label": 0
                },
                {
                    "sent": "And of course this running time is helped greatly by the fact that we had these.",
                    "label": 0
                },
                {
                    "sent": "We can use these closed form projections.",
                    "label": 0
                },
                {
                    "sent": "And most importantly, the algorithm requires.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No eigenvalue decompositions.",
                    "label": 0
                },
                {
                    "sent": "This is a bit surprising because the bird divergance between two matrices can be naturally expressed as a function of the eigenvalues and eigenvectors of the two kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "K&K, not.",
                    "label": 0
                },
                {
                    "sent": "Yet the algorithm does not need to compute them to actually optimize over it.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, many existing methods, I believe neighborhood components analysis must compute eigenvectors project into the Kona positive semidefinite matrices and some other popular metric learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Some work bising all require eigenvalue decompositions, which of course are cubic in the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "I'll discuss a few extensions, so I guess in the discussion early.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today was brought up how to incorporate prior knowledge into metric learning problem, so I'm sort of made this assumption that your baseline.",
                    "label": 0
                },
                {
                    "sent": "So your prior distance metric that you would use is your squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "Fact this is actually not a limitation of our algorithm that can be extended to minimize the KL diversions to other matrices.",
                    "label": 0
                },
                {
                    "sent": "Perhaps, maybe the inverse of your sample covariance matrix may be a natural choice.",
                    "label": 1
                },
                {
                    "sent": "Slack variables or of course.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very important determining these upper and lower bounds can be quite difficult.",
                    "label": 0
                },
                {
                    "sent": "Constraints can be noisy, etc.",
                    "label": 0
                },
                {
                    "sent": "In fact, in our preliminary experiments, which I'll show you in a moment, we do incorporate Slack variables.",
                    "label": 0
                },
                {
                    "sent": "And the running time, the algorithm is no more expensive.",
                    "label": 0
                },
                {
                    "sent": "And finally our frame.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can easily incorporate different types of linear inequality constraints, not just pairwise inequality constraints.",
                    "label": 1
                },
                {
                    "sent": "Some recent work by shots an yokums say they argue that relative pairwise constraints are easier for people at this sort of telesystem.",
                    "label": 0
                },
                {
                    "sent": "You know A&B or closer than CMD etc and our framework can be extended to handle such constraints.",
                    "label": 0
                },
                {
                    "sent": "So next present a few very preliminary experiment experiments.",
                    "label": 0
                },
                {
                    "sent": "So our method methodology is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As such, so we seek to learn a Mahala Novus distance function which maximizes the accuracy of the resulting cannon classifier.",
                    "label": 0
                },
                {
                    "sent": "So our approach is we.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We assume a fully labeled scenario where each input instance has a true label, and we constrain points in the same class to be similar and points in different classes to be dissimilar.",
                    "label": 1
                },
                {
                    "sent": "And we determine these upper and lower bounds by computing the empirical distribution of the squared Euclidean distance between our input points and then selecting the upper on Upper 10th percentile mark as lower bound for the similar dissimilarity constraints and the lower 10% tile as an upper bound for our similarity constraints.",
                    "label": 0
                },
                {
                    "sent": "And finally, a few sort of simplifications for these initial experiments we sampled only shows 100 such constraints.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By by choosing uniformly at random, and we do know parameter tuning with the slack variables, there's of course a gamma parameter which regulates the tradeoff between the slack.",
                    "label": 0
                },
                {
                    "sent": "Very very slack penalty, and the objective function.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we evaluate results via A2 fold cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compare our algorithm, which we call here as I TML information theoretic metric learning to four different methods.",
                    "label": 0
                },
                {
                    "sent": "The first is actually just your standard squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "Your second here is Parameterising Omaha Innovus distance by your inverse covariance.",
                    "label": 0
                },
                {
                    "sent": "The third is LDA, which I which was described in previous slides, so I won't discuss.",
                    "label": 0
                },
                {
                    "sent": "And finally, using an approach just described earlier by Sam MLE algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we see that in general it's.",
                    "label": 0
                },
                {
                    "sent": "There's no clear winner, certainly for so the soybean data set.",
                    "label": 0
                },
                {
                    "sent": "The MLE algorithm seems to do a bit better.",
                    "label": 0
                },
                {
                    "sent": "One thing you'll notice is that since we are regularising against this standard, squared Euclidean distance, sort of the 1st and the 3rd columns behaves similarly.",
                    "label": 0
                },
                {
                    "sent": "Course these experiments are bit preliminary.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we presented an information theoretic formulation to a standard metric learning problem.",
                    "label": 0
                },
                {
                    "sent": "We gave an equivalence between this problem and the lower rank and Anna recently proposed low rank kernel learning problem.",
                    "label": 1
                },
                {
                    "sent": "And from this equivalence we gave memorable to develop efficient algorithms to solve our problem.",
                    "label": 0
                },
                {
                    "sent": "And finally, the experimental results are promising, but definitely much more work is needed here.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I guess just calling up on what Pablo asked.",
                    "label": 0
                },
                {
                    "sent": "It seems a bit aggressive to ask for every point in.",
                    "label": 0
                },
                {
                    "sent": "One class to be.",
                    "label": 0
                },
                {
                    "sent": "Separated from every other point by you and and the other way around.",
                    "label": 0
                },
                {
                    "sent": "Like if you imagine just the boundary right, there's a clean boundary between two classes.",
                    "label": 0
                },
                {
                    "sent": "OK, if I'm sort of, you know, on the North End of the boundary and you're on the South end of my right, it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "That were far away as long as but this algorithm is.",
                    "label": 0
                },
                {
                    "sent": "So is there some geometric assumption here?",
                    "label": 0
                },
                {
                    "sent": "Like in MCM L where it's unimodality I mean.",
                    "label": 0
                },
                {
                    "sent": "At this point I mean, the problem is in its basic form is formulated as you know, these are inputs, but in terms of, you know, unimodality.",
                    "label": 0
                },
                {
                    "sent": "I'm really not sure.",
                    "label": 0
                },
                {
                    "sent": "The constraints relative to some distance you may have already show me if for some reason you had prior knowledge about the pairwise distances.",
                    "label": 0
                },
                {
                    "sent": "Certainly that would be the best way to go, but I guess typically that information is hard to come by.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}