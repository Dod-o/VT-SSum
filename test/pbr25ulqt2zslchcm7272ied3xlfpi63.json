{
    "id": "pbr25ulqt2zslchcm7272ied3xlfpi63",
    "title": "Unsupervised Rank Aggregation with Distance-Based Models",
    "info": {
        "author": [
            "Alexandre Klementiev, University of Illinois at Urbana-Champaign"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/icml08_klementiev_ura/",
    "segmentation": [
        [
            "I had, as Ryan mentioned, this is joint work with my advisor, done Roughton and fellow grad student Kevin Small."
        ],
        [
            "So let me start by a quick motivate few motivational slides.",
            "Let's consider a panel of judges, each one generating a preference of a given set of objects.",
            "And possibly partial preference, and they need to meaningful aggregate aggregate those preferences is a fundamental problem that's ubiquitous.",
            "An number of series, for example information retrieval and LP, etc."
        ],
        [
            "Just to show a couple examples.",
            "One well known example is meta search is where results of queries from multiple search engines are aggregated and so single ranking which hopefully is better than the individual ones.",
            "The problem is hard, partly because the sets of items that are returned by different engines are not the same and also supervision.",
            "If you want to use machine learning techniques, supervision is difficult to get for ranking problems in general.",
            "So people normally resort to using clickthrough data."
        ],
        [
            "Cetera.",
            "Another lesser known example kind of motivating example is cold multilingual named entity discovery and the task there is if you have a bilingual corpus and one side was tagged for named entities, namely locations, organizations, people, etc.",
            "The task is to find their counterparts in the other language and the problem there is that you really don't know anything about the second language.",
            "There's no linguistic information.",
            "So one can imagine many ways to to get what we call incidental supervision from the data to try and rank these candidates.",
            "One example might be since you know that named named entities tend to be transliterated, you can have some translation model that gives you a score for the candidates.",
            "So you can use that score to rank candidates.",
            "Another way might be if these named entities tend to be temporally aligned across corpora, so you can have some sort of temporary temporal alignment score again to record these candidates.",
            "And you can come up with other different ways to do this.",
            "If, for example, if you can score according to contextual similarity or topics in the Arity.",
            "So the general problem is that once you once you have these rankings, how do you combine them?",
            "If you have no supervision?",
            "For example, if bilingual corpus has no temporal information, then the temple rancor is completely noise, so.",
            "So."
        ],
        [
            "So for the rest of the talk, I'm going to start with the kind of giving some stating the problem.",
            "Given a brief overview of what we did and I'm going to come back to their approach later in more detail, and I'm going to give some background about some tools that we use, and then I'll talk about our contribution.",
            "Namely, the framework that we introduce for unsupervised learning, and I'm going to take the framework and instantiate if two different cases of of preferences, the complete permutation and top K lists, and then I'll get to experiment."
        ],
        [
            "Conclude so a little bit more simply, the problem that we're trying to address is that how can we combine partial object preferences from multiple judges into a single joint ranking?",
            "It's it's not a new problem in information retrieval.",
            "It's called data Fusion, and many approaches use heuristics to aggregate these rankings, and that generally tends to assume some domain knowledge that's available.",
            "So supervised machine learning techniques again need labeled training data and for running ranking problems difficult to get, and there's little tends to be a little Internet ater agreement and so."
        ],
        [
            "So, but we'll try to do is we'll try to kind of sidestep these issues and propose an unsupervised learning approach, and the general scenario we're going to consider is that judges will independently generate these rankings, and they will attempt to reproduce some true underlying ranking that we don't see.",
            "And what we'll do is we'll come up with a name based algorithm which will treat the the votes of individual rancors as the observed data and the true ranking as the as the unobserved data.",
            "And then we'll take the framework again and instantiate it in two cases, two permutations and top K lists."
        ],
        [
            "So just to give some brief notes on notation, so small Greek letters are going to refer to permutations over objects X one through XN.",
            "For example, in this case, so Pi would refer to the rank of the object exhibi an \u03c0 to the minus one of Jay will be the index of the object assigned to rank J, so also E will refer to the identity permutation, which is just a natural order X one through XN.",
            "S7 is a suitable and factorial permutations of an elements and the distance.",
            "Just has a regular distance metric properties, but in addition we're going to require one extra property, which is called right in variance.",
            "So essentially what this means is the distance shouldn't really matter on how index the object, so I can go re indexed objects and the distance between two permutations over them should not change.",
            "So, so if that's the case then then I can always given the two permutations that can always re index the objects such as one of 'em is a natural order and the other changes to say some.",
            "Some variable new and so if new is a random variable, then deals news will be the random variable.",
            "So.",
            "But whether or not it's a distance or the random variable, which should be clear from context within the rest of the talk."
        ],
        [
            "So.",
            "So we'll start off with introducing a mellows model.",
            "It's a probability model over permutations, which was introduced in the late 50s, and you can kind of think of it as a as a as a Gaussian over permutations.",
            "It has two parameters, the Theta parameter controls kind of the width of the distribution.",
            "And you know, if it's if it's, if it's zero, then the distribution is uniform and then more negative it becomes the peakier the distribution is.",
            "So another parameters is Sigma and this is the model distribution.",
            "You can think of it as the mode.",
            "The most likely distribution.",
            "Another couple of notes about this model is that the normalization constant Z, as you can see, it's it's very expensive to compute because you have to sum over N factorial permutations.",
            "So we're going to have to keep that in mind.",
            "Another couple of properties is that if these right invariant, then that normalization constant doesn't really depend on Sigma anymore.",
            "It just depends on Theta.",
            "And another property is that if D can be decomposed and as a sum of independent random variables, then the expected value of the we don't need to compute doing factorial computations.",
            "That can actually be reduced to much simpler form."
        ],
        [
            "So let's consider generative story that we're going to.",
            "We're going to use in the rest of the talk.",
            "So what we'll start with will generate a true permutation from some prior PEO pie.",
            "And then we'll draw individual votes from individual experts independently, and the individual experts are going to be those models with that pie that we just drew the troop.",
            "I, as the as the model model.",
            "Parameter pie.",
            "So same location parameter.",
            "So it turns out that for this generative story, the associated conditional."
        ],
        [
            "Model was proposed by Lafferty and Lebanon in 2002, and it's essentially just like a simple models model roughly, but it has a some term here.",
            "It could be derived from the generative story.",
            "Another note is that it's straightforward to generalize both this extension and the original models Model 2 to partial orders.",
            "All you need to do is define an appropriate distance function and you're good to go."
        ],
        [
            "OK, so so that was the.",
            "Preliminaries and an introduction.",
            "So now that we know about mouse models and extent mouse models, we can sort of see what we did."
        ],
        [
            "So so our approach again a little bit more.",
            "Clearly is that we propose a framework to for unsupervised regulation based on the extent of mouse model, mouse formalism and will derive the EM algorithm to actually learn the parameters data.",
            "So our observed data is going to be the votes of individual judges.",
            "So think of the meta search scenario are going to be the rankings given to us by individual search engines, and that's going to be observed data and are an observed data is going to be the underlying true permutation that we don't see.",
            "And we're going to have a queue of these examples.",
            "So this is our observed data in this area and observed data.",
            "So now."
        ],
        [
            "I'm going to skip some math.",
            "Essentially, the algorithm gives us this formulation.",
            "So one thing to note about this is.",
            "Is that both of these sides are are very expensive to compute, so indeed.",
            "This part has as in factorial normalization term.",
            "We're summing over all possible permutations and their N factorial of each of them, and this left hand side is also expensive to compute.",
            "It's the expected value of distance over the standard mouse model, and that has in factorial computations in their normalization term as well.",
            "So."
        ],
        [
            "So what do we do?",
            "This is probably the key slide of the of the talk.",
            "We're going to learn this status for the for this extended mouse model formalism, an for each one of their anchors.",
            "For each one of the eyes here, we're going to estimate the right hand side.",
            "And then we're going to solve the left hand side for Theta I.",
            "So it turns out that for the right hand side you can do a few things.",
            "You can do.",
            "You can use Metropolis Hastings to sample the current version of the model, or you can use heuristics.",
            "I'll talk about that later.",
            "And then also it turns out that.",
            "You can.",
            "Learn for sorry you can solve for Theta.",
            "I quickly for particular types of distance functions which will again touch on it in a few slides.",
            "And then once you learned your status, you can just do sampling to get the most likely pie.",
            "Given the votes that the experts give you."
        ],
        [
            "So note that up to this point, I haven't really committed to any particular type of rankings, so the framework really doesn't care about as long as you have an appropriate distance function.",
            "So in order to instantiate that framework to a particular type of ranking, we need to do two things.",
            "We need to design an appropriate distance function for the for the setting, and again if it has certain properties that the left hand side estimation is going to be easy.",
            "And then we'll need to design A sampling procedure to to estimate the right hand side, so let's.",
            "Kind of take these ideas and plug them into a particular setting in fact."
        ],
        [
            "So let's start with the permutations so and let's commit to a particular distance function, namely the Kendall distance, which is defined as a minimum number of adjacent transpositions.",
            "You need to do to move from one permutation to another.",
            "So it was shown in literature that it could be decomposed as a sum of independent random variables.",
            "Like so.",
            "And.",
            "The expected value could be computed.",
            "You don't have to do in fact or computations anymore.",
            "In fact, it could be reduced to a simple format.",
            "Just end terms, and Moreover it's a very nice monotonically decreasing function, so you can do line search and solve for Theta very quickly.",
            "So it was the left hand side."
        ],
        [
            "The right hand side.",
            "If you remember we need to do the that we need to design A sampling procedure.",
            "So sampling is actually fairly straightforward here.",
            "We'll start with a random permutation would take two arbitrary items in the permutation, we switch them places, and then we'll do the standard Metropolis Hastings kind of dance.",
            "So the one thing to note here is that.",
            "Because we the only difference between the first \u03c0 and \u03c0 prime here is just the two switch elements.",
            "We don't have to recompute the entire distance, so we can just compute the incremental distance.",
            "So if the chain steps are actually very quick and very, very efficient.",
            "And there's some convergence results, which I'll glance over."
        ],
        [
            "So another thing we could do is I mentioned is a heuristic, so as opposed to sampling and compute to estimate the right hand side, we can use circle board account.",
            "Going to add weights to it, so essentially it's a linear combination of of rankings of Franks of each item arc sorted to get the get the complete ranking, except that we're going to use for weights, we're going to use our parameters for.",
            "Current data parameters.",
            "So, so we have two ways to do the right hand side.",
            "So that was permutations and I'll get to the experiments.",
            "In a few slides now."
        ],
        [
            "Even more interesting case is a top K lists.",
            "Is is the is when your experts are given given you preferences over a subset of Cal.",
            "Men's over large set of elements.",
            "Here things a little bit more interesting because the sets of objects, the preferences of objects that they're going to give back are different.",
            "So you might have objects in one ranking that that you don't have any other.",
            "So what we do is we introduce kind of an extension to Kendall's tell and what we do is we say.",
            "So suppose the objective is to get from here to here to measure the distance between this and this so.",
            "Will put all the elements in here, but not in here into the K plus first position.",
            "They're going to share the same rank, and then we're going to define the augmented.",
            "Kendall distance.",
            "Says just minimum number of adjacent transpositions.",
            "Again to bring one into the other.",
            "So that distance can be broken into three parts.",
            "One is, you know you want to bring these Gray boxes to the bottom.",
            "Then you want to switch them with the elements in the K plus first position, and then you just want to do the regular Kendall on the on the white boxes.",
            "So nicely these UYS or UINVI till this are independent of each other and then we can use again the same tricks to to sort of get rid of the complexity of."
        ],
        [
            "Left hand side.",
            "And it turns out that the expected value again could be analytically socota.",
            "Too much simpler form.",
            "So again, it's a monotonically decreasing function.",
            "Linesearch is really easy.",
            "And kind of a small note is that both the expected value, the distance and the distance reduced to the Kendall style one.",
            "If you have the both rankings are over the same object.",
            "So it's kind of a generalization of Kendall style.",
            "And then same sampling and heuristics that I talked about for the permutations are essentially unchanged for for the.",
            "For the top case situation."
        ],
        [
            "So, so we've I've introduced to the kind of the model and the learning procedure and inference procedure.",
            "An sort of showed how it could be instantiated for two different cases.",
            "The permutations in the top K lists.",
            "Now I'll talk about experiments."
        ],
        [
            "So so first we.",
            "Would like to see how the model works with different heuristics with heuristics and sampling on the right hand side on on.",
            "On simulated data, so we have 10 judges.",
            "Each one is actually a models model.",
            "This model their ranking over 30 objects, and they're kind of doing this for 10:10 queries so.",
            "So the line that you see.",
            "On the right is that since we have the true rankings we generated the data we can actually instead of estimating the right hand side, we can compute that average distance.",
            "So so this is a true.",
            "The blue value here is that heuristic that I talked about is the weighted Borda count.",
            "And the red is the sampling procedure, so we see that sampling procedure gets to the true fairly quickly, so.",
            "Just to mention, out of these 10 males models, two were pretty good, six were mediocre and two were completely random.",
            "They had the Theta values were zero, so they're completely uniform, generating random noise."
        ],
        [
            "Another example, it says Meta search.",
            "We wanted to look at what these dispersion parameters that we estimating actually mean.",
            "So we have four search engines.",
            "Call miss one through S4.",
            "They return results to queries Top Top 100 results 250 queries.",
            "And.",
            "We chose queries such that there is kind of an unambiguous right answer to the query that is right page so that we can kind of estimate the results.",
            "So we define the evaluation measures as this mean reciprocal Pagerank.",
            "Essentially, it's one over the page which contain the correct result and in the output.",
            "So it turns out that the as you can see, the Theta does kind of, so the more negative Theta, the more Peaky the Theta the the higher the value of the mineral page rank is an, by the way, are we got zero point 92, which is better out of the best.",
            "Value out of the four."
        ],
        [
            "So let's experiment is is the top key situation which took 38 Trek three ad hoc retrieval shark test participants.",
            "You can think of those essentially search engines, each of which is pretty good.",
            "And they get.",
            "There were 50 queries and each one produced 100 documents for each query.",
            "And what we did is we took the the.",
            "38 rancors 3 rankings an and randomly chose case a bar out of those and replace it with random noise and our baseline is essentially is something that's commonly used in IR.",
            "It's essentially board account weighted by the number of.",
            "Number of rancors that have that element in in the rankings."
        ],
        [
            "So the blue lines are the baseline, the red ones are.",
            "Is is our algorithm and the X axis here is the number of random rancors out of 38 total, so so you can see that up to about 35 we're actually doing, especially for the top 30 case.",
            "We're actually doing fairly well.",
            "So so even with three out of 38 good rancors and the rest complete noise, we were able to learn.",
            "To discard the bed rinkers without supervision."
        ],
        [
            "So to conclude, we proposed mathematical algorithm framework to aggregate.",
            "Partial rankings and we we really don't commit in the model.",
            "We really don't commit to any kind of rankings in order to plug it in, you need to essentially design a distance function and an either heuristic or sampling procedure for the right hand side.",
            "And we instantiate the framework for both permutations and top K lists and derive the novel distance function for the top case.",
            "Correctly then you.",
            "Extend the comparison of the top K rancors cases where the items are not the same and then you solve them again, right?",
            "So so this is.",
            "Another scenario is that the two rancors are not presented with the same items, so solving them at the end might not actually be appropriate for the situation.",
            "You might want to do an imputation in a particular part of the ranking where this item would have ended up with the rank of would have been asked about that.",
            "Uh, no, not exactly actually kind of related to this question.",
            "One thing that we're considering now is that if you can imagine that different items are particularly types, so you can imagine that are anchors is better at ranking certain types of items.",
            "Say for example pictures versus web pages, and So what we're considering is kind of an extension to the model where the objects are of different types, and we're trying to combine them.",
            "So maybe not exactly what you're asking.",
            "Don't really care too much about the low rank items.",
            "References you know.",
            "Weighted down with appropriately so that it really reflects the natural distance between them.",
            "Yeah, so as long as you design a distance function that takes care of that waiting and has these properties that they write invariants and become possibility, you should be able to do this just fine.",
            "So it's a question of designing the right distance function.",
            "So how does?",
            "Decision very relative to the amount of unlabeled baby use or the number of.",
            "Experts speak quickly or do you just like that study?",
            "Actually, it's a good question.",
            "I will never run this sort of experiments, but so for technical reasons, as long as you have three experts more than two things should work.",
            "So clearly, the more experts you have, the better.",
            "And so.",
            "So the underlying assumption.",
            "True breakfast yes.",
            "Preferred ways of ranking?",
            "Could you have some sort of prize in there?",
            "So.",
            "Right, so that's a good question.",
            "So essentially the question is, is you have?",
            "You can potentially have rancors that there are bad and collude, and in this case the thing doesn't work anymore.",
            "So breaks so and we're working on something that would take care of groupings of rankings.",
            "Essentially just like you suggested."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I had, as Ryan mentioned, this is joint work with my advisor, done Roughton and fellow grad student Kevin Small.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me start by a quick motivate few motivational slides.",
                    "label": 0
                },
                {
                    "sent": "Let's consider a panel of judges, each one generating a preference of a given set of objects.",
                    "label": 1
                },
                {
                    "sent": "And possibly partial preference, and they need to meaningful aggregate aggregate those preferences is a fundamental problem that's ubiquitous.",
                    "label": 0
                },
                {
                    "sent": "An number of series, for example information retrieval and LP, etc.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to show a couple examples.",
                    "label": 0
                },
                {
                    "sent": "One well known example is meta search is where results of queries from multiple search engines are aggregated and so single ranking which hopefully is better than the individual ones.",
                    "label": 1
                },
                {
                    "sent": "The problem is hard, partly because the sets of items that are returned by different engines are not the same and also supervision.",
                    "label": 0
                },
                {
                    "sent": "If you want to use machine learning techniques, supervision is difficult to get for ranking problems in general.",
                    "label": 1
                },
                {
                    "sent": "So people normally resort to using clickthrough data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cetera.",
                    "label": 0
                },
                {
                    "sent": "Another lesser known example kind of motivating example is cold multilingual named entity discovery and the task there is if you have a bilingual corpus and one side was tagged for named entities, namely locations, organizations, people, etc.",
                    "label": 1
                },
                {
                    "sent": "The task is to find their counterparts in the other language and the problem there is that you really don't know anything about the second language.",
                    "label": 0
                },
                {
                    "sent": "There's no linguistic information.",
                    "label": 0
                },
                {
                    "sent": "So one can imagine many ways to to get what we call incidental supervision from the data to try and rank these candidates.",
                    "label": 0
                },
                {
                    "sent": "One example might be since you know that named named entities tend to be transliterated, you can have some translation model that gives you a score for the candidates.",
                    "label": 1
                },
                {
                    "sent": "So you can use that score to rank candidates.",
                    "label": 0
                },
                {
                    "sent": "Another way might be if these named entities tend to be temporally aligned across corpora, so you can have some sort of temporary temporal alignment score again to record these candidates.",
                    "label": 1
                },
                {
                    "sent": "And you can come up with other different ways to do this.",
                    "label": 0
                },
                {
                    "sent": "If, for example, if you can score according to contextual similarity or topics in the Arity.",
                    "label": 0
                },
                {
                    "sent": "So the general problem is that once you once you have these rankings, how do you combine them?",
                    "label": 0
                },
                {
                    "sent": "If you have no supervision?",
                    "label": 0
                },
                {
                    "sent": "For example, if bilingual corpus has no temporal information, then the temple rancor is completely noise, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the rest of the talk, I'm going to start with the kind of giving some stating the problem.",
                    "label": 1
                },
                {
                    "sent": "Given a brief overview of what we did and I'm going to come back to their approach later in more detail, and I'm going to give some background about some tools that we use, and then I'll talk about our contribution.",
                    "label": 1
                },
                {
                    "sent": "Namely, the framework that we introduce for unsupervised learning, and I'm going to take the framework and instantiate if two different cases of of preferences, the complete permutation and top K lists, and then I'll get to experiment.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclude so a little bit more simply, the problem that we're trying to address is that how can we combine partial object preferences from multiple judges into a single joint ranking?",
                    "label": 1
                },
                {
                    "sent": "It's it's not a new problem in information retrieval.",
                    "label": 0
                },
                {
                    "sent": "It's called data Fusion, and many approaches use heuristics to aggregate these rankings, and that generally tends to assume some domain knowledge that's available.",
                    "label": 1
                },
                {
                    "sent": "So supervised machine learning techniques again need labeled training data and for running ranking problems difficult to get, and there's little tends to be a little Internet ater agreement and so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but we'll try to do is we'll try to kind of sidestep these issues and propose an unsupervised learning approach, and the general scenario we're going to consider is that judges will independently generate these rankings, and they will attempt to reproduce some true underlying ranking that we don't see.",
                    "label": 0
                },
                {
                    "sent": "And what we'll do is we'll come up with a name based algorithm which will treat the the votes of individual rancors as the observed data and the true ranking as the as the unobserved data.",
                    "label": 1
                },
                {
                    "sent": "And then we'll take the framework again and instantiate it in two cases, two permutations and top K lists.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give some brief notes on notation, so small Greek letters are going to refer to permutations over objects X one through XN.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case, so Pi would refer to the rank of the object exhibi an \u03c0 to the minus one of Jay will be the index of the object assigned to rank J, so also E will refer to the identity permutation, which is just a natural order X one through XN.",
                    "label": 1
                },
                {
                    "sent": "S7 is a suitable and factorial permutations of an elements and the distance.",
                    "label": 0
                },
                {
                    "sent": "Just has a regular distance metric properties, but in addition we're going to require one extra property, which is called right in variance.",
                    "label": 0
                },
                {
                    "sent": "So essentially what this means is the distance shouldn't really matter on how index the object, so I can go re indexed objects and the distance between two permutations over them should not change.",
                    "label": 0
                },
                {
                    "sent": "So, so if that's the case then then I can always given the two permutations that can always re index the objects such as one of 'em is a natural order and the other changes to say some.",
                    "label": 0
                },
                {
                    "sent": "Some variable new and so if new is a random variable, then deals news will be the random variable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But whether or not it's a distance or the random variable, which should be clear from context within the rest of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we'll start off with introducing a mellows model.",
                    "label": 0
                },
                {
                    "sent": "It's a probability model over permutations, which was introduced in the late 50s, and you can kind of think of it as a as a as a Gaussian over permutations.",
                    "label": 0
                },
                {
                    "sent": "It has two parameters, the Theta parameter controls kind of the width of the distribution.",
                    "label": 0
                },
                {
                    "sent": "And you know, if it's if it's, if it's zero, then the distribution is uniform and then more negative it becomes the peakier the distribution is.",
                    "label": 0
                },
                {
                    "sent": "So another parameters is Sigma and this is the model distribution.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as the mode.",
                    "label": 0
                },
                {
                    "sent": "The most likely distribution.",
                    "label": 0
                },
                {
                    "sent": "Another couple of notes about this model is that the normalization constant Z, as you can see, it's it's very expensive to compute because you have to sum over N factorial permutations.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to keep that in mind.",
                    "label": 0
                },
                {
                    "sent": "Another couple of properties is that if these right invariant, then that normalization constant doesn't really depend on Sigma anymore.",
                    "label": 0
                },
                {
                    "sent": "It just depends on Theta.",
                    "label": 0
                },
                {
                    "sent": "And another property is that if D can be decomposed and as a sum of independent random variables, then the expected value of the we don't need to compute doing factorial computations.",
                    "label": 1
                },
                {
                    "sent": "That can actually be reduced to much simpler form.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's consider generative story that we're going to.",
                    "label": 1
                },
                {
                    "sent": "We're going to use in the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "So what we'll start with will generate a true permutation from some prior PEO pie.",
                    "label": 0
                },
                {
                    "sent": "And then we'll draw individual votes from individual experts independently, and the individual experts are going to be those models with that pie that we just drew the troop.",
                    "label": 0
                },
                {
                    "sent": "I, as the as the model model.",
                    "label": 0
                },
                {
                    "sent": "Parameter pie.",
                    "label": 0
                },
                {
                    "sent": "So same location parameter.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that for this generative story, the associated conditional.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model was proposed by Lafferty and Lebanon in 2002, and it's essentially just like a simple models model roughly, but it has a some term here.",
                    "label": 0
                },
                {
                    "sent": "It could be derived from the generative story.",
                    "label": 0
                },
                {
                    "sent": "Another note is that it's straightforward to generalize both this extension and the original models Model 2 to partial orders.",
                    "label": 1
                },
                {
                    "sent": "All you need to do is define an appropriate distance function and you're good to go.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so that was the.",
                    "label": 0
                },
                {
                    "sent": "Preliminaries and an introduction.",
                    "label": 0
                },
                {
                    "sent": "So now that we know about mouse models and extent mouse models, we can sort of see what we did.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so our approach again a little bit more.",
                    "label": 1
                },
                {
                    "sent": "Clearly is that we propose a framework to for unsupervised regulation based on the extent of mouse model, mouse formalism and will derive the EM algorithm to actually learn the parameters data.",
                    "label": 1
                },
                {
                    "sent": "So our observed data is going to be the votes of individual judges.",
                    "label": 0
                },
                {
                    "sent": "So think of the meta search scenario are going to be the rankings given to us by individual search engines, and that's going to be observed data and are an observed data is going to be the underlying true permutation that we don't see.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have a queue of these examples.",
                    "label": 0
                },
                {
                    "sent": "So this is our observed data in this area and observed data.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip some math.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the algorithm gives us this formulation.",
                    "label": 0
                },
                {
                    "sent": "So one thing to note about this is.",
                    "label": 0
                },
                {
                    "sent": "Is that both of these sides are are very expensive to compute, so indeed.",
                    "label": 0
                },
                {
                    "sent": "This part has as in factorial normalization term.",
                    "label": 0
                },
                {
                    "sent": "We're summing over all possible permutations and their N factorial of each of them, and this left hand side is also expensive to compute.",
                    "label": 0
                },
                {
                    "sent": "It's the expected value of distance over the standard mouse model, and that has in factorial computations in their normalization term as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "This is probably the key slide of the of the talk.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn this status for the for this extended mouse model formalism, an for each one of their anchors.",
                    "label": 0
                },
                {
                    "sent": "For each one of the eyes here, we're going to estimate the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to solve the left hand side for Theta I.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that for the right hand side you can do a few things.",
                    "label": 0
                },
                {
                    "sent": "You can do.",
                    "label": 0
                },
                {
                    "sent": "You can use Metropolis Hastings to sample the current version of the model, or you can use heuristics.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that later.",
                    "label": 0
                },
                {
                    "sent": "And then also it turns out that.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Learn for sorry you can solve for Theta.",
                    "label": 0
                },
                {
                    "sent": "I quickly for particular types of distance functions which will again touch on it in a few slides.",
                    "label": 1
                },
                {
                    "sent": "And then once you learned your status, you can just do sampling to get the most likely pie.",
                    "label": 0
                },
                {
                    "sent": "Given the votes that the experts give you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So note that up to this point, I haven't really committed to any particular type of rankings, so the framework really doesn't care about as long as you have an appropriate distance function.",
                    "label": 0
                },
                {
                    "sent": "So in order to instantiate that framework to a particular type of ranking, we need to do two things.",
                    "label": 1
                },
                {
                    "sent": "We need to design an appropriate distance function for the for the setting, and again if it has certain properties that the left hand side estimation is going to be easy.",
                    "label": 1
                },
                {
                    "sent": "And then we'll need to design A sampling procedure to to estimate the right hand side, so let's.",
                    "label": 0
                },
                {
                    "sent": "Kind of take these ideas and plug them into a particular setting in fact.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the permutations so and let's commit to a particular distance function, namely the Kendall distance, which is defined as a minimum number of adjacent transpositions.",
                    "label": 0
                },
                {
                    "sent": "You need to do to move from one permutation to another.",
                    "label": 0
                },
                {
                    "sent": "So it was shown in literature that it could be decomposed as a sum of independent random variables.",
                    "label": 1
                },
                {
                    "sent": "Like so.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "The expected value could be computed.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do in fact or computations anymore.",
                    "label": 0
                },
                {
                    "sent": "In fact, it could be reduced to a simple format.",
                    "label": 0
                },
                {
                    "sent": "Just end terms, and Moreover it's a very nice monotonically decreasing function, so you can do line search and solve for Theta very quickly.",
                    "label": 0
                },
                {
                    "sent": "So it was the left hand side.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The right hand side.",
                    "label": 0
                },
                {
                    "sent": "If you remember we need to do the that we need to design A sampling procedure.",
                    "label": 1
                },
                {
                    "sent": "So sampling is actually fairly straightforward here.",
                    "label": 0
                },
                {
                    "sent": "We'll start with a random permutation would take two arbitrary items in the permutation, we switch them places, and then we'll do the standard Metropolis Hastings kind of dance.",
                    "label": 1
                },
                {
                    "sent": "So the one thing to note here is that.",
                    "label": 1
                },
                {
                    "sent": "Because we the only difference between the first \u03c0 and \u03c0 prime here is just the two switch elements.",
                    "label": 0
                },
                {
                    "sent": "We don't have to recompute the entire distance, so we can just compute the incremental distance.",
                    "label": 0
                },
                {
                    "sent": "So if the chain steps are actually very quick and very, very efficient.",
                    "label": 1
                },
                {
                    "sent": "And there's some convergence results, which I'll glance over.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another thing we could do is I mentioned is a heuristic, so as opposed to sampling and compute to estimate the right hand side, we can use circle board account.",
                    "label": 0
                },
                {
                    "sent": "Going to add weights to it, so essentially it's a linear combination of of rankings of Franks of each item arc sorted to get the get the complete ranking, except that we're going to use for weights, we're going to use our parameters for.",
                    "label": 0
                },
                {
                    "sent": "Current data parameters.",
                    "label": 0
                },
                {
                    "sent": "So, so we have two ways to do the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So that was permutations and I'll get to the experiments.",
                    "label": 0
                },
                {
                    "sent": "In a few slides now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even more interesting case is a top K lists.",
                    "label": 0
                },
                {
                    "sent": "Is is the is when your experts are given given you preferences over a subset of Cal.",
                    "label": 0
                },
                {
                    "sent": "Men's over large set of elements.",
                    "label": 0
                },
                {
                    "sent": "Here things a little bit more interesting because the sets of objects, the preferences of objects that they're going to give back are different.",
                    "label": 0
                },
                {
                    "sent": "So you might have objects in one ranking that that you don't have any other.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we introduce kind of an extension to Kendall's tell and what we do is we say.",
                    "label": 0
                },
                {
                    "sent": "So suppose the objective is to get from here to here to measure the distance between this and this so.",
                    "label": 0
                },
                {
                    "sent": "Will put all the elements in here, but not in here into the K plus first position.",
                    "label": 0
                },
                {
                    "sent": "They're going to share the same rank, and then we're going to define the augmented.",
                    "label": 0
                },
                {
                    "sent": "Kendall distance.",
                    "label": 0
                },
                {
                    "sent": "Says just minimum number of adjacent transpositions.",
                    "label": 0
                },
                {
                    "sent": "Again to bring one into the other.",
                    "label": 0
                },
                {
                    "sent": "So that distance can be broken into three parts.",
                    "label": 0
                },
                {
                    "sent": "One is, you know you want to bring these Gray boxes to the bottom.",
                    "label": 0
                },
                {
                    "sent": "Then you want to switch them with the elements in the K plus first position, and then you just want to do the regular Kendall on the on the white boxes.",
                    "label": 1
                },
                {
                    "sent": "So nicely these UYS or UINVI till this are independent of each other and then we can use again the same tricks to to sort of get rid of the complexity of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Left hand side.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the expected value again could be analytically socota.",
                    "label": 0
                },
                {
                    "sent": "Too much simpler form.",
                    "label": 0
                },
                {
                    "sent": "So again, it's a monotonically decreasing function.",
                    "label": 1
                },
                {
                    "sent": "Linesearch is really easy.",
                    "label": 1
                },
                {
                    "sent": "And kind of a small note is that both the expected value, the distance and the distance reduced to the Kendall style one.",
                    "label": 1
                },
                {
                    "sent": "If you have the both rankings are over the same object.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a generalization of Kendall style.",
                    "label": 0
                },
                {
                    "sent": "And then same sampling and heuristics that I talked about for the permutations are essentially unchanged for for the.",
                    "label": 0
                },
                {
                    "sent": "For the top case situation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we've I've introduced to the kind of the model and the learning procedure and inference procedure.",
                    "label": 1
                },
                {
                    "sent": "An sort of showed how it could be instantiated for two different cases.",
                    "label": 0
                },
                {
                    "sent": "The permutations in the top K lists.",
                    "label": 0
                },
                {
                    "sent": "Now I'll talk about experiments.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so first we.",
                    "label": 0
                },
                {
                    "sent": "Would like to see how the model works with different heuristics with heuristics and sampling on the right hand side on on.",
                    "label": 0
                },
                {
                    "sent": "On simulated data, so we have 10 judges.",
                    "label": 0
                },
                {
                    "sent": "Each one is actually a models model.",
                    "label": 0
                },
                {
                    "sent": "This model their ranking over 30 objects, and they're kind of doing this for 10:10 queries so.",
                    "label": 0
                },
                {
                    "sent": "So the line that you see.",
                    "label": 0
                },
                {
                    "sent": "On the right is that since we have the true rankings we generated the data we can actually instead of estimating the right hand side, we can compute that average distance.",
                    "label": 0
                },
                {
                    "sent": "So so this is a true.",
                    "label": 0
                },
                {
                    "sent": "The blue value here is that heuristic that I talked about is the weighted Borda count.",
                    "label": 0
                },
                {
                    "sent": "And the red is the sampling procedure, so we see that sampling procedure gets to the true fairly quickly, so.",
                    "label": 0
                },
                {
                    "sent": "Just to mention, out of these 10 males models, two were pretty good, six were mediocre and two were completely random.",
                    "label": 0
                },
                {
                    "sent": "They had the Theta values were zero, so they're completely uniform, generating random noise.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example, it says Meta search.",
                    "label": 0
                },
                {
                    "sent": "We wanted to look at what these dispersion parameters that we estimating actually mean.",
                    "label": 1
                },
                {
                    "sent": "So we have four search engines.",
                    "label": 1
                },
                {
                    "sent": "Call miss one through S4.",
                    "label": 0
                },
                {
                    "sent": "They return results to queries Top Top 100 results 250 queries.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We chose queries such that there is kind of an unambiguous right answer to the query that is right page so that we can kind of estimate the results.",
                    "label": 1
                },
                {
                    "sent": "So we define the evaluation measures as this mean reciprocal Pagerank.",
                    "label": 0
                },
                {
                    "sent": "Essentially, it's one over the page which contain the correct result and in the output.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that the as you can see, the Theta does kind of, so the more negative Theta, the more Peaky the Theta the the higher the value of the mineral page rank is an, by the way, are we got zero point 92, which is better out of the best.",
                    "label": 1
                },
                {
                    "sent": "Value out of the four.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's experiment is is the top key situation which took 38 Trek three ad hoc retrieval shark test participants.",
                    "label": 1
                },
                {
                    "sent": "You can think of those essentially search engines, each of which is pretty good.",
                    "label": 0
                },
                {
                    "sent": "And they get.",
                    "label": 0
                },
                {
                    "sent": "There were 50 queries and each one produced 100 documents for each query.",
                    "label": 1
                },
                {
                    "sent": "And what we did is we took the the.",
                    "label": 0
                },
                {
                    "sent": "38 rancors 3 rankings an and randomly chose case a bar out of those and replace it with random noise and our baseline is essentially is something that's commonly used in IR.",
                    "label": 0
                },
                {
                    "sent": "It's essentially board account weighted by the number of.",
                    "label": 1
                },
                {
                    "sent": "Number of rancors that have that element in in the rankings.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the blue lines are the baseline, the red ones are.",
                    "label": 0
                },
                {
                    "sent": "Is is our algorithm and the X axis here is the number of random rancors out of 38 total, so so you can see that up to about 35 we're actually doing, especially for the top 30 case.",
                    "label": 0
                },
                {
                    "sent": "We're actually doing fairly well.",
                    "label": 0
                },
                {
                    "sent": "So so even with three out of 38 good rancors and the rest complete noise, we were able to learn.",
                    "label": 0
                },
                {
                    "sent": "To discard the bed rinkers without supervision.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we proposed mathematical algorithm framework to aggregate.",
                    "label": 0
                },
                {
                    "sent": "Partial rankings and we we really don't commit in the model.",
                    "label": 0
                },
                {
                    "sent": "We really don't commit to any kind of rankings in order to plug it in, you need to essentially design a distance function and an either heuristic or sampling procedure for the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And we instantiate the framework for both permutations and top K lists and derive the novel distance function for the top case.",
                    "label": 1
                },
                {
                    "sent": "Correctly then you.",
                    "label": 0
                },
                {
                    "sent": "Extend the comparison of the top K rancors cases where the items are not the same and then you solve them again, right?",
                    "label": 0
                },
                {
                    "sent": "So so this is.",
                    "label": 0
                },
                {
                    "sent": "Another scenario is that the two rancors are not presented with the same items, so solving them at the end might not actually be appropriate for the situation.",
                    "label": 0
                },
                {
                    "sent": "You might want to do an imputation in a particular part of the ranking where this item would have ended up with the rank of would have been asked about that.",
                    "label": 0
                },
                {
                    "sent": "Uh, no, not exactly actually kind of related to this question.",
                    "label": 0
                },
                {
                    "sent": "One thing that we're considering now is that if you can imagine that different items are particularly types, so you can imagine that are anchors is better at ranking certain types of items.",
                    "label": 0
                },
                {
                    "sent": "Say for example pictures versus web pages, and So what we're considering is kind of an extension to the model where the objects are of different types, and we're trying to combine them.",
                    "label": 0
                },
                {
                    "sent": "So maybe not exactly what you're asking.",
                    "label": 0
                },
                {
                    "sent": "Don't really care too much about the low rank items.",
                    "label": 0
                },
                {
                    "sent": "References you know.",
                    "label": 0
                },
                {
                    "sent": "Weighted down with appropriately so that it really reflects the natural distance between them.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so as long as you design a distance function that takes care of that waiting and has these properties that they write invariants and become possibility, you should be able to do this just fine.",
                    "label": 0
                },
                {
                    "sent": "So it's a question of designing the right distance function.",
                    "label": 0
                },
                {
                    "sent": "So how does?",
                    "label": 0
                },
                {
                    "sent": "Decision very relative to the amount of unlabeled baby use or the number of.",
                    "label": 0
                },
                {
                    "sent": "Experts speak quickly or do you just like that study?",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I will never run this sort of experiments, but so for technical reasons, as long as you have three experts more than two things should work.",
                    "label": 0
                },
                {
                    "sent": "So clearly, the more experts you have, the better.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So the underlying assumption.",
                    "label": 0
                },
                {
                    "sent": "True breakfast yes.",
                    "label": 0
                },
                {
                    "sent": "Preferred ways of ranking?",
                    "label": 0
                },
                {
                    "sent": "Could you have some sort of prize in there?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So essentially the question is, is you have?",
                    "label": 0
                },
                {
                    "sent": "You can potentially have rancors that there are bad and collude, and in this case the thing doesn't work anymore.",
                    "label": 0
                },
                {
                    "sent": "So breaks so and we're working on something that would take care of groupings of rankings.",
                    "label": 0
                },
                {
                    "sent": "Essentially just like you suggested.",
                    "label": 0
                }
            ]
        }
    }
}