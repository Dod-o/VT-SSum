{
    "id": "xu2vdoqhroub2d2blptst5ztsym7hart",
    "title": "Towards Monitoring of Novel Statements in the News",
    "info": {
        "author": [
            "Michael F\u00e4rber, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "July 28, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_farber_novel_statements/",
    "segmentation": [
        [
            "My name is Michael.",
            "I come from the Culture Institute of Technology in Germany in I will present today the topic 2 words monitoring of novel statements in the news."
        ],
        [
            "Imagine that you are a stock broker or investor who wants to invest in some companies or startups.",
            "For example, you want to have recent activities there.",
            "And you Google for example for startup acquisitions or similar keywords.",
            "Here get a list of results like what Quora has written and even maybe some.",
            "Written statements here that eBay acquires an AI startup expert maker.",
            "If you're more in a professional setting, you might use the Bloomberg terminal."
        ],
        [
            "Here you also have news for example, also directly tasked with.",
            "If it's coming from Bloomberg News or if it's a breaking news and so on."
        ],
        [
            "What all these systems have in common is that they only have.",
            "A system which is based on relevance is cartoon but not on novelty.",
            "But this is what we want to focus on now.",
            "So we want to focus on finding novel effects in unstructured documents.",
            "There are few anomaly detection systems which search for novelty like normal information.",
            "Is this cut could be topics, events and so on.",
            "But what all these have in common is that they are mainly statistically but not semantically so they have no grounding a knowledge base."
        ],
        [
            "This is what we can see here.",
            "So there are information extraction systems which have a grounding in knowledge base.",
            "Which means that they either link entities to enter the knowledge base or relations expressing their text to a knowledge base.",
            "Then the earth.",
            "Open information extraction systems, which naturally have no grounding, a knowledge base and then there are dedicated knowledge detection systems which focus on the novelty aspect but do not provide that grounding.",
            "So this is what we can see here.",
            "What we want to tackle is now.",
            "Finding or implementing an over detection systems which have a grounding a knowledge base and which focuses on facts that is statements.",
            "Almost."
        ],
        [
            "Sorry, looks like that that we want to build our semantic Noble effect extraction system and we have basically three of them assumptions which we make.",
            "The first one is that we focus on triples as statements.",
            "I use statements triples and facts interchangeably here.",
            "So we have your eyes for the subject, for the predicate, and for the object.",
            "As you can see here.",
            "The second assumption is that you do not only have one of them, but many, and this is a knowledge base.",
            "You can think of semantic mediawiki's in a company, but also of more generic knowledge graphs like DP to hear the ideas here that.",
            "This knowledge base contains the knowledge the user is currently interested in.",
            "So for example, startups and so on.",
            "And the third assumption is that no effects are contained in unstructured documents.",
            "For example, in use and you want to extract them, you are interested in them.",
            "So in our scenario, our semantic novel fact extraction system extracts no effects from text.",
            "The user can query and few this novel triples and if needed public it to the knowledge base."
        ],
        [
            "Now we need to formalize no effects and how these normal now effects are processed.",
            "For that we make this a statement that no effects are relevant and novel if they are not yet in knowledge base.",
            "But if they somehow overlapped.",
            "So what does it mean?",
            "It means that if you have a triple like here, Apple acquired next and all three parts are in knowledge base.",
            "As you can see here, then this triple is not novel.",
            "Obviously it's only relevant."
        ],
        [
            "If two parts of the triple are in a knowledge base but one partition over through, this could be the subject, the ticket, or the object.",
            "Then we say this triple is novel and relevant because there's an overlap with the knowledge base, so is somehow the context fits.",
            "You can see here.",
            "It's also about acquisitions, and even Apple is also mentioned in the knowledge base."
        ],
        [
            "And finally, we can say if two or three parts of the triple are completely another that is not in knowledge base, then this could be novel and relevant.",
            "But we see it as irrelevant Becauses context is not so much fitting.",
            "We say that.",
            "For example, as you can see here, they extracted triples are often noisy or very abstract.",
            "For example, you don't want to have a triple with coffee shine and Krispy Kreme in a knowledge base about acquisitions of IR startups."
        ],
        [
            "OK then.",
            "Given this definition, that one part needs to be novel in two parts needs to be known.",
            "You can classify this novel triples into subclasses, you can say.",
            "If everything is known, this is this Class A and then you can say if the relation it is if the predicate is not.",
            "It could be that it's Part B, so it could be that.",
            "Be this novel triple.",
            "Here is the subject with which is contained in another triple, which is with the same outgoing relation like your product.",
            "It could be that.",
            "The object is shared with another triple, or it could be that neither of them is the case.",
            "Is this B2 case?",
            "It could also be that their relation, that is the predicate is not known in a knowledge base so far at all, so this would be ontology expansion.",
            "This would be this C case here."
        ],
        [
            "And finally, you also have the case that the subject can be another.",
            "Like here then could be that the object is known and it's also an object of another triple, or it's not the case, or it's a case that the object is normal.",
            "And that the object another object is in a triple of the same subject, or this is not the case?"
        ],
        [
            "OK, so now we can implement our novel effect Ripple effect or affect extraction system.",
            "This basically consists of three steps use given that the user provides some input documents like news.",
            "The idea is that the first do a textual triple extraction, then annoyed space linking, and finally a nobody detection step.",
            "And in following now I will focus on these three steps in more detail."
        ],
        [
            "The first one is called textual triple extraction.",
            "Becaused, given the input sentence, the idea is to extract so-called textual triples.",
            "Textual troubles are statements.",
            "As you can see here with a subject or predicate and object, and they're not your eyes or other.",
            "Praise is extracted from the text so it input sentences.",
            "Here above we do some dependencies through generations.",
            "Some cross detection in some triple generation.",
            "And for that we used it two o'clock hold closely close based information extraction.",
            "So closely extracts propositions, which means you have a subject predicate in one or more arguments, and we adapt this to have triples.",
            "We make some adaptations like transforming passive interactive, neglecting objectives, and so on.",
            "Oak."
        ],
        [
            "Based on given this text to triples.",
            "We need to link them to a knowledge base and for that we need an entity linking step and a predicate linking step.",
            "The entity linking is performed by a text annotation tool or entity linking tool.",
            "For that we use.",
            "Our tool called X Lisa.",
            "It's text notation or interlinking tool.",
            "It's actually 4 cross languages that is.",
            "Four different languages, but we as we will see later we only use it for the English.",
            "We think that we use here and interlinking tool which has some disintegration step so it's more robust than a simple label matching approach."
        ],
        [
            "Predicate linking or relation extraction in China is a very hard task.",
            "Percy, and so you see a known distance supervision method.",
            "The idea is here that you first have a training phase in this training phase.",
            "You provide some input texts and you also say which.",
            "Knowledge based properties you want to focus on.",
            "So for example acquisitions with domain and range.",
            "And then you check.",
            "Given this trip is in a knowledge base and the documents.",
            "You will collect how these relations are mentioned in a text.",
            "So for example, acquired.",
            "This mentioned like acquired by enter on so this is already normalized.",
            "And then in application phase this they actually novel effect extraction.",
            "You just apply these mapping tables and then you can say that which relation which what was mentioned then."
        ],
        [
            "OK, so now you have these text attributes which were either partially or completely mapped to a knowledge base and now you can also take the query of the user into consideration which consists of a relevant Spartan over novelty part.",
            "The Revel in Parliament Sport says that he's interested, for example, only in acquisition, so specific relations or even in specific.",
            "In this case, companies like Apple.",
            "The nobody part would mean that he focuses only on specific knowledge classes, as I demonstrated beforehand.",
            "Like he's interested only in new relations or new objects.",
            "Being that he's, for example, interested in acquisitions of Epley, the company."
        ],
        [
            "OK, so we related that and we use."
        ],
        [
            "Trunk space is a knowledge base.",
            "Crunchbase is an online platform with information about companies, products, key people, investments, investors and so on.",
            "It's based on a web community.",
            "Anti garbled and RDF version of that.",
            "Off last year and had about 17,000 organization entities in about 26,000 people entities.",
            "It's not that much, but what we wanted to have is that we wanted to have these same as relations also to DB pedia.",
            "The simple reason is that we had this entity linking to Lex Lisa and you wanted to reuse that.",
            "Because it's more robust and nothing simpler string matching approach."
        ],
        [
            "We had four relations which we considered in the evaluation acquired competes with this board member or advisor and founded.",
            "Also with this domain and range respectively."
        ],
        [
            "Of course, we also need to use data.",
            "He abused their chasar newsfeed with English news.",
            "They had about 15 days of news which we used for this training of this predicate linking step.",
            "And then we use the rest for the actual novel effect extraction.",
            "Here we filtered out.",
            "Paragraphs which do not contain at least one known entity or known relation."
        ],
        [
            "And then we wanted to elevate three different tasks.",
            "The first one is fact forecast.",
            "The idea is can we detect facts before they were officially announced?",
            "So this announcement date is also written in Crunchbase.",
            "And it's important, for example, for social media monitoring and so on to be informed about what might be happening in the future.",
            "Then there's improved KP population.",
            "The ideas here that can be extracted.",
            "Semantically structured novel travels a lot of texts.",
            "Can we link them to the news where they were extracted from an RV faster than your community basically?",
            "And the third one is impact quantification that is, given known facts in our knowledge base can be checked them over time to have a kind of measure what the impact of them is.",
            "For the 123 you stick very extract or no troubles.",
            "Considering novelty classes with relation acquired and for the task two we also considered some other relations like competes with founded is board member or advisor."
        ],
        [
            "So what do their relation show?",
            "The relation showed that we could extract in over triples, maybe not that much.",
            "As you might expect, we had 30 two different acquired facts, 89 in total.",
            "But also in Crunchbase.",
            "So this was a kind of ground truth here.",
            "We also have a kind of lower bound for recall of 14.3% for acquired facts.",
            "The idea is here that it's only an approximation we didn't want to research in the documents.",
            "For that we have the news in a given time range and crunch basis on all space in this kind of ground truth afterwards and then we know which effects should have been extracted and detected this novel.",
            "Been searching in this news for labels of the entities and of the relations.",
            "Of these acquisition effects, which were later than in Crunchbase.",
            "Then we have task 1, two and three.",
            "Regarding the first one fact forecast, we have two acquisitions which were detected by our tool.",
            "Faster than.",
            "My Crunchbase by the community and faster than they were official, you know, and so these were more like rumors.",
            "Then we have improved KP population we have here 4 facts which were announced in insert into Crunchbase.",
            "So these were really novel.",
            "And finally, have impact quantification."
        ],
        [
            "Here we had several effects which were monitored overtime.",
            "Very prominent ones which occurred several times is focus acquired Okhlos VR.",
            "Verizon Communications acquired AOL and Apple acquired Beats Electronics, so you can.",
            "You can see where it began.",
            "For example, with IB times as source and then you have several peaks here."
        ],
        [
            "As mentioned, we also considered several other relations for their precision, like.",
            "Here competes with founded is board member advisor.",
            "Their conclusion is simply that the precision for this be cases, that is, if the relation is novel about the subject and the predicate is already known, then the precision is quite high.",
            "It's about 86%.",
            "But the precision is not that good, it's about 30% for the D&E cases.",
            "Get this if the subject or the object is Noble.",
            "It's simply because we have this textured triple extraction step and this step.",
            "Extract somehow and overcorrect rebels, but on the other hand also wrong ones in the same from the same sentence and it's hard to distinguish if it's.",
            "Yeah, it's saying the same or not so often.",
            "There are additional phrases attached to it.",
            "There are also other issues which could be tackled like coreference resolution or 15% and about 1/3 of the triples are not representable is triple.",
            "So for example, if you have the company acquired 20% of another company that is not representable, but this is simply an assumption which he made at the beginning and we think that for some kind of relations like these acquisitions, it's still a valid assumption."
        ],
        [
            "OK, it's to conclude.",
            "In our work, we wanted to target the targeted search for another ground effects in a knowledge base in unstructured text.",
            "We had semantic novelty measure and build system which can do affect forecast.",
            "It is finding facts before they were announced.",
            "During improved KP population that is providing semantically structured information from text.",
            "Being faster than your community for example, and doing impact quantification that is tracking the news, tracking their effects overtime.",
            "For future works we thought about improving, especially the recall.",
            "By improving the texture triple extraction step, for example, you could consider this nominal phrases more and implement also coreference resolution step."
        ],
        [
            "Thank you very much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Michael.",
                    "label": 0
                },
                {
                    "sent": "I come from the Culture Institute of Technology in Germany in I will present today the topic 2 words monitoring of novel statements in the news.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine that you are a stock broker or investor who wants to invest in some companies or startups.",
                    "label": 0
                },
                {
                    "sent": "For example, you want to have recent activities there.",
                    "label": 0
                },
                {
                    "sent": "And you Google for example for startup acquisitions or similar keywords.",
                    "label": 0
                },
                {
                    "sent": "Here get a list of results like what Quora has written and even maybe some.",
                    "label": 0
                },
                {
                    "sent": "Written statements here that eBay acquires an AI startup expert maker.",
                    "label": 0
                },
                {
                    "sent": "If you're more in a professional setting, you might use the Bloomberg terminal.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you also have news for example, also directly tasked with.",
                    "label": 0
                },
                {
                    "sent": "If it's coming from Bloomberg News or if it's a breaking news and so on.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What all these systems have in common is that they only have.",
                    "label": 0
                },
                {
                    "sent": "A system which is based on relevance is cartoon but not on novelty.",
                    "label": 0
                },
                {
                    "sent": "But this is what we want to focus on now.",
                    "label": 0
                },
                {
                    "sent": "So we want to focus on finding novel effects in unstructured documents.",
                    "label": 0
                },
                {
                    "sent": "There are few anomaly detection systems which search for novelty like normal information.",
                    "label": 0
                },
                {
                    "sent": "Is this cut could be topics, events and so on.",
                    "label": 0
                },
                {
                    "sent": "But what all these have in common is that they are mainly statistically but not semantically so they have no grounding a knowledge base.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what we can see here.",
                    "label": 0
                },
                {
                    "sent": "So there are information extraction systems which have a grounding in knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Which means that they either link entities to enter the knowledge base or relations expressing their text to a knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Then the earth.",
                    "label": 0
                },
                {
                    "sent": "Open information extraction systems, which naturally have no grounding, a knowledge base and then there are dedicated knowledge detection systems which focus on the novelty aspect but do not provide that grounding.",
                    "label": 0
                },
                {
                    "sent": "So this is what we can see here.",
                    "label": 0
                },
                {
                    "sent": "What we want to tackle is now.",
                    "label": 0
                },
                {
                    "sent": "Finding or implementing an over detection systems which have a grounding a knowledge base and which focuses on facts that is statements.",
                    "label": 0
                },
                {
                    "sent": "Almost.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, looks like that that we want to build our semantic Noble effect extraction system and we have basically three of them assumptions which we make.",
                    "label": 0
                },
                {
                    "sent": "The first one is that we focus on triples as statements.",
                    "label": 0
                },
                {
                    "sent": "I use statements triples and facts interchangeably here.",
                    "label": 0
                },
                {
                    "sent": "So we have your eyes for the subject, for the predicate, and for the object.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "The second assumption is that you do not only have one of them, but many, and this is a knowledge base.",
                    "label": 0
                },
                {
                    "sent": "You can think of semantic mediawiki's in a company, but also of more generic knowledge graphs like DP to hear the ideas here that.",
                    "label": 0
                },
                {
                    "sent": "This knowledge base contains the knowledge the user is currently interested in.",
                    "label": 0
                },
                {
                    "sent": "So for example, startups and so on.",
                    "label": 0
                },
                {
                    "sent": "And the third assumption is that no effects are contained in unstructured documents.",
                    "label": 0
                },
                {
                    "sent": "For example, in use and you want to extract them, you are interested in them.",
                    "label": 0
                },
                {
                    "sent": "So in our scenario, our semantic novel fact extraction system extracts no effects from text.",
                    "label": 1
                },
                {
                    "sent": "The user can query and few this novel triples and if needed public it to the knowledge base.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we need to formalize no effects and how these normal now effects are processed.",
                    "label": 0
                },
                {
                    "sent": "For that we make this a statement that no effects are relevant and novel if they are not yet in knowledge base.",
                    "label": 1
                },
                {
                    "sent": "But if they somehow overlapped.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that if you have a triple like here, Apple acquired next and all three parts are in knowledge base.",
                    "label": 1
                },
                {
                    "sent": "As you can see here, then this triple is not novel.",
                    "label": 0
                },
                {
                    "sent": "Obviously it's only relevant.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If two parts of the triple are in a knowledge base but one partition over through, this could be the subject, the ticket, or the object.",
                    "label": 0
                },
                {
                    "sent": "Then we say this triple is novel and relevant because there's an overlap with the knowledge base, so is somehow the context fits.",
                    "label": 1
                },
                {
                    "sent": "You can see here.",
                    "label": 1
                },
                {
                    "sent": "It's also about acquisitions, and even Apple is also mentioned in the knowledge base.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we can say if two or three parts of the triple are completely another that is not in knowledge base, then this could be novel and relevant.",
                    "label": 0
                },
                {
                    "sent": "But we see it as irrelevant Becauses context is not so much fitting.",
                    "label": 0
                },
                {
                    "sent": "We say that.",
                    "label": 0
                },
                {
                    "sent": "For example, as you can see here, they extracted triples are often noisy or very abstract.",
                    "label": 0
                },
                {
                    "sent": "For example, you don't want to have a triple with coffee shine and Krispy Kreme in a knowledge base about acquisitions of IR startups.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK then.",
                    "label": 0
                },
                {
                    "sent": "Given this definition, that one part needs to be novel in two parts needs to be known.",
                    "label": 0
                },
                {
                    "sent": "You can classify this novel triples into subclasses, you can say.",
                    "label": 0
                },
                {
                    "sent": "If everything is known, this is this Class A and then you can say if the relation it is if the predicate is not.",
                    "label": 0
                },
                {
                    "sent": "It could be that it's Part B, so it could be that.",
                    "label": 0
                },
                {
                    "sent": "Be this novel triple.",
                    "label": 0
                },
                {
                    "sent": "Here is the subject with which is contained in another triple, which is with the same outgoing relation like your product.",
                    "label": 0
                },
                {
                    "sent": "It could be that.",
                    "label": 0
                },
                {
                    "sent": "The object is shared with another triple, or it could be that neither of them is the case.",
                    "label": 0
                },
                {
                    "sent": "Is this B2 case?",
                    "label": 0
                },
                {
                    "sent": "It could also be that their relation, that is the predicate is not known in a knowledge base so far at all, so this would be ontology expansion.",
                    "label": 0
                },
                {
                    "sent": "This would be this C case here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, you also have the case that the subject can be another.",
                    "label": 0
                },
                {
                    "sent": "Like here then could be that the object is known and it's also an object of another triple, or it's not the case, or it's a case that the object is normal.",
                    "label": 0
                },
                {
                    "sent": "And that the object another object is in a triple of the same subject, or this is not the case?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we can implement our novel effect Ripple effect or affect extraction system.",
                    "label": 1
                },
                {
                    "sent": "This basically consists of three steps use given that the user provides some input documents like news.",
                    "label": 1
                },
                {
                    "sent": "The idea is that the first do a textual triple extraction, then annoyed space linking, and finally a nobody detection step.",
                    "label": 0
                },
                {
                    "sent": "And in following now I will focus on these three steps in more detail.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one is called textual triple extraction.",
                    "label": 1
                },
                {
                    "sent": "Becaused, given the input sentence, the idea is to extract so-called textual triples.",
                    "label": 0
                },
                {
                    "sent": "Textual troubles are statements.",
                    "label": 0
                },
                {
                    "sent": "As you can see here with a subject or predicate and object, and they're not your eyes or other.",
                    "label": 0
                },
                {
                    "sent": "Praise is extracted from the text so it input sentences.",
                    "label": 0
                },
                {
                    "sent": "Here above we do some dependencies through generations.",
                    "label": 1
                },
                {
                    "sent": "Some cross detection in some triple generation.",
                    "label": 0
                },
                {
                    "sent": "And for that we used it two o'clock hold closely close based information extraction.",
                    "label": 0
                },
                {
                    "sent": "So closely extracts propositions, which means you have a subject predicate in one or more arguments, and we adapt this to have triples.",
                    "label": 0
                },
                {
                    "sent": "We make some adaptations like transforming passive interactive, neglecting objectives, and so on.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based on given this text to triples.",
                    "label": 1
                },
                {
                    "sent": "We need to link them to a knowledge base and for that we need an entity linking step and a predicate linking step.",
                    "label": 0
                },
                {
                    "sent": "The entity linking is performed by a text annotation tool or entity linking tool.",
                    "label": 1
                },
                {
                    "sent": "For that we use.",
                    "label": 0
                },
                {
                    "sent": "Our tool called X Lisa.",
                    "label": 0
                },
                {
                    "sent": "It's text notation or interlinking tool.",
                    "label": 0
                },
                {
                    "sent": "It's actually 4 cross languages that is.",
                    "label": 0
                },
                {
                    "sent": "Four different languages, but we as we will see later we only use it for the English.",
                    "label": 0
                },
                {
                    "sent": "We think that we use here and interlinking tool which has some disintegration step so it's more robust than a simple label matching approach.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Predicate linking or relation extraction in China is a very hard task.",
                    "label": 1
                },
                {
                    "sent": "Percy, and so you see a known distance supervision method.",
                    "label": 1
                },
                {
                    "sent": "The idea is here that you first have a training phase in this training phase.",
                    "label": 0
                },
                {
                    "sent": "You provide some input texts and you also say which.",
                    "label": 0
                },
                {
                    "sent": "Knowledge based properties you want to focus on.",
                    "label": 0
                },
                {
                    "sent": "So for example acquisitions with domain and range.",
                    "label": 0
                },
                {
                    "sent": "And then you check.",
                    "label": 0
                },
                {
                    "sent": "Given this trip is in a knowledge base and the documents.",
                    "label": 0
                },
                {
                    "sent": "You will collect how these relations are mentioned in a text.",
                    "label": 0
                },
                {
                    "sent": "So for example, acquired.",
                    "label": 0
                },
                {
                    "sent": "This mentioned like acquired by enter on so this is already normalized.",
                    "label": 1
                },
                {
                    "sent": "And then in application phase this they actually novel effect extraction.",
                    "label": 0
                },
                {
                    "sent": "You just apply these mapping tables and then you can say that which relation which what was mentioned then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now you have these text attributes which were either partially or completely mapped to a knowledge base and now you can also take the query of the user into consideration which consists of a relevant Spartan over novelty part.",
                    "label": 0
                },
                {
                    "sent": "The Revel in Parliament Sport says that he's interested, for example, only in acquisition, so specific relations or even in specific.",
                    "label": 0
                },
                {
                    "sent": "In this case, companies like Apple.",
                    "label": 0
                },
                {
                    "sent": "The nobody part would mean that he focuses only on specific knowledge classes, as I demonstrated beforehand.",
                    "label": 0
                },
                {
                    "sent": "Like he's interested only in new relations or new objects.",
                    "label": 0
                },
                {
                    "sent": "Being that he's, for example, interested in acquisitions of Epley, the company.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we related that and we use.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trunk space is a knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Crunchbase is an online platform with information about companies, products, key people, investments, investors and so on.",
                    "label": 0
                },
                {
                    "sent": "It's based on a web community.",
                    "label": 0
                },
                {
                    "sent": "Anti garbled and RDF version of that.",
                    "label": 0
                },
                {
                    "sent": "Off last year and had about 17,000 organization entities in about 26,000 people entities.",
                    "label": 0
                },
                {
                    "sent": "It's not that much, but what we wanted to have is that we wanted to have these same as relations also to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "The simple reason is that we had this entity linking to Lex Lisa and you wanted to reuse that.",
                    "label": 0
                },
                {
                    "sent": "Because it's more robust and nothing simpler string matching approach.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had four relations which we considered in the evaluation acquired competes with this board member or advisor and founded.",
                    "label": 0
                },
                {
                    "sent": "Also with this domain and range respectively.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, we also need to use data.",
                    "label": 0
                },
                {
                    "sent": "He abused their chasar newsfeed with English news.",
                    "label": 0
                },
                {
                    "sent": "They had about 15 days of news which we used for this training of this predicate linking step.",
                    "label": 0
                },
                {
                    "sent": "And then we use the rest for the actual novel effect extraction.",
                    "label": 0
                },
                {
                    "sent": "Here we filtered out.",
                    "label": 0
                },
                {
                    "sent": "Paragraphs which do not contain at least one known entity or known relation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we wanted to elevate three different tasks.",
                    "label": 0
                },
                {
                    "sent": "The first one is fact forecast.",
                    "label": 1
                },
                {
                    "sent": "The idea is can we detect facts before they were officially announced?",
                    "label": 1
                },
                {
                    "sent": "So this announcement date is also written in Crunchbase.",
                    "label": 0
                },
                {
                    "sent": "And it's important, for example, for social media monitoring and so on to be informed about what might be happening in the future.",
                    "label": 0
                },
                {
                    "sent": "Then there's improved KP population.",
                    "label": 0
                },
                {
                    "sent": "The ideas here that can be extracted.",
                    "label": 0
                },
                {
                    "sent": "Semantically structured novel travels a lot of texts.",
                    "label": 1
                },
                {
                    "sent": "Can we link them to the news where they were extracted from an RV faster than your community basically?",
                    "label": 0
                },
                {
                    "sent": "And the third one is impact quantification that is, given known facts in our knowledge base can be checked them over time to have a kind of measure what the impact of them is.",
                    "label": 0
                },
                {
                    "sent": "For the 123 you stick very extract or no troubles.",
                    "label": 0
                },
                {
                    "sent": "Considering novelty classes with relation acquired and for the task two we also considered some other relations like competes with founded is board member or advisor.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do their relation show?",
                    "label": 0
                },
                {
                    "sent": "The relation showed that we could extract in over triples, maybe not that much.",
                    "label": 0
                },
                {
                    "sent": "As you might expect, we had 30 two different acquired facts, 89 in total.",
                    "label": 1
                },
                {
                    "sent": "But also in Crunchbase.",
                    "label": 0
                },
                {
                    "sent": "So this was a kind of ground truth here.",
                    "label": 0
                },
                {
                    "sent": "We also have a kind of lower bound for recall of 14.3% for acquired facts.",
                    "label": 1
                },
                {
                    "sent": "The idea is here that it's only an approximation we didn't want to research in the documents.",
                    "label": 0
                },
                {
                    "sent": "For that we have the news in a given time range and crunch basis on all space in this kind of ground truth afterwards and then we know which effects should have been extracted and detected this novel.",
                    "label": 0
                },
                {
                    "sent": "Been searching in this news for labels of the entities and of the relations.",
                    "label": 0
                },
                {
                    "sent": "Of these acquisition effects, which were later than in Crunchbase.",
                    "label": 0
                },
                {
                    "sent": "Then we have task 1, two and three.",
                    "label": 0
                },
                {
                    "sent": "Regarding the first one fact forecast, we have two acquisitions which were detected by our tool.",
                    "label": 0
                },
                {
                    "sent": "Faster than.",
                    "label": 1
                },
                {
                    "sent": "My Crunchbase by the community and faster than they were official, you know, and so these were more like rumors.",
                    "label": 0
                },
                {
                    "sent": "Then we have improved KP population we have here 4 facts which were announced in insert into Crunchbase.",
                    "label": 0
                },
                {
                    "sent": "So these were really novel.",
                    "label": 0
                },
                {
                    "sent": "And finally, have impact quantification.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we had several effects which were monitored overtime.",
                    "label": 0
                },
                {
                    "sent": "Very prominent ones which occurred several times is focus acquired Okhlos VR.",
                    "label": 0
                },
                {
                    "sent": "Verizon Communications acquired AOL and Apple acquired Beats Electronics, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can see where it began.",
                    "label": 0
                },
                {
                    "sent": "For example, with IB times as source and then you have several peaks here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As mentioned, we also considered several other relations for their precision, like.",
                    "label": 0
                },
                {
                    "sent": "Here competes with founded is board member advisor.",
                    "label": 0
                },
                {
                    "sent": "Their conclusion is simply that the precision for this be cases, that is, if the relation is novel about the subject and the predicate is already known, then the precision is quite high.",
                    "label": 0
                },
                {
                    "sent": "It's about 86%.",
                    "label": 0
                },
                {
                    "sent": "But the precision is not that good, it's about 30% for the D&E cases.",
                    "label": 0
                },
                {
                    "sent": "Get this if the subject or the object is Noble.",
                    "label": 0
                },
                {
                    "sent": "It's simply because we have this textured triple extraction step and this step.",
                    "label": 0
                },
                {
                    "sent": "Extract somehow and overcorrect rebels, but on the other hand also wrong ones in the same from the same sentence and it's hard to distinguish if it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's saying the same or not so often.",
                    "label": 0
                },
                {
                    "sent": "There are additional phrases attached to it.",
                    "label": 0
                },
                {
                    "sent": "There are also other issues which could be tackled like coreference resolution or 15% and about 1/3 of the triples are not representable is triple.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have the company acquired 20% of another company that is not representable, but this is simply an assumption which he made at the beginning and we think that for some kind of relations like these acquisitions, it's still a valid assumption.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, it's to conclude.",
                    "label": 0
                },
                {
                    "sent": "In our work, we wanted to target the targeted search for another ground effects in a knowledge base in unstructured text.",
                    "label": 1
                },
                {
                    "sent": "We had semantic novelty measure and build system which can do affect forecast.",
                    "label": 0
                },
                {
                    "sent": "It is finding facts before they were announced.",
                    "label": 0
                },
                {
                    "sent": "During improved KP population that is providing semantically structured information from text.",
                    "label": 1
                },
                {
                    "sent": "Being faster than your community for example, and doing impact quantification that is tracking the news, tracking their effects overtime.",
                    "label": 0
                },
                {
                    "sent": "For future works we thought about improving, especially the recall.",
                    "label": 0
                },
                {
                    "sent": "By improving the texture triple extraction step, for example, you could consider this nominal phrases more and implement also coreference resolution step.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}