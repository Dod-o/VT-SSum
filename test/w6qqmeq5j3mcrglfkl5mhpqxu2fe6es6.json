{
    "id": "w6qqmeq5j3mcrglfkl5mhpqxu2fe6es6",
    "title": "A Tutorial on Logic-Based Approaches to SRL",
    "info": {
        "author": [
            "James Cussens, Department of Computer Science, University of York"
        ],
        "published": "Sept. 18, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/ilpmlgsrl09_cussens_tlba/",
    "segmentation": [
        [
            "OK, thanks a lot Luc.",
            "That very kind reduction.",
            "Thanks a lot to the organizers for inviting me and I must apologize for having missed the first day of the conference.",
            "This is due to factors outside my control, namely my head of Department keeping me back to do my day job.",
            "So I'm going to give you a tutorial on logic based approaches to SRL so those people who have ESL conference affair amount of this will be probably quite familiar to use, but I'm hoping to bring more people into the SRL fold by explaining how it all works.",
            "But even for those of you who are already familiar with, so hopefully there will be something something you have interest in."
        ],
        [
            "Best for you.",
            "So here's the here's the structure of the talk.",
            "So first of all, I will start very gently is after lunch and some of you had some huge hard and will go through some of the connections between logic and probability.",
            "Time will move into talking about first order probabilistic models, those which use power factors which those of you who are in the previous SRL session will have heard an awful lot about.",
            "I'll be fairly fairly brief about that because there's been a lot of talks about that already in some more detail we're going to look at an alternative way of doing of constructing 1st order probabilistic models, namely the PRISM system.",
            "And there are other systems related to PRISM and then for those of you who maybe know this stuff already.",
            "I'm going to finish with an example, some actual learning.",
            "This is using SRL Markov logic specifically actually to learn Bayesian networks, and these are Bayesian networks which are going to represent family trees.",
            "Are the relationships between between human beings and we're going to learn that from genetic data, so that's using our own statistics.",
            "I'm hoping to sell this particular application domain as an interesting area for people to get involved in, so that's what we're going to finish."
        ],
        [
            "With.",
            "OK, so a lot of what I want to do in the first part of the talk is just to bring out some fairly straightforward connections between these four elements here in the box, we're going to start nice and easy down with propositional logic and work our way up to the lesion fields of 1st order probabilistic models.",
            "OK, when I go basically going two different routes, this is the power factor root, and this is the kind of generative model RT.",
            "Oh, I would argue."
        ],
        [
            "So just propositional logic in where we're going to."
        ],
        [
            "Salt his propositional logic in one slide.",
            "I'm assuming most people have at least some familiarity with it, so propositional atoms are.",
            "You can think of them as just binary variables that represent statements about the world, which are either true or false.",
            "We can associate full suit with the number 0 truth with the number one, so that's a propositional Atom.",
            "If we are a particular joint instantiation of all the atoms in some particular propositional language which satisfy some.",
            "Propositional formula is known as a model of that Formula One the slight problems with connecting probabilistic reasoning.",
            "Analogical reasoning is that word model has different different meanings in the two areas.",
            "So you have to be a bit careful with this word model here, and I'm deliberately choosing rather bizarre way of representing propositional formulas down the bottom here in order to better make the link with probabilistic models, which will come to quite shortly.",
            "For example, and what we're doing here is this saying suppose we just had A and be with the only propositions in a language.",
            "If we have the two propositions, first thing they say is true, the second saying this one.",
            "Here A implies B.",
            "Basically, we can if we so wish represent them as factors OK factors in the in the graphical model sense of functions from.",
            "I'll just wait a second.",
            "Functions from joint instantiations of some of the variables to some number or other sense of doing logic.",
            "These numbers are just going to be 01, so this is this is a is true.",
            "This is not a LB and you can see the only time when its forces when you had a true and B not then be false.",
            "We can view the the notion of conjoining these two is analogous.",
            "We can represent this by multiplying these two factors and what we get over here is a list of all the possible models for these two.",
            "These two guys here and we see this.",
            "Early model we have this formula.",
            "Here is a very true and being true 'cause we're going on there and we got zeros everywhere else and essentially what you could do is if you had your propositional formula in conjunctive normal form, each of your each of your clauses could be written like this.",
            "If you multiply them all together in this way, what you end up with is a massive big factor, usually with more than two variables.",
            "Should be quite large with loads of zeros and ones, the ones corresponding to the models of the of the propositional formula.",
            "Question, so this would be a very bizarre way of doing satisfiability testing, so that's the whole point.",
            "This slide is just to connect propositional formula to these factors with zeros or ones in them."
        ],
        [
            "OK, that's enough on propositional logic.",
            "Let's just move to what I'm going to call propositional probabilistic models.",
            "I don't think there's any formal definition of what a propositional probabilistic model is, but.",
            "Is how long?"
        ],
        [
            "To find them so the basic idea is you can view it as generalizing propositional logic.",
            "So one way of trying to link logic and probability is to say probabilistic inference is the kind of the basic way of doing things.",
            "That's that's the kind of general way, and logic is just a special case of probabilistic inference, and that's basically how I'm going to present it to you today.",
            "There are other approaches, we say logic is kind of where you start, and then you add some probability into that, and that's not what we're going to be doing here.",
            "So basically how you generalize it is very simple.",
            "We now add.",
            "We now allow any non negative values in the factors here, not just zeros or ones, and generally speaking your random variables can don't have to be binary.",
            "You can have as many many values as you like.",
            "In fact they can be continuous.",
            "Although throughout this talk will just deal with discrete ones.",
            "All the examples I will use, I will just stick to binary variables just because things, but on the slide better.",
            "But basically you have the same idea.",
            "If you multiply factors together you get bigger factors, so it's always 20.",
            "Here becomes because we've got a four and a five there.",
            "I haven't got into this factor multiplication stuff, but hopefully people can see what's going on.",
            "I expect most people have come across this previously anyway.",
            "You'll notice that what we've got over here doesn't look much like a probability distribution, 'cause the numbers are rather too too large.",
            "It doesn't nonetheless define a probability distribution, because we can just normalize this.",
            "OK, we just add up these numbers if once we add them up we got a number other than zero, we can just normalize.",
            "So the probability that a zero and B0 in this particular distribution is 20 / 82.",
            "OK, the question of whether Zed here is 0 or not.",
            "It's basically the satisfiability problem from propositional logic.",
            "Revisit it if you think about it.",
            "OK, so that's a factor representation of a of a probability distribution, which I'm sure many people have seen."
        ],
        [
            "Often in any other cases, say some further examples.",
            "So this one at the top here is basically a mark of network and is associated graph to that, so I'm sure many people have come across Markov networks before, so make sure I get the right way round.",
            "So typically in graphical models want asociates if one can the variables and factors with cliques of some graph or other, so that the graph of this guy at the top here will be this one here, and you can read off condition, independence relationships and so on and so forth.",
            "OK, so this defines the distribution with 16 numbers in it, one for each possible instantiations of ABC and D1 down the bottom.",
            "Here it doesn't actually correspond to a graphical model because you can't produce a graph with these guys as the cliques of that graph.",
            "So this is a non graphical hierarchical model, so these are just some further examples of propositional probabilistic models.",
            "By the way, feel free to shout and tell me to stop and ask."
        ],
        [
            "Questions at any point.",
            "OK, so let's look at a particular sort of propositional probabilistic Model 1 where we restrict what sort of numbers can go in these factors, and these are weighted clauses, so we saw some normal propositional clauses and one of the earlier slides where you just have normal propositional clauses.",
            "We can think of weighted clauses.",
            "So here and here we have a clause here saying a is true and the wait.",
            "I'm going to give to a here is infinite.",
            "So I'm basically saying when I say is true, A has to be true.",
            "Any model in which a is false.",
            "You know we're just going to roll it out entirely, and so that would be represented exactly the same as any other sort of logical clause like this.",
            "Over here we have a different another clause, same as before, but now it's waited and we have to wait here.",
            "What we're saying here is that we would like this clause to be true, and the degree to which we want it to be true.",
            "We're going to give a number 22 if it's not true, we don't entirely rule out a propositional model in which it is false, we just make it less probable than one.",
            "Where it is true, all other things being equal.",
            "OK, so we can rewrite.",
            "So before we had a clause where this was a 0 now.",
            "It's a Long story short.",
            "We put it to the power minus two in there and now if we do the multiplication exactly as before, we end up with this situation here.",
            "So now we've got 20 probability models.",
            "This one here is the best model.",
            "It's got a one and this one here eats the minus two.",
            "So the reason why I bring up this particular way of encoding probability distributions is big 'cause there are lots of nice solvers out there which most of all most of them devote themselves to this problem here.",
            "Finding the most problem instantiation given a whole awaited clauses.",
            "Generally speaking many more than two with many more variables than that.",
            "We want to find which model has the highest.",
            "The highest waiting OK in this case is easy.",
            "It's obviously this guy down here equals one B = 1.",
            "Evidently, in a bigger case, you wouldn't want to do this big multiplication to find what that's the highest model it, so there are all sorts of set solvers.",
            "Probably the most famous of which is that the Max Walksat algorithm there are others as well, and we're going to be using this representation later on when we get to the.",
            "Statistical genetics example.",
            "OK, so that's way too close."
        ],
        [
            "Just briefly, I'm sure most people have come across with Bayesian network is before, but just for the record and because we're going to be learning Bayesian networks later if we restrict the factors in the following way, we have one factor for each variable.",
            "The fact the factor for the variable is a conditional distribution of that variable given some other variables known as its parents.",
            "We can then draw a graph using those parent child conditions if it is the case that the graph in question is a cyclic, then we have a Bayesian network.",
            "OK, so this is.",
            "We've seen a special case of weighted clauses.",
            "Another special case is Bayesian networks, where we have products of conditional probability tables OK."
        ],
        [
            "So what problems do we generally want to solve in probabilistic models?",
            "One of the key ones, inference, what does inference mean in probabilistic models?",
            "And again, one has to be somewhat careful here.",
            "The term inference is used in logic and also in various computations.",
            "Recurrent probabilistic models are not exactly the same problem, but the same basic idea that all the information you need is there.",
            "In your model you would probably your job is just to extract the information of interest from that in some way and the usual.",
            "The two most common ones are computing the marginal distribution of one or more variables.",
            "So if we have a distribution like this, we might want to just get the distribution of a by getting rid of BC and D and also finding which particular instantiation's the highest probability.",
            "So there's a common things.",
            "How does one do that?",
            "A lot of the algorithms are based on something called variable elimination, and this variable information algorithm can be summed up in two very two very simple observations.",
            "If we recall these factors that we saw previously.",
            "Which is what we're assuming defines our probability distribution we take through these factors multiplied together and get a bigger factor.",
            "We haven't changed the distribution in question, and Moreover, if a particular variable only occurs in one of these factors, then you just have to sum it out of that factor, and that's OK. As far as something out of the entire distribution.",
            "So the variable information algorithm just takes a factor, multiplies, takes a variable, multiplies all factors together for that variable, getting one big factor with that variable in question sums it out.",
            "Thereby producing a new factor and then just carries on OK, so that's available information in a nutshell."
        ],
        [
            "So we're making our way gradually towards their first order.",
            "Probabilistic models.",
            "Let's have a look at 1st order Logic, so I won't spent the best part of the year of my life studying 1st order logic.",
            "Now I'm going to encapsulate in two slides."
        ],
        [
            "And here we go.",
            "So what are the characteristics of 1st order logic?",
            "So as with propositional logic, we have assertions of what is true and false.",
            "However, now the propositions themselves have some sort of internal structure.",
            "Now we propositions assert properties of objects and also relations of objects.",
            "So the key thing is that we have a language in which we talk about objects.",
            "And these objects are represented by things called ground terms, and the reason it's called 1st order is because we can quantify over objects and we have some quantum University quantified formerly down here.",
            "What is a universally quantified formula?",
            "So two very rough and 1st approximation you can think of it as a kind of template for all its ground instances.",
            "So when we see University quantum formula like this but it says any X if X is even, then F of X is odd sorts.",
            "Here you can think of.",
            "Intuitively, is the successor function, so you can think of that as standing for all the possible ground instances in some fixed language of that, so it's not quite that, but it's kind of more or less that and so we can replace the X here by the ground term 0, and then we get this one.",
            "We can replace the X by the ground turn S 0 and we get this guy you carry on like that.",
            "There's all this in this case, infinitely many ground instances one can generate from this.",
            "Here's another example where we're just sticking in AIDS, bees, and seizes the XYZ and Zeds.",
            "So simple simple observation is that we can use this as a template to compactly represent infinitely many assertions about what's going on in the world.",
            "That's the other key things about first order logic, which will be exploited when we move up to probabilistic stuff."
        ],
        [
            "So we can, if we so wished represent 1st order clause using one of these factor representations.",
            "This is a rather peculiar way of presenting it, but as you can probably guess, the reason I'm doing it is to make the connection to the factors I've used to define probability distributions previously, so we could imagine writing this clause here like this and what this is saying is that for any particular instantiation of X&Y we have a factor.",
            "Corresponding that particular situation so we can think of this this clause here as representing some product of many, many ground instances of said clause, possibly finitely many, depending on the language, possibly infinitely many.",
            "So that's that's one way one way of thinking about what a University of quantified formula looks like."
        ],
        [
            "OK, so just a bit more before we get to the probability.",
            "So what is a model in in first order terms?",
            "Again, this is another big area as a whole industry or the whole academic discipline of 1st order model theory.",
            "We're going to make life easier for ourselves and just consider her Brown models and basically the way to think of her brand model is that it assigns the value true or false to each ground atomic formula in our language in question.",
            "So all these atoms for sure.",
            "So typically one might have infinitely many of these other circumstances that could be just finitely many of these.",
            "So the key point of bringing this up is essentially because of we got this.",
            "We got a number of different possible models you can think of the ground atoms acting like binary variables there either true or their fault.",
            "OK, so here's part of 1 model where P of a here is true and B&B is false or some other model.",
            "Here PV is not false, it's true.",
            "OK, so just so you can think of it is just a big long joint instantiation of a whole bunch of binary variables.",
            "OK, and we're just going to use logic as a convenient way of talking.",
            "About such joints instantiation's.",
            "I mean you can think of 1st order logic is a language for talking about first order models, and that's that's what's going on here.",
            "Excuse me just a second.",
            "OK."
        ],
        [
            "One of the key good things about first order logic is that in order to perform inference in first logic, we do not have to translate down to propositional logic.",
            "OK, so one could imagine a completely crazy way of doing 1st order logic inference, which is when we take a formula like this.",
            "Somehow generates all its ground instances or maybe lots of its ground instances.",
            "Take another formula here, generate loads of ground instances.",
            "This guy then use propositional logic on those ground instances that derive a whole lot of other ground formula, and then somehow generalize and back into some universe University quantified formula.",
            "Back one didn't have to do that, so the key thing is that you can do inference while staying at the 1st order level.",
            "OK, here's a particular case of doing some inference from these two guys.",
            "This one here follows by a very well known rule.",
            "1st order very well known rule of 1st order logic known as resolution.",
            "Interestingly enough, we once had the great pleasure of having Robinson, the famous logic programming pioneer.",
            "Come to give a talk at York.",
            "It was talking about how.",
            "You know when he was first grappling with some of the algorithms which are now form part of logic programming systems.",
            "This this the way he first of all came, looked at doing inference was by this very much grounding out idea.",
            "Then I can't remember what term we had for it and it was a big breakthrough to realize that you could.",
            "You could stay for doing logic programming.",
            "You could stay the 1st order level.",
            "OK so."
        ],
        [
            "Alright, so now we've we've kind of just as good precursor gone through these these approaches here and now we're going to start talking about first order probabilistic models.",
            "So one way of doing it is to say, well.",
            "We've got this propositional probabilistic models usually connected to graphs in one way or another.",
            "What we're going to do is we're going to generalize from then from them.",
            "Excuse me in pretty much the same way to generalize from propositional logic to 1st order logic, and that's the 1st way of doing things that I'm going to consider here today.",
            "I'm going to do it very briefly.",
            "Becausw I've only got a certain amount of time and I want to spend anymore time on this one so.",
            "That's so that's just how how things are going to be organized.",
            "So we'll start off with going from here here, up to here.",
            "I'm going to use something called."
        ],
        [
            "Power factors, so the basic idea is, as you might, if even if you haven't come across this sort of stuff previously, perhaps you'd be buried away in the MLG sessions and haven't been to the SRL sessions.",
            "Maybe would have guessed this is basically what's how things are going to what we can do is we can say, right?",
            "Well, we had we had normal factors which the product of which describes some probability distribution.",
            "What we're going to do now is we're going to use the same trip we haven't first or logic by which we view things like this.",
            "As templates for its ground instances, and we're going to represent a whole bunch of factors with the same structure by a single factor using logical variables.",
            "OK, so that's the basic idea there called power factor is short for parameterized factors.",
            "I think David Poole was the guy who came up with this term.",
            "I am open to correction on that point.",
            "So the key thing here is you can represent many, possibly infinitely many, of these normal factors by in a much more compact, compact way.",
            "OK, and typically one as well as having one is just saying well this represents all the factors you can get by sticking something into Exterran.",
            "Sticking something into why there typically one also has constraints to kind of cut down on the number of ground instances, which this implicitly represents.",
            "So, for example, we might say that we only care about cases where X is different from Y.",
            "Also, quite, it's quite common to say that we're going to have some typing here, so rather than any ground term in the office or language being a candidate for substituting for X where X might be a type thing, they might only have certain sorts of things which can ground X out there, and in fact, when we come to the example later on with the statistical genetics, we would indeed have that have that sort of typing.",
            "Just be aware that if we got a little bit careful if there are infinitely many of these guys, then if it was this particular factoring question that this was infinitely big and it will, all these guys would head off to 0.",
            "So it's a bit.",
            "It's gotta be careful about infinite collections of power factors.",
            "Evidently if we're dealing with a big but finite collection, then everything everything is fine at the end of the day, what we're representing implicitly is many, many factors which would multiply together.",
            "To generate a distribution over over these guys here OK.",
            "So that's it.",
            "That's the basic idea of this way of doing first."
        ],
        [
            "Probabilistic representation, So what sort of probability distribution have we defined?",
            "So what happens now is that each ground Atom becomes a random variable.",
            "OK, and this also means that when you're talking about these things, you have to be careful what you mean by a variable, because we've got logical variables and we've got random variables.",
            "Alright, so if we have something like PX from over here, this is a logical variable.",
            "Once we've ground it, we get this guy here, which is a random variable, even though it doesn't look like a very well.",
            "You might not think it is indeed.",
            "A random variable.",
            "So ground atoms represent random random variables.",
            "If these random variables are binary, which is the you know assumption one.",
            "I'm going to make it in this particular talk.",
            "Then the distribution in question is over her brain models.",
            "OK, so basically each possible joint is imagine you've got these factors represented in the 1st order fashion.",
            "I just showed you.",
            "One typically have more than one.",
            "You can imagine that's the sake of argument.",
            "Imagine that we have.",
            "Finite language, so in principle one could imagine replacing these first order factors by the product of their ground instances, and then one could imagine multiplying all these ground factors together to get one humongous big factor.",
            "Each row in that humongous big big factor will be a joint instantiation of all the ground atoms, which is a Herbrand model, so you so you've got a distribution over possible per gram models, and so these are usually known as.",
            "This is a distribution of possible worlds, because you can think.",
            "Of the 1st order model as a possible description of some, some world of interest, this is quite interesting for someone as like me as absolute said, I come from to certain extent from a philosophical background.",
            "At the very pre history of probability theory, probability distributions were seen as being defined over possible worlds, which is quite interesting.",
            "So if you go back to like knits and read all his philosophical stuff this you know he was there in the beginning where people were solving probability problems to settle their gambling debts and that sort of stuff because of his background in philosophy and because of his whole his whole philosophical tradition was based on this notion that there are many many possible worlds and we somehow even the best of all possible worlds.",
            "And the reason for that is because that's the most perfect world.",
            "So that's the world that God has chosen us to live in all this sort of lava.",
            "This whole kind of massive theory built around it when he got interested in probability, he started introducing the note.",
            "The notion that probabilities are about he didn't put it in this in these terms or partners writing in Latin even put it in these terms.",
            "But basically probability is about.",
            "It is always about the distribution over possible worlds and we just happen to live in the most for one.",
            "So that's kind of interesting.",
            "I should also mention again from the philosophical backgrounds coming from this awful background.",
            "This idea of possible worlds distributions also.",
            "Slightly more recently than it's there's a lot of work by a chap called Carnap who was mercilessly criticized by Popper.",
            "This chap, called Karnak, came up with this again.",
            "A lot of work on distributions over first order models.",
            "Are you possible worlds?",
            "What he was trying to work on was trying to find what is the correct single distribution of a possible worlds.",
            "You know someone writes analogical theory, somethings are entails something is not entailed there for some other things.",
            "Must be, you know, maybe some things are partially entailed.",
            "So here's what homework was on using things like this.",
            "Principle of indifference to come up with a single distribution of always possible worlds, which was the logically correct distribution of a possible worlds.",
            "We are much more pragmatic people.",
            "We learn our distributions of possible words from wells, from data.",
            "So excuse me, my slide for this little digression.",
            "Now let's get back to the technical stuff.",
            "So evidently we can always, if we wish, and sometimes we might even do this, take a power factor representation.",
            "I do not say Paula Factor is short for parameterized factor 'cause the logical variables of parameterising it we can take apart factor an if we got finiteness around we can always ground it out and just carry on doing probabilistic inference as per normal.",
            "OK so it's one thing to mention.",
            "Another thing is to say although we have described the probability distribution over Herbrand models and using logic once we have done that, it is a kind of independently existing mathematical object.",
            "We don't necessarily have to use logic.",
            "To analyze it to fit the parameters of it to do structure learning or whatever.",
            "So so I think that's a point I'd like.",
            "I'd like to make there.",
            "Ascari."
        ],
        [
            "OK, so here's the big question which I'm going to skip scandalously because it's been answered in a number of ways and immediately preceding session.",
            "So the obvious question arises.",
            "One of the nice things about first Order logic is that we can do inference at the 1st order level.",
            "Now we've got these probabilistic models defined using first order variables.",
            "Can we do inference by which we mean marginalization maximization in first order probabilistic models?",
            "And what we basically.",
            "Well, that question amounts to is if we've got a whole load of.",
            "Remember these guys are random variables, right?",
            "There may not look like them, but they are.",
            "We got hold of random variables, all of which are ground instances of some other.",
            "Mother formula with a free variable.",
            "Can we somehow all these guys in one go buy something out this guy?",
            "And evidently if we've got a very if we got a distribution which define if we got a distribution of a very many random variables and we want the marginal distribution over a particular ground Atom, someone says to us, right?",
            "You've got this nice distribution.",
            "What's the probability that I don't know?",
            "Q&A B is true.",
            "Then conceptually one has some out all the other variables, so we want to get rid of this quickly as possible.",
            "So the question is, can we do that and?",
            "In order to avoid gross redundancy, I'm not actually going to go into that big question, but let me just say that is one of the hot topics of of of this of this field.",
            "So for those, if you weren't in the previous session, those who were in the previous SRL session."
        ],
        [
            "We know this to be the case already, so as I said, you're in the right place, sorry.",
            "It's.",
            "Are you assuming there at all independent of each other?",
            "No, certainly not, no, not at all.",
            "So for example."
        ],
        [
            "I mean, there's no reason I mean QA and this guy here and this guy here, so not independent.",
            "I mean even for working the other guys over here, yeah, they don't have to be independent at all.",
            "If later on we'll see a case where.",
            "Quite a lot of ground atoms are independent, but assuming that that's all, otherwise things would be kind of easy.",
            "Sorry, so just yell by the way also."
        ],
        [
            "I'm not looking at your direction.",
            "So this is I get this other kind of version of this talk at York and someone said to me, what are the topics in SRL at the moment?",
            "And I said one of them is definitely this lifted inference, infer sort of probabilistic models.",
            "So I'm not going to go into all the different ways of doing this.",
            "Suffice it to say, many people make their money talks and also posts is probably later on today where you can speak to people about this.",
            "Basic idea is that when we got repeated structure, we want to exploit it when it's available, but often when, especially when he had evidence in lots of a nice symmetries that we have an original verse or probably model might disappear, so it's not at all a trivial trivial problem.",
            "So for those who wanted a tutorial on 1st order inference in first order problem models, I apologize, but it would just taking way too long to do it any justice whatsoever."
        ],
        [
            "OK, so I want to talk about a particular sort of 1st order probabilistic model in this vein mark of logic, which is been around since I think 2006.",
            "The paper with Richardson and I'm English.",
            "So we've seen previously waited propositional clauses, so here's a way to propositional clause.",
            "Nice idea is to say, well, we're going to have waited 1st order clauses and the thing that nice thing about clauses that there certainly I found this as a user is that you can write 1st order rules down.",
            "As clauses, and this is a very.",
            "Psychologically useful way of getting information in your system.",
            "It's a very nice knowledge representation approach so we can have a.",
            "Something like this and what this is what this looks down.",
            "The factor representation is like this.",
            "So here we are.",
            "Again we have the one the one and here with Eaton minus two we could replace this factor by 1 where we had we multiplied all these guys by E to the power of two.",
            "We could get a pair of 2 zero.",
            "Sorry there are two one these.",
            "Apparently there are two would be the same the same distribution, so that's that's the basic idea.",
            "By mark of logic it's to use weighted clauses as the particular sort of power factor facts that we have."
        ],
        [
            "And because we have that particular restriction on what are factors, as are we get a particular sort of probability distribution is kind of much more regular than the kind of general case.",
            "The key thing is that what matters for anyone, and we're going to assume to stay out of controversy.",
            "Taller formula clauses.",
            "What really matters for any one of these weighted clauses?",
            "What really matters for any is how how many true groundings we have, or that clause in any particular urban model, and we end up with this distribution here.",
            "So if X is a Herbrand model, remember this is a joint instantiation of very many binary variables represented by ground atoms.",
            "That Alexis here.",
            "The probability of that particular X we have a unpleasant normalizing constant which will try to forget about the time being Anna distribution question is given by this thing here.",
            "Here's the wait for the clause.",
            "Here's the number of two true groundings of that clause in this particular world.",
            "X and we get this nice formula here for the probability of offset world OK. And so this is at the end of the day.",
            "Eight particular exponential family distribution.",
            "So any any distribution looks like this is known as an exponential family distribution.",
            "And what we call.",
            "This vector of values here for any particular X which is particular world, how often was closed.",
            "One satisfied health is close to satisfied up to close K. That's the key.",
            "The key quantity that in the statistical literature that's known as the Canonical statistic, so that we're going to come back to market logic later.",
            "So it's a nice special."
        ],
        [
            "So that.",
            "Alright, so what's what sort of data are we going to learn?",
            "Such 1st order probabilistic models from?",
            "Well, the distributions over possible worlds, so in the kind of most straightforward case, the data you observe is a particular possible worlds.",
            "OK, so obviously you can have, as you might expect, you have situations we have missing data or you don't see all the possible world in question, but the complete data cases to see a possible world so.",
            "And we counted if you want learn from IID instances of possible worlds.",
            "You know one possible world is your first observation, another possible segmentation, so on and so forth.",
            "And what one could learn from that work, learning that situation often we just have a single observed world which we can view as a relational database.",
            "And you might think having just a single data point isn't enough to do any learning with, but because we've got this repeated structure, you can actually have a reasonable chance of getting reasonably good estimates of the parameters or reasonably good structure learning.",
            "You have, you know there's enough information in a single single case there.",
            "OK in the case of the mark of logic in particular, what really matters is the counts of the true groundings of the formula question, 'cause that's what's going to go into into.",
            "For example, parameter estimation."
        ],
        [
            "OK, so now I want to move onto.",
            "We've had quite a sort of people wanted more on this particular way of doing things then if you weren't at the previous session you made a mistake 'cause there was lots of lots of really interesting talks about that sort of stuff there so far haven't so much about the direction.",
            "I'm not going to go into, which is a more generative approach.",
            "Going more directly in my view from first floor."
        ],
        [
            "So the probability models.",
            "So the inspiration, while this is how I think of it for this particular vein of doing things comes from dynamic probabilistic models.",
            "So we saw just then an example cases of quantifying over random variables.",
            "This is not something which is not been done previously, but only in very special circumstances.",
            "So I once spent some time while she was working in Oxford, down with Steve and David and Co. My other job down there was working on time series analysis of financial data, and I spent much of my time looking at these sort of things here.",
            "So this is also known as an AR-1 model.",
            "In the time series analysis literature, and this says for any day.",
            "The price of your stock on day T is just as a distribution which is given by the price.",
            "Yesterday time something plus some noise OK. And here we got.",
            "We are quantifying over random variables alright, but we're quantifying the particularly restricted since OK and the relationships between the random variables are just ones of 1 coming after the other.",
            "So quantifying of random variables has occurred prior to SRL, but only in very restricted senses, and you get similar things in spatial statistics OK, whereas rather than things, just the random variables being.",
            "Going out in one line, they're going to spread out often in the lattice or grid.",
            "And if you think about it, when you are dealing with stochastic grammars, for example hidden Markov models, which is just sarcastic regular grammars, essentially there are very many random variables involved in such a such a process, which is why they can model infinite infinite processes and similarly dynamic Bayesian networks.",
            "You know you get this repeated structure with this with the same random variables with the same distribution repeated over and over again.",
            "So that's kind of where the inspiration for this.",
            "A number of formalisms, ISL peas in prisons.",
            "Whether they kind of come from this way of thinking."
        ],
        [
            "Let's talk a bit about the prism approach, then have time to go through the whole history of the source stuff.",
            "So in the PRISM approach, the division of Labor is very, very is as follows.",
            "So we have probability logic connected together.",
            "And the division of Labor between the two sides is as follows.",
            "The probability distribution, the basic probability distribution which is defined is extremely simple.",
            "You just have families of independent identically distributed random variables.",
            "Alright, the simplest sort of distribution one can imagine.",
            "You can't do model much with such a distribution.",
            "So to do something even a little bit interesting, we had the logic in the logic.",
            "On the other hand, is not very simple.",
            "We had in arbitrarily complex logic program in order to generate a more interesting and useful probability distribution.",
            "Again, it's a total distribution over Herbrand models, so how is this done exact?"
        ],
        [
            "So let's start with the probability.",
            "So forget about the logic that I'm being.",
            "Imagine if you will you have a whole family, infinitely many just so we don't run out of random variables.",
            "An infinite collection of independent and identically distributed random variables, and just for the sake of argument, suppose this one here.",
            "They all have values.",
            "Why in-n-out?",
            "Suppose just for concreteness that the probability that any one of these takes a value wise .3.",
            "And it's just adding some other another family of random variables.",
            "So all these guys have the same distribution.",
            "These are binary.",
            "All these guys have the same distribution, but they have up to 9 different sorry 10 different values here.",
            "So we can imagine the joint instantiation of all these infinitely many variables at the beginning of.",
            "One would look something like this OK. And of course the nice thing about this distribution is extremely easy to compute marginal distributions.",
            "So if we want to work out what's the probability that I know X 3 = y, it's just going to be .3.",
            "We don't have to do any do any work whatsoever.",
            "If you want to workout the probability of other know of this X 1 = y and Y2 equaling one, it's just a simple multiplication, right?",
            "So it's a very.",
            "Trivial.",
            "Probability distribution.",
            "So, so that's."
        ],
        [
            "So this is by the way, how you define Santa trivial probability distribution, prism.",
            "There is actually slightly more compact syntax one can use, but let's not go there.",
            "So you just say you know the values of that guy is these and these are probabilities.",
            "So implicitly when you type this in your defining not just one random variable X, you're defining an infinite collection of random variables X one X2X3 up to XAF knowledgeably.",
            "OK, so that's what the prison source looks like."
        ],
        [
            "Now let's start linking things to logic, so we can imagine if we have a particular instantiation of all these.",
            "Of of all these variables, we can represent a particular instantiation by proposition in first order logic.",
            "So we could say if if variable X1 takes a value Y, we're going to write that down like this.",
            "So MSW here stands for Multivalued Switch an and so on and so on and so forth.",
            "OK, so this is just another way of writing that down, but in a way more amenable to linking it to logic.",
            "So this is a MSW.",
            "Here was our only predicate.",
            "Then this would be distribution over possible worlds within, so this will be a particular per brand model and this would have a particular probability and other ones would have different probabilities and there you have it.",
            "But there's not a very interesting distribution.",
            "So how are we going to make it more interesting?",
            "That we can use."
        ],
        [
            "Model other things.",
            "Is the basic idea.",
            "The basic idea is is fairly straightforward already?",
            "What we do is we say we have this original distribution and we're going to extend it by adding in a fixed logical theory.",
            "So if we imagine what's going on over here, we F1.",
            "Here is some particular instantiation of all those MSW guys.",
            "F2 is some other instantiation, F3 is yet another instantiation.",
            "If we have some fixed logical theory are then.",
            "What follows from F1 and R is different for photos from F2 and are alright, so we get some.",
            "We got a probability distribution over what follows from our once we've added in in the F1.",
            "OK, so this is how I think of when I think of these things is generative 'cause.",
            "The idea is that you know you got your are your logical theory and it's waiting to receive it's probabilistic, probabilistically generated ground atoms, MSW guys.",
            "And once you've got those, you can infer all sorts of other things and what you can infer changes.",
            "Pending which instantiation you've ended up with.",
            "So by this approach we can have quite a complicated distribution over here, so it's still a distribution of possible worlds.",
            "The number of RAM models is presumably bigger because we've got language will be a bit bigger because we've got.",
            "Predicates here in R, which we haven't seen before.",
            "So if you have a particular formula Fla, what's the probability that formula being true?",
            "Basically, you say, well, it's a probability that if I were to sample one of these infinitely big joint instantiations F and added it to my pre existing or I'd be able to infer the formula question OK?",
            "And there's some stuff which I'm scandalously glossing over to do.",
            "The closed world assumption to make sure this all kind of fits together very, very well.",
            "'cause you can imagine you have a formula which doesn't get in 3rd.",
            "You know neither it nor its negation gets easily inferred, so they wouldn't get probabilities adding up to one.",
            "So there is a closed world assumption in the whole theory built on top of logic programming.",
            "In order to get this order to workout."
        ],
        [
            "So that's the basic idea there.",
            "So we get a distribution of possible worlds, but typically in the prison system we use something called a target predicate to really summarize what's going on in any particular.",
            "Possible world, so it's convenient to specify what's under the target predicate T. In this case, just it doesn't ask me magic, just my examples magic.",
            "And we define it in such a way that exactly 1 ground Atom with this predicate symbol is true in any particular possible world.",
            "So for any particular choice of our instantiation of our base distribution, exactly one of these guys follows alright.",
            "So if we choose F1 here, we get TF2 gives us TBF, three gives us T again.",
            "This evidently defines a distribution over from the logic.",
            "If you're familiar logic programming, you can think of this.",
            "These various guys here as being members of success, set of T, and that gives you a distribution of these guys, and this can be generalized somewhat to allow in each possible world you might have possible worlds in which none of these T guys follows, and then things get a bit more complicated there, but things get also bit more expressive."
        ],
        [
            "So that's what's going on there.",
            "So how do we compute target probabilities from this prison distribution?",
            "So we saw earlier that computing marginal distributions in first order probabilistic models can potentially be quite tricky because we got so many other variables to some out.",
            "What happens here with the prism case, whatever that we don't consider all possible infinite instantiations of the base distribution.",
            "If you're trying to compute the probability of some particular ground Atom here.",
            "There's a requirement in PRISM that probability of that is a finite sum of finite products, so there is a restriction built in here.",
            "So what we do, and this is the kind of where it's nicely logical is that we actually use abduction to do on marginalization for us.",
            "OK, so if we want to compute the probability of that guy going to subduction to compute this probability as follows."
        ],
        [
            "So is an example code for code up.",
            "Again the Markov model as a prison program, one code up other things obviously, but this is just fine example.",
            "We've got this ground Atom here, which is basically saying my hidden Markov model is generated the the output JDA.",
            "What's the probability that?",
            "Well, what we can say is there's two possible things happened here, either.",
            "Well, we always start in State Zero.",
            "Anastasio must spit it out a, which is that guy there.",
            "And then maybe we stayed in State 0, which is what this is saying.",
            "And then if we disdain State Zero, the other two guys are spat out starting from the S 0 or maybe we switch to another state state one and then spit out DNA from state one.",
            "So basically we're saying this is true if these three guys are true or these three guys are true.",
            "So then we look at well what?",
            "Case is this guy true and so this is a."
        ],
        [
            "Suction so we just say well for this gotta be true as this is broken down like this."
        ],
        [
            "And so on."
        ],
        [
            "So I won't go through that."
        ],
        [
            "Gruesome details.",
            "So basically what we're doing is anything abduction to workout what things have to be true to make our ground Atom true?",
            "Once we've done that?",
            "Computing, probably."
        ],
        [
            "Is extremely trivial.",
            "'cause we got this and we can compare."
        ],
        [
            "Probabilities as follows.",
            "We say right this is what we care about.",
            "We want this probability because of our also convenient independence assumptions that we previously made.",
            "We can just compute it as these guys multiplied together.",
            "Plus these guys multiplied together.",
            "You might notice at this point there's a rather convenient assumption that these guys here are exclusive.",
            "If we generalize the prism approach so that this no longer holds, then things become much more much more complicated, and the work with prob log.",
            "People working probably gonna love that.",
            "So basically the way we're computing the probability now.",
            "This follows we're thinking of probabilities as follows.",
            "We got a probability space.",
            "We got something.",
            "Some events in this space whose probability we want to want to compute, and we're basically saying is.",
            "Its probability is basically kind of the area of this thing we're going to do is, first of all, say, well, it's this plus this.",
            "And as I want the property this guy, so that's going to discuss this and so on and so on.",
            "And so we've broken it down into things we can just add up, alright, which is what's going on here?"
        ],
        [
            "So we carry on the.",
            "Prob."
        ],
        [
            "Stick this."
        ],
        [
            "And that's how we would compute the probability of that guy here.",
            "So basically we the the nice thing here is we don't have to.",
            "There are infinitely many random variables in our distribution, we don't care about most of them, 'cause we know because of the restrictions of a prism approach that to get this guy here in the first instance, we just need to worry about these six alright.",
            "And then once we got these six and we have to do it work on this guy, this guy, these ones here, just the probabilities are just given to us.",
            "So I've always been kind of trying to think to myself.",
            "You know, this is kind of lifted inference, but in the prism sense, but.",
            "Because of the particular sections of the prison program, lifted inference kind of reduces a logical inference.",
            "You know, there's not really.",
            "There's not much more to do apart from normal, logical, logical stuff."
        ],
        [
            "Is quite useful, so in that case, what sort of things we learn from and with our data typically look like for this form doesn't make sense.",
            "Again, we assume that our data comes from some some world, so imagine that our our data generating process is generating possible worlds for us, possibly just one.",
            "However, we assume that we only see the instance of our target product, which is true in each one of these worlds.",
            "So the idea is that your data or data generating process generates a world.",
            "And then says this is the ground Atom of your target product ruin this world.",
            "I'm going to show you that but not show you the rest of the world.",
            "OK, because of this you got effectively missing data, so you have to use the EM algorithm or or some other way of dealing with missing data.",
            "Fit the parameters.",
            "So that's that's how parameter fitting would happen happen in prison.",
            "How is it for just on time to?"
        ],
        [
            "Go through application, so that's if you like the end of the purely tutorial part of what I wanted to talk about.",
            "So as you hopefully would have guessed, from what I what I said, what I was really trying to do is get this tutorial not to give people new results, but to try and draw connections between existing things in the hope that was useful to people, particularly those who are from MLG or possibly also from the LP side of things.",
            "Now let's look at using in the last 15 minutes or several 10 minutes, hopefully.",
            "Particular application of SRL, which is what I'm going to do here is.",
            "I'm going to be using inference in a Cisco relational learning formalism, namely Markov logic to do machine learning of Bayesian networks.",
            "OK, so a particular sort of Bayesian network?",
            "Alright, so Bayesian networks are things called pedigrees.",
            "So what is a pedigree pedigree is just a family tree.",
            "Alright, so it can be a family tree relating human beings or other organisms.",
            "So here we have some family tree, so we got four individuals.",
            "Suppose we don't know how they are related.",
            "We could write down their relationships using a Bayesian network.",
            "There are many ways of writing down these Bayesian networks, not just this way here, so it might be the case that Bob and Diane are the parents of case, or like maybe that indicator that parents are both, you know, so you know.",
            "If we if we don't know, then we're basically doing a particular form of Bayesian network learning.",
            "Now, one of the night.",
            "This is quite a nice problem for Bayesian network there.",
            "The principle thing is that you only have two parents for child, right?",
            "Because these are real parents, right?",
            "In the real world, you only have two parents for each child.",
            "OK, so if anyone's ever done any work on Bayesian network learning, limiting the number of parents, any child going to have, this makes everything incredibly much easier, so I should mention, by the way, that Alan here in this pedigree.",
            "Here Adam here doesn't have.",
            "The parents are not shown.",
            "It doesn't mean that Adam doesn't and his parents don't exist.",
            "It means they're just not mentioned in our in our pedigree.",
            "OK, so each variable has at most two parents.",
            "Another interesting aspect of this particular sort of Bayesian network learning is that the parameters are known alright, so if this was the true pedigree and we're going to be dealing with so, we're going to deal with nettik data.",
            "We're going to be the data we're going to be on genetic information about these particular visuals.",
            "If we know about the genetic, say, for example, the blood type of Adam and Kate, then we know the probabilities which Bob here will have particular blood types, so they are given to us, so it's none of this marginalizing over unknown parameters.",
            "Stuff that you do.",
            "If you, for example use the PD score to to do the basic networking.",
            "Motivational thing SRL is there are lots of logical constraints.",
            "Parents have to be different sexes.",
            "For example, there may be two of them.",
            "All sorts of things like that, and another thing is that we're dealing with relationships between objects.",
            "The objects in question are people alright individuals, so we've got relationships in objects.",
            "We've got uncertainty, and so it makes sense to consider using SRL."
        ],
        [
            "So let's have a little look at what the data we're going to be learning from, so we're going to be learning from genetic data, OK?",
            "So a little bit of terminology here.",
            "Everyone has different, even though all human beings we have genetic variation, which is why some people we have different blood groups, different eye color, different hair color, so on and so forth.",
            "The variants of genes are known as alleles.",
            "So my my son has blonde hair.",
            "I have Brown hair so the hair color gene.",
            "Before we just more than one gene.",
            "But let's assume it's one gene differs from in our two cases, so we have different alleles there.",
            "You get one of your alleles because human beings are diploid.",
            "We have two chromosomes.",
            "The way it works is you get one allele from your mother and one of the from your father and this is your genotype.",
            "This is your genetic material and typically when we were doing this learning from pedigrees, when you see your legal you don't know which gene came from the parent in which gene comes the mother in which he came from the father.",
            "So we're going to assume that what we see what those unordered genotypes.",
            "OK, so that's what I think it is going to look like.",
            "You don't know the println origin of a particular allele, so."
        ],
        [
            "That's what it's going to be like.",
            "So what's the problem that we're going to be solving here?",
            "Given a set of possible Bayesian networks.",
            "We will have a prior over these networks so I should mention here.",
            "Having a prior is absolutely crucial in this approach because if you just take the genetic data and find the most probable network from the data, you will end up with a network with vast amounts of promiscuity, incest, alright, and because that's what the data supports, the data is all these people look quite similar.",
            "They probably you know probably painted together, you know, everyone's been having sex with everyone else and you know all this sort of business, so you need to have some sort of prior bias pushing you away from that because we say you know.",
            "If the data comes from human beings, then you know it can happen.",
            "I'm not saying it doesn't happen, but it's less likely.",
            "So you really need some sort of prior bias there.",
            "So that's that's important.",
            "The data is what we call marker data, so this is basically these genotypic data.",
            "So we imagine that we've grabbed these peoples DNA and we found the two alleles, but we don't know which one came from within the father.",
            "And we're also going to assume various nice things about the way that these illegals get passed down from from your parents.",
            "So the task is simple, find the most probable pedigree given the observed data, right?",
            "So it's just Bayesian network optimization task."
        ],
        [
            "So it's a good bayesians always reduce machine learning probabilistic inference.",
            "I'm a good base in at least a committed Bayesian.",
            "Let's say that, so that's what we're going to be doing here.",
            "So it's just going to be probabilistic inference in another Virgin network.",
            "To solve this problem.",
            "So we have to represent everything with variables to construct our probability distribution in which to do inference.",
            "So we're going to have variables which represent the pedigree will see the minute variables represent the UN observed, or the Geno types.",
            "This is going to be hidden data.",
            "This is the data which tells us where.",
            "The different genes came from Mother versus mother father, but we don't see that guy.",
            "This is the the order genotypes, but we might not for some individuals who might not even have this that might be missing as well.",
            "OK, so this is Alexis.",
            "We have the observed stuff, the stuff the people for whom we've seen this and we might have people who we haven't seen this.",
            "And we're also going to moxie variables just to make a little bit harder, which are which I'll come to later.",
            "Once we've done that, we'll put all these together.",
            "We will construct an exponential family distribution using Markov logic, which will be a.",
            "Distribution of a joint distribution of all these guys and we're just going to maximize it to find the instantiation which maximizes the G here.",
            "So it's basically."
        ],
        [
            "How it's going to work?",
            "Just OK, so this let's just go through it.",
            "So the nice thing is of course that you we can represent things in logic.",
            "So far the Palace.",
            "This is one of these ground Atom random variables.",
            "This is true if in the true pedigree, Bob indeed is the fall of Alice.",
            "Alright, so this is what pedigree variables represent.",
            "The pedigree they're going farther and other things also kind of auxiliary variables saying who's older than who.",
            "We might even know this ahead of time.",
            "We might know we might have age information about individuals.",
            "So we have lots of logical constraints which we can represent with are nice because we're using Markov logic.",
            "We can have hard constraints or soft ones there.",
            "Things saying that fixes the father of widen accidentally older, older is transitive.",
            "You only have one father.",
            "All that sort of stuff we throw that sort of thing in there.",
            "OK, if it's all very the nice thing here is everything is very kind of obvious how you do it, you don't have to kind of."
        ],
        [
            "Think of some horrible encoding.",
            "Now we have the information about the things we don't actually observe, so this is saying Bob, his paternal Ellie.",
            "Although when he got his father was illegal A2 maternal allele for Alice, the one she got from her mother, was a four and we can write down all sorts of constraints about this, basically saying that.",
            "You only get one paternal.",
            "You only get one allele passed down from your mother.",
            "Will just dealing with one particular locus.",
            "Here you always have at least something passed down from your father and here we have some things about homozygous inheritance.",
            "So if your if your father effects is the father of Y&X is alleles are both the same, the ones that X got from his father and mother, both A and that's the only one that can possibly passed down to his children, because that's the only one available, right?",
            "So you get a nice logical."
        ],
        [
            "Constraint there.",
            "And then we have other things saying.",
            "Basically, if you observe a genotype AMB two possibilities they came for father be from the mother or a kind of a mother father.",
            "That's the only two possible options for you there.",
            "So you write that down as a hard claws."
        ],
        [
            "These are all sorts of logical constraints, and so you know this is what a possible world might look like.",
            "So we just list all the true ones.",
            "So possible world, not the case where these the relationships, these are the maternal paternal alleles and this is the observed observed stuff.",
            "So that's what possible world."
        ],
        [
            "Look like.",
            "So now we bring in the probabilities.",
            "So basically you'll just have to trust me, these are correct probabilities.",
            "Basically, each time you someone passes down a gene and they had two possible things that could have passed down, a probability of half kind of kicks into the computation.",
            "So we add awaited clause like this.",
            "This is just representing the fact that if you particularly visual heterozygote has two different genes, there's probably half the child to get one, probably half will get the other.",
            "That's essentially what this this minus log half is coming from.",
            "So just trust me that this is this is the correct way of."
        ],
        [
            "Doing that also we have people who don't have parents, so we just have to yank out probability distributions for these illegals from the general population.",
            "So these these numbers here would depend on what's going in the population.",
            "This is basically saying if someone doesn't have a father, at least we don't know about their father.",
            "Father is not represented in the probability if any particular deal is given by some, some given probability in the population.",
            "So that's that's all that that works.",
            "Encoder population frequency."
        ],
        [
            "And we also have a prize on pedigrees.",
            "This is basically saying here what is this saying here for any XY and Z defects?",
            "Is the mother of?",
            "Why or why is the final result?",
            "We would prefer not to be the case that X is also the mother of Zed, so we will rule out insects and preferred.",
            "We don't like incest much and we don't like mothers and fathers to be related.",
            "I mean we could have all sorts of logical things in here.",
            "I should mention that this putting price on pedigrees isn't new to me.",
            "Have been worked by other people.",
            "Similar price, this were not using SRL."
        ],
        [
            "OK, so how do we incorporate evidence?",
            "Well, we just states ground atoms with true.",
            "We might know that John is a father.",
            "Robin, we might observe some genotype data, so this rules out all the possible worlds in which the evidence is not true.",
            "Just like multiplying in a 01 factor in particular sort.",
            "Of course, intelligent approaches to propagate this evidence to kind of knockout things which you know.",
            "However, this makes irrelevant."
        ],
        [
            "OK, so let's just do a very simple example, 'cause sometimes pretty much out now I think suppose we started with this situation here.",
            "Here's our data.",
            "We got this observed genotype stuff and suppose we somehow know that female number one is the mother of female #3 suppose we somehow knew that we don't know about all the other people that we know that so."
        ],
        [
            "What I actually did in this particular case because it was a small example, I want to make a point here.",
            "I did actually ground it out into the propositional clauses and user propositional solver.",
            "At this point you say, well, why did you use SRL?",
            "You could have done a ground thing right from the beginning.",
            "I would disagree to that.",
            "Be able to write it down in the 1st order formalism, even if at some later stage some program grounds it out for you.",
            "It's very important to be able to opportunity to write things down nicely from a usability point of view.",
            "So actually fed this into an exact weighted Max SAT solver.",
            "So this is definitely the most probable world tour.",
            "I didn't use Max Walk sat together, approximate one and in about how long it takes 30 seconds.",
            "It generated this particular most probable world which has information about the pedigree.",
            "So I'm gonna have to."
        ],
        [
            "Just finish up very soon, so if I remove the remembering the previous slide I had."
        ],
        [
            "In about a.",
            "A little bit of information about this relationship here."
        ],
        [
            "If you remove that other thing, taking 30 seconds to find the most problems sensation at times shoots up to 145.",
            "So the key observation here is a small amount of evidence makes a big difference.",
            "It makes the whole optimization problem much easier if you add in a total order.",
            "So if you know if you actually have the ages of all these people, which obviously restricts it an awful lot, you can solve this thing in less than a 10th of a second.",
            "In this particular small case.",
            "Alright, so that makes a massive massive difference.",
            "As always with Bayesian network learning.",
            "OK so I'm I'm pretty much."
        ],
        [
            "Yeah, I'm.",
            "I should mention that what I'm actually doing here isn't actually solving the problem like I meant to set out to do was because I wanted to represent this hidden data explicitly in my representation, but I'm actually I'm actually doing is.",
            "I'm finding the most probable pedigree together with the most proper instantiation of the latent data, so I'm kind of cheating a little bit there.",
            "One can actually rearrange the conditional probabilities to do the whole thing properly.",
            "Basically marginalizing out this Y in the values I put in for the conditional probabilities, but I didn't actually do that.",
            "It would be nice be nicer, directly have this sort of this sort of thing here, where you can actually do maximization and marginalization and some other things.",
            "I know federal is working on this sort of thing in Washington right now, OK, and so that's the end of my little example, so I hope you found interesting.",
            "So it's quite nice to be able to use rather than learning SRL formalism using Excel formulas and to learn something entirely different.",
            "Scenes seem quite a way of doing things.",
            "I should say that he.",
            "You know, I think statistical genetics in general is a big possible growth area for SRL.",
            "I know some people have done work on haplotypes with using SRL techniques elsewhere, so I don't think I'm the only one who thinks that, so I'll call it a day there and I'll if it's time for questions.",
            "Feel free to ask anything, thank you.",
            "Enter.",
            "Exciting.",
            "I think we have time for three questions.",
            "So in your example service this small example and it didn't work constraints.",
            "Yeah, good things.",
            "Like that?",
            "Well, if I if it was a bigger example, so remember how I did this particular one.",
            "Excuse me, second.",
            "I wasn't moving.",
            "So two things about this particular example one.",
            "I grounded the whole thing out.",
            "There's been a lot of work on avoiding grounding the whole thing out.",
            "OK, so you know, that's one thing to say, so you know if I was using a bigger example, I wouldn't necessarily transfer the whole thing to a propositional representation.",
            "Secondly, I was using exact solver alright, so this is guaranteed to be the absolute most probable pedigree.",
            "If you use something like Mac Hawks at you could get.",
            "I did some experiments actually where I used in exact solvers."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thanks a lot Luc.",
                    "label": 0
                },
                {
                    "sent": "That very kind reduction.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot to the organizers for inviting me and I must apologize for having missed the first day of the conference.",
                    "label": 0
                },
                {
                    "sent": "This is due to factors outside my control, namely my head of Department keeping me back to do my day job.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give you a tutorial on logic based approaches to SRL so those people who have ESL conference affair amount of this will be probably quite familiar to use, but I'm hoping to bring more people into the SRL fold by explaining how it all works.",
                    "label": 1
                },
                {
                    "sent": "But even for those of you who are already familiar with, so hopefully there will be something something you have interest in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best for you.",
                    "label": 0
                },
                {
                    "sent": "So here's the here's the structure of the talk.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I will start very gently is after lunch and some of you had some huge hard and will go through some of the connections between logic and probability.",
                    "label": 1
                },
                {
                    "sent": "Time will move into talking about first order probabilistic models, those which use power factors which those of you who are in the previous SRL session will have heard an awful lot about.",
                    "label": 0
                },
                {
                    "sent": "I'll be fairly fairly brief about that because there's been a lot of talks about that already in some more detail we're going to look at an alternative way of doing of constructing 1st order probabilistic models, namely the PRISM system.",
                    "label": 0
                },
                {
                    "sent": "And there are other systems related to PRISM and then for those of you who maybe know this stuff already.",
                    "label": 0
                },
                {
                    "sent": "I'm going to finish with an example, some actual learning.",
                    "label": 1
                },
                {
                    "sent": "This is using SRL Markov logic specifically actually to learn Bayesian networks, and these are Bayesian networks which are going to represent family trees.",
                    "label": 0
                },
                {
                    "sent": "Are the relationships between between human beings and we're going to learn that from genetic data, so that's using our own statistics.",
                    "label": 0
                },
                {
                    "sent": "I'm hoping to sell this particular application domain as an interesting area for people to get involved in, so that's what we're going to finish.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "OK, so a lot of what I want to do in the first part of the talk is just to bring out some fairly straightforward connections between these four elements here in the box, we're going to start nice and easy down with propositional logic and work our way up to the lesion fields of 1st order probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "OK, when I go basically going two different routes, this is the power factor root, and this is the kind of generative model RT.",
                    "label": 0
                },
                {
                    "sent": "Oh, I would argue.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just propositional logic in where we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Salt his propositional logic in one slide.",
                    "label": 1
                },
                {
                    "sent": "I'm assuming most people have at least some familiarity with it, so propositional atoms are.",
                    "label": 1
                },
                {
                    "sent": "You can think of them as just binary variables that represent statements about the world, which are either true or false.",
                    "label": 0
                },
                {
                    "sent": "We can associate full suit with the number 0 truth with the number one, so that's a propositional Atom.",
                    "label": 0
                },
                {
                    "sent": "If we are a particular joint instantiation of all the atoms in some particular propositional language which satisfy some.",
                    "label": 1
                },
                {
                    "sent": "Propositional formula is known as a model of that Formula One the slight problems with connecting probabilistic reasoning.",
                    "label": 1
                },
                {
                    "sent": "Analogical reasoning is that word model has different different meanings in the two areas.",
                    "label": 0
                },
                {
                    "sent": "So you have to be a bit careful with this word model here, and I'm deliberately choosing rather bizarre way of representing propositional formulas down the bottom here in order to better make the link with probabilistic models, which will come to quite shortly.",
                    "label": 1
                },
                {
                    "sent": "For example, and what we're doing here is this saying suppose we just had A and be with the only propositions in a language.",
                    "label": 0
                },
                {
                    "sent": "If we have the two propositions, first thing they say is true, the second saying this one.",
                    "label": 0
                },
                {
                    "sent": "Here A implies B.",
                    "label": 0
                },
                {
                    "sent": "Basically, we can if we so wish represent them as factors OK factors in the in the graphical model sense of functions from.",
                    "label": 0
                },
                {
                    "sent": "I'll just wait a second.",
                    "label": 0
                },
                {
                    "sent": "Functions from joint instantiations of some of the variables to some number or other sense of doing logic.",
                    "label": 0
                },
                {
                    "sent": "These numbers are just going to be 01, so this is this is a is true.",
                    "label": 0
                },
                {
                    "sent": "This is not a LB and you can see the only time when its forces when you had a true and B not then be false.",
                    "label": 0
                },
                {
                    "sent": "We can view the the notion of conjoining these two is analogous.",
                    "label": 0
                },
                {
                    "sent": "We can represent this by multiplying these two factors and what we get over here is a list of all the possible models for these two.",
                    "label": 0
                },
                {
                    "sent": "These two guys here and we see this.",
                    "label": 0
                },
                {
                    "sent": "Early model we have this formula.",
                    "label": 0
                },
                {
                    "sent": "Here is a very true and being true 'cause we're going on there and we got zeros everywhere else and essentially what you could do is if you had your propositional formula in conjunctive normal form, each of your each of your clauses could be written like this.",
                    "label": 0
                },
                {
                    "sent": "If you multiply them all together in this way, what you end up with is a massive big factor, usually with more than two variables.",
                    "label": 0
                },
                {
                    "sent": "Should be quite large with loads of zeros and ones, the ones corresponding to the models of the of the propositional formula.",
                    "label": 0
                },
                {
                    "sent": "Question, so this would be a very bizarre way of doing satisfiability testing, so that's the whole point.",
                    "label": 0
                },
                {
                    "sent": "This slide is just to connect propositional formula to these factors with zeros or ones in them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's enough on propositional logic.",
                    "label": 1
                },
                {
                    "sent": "Let's just move to what I'm going to call propositional probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "I don't think there's any formal definition of what a propositional probabilistic model is, but.",
                    "label": 0
                },
                {
                    "sent": "Is how long?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To find them so the basic idea is you can view it as generalizing propositional logic.",
                    "label": 1
                },
                {
                    "sent": "So one way of trying to link logic and probability is to say probabilistic inference is the kind of the basic way of doing things.",
                    "label": 0
                },
                {
                    "sent": "That's that's the kind of general way, and logic is just a special case of probabilistic inference, and that's basically how I'm going to present it to you today.",
                    "label": 0
                },
                {
                    "sent": "There are other approaches, we say logic is kind of where you start, and then you add some probability into that, and that's not what we're going to be doing here.",
                    "label": 0
                },
                {
                    "sent": "So basically how you generalize it is very simple.",
                    "label": 0
                },
                {
                    "sent": "We now add.",
                    "label": 0
                },
                {
                    "sent": "We now allow any non negative values in the factors here, not just zeros or ones, and generally speaking your random variables can don't have to be binary.",
                    "label": 1
                },
                {
                    "sent": "You can have as many many values as you like.",
                    "label": 0
                },
                {
                    "sent": "In fact they can be continuous.",
                    "label": 0
                },
                {
                    "sent": "Although throughout this talk will just deal with discrete ones.",
                    "label": 0
                },
                {
                    "sent": "All the examples I will use, I will just stick to binary variables just because things, but on the slide better.",
                    "label": 0
                },
                {
                    "sent": "But basically you have the same idea.",
                    "label": 0
                },
                {
                    "sent": "If you multiply factors together you get bigger factors, so it's always 20.",
                    "label": 0
                },
                {
                    "sent": "Here becomes because we've got a four and a five there.",
                    "label": 0
                },
                {
                    "sent": "I haven't got into this factor multiplication stuff, but hopefully people can see what's going on.",
                    "label": 0
                },
                {
                    "sent": "I expect most people have come across this previously anyway.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that what we've got over here doesn't look much like a probability distribution, 'cause the numbers are rather too too large.",
                    "label": 1
                },
                {
                    "sent": "It doesn't nonetheless define a probability distribution, because we can just normalize this.",
                    "label": 0
                },
                {
                    "sent": "OK, we just add up these numbers if once we add them up we got a number other than zero, we can just normalize.",
                    "label": 0
                },
                {
                    "sent": "So the probability that a zero and B0 in this particular distribution is 20 / 82.",
                    "label": 1
                },
                {
                    "sent": "OK, the question of whether Zed here is 0 or not.",
                    "label": 0
                },
                {
                    "sent": "It's basically the satisfiability problem from propositional logic.",
                    "label": 0
                },
                {
                    "sent": "Revisit it if you think about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a factor representation of a of a probability distribution, which I'm sure many people have seen.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Often in any other cases, say some further examples.",
                    "label": 0
                },
                {
                    "sent": "So this one at the top here is basically a mark of network and is associated graph to that, so I'm sure many people have come across Markov networks before, so make sure I get the right way round.",
                    "label": 0
                },
                {
                    "sent": "So typically in graphical models want asociates if one can the variables and factors with cliques of some graph or other, so that the graph of this guy at the top here will be this one here, and you can read off condition, independence relationships and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so this defines the distribution with 16 numbers in it, one for each possible instantiations of ABC and D1 down the bottom.",
                    "label": 0
                },
                {
                    "sent": "Here it doesn't actually correspond to a graphical model because you can't produce a graph with these guys as the cliques of that graph.",
                    "label": 0
                },
                {
                    "sent": "So this is a non graphical hierarchical model, so these are just some further examples of propositional probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "By the way, feel free to shout and tell me to stop and ask.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions at any point.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at a particular sort of propositional probabilistic Model 1 where we restrict what sort of numbers can go in these factors, and these are weighted clauses, so we saw some normal propositional clauses and one of the earlier slides where you just have normal propositional clauses.",
                    "label": 0
                },
                {
                    "sent": "We can think of weighted clauses.",
                    "label": 0
                },
                {
                    "sent": "So here and here we have a clause here saying a is true and the wait.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give to a here is infinite.",
                    "label": 0
                },
                {
                    "sent": "So I'm basically saying when I say is true, A has to be true.",
                    "label": 0
                },
                {
                    "sent": "Any model in which a is false.",
                    "label": 0
                },
                {
                    "sent": "You know we're just going to roll it out entirely, and so that would be represented exactly the same as any other sort of logical clause like this.",
                    "label": 0
                },
                {
                    "sent": "Over here we have a different another clause, same as before, but now it's waited and we have to wait here.",
                    "label": 0
                },
                {
                    "sent": "What we're saying here is that we would like this clause to be true, and the degree to which we want it to be true.",
                    "label": 0
                },
                {
                    "sent": "We're going to give a number 22 if it's not true, we don't entirely rule out a propositional model in which it is false, we just make it less probable than one.",
                    "label": 0
                },
                {
                    "sent": "Where it is true, all other things being equal.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can rewrite.",
                    "label": 0
                },
                {
                    "sent": "So before we had a clause where this was a 0 now.",
                    "label": 0
                },
                {
                    "sent": "It's a Long story short.",
                    "label": 0
                },
                {
                    "sent": "We put it to the power minus two in there and now if we do the multiplication exactly as before, we end up with this situation here.",
                    "label": 0
                },
                {
                    "sent": "So now we've got 20 probability models.",
                    "label": 0
                },
                {
                    "sent": "This one here is the best model.",
                    "label": 0
                },
                {
                    "sent": "It's got a one and this one here eats the minus two.",
                    "label": 0
                },
                {
                    "sent": "So the reason why I bring up this particular way of encoding probability distributions is big 'cause there are lots of nice solvers out there which most of all most of them devote themselves to this problem here.",
                    "label": 0
                },
                {
                    "sent": "Finding the most problem instantiation given a whole awaited clauses.",
                    "label": 0
                },
                {
                    "sent": "Generally speaking many more than two with many more variables than that.",
                    "label": 0
                },
                {
                    "sent": "We want to find which model has the highest.",
                    "label": 0
                },
                {
                    "sent": "The highest waiting OK in this case is easy.",
                    "label": 0
                },
                {
                    "sent": "It's obviously this guy down here equals one B = 1.",
                    "label": 0
                },
                {
                    "sent": "Evidently, in a bigger case, you wouldn't want to do this big multiplication to find what that's the highest model it, so there are all sorts of set solvers.",
                    "label": 0
                },
                {
                    "sent": "Probably the most famous of which is that the Max Walksat algorithm there are others as well, and we're going to be using this representation later on when we get to the.",
                    "label": 0
                },
                {
                    "sent": "Statistical genetics example.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's way too close.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just briefly, I'm sure most people have come across with Bayesian network is before, but just for the record and because we're going to be learning Bayesian networks later if we restrict the factors in the following way, we have one factor for each variable.",
                    "label": 1
                },
                {
                    "sent": "The fact the factor for the variable is a conditional distribution of that variable given some other variables known as its parents.",
                    "label": 1
                },
                {
                    "sent": "We can then draw a graph using those parent child conditions if it is the case that the graph in question is a cyclic, then we have a Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "We've seen a special case of weighted clauses.",
                    "label": 0
                },
                {
                    "sent": "Another special case is Bayesian networks, where we have products of conditional probability tables OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what problems do we generally want to solve in probabilistic models?",
                    "label": 0
                },
                {
                    "sent": "One of the key ones, inference, what does inference mean in probabilistic models?",
                    "label": 0
                },
                {
                    "sent": "And again, one has to be somewhat careful here.",
                    "label": 0
                },
                {
                    "sent": "The term inference is used in logic and also in various computations.",
                    "label": 0
                },
                {
                    "sent": "Recurrent probabilistic models are not exactly the same problem, but the same basic idea that all the information you need is there.",
                    "label": 0
                },
                {
                    "sent": "In your model you would probably your job is just to extract the information of interest from that in some way and the usual.",
                    "label": 0
                },
                {
                    "sent": "The two most common ones are computing the marginal distribution of one or more variables.",
                    "label": 1
                },
                {
                    "sent": "So if we have a distribution like this, we might want to just get the distribution of a by getting rid of BC and D and also finding which particular instantiation's the highest probability.",
                    "label": 0
                },
                {
                    "sent": "So there's a common things.",
                    "label": 0
                },
                {
                    "sent": "How does one do that?",
                    "label": 0
                },
                {
                    "sent": "A lot of the algorithms are based on something called variable elimination, and this variable information algorithm can be summed up in two very two very simple observations.",
                    "label": 0
                },
                {
                    "sent": "If we recall these factors that we saw previously.",
                    "label": 0
                },
                {
                    "sent": "Which is what we're assuming defines our probability distribution we take through these factors multiplied together and get a bigger factor.",
                    "label": 1
                },
                {
                    "sent": "We haven't changed the distribution in question, and Moreover, if a particular variable only occurs in one of these factors, then you just have to sum it out of that factor, and that's OK. As far as something out of the entire distribution.",
                    "label": 0
                },
                {
                    "sent": "So the variable information algorithm just takes a factor, multiplies, takes a variable, multiplies all factors together for that variable, getting one big factor with that variable in question sums it out.",
                    "label": 0
                },
                {
                    "sent": "Thereby producing a new factor and then just carries on OK, so that's available information in a nutshell.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're making our way gradually towards their first order.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look at 1st order Logic, so I won't spent the best part of the year of my life studying 1st order logic.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to encapsulate in two slides.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here we go.",
                    "label": 0
                },
                {
                    "sent": "So what are the characteristics of 1st order logic?",
                    "label": 1
                },
                {
                    "sent": "So as with propositional logic, we have assertions of what is true and false.",
                    "label": 0
                },
                {
                    "sent": "However, now the propositions themselves have some sort of internal structure.",
                    "label": 0
                },
                {
                    "sent": "Now we propositions assert properties of objects and also relations of objects.",
                    "label": 1
                },
                {
                    "sent": "So the key thing is that we have a language in which we talk about objects.",
                    "label": 1
                },
                {
                    "sent": "And these objects are represented by things called ground terms, and the reason it's called 1st order is because we can quantify over objects and we have some quantum University quantified formerly down here.",
                    "label": 1
                },
                {
                    "sent": "What is a universally quantified formula?",
                    "label": 1
                },
                {
                    "sent": "So two very rough and 1st approximation you can think of it as a kind of template for all its ground instances.",
                    "label": 0
                },
                {
                    "sent": "So when we see University quantum formula like this but it says any X if X is even, then F of X is odd sorts.",
                    "label": 0
                },
                {
                    "sent": "Here you can think of.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, is the successor function, so you can think of that as standing for all the possible ground instances in some fixed language of that, so it's not quite that, but it's kind of more or less that and so we can replace the X here by the ground term 0, and then we get this one.",
                    "label": 0
                },
                {
                    "sent": "We can replace the X by the ground turn S 0 and we get this guy you carry on like that.",
                    "label": 0
                },
                {
                    "sent": "There's all this in this case, infinitely many ground instances one can generate from this.",
                    "label": 0
                },
                {
                    "sent": "Here's another example where we're just sticking in AIDS, bees, and seizes the XYZ and Zeds.",
                    "label": 0
                },
                {
                    "sent": "So simple simple observation is that we can use this as a template to compactly represent infinitely many assertions about what's going on in the world.",
                    "label": 0
                },
                {
                    "sent": "That's the other key things about first order logic, which will be exploited when we move up to probabilistic stuff.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can, if we so wished represent 1st order clause using one of these factor representations.",
                    "label": 0
                },
                {
                    "sent": "This is a rather peculiar way of presenting it, but as you can probably guess, the reason I'm doing it is to make the connection to the factors I've used to define probability distributions previously, so we could imagine writing this clause here like this and what this is saying is that for any particular instantiation of X&Y we have a factor.",
                    "label": 0
                },
                {
                    "sent": "Corresponding that particular situation so we can think of this this clause here as representing some product of many, many ground instances of said clause, possibly finitely many, depending on the language, possibly infinitely many.",
                    "label": 0
                },
                {
                    "sent": "So that's that's one way one way of thinking about what a University of quantified formula looks like.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just a bit more before we get to the probability.",
                    "label": 0
                },
                {
                    "sent": "So what is a model in in first order terms?",
                    "label": 0
                },
                {
                    "sent": "Again, this is another big area as a whole industry or the whole academic discipline of 1st order model theory.",
                    "label": 0
                },
                {
                    "sent": "We're going to make life easier for ourselves and just consider her Brown models and basically the way to think of her brand model is that it assigns the value true or false to each ground atomic formula in our language in question.",
                    "label": 1
                },
                {
                    "sent": "So all these atoms for sure.",
                    "label": 0
                },
                {
                    "sent": "So typically one might have infinitely many of these other circumstances that could be just finitely many of these.",
                    "label": 0
                },
                {
                    "sent": "So the key point of bringing this up is essentially because of we got this.",
                    "label": 1
                },
                {
                    "sent": "We got a number of different possible models you can think of the ground atoms acting like binary variables there either true or their fault.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's part of 1 model where P of a here is true and B&B is false or some other model.",
                    "label": 0
                },
                {
                    "sent": "Here PV is not false, it's true.",
                    "label": 0
                },
                {
                    "sent": "OK, so just so you can think of it is just a big long joint instantiation of a whole bunch of binary variables.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're just going to use logic as a convenient way of talking.",
                    "label": 0
                },
                {
                    "sent": "About such joints instantiation's.",
                    "label": 0
                },
                {
                    "sent": "I mean you can think of 1st order logic is a language for talking about first order models, and that's that's what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Excuse me just a second.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the key good things about first order logic is that in order to perform inference in first logic, we do not have to translate down to propositional logic.",
                    "label": 1
                },
                {
                    "sent": "OK, so one could imagine a completely crazy way of doing 1st order logic inference, which is when we take a formula like this.",
                    "label": 0
                },
                {
                    "sent": "Somehow generates all its ground instances or maybe lots of its ground instances.",
                    "label": 0
                },
                {
                    "sent": "Take another formula here, generate loads of ground instances.",
                    "label": 0
                },
                {
                    "sent": "This guy then use propositional logic on those ground instances that derive a whole lot of other ground formula, and then somehow generalize and back into some universe University quantified formula.",
                    "label": 0
                },
                {
                    "sent": "Back one didn't have to do that, so the key thing is that you can do inference while staying at the 1st order level.",
                    "label": 0
                },
                {
                    "sent": "OK, here's a particular case of doing some inference from these two guys.",
                    "label": 0
                },
                {
                    "sent": "This one here follows by a very well known rule.",
                    "label": 0
                },
                {
                    "sent": "1st order very well known rule of 1st order logic known as resolution.",
                    "label": 0
                },
                {
                    "sent": "Interestingly enough, we once had the great pleasure of having Robinson, the famous logic programming pioneer.",
                    "label": 0
                },
                {
                    "sent": "Come to give a talk at York.",
                    "label": 0
                },
                {
                    "sent": "It was talking about how.",
                    "label": 0
                },
                {
                    "sent": "You know when he was first grappling with some of the algorithms which are now form part of logic programming systems.",
                    "label": 1
                },
                {
                    "sent": "This this the way he first of all came, looked at doing inference was by this very much grounding out idea.",
                    "label": 0
                },
                {
                    "sent": "Then I can't remember what term we had for it and it was a big breakthrough to realize that you could.",
                    "label": 0
                },
                {
                    "sent": "You could stay for doing logic programming.",
                    "label": 0
                },
                {
                    "sent": "You could stay the 1st order level.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now we've we've kind of just as good precursor gone through these these approaches here and now we're going to start talking about first order probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing it is to say, well.",
                    "label": 0
                },
                {
                    "sent": "We've got this propositional probabilistic models usually connected to graphs in one way or another.",
                    "label": 1
                },
                {
                    "sent": "What we're going to do is we're going to generalize from then from them.",
                    "label": 0
                },
                {
                    "sent": "Excuse me in pretty much the same way to generalize from propositional logic to 1st order logic, and that's the 1st way of doing things that I'm going to consider here today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do it very briefly.",
                    "label": 0
                },
                {
                    "sent": "Becausw I've only got a certain amount of time and I want to spend anymore time on this one so.",
                    "label": 0
                },
                {
                    "sent": "That's so that's just how how things are going to be organized.",
                    "label": 0
                },
                {
                    "sent": "So we'll start off with going from here here, up to here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use something called.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Power factors, so the basic idea is, as you might, if even if you haven't come across this sort of stuff previously, perhaps you'd be buried away in the MLG sessions and haven't been to the SRL sessions.",
                    "label": 0
                },
                {
                    "sent": "Maybe would have guessed this is basically what's how things are going to what we can do is we can say, right?",
                    "label": 0
                },
                {
                    "sent": "Well, we had we had normal factors which the product of which describes some probability distribution.",
                    "label": 1
                },
                {
                    "sent": "What we're going to do now is we're going to use the same trip we haven't first or logic by which we view things like this.",
                    "label": 0
                },
                {
                    "sent": "As templates for its ground instances, and we're going to represent a whole bunch of factors with the same structure by a single factor using logical variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the basic idea there called power factor is short for parameterized factors.",
                    "label": 0
                },
                {
                    "sent": "I think David Poole was the guy who came up with this term.",
                    "label": 0
                },
                {
                    "sent": "I am open to correction on that point.",
                    "label": 0
                },
                {
                    "sent": "So the key thing here is you can represent many, possibly infinitely many, of these normal factors by in a much more compact, compact way.",
                    "label": 0
                },
                {
                    "sent": "OK, and typically one as well as having one is just saying well this represents all the factors you can get by sticking something into Exterran.",
                    "label": 1
                },
                {
                    "sent": "Sticking something into why there typically one also has constraints to kind of cut down on the number of ground instances, which this implicitly represents.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might say that we only care about cases where X is different from Y.",
                    "label": 0
                },
                {
                    "sent": "Also, quite, it's quite common to say that we're going to have some typing here, so rather than any ground term in the office or language being a candidate for substituting for X where X might be a type thing, they might only have certain sorts of things which can ground X out there, and in fact, when we come to the example later on with the statistical genetics, we would indeed have that have that sort of typing.",
                    "label": 0
                },
                {
                    "sent": "Just be aware that if we got a little bit careful if there are infinitely many of these guys, then if it was this particular factoring question that this was infinitely big and it will, all these guys would head off to 0.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit.",
                    "label": 1
                },
                {
                    "sent": "It's gotta be careful about infinite collections of power factors.",
                    "label": 0
                },
                {
                    "sent": "Evidently if we're dealing with a big but finite collection, then everything everything is fine at the end of the day, what we're representing implicitly is many, many factors which would multiply together.",
                    "label": 0
                },
                {
                    "sent": "To generate a distribution over over these guys here OK.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "That's the basic idea of this way of doing first.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probabilistic representation, So what sort of probability distribution have we defined?",
                    "label": 1
                },
                {
                    "sent": "So what happens now is that each ground Atom becomes a random variable.",
                    "label": 1
                },
                {
                    "sent": "OK, and this also means that when you're talking about these things, you have to be careful what you mean by a variable, because we've got logical variables and we've got random variables.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if we have something like PX from over here, this is a logical variable.",
                    "label": 0
                },
                {
                    "sent": "Once we've ground it, we get this guy here, which is a random variable, even though it doesn't look like a very well.",
                    "label": 0
                },
                {
                    "sent": "You might not think it is indeed.",
                    "label": 0
                },
                {
                    "sent": "A random variable.",
                    "label": 1
                },
                {
                    "sent": "So ground atoms represent random random variables.",
                    "label": 0
                },
                {
                    "sent": "If these random variables are binary, which is the you know assumption one.",
                    "label": 1
                },
                {
                    "sent": "I'm going to make it in this particular talk.",
                    "label": 0
                },
                {
                    "sent": "Then the distribution in question is over her brain models.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically each possible joint is imagine you've got these factors represented in the 1st order fashion.",
                    "label": 0
                },
                {
                    "sent": "I just showed you.",
                    "label": 0
                },
                {
                    "sent": "One typically have more than one.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that's the sake of argument.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we have.",
                    "label": 0
                },
                {
                    "sent": "Finite language, so in principle one could imagine replacing these first order factors by the product of their ground instances, and then one could imagine multiplying all these ground factors together to get one humongous big factor.",
                    "label": 0
                },
                {
                    "sent": "Each row in that humongous big big factor will be a joint instantiation of all the ground atoms, which is a Herbrand model, so you so you've got a distribution over possible per gram models, and so these are usually known as.",
                    "label": 0
                },
                {
                    "sent": "This is a distribution of possible worlds, because you can think.",
                    "label": 0
                },
                {
                    "sent": "Of the 1st order model as a possible description of some, some world of interest, this is quite interesting for someone as like me as absolute said, I come from to certain extent from a philosophical background.",
                    "label": 0
                },
                {
                    "sent": "At the very pre history of probability theory, probability distributions were seen as being defined over possible worlds, which is quite interesting.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to like knits and read all his philosophical stuff this you know he was there in the beginning where people were solving probability problems to settle their gambling debts and that sort of stuff because of his background in philosophy and because of his whole his whole philosophical tradition was based on this notion that there are many many possible worlds and we somehow even the best of all possible worlds.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is because that's the most perfect world.",
                    "label": 0
                },
                {
                    "sent": "So that's the world that God has chosen us to live in all this sort of lava.",
                    "label": 0
                },
                {
                    "sent": "This whole kind of massive theory built around it when he got interested in probability, he started introducing the note.",
                    "label": 0
                },
                {
                    "sent": "The notion that probabilities are about he didn't put it in this in these terms or partners writing in Latin even put it in these terms.",
                    "label": 0
                },
                {
                    "sent": "But basically probability is about.",
                    "label": 0
                },
                {
                    "sent": "It is always about the distribution over possible worlds and we just happen to live in the most for one.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "I should also mention again from the philosophical backgrounds coming from this awful background.",
                    "label": 0
                },
                {
                    "sent": "This idea of possible worlds distributions also.",
                    "label": 0
                },
                {
                    "sent": "Slightly more recently than it's there's a lot of work by a chap called Carnap who was mercilessly criticized by Popper.",
                    "label": 0
                },
                {
                    "sent": "This chap, called Karnak, came up with this again.",
                    "label": 0
                },
                {
                    "sent": "A lot of work on distributions over first order models.",
                    "label": 0
                },
                {
                    "sent": "Are you possible worlds?",
                    "label": 0
                },
                {
                    "sent": "What he was trying to work on was trying to find what is the correct single distribution of a possible worlds.",
                    "label": 0
                },
                {
                    "sent": "You know someone writes analogical theory, somethings are entails something is not entailed there for some other things.",
                    "label": 0
                },
                {
                    "sent": "Must be, you know, maybe some things are partially entailed.",
                    "label": 0
                },
                {
                    "sent": "So here's what homework was on using things like this.",
                    "label": 0
                },
                {
                    "sent": "Principle of indifference to come up with a single distribution of always possible worlds, which was the logically correct distribution of a possible worlds.",
                    "label": 0
                },
                {
                    "sent": "We are much more pragmatic people.",
                    "label": 0
                },
                {
                    "sent": "We learn our distributions of possible words from wells, from data.",
                    "label": 0
                },
                {
                    "sent": "So excuse me, my slide for this little digression.",
                    "label": 0
                },
                {
                    "sent": "Now let's get back to the technical stuff.",
                    "label": 0
                },
                {
                    "sent": "So evidently we can always, if we wish, and sometimes we might even do this, take a power factor representation.",
                    "label": 0
                },
                {
                    "sent": "I do not say Paula Factor is short for parameterized factor 'cause the logical variables of parameterising it we can take apart factor an if we got finiteness around we can always ground it out and just carry on doing probabilistic inference as per normal.",
                    "label": 0
                },
                {
                    "sent": "OK so it's one thing to mention.",
                    "label": 0
                },
                {
                    "sent": "Another thing is to say although we have described the probability distribution over Herbrand models and using logic once we have done that, it is a kind of independently existing mathematical object.",
                    "label": 1
                },
                {
                    "sent": "We don't necessarily have to use logic.",
                    "label": 0
                },
                {
                    "sent": "To analyze it to fit the parameters of it to do structure learning or whatever.",
                    "label": 0
                },
                {
                    "sent": "So so I think that's a point I'd like.",
                    "label": 0
                },
                {
                    "sent": "I'd like to make there.",
                    "label": 0
                },
                {
                    "sent": "Ascari.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the big question which I'm going to skip scandalously because it's been answered in a number of ways and immediately preceding session.",
                    "label": 0
                },
                {
                    "sent": "So the obvious question arises.",
                    "label": 0
                },
                {
                    "sent": "One of the nice things about first Order logic is that we can do inference at the 1st order level.",
                    "label": 0
                },
                {
                    "sent": "Now we've got these probabilistic models defined using first order variables.",
                    "label": 0
                },
                {
                    "sent": "Can we do inference by which we mean marginalization maximization in first order probabilistic models?",
                    "label": 1
                },
                {
                    "sent": "And what we basically.",
                    "label": 0
                },
                {
                    "sent": "Well, that question amounts to is if we've got a whole load of.",
                    "label": 0
                },
                {
                    "sent": "Remember these guys are random variables, right?",
                    "label": 0
                },
                {
                    "sent": "There may not look like them, but they are.",
                    "label": 0
                },
                {
                    "sent": "We got hold of random variables, all of which are ground instances of some other.",
                    "label": 0
                },
                {
                    "sent": "Mother formula with a free variable.",
                    "label": 1
                },
                {
                    "sent": "Can we somehow all these guys in one go buy something out this guy?",
                    "label": 0
                },
                {
                    "sent": "And evidently if we've got a very if we got a distribution which define if we got a distribution of a very many random variables and we want the marginal distribution over a particular ground Atom, someone says to us, right?",
                    "label": 0
                },
                {
                    "sent": "You've got this nice distribution.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that I don't know?",
                    "label": 0
                },
                {
                    "sent": "Q&A B is true.",
                    "label": 0
                },
                {
                    "sent": "Then conceptually one has some out all the other variables, so we want to get rid of this quickly as possible.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we do that and?",
                    "label": 0
                },
                {
                    "sent": "In order to avoid gross redundancy, I'm not actually going to go into that big question, but let me just say that is one of the hot topics of of of this of this field.",
                    "label": 0
                },
                {
                    "sent": "So for those, if you weren't in the previous session, those who were in the previous SRL session.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We know this to be the case already, so as I said, you're in the right place, sorry.",
                    "label": 1
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Are you assuming there at all independent of each other?",
                    "label": 0
                },
                {
                    "sent": "No, certainly not, no, not at all.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, there's no reason I mean QA and this guy here and this guy here, so not independent.",
                    "label": 0
                },
                {
                    "sent": "I mean even for working the other guys over here, yeah, they don't have to be independent at all.",
                    "label": 0
                },
                {
                    "sent": "If later on we'll see a case where.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of ground atoms are independent, but assuming that that's all, otherwise things would be kind of easy.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so just yell by the way also.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not looking at your direction.",
                    "label": 0
                },
                {
                    "sent": "So this is I get this other kind of version of this talk at York and someone said to me, what are the topics in SRL at the moment?",
                    "label": 0
                },
                {
                    "sent": "And I said one of them is definitely this lifted inference, infer sort of probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "So I'm not going to go into all the different ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "Suffice it to say, many people make their money talks and also posts is probably later on today where you can speak to people about this.",
                    "label": 0
                },
                {
                    "sent": "Basic idea is that when we got repeated structure, we want to exploit it when it's available, but often when, especially when he had evidence in lots of a nice symmetries that we have an original verse or probably model might disappear, so it's not at all a trivial trivial problem.",
                    "label": 1
                },
                {
                    "sent": "So for those who wanted a tutorial on 1st order inference in first order problem models, I apologize, but it would just taking way too long to do it any justice whatsoever.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I want to talk about a particular sort of 1st order probabilistic model in this vein mark of logic, which is been around since I think 2006.",
                    "label": 0
                },
                {
                    "sent": "The paper with Richardson and I'm English.",
                    "label": 0
                },
                {
                    "sent": "So we've seen previously waited propositional clauses, so here's a way to propositional clause.",
                    "label": 0
                },
                {
                    "sent": "Nice idea is to say, well, we're going to have waited 1st order clauses and the thing that nice thing about clauses that there certainly I found this as a user is that you can write 1st order rules down.",
                    "label": 0
                },
                {
                    "sent": "As clauses, and this is a very.",
                    "label": 0
                },
                {
                    "sent": "Psychologically useful way of getting information in your system.",
                    "label": 0
                },
                {
                    "sent": "It's a very nice knowledge representation approach so we can have a.",
                    "label": 0
                },
                {
                    "sent": "Something like this and what this is what this looks down.",
                    "label": 0
                },
                {
                    "sent": "The factor representation is like this.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "Again we have the one the one and here with Eaton minus two we could replace this factor by 1 where we had we multiplied all these guys by E to the power of two.",
                    "label": 0
                },
                {
                    "sent": "We could get a pair of 2 zero.",
                    "label": 0
                },
                {
                    "sent": "Sorry there are two one these.",
                    "label": 0
                },
                {
                    "sent": "Apparently there are two would be the same the same distribution, so that's that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "By mark of logic it's to use weighted clauses as the particular sort of power factor facts that we have.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And because we have that particular restriction on what are factors, as are we get a particular sort of probability distribution is kind of much more regular than the kind of general case.",
                    "label": 0
                },
                {
                    "sent": "The key thing is that what matters for anyone, and we're going to assume to stay out of controversy.",
                    "label": 0
                },
                {
                    "sent": "Taller formula clauses.",
                    "label": 0
                },
                {
                    "sent": "What really matters for any one of these weighted clauses?",
                    "label": 0
                },
                {
                    "sent": "What really matters for any is how how many true groundings we have, or that clause in any particular urban model, and we end up with this distribution here.",
                    "label": 0
                },
                {
                    "sent": "So if X is a Herbrand model, remember this is a joint instantiation of very many binary variables represented by ground atoms.",
                    "label": 0
                },
                {
                    "sent": "That Alexis here.",
                    "label": 0
                },
                {
                    "sent": "The probability of that particular X we have a unpleasant normalizing constant which will try to forget about the time being Anna distribution question is given by this thing here.",
                    "label": 0
                },
                {
                    "sent": "Here's the wait for the clause.",
                    "label": 0
                },
                {
                    "sent": "Here's the number of two true groundings of that clause in this particular world.",
                    "label": 1
                },
                {
                    "sent": "X and we get this nice formula here for the probability of offset world OK. And so this is at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "Eight particular exponential family distribution.",
                    "label": 0
                },
                {
                    "sent": "So any any distribution looks like this is known as an exponential family distribution.",
                    "label": 0
                },
                {
                    "sent": "And what we call.",
                    "label": 0
                },
                {
                    "sent": "This vector of values here for any particular X which is particular world, how often was closed.",
                    "label": 0
                },
                {
                    "sent": "One satisfied health is close to satisfied up to close K. That's the key.",
                    "label": 1
                },
                {
                    "sent": "The key quantity that in the statistical literature that's known as the Canonical statistic, so that we're going to come back to market logic later.",
                    "label": 0
                },
                {
                    "sent": "So it's a nice special.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so what's what sort of data are we going to learn?",
                    "label": 0
                },
                {
                    "sent": "Such 1st order probabilistic models from?",
                    "label": 0
                },
                {
                    "sent": "Well, the distributions over possible worlds, so in the kind of most straightforward case, the data you observe is a particular possible worlds.",
                    "label": 0
                },
                {
                    "sent": "OK, so obviously you can have, as you might expect, you have situations we have missing data or you don't see all the possible world in question, but the complete data cases to see a possible world so.",
                    "label": 0
                },
                {
                    "sent": "And we counted if you want learn from IID instances of possible worlds.",
                    "label": 0
                },
                {
                    "sent": "You know one possible world is your first observation, another possible segmentation, so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And what one could learn from that work, learning that situation often we just have a single observed world which we can view as a relational database.",
                    "label": 0
                },
                {
                    "sent": "And you might think having just a single data point isn't enough to do any learning with, but because we've got this repeated structure, you can actually have a reasonable chance of getting reasonably good estimates of the parameters or reasonably good structure learning.",
                    "label": 0
                },
                {
                    "sent": "You have, you know there's enough information in a single single case there.",
                    "label": 0
                },
                {
                    "sent": "OK in the case of the mark of logic in particular, what really matters is the counts of the true groundings of the formula question, 'cause that's what's going to go into into.",
                    "label": 0
                },
                {
                    "sent": "For example, parameter estimation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I want to move onto.",
                    "label": 0
                },
                {
                    "sent": "We've had quite a sort of people wanted more on this particular way of doing things then if you weren't at the previous session you made a mistake 'cause there was lots of lots of really interesting talks about that sort of stuff there so far haven't so much about the direction.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into, which is a more generative approach.",
                    "label": 0
                },
                {
                    "sent": "Going more directly in my view from first floor.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the probability models.",
                    "label": 0
                },
                {
                    "sent": "So the inspiration, while this is how I think of it for this particular vein of doing things comes from dynamic probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "So we saw just then an example cases of quantifying over random variables.",
                    "label": 0
                },
                {
                    "sent": "This is not something which is not been done previously, but only in very special circumstances.",
                    "label": 0
                },
                {
                    "sent": "So I once spent some time while she was working in Oxford, down with Steve and David and Co. My other job down there was working on time series analysis of financial data, and I spent much of my time looking at these sort of things here.",
                    "label": 0
                },
                {
                    "sent": "So this is also known as an AR-1 model.",
                    "label": 0
                },
                {
                    "sent": "In the time series analysis literature, and this says for any day.",
                    "label": 0
                },
                {
                    "sent": "The price of your stock on day T is just as a distribution which is given by the price.",
                    "label": 0
                },
                {
                    "sent": "Yesterday time something plus some noise OK. And here we got.",
                    "label": 0
                },
                {
                    "sent": "We are quantifying over random variables alright, but we're quantifying the particularly restricted since OK and the relationships between the random variables are just ones of 1 coming after the other.",
                    "label": 0
                },
                {
                    "sent": "So quantifying of random variables has occurred prior to SRL, but only in very restricted senses, and you get similar things in spatial statistics OK, whereas rather than things, just the random variables being.",
                    "label": 0
                },
                {
                    "sent": "Going out in one line, they're going to spread out often in the lattice or grid.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, when you are dealing with stochastic grammars, for example hidden Markov models, which is just sarcastic regular grammars, essentially there are very many random variables involved in such a such a process, which is why they can model infinite infinite processes and similarly dynamic Bayesian networks.",
                    "label": 1
                },
                {
                    "sent": "You know you get this repeated structure with this with the same random variables with the same distribution repeated over and over again.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of where the inspiration for this.",
                    "label": 0
                },
                {
                    "sent": "A number of formalisms, ISL peas in prisons.",
                    "label": 1
                },
                {
                    "sent": "Whether they kind of come from this way of thinking.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's talk a bit about the prism approach, then have time to go through the whole history of the source stuff.",
                    "label": 1
                },
                {
                    "sent": "So in the PRISM approach, the division of Labor is very, very is as follows.",
                    "label": 1
                },
                {
                    "sent": "So we have probability logic connected together.",
                    "label": 0
                },
                {
                    "sent": "And the division of Labor between the two sides is as follows.",
                    "label": 0
                },
                {
                    "sent": "The probability distribution, the basic probability distribution which is defined is extremely simple.",
                    "label": 1
                },
                {
                    "sent": "You just have families of independent identically distributed random variables.",
                    "label": 0
                },
                {
                    "sent": "Alright, the simplest sort of distribution one can imagine.",
                    "label": 0
                },
                {
                    "sent": "You can't do model much with such a distribution.",
                    "label": 0
                },
                {
                    "sent": "So to do something even a little bit interesting, we had the logic in the logic.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, is not very simple.",
                    "label": 0
                },
                {
                    "sent": "We had in arbitrarily complex logic program in order to generate a more interesting and useful probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a total distribution over Herbrand models, so how is this done exact?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the probability.",
                    "label": 0
                },
                {
                    "sent": "So forget about the logic that I'm being.",
                    "label": 0
                },
                {
                    "sent": "Imagine if you will you have a whole family, infinitely many just so we don't run out of random variables.",
                    "label": 0
                },
                {
                    "sent": "An infinite collection of independent and identically distributed random variables, and just for the sake of argument, suppose this one here.",
                    "label": 1
                },
                {
                    "sent": "They all have values.",
                    "label": 0
                },
                {
                    "sent": "Why in-n-out?",
                    "label": 0
                },
                {
                    "sent": "Suppose just for concreteness that the probability that any one of these takes a value wise .3.",
                    "label": 0
                },
                {
                    "sent": "And it's just adding some other another family of random variables.",
                    "label": 0
                },
                {
                    "sent": "So all these guys have the same distribution.",
                    "label": 0
                },
                {
                    "sent": "These are binary.",
                    "label": 0
                },
                {
                    "sent": "All these guys have the same distribution, but they have up to 9 different sorry 10 different values here.",
                    "label": 1
                },
                {
                    "sent": "So we can imagine the joint instantiation of all these infinitely many variables at the beginning of.",
                    "label": 0
                },
                {
                    "sent": "One would look something like this OK. And of course the nice thing about this distribution is extremely easy to compute marginal distributions.",
                    "label": 0
                },
                {
                    "sent": "So if we want to work out what's the probability that I know X 3 = y, it's just going to be .3.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do any do any work whatsoever.",
                    "label": 0
                },
                {
                    "sent": "If you want to workout the probability of other know of this X 1 = y and Y2 equaling one, it's just a simple multiplication, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a very.",
                    "label": 0
                },
                {
                    "sent": "Trivial.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is by the way, how you define Santa trivial probability distribution, prism.",
                    "label": 0
                },
                {
                    "sent": "There is actually slightly more compact syntax one can use, but let's not go there.",
                    "label": 0
                },
                {
                    "sent": "So you just say you know the values of that guy is these and these are probabilities.",
                    "label": 0
                },
                {
                    "sent": "So implicitly when you type this in your defining not just one random variable X, you're defining an infinite collection of random variables X one X2X3 up to XAF knowledgeably.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what the prison source looks like.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's start linking things to logic, so we can imagine if we have a particular instantiation of all these.",
                    "label": 0
                },
                {
                    "sent": "Of of all these variables, we can represent a particular instantiation by proposition in first order logic.",
                    "label": 0
                },
                {
                    "sent": "So we could say if if variable X1 takes a value Y, we're going to write that down like this.",
                    "label": 0
                },
                {
                    "sent": "So MSW here stands for Multivalued Switch an and so on and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just another way of writing that down, but in a way more amenable to linking it to logic.",
                    "label": 0
                },
                {
                    "sent": "So this is a MSW.",
                    "label": 0
                },
                {
                    "sent": "Here was our only predicate.",
                    "label": 0
                },
                {
                    "sent": "Then this would be distribution over possible worlds within, so this will be a particular per brand model and this would have a particular probability and other ones would have different probabilities and there you have it.",
                    "label": 0
                },
                {
                    "sent": "But there's not a very interesting distribution.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to make it more interesting?",
                    "label": 0
                },
                {
                    "sent": "That we can use.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model other things.",
                    "label": 0
                },
                {
                    "sent": "Is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is is fairly straightforward already?",
                    "label": 0
                },
                {
                    "sent": "What we do is we say we have this original distribution and we're going to extend it by adding in a fixed logical theory.",
                    "label": 1
                },
                {
                    "sent": "So if we imagine what's going on over here, we F1.",
                    "label": 0
                },
                {
                    "sent": "Here is some particular instantiation of all those MSW guys.",
                    "label": 0
                },
                {
                    "sent": "F2 is some other instantiation, F3 is yet another instantiation.",
                    "label": 1
                },
                {
                    "sent": "If we have some fixed logical theory are then.",
                    "label": 0
                },
                {
                    "sent": "What follows from F1 and R is different for photos from F2 and are alright, so we get some.",
                    "label": 0
                },
                {
                    "sent": "We got a probability distribution over what follows from our once we've added in in the F1.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how I think of when I think of these things is generative 'cause.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you know you got your are your logical theory and it's waiting to receive it's probabilistic, probabilistically generated ground atoms, MSW guys.",
                    "label": 0
                },
                {
                    "sent": "And once you've got those, you can infer all sorts of other things and what you can infer changes.",
                    "label": 0
                },
                {
                    "sent": "Pending which instantiation you've ended up with.",
                    "label": 0
                },
                {
                    "sent": "So by this approach we can have quite a complicated distribution over here, so it's still a distribution of possible worlds.",
                    "label": 0
                },
                {
                    "sent": "The number of RAM models is presumably bigger because we've got language will be a bit bigger because we've got.",
                    "label": 1
                },
                {
                    "sent": "Predicates here in R, which we haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "So if you have a particular formula Fla, what's the probability that formula being true?",
                    "label": 0
                },
                {
                    "sent": "Basically, you say, well, it's a probability that if I were to sample one of these infinitely big joint instantiations F and added it to my pre existing or I'd be able to infer the formula question OK?",
                    "label": 0
                },
                {
                    "sent": "And there's some stuff which I'm scandalously glossing over to do.",
                    "label": 1
                },
                {
                    "sent": "The closed world assumption to make sure this all kind of fits together very, very well.",
                    "label": 0
                },
                {
                    "sent": "'cause you can imagine you have a formula which doesn't get in 3rd.",
                    "label": 0
                },
                {
                    "sent": "You know neither it nor its negation gets easily inferred, so they wouldn't get probabilities adding up to one.",
                    "label": 1
                },
                {
                    "sent": "So there is a closed world assumption in the whole theory built on top of logic programming.",
                    "label": 0
                },
                {
                    "sent": "In order to get this order to workout.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the basic idea there.",
                    "label": 0
                },
                {
                    "sent": "So we get a distribution of possible worlds, but typically in the prison system we use something called a target predicate to really summarize what's going on in any particular.",
                    "label": 0
                },
                {
                    "sent": "Possible world, so it's convenient to specify what's under the target predicate T. In this case, just it doesn't ask me magic, just my examples magic.",
                    "label": 1
                },
                {
                    "sent": "And we define it in such a way that exactly 1 ground Atom with this predicate symbol is true in any particular possible world.",
                    "label": 1
                },
                {
                    "sent": "So for any particular choice of our instantiation of our base distribution, exactly one of these guys follows alright.",
                    "label": 0
                },
                {
                    "sent": "So if we choose F1 here, we get TF2 gives us TBF, three gives us T again.",
                    "label": 0
                },
                {
                    "sent": "This evidently defines a distribution over from the logic.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar logic programming, you can think of this.",
                    "label": 1
                },
                {
                    "sent": "These various guys here as being members of success, set of T, and that gives you a distribution of these guys, and this can be generalized somewhat to allow in each possible world you might have possible worlds in which none of these T guys follows, and then things get a bit more complicated there, but things get also bit more expressive.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's what's going on there.",
                    "label": 0
                },
                {
                    "sent": "So how do we compute target probabilities from this prison distribution?",
                    "label": 1
                },
                {
                    "sent": "So we saw earlier that computing marginal distributions in first order probabilistic models can potentially be quite tricky because we got so many other variables to some out.",
                    "label": 0
                },
                {
                    "sent": "What happens here with the prism case, whatever that we don't consider all possible infinite instantiations of the base distribution.",
                    "label": 1
                },
                {
                    "sent": "If you're trying to compute the probability of some particular ground Atom here.",
                    "label": 1
                },
                {
                    "sent": "There's a requirement in PRISM that probability of that is a finite sum of finite products, so there is a restriction built in here.",
                    "label": 0
                },
                {
                    "sent": "So what we do, and this is the kind of where it's nicely logical is that we actually use abduction to do on marginalization for us.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we want to compute the probability of that guy going to subduction to compute this probability as follows.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So is an example code for code up.",
                    "label": 0
                },
                {
                    "sent": "Again the Markov model as a prison program, one code up other things obviously, but this is just fine example.",
                    "label": 0
                },
                {
                    "sent": "We've got this ground Atom here, which is basically saying my hidden Markov model is generated the the output JDA.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that?",
                    "label": 0
                },
                {
                    "sent": "Well, what we can say is there's two possible things happened here, either.",
                    "label": 0
                },
                {
                    "sent": "Well, we always start in State Zero.",
                    "label": 0
                },
                {
                    "sent": "Anastasio must spit it out a, which is that guy there.",
                    "label": 0
                },
                {
                    "sent": "And then maybe we stayed in State 0, which is what this is saying.",
                    "label": 0
                },
                {
                    "sent": "And then if we disdain State Zero, the other two guys are spat out starting from the S 0 or maybe we switch to another state state one and then spit out DNA from state one.",
                    "label": 0
                },
                {
                    "sent": "So basically we're saying this is true if these three guys are true or these three guys are true.",
                    "label": 0
                },
                {
                    "sent": "So then we look at well what?",
                    "label": 0
                },
                {
                    "sent": "Case is this guy true and so this is a.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suction so we just say well for this gotta be true as this is broken down like this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I won't go through that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gruesome details.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're doing is anything abduction to workout what things have to be true to make our ground Atom true?",
                    "label": 0
                },
                {
                    "sent": "Once we've done that?",
                    "label": 0
                },
                {
                    "sent": "Computing, probably.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is extremely trivial.",
                    "label": 0
                },
                {
                    "sent": "'cause we got this and we can compare.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilities as follows.",
                    "label": 0
                },
                {
                    "sent": "We say right this is what we care about.",
                    "label": 0
                },
                {
                    "sent": "We want this probability because of our also convenient independence assumptions that we previously made.",
                    "label": 0
                },
                {
                    "sent": "We can just compute it as these guys multiplied together.",
                    "label": 0
                },
                {
                    "sent": "Plus these guys multiplied together.",
                    "label": 0
                },
                {
                    "sent": "You might notice at this point there's a rather convenient assumption that these guys here are exclusive.",
                    "label": 0
                },
                {
                    "sent": "If we generalize the prism approach so that this no longer holds, then things become much more much more complicated, and the work with prob log.",
                    "label": 0
                },
                {
                    "sent": "People working probably gonna love that.",
                    "label": 0
                },
                {
                    "sent": "So basically the way we're computing the probability now.",
                    "label": 0
                },
                {
                    "sent": "This follows we're thinking of probabilities as follows.",
                    "label": 0
                },
                {
                    "sent": "We got a probability space.",
                    "label": 0
                },
                {
                    "sent": "We got something.",
                    "label": 0
                },
                {
                    "sent": "Some events in this space whose probability we want to want to compute, and we're basically saying is.",
                    "label": 0
                },
                {
                    "sent": "Its probability is basically kind of the area of this thing we're going to do is, first of all, say, well, it's this plus this.",
                    "label": 0
                },
                {
                    "sent": "And as I want the property this guy, so that's going to discuss this and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And so we've broken it down into things we can just add up, alright, which is what's going on here?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we carry on the.",
                    "label": 0
                },
                {
                    "sent": "Prob.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's how we would compute the probability of that guy here.",
                    "label": 0
                },
                {
                    "sent": "So basically we the the nice thing here is we don't have to.",
                    "label": 0
                },
                {
                    "sent": "There are infinitely many random variables in our distribution, we don't care about most of them, 'cause we know because of the restrictions of a prism approach that to get this guy here in the first instance, we just need to worry about these six alright.",
                    "label": 0
                },
                {
                    "sent": "And then once we got these six and we have to do it work on this guy, this guy, these ones here, just the probabilities are just given to us.",
                    "label": 0
                },
                {
                    "sent": "So I've always been kind of trying to think to myself.",
                    "label": 0
                },
                {
                    "sent": "You know, this is kind of lifted inference, but in the prism sense, but.",
                    "label": 0
                },
                {
                    "sent": "Because of the particular sections of the prison program, lifted inference kind of reduces a logical inference.",
                    "label": 0
                },
                {
                    "sent": "You know, there's not really.",
                    "label": 0
                },
                {
                    "sent": "There's not much more to do apart from normal, logical, logical stuff.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is quite useful, so in that case, what sort of things we learn from and with our data typically look like for this form doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "Again, we assume that our data comes from some some world, so imagine that our our data generating process is generating possible worlds for us, possibly just one.",
                    "label": 0
                },
                {
                    "sent": "However, we assume that we only see the instance of our target product, which is true in each one of these worlds.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that your data or data generating process generates a world.",
                    "label": 0
                },
                {
                    "sent": "And then says this is the ground Atom of your target product ruin this world.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you that but not show you the rest of the world.",
                    "label": 0
                },
                {
                    "sent": "OK, because of this you got effectively missing data, so you have to use the EM algorithm or or some other way of dealing with missing data.",
                    "label": 0
                },
                {
                    "sent": "Fit the parameters.",
                    "label": 0
                },
                {
                    "sent": "So that's that's how parameter fitting would happen happen in prison.",
                    "label": 0
                },
                {
                    "sent": "How is it for just on time to?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go through application, so that's if you like the end of the purely tutorial part of what I wanted to talk about.",
                    "label": 0
                },
                {
                    "sent": "So as you hopefully would have guessed, from what I what I said, what I was really trying to do is get this tutorial not to give people new results, but to try and draw connections between existing things in the hope that was useful to people, particularly those who are from MLG or possibly also from the LP side of things.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at using in the last 15 minutes or several 10 minutes, hopefully.",
                    "label": 0
                },
                {
                    "sent": "Particular application of SRL, which is what I'm going to do here is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be using inference in a Cisco relational learning formalism, namely Markov logic to do machine learning of Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "OK, so a particular sort of Bayesian network?",
                    "label": 0
                },
                {
                    "sent": "Alright, so Bayesian networks are things called pedigrees.",
                    "label": 0
                },
                {
                    "sent": "So what is a pedigree pedigree is just a family tree.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it can be a family tree relating human beings or other organisms.",
                    "label": 0
                },
                {
                    "sent": "So here we have some family tree, so we got four individuals.",
                    "label": 0
                },
                {
                    "sent": "Suppose we don't know how they are related.",
                    "label": 0
                },
                {
                    "sent": "We could write down their relationships using a Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "There are many ways of writing down these Bayesian networks, not just this way here, so it might be the case that Bob and Diane are the parents of case, or like maybe that indicator that parents are both, you know, so you know.",
                    "label": 0
                },
                {
                    "sent": "If we if we don't know, then we're basically doing a particular form of Bayesian network learning.",
                    "label": 0
                },
                {
                    "sent": "Now, one of the night.",
                    "label": 1
                },
                {
                    "sent": "This is quite a nice problem for Bayesian network there.",
                    "label": 0
                },
                {
                    "sent": "The principle thing is that you only have two parents for child, right?",
                    "label": 0
                },
                {
                    "sent": "Because these are real parents, right?",
                    "label": 0
                },
                {
                    "sent": "In the real world, you only have two parents for each child.",
                    "label": 0
                },
                {
                    "sent": "OK, so if anyone's ever done any work on Bayesian network learning, limiting the number of parents, any child going to have, this makes everything incredibly much easier, so I should mention, by the way, that Alan here in this pedigree.",
                    "label": 0
                },
                {
                    "sent": "Here Adam here doesn't have.",
                    "label": 0
                },
                {
                    "sent": "The parents are not shown.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that Adam doesn't and his parents don't exist.",
                    "label": 0
                },
                {
                    "sent": "It means they're just not mentioned in our in our pedigree.",
                    "label": 0
                },
                {
                    "sent": "OK, so each variable has at most two parents.",
                    "label": 0
                },
                {
                    "sent": "Another interesting aspect of this particular sort of Bayesian network learning is that the parameters are known alright, so if this was the true pedigree and we're going to be dealing with so, we're going to deal with nettik data.",
                    "label": 1
                },
                {
                    "sent": "We're going to be the data we're going to be on genetic information about these particular visuals.",
                    "label": 0
                },
                {
                    "sent": "If we know about the genetic, say, for example, the blood type of Adam and Kate, then we know the probabilities which Bob here will have particular blood types, so they are given to us, so it's none of this marginalizing over unknown parameters.",
                    "label": 0
                },
                {
                    "sent": "Stuff that you do.",
                    "label": 0
                },
                {
                    "sent": "If you, for example use the PD score to to do the basic networking.",
                    "label": 0
                },
                {
                    "sent": "Motivational thing SRL is there are lots of logical constraints.",
                    "label": 1
                },
                {
                    "sent": "Parents have to be different sexes.",
                    "label": 0
                },
                {
                    "sent": "For example, there may be two of them.",
                    "label": 0
                },
                {
                    "sent": "All sorts of things like that, and another thing is that we're dealing with relationships between objects.",
                    "label": 0
                },
                {
                    "sent": "The objects in question are people alright individuals, so we've got relationships in objects.",
                    "label": 0
                },
                {
                    "sent": "We've got uncertainty, and so it makes sense to consider using SRL.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's have a little look at what the data we're going to be learning from, so we're going to be learning from genetic data, OK?",
                    "label": 0
                },
                {
                    "sent": "So a little bit of terminology here.",
                    "label": 0
                },
                {
                    "sent": "Everyone has different, even though all human beings we have genetic variation, which is why some people we have different blood groups, different eye color, different hair color, so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "The variants of genes are known as alleles.",
                    "label": 0
                },
                {
                    "sent": "So my my son has blonde hair.",
                    "label": 0
                },
                {
                    "sent": "I have Brown hair so the hair color gene.",
                    "label": 0
                },
                {
                    "sent": "Before we just more than one gene.",
                    "label": 0
                },
                {
                    "sent": "But let's assume it's one gene differs from in our two cases, so we have different alleles there.",
                    "label": 0
                },
                {
                    "sent": "You get one of your alleles because human beings are diploid.",
                    "label": 0
                },
                {
                    "sent": "We have two chromosomes.",
                    "label": 0
                },
                {
                    "sent": "The way it works is you get one allele from your mother and one of the from your father and this is your genotype.",
                    "label": 0
                },
                {
                    "sent": "This is your genetic material and typically when we were doing this learning from pedigrees, when you see your legal you don't know which gene came from the parent in which gene comes the mother in which he came from the father.",
                    "label": 0
                },
                {
                    "sent": "So we're going to assume that what we see what those unordered genotypes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what I think it is going to look like.",
                    "label": 0
                },
                {
                    "sent": "You don't know the println origin of a particular allele, so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what it's going to be like.",
                    "label": 0
                },
                {
                    "sent": "So what's the problem that we're going to be solving here?",
                    "label": 0
                },
                {
                    "sent": "Given a set of possible Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "We will have a prior over these networks so I should mention here.",
                    "label": 0
                },
                {
                    "sent": "Having a prior is absolutely crucial in this approach because if you just take the genetic data and find the most probable network from the data, you will end up with a network with vast amounts of promiscuity, incest, alright, and because that's what the data supports, the data is all these people look quite similar.",
                    "label": 0
                },
                {
                    "sent": "They probably you know probably painted together, you know, everyone's been having sex with everyone else and you know all this sort of business, so you need to have some sort of prior bias pushing you away from that because we say you know.",
                    "label": 0
                },
                {
                    "sent": "If the data comes from human beings, then you know it can happen.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying it doesn't happen, but it's less likely.",
                    "label": 0
                },
                {
                    "sent": "So you really need some sort of prior bias there.",
                    "label": 0
                },
                {
                    "sent": "So that's that's important.",
                    "label": 0
                },
                {
                    "sent": "The data is what we call marker data, so this is basically these genotypic data.",
                    "label": 0
                },
                {
                    "sent": "So we imagine that we've grabbed these peoples DNA and we found the two alleles, but we don't know which one came from within the father.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to assume various nice things about the way that these illegals get passed down from from your parents.",
                    "label": 0
                },
                {
                    "sent": "So the task is simple, find the most probable pedigree given the observed data, right?",
                    "label": 0
                },
                {
                    "sent": "So it's just Bayesian network optimization task.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a good bayesians always reduce machine learning probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "I'm a good base in at least a committed Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Let's say that, so that's what we're going to be doing here.",
                    "label": 0
                },
                {
                    "sent": "So it's just going to be probabilistic inference in another Virgin network.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So we have to represent everything with variables to construct our probability distribution in which to do inference.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have variables which represent the pedigree will see the minute variables represent the UN observed, or the Geno types.",
                    "label": 0
                },
                {
                    "sent": "This is going to be hidden data.",
                    "label": 0
                },
                {
                    "sent": "This is the data which tells us where.",
                    "label": 0
                },
                {
                    "sent": "The different genes came from Mother versus mother father, but we don't see that guy.",
                    "label": 0
                },
                {
                    "sent": "This is the the order genotypes, but we might not for some individuals who might not even have this that might be missing as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is Alexis.",
                    "label": 0
                },
                {
                    "sent": "We have the observed stuff, the stuff the people for whom we've seen this and we might have people who we haven't seen this.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to moxie variables just to make a little bit harder, which are which I'll come to later.",
                    "label": 0
                },
                {
                    "sent": "Once we've done that, we'll put all these together.",
                    "label": 0
                },
                {
                    "sent": "We will construct an exponential family distribution using Markov logic, which will be a.",
                    "label": 0
                },
                {
                    "sent": "Distribution of a joint distribution of all these guys and we're just going to maximize it to find the instantiation which maximizes the G here.",
                    "label": 0
                },
                {
                    "sent": "So it's basically.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How it's going to work?",
                    "label": 0
                },
                {
                    "sent": "Just OK, so this let's just go through it.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing is of course that you we can represent things in logic.",
                    "label": 0
                },
                {
                    "sent": "So far the Palace.",
                    "label": 0
                },
                {
                    "sent": "This is one of these ground Atom random variables.",
                    "label": 0
                },
                {
                    "sent": "This is true if in the true pedigree, Bob indeed is the fall of Alice.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is what pedigree variables represent.",
                    "label": 0
                },
                {
                    "sent": "The pedigree they're going farther and other things also kind of auxiliary variables saying who's older than who.",
                    "label": 0
                },
                {
                    "sent": "We might even know this ahead of time.",
                    "label": 0
                },
                {
                    "sent": "We might know we might have age information about individuals.",
                    "label": 0
                },
                {
                    "sent": "So we have lots of logical constraints which we can represent with are nice because we're using Markov logic.",
                    "label": 0
                },
                {
                    "sent": "We can have hard constraints or soft ones there.",
                    "label": 0
                },
                {
                    "sent": "Things saying that fixes the father of widen accidentally older, older is transitive.",
                    "label": 0
                },
                {
                    "sent": "You only have one father.",
                    "label": 0
                },
                {
                    "sent": "All that sort of stuff we throw that sort of thing in there.",
                    "label": 0
                },
                {
                    "sent": "OK, if it's all very the nice thing here is everything is very kind of obvious how you do it, you don't have to kind of.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of some horrible encoding.",
                    "label": 0
                },
                {
                    "sent": "Now we have the information about the things we don't actually observe, so this is saying Bob, his paternal Ellie.",
                    "label": 0
                },
                {
                    "sent": "Although when he got his father was illegal A2 maternal allele for Alice, the one she got from her mother, was a four and we can write down all sorts of constraints about this, basically saying that.",
                    "label": 0
                },
                {
                    "sent": "You only get one paternal.",
                    "label": 0
                },
                {
                    "sent": "You only get one allele passed down from your mother.",
                    "label": 0
                },
                {
                    "sent": "Will just dealing with one particular locus.",
                    "label": 0
                },
                {
                    "sent": "Here you always have at least something passed down from your father and here we have some things about homozygous inheritance.",
                    "label": 0
                },
                {
                    "sent": "So if your if your father effects is the father of Y&X is alleles are both the same, the ones that X got from his father and mother, both A and that's the only one that can possibly passed down to his children, because that's the only one available, right?",
                    "label": 0
                },
                {
                    "sent": "So you get a nice logical.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraint there.",
                    "label": 0
                },
                {
                    "sent": "And then we have other things saying.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you observe a genotype AMB two possibilities they came for father be from the mother or a kind of a mother father.",
                    "label": 0
                },
                {
                    "sent": "That's the only two possible options for you there.",
                    "label": 0
                },
                {
                    "sent": "So you write that down as a hard claws.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are all sorts of logical constraints, and so you know this is what a possible world might look like.",
                    "label": 0
                },
                {
                    "sent": "So we just list all the true ones.",
                    "label": 0
                },
                {
                    "sent": "So possible world, not the case where these the relationships, these are the maternal paternal alleles and this is the observed observed stuff.",
                    "label": 0
                },
                {
                    "sent": "So that's what possible world.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look like.",
                    "label": 0
                },
                {
                    "sent": "So now we bring in the probabilities.",
                    "label": 0
                },
                {
                    "sent": "So basically you'll just have to trust me, these are correct probabilities.",
                    "label": 0
                },
                {
                    "sent": "Basically, each time you someone passes down a gene and they had two possible things that could have passed down, a probability of half kind of kicks into the computation.",
                    "label": 0
                },
                {
                    "sent": "So we add awaited clause like this.",
                    "label": 0
                },
                {
                    "sent": "This is just representing the fact that if you particularly visual heterozygote has two different genes, there's probably half the child to get one, probably half will get the other.",
                    "label": 0
                },
                {
                    "sent": "That's essentially what this this minus log half is coming from.",
                    "label": 0
                },
                {
                    "sent": "So just trust me that this is this is the correct way of.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing that also we have people who don't have parents, so we just have to yank out probability distributions for these illegals from the general population.",
                    "label": 0
                },
                {
                    "sent": "So these these numbers here would depend on what's going in the population.",
                    "label": 0
                },
                {
                    "sent": "This is basically saying if someone doesn't have a father, at least we don't know about their father.",
                    "label": 0
                },
                {
                    "sent": "Father is not represented in the probability if any particular deal is given by some, some given probability in the population.",
                    "label": 0
                },
                {
                    "sent": "So that's that's all that that works.",
                    "label": 0
                },
                {
                    "sent": "Encoder population frequency.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also have a prize on pedigrees.",
                    "label": 0
                },
                {
                    "sent": "This is basically saying here what is this saying here for any XY and Z defects?",
                    "label": 0
                },
                {
                    "sent": "Is the mother of?",
                    "label": 0
                },
                {
                    "sent": "Why or why is the final result?",
                    "label": 0
                },
                {
                    "sent": "We would prefer not to be the case that X is also the mother of Zed, so we will rule out insects and preferred.",
                    "label": 0
                },
                {
                    "sent": "We don't like incest much and we don't like mothers and fathers to be related.",
                    "label": 0
                },
                {
                    "sent": "I mean we could have all sorts of logical things in here.",
                    "label": 0
                },
                {
                    "sent": "I should mention that this putting price on pedigrees isn't new to me.",
                    "label": 0
                },
                {
                    "sent": "Have been worked by other people.",
                    "label": 0
                },
                {
                    "sent": "Similar price, this were not using SRL.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we incorporate evidence?",
                    "label": 0
                },
                {
                    "sent": "Well, we just states ground atoms with true.",
                    "label": 0
                },
                {
                    "sent": "We might know that John is a father.",
                    "label": 0
                },
                {
                    "sent": "Robin, we might observe some genotype data, so this rules out all the possible worlds in which the evidence is not true.",
                    "label": 0
                },
                {
                    "sent": "Just like multiplying in a 01 factor in particular sort.",
                    "label": 0
                },
                {
                    "sent": "Of course, intelligent approaches to propagate this evidence to kind of knockout things which you know.",
                    "label": 0
                },
                {
                    "sent": "However, this makes irrelevant.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's just do a very simple example, 'cause sometimes pretty much out now I think suppose we started with this situation here.",
                    "label": 0
                },
                {
                    "sent": "Here's our data.",
                    "label": 0
                },
                {
                    "sent": "We got this observed genotype stuff and suppose we somehow know that female number one is the mother of female #3 suppose we somehow knew that we don't know about all the other people that we know that so.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I actually did in this particular case because it was a small example, I want to make a point here.",
                    "label": 0
                },
                {
                    "sent": "I did actually ground it out into the propositional clauses and user propositional solver.",
                    "label": 0
                },
                {
                    "sent": "At this point you say, well, why did you use SRL?",
                    "label": 0
                },
                {
                    "sent": "You could have done a ground thing right from the beginning.",
                    "label": 0
                },
                {
                    "sent": "I would disagree to that.",
                    "label": 0
                },
                {
                    "sent": "Be able to write it down in the 1st order formalism, even if at some later stage some program grounds it out for you.",
                    "label": 0
                },
                {
                    "sent": "It's very important to be able to opportunity to write things down nicely from a usability point of view.",
                    "label": 0
                },
                {
                    "sent": "So actually fed this into an exact weighted Max SAT solver.",
                    "label": 0
                },
                {
                    "sent": "So this is definitely the most probable world tour.",
                    "label": 0
                },
                {
                    "sent": "I didn't use Max Walk sat together, approximate one and in about how long it takes 30 seconds.",
                    "label": 0
                },
                {
                    "sent": "It generated this particular most probable world which has information about the pedigree.",
                    "label": 0
                },
                {
                    "sent": "So I'm gonna have to.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just finish up very soon, so if I remove the remembering the previous slide I had.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In about a.",
                    "label": 0
                },
                {
                    "sent": "A little bit of information about this relationship here.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you remove that other thing, taking 30 seconds to find the most problems sensation at times shoots up to 145.",
                    "label": 0
                },
                {
                    "sent": "So the key observation here is a small amount of evidence makes a big difference.",
                    "label": 0
                },
                {
                    "sent": "It makes the whole optimization problem much easier if you add in a total order.",
                    "label": 0
                },
                {
                    "sent": "So if you know if you actually have the ages of all these people, which obviously restricts it an awful lot, you can solve this thing in less than a 10th of a second.",
                    "label": 0
                },
                {
                    "sent": "In this particular small case.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that makes a massive massive difference.",
                    "label": 0
                },
                {
                    "sent": "As always with Bayesian network learning.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm I'm pretty much.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I'm.",
                    "label": 0
                },
                {
                    "sent": "I should mention that what I'm actually doing here isn't actually solving the problem like I meant to set out to do was because I wanted to represent this hidden data explicitly in my representation, but I'm actually I'm actually doing is.",
                    "label": 0
                },
                {
                    "sent": "I'm finding the most probable pedigree together with the most proper instantiation of the latent data, so I'm kind of cheating a little bit there.",
                    "label": 0
                },
                {
                    "sent": "One can actually rearrange the conditional probabilities to do the whole thing properly.",
                    "label": 0
                },
                {
                    "sent": "Basically marginalizing out this Y in the values I put in for the conditional probabilities, but I didn't actually do that.",
                    "label": 0
                },
                {
                    "sent": "It would be nice be nicer, directly have this sort of this sort of thing here, where you can actually do maximization and marginalization and some other things.",
                    "label": 0
                },
                {
                    "sent": "I know federal is working on this sort of thing in Washington right now, OK, and so that's the end of my little example, so I hope you found interesting.",
                    "label": 0
                },
                {
                    "sent": "So it's quite nice to be able to use rather than learning SRL formalism using Excel formulas and to learn something entirely different.",
                    "label": 0
                },
                {
                    "sent": "Scenes seem quite a way of doing things.",
                    "label": 0
                },
                {
                    "sent": "I should say that he.",
                    "label": 0
                },
                {
                    "sent": "You know, I think statistical genetics in general is a big possible growth area for SRL.",
                    "label": 0
                },
                {
                    "sent": "I know some people have done work on haplotypes with using SRL techniques elsewhere, so I don't think I'm the only one who thinks that, so I'll call it a day there and I'll if it's time for questions.",
                    "label": 0
                },
                {
                    "sent": "Feel free to ask anything, thank you.",
                    "label": 0
                },
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "Exciting.",
                    "label": 0
                },
                {
                    "sent": "I think we have time for three questions.",
                    "label": 0
                },
                {
                    "sent": "So in your example service this small example and it didn't work constraints.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good things.",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "Well, if I if it was a bigger example, so remember how I did this particular one.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, second.",
                    "label": 0
                },
                {
                    "sent": "I wasn't moving.",
                    "label": 0
                },
                {
                    "sent": "So two things about this particular example one.",
                    "label": 0
                },
                {
                    "sent": "I grounded the whole thing out.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work on avoiding grounding the whole thing out.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know, that's one thing to say, so you know if I was using a bigger example, I wouldn't necessarily transfer the whole thing to a propositional representation.",
                    "label": 0
                },
                {
                    "sent": "Secondly, I was using exact solver alright, so this is guaranteed to be the absolute most probable pedigree.",
                    "label": 0
                },
                {
                    "sent": "If you use something like Mac Hawks at you could get.",
                    "label": 0
                },
                {
                    "sent": "I did some experiments actually where I used in exact solvers.",
                    "label": 0
                }
            ]
        }
    }
}