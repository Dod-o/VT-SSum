{
    "id": "4zr5hyqrb2i52olkkhvyyrpsiye2tybe",
    "title": "MPI & OpenMP (Part 5)",
    "info": {
        "author": [
            "David Henty, EPCC, University of Edinburgh"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_henty_MPI_openMP_part5/",
    "segmentation": [
        [
            "OK, so the idea is.",
            "So last year Jaune ran this in Mark I I'm doing it this year, so who knows how will go.",
            "But this is the second challenge.",
            "I will.",
            "This trophy bears no relationship to reality, but there may be some prize but it would be nice if people had a go at this.",
            "As I said, we've not done this on bridges before so even very simple techniques or at the box might be fast.",
            "I don't know."
        ],
        [
            "So the general rules are all that due Thursday night, if that's when the event is, but I don't know what that means is.",
            "I'll run the codes and and get the performance on Friday morning.",
            "That's what it really means, but I don't expect you want it.",
            "You should use for.",
            "You can use up to four nodes of bridges and you can use any combination of MPI Open ACC Python.",
            "I think you'll be struggling to get performance, but an open MP, so I expect you will want to use a hybrid MPI and open MP.",
            "How fast can you run a 10K by 10K?",
            "That's 10,000 by 10,000.",
            "Ashford Convergence, so we I'll give you.",
            "A brief overview of what that is.",
            "Message."
        ],
        [
            "Just some specifics.",
            "You can't change.",
            "The kernels must retain 2 core loops in the source.",
            "You can't change the number, you can change them by processes, I mean 112 if you're on four nodes, 112 would be a pure MPI job 4 * 28.",
            "4 might be a appear.",
            "One MPI process on each node and then the other cores being used with open MP.",
            "But you can use anything just one source file for simplicity and would be nice if you if you're doing things like setting environment variables, just give a single script so I just do.",
            "Run script or something so it will set, compile and submit.",
            "I mean anything reasonable but you try and make it as easy as possible to run your solutions and that's my email address d.hentaiepcc.ed.ac.uk by the deadline.",
            "If there's any changes on email you, but that's the specifics."
        ],
        [
            "The rules, no libraries don't match without the timer is just.",
            "I mean really, it's a matter of you know, doing reasonable things.",
            "The obvious things."
        ],
        [
            "You might want to do so.",
            "The Serocco converges at three 578 time steps.",
            "Yours should too, so the code is just iterating forward and there's a tolerance.",
            "And when it reaches that tolerance it stops and that should be 3578.",
            "Jaunes pointed out this isn't actually enough to verify correctness becausw.",
            "You can have a slightly incorrect code which still gives the right still converges you should find.",
            "This point seven, 500, nine, 950 or 9 nine 57504 times the C in the Fortran examples of transposed simply because of the C and fortran's disagreement about what order array indices are in about 17 plus or minus one.",
            "So as as distributed, the code is 1000 by 1000 example just for development, but you should run 10,010 thousand.",
            "This is a really really really dumb.",
            "Algorithm for solving this this equation you could make it 100 times faster 10 times fast by using Gauss Seidel off, but that's that's not the point.",
            "OK, the point is to make this algorithm go fast using parallelism.",
            "Yes, you could use more sophisticated algorithm, which would be much quicker.",
            "You could use Fourier transforms or something, but that's not we're interested interested in because what you do to speed up this simple code could equally well speed up the guy Seidel or the more if you don't understand this stuff it doesn't matter, but you know it's not a big deal."
        ],
        [
            "Things to explore.",
            "You can.",
            "I don't know for this, but you can do compiler flags.",
            "You'll have to look at the manpage for that.",
            "You can select different compilers.",
            "By default, the MPI compiler calls the new compilers, but the bridge is.",
            "Documentation will tell you how to use different modules so you can change the MPI compiler to use the Intel compiler if you want, and the documentation explains how to do that.",
            "There may be environment variables you can change this thread if you're using open MP, thread placement can be important.",
            "That this is mentioned in the.",
            "This is mentioned in the document in the exercise KMP.",
            "Affinity is a way of of.",
            "Trying to get the operating system to schedule your threads more predictably, and that's one of the exercises on the sheet.",
            "So I hope that's right.",
            "Only if using Intel, so I should set OK so so so my open MP examples that I've given out in the course are set up to use the Intel compiler so you can use this if you're going to do it in the.",
            "In this example, you'll need to change the Intel compiler.",
            "That's probably a good thing to do actually I don't know is it module load is just module.",
            "Maybe I should probably email around how to do that actually.",
            "I don't think it is if you look.",
            "I I just got the default here on bridges.",
            "And if I do MPI CC minus minus version.",
            "I get it's canoe.",
            "OK.",
            "If you yeah, there's nothing wrong with the new compilers, but it's just that they they.",
            "They don't have such control.",
            "Environment variable yeah.",
            "Yeah, I mean you'd expect the Intel compilers ought to give slightly better performance on an Intel chip, but who knows.",
            "So I have so I forgot to update this from last year.",
            "Blue Waters User Guide is your friend.",
            "Apologies, that's that's out of date.",
            "So we're using we're using bridges, but the bridges documentation is easy to find."
        ],
        [
            "Um?",
            "I'll take the top self reported speeds and run them and I'll do that on Friday and just a few rules.",
            "But I mean really, we're just looking for what you can do, so that's the rules.",
            "There is quite a long presentation here which which is on the wiki.",
            "Going through the details, but I will go through very quickly because I really want to point out.",
            "The interesting stuff.",
            "This rather lot here, but basically."
        ],
        [
            "The plan is very."
        ],
        [
            "Standard exercise, it's it's just that we're solving.",
            "Grad squared F = 0.",
            "Interpret that as being anything you want, but you can think of this being as as heat flow, so the simulation has got a metal plate which is being heated round the edges and you're you're finding out what the heat flow is.",
            "But in fact."
        ],
        [
            "The Laplace equation states that each gridpoint the average of its four neighbours, so that if you discretize a 2D.",
            "Grad Squared, 22D second derivative.",
            "All you do in this in this exercise is you loop over every point in the grid and replace each point by the average rates for neighbors and then you do that again and again, and again, and again, and remarkably, that converges to the correct solution.",
            "You just each iteration replace AIJ iteration K plus one with a I -- 1 I plus one I J -- 1 idea plus one.",
            "So this is exactly the same structure as the traffic model.",
            "The state of each cell at the next situation to pay depends on the state of.",
            "It's for the well itself and its nearest neighbors at this iteration.",
            "And so basically you stop when things when things stop changing you think well, if things aren't changing, I must be the right answer.",
            "So it computes the Max.",
            "It it looks at the maximum change, but it's really quite.",
            "Quite straightforward.",
            "Um?"
        ],
        [
            "The serial code is very simple.",
            "The temperature is 1/4 of the temperature, so this is just like new Road is a function of old road.",
            "I + 1 -- 1 J plus four J minus one in Fortran and Johns put the loops the right way round, I = J equals or Ji in Fortran, just just to be."
        ],
        [
            "And this is the.",
            "This is the source code.",
            "We loop while the changes while the change is not too big.",
            "So well, the changes is too big.",
            "We update the new temperature.",
            "So this is this is new road equals a function of old road.",
            "We then check if we have converged.",
            "We look at the difference between the new and the old arrays and we take the maximum across all the arrays and then we do a copy back and we do it in the same loop.",
            "And then we.",
            "So this is just you know.",
            "Update Rd, copy back and compute the velocity that the compute the number of moves we."
        ],
        [
            "Update.",
            "We also we sweep across and we look for the maximum temperature change, but we also do the copy back at the same time and then we do some output every now and again."
        ],
        [
            "Aw man, there's some fun."
        ],
        [
            "Is there?"
        ],
        [
            "So the way that the code is written in, so I'm giving you a working.",
            "It's John's gonna give you the working MPI code.",
            "This is working open MP code there as well, but the MPI code.",
            "Doesn't do a 2D decomposition of the domain, what it?"
        ],
        [
            "Cause is."
        ],
        [
            "It does a slice wise decomposition.",
            "And so this is much simpler.",
            "Again, like the traffic model, you just have two neighbors up and down.",
            "You have to swap data up and down.",
            "The only difference is in the traffic model, the boundary date was a single element to each edge, and here the boundary data is a whole slice, so it's what data point here.",
            "You need values from your four neighbors.",
            "So in the interior that's fine.",
            "But at the edges you need to value from from from from from your nearest neighbor, just like in the traffic model.",
            "And so you swap Halos.",
            "But the Halos are complete.",
            "Labs of data.",
            "Rather than a single element, but the communication partners the same send up send down, receive up, received an, except there's no periodic boundary conditions here.",
            "This hard boundary conditions the top and bottom 'cause this is a plate, but that's that's that's really a technicality."
        ],
        [
            "So I mean, it turns out in C It's much better to decompose the array in the in the first dimension.",
            "And in Fortran, it's much."
        ],
        [
            "Better to decompose the array."
        ],
        [
            "In the second dimension, the reason is you can go around."
        ],
        [
            "Going through this that basically.",
            "If you have a CRA.",
            "Which is say 8 by 12 by draw it like a matrix in C, then it is it is the Rose which are contiguous in memory.",
            "So the problem in MPI the fundamental way of sending a message you send this block of of data and that's fine.",
            "If you want to send that data that data that data that data, you just take the starting point in the address of three.",
            "One which is there and you say I want said four integers to reach these four.",
            "The problem in see if you want to send that one that one that one that one there not next to each other in memory there strided and there are way.",
            "From doing that, but it's a bit of a hassle, so just means naturally the way that it's drawn here in.",
            "See it's not more natural to send columns, but Fortran is transposed with respect to seasons.",
            "Fortunate it's more natural to send Rose, and so in Fortran it's more natural to divide the column up.",
            "To have whole columns on a processor and see it's more natural to have home rose and that just means that one is decomposed in slices and one is strips and weather.",
            "And yeah, all depends on how you draw it."
        ],
        [
            "So basically sending multiple elements is you can just send a whole slice.",
            "So and the code the code is there, the code is there, it works.",
            "So."
        ],
        [
            "So again, in Fortran you declare, you slice it up into into vertical strips."
        ],
        [
            "And again, it's just the same, so I think the codes fairly codes fairly."
        ],
        [
            "Is the word fairly explanatory?",
            "So there is a working code, the band."
        ],
        [
            "Conditions are set up for you, so don't bother about that.",
            "They're done in parallel, so that's all OK."
        ],
        [
            "So actually I've given you Laplace MPI dot seen Laplacian PDF 90.",
            "I don't actually I would just start from these.",
            "You know these are working quite naive MPI codes which will run.",
            "But you might want to improve them and there's a number of ways you can improve them.",
            "You might want to improve the Halo swapping 'cause it just use MPI send.",
            "MPI Recv doesn't use nonblocking or send receive.",
            "Or you could you could introduce open MP and say I will and I'll talk about how to that next edge, how to combine MPI and open MP."
        ],
        [
            "So I won't go through this."
        ],
        [
            "There."
        ],
        [
            "You can do stuff with not going to do this.",
            "I wouldn't, I think.",
            "This stuff I wouldn't bother with.",
            "I have to say I think I would just maybe play around again, look at what the suggestions were for the traffic model and see if they make any difference here."
        ],
        [
            "OK, but I've given you.",
            "This is really instructions for if you don't have a working NPI cover and giving you a working MPI codes as John said when he ran this previously people had a lot longer to work on it, so I want to give you a bit more head start."
        ],
        [
            "How do you know you're correct?",
            "The solution converges at three 372 time steps on."
        ],
        [
            "Fortunately, that."
        ],
        [
            "Enough becausw.",
            "All the action is in is in the bottom right hand corner, and even if you disable the MPI routines you see you get these artifacts because you're not communicating between top and bottom.",
            "We actually get the right answer down here.",
            "It's just a slight something like work."
        ],
        [
            "And so you should actually."
        ],
        [
            "Print out the."
        ],
        [
            "The actual value.",
            "So.",
            "Yeah, I wouldn't bother doing this."
        ],
        [
            "But John was playing around so that Johns picked the point here.",
            "This magic point that he that he that he gave the coordinates of."
        ],
        [
            "So as I said, some of this stuff is slightly out of date.",
            "That's for last year, but you should have all that, but I would work on there's Laplace MPI dot C or Laplace MPI dot F-90 and then that will run out the box and then the idea is to try and improve it.",
            "So that was really the and all the stuff is up there.",
            "And what?",
            "Alright.",
            "Is that the new Intel compilers they decided to make the default?",
            "We also allowed to go to do so PICC an NPC XX by default goes OK. Oh minus minus.",
            "I.",
            "The PC.",
            "Is MPI ICC for the season?",
            "What is MPI Fort?",
            "Is it?",
            "For.",
            "Documentation has them all listed there, so I think if we just look at the if I do the makefile.",
            "MPI ICC, I guess.",
            "Yep, that's right.",
            "And if I do.",
            "MPI four yeah.",
            "Yeah, that seems to work.",
            "Yeah, so OK, so it's NPC at MPI Fort.",
            "Are your friends here?",
            "So I didn't want to eat too, but is that?",
            "Is it sort of Clearwater we please ask me any questions around?",
            "I'm around for the rest of the thing, but I think good front up.",
            "As I said, I've really no idea how much effect I mean.",
            "I would recommend replacing send and receive with send receive or I send I receive and see if that makes any difference.",
            "And then.",
            "There may be some.",
            "There may be some MPI settings if you look at the man pages, but I think the next thing to do would try hybrid MPI Open MP.",
            "Now that may be challenging, but it's definitely.",
            "It will be definitely fun to do so.",
            "I'm going to talk about that now.",
            "I'm going to talk about.",
            "Two things.",
            "And I do some further topic, I'll just do."
        ],
        [
            "Some selected.",
            "I'll just do some selected further topics and then I'll do the hybrid.",
            "So this is really a few bits and pieces that you might be interested."
        ],
        [
            "Then nested parallelism.",
            "I thought put up previous.",
            "I mean open MP was actually one like MPI actually open MP was a perfect example of how things should work.",
            "Shared memory of computers were invented.",
            "Lots of people came up with ideas for ways of doing directive based shared memory parallelism.",
            "And then after three or four years, people got really annoyed that everyone was different.",
            "They all got together and came up with the unified standard, which incorporates all the best ideas from everything which was the same as MPI but most previous directory systems, nested parallelism, so I didn't have parallelism enabled.",
            "So OMP nested environment variable.",
            "Or you can.",
            "I would export OMP nested equals.",
            "1.",
            "So if a parallel directors account with another product review team with Red will be created.",
            "However, by default if you have OMP nested equals false and you do MP parallel OMP parallel, the first one will use up all the threads in the Second World.",
            "Effectively be ignored for the code will run, but not, you know.",
            "But in a kind of default way.",
            "So I didn't talk about sections because that's."
        ],
        [
            "Really, that interest."
        ],
        [
            "Doing so."
        ],
        [
            "But but but it's really.",
            "This is the useful thing.",
            "This is a classic place where.",
            "I mean, you could do collapse, but you're saying look?",
            "I want to paralyze both loops, but there's only four here, so I'll do parallel do NUM threads for that uses 4 threads here and then I have to.",
            "It's a bit ugly 'cause you have to specify the number of threads down here, but actually this this NUM threads cause supersedes the value that we set by with OMP NUM threads.",
            "Actually.",
            "So this overrides RMP NUM threads.",
            "You need to be careful to get this right.",
            "So it's all a bit, it's just a bit horrible because you have to say I use 4 here now.",
            "Used his toaster threads over.",
            "We have to be very explicit so it's a bit.",
            "It's a bit messy, but you can do it.",
            "I."
        ],
        [
            "Orphan directives are very useful are one of the exercises not explain what they are so basically.",
            "This so this OK is allowed in open MP, so this is you call a subroutine Fred here and you have do I want to end.",
            "And you do MP do so.",
            "MP DO says split this loop up amongst the available threads, but as it stands K it looks like this that this isn't in the parallel region.",
            "OK, but the point is parallel regions persist, so if you have OMP parallel calls, Freden parallel Fred is called multiple times to take four, and the runtime remembers that you're in a parallel region.",
            "So if this is called from a serial region, this just works in cereal.",
            "If this is called from a parallel region, it works in parallel.",
            "So, but it's important if I do the parallelization.",
            "If this was an MP parallel do down here I would, and this wasn't here.",
            "If the parallel directors down here, I would call one, I would call one copy of Fred and then that would split the loop into four threads.",
            "Here I'm calling 4 individual, making four calls to the function thread.",
            "All of Fred, all of which are are independent.",
            "But this OMP do causes them to split up the loop amongst each other, so it is actually a different model.",
            "But the reason why this model is actually.",
            "Um?",
            "While this model is surprisingly good, is that.",
            "If you have lots and lots of private variables, you sometimes do up here you have to earn P parallel private and you have to about 50 private variables and it's really annoying.",
            "But if you declare variables locally within a function, they declared private to the funk you called the function four times.",
            "Each function is saying integer J integer K there by default private.",
            "There they are created on the stack.",
            "If that's the way you like to think about things so by by using orphan directors pushing your count.",
            "Computations down into functions or subroutines and declaring the local variables locally in the subroutine or function your data scoping.",
            "The code can look a lot more elegant.",
            "The data scoping just actually works out, so you almost never have to say private because you just make sure you got lots of private variables.",
            "You push that function into a subroutine, you put operations a subroutine and declare them locally here and there, but by default private there are some rules about.",
            "It can be complicated.",
            "This up scrolls, but datascope attributes and what happens to the data scoping of the?",
            "Arguments of the function and it gets a bit complicated, but.",
            "It kind of these complicated rules, but basically it kind of works out that the real problem comes with.",
            "With C Becausw in C. Um?",
            "If you've got a shared variable in Fortran and you pass it to a function.",
            "Then it's still it's still shared.",
            "It's hard to explain 'cause it 'cause Fortran passes by reference always passes by reference.",
            "But but see passes by value so.",
            "You you can't pass the shared variable through through a function call.",
            "You might think.",
            "Well, I didn't take the address of it, but then each function gets its own.",
            "Private copy of the same.",
            "Pointer, which isn't the same thing.",
            "It's a bit weird, and if anyone so in C doing the update Rd function in with nested with orphan directives is quite challenging where it does your head in a bit, because because of the data scrubbing in Fortran, it's easy in C because C always passes by value.",
            "Its approach light problem, but anyway, but you should nest in power.",
            "Programming using offered directives is a really good way to make sure you understand open MP 'cause it's quite.",
            "It's quite I find it quite and slightly challenging Luckily.",
            "In the same corridor is me is is Mark Bull, who was involved in writing the standard, so I just go and ask him, but that's the times I've gone had to speak to him most often when I'm doing often directives, 'cause it it's more complex."
        ],
        [
            "You might think."
        ],
        [
            "Um?"
        ],
        [
            "Thread private global variables.",
            "I'm not going to talk about this, but.",
            "There's a real problem that sometimes people like to stick stick data in global global scope, so that's that's module data or common blocks in Fortran or in C, It's stuff which is declared outside of a function.",
            "So extern type variables.",
            "It's a disgusting programming style you should never do it, but there's a problem whereby you might want each thread to have its own copy of this global data if they're using it for scratch storage.",
            "If you're just using it as, you might want each thread have its own copy, and that's called thread private global variables.",
            "It is, but basically you should not have global data in your in your program at all.",
            "To disgusting programming style."
        ],
        [
            "And you just but anyway, but maybe you're programming somebody else.",
            "I'm sure you don't member.",
            "You may be having to paralyze somebody elses code who uses this thing so, but you know, global variables are just.",
            "Just horrible so they."
        ],
        [
            "Syntax is equally horrible, so again you need.",
            "This is the only time I've read the Open MP standard in any in any detail.",
            "Was trying to paralyze a code where they had where they had global variables in global storage and it's just horrible."
        ],
        [
            "Copying is another thing I would cover that."
        ],
        [
            "Timing routines open MP has.",
            "Nice timing routine called OMP get W time so you can get the wall Clock time if you want to time something there's an equivalent routine MPI called MPI W Time.",
            "It just allows you to time functions really easily."
        ],
        [
            "I think I should use it.",
            "I think I use it in the traffic model.",
            "So.",
            "Yeah.",
            "So you just do this start time, get time, and then you time you subtract it so they're quite quite useful.",
            "So that was just a run through.",
            "This is other stuff you want.",
            "Might want to know exists and therefore.",
            "And therefore I want to look at.",
            "So I think the other thing which I wanted to just very briefly cover for going on to hybrid programming was.",
            "I'm not going to do it in any detail, but in fact I don't actually have.",
            "I'll need to put it up on the.",
            "Up on the.",
            "So apologies for sale.",
            "Need to put this up on the wiki, but I thought it would be useful just to briefly mention how.",
            "OK.",
            "I just have to find this.",
            "I'll put this up on the."
        ],
        [
            "You know everything I've talked about so far.",
            "As has been about loops, you have a big loop and you want to flatten out for a large class of scientific and technical codes that that's that's useful, but increasingly people looking at task parallelism, saying you know I'm actually defining my problem in terms of a set of independent, not set of tasks which may have dependencies between them, and I want them to be executed and so.",
            "Open MP was extended to include tasks it it can be quite useful.",
            "Until recently, a lot of the implementations weren't particularly efficient, but the way it works.",
            "My battery is optional.",
            "It."
        ],
        [
            "It's surprisingly so that the task.",
            "Concept defines a section of code.",
            "And inside the parallel Rethreading County attachment packet, the task for execution and some thread in their original issue that at some point in the future.",
            "So the point about loop parallelism as we have one to one mapping between work that we want to do and threads you say.",
            "Here's a loop.",
            "I want iterations 5 to 7 to be done by that thread 8 to 10 to be done by that thread, OK.",
            "Task."
        ],
        [
            "Um?",
            "Get rid of that to.",
            "Basically, you can generate as many tasks as you want and they will be executed in parallel by the threads, but you don't know what the mapping is.",
            "A much more generic model, so the model of tasks that you deliberately generate more tasks than there are threads to allow there to be load balancing and a clever runtime system will run the tasks in a way to balance the load.",
            "And the syntax is really quite straightforward.",
            "You just have a piece of code and you say OMP task.",
            "And that's a task.",
            "And basically it's magic.",
            "It's executed so for example.",
            "So this is my battery is not."
        ],
        [
            "You could add.",
            "The scoping can be a bit difficult, but for example, you might have a task which computes some things and what this does is this doesn't.",
            "This doesn't execute the task.",
            "This creates the task which is dispatched to the runtime system and the task is executed.",
            "Sometime in the future, and it is guaranteed to be completed by the next synchronization point by the next barrier or end of a parallel.",
            "Reads are typically what you do is you start the parallel region, create lots of tasks, end the parallel region and then at that point they would all be processed so they're guaranteed to be computed to be to be completed sometime in the future, but they're not, they're not.",
            "They're not done immediately.",
            "The OMP task construct does not execute the task, it creates the task and submits it for execution.",
            "I actually think it's more the shadowing from this, actually, 'cause I've stupidly put it on this side, I think rather bike and try.",
            "Actually that might be useful.",
            "Or these are the wrong ones OK?"
        ],
        [
            "Um?",
            "So, um.",
            "When a task completed with that thread barriers explicit or implicit or so, that's the normal way.",
            "Or you can you can do a task, wait, you can do.",
            "There's a task wait Directive which says OK, wait and tall all the task generating the current task have been have been completed and this allows you to do quite general things.",
            "There are some examples here, but."
        ],
        [
            "It would have time to go into them, but this is a useful thing to do, so this can't be paralyzed.",
            "You can open MP loop account.",
            "You have a list while.",
            "While there's another item in the list, you process that item and go on to the next item.",
            "So you just have to put a link list of tasks and then which linked list of structures each of which is a task, and you're just reversing it and executing them.",
            "Or there's no loop here.",
            "There's no for.",
            "You can't paralyze this with open MP easily.",
            "So."
        ],
        [
            "What you do is you.",
            "You do hashtag where OMP parallel, so you start to parallel region.",
            "So there's lots of threads here and this gets slightly slightly weird.",
            "But what we're doing here is we're only.",
            "Traversing the list is a serial operation.",
            "OK, you can only go to the next.",
            "The next linked list item having having been at the current one, you can only go one way through, so you have to generate the tasks in cereal so you have one thread which which ripped through the linked list and generates the task, but unlike here is not actually processing them, is just generating the tasks.",
            "So this is 1 thread which is in a single hashtag, one piece single zip through.",
            "The linked list.",
            "But don't actually process it.",
            "Just generate a task.",
            "So all this is do is hopefully zipping through the link list and submitting all these tasks, and while they're being submitted, the other threads 'cause member, you have multiple threads because you're in a parallel region and only one thread is generating the task.",
            "The other threads will be picking them up and processing them.",
            "But it's guaranteed they'll all be done at the end of here.",
            "So that's, uh, you know, that's a reasonable example of how to do it.",
            "It looks quite natural, but in most systems this is actually quite isn't executed particularly far.",
            "I mean it's very difficult to process.",
            "At some point the runtime system has to be clever.",
            "It has to say you've generated 50,000 tasks, but you just gotta stop now, right?",
            "So I have to stop this and go into a phase right process is them and then say right right?",
            "OK, I've got.",
            "I've got some space down.",
            "Go back and there's a lot of machinery under the hood which needs to go on here.",
            "But that is there's other examples here, but this is really the.",
            "This is really the one which illustrates how it worked.",
            "So the important point is this does not process the task, it submits this function to be executed as a task.",
            "The tasks here are generated in cereal by a single thread.",
            "They have to because point at this link pointer chasing the linked list is a serial operation, but all the other threads are available to process the task.",
            "You don't know who does what and they just pick them up and it's all magic, but this does work, but it is.",
            "It's useful in principle, but in practice you should use it with care.",
            "What know you should always test the performance.",
            "This.",
            "The other thing you can do is you could actually.",
            "You could just create an array.",
            "OK, you could traverse the linked list and just populate the array with the pointers and you could then do a for loop oh MP4 process, PIK, and make that a dynamic for loop that can actually be faster.",
            "In practice it's a bit ugly.",
            "This is the elegant way of doing it, but it may be it may not be.",
            "The most efficient so that I just wanted to tell you that tasks exist without being quite a major.",
            "I say recent, but probably in the last 10 years between fire and extension to open MP it was seen as quite a.",
            "A weak spot in the standard.",
            "So before I fell, I'll just.",
            "Do the last lecture I'll talk about hybrid programming.",
            "Which should be on.",
            "I'll put that slide back up on the.",
            "Where is it this?",
            "One is on the wiki.",
            "Yeah."
        ],
        [
            "So open MP and MPI.",
            "We see the MPI is based on processes.",
            "You create multiple processes.",
            "Each process is independent from the other and can't share memory.",
            "We've seen that that that open MP you launch a single process and that process can at runtime spawn multiple threads.",
            "Well, can't you do both?",
            "And yes you can.",
            "There's no reason not to do."
        ],
        [
            "Both so as I said in the recent years, been tend towards clustered architectures.",
            "In the old days it was much simpler.",
            "You either bought a distributed memory machine which was lots of single core processors, or you bought a shared memory machine which was a large number which was a multicore machine.",
            "Now every distributed machine, every large scale machine is a collection of nodes like these laptops which have multiple cores in them.",
            "So there's no reason not to not to combine the two models.",
            "So I said distributed memory systems for each node consist traditional shared memory multiprocessor or multicore system.",
            "A single address space with each node but separate notes have separate address spaces that that's our our standard cluster of laptops model."
        ],
        [
            "So it's like this you have.",
            "This would be this is some memory.",
            "You have multiple.",
            "This is 1 node.",
            "This is another node, another node, another across.",
            "Some indicates this will be 4 laptops connected together by some interconnect."
        ],
        [
            "So how should we process program such a machine where you can use MPI across the whole system and that's the default way if you do.",
            "On on on bridges.",
            "If you did MPI run minus N 56 you would get 28 court processes on node 128 processes on node 2.",
            "We count in general use open MP threads across the whole system.",
            "You can support this in software and hardware, but it's very expensive, so it's not really it done.",
            "But could we use open MP threads within a node?",
            "In MPI between the nodes is now the question you have to do.",
            "Is there any advantage to this so the worst thing that people do is they have their code.",
            "They are not satisfied with the scaling of it, they jump to the conclusion that it's.",
            "A problem with MPI, so then they spend six months writing a hybrid MPI Open MP code.",
            "Then they come back and they run.",
            "It's not any faster, so you know you really need to.",
            "You need to say to somebody, why do you think your code will be faster with hybrid MPI Open MP?",
            "Or yes, you need to ask that question 'cause you know it's not magic.",
            "But there can be advantages."
        ],
        [
            "So we need to consider development, maintenance, cost, portability, and performance, so these are negative."
        ],
        [
            "Development will be harder for the MPI code and much harder than from the Open MP code.",
            "So.",
            "I'll say the development of a hybrid code is harder than open MP code, and much harder harder than NP.",
            "I could have a child with open MP code, but probably if over here already written the MPI code, you're using multiple nodes, you must have written the MPI codes.",
            "The question is how much extra effort is it to add in open MP so?",
            "That's possible now.",
            "The classic ones in some cases maybe possibly similar MPI implementation with the need for scalability is reduced, EG 01 D. Domain key comes off the composition instead of 2D, so for example.",
            "If you take this this this.",
            "Laplace problem that we're going to do for the.",
            "That's a good time to bring up that slide, actually."
        ],
        [
            "Cake.",
            "So I'm going to this is my domain decomposition here OK if the grid is 1000 by 1000 K?",
            "What's the maximum number of MPI processes I can use?",
            "In this decomposition.",
            "It's not a trick 1000 exactly 'cause even at 1000 each.",
            "Each MPI process is going to have a single strip, so you're going to be sending twice as much data as you own.",
            "You know so clearly, but this is much simpler to write.",
            "So what you should, you might say that they have to do a 2D decomposition if I want to use 1024 processes, I have to do a 32 by 32 decomposition that work, but this is a lot harder to write, so this is hard to write in MPI, harder to write.",
            "This is simple.",
            "OK, so you might say R. Here's the trick.",
            "OK, if I use hybrid MPI open MP OK then?",
            "Then basically I can run on.",
            "I could, my limit is then 1000 nodes OK?",
            "I could use each node could have a single strip, but then within the node I could split that up using open MP OK.",
            "So because there are 28 cores in a node using hybrid MPI open MP you can improve the scalability of this by a factor of 28.",
            "Yeah, because so that say you've got two choices here.",
            "One is user simple MPI decomposition but extend it.",
            "By a factor of number of processes per node, by using open MP or."
        ],
        [
            "You could do a more difficult MPI decomposition, so that's really that's really up to you, but your whole code may be completely based on this, and you may think so in that that's a reasonable example and the real reason the level where people do that the the place where people do that and I don't have time to go into details, but is when people."
        ],
        [
            "Did for a transforms.",
            "Which a lot of scientific and technical codes do you have these limitations that that slice wise decomposition is very simple to write, but the more complicated decomposition isn't possibly a nightmare to write.",
            "So this really happens.",
            "So a 1D decomposition might, instead of a 2D, so it's it may be possible to simply implementation because the need for scalability is reduced.",
            "The MPI can scale to number of nodes rather than number of cores, and there's a factor of a large between them."
        ],
        [
            "So.",
            "NPR, NPR put themselves highly force, but not perfect.",
            "Combined MPI Open MP is less portable and the the real problem is thread safety of MPI.",
            "Basically MPI.",
            "The guys that wrote the MPI library assumed it was only being called by single thread at a time by a single process.",
            "So you might have a data structure in MPI.",
            "But if multiple threads are going to call the MPI library at the same time, then that you could all help you know who knows what's going on.",
            "OK, so so the data structures can be completely corrupted.",
            "So it's very difficult.",
            "It's difficult to write an efficient threadsafe MPI library because you know they haven't been designed for that.",
            "So.",
            "And you may want to.",
            "I mean, you maybe want to have a code which can run as a standalone MPI, 'cause that's really possibly the."
        ],
        [
            "Issue, but the main thing is thread safety.",
            "Making library threats that can be difficult.",
            "You can lock access to data structure, but if they all add significant overhead, so you'd be really annoyed if you had a standard MPI code.",
            "OK and the upgraded the operating system and the compilers on your MPI code went 10 times slower, and you say why, oh, 'cause we've made the MPI library thread safe and you're like, well, I don't want it to be.",
            "Thread safe.",
            "OK, so it needs to be a mechanism to allow those who want to exploit this functionality to exploit it.",
            "But for those who don't need thread safety, which is most people who just run one process, MPI process, no threads.",
            "They aren't hamper and what happens is that you could you, could you ask the MPI library when you initialize the MPI library?",
            "Tell it look, I want to use the MPI library, but I want it.",
            "I will need thread safety or I won't need thread safety and it can help.",
            "You can have different implementations."
        ],
        [
            "So perform so.",
            "This is for reasons why you might want to mix the MPI Open MP code replicated data poorly scaling MPI code, limited MPI process numbers, or the MP implementation not tuned for these shared memory clusters.",
            "Now go through these one replicated data."
        ],
        [
            "The real issue?",
            "Some MPI approaches are replicated data strategies.",
            "So, for example, you might have some look up table or some data structure which everybody needs to read all the time.",
            "Some database or something and MPI to avoid vast amounts of communication.",
            "You probably want each process to have.",
            "Its own.",
            "Each process will need to have its own copy of that data structure.",
            "And so so on bridges.",
            "If you run 28 processes per node, you'll have 28 copies of the data structure on the node, which is kind of crazy 'cause you know it's shared memory.",
            "Why can't I have one copy per node and everyone read that?",
            "Well, it's not easy to do that in MPI.",
            "You might say I don't have any replicated data where you probably do, 'cause when you have domain decomposition of replicated data, it's not so obvious in the traffic model because the Halos are so small, but the Halos are replicated data, their copies of data on another process.",
            "Now in the 1D decomposition of the Halo.",
            "It's very very small overhead, but it can become significant if you had a mixed hybrid MPI Open MP code.",
            "The data structure could be shared by multiple threads within a process, and you wouldn't need a lot of extra storage, so this is becoming increasingly important because it might have memory per core, is not likely to increase.",
            "People are sticking more and more cores.",
            "I don't know how much.",
            "How much a memory a Knights landing processor has, but it's going to be shared between you know hundreds of cores so that the amount of memory per core is going is dropping dramatically, so memory saving techniques will become quite quite important, so I said."
        ],
        [
            "Halo regions are type of replicated data and it may not be obvious, but if you look at it.",
            "If you have.",
            "Um?",
            "A um?",
            "A local domain size of 50 ^3 OK and you had it.",
            "This is a 3D decomposition and a lot of simulations are 3D.",
            "OK if you have a Halo so in fact you actual array isn't 50 by 50 by 50 is 52 by 52 by 52 because you need to have a room on the face every face of the cube to store the incoming data 52 ^3 -- 50 ^3 is 11%.",
            "So 11% of your data is already in Halos.",
            "But then you say well I want to run on.",
            "I want to run on more and more processes so I go up to this stage.",
            "I'm running on more processes with the fixed problem size.",
            "This is my local problem.",
            "Size is 10 ^3 my eraser.",
            "12 cubes.",
            "You've already got almost 50.",
            "You've already doubled your memory usage.",
            "Because just because you have to add 1 onto every dimension and that that you know if you have to have Halos of depth two or more with some codes do this just come completely blow you.",
            "And so the important point is that if you do a hybrid MPI open MP, if you run one MPI process per node, you only have one Halo per node, not one Halo per core, and so this becomes dramatically.",
            "The overhead becomes dramatically smaller.",
            "So this could be this is possibly the more significant."
        ],
        [
            "The other one is poorly getting MPI codes.",
            "If the MPI, Russian, those scales poorly, the mixed mpongo could may scale better.",
            "There can be algorithmic reasons load balancing in MPI is really difficult becausw.",
            "If I have a task that I.",
            "The I don't want to do and I want somebody else to do it.",
            "I not only have to tell them to do the task, I have to give them the data.",
            "So if you want to transfer work you have to transfer the data, which is quite difficult, but so it's a.",
            "It's a big overhead in an open MP you don't have to transfer the data, all the data in shared memory.",
            "If you want someone to do a task, you just say I can't be bothered doing it, you do it and they just walk in and do it.",
            "So load balancing is much easier in open MP.",
            "So it's possible that for adaptive, irregular problems, load balancing is better solved by hybrid MPI Open MP, however.",
            "If you're running on multiple nodes, you still have to solve load balancing between the nodes yourself, so it's not.",
            "It's not guaranteed, but it's possible.",
            "Simplicity reasons is often the real reason.",
            "That's often the real reason why people so actually running out of memory and simplicity reasons.",
            "The two driving factors for people doing hybrid MPI Open MP so often you'll find people are running fewer MPI processes than there are cores on a node, because they're saying I need a GB per core, there's 28 cores, but you've only got 10 gigabytes.",
            "I can't use all the cores, OK?",
            "So even if there open MP implementation is a bit rubbish, even if it only goes a few percent faster.",
            "It's worth doing because otherwise there's quarter going unused purely for memory reasons, and again, simplicity reasons people may say look, my algorithm is fundamentally doesn't scale beyond 2000 processes.",
            "'cause I'm my array is 2000 by 2000, and so I can extend it that way."
        ],
        [
            "Load balancing as I've talked about.",
            "Is easier, so that can be possible."
        ],
        [
            "Limited MPI process numbers I mean.",
            "MPI libraries may not be able to handle it processes adequately.",
            "This is really a problem for MPI developers, but basically on very large systems it's going to become very difficult to write in MPI implementation, which which can scale to millions, for example example.",
            "Matching wildcards it seems quite nice on MPI that you can do a wild card you can receive from anybody, but if that energy is 1 from a million, you know that's going to be a hell of a lot of searching.",
            "Some of these wildcards can anytime that the library needs.",
            "Storage which is.",
            "Of order P, which scale the number of processes, then you're really, you're really.",
            "It's really difficult so.",
            "On very large systems, it may be that you know that the MPI library just kind of falls over for very large numbers, and you can just reduce that overhead by having some threads in there."
        ],
        [
            "This was true in the old days.",
            "MPM did not change for SMP clusters.",
            "This is lesser problem these days.",
            "MPI libraries tend to be quite.",
            "Sophisticated they will notice if two processes are on the same node and they will, they won't, they won't.",
            "They won't talk to themselves by the network, they'll do it through the shared memory, however, so someone mentioned this the other day.",
            "But people, however this is this.",
            "This is a big issue, potentially.",
            "Mixed mode code tends to migrate messages.",
            "You typically send with a hybrid code, hybrid MPI open MP.",
            "You're typically send one message per node, so you'll send a small number of large messages, whereas whereas the pure MPI code will send the large number of small messages.",
            "So, for example, if I run a pure MPI code on bridges, there are 20 M 28 MPI processes on each node.",
            "Probably cannot connect to the same network interface.",
            "The network interface is having to cope with 28 processes simultaneously sending out messages to send.",
            "And that that that can really kill networks.",
            "It's actually nicer if you aggregated them and said one big message.",
            "Networks hates more messages, you know it's just.",
            "It's just very very.",
            "It's a difference between you got 100 people to go from between 2 cities that reset them 100 people each in their own car.",
            "Or is 100 people in in a coach?",
            "Of course, 100 people in the coach is much more efficient, but you know, hundred cars.",
            "There's also congestion and stuff, so it's so this is a genuine issue.",
            "Modern networks are getting better, but.",
            "If you're on a cluster which maybe just has.",
            "Ethernet or something?",
            "This could be a real issue potentially.",
            "So people always quote the bandwidth and latency of their network, but often the real question is how many messages per second can it?",
            "Can it sustain?",
            "Not, not really."
        ],
        [
            "So there's many styles of mixed mode programming.",
            "I'm only where they're going to go through master only and talk about the other ones.",
            "Well, actually go through them quickly, but master only is really the simplest thing to do.",
            "All the MPI communication takes place in the sequential part of the opening program.",
            "There's no MPI in parallel regions, and this is really the most really the simplest way to do it."
        ],
        [
            "You have some work.",
            "Then you call some operation and then you have some other work OK, so basically this is kind of the obvious thing to do.",
            "You basically you accelerate the work, but you do all your MPI through.",
            "Remember at this point only one thread which is the master thread which is the parent processes operating.",
            "So this is just like an accelerated MPI code.",
            "All the communications is the same when you accelerate the work and that that's the simplest.",
            "Thing to do.",
            "The MPI Library doesn't need to be thread aware or thread it, just it doesn't care.",
            "Sing"
        ],
        [
            "Italy funneled is the same in funneled.",
            "You may call MPI within within a parallel region, but you make sure that it's only called on a single master thread, so this is just achieving the same thing.",
            "So again only one thread.",
            "The master thread is ever calling the MPI library, so again this works fine."
        ],
        [
            "Serialized exactly the same thing, but again.",
            "You, as a programmer, guarantee you're saying OK, multiple threads will call the MPI library, but they will never do it at the same time.",
            "And again that means the MPI library doesn't really need to be to be thread safe, so you have to put your MPI calls in one of these critical regions, which I mentioned before.",
            "So all those work recently."
        ],
        [
            "Well, the one which which is very difficult as it's called multiple where basically any thread can call the MPI library anytime.",
            "And this is this is the one which is difficult.",
            "This is where the MPI library has to do a lot of work.",
            "And so those are the three, the four versions.",
            "What are they called?",
            "Master only funnels serialized among?"
        ],
        [
            "And so if you're gonna, if you're going to run a hybrid MPI Open MP code rather than MPI and it you call MPI init thread and you tell it what kind of level of support you want, and they are.",
            "It's a bit weird you ask.",
            "You ask for a level which is 1, two, three or four and it tells you what it can provide, and it may give you back a lower number than you asked for.",
            "You may say I want."
        ],
        [
            "MPI thread multiple that may come back and say I can't do that.",
            "OK, but actually.",
            "Most MPI implementations could support single funneled and probably serialized.",
            "Yeah, can probably support all of these with very little overhead, but this is the one which is difficult, free for all.",
            "Any thread can call any can call any MPI routine any actually this is.",
            "Yeah yeah, this is well this this is difficult.",
            "The other problem."
        ],
        [
            "You have.",
            "Um?",
            "The this is quite briefly, MPI stuff, but basically you asked for.",
            "You asked for something that gives you back a number, which is the highest it can provide, which is, you need to check that it's providing what you want.",
            "But I would really recommend at least definitely for the hybrid challenge."
        ],
        [
            "Program in the master owning and in fact in the master only style.",
            "You could just call MPI and that you don't need to call me on that thread because it's that you know it's it's.",
            "It's it's fine so I would really recommend just doing this style if you want to try it.",
            "Don't bother with anything else."
        ],
        [
            "There are some issues though.",
            "Where is this so?",
            "Well, there's a lot of, so I don't really want to go through all that you can.",
            "There's a, there's lots of support."
        ],
        [
            "Routines I can talk about lots of pitfalls."
        ],
        [
            "And issues, but they're up there.",
            "I don't really want to go through all these because I want to leave time for the for the for the hands on."
        ],
        [
            "But there's stuff there with your interest."
        ],
        [
            "Talking about where the issues."
        ],
        [
            "But it's a little bit 10."
        ],
        [
            "For this for this."
        ],
        [
            "For this talk."
        ],
        [
            "The one thing which is."
        ],
        [
            "Is it weird?"
        ],
        [
            "Is."
        ],
        [
            "Multiple."
        ],
        [
            "Endpoints are sorry.",
            "So, um.",
            "Try."
        ],
        [
            "Are trying to be outside.",
            "The problem when you do and then when you do an MPI send, you specify a process OK.",
            "Which that MPI routine, which that MPI message goes to.",
            "But if you have multiple threads within that process, you might want that message to go to particular thread with that process, and MPI doesn't actually allow you to do that.",
            "So the only real way you can do it is you can use these tags.",
            "I said that each message has A tag.",
            "I said they weren't normally useful, but this is a case where they are.",
            "If you want to send to thread 5 on process 3.",
            "You would send the message to process three and tag it with tag 5, which is just a hint that at the other end that message is meant for thread 5, so it's not really what tags were invented for, but you can do it, but it is important to realize that MPI specifies the destination process, but if you have multiple threads.",
            "Then you have to do something else to make sure that it goes to the correct thread within that process again with the master only style.",
            "It's irrelevant because you know when you're sends and receives from the master thread, so it's a fine that it doesn't matter.",
            "So any in this multiple style, we have multiple threads calling MPI simultaneously, but this becomes an issue.",
            "It all gets a bit messy, but it might."
        ],
        [
            "Experience, and there's a.",
            "There's some discussion in MPI have something called endpoints which will solve this, but that's really beyond this."
        ],
        [
            "Nope.",
            "In my experience, people who have used the multiple.",
            "The multiple version have suffered.",
            "I've never really got very good performance because it's so much effort for the MPI library to support multiple thread safety that it that it kills the performance and the other thing which people get wrong is this is the classic thing they do, so they've got their MPI program OK running on 6 threads, OK?",
            "And they find that this routine here.",
            "OK, there's a routine which is taking place here is very slow in MPI.",
            "So what they do?",
            "If they take that routine and they go away and they give it to some clever person and say I've got this routine which is very slow and pure MPI, could you write me a hybrid MPI Open MP version and they do everything look it goes faster.",
            "Oh that's brilliant.",
            "OK so I'll plug that back into my main program.",
            "But the problem is in hybrid MPI Open MP.",
            "There's only one MPI process running OK. People seem to think people think that hybrid MPI OMP is like you have four MPI processes, and then when you go into a parallel region, they become four open MP threads.",
            "No, you have one MPI process.",
            "And every now and again you spawn multiple threads.",
            "What that means is every time you're not using open MP and a hybrid MPI Open MP job, you're wasting your wasting cores.",
            "They're not running here, so you have sped up your kernel.",
            "This bit here in this bit here, which is great, but the other stuff is now running running on.",
            "You may be gained a factor of two here.",
            "You've lost a factor of 6 here.",
            "'cause you're running this on one on one one core, so then you have to go and put open MP in here.",
            "This might be if you got a million line code.",
            "Your kernel might be 1000 lines.",
            "You have to spend putting open MP into 990,000 lines of code which don't need it, they but they need it because to run hybrid MPI Open MP you have to run fewer MPI processes than there are cores.",
            "It's a very naive observation, but I'm astounded how many people don't.",
            "Don't, you know, it's a real problem.",
            "It's a real real problem you cannot.",
            "It's not an incremental, it's not an incremental approach.",
            "Now MPI has been extended and MPI does have its own shared memory model now.",
            "Anne.",
            "The the the side if you look at the standard MPI single sided communication supports a shared memory model and so.",
            "It is possible to have MPI processes behaving like threads sometimes, but it's all a bit messy.",
            "But this is the most important thing.",
            "OK, that basically the bits that don't use open MP will go slower because they're only running on one process, and they're probably the bits that you don't care about.",
            "But you have to put open MP into them."
        ],
        [
            "And the result is that the performance goes like this.",
            "So basically you start off with a pure open MPI code.",
            "You then have to run it on one process to do the hybrid model.",
            "So you take a huge performance hit of a factor of 6.",
            "Then as you start paralyzing the open MP parts your performance grows.",
            "But does it ever get it takes time?",
            "This is developer time this month.",
            "This is PhD start, PhD end or something like that.",
            "Contract, start, contract end, you know, do you get back up to where you start?",
            "If you started with a serial program, this isn't an issue.",
            "'cause with the serial program it started here everything you do in open MP is a benefit, but if you started with a parallel MPI program you take a hit by going to hybrid MPI Open MP and you don't know if you get back to where you started.",
            "So you really have to think carefully before you do hybrid parallelization it what?",
            "Why do you think that this will end up higher than that?",
            "And if you say Oh well, I took out the computational kernel, it goes a lot better, like, well wait a second, you still got the other, you got it.",
            "The rest of the code to worry about.",
            "So."
        ],
        [
            "Hi, refers to the major research topic.",
            "At.",
            "Maybe people say it's the key to exascale, but.",
            "It you know, it's very difficult.",
            "Achieving correctness is hard with this MPI thread multiple if you do MPI thread multiple you have to worry about race conditions not only in your open MP code, but race conditions between MPI messages, and that's going to do your head in a bit.",
            "Achievement forms is hard 'cause the entire application must be threaded.",
            "If you have a hybrid MPI Open MP code.",
            "The first thing you should do is run it in a variety of modes.",
            "Run the pure MPI version and then run it with.",
            "You know?",
            "So that we can bridge that we 20 MPI processes product, then with 14 and two threads seven and four 3 1/2 and then it doesn't work anyway.",
            "Something like that and look for the sweet spot look, look for the sweet spot we had this the cosmology talk.",
            "This one.",
            "I apologize that I misinterpreted the graph, had done exactly that.",
            "Had done a scan across threads and processes and looked for the sweet spot and that's just a freebie.",
            "If somebody's done all the work but don't don't say don't immediately assume that.",
            "The best thing is 1 process per node.",
            "And 14 threads.",
            "It might not be and just do a quick scan and see what you get.",
            "And the other problem is placement of processor threads on numerous architectures.",
            "Bridges, like most architectures, isn't.",
            "I say it's a 2028 core machine.",
            "What it actually is is 214 core CPUs stitched together and communications between the two CPUs.",
            "This is an open MP is slower than communication within the CPU.",
            "It's called a Numa architecture.",
            "Non uniform memory access, so it's hard to get open MP to scale beyond.",
            "A single CPU you can call that socket.",
            "OK, so probably on bridge is what you want is you want two MPI processes per node, each with 14 threads, but you want to have one MPI process on each node, and you want the threads to sit within the nodes.",
            "And at.",
            "I think bridges is quite well set up.",
            "I think it may do that for you or it may not.",
            "I don't know.",
            "That's always a question, but that you have to start worrying about these nasty hardware details that you would rather not bother about.",
            "I think on bridges it might be quite easy.",
            "I think bridges I tried to look at this.",
            "I find it difficult to understand.",
            "But my first guess is I should check actually by.",
            "My first guess is that bridge is not the right thing, but if you ask for two MPI processes, it puts the first process on Core 0 and the second process on Core 14, which is on the other processor and then your empty your open MP threads will be bound to those two CPUs.",
            "I think.",
            "Intel is even more complicated 'cause if you ask for eight threads at Intel, they create a helper thread as well, which is nothing but the sort is completely unhelpful so.",
            "Intel MPI can become even more complicated, but I will look at that.",
            "I think bridges does the right thing out of the box, but I need to check that I do have a program I can run to check it.",
            "So I think what I would say is that I would.",
            "I really, really, really, really good if somebody had a look at hybrid MPI Open MP for the Laplace model.",
            "Oh, I would recommend that you do the very much recommend that you do the master only approach, which is the obvious one.",
            "Sorry this is.",
            "Sorry, I've.",
            "Lost my place in the witches."
        ],
        [
            "This just do this.",
            "Just take the MPI code and markup as you pretend it's pretend it's an open MP code and just mark up the work and leave this as it is.",
            "I don't know how well that's going to perform, but it may not perform any better than the MPI, but I mean, it's definitely worth a look.",
            "I think it might.",
            "It might do better, I don't know so.",
            "I'm quite happy to give a price the best Hibernate NPI NP could, even if it doesn't meet the MPI.",
            "So you had a question to get there.",
            "So microphone it'll repeat that I just repeat the question, it's OK. To avoid the issues with the MPI libraries not handling data from multiple threads simultaneously.",
            "First, I'd like to say are there negative consequences to using just bare MPI in it?",
            "And then if you like using barriers or something to make sure that no more than one thread is communicating at a time?",
            "I mean, I understand that there's a special mode for that, but what if you don't use that mode?",
            "I think I.",
            "So that's called serialized.",
            "My guess is the standard out the box MPI library should be able to cope with serialized.",
            "Access and why would it care that it's being called from different threads?",
            "It it doesn't know.",
            "It's just, you know, it's just doing stuff so.",
            "I just don't think there's any issue with nonblocking communication, is that that's about just worried about one thread initiating a nonblocking communication, and the other one.",
            "The other one.",
            "Trying to receive it.",
            "Open master and.",
            "Funneled are the same in the sense that only only only the only threads you ever called the MPI library.",
            "You don't need any special support that whether you need special support for serialized.",
            "I don't know.",
            "It's a good question.",
            "I would guess, probably not.",
            "But I I wouldn't like there.",
            "Maybe there may be subtleties that I'm not thinking about so.",
            "Idita my worry is that you have nonblocking communications initiated by one thread.",
            "And then that then another thread is calling the library and just trying to think if there's an issue there.",
            "I don't know enough.",
            "My guess is it would work, but it would work, but I am not 100% sure.",
            "So there might be other problems with MPI serialized and thread multiple, so I not aware that any of the performance tools support this mode, so you can do some measurements, but doing automatic analysis doesn't work 'cause you don't know which thread receives the message.",
            "So this is a real problem and it will only be solved if the MPI standard includes something like the endpoints you already mentioned.",
            "Yeah, yeah, so that's yeah.",
            "So you really basically.",
            "You're trying to.",
            "You've got 2 tickets, completely different standards written by different people at different times, and you're trying to make them work together, and it's not.",
            "It's it's tough.",
            "I wanted to ask how the cost for.",
            "I'm just going to close these doors.",
            "I know that's not, there was just a bit noisy at the cost for four or.",
            "Yeah, yeah I can.",
            "Well, I'll repeat the question, it's OK. No no no no.",
            "Work, sorry.",
            "Well, where water was the first thing?",
            "Yeah.",
            "OK, so the question was sporting open MP threads compared to MPI communication.",
            "So actually, although logically in the model is that you spawn threads at parallel region.",
            "In fact, that's not what happens.",
            "You know what will happen is the MPI like on a dedicated node on bridges.",
            "The MPI Lybrel create the 27 helper threads at the start and they'll be sitting there buzzing away waiting to be woken up.",
            "So so logically in the way you think about NPI to write correct open MP programs?",
            "You should think that apparel region it spawns multiple threads, but in fact they're sitting there waiting and it kind of kicks them up so so people have worked really hard to make the overhead, but that's a good question that actually.",
            "22 Open MP threads.",
            "Copying data between each other can be almost as costly as an MPI message because because MPI will notice that it's on a shared memory node and it will just do a copy.",
            "There will be some processing overhead in the MPI stack, but but then it's a bit deeper to explain what the cache coherency issues with copying data between different threads could be quite high as well, so.",
            "Typically.",
            "Yeah, yeah, so it's it's.",
            "It's it's normally.",
            "My trying to say.",
            "Um?",
            "Where where it benefits you is in it.",
            "If you know that you want.",
            "Course to randomly read pieces of memory in open MP you just say put that database in shared memory.",
            "Every thread can read what it wants.",
            "If you do that in MPI, everytime you if the database is distributed every time you want something to just read one byte of data, you have to send a message to somebody they have to send the message back on it.",
            "So so in extreme versions like that it can work, but it's I don't know.",
            "So my colleague Mark Bolts.",
            "OK, here's like there are standard MPI lights.",
            "There are standard libraries for measuring MPI communications.",
            "Overheads, so it's called the.",
            "Intel micro benchmarks.",
            "It didn't used to be called the Intel Micro benchmarks, but it's called the Internet for the Intel IMB and you can download that and run it and you'll say the latency of sending a message is so many microseconds, but my colleague Mark Bull has also written us along time ago.",
            "A simple benchmark suite for measuring open MP overheads, and that's called the Open MP micro benchmark.",
            "So if I just show you.",
            "So I want don't want to go here.",
            "So if you look for.",
            "Intel.",
            "MPI benchmark it's it's.",
            "It's there you'll find it.",
            "And it benchmarks point to point, so you can measure these things.",
            "And then if I do open MP.",
            "Micro benchmark.",
            "Then that I see our websites gone.",
            "There's some there's some problem with website and need to report it.",
            "I think it worked.",
            "I think it's a Firefox problem.",
            "That's unfortunately anyway.",
            "There are versions there and they they.",
            "I'll report that there's some some some stylesheet problem there.",
            "I'll that can measure the overheads of how, how much, how much, how long does it take to spawn apparel, or even how long does the barrier take?",
            "How does no MP4 take things like that?",
            "So that would give you some ballpark figure.",
            "Yeah.",
            "Yes, you had it.",
            "I think I think I'll be fine here.",
            "I think it's to do with the fan.",
            "Um, are we actually competing with the open ACC Group for the fastest Laplace solver?",
            "And if so, do they have access to the GPU nodes on bridges?",
            "They do have access GPS, so I'll probably give a price so.",
            "If I knew what the performances were, and they were roughly the same, then I'd say yes.",
            "But I think yes, you are competing with them, but I mean, it's probably it may or may not you.",
            "It may be.",
            "It may be easier for us.",
            "You know, for example, you know the GPU nodes only have we have 56 cores per node and the GPU knows presumably have one GPU and only the other socket is probably 14 cores.",
            "So and the processes here are very new and the GPU they're getting, they're getting new GPU soon.",
            "But the current GPU's are sort of previous.",
            "Are the current generation so the CPU's on bridge is a kind of a half a step ahead of the GPU's.",
            "So it's not clear which the fastest one will be.",
            "I have to say.",
            "But I'll, I'll probably give a prize for the best in both camps.",
            "It wouldn't be fair to say you know that.",
            "Wait, so are we also running on the GPU notebook with Intel Xeons right on the single?",
            "Just run on the just run on the standard nodes, the ones that we?",
            "I think it's minus RM or something.",
            "OK, yeah fine yeah.",
            "I wish somebody asked this before, but it is so.",
            "If somebody is if.",
            "There's two different questions here.",
            "If you're given access to a GPU enabled machine OK, or your research group is bought dependable machine, it's worth.",
            "Using open ACC to use those GPU's it's great.",
            "And you'll probably get performance benefit.",
            "However, taking a step back if you had a fixed amount of money and you were to buy a machine, the question is whether it's more cost effective to buy GPU or CPU's.",
            "A different question because you know, but if the machines being bought and I'm not slagging off GPS, I'm just saying it's different to completely different calculation.",
            "So as I said, you know if the GPU's have been bought great putting up this, he used them, get the best performance, but if someone gave you a fixed amount of money and remember money.",
            "Translates into hardware, but also the people time.",
            "You might say, well, you know the codes are going to three times as fast, but it's going to take me a year of effort and that might be worth it.",
            "Or it might not, so it's a different count there was there was a I've only seen one paper which did the whole.",
            "There's a German Group A year or two ago at supercomputing.",
            "I can't remember who they were to the whole life cycle analysis of cost, performance and they they said it was into this.",
            "It was sort of 5050, you know, some codes were a lot better, some cultural lot worse.",
            "So it sort of said.",
            "GPU machines make sense for special.",
            "As for special, not special, but for specific groups but not as general purpose machines.",
            "And that was effective.",
            "They were the only people I've seen who tried to estimate end to end taking into account cost of ownership, development costs, running costs, everything.",
            "It was quite nice.",
            "They got a lot of abuse from people who bought.",
            "I mean, you know, they actually just done something very.",
            "Heaven forbid cyantific.",
            "Just looked at the numbers and worked out what it was and it's quite nice.",
            "It said, you know, some codes do well, some do badly.",
            "So you need to think about this, but didn't go down very well.",
            "Out this question at the back, yeah.",
            "I was thinking about the example you showed it toward the end of the presentation where you had you were taking MPI code and then trying to add open MP3 open MP threads to it.",
            "Yeah, and how you're saying that as soon as you even start the first thing you're going to do is we're going to drop to a single MPI.",
            "Yeah, right.",
            "Now what I was thinking is.",
            "In two parts of the of the code, if you've got two kernels in there where you're going to use the open MP threads, suppose you got like A6 core processor like in that example you could run the six threads there, could you?",
            "Could you write your code so that those six threads are assigned to your first MPI rank, and then as soon as that that code ends, the 2nd through 6th MPI ranks then become active again?",
            "And wouldn't the OS handle scheduling issues, right?",
            "You would like to think the OS would do this for.",
            "The problem is that the way that you get performance out of.",
            "Open MP is the threads wants be active all the time.",
            "They busy way and the same in MPI you know MPI.",
            "There's a big cost of scheduling the scheduling.",
            "You know it's it's charging and discharging is of the order of.",
            "Milliseconds so yes, So what?",
            "People a lot of people have said and people have have.",
            "Mooted this Rex get architects you need architecture which can switch contexts really quickly on the top.",
            "But they do exist.",
            "There was the terror terror empty it got renamed and re bought, but people have have built architectures which can support where can't where.",
            "Context switching between threads or processes very low overhead and so on.",
            "A modern I mean on modern Linux I don't think you could do it, but there's no reason you could design an operating system.",
            "Or you could design A chip which could do it.",
            "The problem is.",
            "You know we're in this.",
            "This this completely monolithic region where where everyone uses everyone, uses Linux and so you know that that.",
            "I'm sure there's research in that, but it seems to have, yeah, but there's no reason.",
            "Yeah, that's that's that's a perfect reason, but I don't think you could do it with Linux.",
            "I'm not an operating system match, but by any stretch the imagination.",
            "But you could design an operating system and an architecture which could do that.",
            "You write.",
            "In fact, it's kind of mad that we have this difference in Linux processes and threads are.",
            "Almost identical, there really isn't any, and maybe the I think the MPI 3 model might be the way that you go, 'cause there you can, although it's a bit overblown.",
            "Because it has to conform with the rest of the MPI standard, which has a lot of subtleties but but their MPI processes on a node can create shared data structures and access them.",
            "You don't have the support of OMP four and do for splitting up loops, but at least you can do that by hand, so that's probably the way to go, and I haven't looked at that yet.",
            "That's quite recent.",
            "There was somebody in the audience who's been looking at it, but, but that is quite recent, but that is probably the way to go.",
            "Um?",
            "To solve both problems.",
            "So OK, so no more questions.",
            "Got it.",
            "Well that exercise to half time.",
            "Welcome to happy to help you out with anything.",
            "I think all the material is up there.",
            "I will put up that one lecture.",
            "What's the one I gave both tasks?",
            "I'll put the task lecture.",
            "I just wanted to mention there existed and I hope you find it useful, so I apologize that I know I. I know that the material came from lots of different places, 'cause this I've had to refactor this course, but hopefully it fitted together reasonably well and you found it useful and I'll be interested to.",
            "To see.",
            "How people get on with the programming challenge.",
            "And yeah well thanks everyone.",
            "So hope you find it useful and hope you enjoy the rest of the rest of the workshop in the summer."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the idea is.",
                    "label": 0
                },
                {
                    "sent": "So last year Jaune ran this in Mark I I'm doing it this year, so who knows how will go.",
                    "label": 0
                },
                {
                    "sent": "But this is the second challenge.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "This trophy bears no relationship to reality, but there may be some prize but it would be nice if people had a go at this.",
                    "label": 1
                },
                {
                    "sent": "As I said, we've not done this on bridges before so even very simple techniques or at the box might be fast.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general rules are all that due Thursday night, if that's when the event is, but I don't know what that means is.",
                    "label": 0
                },
                {
                    "sent": "I'll run the codes and and get the performance on Friday morning.",
                    "label": 0
                },
                {
                    "sent": "That's what it really means, but I don't expect you want it.",
                    "label": 0
                },
                {
                    "sent": "You should use for.",
                    "label": 0
                },
                {
                    "sent": "You can use up to four nodes of bridges and you can use any combination of MPI Open ACC Python.",
                    "label": 1
                },
                {
                    "sent": "I think you'll be struggling to get performance, but an open MP, so I expect you will want to use a hybrid MPI and open MP.",
                    "label": 0
                },
                {
                    "sent": "How fast can you run a 10K by 10K?",
                    "label": 1
                },
                {
                    "sent": "That's 10,000 by 10,000.",
                    "label": 0
                },
                {
                    "sent": "Ashford Convergence, so we I'll give you.",
                    "label": 0
                },
                {
                    "sent": "A brief overview of what that is.",
                    "label": 0
                },
                {
                    "sent": "Message.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just some specifics.",
                    "label": 0
                },
                {
                    "sent": "You can't change.",
                    "label": 0
                },
                {
                    "sent": "The kernels must retain 2 core loops in the source.",
                    "label": 1
                },
                {
                    "sent": "You can't change the number, you can change them by processes, I mean 112 if you're on four nodes, 112 would be a pure MPI job 4 * 28.",
                    "label": 0
                },
                {
                    "sent": "4 might be a appear.",
                    "label": 0
                },
                {
                    "sent": "One MPI process on each node and then the other cores being used with open MP.",
                    "label": 0
                },
                {
                    "sent": "But you can use anything just one source file for simplicity and would be nice if you if you're doing things like setting environment variables, just give a single script so I just do.",
                    "label": 0
                },
                {
                    "sent": "Run script or something so it will set, compile and submit.",
                    "label": 0
                },
                {
                    "sent": "I mean anything reasonable but you try and make it as easy as possible to run your solutions and that's my email address d.hentaiepcc.ed.ac.uk by the deadline.",
                    "label": 1
                },
                {
                    "sent": "If there's any changes on email you, but that's the specifics.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rules, no libraries don't match without the timer is just.",
                    "label": 1
                },
                {
                    "sent": "I mean really, it's a matter of you know, doing reasonable things.",
                    "label": 0
                },
                {
                    "sent": "The obvious things.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You might want to do so.",
                    "label": 0
                },
                {
                    "sent": "The Serocco converges at three 578 time steps.",
                    "label": 1
                },
                {
                    "sent": "Yours should too, so the code is just iterating forward and there's a tolerance.",
                    "label": 0
                },
                {
                    "sent": "And when it reaches that tolerance it stops and that should be 3578.",
                    "label": 0
                },
                {
                    "sent": "Jaunes pointed out this isn't actually enough to verify correctness becausw.",
                    "label": 1
                },
                {
                    "sent": "You can have a slightly incorrect code which still gives the right still converges you should find.",
                    "label": 0
                },
                {
                    "sent": "This point seven, 500, nine, 950 or 9 nine 57504 times the C in the Fortran examples of transposed simply because of the C and fortran's disagreement about what order array indices are in about 17 plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So as as distributed, the code is 1000 by 1000 example just for development, but you should run 10,010 thousand.",
                    "label": 1
                },
                {
                    "sent": "This is a really really really dumb.",
                    "label": 0
                },
                {
                    "sent": "Algorithm for solving this this equation you could make it 100 times faster 10 times fast by using Gauss Seidel off, but that's that's not the point.",
                    "label": 0
                },
                {
                    "sent": "OK, the point is to make this algorithm go fast using parallelism.",
                    "label": 0
                },
                {
                    "sent": "Yes, you could use more sophisticated algorithm, which would be much quicker.",
                    "label": 0
                },
                {
                    "sent": "You could use Fourier transforms or something, but that's not we're interested interested in because what you do to speed up this simple code could equally well speed up the guy Seidel or the more if you don't understand this stuff it doesn't matter, but you know it's not a big deal.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things to explore.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "I don't know for this, but you can do compiler flags.",
                    "label": 0
                },
                {
                    "sent": "You'll have to look at the manpage for that.",
                    "label": 0
                },
                {
                    "sent": "You can select different compilers.",
                    "label": 0
                },
                {
                    "sent": "By default, the MPI compiler calls the new compilers, but the bridge is.",
                    "label": 0
                },
                {
                    "sent": "Documentation will tell you how to use different modules so you can change the MPI compiler to use the Intel compiler if you want, and the documentation explains how to do that.",
                    "label": 0
                },
                {
                    "sent": "There may be environment variables you can change this thread if you're using open MP, thread placement can be important.",
                    "label": 0
                },
                {
                    "sent": "That this is mentioned in the.",
                    "label": 0
                },
                {
                    "sent": "This is mentioned in the document in the exercise KMP.",
                    "label": 0
                },
                {
                    "sent": "Affinity is a way of of.",
                    "label": 0
                },
                {
                    "sent": "Trying to get the operating system to schedule your threads more predictably, and that's one of the exercises on the sheet.",
                    "label": 0
                },
                {
                    "sent": "So I hope that's right.",
                    "label": 0
                },
                {
                    "sent": "Only if using Intel, so I should set OK so so so my open MP examples that I've given out in the course are set up to use the Intel compiler so you can use this if you're going to do it in the.",
                    "label": 0
                },
                {
                    "sent": "In this example, you'll need to change the Intel compiler.",
                    "label": 0
                },
                {
                    "sent": "That's probably a good thing to do actually I don't know is it module load is just module.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should probably email around how to do that actually.",
                    "label": 0
                },
                {
                    "sent": "I don't think it is if you look.",
                    "label": 0
                },
                {
                    "sent": "I I just got the default here on bridges.",
                    "label": 0
                },
                {
                    "sent": "And if I do MPI CC minus minus version.",
                    "label": 0
                },
                {
                    "sent": "I get it's canoe.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you yeah, there's nothing wrong with the new compilers, but it's just that they they.",
                    "label": 0
                },
                {
                    "sent": "They don't have such control.",
                    "label": 0
                },
                {
                    "sent": "Environment variable yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean you'd expect the Intel compilers ought to give slightly better performance on an Intel chip, but who knows.",
                    "label": 0
                },
                {
                    "sent": "So I have so I forgot to update this from last year.",
                    "label": 0
                },
                {
                    "sent": "Blue Waters User Guide is your friend.",
                    "label": 1
                },
                {
                    "sent": "Apologies, that's that's out of date.",
                    "label": 0
                },
                {
                    "sent": "So we're using we're using bridges, but the bridges documentation is easy to find.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'll take the top self reported speeds and run them and I'll do that on Friday and just a few rules.",
                    "label": 1
                },
                {
                    "sent": "But I mean really, we're just looking for what you can do, so that's the rules.",
                    "label": 0
                },
                {
                    "sent": "There is quite a long presentation here which which is on the wiki.",
                    "label": 0
                },
                {
                    "sent": "Going through the details, but I will go through very quickly because I really want to point out.",
                    "label": 0
                },
                {
                    "sent": "The interesting stuff.",
                    "label": 0
                },
                {
                    "sent": "This rather lot here, but basically.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The plan is very.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard exercise, it's it's just that we're solving.",
                    "label": 0
                },
                {
                    "sent": "Grad squared F = 0.",
                    "label": 0
                },
                {
                    "sent": "Interpret that as being anything you want, but you can think of this being as as heat flow, so the simulation has got a metal plate which is being heated round the edges and you're you're finding out what the heat flow is.",
                    "label": 0
                },
                {
                    "sent": "But in fact.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Laplace equation states that each gridpoint the average of its four neighbours, so that if you discretize a 2D.",
                    "label": 1
                },
                {
                    "sent": "Grad Squared, 22D second derivative.",
                    "label": 0
                },
                {
                    "sent": "All you do in this in this exercise is you loop over every point in the grid and replace each point by the average rates for neighbors and then you do that again and again, and again, and again, and remarkably, that converges to the correct solution.",
                    "label": 0
                },
                {
                    "sent": "You just each iteration replace AIJ iteration K plus one with a I -- 1 I plus one I J -- 1 idea plus one.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the same structure as the traffic model.",
                    "label": 0
                },
                {
                    "sent": "The state of each cell at the next situation to pay depends on the state of.",
                    "label": 0
                },
                {
                    "sent": "It's for the well itself and its nearest neighbors at this iteration.",
                    "label": 0
                },
                {
                    "sent": "And so basically you stop when things when things stop changing you think well, if things aren't changing, I must be the right answer.",
                    "label": 0
                },
                {
                    "sent": "So it computes the Max.",
                    "label": 0
                },
                {
                    "sent": "It it looks at the maximum change, but it's really quite.",
                    "label": 0
                },
                {
                    "sent": "Quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The serial code is very simple.",
                    "label": 1
                },
                {
                    "sent": "The temperature is 1/4 of the temperature, so this is just like new Road is a function of old road.",
                    "label": 1
                },
                {
                    "sent": "I + 1 -- 1 J plus four J minus one in Fortran and Johns put the loops the right way round, I = J equals or Ji in Fortran, just just to be.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the source code.",
                    "label": 0
                },
                {
                    "sent": "We loop while the changes while the change is not too big.",
                    "label": 0
                },
                {
                    "sent": "So well, the changes is too big.",
                    "label": 0
                },
                {
                    "sent": "We update the new temperature.",
                    "label": 0
                },
                {
                    "sent": "So this is this is new road equals a function of old road.",
                    "label": 0
                },
                {
                    "sent": "We then check if we have converged.",
                    "label": 0
                },
                {
                    "sent": "We look at the difference between the new and the old arrays and we take the maximum across all the arrays and then we do a copy back and we do it in the same loop.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                },
                {
                    "sent": "So this is just you know.",
                    "label": 0
                },
                {
                    "sent": "Update Rd, copy back and compute the velocity that the compute the number of moves we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update.",
                    "label": 0
                },
                {
                    "sent": "We also we sweep across and we look for the maximum temperature change, but we also do the copy back at the same time and then we do some output every now and again.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aw man, there's some fun.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is there?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way that the code is written in, so I'm giving you a working.",
                    "label": 0
                },
                {
                    "sent": "It's John's gonna give you the working MPI code.",
                    "label": 0
                },
                {
                    "sent": "This is working open MP code there as well, but the MPI code.",
                    "label": 0
                },
                {
                    "sent": "Doesn't do a 2D decomposition of the domain, what it?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cause is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It does a slice wise decomposition.",
                    "label": 0
                },
                {
                    "sent": "And so this is much simpler.",
                    "label": 0
                },
                {
                    "sent": "Again, like the traffic model, you just have two neighbors up and down.",
                    "label": 0
                },
                {
                    "sent": "You have to swap data up and down.",
                    "label": 1
                },
                {
                    "sent": "The only difference is in the traffic model, the boundary date was a single element to each edge, and here the boundary data is a whole slice, so it's what data point here.",
                    "label": 0
                },
                {
                    "sent": "You need values from your four neighbors.",
                    "label": 0
                },
                {
                    "sent": "So in the interior that's fine.",
                    "label": 0
                },
                {
                    "sent": "But at the edges you need to value from from from from from your nearest neighbor, just like in the traffic model.",
                    "label": 0
                },
                {
                    "sent": "And so you swap Halos.",
                    "label": 0
                },
                {
                    "sent": "But the Halos are complete.",
                    "label": 0
                },
                {
                    "sent": "Labs of data.",
                    "label": 0
                },
                {
                    "sent": "Rather than a single element, but the communication partners the same send up send down, receive up, received an, except there's no periodic boundary conditions here.",
                    "label": 0
                },
                {
                    "sent": "This hard boundary conditions the top and bottom 'cause this is a plate, but that's that's that's really a technicality.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mean, it turns out in C It's much better to decompose the array in the in the first dimension.",
                    "label": 0
                },
                {
                    "sent": "And in Fortran, it's much.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better to decompose the array.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the second dimension, the reason is you can go around.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going through this that basically.",
                    "label": 0
                },
                {
                    "sent": "If you have a CRA.",
                    "label": 0
                },
                {
                    "sent": "Which is say 8 by 12 by draw it like a matrix in C, then it is it is the Rose which are contiguous in memory.",
                    "label": 0
                },
                {
                    "sent": "So the problem in MPI the fundamental way of sending a message you send this block of of data and that's fine.",
                    "label": 0
                },
                {
                    "sent": "If you want to send that data that data that data that data, you just take the starting point in the address of three.",
                    "label": 0
                },
                {
                    "sent": "One which is there and you say I want said four integers to reach these four.",
                    "label": 0
                },
                {
                    "sent": "The problem in see if you want to send that one that one that one that one there not next to each other in memory there strided and there are way.",
                    "label": 0
                },
                {
                    "sent": "From doing that, but it's a bit of a hassle, so just means naturally the way that it's drawn here in.",
                    "label": 0
                },
                {
                    "sent": "See it's not more natural to send columns, but Fortran is transposed with respect to seasons.",
                    "label": 0
                },
                {
                    "sent": "Fortunate it's more natural to send Rose, and so in Fortran it's more natural to divide the column up.",
                    "label": 0
                },
                {
                    "sent": "To have whole columns on a processor and see it's more natural to have home rose and that just means that one is decomposed in slices and one is strips and weather.",
                    "label": 0
                },
                {
                    "sent": "And yeah, all depends on how you draw it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically sending multiple elements is you can just send a whole slice.",
                    "label": 1
                },
                {
                    "sent": "So and the code the code is there, the code is there, it works.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, in Fortran you declare, you slice it up into into vertical strips.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, it's just the same, so I think the codes fairly codes fairly.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the word fairly explanatory?",
                    "label": 0
                },
                {
                    "sent": "So there is a working code, the band.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conditions are set up for you, so don't bother about that.",
                    "label": 0
                },
                {
                    "sent": "They're done in parallel, so that's all OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually I've given you Laplace MPI dot seen Laplacian PDF 90.",
                    "label": 0
                },
                {
                    "sent": "I don't actually I would just start from these.",
                    "label": 1
                },
                {
                    "sent": "You know these are working quite naive MPI codes which will run.",
                    "label": 1
                },
                {
                    "sent": "But you might want to improve them and there's a number of ways you can improve them.",
                    "label": 0
                },
                {
                    "sent": "You might want to improve the Halo swapping 'cause it just use MPI send.",
                    "label": 0
                },
                {
                    "sent": "MPI Recv doesn't use nonblocking or send receive.",
                    "label": 0
                },
                {
                    "sent": "Or you could you could introduce open MP and say I will and I'll talk about how to that next edge, how to combine MPI and open MP.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I won't go through this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do stuff with not going to do this.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't, I think.",
                    "label": 0
                },
                {
                    "sent": "This stuff I wouldn't bother with.",
                    "label": 0
                },
                {
                    "sent": "I have to say I think I would just maybe play around again, look at what the suggestions were for the traffic model and see if they make any difference here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but I've given you.",
                    "label": 0
                },
                {
                    "sent": "This is really instructions for if you don't have a working NPI cover and giving you a working MPI codes as John said when he ran this previously people had a lot longer to work on it, so I want to give you a bit more head start.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you know you're correct?",
                    "label": 0
                },
                {
                    "sent": "The solution converges at three 372 time steps on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fortunately, that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enough becausw.",
                    "label": 0
                },
                {
                    "sent": "All the action is in is in the bottom right hand corner, and even if you disable the MPI routines you see you get these artifacts because you're not communicating between top and bottom.",
                    "label": 1
                },
                {
                    "sent": "We actually get the right answer down here.",
                    "label": 0
                },
                {
                    "sent": "It's just a slight something like work.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you should actually.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Print out the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual value.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I wouldn't bother doing this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But John was playing around so that Johns picked the point here.",
                    "label": 0
                },
                {
                    "sent": "This magic point that he that he that he gave the coordinates of.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, some of this stuff is slightly out of date.",
                    "label": 0
                },
                {
                    "sent": "That's for last year, but you should have all that, but I would work on there's Laplace MPI dot C or Laplace MPI dot F-90 and then that will run out the box and then the idea is to try and improve it.",
                    "label": 0
                },
                {
                    "sent": "So that was really the and all the stuff is up there.",
                    "label": 0
                },
                {
                    "sent": "And what?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Is that the new Intel compilers they decided to make the default?",
                    "label": 0
                },
                {
                    "sent": "We also allowed to go to do so PICC an NPC XX by default goes OK. Oh minus minus.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The PC.",
                    "label": 0
                },
                {
                    "sent": "Is MPI ICC for the season?",
                    "label": 0
                },
                {
                    "sent": "What is MPI Fort?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Documentation has them all listed there, so I think if we just look at the if I do the makefile.",
                    "label": 0
                },
                {
                    "sent": "MPI ICC, I guess.",
                    "label": 0
                },
                {
                    "sent": "Yep, that's right.",
                    "label": 0
                },
                {
                    "sent": "And if I do.",
                    "label": 0
                },
                {
                    "sent": "MPI four yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that seems to work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so OK, so it's NPC at MPI Fort.",
                    "label": 0
                },
                {
                    "sent": "Are your friends here?",
                    "label": 0
                },
                {
                    "sent": "So I didn't want to eat too, but is that?",
                    "label": 0
                },
                {
                    "sent": "Is it sort of Clearwater we please ask me any questions around?",
                    "label": 0
                },
                {
                    "sent": "I'm around for the rest of the thing, but I think good front up.",
                    "label": 0
                },
                {
                    "sent": "As I said, I've really no idea how much effect I mean.",
                    "label": 0
                },
                {
                    "sent": "I would recommend replacing send and receive with send receive or I send I receive and see if that makes any difference.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "There may be some.",
                    "label": 0
                },
                {
                    "sent": "There may be some MPI settings if you look at the man pages, but I think the next thing to do would try hybrid MPI Open MP.",
                    "label": 0
                },
                {
                    "sent": "Now that may be challenging, but it's definitely.",
                    "label": 0
                },
                {
                    "sent": "It will be definitely fun to do so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about that now.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Two things.",
                    "label": 0
                },
                {
                    "sent": "And I do some further topic, I'll just do.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some selected.",
                    "label": 0
                },
                {
                    "sent": "I'll just do some selected further topics and then I'll do the hybrid.",
                    "label": 1
                },
                {
                    "sent": "So this is really a few bits and pieces that you might be interested.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then nested parallelism.",
                    "label": 0
                },
                {
                    "sent": "I thought put up previous.",
                    "label": 0
                },
                {
                    "sent": "I mean open MP was actually one like MPI actually open MP was a perfect example of how things should work.",
                    "label": 0
                },
                {
                    "sent": "Shared memory of computers were invented.",
                    "label": 0
                },
                {
                    "sent": "Lots of people came up with ideas for ways of doing directive based shared memory parallelism.",
                    "label": 0
                },
                {
                    "sent": "And then after three or four years, people got really annoyed that everyone was different.",
                    "label": 0
                },
                {
                    "sent": "They all got together and came up with the unified standard, which incorporates all the best ideas from everything which was the same as MPI but most previous directory systems, nested parallelism, so I didn't have parallelism enabled.",
                    "label": 1
                },
                {
                    "sent": "So OMP nested environment variable.",
                    "label": 0
                },
                {
                    "sent": "Or you can.",
                    "label": 0
                },
                {
                    "sent": "I would export OMP nested equals.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "So if a parallel directors account with another product review team with Red will be created.",
                    "label": 1
                },
                {
                    "sent": "However, by default if you have OMP nested equals false and you do MP parallel OMP parallel, the first one will use up all the threads in the Second World.",
                    "label": 0
                },
                {
                    "sent": "Effectively be ignored for the code will run, but not, you know.",
                    "label": 0
                },
                {
                    "sent": "But in a kind of default way.",
                    "label": 0
                },
                {
                    "sent": "So I didn't talk about sections because that's.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, that interest.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But but but it's really.",
                    "label": 0
                },
                {
                    "sent": "This is the useful thing.",
                    "label": 0
                },
                {
                    "sent": "This is a classic place where.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could do collapse, but you're saying look?",
                    "label": 0
                },
                {
                    "sent": "I want to paralyze both loops, but there's only four here, so I'll do parallel do NUM threads for that uses 4 threads here and then I have to.",
                    "label": 0
                },
                {
                    "sent": "It's a bit ugly 'cause you have to specify the number of threads down here, but actually this this NUM threads cause supersedes the value that we set by with OMP NUM threads.",
                    "label": 1
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "So this overrides RMP NUM threads.",
                    "label": 0
                },
                {
                    "sent": "You need to be careful to get this right.",
                    "label": 0
                },
                {
                    "sent": "So it's all a bit, it's just a bit horrible because you have to say I use 4 here now.",
                    "label": 1
                },
                {
                    "sent": "Used his toaster threads over.",
                    "label": 0
                },
                {
                    "sent": "We have to be very explicit so it's a bit.",
                    "label": 0
                },
                {
                    "sent": "It's a bit messy, but you can do it.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Orphan directives are very useful are one of the exercises not explain what they are so basically.",
                    "label": 0
                },
                {
                    "sent": "This so this OK is allowed in open MP, so this is you call a subroutine Fred here and you have do I want to end.",
                    "label": 0
                },
                {
                    "sent": "And you do MP do so.",
                    "label": 0
                },
                {
                    "sent": "MP DO says split this loop up amongst the available threads, but as it stands K it looks like this that this isn't in the parallel region.",
                    "label": 0
                },
                {
                    "sent": "OK, but the point is parallel regions persist, so if you have OMP parallel calls, Freden parallel Fred is called multiple times to take four, and the runtime remembers that you're in a parallel region.",
                    "label": 1
                },
                {
                    "sent": "So if this is called from a serial region, this just works in cereal.",
                    "label": 0
                },
                {
                    "sent": "If this is called from a parallel region, it works in parallel.",
                    "label": 0
                },
                {
                    "sent": "So, but it's important if I do the parallelization.",
                    "label": 0
                },
                {
                    "sent": "If this was an MP parallel do down here I would, and this wasn't here.",
                    "label": 0
                },
                {
                    "sent": "If the parallel directors down here, I would call one, I would call one copy of Fred and then that would split the loop into four threads.",
                    "label": 0
                },
                {
                    "sent": "Here I'm calling 4 individual, making four calls to the function thread.",
                    "label": 0
                },
                {
                    "sent": "All of Fred, all of which are are independent.",
                    "label": 0
                },
                {
                    "sent": "But this OMP do causes them to split up the loop amongst each other, so it is actually a different model.",
                    "label": 0
                },
                {
                    "sent": "But the reason why this model is actually.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "While this model is surprisingly good, is that.",
                    "label": 0
                },
                {
                    "sent": "If you have lots and lots of private variables, you sometimes do up here you have to earn P parallel private and you have to about 50 private variables and it's really annoying.",
                    "label": 0
                },
                {
                    "sent": "But if you declare variables locally within a function, they declared private to the funk you called the function four times.",
                    "label": 0
                },
                {
                    "sent": "Each function is saying integer J integer K there by default private.",
                    "label": 0
                },
                {
                    "sent": "There they are created on the stack.",
                    "label": 0
                },
                {
                    "sent": "If that's the way you like to think about things so by by using orphan directors pushing your count.",
                    "label": 0
                },
                {
                    "sent": "Computations down into functions or subroutines and declaring the local variables locally in the subroutine or function your data scoping.",
                    "label": 0
                },
                {
                    "sent": "The code can look a lot more elegant.",
                    "label": 0
                },
                {
                    "sent": "The data scoping just actually works out, so you almost never have to say private because you just make sure you got lots of private variables.",
                    "label": 0
                },
                {
                    "sent": "You push that function into a subroutine, you put operations a subroutine and declare them locally here and there, but by default private there are some rules about.",
                    "label": 0
                },
                {
                    "sent": "It can be complicated.",
                    "label": 0
                },
                {
                    "sent": "This up scrolls, but datascope attributes and what happens to the data scoping of the?",
                    "label": 0
                },
                {
                    "sent": "Arguments of the function and it gets a bit complicated, but.",
                    "label": 0
                },
                {
                    "sent": "It kind of these complicated rules, but basically it kind of works out that the real problem comes with.",
                    "label": 0
                },
                {
                    "sent": "With C Becausw in C. Um?",
                    "label": 0
                },
                {
                    "sent": "If you've got a shared variable in Fortran and you pass it to a function.",
                    "label": 0
                },
                {
                    "sent": "Then it's still it's still shared.",
                    "label": 0
                },
                {
                    "sent": "It's hard to explain 'cause it 'cause Fortran passes by reference always passes by reference.",
                    "label": 0
                },
                {
                    "sent": "But but see passes by value so.",
                    "label": 0
                },
                {
                    "sent": "You you can't pass the shared variable through through a function call.",
                    "label": 0
                },
                {
                    "sent": "You might think.",
                    "label": 0
                },
                {
                    "sent": "Well, I didn't take the address of it, but then each function gets its own.",
                    "label": 0
                },
                {
                    "sent": "Private copy of the same.",
                    "label": 0
                },
                {
                    "sent": "Pointer, which isn't the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's a bit weird, and if anyone so in C doing the update Rd function in with nested with orphan directives is quite challenging where it does your head in a bit, because because of the data scrubbing in Fortran, it's easy in C because C always passes by value.",
                    "label": 0
                },
                {
                    "sent": "Its approach light problem, but anyway, but you should nest in power.",
                    "label": 0
                },
                {
                    "sent": "Programming using offered directives is a really good way to make sure you understand open MP 'cause it's quite.",
                    "label": 0
                },
                {
                    "sent": "It's quite I find it quite and slightly challenging Luckily.",
                    "label": 0
                },
                {
                    "sent": "In the same corridor is me is is Mark Bull, who was involved in writing the standard, so I just go and ask him, but that's the times I've gone had to speak to him most often when I'm doing often directives, 'cause it it's more complex.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might think.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thread private global variables.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about this, but.",
                    "label": 0
                },
                {
                    "sent": "There's a real problem that sometimes people like to stick stick data in global global scope, so that's that's module data or common blocks in Fortran or in C, It's stuff which is declared outside of a function.",
                    "label": 0
                },
                {
                    "sent": "So extern type variables.",
                    "label": 0
                },
                {
                    "sent": "It's a disgusting programming style you should never do it, but there's a problem whereby you might want each thread to have its own copy of this global data if they're using it for scratch storage.",
                    "label": 0
                },
                {
                    "sent": "If you're just using it as, you might want each thread have its own copy, and that's called thread private global variables.",
                    "label": 0
                },
                {
                    "sent": "It is, but basically you should not have global data in your in your program at all.",
                    "label": 0
                },
                {
                    "sent": "To disgusting programming style.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you just but anyway, but maybe you're programming somebody else.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you don't member.",
                    "label": 0
                },
                {
                    "sent": "You may be having to paralyze somebody elses code who uses this thing so, but you know, global variables are just.",
                    "label": 0
                },
                {
                    "sent": "Just horrible so they.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Syntax is equally horrible, so again you need.",
                    "label": 0
                },
                {
                    "sent": "This is the only time I've read the Open MP standard in any in any detail.",
                    "label": 0
                },
                {
                    "sent": "Was trying to paralyze a code where they had where they had global variables in global storage and it's just horrible.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Copying is another thing I would cover that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Timing routines open MP has.",
                    "label": 0
                },
                {
                    "sent": "Nice timing routine called OMP get W time so you can get the wall Clock time if you want to time something there's an equivalent routine MPI called MPI W Time.",
                    "label": 0
                },
                {
                    "sent": "It just allows you to time functions really easily.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I should use it.",
                    "label": 0
                },
                {
                    "sent": "I think I use it in the traffic model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you just do this start time, get time, and then you time you subtract it so they're quite quite useful.",
                    "label": 0
                },
                {
                    "sent": "So that was just a run through.",
                    "label": 0
                },
                {
                    "sent": "This is other stuff you want.",
                    "label": 0
                },
                {
                    "sent": "Might want to know exists and therefore.",
                    "label": 0
                },
                {
                    "sent": "And therefore I want to look at.",
                    "label": 0
                },
                {
                    "sent": "So I think the other thing which I wanted to just very briefly cover for going on to hybrid programming was.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to do it in any detail, but in fact I don't actually have.",
                    "label": 0
                },
                {
                    "sent": "I'll need to put it up on the.",
                    "label": 0
                },
                {
                    "sent": "Up on the.",
                    "label": 0
                },
                {
                    "sent": "So apologies for sale.",
                    "label": 0
                },
                {
                    "sent": "Need to put this up on the wiki, but I thought it would be useful just to briefly mention how.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I just have to find this.",
                    "label": 0
                },
                {
                    "sent": "I'll put this up on the.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know everything I've talked about so far.",
                    "label": 0
                },
                {
                    "sent": "As has been about loops, you have a big loop and you want to flatten out for a large class of scientific and technical codes that that's that's useful, but increasingly people looking at task parallelism, saying you know I'm actually defining my problem in terms of a set of independent, not set of tasks which may have dependencies between them, and I want them to be executed and so.",
                    "label": 0
                },
                {
                    "sent": "Open MP was extended to include tasks it it can be quite useful.",
                    "label": 0
                },
                {
                    "sent": "Until recently, a lot of the implementations weren't particularly efficient, but the way it works.",
                    "label": 0
                },
                {
                    "sent": "My battery is optional.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's surprisingly so that the task.",
                    "label": 0
                },
                {
                    "sent": "Concept defines a section of code.",
                    "label": 0
                },
                {
                    "sent": "And inside the parallel Rethreading County attachment packet, the task for execution and some thread in their original issue that at some point in the future.",
                    "label": 0
                },
                {
                    "sent": "So the point about loop parallelism as we have one to one mapping between work that we want to do and threads you say.",
                    "label": 0
                },
                {
                    "sent": "Here's a loop.",
                    "label": 0
                },
                {
                    "sent": "I want iterations 5 to 7 to be done by that thread 8 to 10 to be done by that thread, OK.",
                    "label": 0
                },
                {
                    "sent": "Task.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Get rid of that to.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can generate as many tasks as you want and they will be executed in parallel by the threads, but you don't know what the mapping is.",
                    "label": 0
                },
                {
                    "sent": "A much more generic model, so the model of tasks that you deliberately generate more tasks than there are threads to allow there to be load balancing and a clever runtime system will run the tasks in a way to balance the load.",
                    "label": 0
                },
                {
                    "sent": "And the syntax is really quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "You just have a piece of code and you say OMP task.",
                    "label": 0
                },
                {
                    "sent": "And that's a task.",
                    "label": 0
                },
                {
                    "sent": "And basically it's magic.",
                    "label": 0
                },
                {
                    "sent": "It's executed so for example.",
                    "label": 0
                },
                {
                    "sent": "So this is my battery is not.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You could add.",
                    "label": 0
                },
                {
                    "sent": "The scoping can be a bit difficult, but for example, you might have a task which computes some things and what this does is this doesn't.",
                    "label": 0
                },
                {
                    "sent": "This doesn't execute the task.",
                    "label": 0
                },
                {
                    "sent": "This creates the task which is dispatched to the runtime system and the task is executed.",
                    "label": 0
                },
                {
                    "sent": "Sometime in the future, and it is guaranteed to be completed by the next synchronization point by the next barrier or end of a parallel.",
                    "label": 0
                },
                {
                    "sent": "Reads are typically what you do is you start the parallel region, create lots of tasks, end the parallel region and then at that point they would all be processed so they're guaranteed to be computed to be to be completed sometime in the future, but they're not, they're not.",
                    "label": 0
                },
                {
                    "sent": "They're not done immediately.",
                    "label": 0
                },
                {
                    "sent": "The OMP task construct does not execute the task, it creates the task and submits it for execution.",
                    "label": 1
                },
                {
                    "sent": "I actually think it's more the shadowing from this, actually, 'cause I've stupidly put it on this side, I think rather bike and try.",
                    "label": 0
                },
                {
                    "sent": "Actually that might be useful.",
                    "label": 0
                },
                {
                    "sent": "Or these are the wrong ones OK?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "When a task completed with that thread barriers explicit or implicit or so, that's the normal way.",
                    "label": 0
                },
                {
                    "sent": "Or you can you can do a task, wait, you can do.",
                    "label": 0
                },
                {
                    "sent": "There's a task wait Directive which says OK, wait and tall all the task generating the current task have been have been completed and this allows you to do quite general things.",
                    "label": 0
                },
                {
                    "sent": "There are some examples here, but.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It would have time to go into them, but this is a useful thing to do, so this can't be paralyzed.",
                    "label": 0
                },
                {
                    "sent": "You can open MP loop account.",
                    "label": 0
                },
                {
                    "sent": "You have a list while.",
                    "label": 0
                },
                {
                    "sent": "While there's another item in the list, you process that item and go on to the next item.",
                    "label": 1
                },
                {
                    "sent": "So you just have to put a link list of tasks and then which linked list of structures each of which is a task, and you're just reversing it and executing them.",
                    "label": 0
                },
                {
                    "sent": "Or there's no loop here.",
                    "label": 0
                },
                {
                    "sent": "There's no for.",
                    "label": 0
                },
                {
                    "sent": "You can't paralyze this with open MP easily.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you do is you.",
                    "label": 0
                },
                {
                    "sent": "You do hashtag where OMP parallel, so you start to parallel region.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of threads here and this gets slightly slightly weird.",
                    "label": 0
                },
                {
                    "sent": "But what we're doing here is we're only.",
                    "label": 0
                },
                {
                    "sent": "Traversing the list is a serial operation.",
                    "label": 1
                },
                {
                    "sent": "OK, you can only go to the next.",
                    "label": 0
                },
                {
                    "sent": "The next linked list item having having been at the current one, you can only go one way through, so you have to generate the tasks in cereal so you have one thread which which ripped through the linked list and generates the task, but unlike here is not actually processing them, is just generating the tasks.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 thread which is in a single hashtag, one piece single zip through.",
                    "label": 0
                },
                {
                    "sent": "The linked list.",
                    "label": 0
                },
                {
                    "sent": "But don't actually process it.",
                    "label": 0
                },
                {
                    "sent": "Just generate a task.",
                    "label": 0
                },
                {
                    "sent": "So all this is do is hopefully zipping through the link list and submitting all these tasks, and while they're being submitted, the other threads 'cause member, you have multiple threads because you're in a parallel region and only one thread is generating the task.",
                    "label": 0
                },
                {
                    "sent": "The other threads will be picking them up and processing them.",
                    "label": 0
                },
                {
                    "sent": "But it's guaranteed they'll all be done at the end of here.",
                    "label": 0
                },
                {
                    "sent": "So that's, uh, you know, that's a reasonable example of how to do it.",
                    "label": 0
                },
                {
                    "sent": "It looks quite natural, but in most systems this is actually quite isn't executed particularly far.",
                    "label": 0
                },
                {
                    "sent": "I mean it's very difficult to process.",
                    "label": 0
                },
                {
                    "sent": "At some point the runtime system has to be clever.",
                    "label": 0
                },
                {
                    "sent": "It has to say you've generated 50,000 tasks, but you just gotta stop now, right?",
                    "label": 0
                },
                {
                    "sent": "So I have to stop this and go into a phase right process is them and then say right right?",
                    "label": 0
                },
                {
                    "sent": "OK, I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got some space down.",
                    "label": 0
                },
                {
                    "sent": "Go back and there's a lot of machinery under the hood which needs to go on here.",
                    "label": 0
                },
                {
                    "sent": "But that is there's other examples here, but this is really the.",
                    "label": 0
                },
                {
                    "sent": "This is really the one which illustrates how it worked.",
                    "label": 0
                },
                {
                    "sent": "So the important point is this does not process the task, it submits this function to be executed as a task.",
                    "label": 0
                },
                {
                    "sent": "The tasks here are generated in cereal by a single thread.",
                    "label": 0
                },
                {
                    "sent": "They have to because point at this link pointer chasing the linked list is a serial operation, but all the other threads are available to process the task.",
                    "label": 0
                },
                {
                    "sent": "You don't know who does what and they just pick them up and it's all magic, but this does work, but it is.",
                    "label": 0
                },
                {
                    "sent": "It's useful in principle, but in practice you should use it with care.",
                    "label": 0
                },
                {
                    "sent": "What know you should always test the performance.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "The other thing you can do is you could actually.",
                    "label": 0
                },
                {
                    "sent": "You could just create an array.",
                    "label": 0
                },
                {
                    "sent": "OK, you could traverse the linked list and just populate the array with the pointers and you could then do a for loop oh MP4 process, PIK, and make that a dynamic for loop that can actually be faster.",
                    "label": 0
                },
                {
                    "sent": "In practice it's a bit ugly.",
                    "label": 0
                },
                {
                    "sent": "This is the elegant way of doing it, but it may be it may not be.",
                    "label": 0
                },
                {
                    "sent": "The most efficient so that I just wanted to tell you that tasks exist without being quite a major.",
                    "label": 0
                },
                {
                    "sent": "I say recent, but probably in the last 10 years between fire and extension to open MP it was seen as quite a.",
                    "label": 0
                },
                {
                    "sent": "A weak spot in the standard.",
                    "label": 1
                },
                {
                    "sent": "So before I fell, I'll just.",
                    "label": 0
                },
                {
                    "sent": "Do the last lecture I'll talk about hybrid programming.",
                    "label": 0
                },
                {
                    "sent": "Which should be on.",
                    "label": 0
                },
                {
                    "sent": "I'll put that slide back up on the.",
                    "label": 0
                },
                {
                    "sent": "Where is it this?",
                    "label": 0
                },
                {
                    "sent": "One is on the wiki.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So open MP and MPI.",
                    "label": 0
                },
                {
                    "sent": "We see the MPI is based on processes.",
                    "label": 0
                },
                {
                    "sent": "You create multiple processes.",
                    "label": 0
                },
                {
                    "sent": "Each process is independent from the other and can't share memory.",
                    "label": 0
                },
                {
                    "sent": "We've seen that that that open MP you launch a single process and that process can at runtime spawn multiple threads.",
                    "label": 0
                },
                {
                    "sent": "Well, can't you do both?",
                    "label": 0
                },
                {
                    "sent": "And yes you can.",
                    "label": 0
                },
                {
                    "sent": "There's no reason not to do.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both so as I said in the recent years, been tend towards clustered architectures.",
                    "label": 0
                },
                {
                    "sent": "In the old days it was much simpler.",
                    "label": 0
                },
                {
                    "sent": "You either bought a distributed memory machine which was lots of single core processors, or you bought a shared memory machine which was a large number which was a multicore machine.",
                    "label": 0
                },
                {
                    "sent": "Now every distributed machine, every large scale machine is a collection of nodes like these laptops which have multiple cores in them.",
                    "label": 0
                },
                {
                    "sent": "So there's no reason not to not to combine the two models.",
                    "label": 0
                },
                {
                    "sent": "So I said distributed memory systems for each node consist traditional shared memory multiprocessor or multicore system.",
                    "label": 0
                },
                {
                    "sent": "A single address space with each node but separate notes have separate address spaces that that's our our standard cluster of laptops model.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's like this you have.",
                    "label": 0
                },
                {
                    "sent": "This would be this is some memory.",
                    "label": 0
                },
                {
                    "sent": "You have multiple.",
                    "label": 0
                },
                {
                    "sent": "This is 1 node.",
                    "label": 0
                },
                {
                    "sent": "This is another node, another node, another across.",
                    "label": 0
                },
                {
                    "sent": "Some indicates this will be 4 laptops connected together by some interconnect.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how should we process program such a machine where you can use MPI across the whole system and that's the default way if you do.",
                    "label": 0
                },
                {
                    "sent": "On on on bridges.",
                    "label": 0
                },
                {
                    "sent": "If you did MPI run minus N 56 you would get 28 court processes on node 128 processes on node 2.",
                    "label": 0
                },
                {
                    "sent": "We count in general use open MP threads across the whole system.",
                    "label": 0
                },
                {
                    "sent": "You can support this in software and hardware, but it's very expensive, so it's not really it done.",
                    "label": 0
                },
                {
                    "sent": "But could we use open MP threads within a node?",
                    "label": 0
                },
                {
                    "sent": "In MPI between the nodes is now the question you have to do.",
                    "label": 0
                },
                {
                    "sent": "Is there any advantage to this so the worst thing that people do is they have their code.",
                    "label": 0
                },
                {
                    "sent": "They are not satisfied with the scaling of it, they jump to the conclusion that it's.",
                    "label": 0
                },
                {
                    "sent": "A problem with MPI, so then they spend six months writing a hybrid MPI Open MP code.",
                    "label": 0
                },
                {
                    "sent": "Then they come back and they run.",
                    "label": 0
                },
                {
                    "sent": "It's not any faster, so you know you really need to.",
                    "label": 0
                },
                {
                    "sent": "You need to say to somebody, why do you think your code will be faster with hybrid MPI Open MP?",
                    "label": 0
                },
                {
                    "sent": "Or yes, you need to ask that question 'cause you know it's not magic.",
                    "label": 0
                },
                {
                    "sent": "But there can be advantages.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to consider development, maintenance, cost, portability, and performance, so these are negative.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Development will be harder for the MPI code and much harder than from the Open MP code.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'll say the development of a hybrid code is harder than open MP code, and much harder harder than NP.",
                    "label": 0
                },
                {
                    "sent": "I could have a child with open MP code, but probably if over here already written the MPI code, you're using multiple nodes, you must have written the MPI codes.",
                    "label": 0
                },
                {
                    "sent": "The question is how much extra effort is it to add in open MP so?",
                    "label": 0
                },
                {
                    "sent": "That's possible now.",
                    "label": 0
                },
                {
                    "sent": "The classic ones in some cases maybe possibly similar MPI implementation with the need for scalability is reduced, EG 01 D. Domain key comes off the composition instead of 2D, so for example.",
                    "label": 0
                },
                {
                    "sent": "If you take this this this.",
                    "label": 0
                },
                {
                    "sent": "Laplace problem that we're going to do for the.",
                    "label": 0
                },
                {
                    "sent": "That's a good time to bring up that slide, actually.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cake.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to this is my domain decomposition here OK if the grid is 1000 by 1000 K?",
                    "label": 0
                },
                {
                    "sent": "What's the maximum number of MPI processes I can use?",
                    "label": 0
                },
                {
                    "sent": "In this decomposition.",
                    "label": 0
                },
                {
                    "sent": "It's not a trick 1000 exactly 'cause even at 1000 each.",
                    "label": 0
                },
                {
                    "sent": "Each MPI process is going to have a single strip, so you're going to be sending twice as much data as you own.",
                    "label": 0
                },
                {
                    "sent": "You know so clearly, but this is much simpler to write.",
                    "label": 0
                },
                {
                    "sent": "So what you should, you might say that they have to do a 2D decomposition if I want to use 1024 processes, I have to do a 32 by 32 decomposition that work, but this is a lot harder to write, so this is hard to write in MPI, harder to write.",
                    "label": 0
                },
                {
                    "sent": "This is simple.",
                    "label": 0
                },
                {
                    "sent": "OK, so you might say R. Here's the trick.",
                    "label": 0
                },
                {
                    "sent": "OK, if I use hybrid MPI open MP OK then?",
                    "label": 0
                },
                {
                    "sent": "Then basically I can run on.",
                    "label": 0
                },
                {
                    "sent": "I could, my limit is then 1000 nodes OK?",
                    "label": 0
                },
                {
                    "sent": "I could use each node could have a single strip, but then within the node I could split that up using open MP OK.",
                    "label": 0
                },
                {
                    "sent": "So because there are 28 cores in a node using hybrid MPI open MP you can improve the scalability of this by a factor of 28.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because so that say you've got two choices here.",
                    "label": 0
                },
                {
                    "sent": "One is user simple MPI decomposition but extend it.",
                    "label": 0
                },
                {
                    "sent": "By a factor of number of processes per node, by using open MP or.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could do a more difficult MPI decomposition, so that's really that's really up to you, but your whole code may be completely based on this, and you may think so in that that's a reasonable example and the real reason the level where people do that the the place where people do that and I don't have time to go into details, but is when people.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did for a transforms.",
                    "label": 0
                },
                {
                    "sent": "Which a lot of scientific and technical codes do you have these limitations that that slice wise decomposition is very simple to write, but the more complicated decomposition isn't possibly a nightmare to write.",
                    "label": 0
                },
                {
                    "sent": "So this really happens.",
                    "label": 0
                },
                {
                    "sent": "So a 1D decomposition might, instead of a 2D, so it's it may be possible to simply implementation because the need for scalability is reduced.",
                    "label": 0
                },
                {
                    "sent": "The MPI can scale to number of nodes rather than number of cores, and there's a factor of a large between them.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "NPR, NPR put themselves highly force, but not perfect.",
                    "label": 1
                },
                {
                    "sent": "Combined MPI Open MP is less portable and the the real problem is thread safety of MPI.",
                    "label": 0
                },
                {
                    "sent": "Basically MPI.",
                    "label": 0
                },
                {
                    "sent": "The guys that wrote the MPI library assumed it was only being called by single thread at a time by a single process.",
                    "label": 0
                },
                {
                    "sent": "So you might have a data structure in MPI.",
                    "label": 0
                },
                {
                    "sent": "But if multiple threads are going to call the MPI library at the same time, then that you could all help you know who knows what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the data structures can be completely corrupted.",
                    "label": 0
                },
                {
                    "sent": "So it's very difficult.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to write an efficient threadsafe MPI library because you know they haven't been designed for that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And you may want to.",
                    "label": 0
                },
                {
                    "sent": "I mean, you maybe want to have a code which can run as a standalone MPI, 'cause that's really possibly the.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issue, but the main thing is thread safety.",
                    "label": 0
                },
                {
                    "sent": "Making library threats that can be difficult.",
                    "label": 0
                },
                {
                    "sent": "You can lock access to data structure, but if they all add significant overhead, so you'd be really annoyed if you had a standard MPI code.",
                    "label": 0
                },
                {
                    "sent": "OK and the upgraded the operating system and the compilers on your MPI code went 10 times slower, and you say why, oh, 'cause we've made the MPI library thread safe and you're like, well, I don't want it to be.",
                    "label": 0
                },
                {
                    "sent": "Thread safe.",
                    "label": 0
                },
                {
                    "sent": "OK, so it needs to be a mechanism to allow those who want to exploit this functionality to exploit it.",
                    "label": 0
                },
                {
                    "sent": "But for those who don't need thread safety, which is most people who just run one process, MPI process, no threads.",
                    "label": 0
                },
                {
                    "sent": "They aren't hamper and what happens is that you could you, could you ask the MPI library when you initialize the MPI library?",
                    "label": 0
                },
                {
                    "sent": "Tell it look, I want to use the MPI library, but I want it.",
                    "label": 0
                },
                {
                    "sent": "I will need thread safety or I won't need thread safety and it can help.",
                    "label": 0
                },
                {
                    "sent": "You can have different implementations.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So perform so.",
                    "label": 0
                },
                {
                    "sent": "This is for reasons why you might want to mix the MPI Open MP code replicated data poorly scaling MPI code, limited MPI process numbers, or the MP implementation not tuned for these shared memory clusters.",
                    "label": 0
                },
                {
                    "sent": "Now go through these one replicated data.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The real issue?",
                    "label": 0
                },
                {
                    "sent": "Some MPI approaches are replicated data strategies.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you might have some look up table or some data structure which everybody needs to read all the time.",
                    "label": 0
                },
                {
                    "sent": "Some database or something and MPI to avoid vast amounts of communication.",
                    "label": 0
                },
                {
                    "sent": "You probably want each process to have.",
                    "label": 0
                },
                {
                    "sent": "Its own.",
                    "label": 0
                },
                {
                    "sent": "Each process will need to have its own copy of that data structure.",
                    "label": 0
                },
                {
                    "sent": "And so so on bridges.",
                    "label": 0
                },
                {
                    "sent": "If you run 28 processes per node, you'll have 28 copies of the data structure on the node, which is kind of crazy 'cause you know it's shared memory.",
                    "label": 0
                },
                {
                    "sent": "Why can't I have one copy per node and everyone read that?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not easy to do that in MPI.",
                    "label": 0
                },
                {
                    "sent": "You might say I don't have any replicated data where you probably do, 'cause when you have domain decomposition of replicated data, it's not so obvious in the traffic model because the Halos are so small, but the Halos are replicated data, their copies of data on another process.",
                    "label": 0
                },
                {
                    "sent": "Now in the 1D decomposition of the Halo.",
                    "label": 0
                },
                {
                    "sent": "It's very very small overhead, but it can become significant if you had a mixed hybrid MPI Open MP code.",
                    "label": 0
                },
                {
                    "sent": "The data structure could be shared by multiple threads within a process, and you wouldn't need a lot of extra storage, so this is becoming increasingly important because it might have memory per core, is not likely to increase.",
                    "label": 0
                },
                {
                    "sent": "People are sticking more and more cores.",
                    "label": 0
                },
                {
                    "sent": "I don't know how much.",
                    "label": 0
                },
                {
                    "sent": "How much a memory a Knights landing processor has, but it's going to be shared between you know hundreds of cores so that the amount of memory per core is going is dropping dramatically, so memory saving techniques will become quite quite important, so I said.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Halo regions are type of replicated data and it may not be obvious, but if you look at it.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "A um?",
                    "label": 0
                },
                {
                    "sent": "A local domain size of 50 ^3 OK and you had it.",
                    "label": 0
                },
                {
                    "sent": "This is a 3D decomposition and a lot of simulations are 3D.",
                    "label": 0
                },
                {
                    "sent": "OK if you have a Halo so in fact you actual array isn't 50 by 50 by 50 is 52 by 52 by 52 because you need to have a room on the face every face of the cube to store the incoming data 52 ^3 -- 50 ^3 is 11%.",
                    "label": 0
                },
                {
                    "sent": "So 11% of your data is already in Halos.",
                    "label": 0
                },
                {
                    "sent": "But then you say well I want to run on.",
                    "label": 0
                },
                {
                    "sent": "I want to run on more and more processes so I go up to this stage.",
                    "label": 0
                },
                {
                    "sent": "I'm running on more processes with the fixed problem size.",
                    "label": 0
                },
                {
                    "sent": "This is my local problem.",
                    "label": 0
                },
                {
                    "sent": "Size is 10 ^3 my eraser.",
                    "label": 0
                },
                {
                    "sent": "12 cubes.",
                    "label": 0
                },
                {
                    "sent": "You've already got almost 50.",
                    "label": 0
                },
                {
                    "sent": "You've already doubled your memory usage.",
                    "label": 0
                },
                {
                    "sent": "Because just because you have to add 1 onto every dimension and that that you know if you have to have Halos of depth two or more with some codes do this just come completely blow you.",
                    "label": 0
                },
                {
                    "sent": "And so the important point is that if you do a hybrid MPI open MP, if you run one MPI process per node, you only have one Halo per node, not one Halo per core, and so this becomes dramatically.",
                    "label": 0
                },
                {
                    "sent": "The overhead becomes dramatically smaller.",
                    "label": 0
                },
                {
                    "sent": "So this could be this is possibly the more significant.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other one is poorly getting MPI codes.",
                    "label": 0
                },
                {
                    "sent": "If the MPI, Russian, those scales poorly, the mixed mpongo could may scale better.",
                    "label": 0
                },
                {
                    "sent": "There can be algorithmic reasons load balancing in MPI is really difficult becausw.",
                    "label": 0
                },
                {
                    "sent": "If I have a task that I.",
                    "label": 0
                },
                {
                    "sent": "The I don't want to do and I want somebody else to do it.",
                    "label": 0
                },
                {
                    "sent": "I not only have to tell them to do the task, I have to give them the data.",
                    "label": 0
                },
                {
                    "sent": "So if you want to transfer work you have to transfer the data, which is quite difficult, but so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a big overhead in an open MP you don't have to transfer the data, all the data in shared memory.",
                    "label": 0
                },
                {
                    "sent": "If you want someone to do a task, you just say I can't be bothered doing it, you do it and they just walk in and do it.",
                    "label": 0
                },
                {
                    "sent": "So load balancing is much easier in open MP.",
                    "label": 0
                },
                {
                    "sent": "So it's possible that for adaptive, irregular problems, load balancing is better solved by hybrid MPI Open MP, however.",
                    "label": 0
                },
                {
                    "sent": "If you're running on multiple nodes, you still have to solve load balancing between the nodes yourself, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not guaranteed, but it's possible.",
                    "label": 0
                },
                {
                    "sent": "Simplicity reasons is often the real reason.",
                    "label": 0
                },
                {
                    "sent": "That's often the real reason why people so actually running out of memory and simplicity reasons.",
                    "label": 0
                },
                {
                    "sent": "The two driving factors for people doing hybrid MPI Open MP so often you'll find people are running fewer MPI processes than there are cores on a node, because they're saying I need a GB per core, there's 28 cores, but you've only got 10 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "I can't use all the cores, OK?",
                    "label": 0
                },
                {
                    "sent": "So even if there open MP implementation is a bit rubbish, even if it only goes a few percent faster.",
                    "label": 0
                },
                {
                    "sent": "It's worth doing because otherwise there's quarter going unused purely for memory reasons, and again, simplicity reasons people may say look, my algorithm is fundamentally doesn't scale beyond 2000 processes.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm my array is 2000 by 2000, and so I can extend it that way.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Load balancing as I've talked about.",
                    "label": 0
                },
                {
                    "sent": "Is easier, so that can be possible.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Limited MPI process numbers I mean.",
                    "label": 0
                },
                {
                    "sent": "MPI libraries may not be able to handle it processes adequately.",
                    "label": 0
                },
                {
                    "sent": "This is really a problem for MPI developers, but basically on very large systems it's going to become very difficult to write in MPI implementation, which which can scale to millions, for example example.",
                    "label": 0
                },
                {
                    "sent": "Matching wildcards it seems quite nice on MPI that you can do a wild card you can receive from anybody, but if that energy is 1 from a million, you know that's going to be a hell of a lot of searching.",
                    "label": 0
                },
                {
                    "sent": "Some of these wildcards can anytime that the library needs.",
                    "label": 0
                },
                {
                    "sent": "Storage which is.",
                    "label": 0
                },
                {
                    "sent": "Of order P, which scale the number of processes, then you're really, you're really.",
                    "label": 0
                },
                {
                    "sent": "It's really difficult so.",
                    "label": 0
                },
                {
                    "sent": "On very large systems, it may be that you know that the MPI library just kind of falls over for very large numbers, and you can just reduce that overhead by having some threads in there.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was true in the old days.",
                    "label": 0
                },
                {
                    "sent": "MPM did not change for SMP clusters.",
                    "label": 0
                },
                {
                    "sent": "This is lesser problem these days.",
                    "label": 0
                },
                {
                    "sent": "MPI libraries tend to be quite.",
                    "label": 0
                },
                {
                    "sent": "Sophisticated they will notice if two processes are on the same node and they will, they won't, they won't.",
                    "label": 0
                },
                {
                    "sent": "They won't talk to themselves by the network, they'll do it through the shared memory, however, so someone mentioned this the other day.",
                    "label": 0
                },
                {
                    "sent": "But people, however this is this.",
                    "label": 0
                },
                {
                    "sent": "This is a big issue, potentially.",
                    "label": 0
                },
                {
                    "sent": "Mixed mode code tends to migrate messages.",
                    "label": 0
                },
                {
                    "sent": "You typically send with a hybrid code, hybrid MPI open MP.",
                    "label": 0
                },
                {
                    "sent": "You're typically send one message per node, so you'll send a small number of large messages, whereas whereas the pure MPI code will send the large number of small messages.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if I run a pure MPI code on bridges, there are 20 M 28 MPI processes on each node.",
                    "label": 0
                },
                {
                    "sent": "Probably cannot connect to the same network interface.",
                    "label": 0
                },
                {
                    "sent": "The network interface is having to cope with 28 processes simultaneously sending out messages to send.",
                    "label": 0
                },
                {
                    "sent": "And that that that can really kill networks.",
                    "label": 0
                },
                {
                    "sent": "It's actually nicer if you aggregated them and said one big message.",
                    "label": 0
                },
                {
                    "sent": "Networks hates more messages, you know it's just.",
                    "label": 0
                },
                {
                    "sent": "It's just very very.",
                    "label": 0
                },
                {
                    "sent": "It's a difference between you got 100 people to go from between 2 cities that reset them 100 people each in their own car.",
                    "label": 0
                },
                {
                    "sent": "Or is 100 people in in a coach?",
                    "label": 0
                },
                {
                    "sent": "Of course, 100 people in the coach is much more efficient, but you know, hundred cars.",
                    "label": 0
                },
                {
                    "sent": "There's also congestion and stuff, so it's so this is a genuine issue.",
                    "label": 0
                },
                {
                    "sent": "Modern networks are getting better, but.",
                    "label": 0
                },
                {
                    "sent": "If you're on a cluster which maybe just has.",
                    "label": 0
                },
                {
                    "sent": "Ethernet or something?",
                    "label": 0
                },
                {
                    "sent": "This could be a real issue potentially.",
                    "label": 0
                },
                {
                    "sent": "So people always quote the bandwidth and latency of their network, but often the real question is how many messages per second can it?",
                    "label": 0
                },
                {
                    "sent": "Can it sustain?",
                    "label": 0
                },
                {
                    "sent": "Not, not really.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's many styles of mixed mode programming.",
                    "label": 0
                },
                {
                    "sent": "I'm only where they're going to go through master only and talk about the other ones.",
                    "label": 0
                },
                {
                    "sent": "Well, actually go through them quickly, but master only is really the simplest thing to do.",
                    "label": 0
                },
                {
                    "sent": "All the MPI communication takes place in the sequential part of the opening program.",
                    "label": 0
                },
                {
                    "sent": "There's no MPI in parallel regions, and this is really the most really the simplest way to do it.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have some work.",
                    "label": 0
                },
                {
                    "sent": "Then you call some operation and then you have some other work OK, so basically this is kind of the obvious thing to do.",
                    "label": 0
                },
                {
                    "sent": "You basically you accelerate the work, but you do all your MPI through.",
                    "label": 0
                },
                {
                    "sent": "Remember at this point only one thread which is the master thread which is the parent processes operating.",
                    "label": 0
                },
                {
                    "sent": "So this is just like an accelerated MPI code.",
                    "label": 0
                },
                {
                    "sent": "All the communications is the same when you accelerate the work and that that's the simplest.",
                    "label": 0
                },
                {
                    "sent": "Thing to do.",
                    "label": 0
                },
                {
                    "sent": "The MPI Library doesn't need to be thread aware or thread it, just it doesn't care.",
                    "label": 0
                },
                {
                    "sent": "Sing",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Italy funneled is the same in funneled.",
                    "label": 0
                },
                {
                    "sent": "You may call MPI within within a parallel region, but you make sure that it's only called on a single master thread, so this is just achieving the same thing.",
                    "label": 1
                },
                {
                    "sent": "So again only one thread.",
                    "label": 1
                },
                {
                    "sent": "The master thread is ever calling the MPI library, so again this works fine.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Serialized exactly the same thing, but again.",
                    "label": 0
                },
                {
                    "sent": "You, as a programmer, guarantee you're saying OK, multiple threads will call the MPI library, but they will never do it at the same time.",
                    "label": 0
                },
                {
                    "sent": "And again that means the MPI library doesn't really need to be to be thread safe, so you have to put your MPI calls in one of these critical regions, which I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "So all those work recently.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the one which which is very difficult as it's called multiple where basically any thread can call the MPI library anytime.",
                    "label": 0
                },
                {
                    "sent": "And this is this is the one which is difficult.",
                    "label": 0
                },
                {
                    "sent": "This is where the MPI library has to do a lot of work.",
                    "label": 0
                },
                {
                    "sent": "And so those are the three, the four versions.",
                    "label": 0
                },
                {
                    "sent": "What are they called?",
                    "label": 0
                },
                {
                    "sent": "Master only funnels serialized among?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if you're gonna, if you're going to run a hybrid MPI Open MP code rather than MPI and it you call MPI init thread and you tell it what kind of level of support you want, and they are.",
                    "label": 0
                },
                {
                    "sent": "It's a bit weird you ask.",
                    "label": 0
                },
                {
                    "sent": "You ask for a level which is 1, two, three or four and it tells you what it can provide, and it may give you back a lower number than you asked for.",
                    "label": 0
                },
                {
                    "sent": "You may say I want.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "MPI thread multiple that may come back and say I can't do that.",
                    "label": 0
                },
                {
                    "sent": "OK, but actually.",
                    "label": 0
                },
                {
                    "sent": "Most MPI implementations could support single funneled and probably serialized.",
                    "label": 0
                },
                {
                    "sent": "Yeah, can probably support all of these with very little overhead, but this is the one which is difficult, free for all.",
                    "label": 0
                },
                {
                    "sent": "Any thread can call any can call any MPI routine any actually this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, this is well this this is difficult.",
                    "label": 0
                },
                {
                    "sent": "The other problem.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The this is quite briefly, MPI stuff, but basically you asked for.",
                    "label": 0
                },
                {
                    "sent": "You asked for something that gives you back a number, which is the highest it can provide, which is, you need to check that it's providing what you want.",
                    "label": 0
                },
                {
                    "sent": "But I would really recommend at least definitely for the hybrid challenge.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Program in the master owning and in fact in the master only style.",
                    "label": 0
                },
                {
                    "sent": "You could just call MPI and that you don't need to call me on that thread because it's that you know it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's it's fine so I would really recommend just doing this style if you want to try it.",
                    "label": 0
                },
                {
                    "sent": "Don't bother with anything else.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are some issues though.",
                    "label": 0
                },
                {
                    "sent": "Where is this so?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a lot of, so I don't really want to go through all that you can.",
                    "label": 0
                },
                {
                    "sent": "There's a, there's lots of support.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Routines I can talk about lots of pitfalls.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And issues, but they're up there.",
                    "label": 0
                },
                {
                    "sent": "I don't really want to go through all these because I want to leave time for the for the for the hands on.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's stuff there with your interest.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talking about where the issues.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's a little bit 10.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this for this.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this talk.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The one thing which is.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it weird?",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiple.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Endpoints are sorry.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Try.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are trying to be outside.",
                    "label": 0
                },
                {
                    "sent": "The problem when you do and then when you do an MPI send, you specify a process OK.",
                    "label": 0
                },
                {
                    "sent": "Which that MPI routine, which that MPI message goes to.",
                    "label": 0
                },
                {
                    "sent": "But if you have multiple threads within that process, you might want that message to go to particular thread with that process, and MPI doesn't actually allow you to do that.",
                    "label": 0
                },
                {
                    "sent": "So the only real way you can do it is you can use these tags.",
                    "label": 0
                },
                {
                    "sent": "I said that each message has A tag.",
                    "label": 0
                },
                {
                    "sent": "I said they weren't normally useful, but this is a case where they are.",
                    "label": 0
                },
                {
                    "sent": "If you want to send to thread 5 on process 3.",
                    "label": 0
                },
                {
                    "sent": "You would send the message to process three and tag it with tag 5, which is just a hint that at the other end that message is meant for thread 5, so it's not really what tags were invented for, but you can do it, but it is important to realize that MPI specifies the destination process, but if you have multiple threads.",
                    "label": 0
                },
                {
                    "sent": "Then you have to do something else to make sure that it goes to the correct thread within that process again with the master only style.",
                    "label": 0
                },
                {
                    "sent": "It's irrelevant because you know when you're sends and receives from the master thread, so it's a fine that it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So any in this multiple style, we have multiple threads calling MPI simultaneously, but this becomes an issue.",
                    "label": 0
                },
                {
                    "sent": "It all gets a bit messy, but it might.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experience, and there's a.",
                    "label": 0
                },
                {
                    "sent": "There's some discussion in MPI have something called endpoints which will solve this, but that's really beyond this.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nope.",
                    "label": 0
                },
                {
                    "sent": "In my experience, people who have used the multiple.",
                    "label": 0
                },
                {
                    "sent": "The multiple version have suffered.",
                    "label": 0
                },
                {
                    "sent": "I've never really got very good performance because it's so much effort for the MPI library to support multiple thread safety that it that it kills the performance and the other thing which people get wrong is this is the classic thing they do, so they've got their MPI program OK running on 6 threads, OK?",
                    "label": 0
                },
                {
                    "sent": "And they find that this routine here.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a routine which is taking place here is very slow in MPI.",
                    "label": 0
                },
                {
                    "sent": "So what they do?",
                    "label": 0
                },
                {
                    "sent": "If they take that routine and they go away and they give it to some clever person and say I've got this routine which is very slow and pure MPI, could you write me a hybrid MPI Open MP version and they do everything look it goes faster.",
                    "label": 0
                },
                {
                    "sent": "Oh that's brilliant.",
                    "label": 0
                },
                {
                    "sent": "OK so I'll plug that back into my main program.",
                    "label": 0
                },
                {
                    "sent": "But the problem is in hybrid MPI Open MP.",
                    "label": 0
                },
                {
                    "sent": "There's only one MPI process running OK. People seem to think people think that hybrid MPI OMP is like you have four MPI processes, and then when you go into a parallel region, they become four open MP threads.",
                    "label": 0
                },
                {
                    "sent": "No, you have one MPI process.",
                    "label": 0
                },
                {
                    "sent": "And every now and again you spawn multiple threads.",
                    "label": 0
                },
                {
                    "sent": "What that means is every time you're not using open MP and a hybrid MPI Open MP job, you're wasting your wasting cores.",
                    "label": 0
                },
                {
                    "sent": "They're not running here, so you have sped up your kernel.",
                    "label": 0
                },
                {
                    "sent": "This bit here in this bit here, which is great, but the other stuff is now running running on.",
                    "label": 0
                },
                {
                    "sent": "You may be gained a factor of two here.",
                    "label": 0
                },
                {
                    "sent": "You've lost a factor of 6 here.",
                    "label": 0
                },
                {
                    "sent": "'cause you're running this on one on one one core, so then you have to go and put open MP in here.",
                    "label": 0
                },
                {
                    "sent": "This might be if you got a million line code.",
                    "label": 0
                },
                {
                    "sent": "Your kernel might be 1000 lines.",
                    "label": 0
                },
                {
                    "sent": "You have to spend putting open MP into 990,000 lines of code which don't need it, they but they need it because to run hybrid MPI Open MP you have to run fewer MPI processes than there are cores.",
                    "label": 0
                },
                {
                    "sent": "It's a very naive observation, but I'm astounded how many people don't.",
                    "label": 0
                },
                {
                    "sent": "Don't, you know, it's a real problem.",
                    "label": 0
                },
                {
                    "sent": "It's a real real problem you cannot.",
                    "label": 0
                },
                {
                    "sent": "It's not an incremental, it's not an incremental approach.",
                    "label": 0
                },
                {
                    "sent": "Now MPI has been extended and MPI does have its own shared memory model now.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The the the side if you look at the standard MPI single sided communication supports a shared memory model and so.",
                    "label": 0
                },
                {
                    "sent": "It is possible to have MPI processes behaving like threads sometimes, but it's all a bit messy.",
                    "label": 0
                },
                {
                    "sent": "But this is the most important thing.",
                    "label": 0
                },
                {
                    "sent": "OK, that basically the bits that don't use open MP will go slower because they're only running on one process, and they're probably the bits that you don't care about.",
                    "label": 0
                },
                {
                    "sent": "But you have to put open MP into them.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the result is that the performance goes like this.",
                    "label": 0
                },
                {
                    "sent": "So basically you start off with a pure open MPI code.",
                    "label": 0
                },
                {
                    "sent": "You then have to run it on one process to do the hybrid model.",
                    "label": 0
                },
                {
                    "sent": "So you take a huge performance hit of a factor of 6.",
                    "label": 0
                },
                {
                    "sent": "Then as you start paralyzing the open MP parts your performance grows.",
                    "label": 0
                },
                {
                    "sent": "But does it ever get it takes time?",
                    "label": 0
                },
                {
                    "sent": "This is developer time this month.",
                    "label": 0
                },
                {
                    "sent": "This is PhD start, PhD end or something like that.",
                    "label": 0
                },
                {
                    "sent": "Contract, start, contract end, you know, do you get back up to where you start?",
                    "label": 0
                },
                {
                    "sent": "If you started with a serial program, this isn't an issue.",
                    "label": 0
                },
                {
                    "sent": "'cause with the serial program it started here everything you do in open MP is a benefit, but if you started with a parallel MPI program you take a hit by going to hybrid MPI Open MP and you don't know if you get back to where you started.",
                    "label": 0
                },
                {
                    "sent": "So you really have to think carefully before you do hybrid parallelization it what?",
                    "label": 0
                },
                {
                    "sent": "Why do you think that this will end up higher than that?",
                    "label": 0
                },
                {
                    "sent": "And if you say Oh well, I took out the computational kernel, it goes a lot better, like, well wait a second, you still got the other, you got it.",
                    "label": 0
                },
                {
                    "sent": "The rest of the code to worry about.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, refers to the major research topic.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Maybe people say it's the key to exascale, but.",
                    "label": 0
                },
                {
                    "sent": "It you know, it's very difficult.",
                    "label": 0
                },
                {
                    "sent": "Achieving correctness is hard with this MPI thread multiple if you do MPI thread multiple you have to worry about race conditions not only in your open MP code, but race conditions between MPI messages, and that's going to do your head in a bit.",
                    "label": 0
                },
                {
                    "sent": "Achievement forms is hard 'cause the entire application must be threaded.",
                    "label": 0
                },
                {
                    "sent": "If you have a hybrid MPI Open MP code.",
                    "label": 0
                },
                {
                    "sent": "The first thing you should do is run it in a variety of modes.",
                    "label": 0
                },
                {
                    "sent": "Run the pure MPI version and then run it with.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "So that we can bridge that we 20 MPI processes product, then with 14 and two threads seven and four 3 1/2 and then it doesn't work anyway.",
                    "label": 0
                },
                {
                    "sent": "Something like that and look for the sweet spot look, look for the sweet spot we had this the cosmology talk.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "I apologize that I misinterpreted the graph, had done exactly that.",
                    "label": 0
                },
                {
                    "sent": "Had done a scan across threads and processes and looked for the sweet spot and that's just a freebie.",
                    "label": 0
                },
                {
                    "sent": "If somebody's done all the work but don't don't say don't immediately assume that.",
                    "label": 0
                },
                {
                    "sent": "The best thing is 1 process per node.",
                    "label": 0
                },
                {
                    "sent": "And 14 threads.",
                    "label": 0
                },
                {
                    "sent": "It might not be and just do a quick scan and see what you get.",
                    "label": 0
                },
                {
                    "sent": "And the other problem is placement of processor threads on numerous architectures.",
                    "label": 0
                },
                {
                    "sent": "Bridges, like most architectures, isn't.",
                    "label": 0
                },
                {
                    "sent": "I say it's a 2028 core machine.",
                    "label": 0
                },
                {
                    "sent": "What it actually is is 214 core CPUs stitched together and communications between the two CPUs.",
                    "label": 0
                },
                {
                    "sent": "This is an open MP is slower than communication within the CPU.",
                    "label": 0
                },
                {
                    "sent": "It's called a Numa architecture.",
                    "label": 0
                },
                {
                    "sent": "Non uniform memory access, so it's hard to get open MP to scale beyond.",
                    "label": 0
                },
                {
                    "sent": "A single CPU you can call that socket.",
                    "label": 0
                },
                {
                    "sent": "OK, so probably on bridge is what you want is you want two MPI processes per node, each with 14 threads, but you want to have one MPI process on each node, and you want the threads to sit within the nodes.",
                    "label": 0
                },
                {
                    "sent": "And at.",
                    "label": 0
                },
                {
                    "sent": "I think bridges is quite well set up.",
                    "label": 0
                },
                {
                    "sent": "I think it may do that for you or it may not.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "That's always a question, but that you have to start worrying about these nasty hardware details that you would rather not bother about.",
                    "label": 0
                },
                {
                    "sent": "I think on bridges it might be quite easy.",
                    "label": 0
                },
                {
                    "sent": "I think bridges I tried to look at this.",
                    "label": 0
                },
                {
                    "sent": "I find it difficult to understand.",
                    "label": 0
                },
                {
                    "sent": "But my first guess is I should check actually by.",
                    "label": 0
                },
                {
                    "sent": "My first guess is that bridge is not the right thing, but if you ask for two MPI processes, it puts the first process on Core 0 and the second process on Core 14, which is on the other processor and then your empty your open MP threads will be bound to those two CPUs.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Intel is even more complicated 'cause if you ask for eight threads at Intel, they create a helper thread as well, which is nothing but the sort is completely unhelpful so.",
                    "label": 0
                },
                {
                    "sent": "Intel MPI can become even more complicated, but I will look at that.",
                    "label": 0
                },
                {
                    "sent": "I think bridges does the right thing out of the box, but I need to check that I do have a program I can run to check it.",
                    "label": 0
                },
                {
                    "sent": "So I think what I would say is that I would.",
                    "label": 0
                },
                {
                    "sent": "I really, really, really, really good if somebody had a look at hybrid MPI Open MP for the Laplace model.",
                    "label": 0
                },
                {
                    "sent": "Oh, I would recommend that you do the very much recommend that you do the master only approach, which is the obvious one.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I've.",
                    "label": 0
                },
                {
                    "sent": "Lost my place in the witches.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This just do this.",
                    "label": 0
                },
                {
                    "sent": "Just take the MPI code and markup as you pretend it's pretend it's an open MP code and just mark up the work and leave this as it is.",
                    "label": 0
                },
                {
                    "sent": "I don't know how well that's going to perform, but it may not perform any better than the MPI, but I mean, it's definitely worth a look.",
                    "label": 0
                },
                {
                    "sent": "I think it might.",
                    "label": 0
                },
                {
                    "sent": "It might do better, I don't know so.",
                    "label": 0
                },
                {
                    "sent": "I'm quite happy to give a price the best Hibernate NPI NP could, even if it doesn't meet the MPI.",
                    "label": 0
                },
                {
                    "sent": "So you had a question to get there.",
                    "label": 0
                },
                {
                    "sent": "So microphone it'll repeat that I just repeat the question, it's OK. To avoid the issues with the MPI libraries not handling data from multiple threads simultaneously.",
                    "label": 0
                },
                {
                    "sent": "First, I'd like to say are there negative consequences to using just bare MPI in it?",
                    "label": 0
                },
                {
                    "sent": "And then if you like using barriers or something to make sure that no more than one thread is communicating at a time?",
                    "label": 0
                },
                {
                    "sent": "I mean, I understand that there's a special mode for that, but what if you don't use that mode?",
                    "label": 0
                },
                {
                    "sent": "I think I.",
                    "label": 0
                },
                {
                    "sent": "So that's called serialized.",
                    "label": 0
                },
                {
                    "sent": "My guess is the standard out the box MPI library should be able to cope with serialized.",
                    "label": 0
                },
                {
                    "sent": "Access and why would it care that it's being called from different threads?",
                    "label": 1
                },
                {
                    "sent": "It it doesn't know.",
                    "label": 0
                },
                {
                    "sent": "It's just, you know, it's just doing stuff so.",
                    "label": 0
                },
                {
                    "sent": "I just don't think there's any issue with nonblocking communication, is that that's about just worried about one thread initiating a nonblocking communication, and the other one.",
                    "label": 0
                },
                {
                    "sent": "The other one.",
                    "label": 0
                },
                {
                    "sent": "Trying to receive it.",
                    "label": 0
                },
                {
                    "sent": "Open master and.",
                    "label": 0
                },
                {
                    "sent": "Funneled are the same in the sense that only only only the only threads you ever called the MPI library.",
                    "label": 1
                },
                {
                    "sent": "You don't need any special support that whether you need special support for serialized.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "I would guess, probably not.",
                    "label": 0
                },
                {
                    "sent": "But I I wouldn't like there.",
                    "label": 0
                },
                {
                    "sent": "Maybe there may be subtleties that I'm not thinking about so.",
                    "label": 0
                },
                {
                    "sent": "Idita my worry is that you have nonblocking communications initiated by one thread.",
                    "label": 0
                },
                {
                    "sent": "And then that then another thread is calling the library and just trying to think if there's an issue there.",
                    "label": 0
                },
                {
                    "sent": "I don't know enough.",
                    "label": 0
                },
                {
                    "sent": "My guess is it would work, but it would work, but I am not 100% sure.",
                    "label": 0
                },
                {
                    "sent": "So there might be other problems with MPI serialized and thread multiple, so I not aware that any of the performance tools support this mode, so you can do some measurements, but doing automatic analysis doesn't work 'cause you don't know which thread receives the message.",
                    "label": 0
                },
                {
                    "sent": "So this is a real problem and it will only be solved if the MPI standard includes something like the endpoints you already mentioned.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so that's yeah.",
                    "label": 0
                },
                {
                    "sent": "So you really basically.",
                    "label": 0
                },
                {
                    "sent": "You're trying to.",
                    "label": 0
                },
                {
                    "sent": "You've got 2 tickets, completely different standards written by different people at different times, and you're trying to make them work together, and it's not.",
                    "label": 0
                },
                {
                    "sent": "It's it's tough.",
                    "label": 0
                },
                {
                    "sent": "I wanted to ask how the cost for.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to close these doors.",
                    "label": 0
                },
                {
                    "sent": "I know that's not, there was just a bit noisy at the cost for four or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah I can.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll repeat the question, it's OK. No no no no.",
                    "label": 0
                },
                {
                    "sent": "Work, sorry.",
                    "label": 0
                },
                {
                    "sent": "Well, where water was the first thing?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question was sporting open MP threads compared to MPI communication.",
                    "label": 1
                },
                {
                    "sent": "So actually, although logically in the model is that you spawn threads at parallel region.",
                    "label": 0
                },
                {
                    "sent": "In fact, that's not what happens.",
                    "label": 0
                },
                {
                    "sent": "You know what will happen is the MPI like on a dedicated node on bridges.",
                    "label": 0
                },
                {
                    "sent": "The MPI Lybrel create the 27 helper threads at the start and they'll be sitting there buzzing away waiting to be woken up.",
                    "label": 0
                },
                {
                    "sent": "So so logically in the way you think about NPI to write correct open MP programs?",
                    "label": 0
                },
                {
                    "sent": "You should think that apparel region it spawns multiple threads, but in fact they're sitting there waiting and it kind of kicks them up so so people have worked really hard to make the overhead, but that's a good question that actually.",
                    "label": 0
                },
                {
                    "sent": "22 Open MP threads.",
                    "label": 0
                },
                {
                    "sent": "Copying data between each other can be almost as costly as an MPI message because because MPI will notice that it's on a shared memory node and it will just do a copy.",
                    "label": 0
                },
                {
                    "sent": "There will be some processing overhead in the MPI stack, but but then it's a bit deeper to explain what the cache coherency issues with copying data between different threads could be quite high as well, so.",
                    "label": 0
                },
                {
                    "sent": "Typically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's it's normally.",
                    "label": 0
                },
                {
                    "sent": "My trying to say.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Where where it benefits you is in it.",
                    "label": 0
                },
                {
                    "sent": "If you know that you want.",
                    "label": 0
                },
                {
                    "sent": "Course to randomly read pieces of memory in open MP you just say put that database in shared memory.",
                    "label": 0
                },
                {
                    "sent": "Every thread can read what it wants.",
                    "label": 0
                },
                {
                    "sent": "If you do that in MPI, everytime you if the database is distributed every time you want something to just read one byte of data, you have to send a message to somebody they have to send the message back on it.",
                    "label": 0
                },
                {
                    "sent": "So so in extreme versions like that it can work, but it's I don't know.",
                    "label": 0
                },
                {
                    "sent": "So my colleague Mark Bolts.",
                    "label": 0
                },
                {
                    "sent": "OK, here's like there are standard MPI lights.",
                    "label": 0
                },
                {
                    "sent": "There are standard libraries for measuring MPI communications.",
                    "label": 0
                },
                {
                    "sent": "Overheads, so it's called the.",
                    "label": 0
                },
                {
                    "sent": "Intel micro benchmarks.",
                    "label": 0
                },
                {
                    "sent": "It didn't used to be called the Intel Micro benchmarks, but it's called the Internet for the Intel IMB and you can download that and run it and you'll say the latency of sending a message is so many microseconds, but my colleague Mark Bull has also written us along time ago.",
                    "label": 0
                },
                {
                    "sent": "A simple benchmark suite for measuring open MP overheads, and that's called the Open MP micro benchmark.",
                    "label": 0
                },
                {
                    "sent": "So if I just show you.",
                    "label": 0
                },
                {
                    "sent": "So I want don't want to go here.",
                    "label": 0
                },
                {
                    "sent": "So if you look for.",
                    "label": 0
                },
                {
                    "sent": "Intel.",
                    "label": 0
                },
                {
                    "sent": "MPI benchmark it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's there you'll find it.",
                    "label": 0
                },
                {
                    "sent": "And it benchmarks point to point, so you can measure these things.",
                    "label": 0
                },
                {
                    "sent": "And then if I do open MP.",
                    "label": 0
                },
                {
                    "sent": "Micro benchmark.",
                    "label": 1
                },
                {
                    "sent": "Then that I see our websites gone.",
                    "label": 0
                },
                {
                    "sent": "There's some there's some problem with website and need to report it.",
                    "label": 0
                },
                {
                    "sent": "I think it worked.",
                    "label": 0
                },
                {
                    "sent": "I think it's a Firefox problem.",
                    "label": 0
                },
                {
                    "sent": "That's unfortunately anyway.",
                    "label": 0
                },
                {
                    "sent": "There are versions there and they they.",
                    "label": 0
                },
                {
                    "sent": "I'll report that there's some some some stylesheet problem there.",
                    "label": 0
                },
                {
                    "sent": "I'll that can measure the overheads of how, how much, how much, how long does it take to spawn apparel, or even how long does the barrier take?",
                    "label": 0
                },
                {
                    "sent": "How does no MP4 take things like that?",
                    "label": 0
                },
                {
                    "sent": "So that would give you some ballpark figure.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, you had it.",
                    "label": 0
                },
                {
                    "sent": "I think I think I'll be fine here.",
                    "label": 0
                },
                {
                    "sent": "I think it's to do with the fan.",
                    "label": 0
                },
                {
                    "sent": "Um, are we actually competing with the open ACC Group for the fastest Laplace solver?",
                    "label": 0
                },
                {
                    "sent": "And if so, do they have access to the GPU nodes on bridges?",
                    "label": 0
                },
                {
                    "sent": "They do have access GPS, so I'll probably give a price so.",
                    "label": 0
                },
                {
                    "sent": "If I knew what the performances were, and they were roughly the same, then I'd say yes.",
                    "label": 0
                },
                {
                    "sent": "But I think yes, you are competing with them, but I mean, it's probably it may or may not you.",
                    "label": 0
                },
                {
                    "sent": "It may be.",
                    "label": 1
                },
                {
                    "sent": "It may be easier for us.",
                    "label": 0
                },
                {
                    "sent": "You know, for example, you know the GPU nodes only have we have 56 cores per node and the GPU knows presumably have one GPU and only the other socket is probably 14 cores.",
                    "label": 0
                },
                {
                    "sent": "So and the processes here are very new and the GPU they're getting, they're getting new GPU soon.",
                    "label": 0
                },
                {
                    "sent": "But the current GPU's are sort of previous.",
                    "label": 0
                },
                {
                    "sent": "Are the current generation so the CPU's on bridge is a kind of a half a step ahead of the GPU's.",
                    "label": 0
                },
                {
                    "sent": "So it's not clear which the fastest one will be.",
                    "label": 0
                },
                {
                    "sent": "I have to say.",
                    "label": 0
                },
                {
                    "sent": "But I'll, I'll probably give a prize for the best in both camps.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't be fair to say you know that.",
                    "label": 0
                },
                {
                    "sent": "Wait, so are we also running on the GPU notebook with Intel Xeons right on the single?",
                    "label": 0
                },
                {
                    "sent": "Just run on the just run on the standard nodes, the ones that we?",
                    "label": 0
                },
                {
                    "sent": "I think it's minus RM or something.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah fine yeah.",
                    "label": 0
                },
                {
                    "sent": "I wish somebody asked this before, but it is so.",
                    "label": 0
                },
                {
                    "sent": "If somebody is if.",
                    "label": 1
                },
                {
                    "sent": "There's two different questions here.",
                    "label": 0
                },
                {
                    "sent": "If you're given access to a GPU enabled machine OK, or your research group is bought dependable machine, it's worth.",
                    "label": 0
                },
                {
                    "sent": "Using open ACC to use those GPU's it's great.",
                    "label": 0
                },
                {
                    "sent": "And you'll probably get performance benefit.",
                    "label": 0
                },
                {
                    "sent": "However, taking a step back if you had a fixed amount of money and you were to buy a machine, the question is whether it's more cost effective to buy GPU or CPU's.",
                    "label": 0
                },
                {
                    "sent": "A different question because you know, but if the machines being bought and I'm not slagging off GPS, I'm just saying it's different to completely different calculation.",
                    "label": 0
                },
                {
                    "sent": "So as I said, you know if the GPU's have been bought great putting up this, he used them, get the best performance, but if someone gave you a fixed amount of money and remember money.",
                    "label": 0
                },
                {
                    "sent": "Translates into hardware, but also the people time.",
                    "label": 0
                },
                {
                    "sent": "You might say, well, you know the codes are going to three times as fast, but it's going to take me a year of effort and that might be worth it.",
                    "label": 0
                },
                {
                    "sent": "Or it might not, so it's a different count there was there was a I've only seen one paper which did the whole.",
                    "label": 0
                },
                {
                    "sent": "There's a German Group A year or two ago at supercomputing.",
                    "label": 0
                },
                {
                    "sent": "I can't remember who they were to the whole life cycle analysis of cost, performance and they they said it was into this.",
                    "label": 0
                },
                {
                    "sent": "It was sort of 5050, you know, some codes were a lot better, some cultural lot worse.",
                    "label": 0
                },
                {
                    "sent": "So it sort of said.",
                    "label": 0
                },
                {
                    "sent": "GPU machines make sense for special.",
                    "label": 0
                },
                {
                    "sent": "As for special, not special, but for specific groups but not as general purpose machines.",
                    "label": 0
                },
                {
                    "sent": "And that was effective.",
                    "label": 0
                },
                {
                    "sent": "They were the only people I've seen who tried to estimate end to end taking into account cost of ownership, development costs, running costs, everything.",
                    "label": 0
                },
                {
                    "sent": "It was quite nice.",
                    "label": 0
                },
                {
                    "sent": "They got a lot of abuse from people who bought.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know, they actually just done something very.",
                    "label": 0
                },
                {
                    "sent": "Heaven forbid cyantific.",
                    "label": 0
                },
                {
                    "sent": "Just looked at the numbers and worked out what it was and it's quite nice.",
                    "label": 0
                },
                {
                    "sent": "It said, you know, some codes do well, some do badly.",
                    "label": 0
                },
                {
                    "sent": "So you need to think about this, but didn't go down very well.",
                    "label": 0
                },
                {
                    "sent": "Out this question at the back, yeah.",
                    "label": 0
                },
                {
                    "sent": "I was thinking about the example you showed it toward the end of the presentation where you had you were taking MPI code and then trying to add open MP3 open MP threads to it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and how you're saying that as soon as you even start the first thing you're going to do is we're going to drop to a single MPI.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right.",
                    "label": 0
                },
                {
                    "sent": "Now what I was thinking is.",
                    "label": 0
                },
                {
                    "sent": "In two parts of the of the code, if you've got two kernels in there where you're going to use the open MP threads, suppose you got like A6 core processor like in that example you could run the six threads there, could you?",
                    "label": 0
                },
                {
                    "sent": "Could you write your code so that those six threads are assigned to your first MPI rank, and then as soon as that that code ends, the 2nd through 6th MPI ranks then become active again?",
                    "label": 0
                },
                {
                    "sent": "And wouldn't the OS handle scheduling issues, right?",
                    "label": 0
                },
                {
                    "sent": "You would like to think the OS would do this for.",
                    "label": 0
                },
                {
                    "sent": "The problem is that the way that you get performance out of.",
                    "label": 0
                },
                {
                    "sent": "Open MP is the threads wants be active all the time.",
                    "label": 0
                },
                {
                    "sent": "They busy way and the same in MPI you know MPI.",
                    "label": 0
                },
                {
                    "sent": "There's a big cost of scheduling the scheduling.",
                    "label": 0
                },
                {
                    "sent": "You know it's it's charging and discharging is of the order of.",
                    "label": 0
                },
                {
                    "sent": "Milliseconds so yes, So what?",
                    "label": 0
                },
                {
                    "sent": "People a lot of people have said and people have have.",
                    "label": 0
                },
                {
                    "sent": "Mooted this Rex get architects you need architecture which can switch contexts really quickly on the top.",
                    "label": 0
                },
                {
                    "sent": "But they do exist.",
                    "label": 0
                },
                {
                    "sent": "There was the terror terror empty it got renamed and re bought, but people have have built architectures which can support where can't where.",
                    "label": 0
                },
                {
                    "sent": "Context switching between threads or processes very low overhead and so on.",
                    "label": 0
                },
                {
                    "sent": "A modern I mean on modern Linux I don't think you could do it, but there's no reason you could design an operating system.",
                    "label": 0
                },
                {
                    "sent": "Or you could design A chip which could do it.",
                    "label": 0
                },
                {
                    "sent": "The problem is.",
                    "label": 0
                },
                {
                    "sent": "You know we're in this.",
                    "label": 0
                },
                {
                    "sent": "This this completely monolithic region where where everyone uses everyone, uses Linux and so you know that that.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there's research in that, but it seems to have, yeah, but there's no reason.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's that's a perfect reason, but I don't think you could do it with Linux.",
                    "label": 0
                },
                {
                    "sent": "I'm not an operating system match, but by any stretch the imagination.",
                    "label": 0
                },
                {
                    "sent": "But you could design an operating system and an architecture which could do that.",
                    "label": 0
                },
                {
                    "sent": "You write.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's kind of mad that we have this difference in Linux processes and threads are.",
                    "label": 0
                },
                {
                    "sent": "Almost identical, there really isn't any, and maybe the I think the MPI 3 model might be the way that you go, 'cause there you can, although it's a bit overblown.",
                    "label": 0
                },
                {
                    "sent": "Because it has to conform with the rest of the MPI standard, which has a lot of subtleties but but their MPI processes on a node can create shared data structures and access them.",
                    "label": 0
                },
                {
                    "sent": "You don't have the support of OMP four and do for splitting up loops, but at least you can do that by hand, so that's probably the way to go, and I haven't looked at that yet.",
                    "label": 0
                },
                {
                    "sent": "That's quite recent.",
                    "label": 0
                },
                {
                    "sent": "There was somebody in the audience who's been looking at it, but, but that is quite recent, but that is probably the way to go.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To solve both problems.",
                    "label": 0
                },
                {
                    "sent": "So OK, so no more questions.",
                    "label": 0
                },
                {
                    "sent": "Got it.",
                    "label": 0
                },
                {
                    "sent": "Well that exercise to half time.",
                    "label": 0
                },
                {
                    "sent": "Welcome to happy to help you out with anything.",
                    "label": 0
                },
                {
                    "sent": "I think all the material is up there.",
                    "label": 0
                },
                {
                    "sent": "I will put up that one lecture.",
                    "label": 0
                },
                {
                    "sent": "What's the one I gave both tasks?",
                    "label": 0
                },
                {
                    "sent": "I'll put the task lecture.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to mention there existed and I hope you find it useful, so I apologize that I know I. I know that the material came from lots of different places, 'cause this I've had to refactor this course, but hopefully it fitted together reasonably well and you found it useful and I'll be interested to.",
                    "label": 0
                },
                {
                    "sent": "To see.",
                    "label": 0
                },
                {
                    "sent": "How people get on with the programming challenge.",
                    "label": 0
                },
                {
                    "sent": "And yeah well thanks everyone.",
                    "label": 0
                },
                {
                    "sent": "So hope you find it useful and hope you enjoy the rest of the rest of the workshop in the summer.",
                    "label": 0
                }
            ]
        }
    }
}