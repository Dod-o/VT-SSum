{
    "id": "ndvcjzhtwzsehtl6prj6ualtavhrma23",
    "title": "Automatically Generating Data Linkages Using a Domain-Independent Candidate Selection Approach",
    "info": {
        "author": [
            "Dezhao Song, Lehigh University"
        ],
        "published": "Nov. 25, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Semantic Web->RDF - Resource Description Framework",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2011_song_linkages/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "Thank you for coming for this last talk.",
            "In this session, my name is George Olson from the research lab in the Department of Computer Science and Engineering, Lehigh University, and this paper was authored by myself and my advisor, Professor Jeff Happening.",
            "Let's start."
        ],
        [
            "So.",
            "Here's to all I will follow in this presentation.",
            "Will give a brief introduction of what we are actually doing in this paper and some related work in the literature and also after that we formally present or algorithms and some evaluation results, and we conclude with future work."
        ],
        [
            "So when we see the title, the question will ask is actually what is distinguished generation and in this paper the concept means we try to find the instances to actually represent the same real world entity.",
            "And this is actually an indica reference problem where people try to find.",
            "For example, the mentions in free tax.",
            "The people names in free tax actually refer to the same railroad entity, or they try to find.",
            "The database records that are actually duplicated or in a semantic web people try to find the ontology instances that actually represents in real world unity, but different in their syntactical representation.",
            "Different ur eyes and in the semantic web we use the same as property to link those instances actually represent same role or entity.",
            "So.",
            "Sorry, let's go back to this so our work can be treated as a preprocessing step for us to efficiently find out the outside math links between his original semantic web datasets."
        ],
        [
            "And So what is this general?",
            "Or we kill the naive approach for us to detect the LCMS links between the two semantic web data size?",
            "Let's say so simple.",
            "We have two data set here, A and BA12 AI are the instances in this data set, and also we have B1 to BJ, which are the instance in difficult be.",
            "So?",
            "I mean intuitively we just say let's compare as apply algorithm that to every pair of instances.",
            "Between the two data sets, but this is could be.",
            "This could be very expensive when we have a wire large scale datasets.",
            "So suppose we have 1 million instances in a single data set and we try to detect also math links between the days like this, then it's going to be very very expensive so.",
            "The solution here we try to find our way to reduce the total search space for this also master link detection problem where let's say we just.",
            "Develop an algorithm or find out approach where we could very efficiently filter out those instance pairs that are not likely to be Co referent.",
            "Therefore we just select a few pairs between the two datasets and we only apply an expensive entity coreference algorithm to those selected pairs just to reduce the total computational cost.",
            "And so.",
            "One possibility here is I mean how to do this filtering.",
            "We could just select a partial context or partial information for each of the for each instance and we compare the similarity between the partial context between of two instances.",
            "So the context here could be just some triples, but not all the triples, for instance, but some selected triples.",
            "But we want to select the triples.",
            "Would like to do this.",
            "Filtering in a scalable and domain independent way.",
            "And with this filtering we try to maximally reduce the number of pairs that we we keep.",
            "And we also want to return as many coreference pairs as we want as possible, so we do not want this filtering mechanism to filter out.",
            "Too many coreference pairs in that case, we lose a lot of Rico.",
            "And some requirements for the filtering process, so this filtering process is designed to make the entity Coreference algorithm to scale to large scale datasets.",
            "Therefore, this filtering mechanism itself needs to scale as well, and another requirement here is that we want this.",
            "After applying our filter children mechanism, we want the entire entity Coreference process scale very well and also we want the F1 score always say.",
            "Let's say we want the entire entity reference process still has a very high precision high recall.",
            "Anne.",
            "All this can be summarized to be.",
            "Let's say this is a blocking, or we call Kenneth Selection."
        ],
        [
            "And here are some really work in the literature.",
            "We shall compare our algorithm against this approach.",
            "Is all the details are can be found in our paper and in this presentation we sort of comprise our results to give the highlights.",
            "As we will see later."
        ],
        [
            "Here is the system framework, so there are several components.",
            "First of all, given the data.",
            "We learn a set of properties.",
            "A set of RDF properties that we actually who's our providers will actually be used for Earth to do this filtering, and in this paper we only care about data type properties because we want to use the similarity.",
            "And the next component is just a indexer where they build an inverted index.",
            "For the instances that we we try to filter out, sorry, which we build an index on the on the instance from a data set on the object values of the learning properties.",
            "Finally, with the index with with with the index, we couldn't query the index.",
            "What's the opera values of the selected property for some instances?",
            "And and also in this Candace appear selector, we apply a second level similarity measure to further refine the return payers by querying the index."
        ],
        [
            "So we'll follow this framework to introduce each of these three components.",
            "So first of all, the blocking property learner.",
            "Ask give example first in order to select the candidate pairs, we try to find some, apply some similarity measure to Diablo values of the learner properties.",
            "So for example, given some instances we could say for them for some person instances we could simply say let's use their given name or the full name as the value 2.",
            "To compute this similarity measure between between the instances, but we as we said before, we want this properties selecting process to be domain independent.",
            "Therefore we do not want to say OK given person instance, we just use their name and given publication instances.",
            "We just used our title or giving some other instances we manually identify.",
            "The property we want to use, so we want this property choosing.",
            "Process to be domain independent and here we are.",
            "The three metrics we used to select the properties.",
            "First one is called discriminability.",
            "So basically the idea is that.",
            "Give me property so we we curb which we look at the diversity of its opera office values.",
            "So the more.",
            "Diverse opera value set.",
            "A property has the.",
            "More discriminating are this probably will be and therefore the higher disability at this property will have, so we use this discount ability to say we do not want we want this selected property to be discriminating enough.",
            "Therefore we do not select too many pairs.",
            "Also coverage the coverage is just saying.",
            "How many instances actually use a property?",
            "So we want to compare some similarity between the obvious values of the learner properties therefore.",
            "If.",
            "If given two instances if there if they do not use some learning property a same time, then there's no similarity measure.",
            "We can compute too.",
            "We want the instances to use the properties and finally we compute AFL score, which is essentially a F1 score between the discrete ability and courage.",
            "And we'll show how how we actually use these three metrics in our learning process."
        ],
        [
            "So.",
            "So as we said we have we have three metrics for Earth to select the properties to be used, but.",
            "The problem is that a single property may not work well.",
            "For first to do this kind of selection, so for example.",
            "Why don't we so we talk about we could use the given name or the last name for kind of selection, but why don't we simply use nation of last name given name?",
            "Would that actually do better?",
            "So this diagram here shows how we we find out a set of properties for us to do this kind of selection.",
            "So given, so given the data, the input is just the RDF data.",
            "Given the data, we compute the F score we just saw in the previous slide for the order data have properties.",
            "And then if the maximum score is actually higher than a predefined threshold, then we just.",
            "And then we peak.",
            "The property that actually has the highest FL score otherwise will still find out the property that actually has the highest FL score, which is a. Instagram and then we combine a with every other property.",
            "To form some words, some combined properties from virtual properties.",
            "Now we also concatenate their upper values and we add this newly formed prop properties to RDF data set towards the left graph and we also.",
            "With their complaining about values and we removed all, we remove the old ones.",
            "So in that sense we can just process intuitively till we reach this stopping criteria."
        ],
        [
            "And in the end we form a set of triples in the form of instance property and value."
        ],
        [
            "And next I'll talk about indexer.",
            "So.",
            "So we as we just talked about, we slept a few properties that are discriminating enough, and there are used by the majority of instances in a data set for kind of selection.",
            "But even with those few selected pairs, sorry we missed those few selected properties.",
            "If we compare the similarity between the other values of those properties for all pairs instances, it would still be very expensive for large scale datasets.",
            "So we just build this index.",
            "For us to do some efficient look up.",
            "And index is just a traditional inverted index, so here's the RDF data where I&J are the two instances we try to compare.",
            "An full name is just a property and we have the other values.",
            "John Smith and James Smith, and here is the index.",
            "So the index contains multiple multiple fields and each field is just a property.",
            "And we see.",
            "Term list is just tokenized values of the opera values in the RDF data and becausw instance I contains the token, so there so in the document list.",
            "Of of the term drawing we have inside and.",
            "We have and also because I focus on Inj, contend talking Smith.",
            "Therefore the document list of the term Smith have.",
            "Post instances there."
        ],
        [
            "And finally we come to this candidate pairs lecture, the last component.",
            "So Even so, with the index mean alot of pairs will actually be returned if you just issue a disjunctive query with the token left upper value of an instance to the index.",
            "So.",
            "We want to find a way where we can further refine the return pairs by querying the index.",
            "So here we call that we need a second life second level similarity measure.",
            "And this is used to further reduce the non Co referent pairs.",
            "But here we need to be careful in the sense that we do not want this second level similarity measure to filter out too many coreference pairs there so that.",
            "If we apply a too strict similarity measure than a lot of I mean a certain amount correct paragraph should be filtered out be cause.",
            "For example there are different in their syntactical representation and so on so forth so."
        ],
        [
            "In this paper, we compared.",
            "Three alternatives as the second second level similarity measure, the first one is pretty.",
            "I mean, first one is just.",
            "We call that direct call, so we just compute, apply some string matching algorithm to the upper values of two instances and if their similarity is hard, any threshold and we just pick them up.",
            "The second one is the tokens they were, which is Def.",
            "Computing the similarity of the two upper values on their whole.",
            "We tokenize, we try to check the percentage of the shared highly similar similar tokens.",
            "So we tokenize the other values and compute similarity between every pair, pocus from 2 strings.",
            "And final one is the one we actually used in this.",
            "In your final algorithm and we call that cultural background similarity measure, which instead of computing the token level similarity, we simply check the percentage of the shared care level diagrams.",
            "We just extract bigrams from strings and we checked.",
            "How many background back shared from better to strings?"
        ],
        [
            "Anne.",
            "Now we come to this evaluation section, so here are some metrics that are frequently used for evaluating, blocking or candidate selection systems.",
            "Paris completeness is simply says.",
            "You slept a a set of pairs, except for instance pairs, which we call that instance set.",
            "Sorry kinda sad and we simply say how many true matches are covered in this.",
            "Set and the reduction ratio is simply say OK, you select a set of pairs and how many pairs are actually.",
            "Did you actually reduce as is simply a F1 score between two metrics and the rest three metrics are pretty standard metrics for evaluating integral systems.",
            "Precision recall F1."
        ],
        [
            "So here are the data set we tested are almost six datasets.",
            "The 1st three RDF datasets for the last three, or some structured datasets."
        ],
        [
            "And here is the learned Kenneth section key for first one.",
            "Just know the first one has a lot of keys selected.",
            "Becausw full name is pretty much used by every instance but and the rest of the four properties are still.",
            "Are very discriminating but are not used in about a lot of instances."
        ],
        [
            "In this data set.",
            "Here are some highlights of our results, so bigrams over proposed system and we can see that on all the three RDF DSS it was able to achieve the highest performance on pretty much all.",
            "Most of the metrics.",
            "So last one naive is an actual entity coreference system we developed last year, and we compared to this one to show our system was actually able to achieve a higher correct F1 score.",
            "And it's actually achieved a lot of imagine a much higher runtime.",
            "Sorry if it actually runs much faster than naive one because the knife doesn't actually include any kind of selection."
        ],
        [
            "And here are the results for the three largest data set which took.",
            "We converted the RDF data RDF simply by treating each attribute as it did have property.",
            "And for further first 2 deaths observed similar.",
            "We have we had a similar observation here where diagram typically has the best performance and for the last one this is a synthetic sensor status that where our systems actually pretty bad and we're still exploring the actual reason for this, just to note.",
            "For some of systems, we simply compute the number of selected pairs.",
            "For some of the systems because they simply reported their reduction ratio."
        ],
        [
            "And the last finally, let's see the scalability of candy selection.",
            "We tested our scalability over our algorithm."
        ],
        [
            "On this machine, up to 1 million instances on four datasets and we can see, although it's not linear, scalability still.",
            "Is still scales well.",
            "But I admit that 1,000,000 is still small compared to the scale of the sun."
        ],
        [
            "What day today?",
            "And the last experiment is that we try to show off faster.",
            "Our algorithm can actually run by actual integration algorithm can actually run by applying our collection technique.",
            "So we.",
            "So here's the speed up factor, which is computed as the runtime we.",
            "Without any section divided by the runtime with connection and.",
            "So we see an up to 20,000 instances on three data RDF datasets by applying our construction technique.",
            "The naive algorithm actually will speed up by two to three orders of magnitude."
        ],
        [
            "And here's just conclusion and future work.",
            "We developed a system for counting selection which has high Rico and high reduction ratio, and the algorithm itself scales wild, large scale datasets and particularly when applying this technique, a naive question algorithm actually scales pretty well.",
            "And here's some future work.",
            "For example, we could apply this technique to larger datasets, or we could apply other system apply or other in different systems to select players.",
            "Our connection technique and maybe also we could explore more appropriate measures for matching numeric data where we simply treat as strings.",
            "At this point in time.",
            "Thank you.",
            "OK, thanks for speaking.",
            "You seem to.",
            "Assume that these properties are in there and the distributions of the values would be distributed fairly evenly across the world.",
            "But especially in the name in the case of name disambiguation, it's well known that distributions are quite different, like in China and Korea from Europe, and so I was wondering if you, for example did this for data describing, say, publications in Chinese traditional medicine, you would get a different set of properties then if you say did the same thing for publications.",
            "On German language.",
            "OK, very good questions, so my answer would be that.",
            "Currently we only care about.",
            "English language and we indeed didn't apply our technique to datasets in other languages, but.",
            "So our assumption is actually that.",
            "Given the data data as the input and we compute.",
            "Sorry to select the properties but.",
            "Agree with you that in different in different in the data of different languages.",
            "The select properties may vary and the metrics may not work as well as it worked on the English language datasets.",
            "Thanks for speaking."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "Thank you for coming for this last talk.",
                    "label": 0
                },
                {
                    "sent": "In this session, my name is George Olson from the research lab in the Department of Computer Science and Engineering, Lehigh University, and this paper was authored by myself and my advisor, Professor Jeff Happening.",
                    "label": 1
                },
                {
                    "sent": "Let's start.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's to all I will follow in this presentation.",
                    "label": 0
                },
                {
                    "sent": "Will give a brief introduction of what we are actually doing in this paper and some related work in the literature and also after that we formally present or algorithms and some evaluation results, and we conclude with future work.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we see the title, the question will ask is actually what is distinguished generation and in this paper the concept means we try to find the instances to actually represent the same real world entity.",
                    "label": 1
                },
                {
                    "sent": "And this is actually an indica reference problem where people try to find.",
                    "label": 0
                },
                {
                    "sent": "For example, the mentions in free tax.",
                    "label": 0
                },
                {
                    "sent": "The people names in free tax actually refer to the same railroad entity, or they try to find.",
                    "label": 1
                },
                {
                    "sent": "The database records that are actually duplicated or in a semantic web people try to find the ontology instances that actually represents in real world unity, but different in their syntactical representation.",
                    "label": 0
                },
                {
                    "sent": "Different ur eyes and in the semantic web we use the same as property to link those instances actually represent same role or entity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry, let's go back to this so our work can be treated as a preprocessing step for us to efficiently find out the outside math links between his original semantic web datasets.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what is this general?",
                    "label": 0
                },
                {
                    "sent": "Or we kill the naive approach for us to detect the LCMS links between the two semantic web data size?",
                    "label": 0
                },
                {
                    "sent": "Let's say so simple.",
                    "label": 0
                },
                {
                    "sent": "We have two data set here, A and BA12 AI are the instances in this data set, and also we have B1 to BJ, which are the instance in difficult be.",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "I mean intuitively we just say let's compare as apply algorithm that to every pair of instances.",
                    "label": 0
                },
                {
                    "sent": "Between the two data sets, but this is could be.",
                    "label": 0
                },
                {
                    "sent": "This could be very expensive when we have a wire large scale datasets.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have 1 million instances in a single data set and we try to detect also math links between the days like this, then it's going to be very very expensive so.",
                    "label": 0
                },
                {
                    "sent": "The solution here we try to find our way to reduce the total search space for this also master link detection problem where let's say we just.",
                    "label": 0
                },
                {
                    "sent": "Develop an algorithm or find out approach where we could very efficiently filter out those instance pairs that are not likely to be Co referent.",
                    "label": 1
                },
                {
                    "sent": "Therefore we just select a few pairs between the two datasets and we only apply an expensive entity coreference algorithm to those selected pairs just to reduce the total computational cost.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "One possibility here is I mean how to do this filtering.",
                    "label": 1
                },
                {
                    "sent": "We could just select a partial context or partial information for each of the for each instance and we compare the similarity between the partial context between of two instances.",
                    "label": 0
                },
                {
                    "sent": "So the context here could be just some triples, but not all the triples, for instance, but some selected triples.",
                    "label": 0
                },
                {
                    "sent": "But we want to select the triples.",
                    "label": 1
                },
                {
                    "sent": "Would like to do this.",
                    "label": 1
                },
                {
                    "sent": "Filtering in a scalable and domain independent way.",
                    "label": 0
                },
                {
                    "sent": "And with this filtering we try to maximally reduce the number of pairs that we we keep.",
                    "label": 0
                },
                {
                    "sent": "And we also want to return as many coreference pairs as we want as possible, so we do not want this filtering mechanism to filter out.",
                    "label": 0
                },
                {
                    "sent": "Too many coreference pairs in that case, we lose a lot of Rico.",
                    "label": 0
                },
                {
                    "sent": "And some requirements for the filtering process, so this filtering process is designed to make the entity Coreference algorithm to scale to large scale datasets.",
                    "label": 0
                },
                {
                    "sent": "Therefore, this filtering mechanism itself needs to scale as well, and another requirement here is that we want this.",
                    "label": 0
                },
                {
                    "sent": "After applying our filter children mechanism, we want the entire entity Coreference process scale very well and also we want the F1 score always say.",
                    "label": 1
                },
                {
                    "sent": "Let's say we want the entire entity reference process still has a very high precision high recall.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "All this can be summarized to be.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is a blocking, or we call Kenneth Selection.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some really work in the literature.",
                    "label": 0
                },
                {
                    "sent": "We shall compare our algorithm against this approach.",
                    "label": 0
                },
                {
                    "sent": "Is all the details are can be found in our paper and in this presentation we sort of comprise our results to give the highlights.",
                    "label": 0
                },
                {
                    "sent": "As we will see later.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the system framework, so there are several components.",
                    "label": 1
                },
                {
                    "sent": "First of all, given the data.",
                    "label": 0
                },
                {
                    "sent": "We learn a set of properties.",
                    "label": 0
                },
                {
                    "sent": "A set of RDF properties that we actually who's our providers will actually be used for Earth to do this filtering, and in this paper we only care about data type properties because we want to use the similarity.",
                    "label": 1
                },
                {
                    "sent": "And the next component is just a indexer where they build an inverted index.",
                    "label": 0
                },
                {
                    "sent": "For the instances that we we try to filter out, sorry, which we build an index on the on the instance from a data set on the object values of the learning properties.",
                    "label": 0
                },
                {
                    "sent": "Finally, with the index with with with the index, we couldn't query the index.",
                    "label": 0
                },
                {
                    "sent": "What's the opera values of the selected property for some instances?",
                    "label": 0
                },
                {
                    "sent": "And and also in this Candace appear selector, we apply a second level similarity measure to further refine the return payers by querying the index.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll follow this framework to introduce each of these three components.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the blocking property learner.",
                    "label": 0
                },
                {
                    "sent": "Ask give example first in order to select the candidate pairs, we try to find some, apply some similarity measure to Diablo values of the learner properties.",
                    "label": 0
                },
                {
                    "sent": "So for example, given some instances we could say for them for some person instances we could simply say let's use their given name or the full name as the value 2.",
                    "label": 1
                },
                {
                    "sent": "To compute this similarity measure between between the instances, but we as we said before, we want this properties selecting process to be domain independent.",
                    "label": 0
                },
                {
                    "sent": "Therefore we do not want to say OK given person instance, we just use their name and given publication instances.",
                    "label": 0
                },
                {
                    "sent": "We just used our title or giving some other instances we manually identify.",
                    "label": 0
                },
                {
                    "sent": "The property we want to use, so we want this property choosing.",
                    "label": 0
                },
                {
                    "sent": "Process to be domain independent and here we are.",
                    "label": 0
                },
                {
                    "sent": "The three metrics we used to select the properties.",
                    "label": 0
                },
                {
                    "sent": "First one is called discriminability.",
                    "label": 0
                },
                {
                    "sent": "So basically the idea is that.",
                    "label": 0
                },
                {
                    "sent": "Give me property so we we curb which we look at the diversity of its opera office values.",
                    "label": 0
                },
                {
                    "sent": "So the more.",
                    "label": 0
                },
                {
                    "sent": "Diverse opera value set.",
                    "label": 0
                },
                {
                    "sent": "A property has the.",
                    "label": 0
                },
                {
                    "sent": "More discriminating are this probably will be and therefore the higher disability at this property will have, so we use this discount ability to say we do not want we want this selected property to be discriminating enough.",
                    "label": 0
                },
                {
                    "sent": "Therefore we do not select too many pairs.",
                    "label": 0
                },
                {
                    "sent": "Also coverage the coverage is just saying.",
                    "label": 1
                },
                {
                    "sent": "How many instances actually use a property?",
                    "label": 0
                },
                {
                    "sent": "So we want to compare some similarity between the obvious values of the learner properties therefore.",
                    "label": 1
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "If given two instances if there if they do not use some learning property a same time, then there's no similarity measure.",
                    "label": 0
                },
                {
                    "sent": "We can compute too.",
                    "label": 0
                },
                {
                    "sent": "We want the instances to use the properties and finally we compute AFL score, which is essentially a F1 score between the discrete ability and courage.",
                    "label": 0
                },
                {
                    "sent": "And we'll show how how we actually use these three metrics in our learning process.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So as we said we have we have three metrics for Earth to select the properties to be used, but.",
                    "label": 0
                },
                {
                    "sent": "The problem is that a single property may not work well.",
                    "label": 1
                },
                {
                    "sent": "For first to do this kind of selection, so for example.",
                    "label": 1
                },
                {
                    "sent": "Why don't we so we talk about we could use the given name or the last name for kind of selection, but why don't we simply use nation of last name given name?",
                    "label": 0
                },
                {
                    "sent": "Would that actually do better?",
                    "label": 0
                },
                {
                    "sent": "So this diagram here shows how we we find out a set of properties for us to do this kind of selection.",
                    "label": 0
                },
                {
                    "sent": "So given, so given the data, the input is just the RDF data.",
                    "label": 0
                },
                {
                    "sent": "Given the data, we compute the F score we just saw in the previous slide for the order data have properties.",
                    "label": 0
                },
                {
                    "sent": "And then if the maximum score is actually higher than a predefined threshold, then we just.",
                    "label": 0
                },
                {
                    "sent": "And then we peak.",
                    "label": 1
                },
                {
                    "sent": "The property that actually has the highest FL score otherwise will still find out the property that actually has the highest FL score, which is a. Instagram and then we combine a with every other property.",
                    "label": 0
                },
                {
                    "sent": "To form some words, some combined properties from virtual properties.",
                    "label": 0
                },
                {
                    "sent": "Now we also concatenate their upper values and we add this newly formed prop properties to RDF data set towards the left graph and we also.",
                    "label": 0
                },
                {
                    "sent": "With their complaining about values and we removed all, we remove the old ones.",
                    "label": 0
                },
                {
                    "sent": "So in that sense we can just process intuitively till we reach this stopping criteria.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the end we form a set of triples in the form of instance property and value.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And next I'll talk about indexer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we as we just talked about, we slept a few properties that are discriminating enough, and there are used by the majority of instances in a data set for kind of selection.",
                    "label": 0
                },
                {
                    "sent": "But even with those few selected pairs, sorry we missed those few selected properties.",
                    "label": 0
                },
                {
                    "sent": "If we compare the similarity between the other values of those properties for all pairs instances, it would still be very expensive for large scale datasets.",
                    "label": 0
                },
                {
                    "sent": "So we just build this index.",
                    "label": 0
                },
                {
                    "sent": "For us to do some efficient look up.",
                    "label": 0
                },
                {
                    "sent": "And index is just a traditional inverted index, so here's the RDF data where I&J are the two instances we try to compare.",
                    "label": 0
                },
                {
                    "sent": "An full name is just a property and we have the other values.",
                    "label": 0
                },
                {
                    "sent": "John Smith and James Smith, and here is the index.",
                    "label": 0
                },
                {
                    "sent": "So the index contains multiple multiple fields and each field is just a property.",
                    "label": 0
                },
                {
                    "sent": "And we see.",
                    "label": 0
                },
                {
                    "sent": "Term list is just tokenized values of the opera values in the RDF data and becausw instance I contains the token, so there so in the document list.",
                    "label": 0
                },
                {
                    "sent": "Of of the term drawing we have inside and.",
                    "label": 0
                },
                {
                    "sent": "We have and also because I focus on Inj, contend talking Smith.",
                    "label": 0
                },
                {
                    "sent": "Therefore the document list of the term Smith have.",
                    "label": 0
                },
                {
                    "sent": "Post instances there.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally we come to this candidate pairs lecture, the last component.",
                    "label": 0
                },
                {
                    "sent": "So Even so, with the index mean alot of pairs will actually be returned if you just issue a disjunctive query with the token left upper value of an instance to the index.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We want to find a way where we can further refine the return pairs by querying the index.",
                    "label": 0
                },
                {
                    "sent": "So here we call that we need a second life second level similarity measure.",
                    "label": 0
                },
                {
                    "sent": "And this is used to further reduce the non Co referent pairs.",
                    "label": 1
                },
                {
                    "sent": "But here we need to be careful in the sense that we do not want this second level similarity measure to filter out too many coreference pairs there so that.",
                    "label": 1
                },
                {
                    "sent": "If we apply a too strict similarity measure than a lot of I mean a certain amount correct paragraph should be filtered out be cause.",
                    "label": 0
                },
                {
                    "sent": "For example there are different in their syntactical representation and so on so forth so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this paper, we compared.",
                    "label": 0
                },
                {
                    "sent": "Three alternatives as the second second level similarity measure, the first one is pretty.",
                    "label": 0
                },
                {
                    "sent": "I mean, first one is just.",
                    "label": 0
                },
                {
                    "sent": "We call that direct call, so we just compute, apply some string matching algorithm to the upper values of two instances and if their similarity is hard, any threshold and we just pick them up.",
                    "label": 0
                },
                {
                    "sent": "The second one is the tokens they were, which is Def.",
                    "label": 0
                },
                {
                    "sent": "Computing the similarity of the two upper values on their whole.",
                    "label": 0
                },
                {
                    "sent": "We tokenize, we try to check the percentage of the shared highly similar similar tokens.",
                    "label": 1
                },
                {
                    "sent": "So we tokenize the other values and compute similarity between every pair, pocus from 2 strings.",
                    "label": 1
                },
                {
                    "sent": "And final one is the one we actually used in this.",
                    "label": 1
                },
                {
                    "sent": "In your final algorithm and we call that cultural background similarity measure, which instead of computing the token level similarity, we simply check the percentage of the shared care level diagrams.",
                    "label": 0
                },
                {
                    "sent": "We just extract bigrams from strings and we checked.",
                    "label": 0
                },
                {
                    "sent": "How many background back shared from better to strings?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Now we come to this evaluation section, so here are some metrics that are frequently used for evaluating, blocking or candidate selection systems.",
                    "label": 1
                },
                {
                    "sent": "Paris completeness is simply says.",
                    "label": 0
                },
                {
                    "sent": "You slept a a set of pairs, except for instance pairs, which we call that instance set.",
                    "label": 1
                },
                {
                    "sent": "Sorry kinda sad and we simply say how many true matches are covered in this.",
                    "label": 1
                },
                {
                    "sent": "Set and the reduction ratio is simply say OK, you select a set of pairs and how many pairs are actually.",
                    "label": 0
                },
                {
                    "sent": "Did you actually reduce as is simply a F1 score between two metrics and the rest three metrics are pretty standard metrics for evaluating integral systems.",
                    "label": 0
                },
                {
                    "sent": "Precision recall F1.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the data set we tested are almost six datasets.",
                    "label": 0
                },
                {
                    "sent": "The 1st three RDF datasets for the last three, or some structured datasets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the learned Kenneth section key for first one.",
                    "label": 0
                },
                {
                    "sent": "Just know the first one has a lot of keys selected.",
                    "label": 0
                },
                {
                    "sent": "Becausw full name is pretty much used by every instance but and the rest of the four properties are still.",
                    "label": 0
                },
                {
                    "sent": "Are very discriminating but are not used in about a lot of instances.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this data set.",
                    "label": 0
                },
                {
                    "sent": "Here are some highlights of our results, so bigrams over proposed system and we can see that on all the three RDF DSS it was able to achieve the highest performance on pretty much all.",
                    "label": 0
                },
                {
                    "sent": "Most of the metrics.",
                    "label": 0
                },
                {
                    "sent": "So last one naive is an actual entity coreference system we developed last year, and we compared to this one to show our system was actually able to achieve a higher correct F1 score.",
                    "label": 0
                },
                {
                    "sent": "And it's actually achieved a lot of imagine a much higher runtime.",
                    "label": 0
                },
                {
                    "sent": "Sorry if it actually runs much faster than naive one because the knife doesn't actually include any kind of selection.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are the results for the three largest data set which took.",
                    "label": 1
                },
                {
                    "sent": "We converted the RDF data RDF simply by treating each attribute as it did have property.",
                    "label": 0
                },
                {
                    "sent": "And for further first 2 deaths observed similar.",
                    "label": 0
                },
                {
                    "sent": "We have we had a similar observation here where diagram typically has the best performance and for the last one this is a synthetic sensor status that where our systems actually pretty bad and we're still exploring the actual reason for this, just to note.",
                    "label": 1
                },
                {
                    "sent": "For some of systems, we simply compute the number of selected pairs.",
                    "label": 1
                },
                {
                    "sent": "For some of the systems because they simply reported their reduction ratio.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last finally, let's see the scalability of candy selection.",
                    "label": 0
                },
                {
                    "sent": "We tested our scalability over our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this machine, up to 1 million instances on four datasets and we can see, although it's not linear, scalability still.",
                    "label": 0
                },
                {
                    "sent": "Is still scales well.",
                    "label": 0
                },
                {
                    "sent": "But I admit that 1,000,000 is still small compared to the scale of the sun.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What day today?",
                    "label": 0
                },
                {
                    "sent": "And the last experiment is that we try to show off faster.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm can actually run by actual integration algorithm can actually run by applying our collection technique.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "So here's the speed up factor, which is computed as the runtime we.",
                    "label": 0
                },
                {
                    "sent": "Without any section divided by the runtime with connection and.",
                    "label": 0
                },
                {
                    "sent": "So we see an up to 20,000 instances on three data RDF datasets by applying our construction technique.",
                    "label": 0
                },
                {
                    "sent": "The naive algorithm actually will speed up by two to three orders of magnitude.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's just conclusion and future work.",
                    "label": 1
                },
                {
                    "sent": "We developed a system for counting selection which has high Rico and high reduction ratio, and the algorithm itself scales wild, large scale datasets and particularly when applying this technique, a naive question algorithm actually scales pretty well.",
                    "label": 0
                },
                {
                    "sent": "And here's some future work.",
                    "label": 0
                },
                {
                    "sent": "For example, we could apply this technique to larger datasets, or we could apply other system apply or other in different systems to select players.",
                    "label": 1
                },
                {
                    "sent": "Our connection technique and maybe also we could explore more appropriate measures for matching numeric data where we simply treat as strings.",
                    "label": 0
                },
                {
                    "sent": "At this point in time.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for speaking.",
                    "label": 0
                },
                {
                    "sent": "You seem to.",
                    "label": 0
                },
                {
                    "sent": "Assume that these properties are in there and the distributions of the values would be distributed fairly evenly across the world.",
                    "label": 0
                },
                {
                    "sent": "But especially in the name in the case of name disambiguation, it's well known that distributions are quite different, like in China and Korea from Europe, and so I was wondering if you, for example did this for data describing, say, publications in Chinese traditional medicine, you would get a different set of properties then if you say did the same thing for publications.",
                    "label": 0
                },
                {
                    "sent": "On German language.",
                    "label": 0
                },
                {
                    "sent": "OK, very good questions, so my answer would be that.",
                    "label": 0
                },
                {
                    "sent": "Currently we only care about.",
                    "label": 0
                },
                {
                    "sent": "English language and we indeed didn't apply our technique to datasets in other languages, but.",
                    "label": 0
                },
                {
                    "sent": "So our assumption is actually that.",
                    "label": 0
                },
                {
                    "sent": "Given the data data as the input and we compute.",
                    "label": 0
                },
                {
                    "sent": "Sorry to select the properties but.",
                    "label": 0
                },
                {
                    "sent": "Agree with you that in different in different in the data of different languages.",
                    "label": 0
                },
                {
                    "sent": "The select properties may vary and the metrics may not work as well as it worked on the English language datasets.",
                    "label": 0
                },
                {
                    "sent": "Thanks for speaking.",
                    "label": 0
                }
            ]
        }
    }
}