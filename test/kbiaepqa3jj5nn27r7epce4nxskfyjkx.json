{
    "id": "kbiaepqa3jj5nn27r7epce4nxskfyjkx",
    "title": "Adaptive Fourier-Domain Inference on the Symmetric Group",
    "info": {
        "author": [
            "Jonathan Huang, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/aml08_huang_afdisg/",
    "segmentation": [
        [
            "OK, my name is Jonathan and the title of my talk is adapted for a domain inference on the symmetric group.",
            "This is joint work with Carlos Guestrin, my advisor Shaya.",
            "We just gave the talk and Leo gave us from Stanford.",
            "OK, so I'm going to begin by."
        ],
        [
            "Are you talking about the identity management problem?",
            "Coincidentally, me and Jonathan made very similar sides.",
            "The problems that you have.",
            "Some people they walk along some tracks, tracks, label tracks, one and two in this case, and sometimes these tracks walk together and we call that a mixing event like this and the problem of identity management is to answer questions like this.",
            "Where is Donald Duck OK at the end?"
        ],
        [
            "Here's a bigger problem you may have seen this already too.",
            "Looks a little bit like this.",
            "In this case we observe make an observation at the very end we see that goofy goes somewhere here an we have three mixing events.",
            "Between tracks 1213 and then one and four and the question is where did Biscoe right?",
            "If you look carefully at the kind of footprints over here, you can tell that beast must have gone to the very end, right?",
            "Because if Goofy went here, he can only go like this, right?",
            "So that's that problem."
        ],
        [
            "In general, we're going to be.",
            "Concerned with uncertainty in this problem.",
            "Alright, so sometimes we make noisy observations or we don't know who went where in a mixed event and we model this uncertainty using distributions over permutations and so here's an example of such a distribution.",
            "Here we have all the permutations of four 4 tracks, and for each track we associated with with their probability.",
            "So for example 1324.",
            "Into 110th equal says something like Alice is at track.",
            "1 bugs attract three.",
            "Kathy's that track two, and Davids that checked four with probability 110th.",
            "OK, this is good.",
            "But the problem is that we can never actually hold this table right in memory, because the."
        ],
        [
            "If you have any people, then there N factorial permutations and you know it's not so bad when end is really really small.",
            "But friend equals 15.",
            "For example you need 2000 petabytes.",
            "That's just the store that distribution.",
            "Alright?",
            "What's even worse is that these graphical models that were so used to using in machine learning they're not very effective because of what we call mutual exclusivity constraints that say that two people can't be at the same track at the same time.",
            "Alright, So what do we do?"
        ],
        [
            "Well, here's an idea that some people have used before, and it's the idea of storing the marginal probability that a single identity J Maps to a single track.",
            "I OK, so, for example, we might want to just store the probability that Davids at track four, and to do that we just go through this table, we sum over the entries where D map to four, and in this case we got seven 20th.",
            "OK, very simple.",
            "Of course."
        ],
        [
            "We could do this for every pair, and doing that we get we can put that in a matrix of N squared numbers and so for example, this entry says that track two with zero probability and essentially might say that Kathy is at track three with probability 120th.",
            "OK Ann, we started off with N factorial numbers.",
            "Now we have N squared numbers.",
            "OK So what kind of things can we capture with this kind of represent?"
        ],
        [
            "Well, we can capture.",
            "These sorts of statements, right Alice is at track one with some probability Bobs track two with some probability.",
            "Now suppose that tracks one and two are close, and I tell you that Alice and Bob absolutely hate each other.",
            "They can't stand to be next to each other, OK?",
            "That says in these mathematical terms is that Alice and Bob occupied tracks, one track one and two jointly.",
            "With probability 0.",
            "Alright, so this is an example of a higher order probability and so kind of.",
            "The lesson is that first order summaries can't capture these higher order dependencies that we might.",
            "That might be really important in tree."
        ],
        [
            "So we're going to fix this idea with the second idea, which is the store a matrix of marginals over ordered pairs.",
            "So, for example, we might want to store the fact that Kathy is at track 3.",
            "And David's at track four with zero probability, and we just sum over these two entries.",
            "In the table.",
            "OK."
        ],
        [
            "We can do the same thing.",
            "We can put this in a matrix of marginal probabilities and this entry might say that Bob that track one and Alice is that track three with probability 112.",
            "Now, doing this requires all of end to the 4th storage because there is N squared entries along each each dimension here.",
            "As opposed to N squared.",
            "Sorry, we can go on."
        ],
        [
            "Can define of course 3rd order marginals 4th order marginals all the way up to NTH order marginals, at which point we've kind of stored everything that was in the distribution to begin with.",
            "I'm also going to talk about zero order marginals, which is kind of the normalization constant is summing over everything in the distribution and that always equals one.",
            "Anyway, so we hit this tradeoff, which is that we can always capture these higher order dependencies by storing more numbers."
        ],
        [
            "OK, what's interesting is that these marginal summaries are really connected to Fourier analysis, and these are first used in the kind of the machine learning literature by Richie in a paper at a stats in 2007.",
            "This is the intuition 1st order marginals are the lowest frequency setof.",
            "Of free space is second order marginals contain higher frequencies in the 1st order Marginals 3rd order marginals contain even higher frequency market responses?",
            "OK, now you might notice that if if you give me a high order marginal then I can always sum it out to get lower matrix lower order matrix of marginals and so in some sense these marginals aren't 48.",
            "Coefficients themselves because they are not orthogonal to each other, right?",
            "So I'm not going to tell you too much about actually what these 48 coefficients are really talked a little bit about your dusable's yesterday, but just to give you some."
        ],
        [
            "Yeah, the four day coefficients on permutations appear as a collection of square matrices.",
            "OK, and they're ordered in some sense by frequency parameter, and so this first coefficient here represents represents a zero with order kind of information.",
            "That's just one OK.",
            "If we take these two first two blocks, we can reconstruct 1st order first order marginals.",
            "We can take the 1st three blocks and Construct 2nd order marginals and if we go all the way like using all of the blocks, we can construct the entire distribution.",
            "OK, and like I said I won't go into too much detail, but just to give you some idea the way we reconstruct marginals is that we can put the Fourier coefficient matrices by block diagonal kind of components into this bigger matrix, an conjugate it biono matrix.",
            "See, these are kind of constant matrices and they reconstruct the 1st order marginals.",
            "Alright."
        ],
        [
            "So this is what I really want to talk about, which is how to do inference using these representations.",
            "OK, so just for concreteness we're going to focus on hidden Markov models and this setting.",
            "We have a sequence of latent permutations that kind of evolve with respect to time with respect to a transition model or mixing model.",
            "Actually, that's what we call it, which says that for example, checks to him three swap their identities with some probability OK, and at each time step we also get.",
            "A identity observation and this might take the form of the following statement that says that, oh, I saw a green blob at track three at time T. OK. And so the problem statement is the following.",
            "For each time step, we're going to want to find a set of posterior marginals condition on all on all pass observations.",
            "And to do this using our Fourier summaries.",
            "We're going to need to rewrite all of the inference operations for the hidden Markov model in with respect to the Fourier domain."
        ],
        [
            "All right now, just to remind you there's two basic operations for hidden Markov models.",
            "The first one is called the Prediction rollup set, where we take the distribution at time step T. We multiply it by a transition model PF Sigma T + 1 given Sigma T and we somehow the less.",
            "OK, the second position is called conditioning, and that's just base rule.",
            "So we got in observation.",
            "We multiply the prior by the likelihood distribution and that gives us theory are and again.",
            "The question we want to answer is how do we do these operations without enumerating over all N factorial permutations?",
            "So let me just talk about prediction rule at first.",
            "Alright.",
            "We're"
        ],
        [
            "I assume what we call a random walk transition model, and that's to say that every every time step T we're going to draw random a permutation Tao from mixing model Q of Tau OK, and then we're going to simply set Sigma T + 1 to be Tau composed with Sigma T. So just as an example, if QF213 four is 1/2, then that means that tracks one and two swap their identities with probability 1/2.",
            "OK, looks a little bit like this."
        ],
        [
            "OK. No, the reason why we assume this random walk transition random walk model is because that's.",
            "Roll up step can be written as a convolution under these assumptions.",
            "And so, given a prior distribution P of Sigma Tiana mixing model Q, then P of of T + 1 is simply the convolution between Q&PT.",
            "And if you're familiar with the analysis, then you know what's coming."
        ],
        [
            "In fact, the convolutions can be thought of as pointwise products in the Fourier German.",
            "OK, so just to kind of give you some idea if these blocks correspond to the four coefficients of piercing multi and these blocks correspond to the coefficients of the mixing model Q.",
            "Then we can get the P of Sigma sorry PS1 simply by multiplying each block individually using matrix multiplication OK?",
            "And that gives us a representation of piercing multi plus one.",
            "Now the interesting thing is that.",
            "Under with prediction rule up, you never increase your complexity, your representational complexity, and so that's to say that if we started off with 3rd order marginals for example, then after one step prediction roll up we're going to get 3rd order marginals exactly.",
            "OK."
        ],
        [
            "So now let me tell you about conditioning.",
            "Conditioning, unlike prediction rollup, is a pointwise product of the likelihood function in the prior distribution.",
            "And so, just as an example of the likelihood function.",
            "Have that the probability that we see green at track one, given that Alice is that track one is 9/10.",
            "So maybe we know that Alice likes to wear green 9/10 of the time."
        ],
        [
            "Or something?",
            "OK, now, unlike prediction, rollup conditioning does increase the representational complexity.",
            "So just as an example, suppose we start off with first order marginals of the prior distribution and these are the 1st order marginals.",
            "You know that Alice is a track one or track two with probability .9.",
            "We know that's Bob is.",
            "That is also a track one or track two ability .9.",
            "And now suppose that I hand you the following.",
            "1st Order Observation, which is that Kathy is at track one or track two with probability one.",
            "OK, so if you think about that, it means that Alice and Bob can't both be attracts one and two at the same time, right?",
            "And so this is an example of a second order probability.",
            "OK, so we started off with first order probabilities.",
            "After conditioning we ended up with a second order probability.",
            "OK."
        ],
        [
            "Algorithmically, pointwise products correspond to convolutions in the for your domain, with the twists that instead of real valued multiplication, we're going to have to use kind of products.",
            "In our case, and so it looks kind of like polynomial multiplication where we have these blocks for P and these blocks for the likelihood for the prior in the likelihood.",
            "We multiply every pair of them using Kronecker products.",
            "Then there's a projection to the Fourier domain, which I won't have time to talk about.",
            "You can ask me later and we sum up these these projections and that gives us the representation of the posterior distribution.",
            "OK, and again we started off with just kind of two blocks of the prior in the likelihood and we ended up with more blocks in the posterior.",
            "OK, so conditioning can increase your representation of complexity.",
            "Kind of the corollary of this is that conditioning steps can propagate errors."
        ],
        [
            "So if you're doing some kind of band limiting approximation where you're setting high frequency coefficients to 0, then these errors can propagate inside after conditioning.",
            "Anne, what can happen is that sometimes our approximate approximate marginal probabilities can even be negative, so that's a bad thing.",
            "What we do.",
            "As we project to a relaxed marginal polytope corresponding to the 448 coefficients that correspond to non negative marginal probabilities.",
            "OK, and we can formulate this as a QB and I won't go."
        ],
        [
            "Detail, but let me just give you an example of an application that we applied these algorithms to.",
            "This is the same setup as Jonathan's experiments, except now the task is to predict jointly predict the labels of every individual.",
            "OK, this is the performance of a mission tracker, so he knows the outcome of every mixing events.",
            "This is the performance of time independent classification, so these this classifier does not use any time mixing information to do its prediction, so it's only predicting on color histograms.",
            "This is our algorithm without the projection step.",
            "And this is our algorithm with projection.",
            "And it's doing pretty well, like very close to on mission checking, and so we think it's it's pretty much as good as you could do in this case, at least.",
            "Seeing the color histogram cues that we have been using.",
            "OK."
        ],
        [
            "This is a part of scaling, so I'm showing you in the purple line here how exact inference would scale.",
            "In seconds and how how the Fourier domain inference would scale with respect to N. Now you might point out that I'm not plotting in very high here.",
            "It's a little bit misleading, but.",
            "Let's go really fast.",
            "OK?",
            "We she's laughing 1st order.",
            "It's not so bad for 1st order representation.",
            "Def in this case we only need to store N squared numbers, but.",
            "Is that 4th order we're storing over into the 8th numbers, and that's just that's pretty big for if you want to track really large N. So we ask, well, what kind of other structure can we?",
            "Can we exploit other than this for your structure?"
        ],
        [
            "Well, you might notice that if you want to track a lot of people over large spaces, it's sometimes not necessary to check everyone at once.",
            "OK, so just as a cartoon you might have these people that are mixing amongst themselves and it's OK to check them independently.",
            "Sometimes of course, people from the groups walk together OK, The mix, and that means we have to join these groups and track them jointly.",
            "OK, but every now and then we make make an observation.",
            "So for example we see is Bob and if we knew that Bob was originally in the Blue Group which was on this side.",
            "And then we can split the split the groups up again and reason about them independently once once more.",
            "OK, so now we have 3 three groups, three smaller groups that we can reason with.",
            "OK, so this observation forms the basis of what we call our adaptive approach to identity management and is to kind of kind of adaptively break these larger problems into smaller problems an when it's necessary we can join them together to do some kind of joint inference."
        ],
        [
            "So mathematically, we're going to be looking at joint distributions, H. That factor as a product independent product of independent factors F&G, where F is a distribution over the first tracks and G is distribution over the remaining checks.",
            "OK, and there's two problems that we have to consider.",
            "The first one is the joint where we called the joint problem, and that's how we get the Fourier coefficients of the joint given the Fourier coefficients of the factors and.",
            "The split problem is the inverse, which is how do we get the four coefficients.",
            "Factors given Fourier coefficients of the joint distribution.",
            "OK."
        ],
        [
            "I'll tell you first.",
            "Russia's first focus on 1st order marginals.",
            "OK, so the problem is if we have first order marginals of Ng, what's the matrix of 1st order marginals of the joint distribution?",
            "Alright, so this is marginals of FG.",
            "Turns out it's very simple.",
            "It's just going to be a block, kind of a block.",
            "Some of these first order marginals.",
            "Alright.",
            "So that's nice.",
            "It's very simple and just to remind you, you know if this says that Kathy is at track four with probability 120th.",
            "Then, if Kathy and Check 10 aren't in the same group that we're reasoning over, then Kathy is at track 10 with zero probability.",
            "That's what this first order independence thing means.",
            "So what happens at higher order?",
            "Well, for higher order Fourier coefficients, kind of a similar block diagonal structure appears.",
            "But this time we also get kind of product."
        ],
        [
            "For each block.",
            "So it looks a little bit like this.",
            "If these are the blocks for F hat for the 48 transform of F, and these are the four coefficients of G, then each Fourier coefficient matrix of the joint H looks a little bit like this.",
            "Something like this?",
            "OK, in some basis that I won't talk too much about."
        ],
        [
            "The equation for H had is this, so this is just a block.",
            "Some of chronic care products and I'll just say that these multiplicities here are kind of equivalent to the Littlewood Richardson coefficients that have been studied in several mathematical contexts.",
            "There should be hard to compute in general, but for for low order decompositions there they're very tractable and there not a problem to compute in practice.",
            "OK, and the complexity of this operation with known black multiplicities is just the same as prediction rollup for the joint distribution."
        ],
        [
            "OK, so that's how to join.",
            "Let me tell you how to split.",
            "Intuitively, we'd like to invoke."
        ],
        [
            "The join process that is given this matrix here.",
            "We'd like to recover fhat dingy hats OK, and so for example, if you want to recover the second for a block here, you might imagine going down and looking at just kind of grabbing one of these.",
            "Blocks of the joint.",
            "OK, now the problem with using the second block here is that we would have to recover A&B from a chronic or B, which is only possible to do up to a scaling factor.",
            "So what we do instead is to just always look for blocks of the Form F had chronic or one or one chronic or jihad.",
            "An kind of a surprising fact is that these blocks are always exist in every joint, and their efficient define.",
            "OK, so this is how we split, and it's very efficient."
        ],
        [
            "OK, so that's good now.",
            "The remaining problem is that what if we have to use really high high order coefficients of the joint?",
            "For example to get high order coefficients or lower coefficients of the factors.",
            "Turns out we have this marginal preservation guarantee, so let me just say it given order marginals of the independent vectors then we can always recover the NTH order marginals of the joint exactly.",
            "And we have the converse, which is that you know, given NTH order marginals of the joint distribution we can always recover NTH order marginals of the factors."
        ],
        [
            "OK, so let me just tell you about this final problem, which is that great.",
            "We have this machinery for joining and splitting, but how do we know when it's OK to do so?",
            "How do we detect independence?",
            "Alright, so now let me just remind you what 1st order independence looks like if we have, there's three these three groups that were reasoning over then the 1st order marginals looks like this $3 OK. That's almost the case.",
            "The problem is that we never really know what the ordering on the checks in their identities are a priority.",
            "OK, so usually it looks more like this where all the tracks and identities are permitted.",
            "So how do we go from this matrix to this matrix?",
            "Well, that's a well known problem.",
            "It's called clustering, or in our case by clustering.",
            "And there's actually a little bit of A twist that we need to enforce what we call a balance constraint, which forces square blocks in the clustering.",
            "Go into detail about."
        ],
        [
            "OK, so that's detecting independence at a first order level.",
            "Now first order conditions are insufficient.",
            "And let me just tell you why.",
            "So you might imagine checking some football player players here and.",
            "You know you might think it's OK to track the yellow players and the white players independently, because you can detect them reliably and you just split it into two smaller problems.",
            "The problem is that if Alice is on the yellow team here, and Bob is on the white team.",
            "Anne, I tell you that Alice always guards Bob.",
            "Then you would do a lot better at tracking Alice if you knew where Bob was, because they're always next to each other, right?",
            "So that's kind of a higher order dependence that isn't captured by the 1st order independence criterion.",
            "Alright."
        ],
        [
            "Now, so let me So what we do is we always detect that first order, but we can always measure the departure from independence of higher orders and that's what we actually can do.",
            "Alright, and it turns out when this higher order independence doesn't hold, but we can cluster and get the 1st order independence.",
            "Then we have the following result, which is that we can get exact marginals of the subset of checks of each subset of checks when first order independence holds.",
            "Alright, so."
        ],
        [
            "Very nice, let me just give you an example, an experiment.",
            "So this is the task here is to track these ants where every time step we've revealed the identity of each end with some probability an along this axis.",
            "Here is the ratio of observations, so more observations on the right side and not so many observations here.",
            "This is the performance of a non adaptive algorithm and you can see that we get a lot better at predicting the identities, events when we get more observations.",
            "Not too surprising and with our adaptive algorithm we get the following performance which is quite similar.",
            "We don't do so well when there's not so many observations because, well, the the domain we can handle these very smooth distributions quite well already OK, but we do.",
            "We do comparably.",
            "On the other end, where we really."
        ],
        [
            "When is in scaling so."
        ],
        [
            "So in the adaptive were constantly changing, so for for the kind of the smaller groups were allowed to keep like more terms, so it's a little bit complicated to explain what that answer is for the non adaptive.",
            "I'm doing a first order just keeping 1st order, but I don't know.",
            "Sorry backwards keeping 2nd order marginals and making first order observations."
        ],
        [
            "So here, so here's the same experiment, but measuring time running time.",
            "This is the performance of Nonadaptive and so with more observations we need to do more work to condition and so it goes up like this.",
            "This is the running time of the adaptive algorithm.",
            "OK, so it does so with preservations it's allowed to break the problem into more into smaller pieces and so it gets a lot faster.",
            "This is the running time as as as we scale the number of events from 2200 here and this is the running time of nonadaptive which we were not able to scale really past 20."
        ],
        [
            "OK, so in conclusion I told you about a kind of a principled representation for these distributions over permutations.",
            "They have 48 analytic interpretations.",
            "We talked about some inference operations, prediction related conditioning, joined and split.",
            "We talked about approximation and scalability issues and we applied it on some some data.",
            "And so we believe that it kind of open some new and interesting research opportunities in this field.",
            "So let me just end."
        ],
        [
            "100 What the the data set is actually 20 and what we do is we copy the data set several times and we simulate kind events jumping from data copies we don't have.",
            "We don't have 100 points.",
            "That's right.",
            "What are the sizes of these groups that you spin in to make it on your last so so we limit?",
            "Number of.",
            "We do have lots of groups of 1, but we have bigger so they go up to about 15.",
            "In general, so I mean we have a.",
            "So what I do in my code is I have a cap on the size of the matrix and if it gets too big for too big for me to kind of tolerate the running time, then I split it.",
            "You mentioned some open problems or other groups, so you want to work.",
            "On other groups.",
            "Yeah so.",
            "So what's interesting about the some of the operations here is that extends to.",
            "Other groups, in particular, if you look at the prediction, roll up an conditioning steps.",
            "We don't use anything that's particular to this, so you might imagine.",
            "Filtering over SO3 the rotations so.",
            "It might be useful in robotics if you have.",
            "If you want to check maybe a robot arm and you're making noisy measurements about it.",
            "Answer.",
            "National problem, that's right.",
            "So so I think I think the symmetric group is really compelling case where we need to do some kind of aggressive approximations 'cause it's just so big.",
            "I can build you a snake and then you have a lot of them.",
            "Might not be very reliable.",
            "Actually, there has been quite a lot of work on that because when you have it.",
            "He joins when you're essentially doing convolution over.",
            "So that's right, that's right."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, my name is Jonathan and the title of my talk is adapted for a domain inference on the symmetric group.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Carlos Guestrin, my advisor Shaya.",
                    "label": 1
                },
                {
                    "sent": "We just gave the talk and Leo gave us from Stanford.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to begin by.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are you talking about the identity management problem?",
                    "label": 0
                },
                {
                    "sent": "Coincidentally, me and Jonathan made very similar sides.",
                    "label": 0
                },
                {
                    "sent": "The problems that you have.",
                    "label": 0
                },
                {
                    "sent": "Some people they walk along some tracks, tracks, label tracks, one and two in this case, and sometimes these tracks walk together and we call that a mixing event like this and the problem of identity management is to answer questions like this.",
                    "label": 0
                },
                {
                    "sent": "Where is Donald Duck OK at the end?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a bigger problem you may have seen this already too.",
                    "label": 0
                },
                {
                    "sent": "Looks a little bit like this.",
                    "label": 0
                },
                {
                    "sent": "In this case we observe make an observation at the very end we see that goofy goes somewhere here an we have three mixing events.",
                    "label": 0
                },
                {
                    "sent": "Between tracks 1213 and then one and four and the question is where did Biscoe right?",
                    "label": 0
                },
                {
                    "sent": "If you look carefully at the kind of footprints over here, you can tell that beast must have gone to the very end, right?",
                    "label": 0
                },
                {
                    "sent": "Because if Goofy went here, he can only go like this, right?",
                    "label": 0
                },
                {
                    "sent": "So that's that problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In general, we're going to be.",
                    "label": 0
                },
                {
                    "sent": "Concerned with uncertainty in this problem.",
                    "label": 1
                },
                {
                    "sent": "Alright, so sometimes we make noisy observations or we don't know who went where in a mixed event and we model this uncertainty using distributions over permutations and so here's an example of such a distribution.",
                    "label": 1
                },
                {
                    "sent": "Here we have all the permutations of four 4 tracks, and for each track we associated with with their probability.",
                    "label": 0
                },
                {
                    "sent": "So for example 1324.",
                    "label": 1
                },
                {
                    "sent": "Into 110th equal says something like Alice is at track.",
                    "label": 1
                },
                {
                    "sent": "1 bugs attract three.",
                    "label": 0
                },
                {
                    "sent": "Kathy's that track two, and Davids that checked four with probability 110th.",
                    "label": 0
                },
                {
                    "sent": "OK, this is good.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that we can never actually hold this table right in memory, because the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you have any people, then there N factorial permutations and you know it's not so bad when end is really really small.",
                    "label": 0
                },
                {
                    "sent": "But friend equals 15.",
                    "label": 0
                },
                {
                    "sent": "For example you need 2000 petabytes.",
                    "label": 0
                },
                {
                    "sent": "That's just the store that distribution.",
                    "label": 0
                },
                {
                    "sent": "Alright?",
                    "label": 0
                },
                {
                    "sent": "What's even worse is that these graphical models that were so used to using in machine learning they're not very effective because of what we call mutual exclusivity constraints that say that two people can't be at the same track at the same time.",
                    "label": 1
                },
                {
                    "sent": "Alright, So what do we do?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, here's an idea that some people have used before, and it's the idea of storing the marginal probability that a single identity J Maps to a single track.",
                    "label": 1
                },
                {
                    "sent": "I OK, so, for example, we might want to just store the probability that Davids at track four, and to do that we just go through this table, we sum over the entries where D map to four, and in this case we got seven 20th.",
                    "label": 0
                },
                {
                    "sent": "OK, very simple.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could do this for every pair, and doing that we get we can put that in a matrix of N squared numbers and so for example, this entry says that track two with zero probability and essentially might say that Kathy is at track three with probability 120th.",
                    "label": 1
                },
                {
                    "sent": "OK Ann, we started off with N factorial numbers.",
                    "label": 0
                },
                {
                    "sent": "Now we have N squared numbers.",
                    "label": 0
                },
                {
                    "sent": "OK So what kind of things can we capture with this kind of represent?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we can capture.",
                    "label": 0
                },
                {
                    "sent": "These sorts of statements, right Alice is at track one with some probability Bobs track two with some probability.",
                    "label": 0
                },
                {
                    "sent": "Now suppose that tracks one and two are close, and I tell you that Alice and Bob absolutely hate each other.",
                    "label": 1
                },
                {
                    "sent": "They can't stand to be next to each other, OK?",
                    "label": 0
                },
                {
                    "sent": "That says in these mathematical terms is that Alice and Bob occupied tracks, one track one and two jointly.",
                    "label": 0
                },
                {
                    "sent": "With probability 0.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is an example of a higher order probability and so kind of.",
                    "label": 0
                },
                {
                    "sent": "The lesson is that first order summaries can't capture these higher order dependencies that we might.",
                    "label": 0
                },
                {
                    "sent": "That might be really important in tree.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to fix this idea with the second idea, which is the store a matrix of marginals over ordered pairs.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might want to store the fact that Kathy is at track 3.",
                    "label": 0
                },
                {
                    "sent": "And David's at track four with zero probability, and we just sum over these two entries.",
                    "label": 0
                },
                {
                    "sent": "In the table.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We can put this in a matrix of marginal probabilities and this entry might say that Bob that track one and Alice is that track three with probability 112.",
                    "label": 1
                },
                {
                    "sent": "Now, doing this requires all of end to the 4th storage because there is N squared entries along each each dimension here.",
                    "label": 0
                },
                {
                    "sent": "As opposed to N squared.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we can go on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can define of course 3rd order marginals 4th order marginals all the way up to NTH order marginals, at which point we've kind of stored everything that was in the distribution to begin with.",
                    "label": 0
                },
                {
                    "sent": "I'm also going to talk about zero order marginals, which is kind of the normalization constant is summing over everything in the distribution and that always equals one.",
                    "label": 1
                },
                {
                    "sent": "Anyway, so we hit this tradeoff, which is that we can always capture these higher order dependencies by storing more numbers.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what's interesting is that these marginal summaries are really connected to Fourier analysis, and these are first used in the kind of the machine learning literature by Richie in a paper at a stats in 2007.",
                    "label": 0
                },
                {
                    "sent": "This is the intuition 1st order marginals are the lowest frequency setof.",
                    "label": 1
                },
                {
                    "sent": "Of free space is second order marginals contain higher frequencies in the 1st order Marginals 3rd order marginals contain even higher frequency market responses?",
                    "label": 1
                },
                {
                    "sent": "OK, now you might notice that if if you give me a high order marginal then I can always sum it out to get lower matrix lower order matrix of marginals and so in some sense these marginals aren't 48.",
                    "label": 0
                },
                {
                    "sent": "Coefficients themselves because they are not orthogonal to each other, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to tell you too much about actually what these 48 coefficients are really talked a little bit about your dusable's yesterday, but just to give you some.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, the four day coefficients on permutations appear as a collection of square matrices.",
                    "label": 1
                },
                {
                    "sent": "OK, and they're ordered in some sense by frequency parameter, and so this first coefficient here represents represents a zero with order kind of information.",
                    "label": 0
                },
                {
                    "sent": "That's just one OK.",
                    "label": 1
                },
                {
                    "sent": "If we take these two first two blocks, we can reconstruct 1st order first order marginals.",
                    "label": 0
                },
                {
                    "sent": "We can take the 1st three blocks and Construct 2nd order marginals and if we go all the way like using all of the blocks, we can construct the entire distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and like I said I won't go into too much detail, but just to give you some idea the way we reconstruct marginals is that we can put the Fourier coefficient matrices by block diagonal kind of components into this bigger matrix, an conjugate it biono matrix.",
                    "label": 0
                },
                {
                    "sent": "See, these are kind of constant matrices and they reconstruct the 1st order marginals.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what I really want to talk about, which is how to do inference using these representations.",
                    "label": 0
                },
                {
                    "sent": "OK, so just for concreteness we're going to focus on hidden Markov models and this setting.",
                    "label": 0
                },
                {
                    "sent": "We have a sequence of latent permutations that kind of evolve with respect to time with respect to a transition model or mixing model.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's what we call it, which says that for example, checks to him three swap their identities with some probability OK, and at each time step we also get.",
                    "label": 0
                },
                {
                    "sent": "A identity observation and this might take the form of the following statement that says that, oh, I saw a green blob at track three at time T. OK. And so the problem statement is the following.",
                    "label": 0
                },
                {
                    "sent": "For each time step, we're going to want to find a set of posterior marginals condition on all on all pass observations.",
                    "label": 0
                },
                {
                    "sent": "And to do this using our Fourier summaries.",
                    "label": 0
                },
                {
                    "sent": "We're going to need to rewrite all of the inference operations for the hidden Markov model in with respect to the Fourier domain.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All right now, just to remind you there's two basic operations for hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "The first one is called the Prediction rollup set, where we take the distribution at time step T. We multiply it by a transition model PF Sigma T + 1 given Sigma T and we somehow the less.",
                    "label": 0
                },
                {
                    "sent": "OK, the second position is called conditioning, and that's just base rule.",
                    "label": 0
                },
                {
                    "sent": "So we got in observation.",
                    "label": 0
                },
                {
                    "sent": "We multiply the prior by the likelihood distribution and that gives us theory are and again.",
                    "label": 0
                },
                {
                    "sent": "The question we want to answer is how do we do these operations without enumerating over all N factorial permutations?",
                    "label": 1
                },
                {
                    "sent": "So let me just talk about prediction rule at first.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "We're",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I assume what we call a random walk transition model, and that's to say that every every time step T we're going to draw random a permutation Tao from mixing model Q of Tau OK, and then we're going to simply set Sigma T + 1 to be Tau composed with Sigma T. So just as an example, if QF213 four is 1/2, then that means that tracks one and two swap their identities with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "OK, looks a little bit like this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. No, the reason why we assume this random walk transition random walk model is because that's.",
                    "label": 0
                },
                {
                    "sent": "Roll up step can be written as a convolution under these assumptions.",
                    "label": 1
                },
                {
                    "sent": "And so, given a prior distribution P of Sigma Tiana mixing model Q, then P of of T + 1 is simply the convolution between Q&PT.",
                    "label": 0
                },
                {
                    "sent": "And if you're familiar with the analysis, then you know what's coming.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, the convolutions can be thought of as pointwise products in the Fourier German.",
                    "label": 1
                },
                {
                    "sent": "OK, so just to kind of give you some idea if these blocks correspond to the four coefficients of piercing multi and these blocks correspond to the coefficients of the mixing model Q.",
                    "label": 0
                },
                {
                    "sent": "Then we can get the P of Sigma sorry PS1 simply by multiplying each block individually using matrix multiplication OK?",
                    "label": 0
                },
                {
                    "sent": "And that gives us a representation of piercing multi plus one.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing is that.",
                    "label": 0
                },
                {
                    "sent": "Under with prediction rule up, you never increase your complexity, your representational complexity, and so that's to say that if we started off with 3rd order marginals for example, then after one step prediction roll up we're going to get 3rd order marginals exactly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me tell you about conditioning.",
                    "label": 0
                },
                {
                    "sent": "Conditioning, unlike prediction rollup, is a pointwise product of the likelihood function in the prior distribution.",
                    "label": 1
                },
                {
                    "sent": "And so, just as an example of the likelihood function.",
                    "label": 1
                },
                {
                    "sent": "Have that the probability that we see green at track one, given that Alice is that track one is 9/10.",
                    "label": 0
                },
                {
                    "sent": "So maybe we know that Alice likes to wear green 9/10 of the time.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or something?",
                    "label": 0
                },
                {
                    "sent": "OK, now, unlike prediction, rollup conditioning does increase the representational complexity.",
                    "label": 0
                },
                {
                    "sent": "So just as an example, suppose we start off with first order marginals of the prior distribution and these are the 1st order marginals.",
                    "label": 1
                },
                {
                    "sent": "You know that Alice is a track one or track two with probability .9.",
                    "label": 0
                },
                {
                    "sent": "We know that's Bob is.",
                    "label": 0
                },
                {
                    "sent": "That is also a track one or track two ability .9.",
                    "label": 0
                },
                {
                    "sent": "And now suppose that I hand you the following.",
                    "label": 1
                },
                {
                    "sent": "1st Order Observation, which is that Kathy is at track one or track two with probability one.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you think about that, it means that Alice and Bob can't both be attracts one and two at the same time, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is an example of a second order probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so we started off with first order probabilities.",
                    "label": 0
                },
                {
                    "sent": "After conditioning we ended up with a second order probability.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithmically, pointwise products correspond to convolutions in the for your domain, with the twists that instead of real valued multiplication, we're going to have to use kind of products.",
                    "label": 1
                },
                {
                    "sent": "In our case, and so it looks kind of like polynomial multiplication where we have these blocks for P and these blocks for the likelihood for the prior in the likelihood.",
                    "label": 0
                },
                {
                    "sent": "We multiply every pair of them using Kronecker products.",
                    "label": 1
                },
                {
                    "sent": "Then there's a projection to the Fourier domain, which I won't have time to talk about.",
                    "label": 0
                },
                {
                    "sent": "You can ask me later and we sum up these these projections and that gives us the representation of the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and again we started off with just kind of two blocks of the prior in the likelihood and we ended up with more blocks in the posterior.",
                    "label": 1
                },
                {
                    "sent": "OK, so conditioning can increase your representation of complexity.",
                    "label": 0
                },
                {
                    "sent": "Kind of the corollary of this is that conditioning steps can propagate errors.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you're doing some kind of band limiting approximation where you're setting high frequency coefficients to 0, then these errors can propagate inside after conditioning.",
                    "label": 0
                },
                {
                    "sent": "Anne, what can happen is that sometimes our approximate approximate marginal probabilities can even be negative, so that's a bad thing.",
                    "label": 0
                },
                {
                    "sent": "What we do.",
                    "label": 0
                },
                {
                    "sent": "As we project to a relaxed marginal polytope corresponding to the 448 coefficients that correspond to non negative marginal probabilities.",
                    "label": 1
                },
                {
                    "sent": "OK, and we can formulate this as a QB and I won't go.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detail, but let me just give you an example of an application that we applied these algorithms to.",
                    "label": 0
                },
                {
                    "sent": "This is the same setup as Jonathan's experiments, except now the task is to predict jointly predict the labels of every individual.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the performance of a mission tracker, so he knows the outcome of every mixing events.",
                    "label": 1
                },
                {
                    "sent": "This is the performance of time independent classification, so these this classifier does not use any time mixing information to do its prediction, so it's only predicting on color histograms.",
                    "label": 1
                },
                {
                    "sent": "This is our algorithm without the projection step.",
                    "label": 0
                },
                {
                    "sent": "And this is our algorithm with projection.",
                    "label": 1
                },
                {
                    "sent": "And it's doing pretty well, like very close to on mission checking, and so we think it's it's pretty much as good as you could do in this case, at least.",
                    "label": 0
                },
                {
                    "sent": "Seeing the color histogram cues that we have been using.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a part of scaling, so I'm showing you in the purple line here how exact inference would scale.",
                    "label": 0
                },
                {
                    "sent": "In seconds and how how the Fourier domain inference would scale with respect to N. Now you might point out that I'm not plotting in very high here.",
                    "label": 1
                },
                {
                    "sent": "It's a little bit misleading, but.",
                    "label": 0
                },
                {
                    "sent": "Let's go really fast.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "We she's laughing 1st order.",
                    "label": 0
                },
                {
                    "sent": "It's not so bad for 1st order representation.",
                    "label": 0
                },
                {
                    "sent": "Def in this case we only need to store N squared numbers, but.",
                    "label": 0
                },
                {
                    "sent": "Is that 4th order we're storing over into the 8th numbers, and that's just that's pretty big for if you want to track really large N. So we ask, well, what kind of other structure can we?",
                    "label": 0
                },
                {
                    "sent": "Can we exploit other than this for your structure?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, you might notice that if you want to track a lot of people over large spaces, it's sometimes not necessary to check everyone at once.",
                    "label": 0
                },
                {
                    "sent": "OK, so just as a cartoon you might have these people that are mixing amongst themselves and it's OK to check them independently.",
                    "label": 0
                },
                {
                    "sent": "Sometimes of course, people from the groups walk together OK, The mix, and that means we have to join these groups and track them jointly.",
                    "label": 0
                },
                {
                    "sent": "OK, but every now and then we make make an observation.",
                    "label": 0
                },
                {
                    "sent": "So for example we see is Bob and if we knew that Bob was originally in the Blue Group which was on this side.",
                    "label": 1
                },
                {
                    "sent": "And then we can split the split the groups up again and reason about them independently once once more.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have 3 three groups, three smaller groups that we can reason with.",
                    "label": 0
                },
                {
                    "sent": "OK, so this observation forms the basis of what we call our adaptive approach to identity management and is to kind of kind of adaptively break these larger problems into smaller problems an when it's necessary we can join them together to do some kind of joint inference.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So mathematically, we're going to be looking at joint distributions, H. That factor as a product independent product of independent factors F&G, where F is a distribution over the first tracks and G is distribution over the remaining checks.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's two problems that we have to consider.",
                    "label": 0
                },
                {
                    "sent": "The first one is the joint where we called the joint problem, and that's how we get the Fourier coefficients of the joint given the Fourier coefficients of the factors and.",
                    "label": 1
                },
                {
                    "sent": "The split problem is the inverse, which is how do we get the four coefficients.",
                    "label": 0
                },
                {
                    "sent": "Factors given Fourier coefficients of the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll tell you first.",
                    "label": 0
                },
                {
                    "sent": "Russia's first focus on 1st order marginals.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem is if we have first order marginals of Ng, what's the matrix of 1st order marginals of the joint distribution?",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is marginals of FG.",
                    "label": 0
                },
                {
                    "sent": "Turns out it's very simple.",
                    "label": 0
                },
                {
                    "sent": "It's just going to be a block, kind of a block.",
                    "label": 0
                },
                {
                    "sent": "Some of these first order marginals.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "It's very simple and just to remind you, you know if this says that Kathy is at track four with probability 120th.",
                    "label": 0
                },
                {
                    "sent": "Then, if Kathy and Check 10 aren't in the same group that we're reasoning over, then Kathy is at track 10 with zero probability.",
                    "label": 0
                },
                {
                    "sent": "That's what this first order independence thing means.",
                    "label": 0
                },
                {
                    "sent": "So what happens at higher order?",
                    "label": 0
                },
                {
                    "sent": "Well, for higher order Fourier coefficients, kind of a similar block diagonal structure appears.",
                    "label": 0
                },
                {
                    "sent": "But this time we also get kind of product.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each block.",
                    "label": 0
                },
                {
                    "sent": "So it looks a little bit like this.",
                    "label": 0
                },
                {
                    "sent": "If these are the blocks for F hat for the 48 transform of F, and these are the four coefficients of G, then each Fourier coefficient matrix of the joint H looks a little bit like this.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "OK, in some basis that I won't talk too much about.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The equation for H had is this, so this is just a block.",
                    "label": 0
                },
                {
                    "sent": "Some of chronic care products and I'll just say that these multiplicities here are kind of equivalent to the Littlewood Richardson coefficients that have been studied in several mathematical contexts.",
                    "label": 0
                },
                {
                    "sent": "There should be hard to compute in general, but for for low order decompositions there they're very tractable and there not a problem to compute in practice.",
                    "label": 1
                },
                {
                    "sent": "OK, and the complexity of this operation with known black multiplicities is just the same as prediction rollup for the joint distribution.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's how to join.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you how to split.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, we'd like to invoke.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The join process that is given this matrix here.",
                    "label": 0
                },
                {
                    "sent": "We'd like to recover fhat dingy hats OK, and so for example, if you want to recover the second for a block here, you might imagine going down and looking at just kind of grabbing one of these.",
                    "label": 0
                },
                {
                    "sent": "Blocks of the joint.",
                    "label": 0
                },
                {
                    "sent": "OK, now the problem with using the second block here is that we would have to recover A&B from a chronic or B, which is only possible to do up to a scaling factor.",
                    "label": 1
                },
                {
                    "sent": "So what we do instead is to just always look for blocks of the Form F had chronic or one or one chronic or jihad.",
                    "label": 0
                },
                {
                    "sent": "An kind of a surprising fact is that these blocks are always exist in every joint, and their efficient define.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how we split, and it's very efficient.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's good now.",
                    "label": 0
                },
                {
                    "sent": "The remaining problem is that what if we have to use really high high order coefficients of the joint?",
                    "label": 0
                },
                {
                    "sent": "For example to get high order coefficients or lower coefficients of the factors.",
                    "label": 0
                },
                {
                    "sent": "Turns out we have this marginal preservation guarantee, so let me just say it given order marginals of the independent vectors then we can always recover the NTH order marginals of the joint exactly.",
                    "label": 1
                },
                {
                    "sent": "And we have the converse, which is that you know, given NTH order marginals of the joint distribution we can always recover NTH order marginals of the factors.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me just tell you about this final problem, which is that great.",
                    "label": 0
                },
                {
                    "sent": "We have this machinery for joining and splitting, but how do we know when it's OK to do so?",
                    "label": 0
                },
                {
                    "sent": "How do we detect independence?",
                    "label": 0
                },
                {
                    "sent": "Alright, so now let me just remind you what 1st order independence looks like if we have, there's three these three groups that were reasoning over then the 1st order marginals looks like this $3 OK. That's almost the case.",
                    "label": 1
                },
                {
                    "sent": "The problem is that we never really know what the ordering on the checks in their identities are a priority.",
                    "label": 0
                },
                {
                    "sent": "OK, so usually it looks more like this where all the tracks and identities are permitted.",
                    "label": 0
                },
                {
                    "sent": "So how do we go from this matrix to this matrix?",
                    "label": 0
                },
                {
                    "sent": "Well, that's a well known problem.",
                    "label": 0
                },
                {
                    "sent": "It's called clustering, or in our case by clustering.",
                    "label": 0
                },
                {
                    "sent": "And there's actually a little bit of A twist that we need to enforce what we call a balance constraint, which forces square blocks in the clustering.",
                    "label": 1
                },
                {
                    "sent": "Go into detail about.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's detecting independence at a first order level.",
                    "label": 0
                },
                {
                    "sent": "Now first order conditions are insufficient.",
                    "label": 0
                },
                {
                    "sent": "And let me just tell you why.",
                    "label": 0
                },
                {
                    "sent": "So you might imagine checking some football player players here and.",
                    "label": 0
                },
                {
                    "sent": "You know you might think it's OK to track the yellow players and the white players independently, because you can detect them reliably and you just split it into two smaller problems.",
                    "label": 0
                },
                {
                    "sent": "The problem is that if Alice is on the yellow team here, and Bob is on the white team.",
                    "label": 1
                },
                {
                    "sent": "Anne, I tell you that Alice always guards Bob.",
                    "label": 0
                },
                {
                    "sent": "Then you would do a lot better at tracking Alice if you knew where Bob was, because they're always next to each other, right?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a higher order dependence that isn't captured by the 1st order independence criterion.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, so let me So what we do is we always detect that first order, but we can always measure the departure from independence of higher orders and that's what we actually can do.",
                    "label": 1
                },
                {
                    "sent": "Alright, and it turns out when this higher order independence doesn't hold, but we can cluster and get the 1st order independence.",
                    "label": 0
                },
                {
                    "sent": "Then we have the following result, which is that we can get exact marginals of the subset of checks of each subset of checks when first order independence holds.",
                    "label": 1
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very nice, let me just give you an example, an experiment.",
                    "label": 0
                },
                {
                    "sent": "So this is the task here is to track these ants where every time step we've revealed the identity of each end with some probability an along this axis.",
                    "label": 0
                },
                {
                    "sent": "Here is the ratio of observations, so more observations on the right side and not so many observations here.",
                    "label": 1
                },
                {
                    "sent": "This is the performance of a non adaptive algorithm and you can see that we get a lot better at predicting the identities, events when we get more observations.",
                    "label": 0
                },
                {
                    "sent": "Not too surprising and with our adaptive algorithm we get the following performance which is quite similar.",
                    "label": 0
                },
                {
                    "sent": "We don't do so well when there's not so many observations because, well, the the domain we can handle these very smooth distributions quite well already OK, but we do.",
                    "label": 0
                },
                {
                    "sent": "We do comparably.",
                    "label": 0
                },
                {
                    "sent": "On the other end, where we really.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When is in scaling so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the adaptive were constantly changing, so for for the kind of the smaller groups were allowed to keep like more terms, so it's a little bit complicated to explain what that answer is for the non adaptive.",
                    "label": 0
                },
                {
                    "sent": "I'm doing a first order just keeping 1st order, but I don't know.",
                    "label": 0
                },
                {
                    "sent": "Sorry backwards keeping 2nd order marginals and making first order observations.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, so here's the same experiment, but measuring time running time.",
                    "label": 0
                },
                {
                    "sent": "This is the performance of Nonadaptive and so with more observations we need to do more work to condition and so it goes up like this.",
                    "label": 0
                },
                {
                    "sent": "This is the running time of the adaptive algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so it does so with preservations it's allowed to break the problem into more into smaller pieces and so it gets a lot faster.",
                    "label": 0
                },
                {
                    "sent": "This is the running time as as as we scale the number of events from 2200 here and this is the running time of nonadaptive which we were not able to scale really past 20.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion I told you about a kind of a principled representation for these distributions over permutations.",
                    "label": 1
                },
                {
                    "sent": "They have 48 analytic interpretations.",
                    "label": 1
                },
                {
                    "sent": "We talked about some inference operations, prediction related conditioning, joined and split.",
                    "label": 0
                },
                {
                    "sent": "We talked about approximation and scalability issues and we applied it on some some data.",
                    "label": 1
                },
                {
                    "sent": "And so we believe that it kind of open some new and interesting research opportunities in this field.",
                    "label": 0
                },
                {
                    "sent": "So let me just end.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "100 What the the data set is actually 20 and what we do is we copy the data set several times and we simulate kind events jumping from data copies we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have 100 points.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "What are the sizes of these groups that you spin in to make it on your last so so we limit?",
                    "label": 0
                },
                {
                    "sent": "Number of.",
                    "label": 0
                },
                {
                    "sent": "We do have lots of groups of 1, but we have bigger so they go up to about 15.",
                    "label": 0
                },
                {
                    "sent": "In general, so I mean we have a.",
                    "label": 0
                },
                {
                    "sent": "So what I do in my code is I have a cap on the size of the matrix and if it gets too big for too big for me to kind of tolerate the running time, then I split it.",
                    "label": 0
                },
                {
                    "sent": "You mentioned some open problems or other groups, so you want to work.",
                    "label": 0
                },
                {
                    "sent": "On other groups.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting about the some of the operations here is that extends to.",
                    "label": 0
                },
                {
                    "sent": "Other groups, in particular, if you look at the prediction, roll up an conditioning steps.",
                    "label": 0
                },
                {
                    "sent": "We don't use anything that's particular to this, so you might imagine.",
                    "label": 0
                },
                {
                    "sent": "Filtering over SO3 the rotations so.",
                    "label": 0
                },
                {
                    "sent": "It might be useful in robotics if you have.",
                    "label": 0
                },
                {
                    "sent": "If you want to check maybe a robot arm and you're making noisy measurements about it.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                },
                {
                    "sent": "National problem, that's right.",
                    "label": 0
                },
                {
                    "sent": "So so I think I think the symmetric group is really compelling case where we need to do some kind of aggressive approximations 'cause it's just so big.",
                    "label": 0
                },
                {
                    "sent": "I can build you a snake and then you have a lot of them.",
                    "label": 0
                },
                {
                    "sent": "Might not be very reliable.",
                    "label": 0
                },
                {
                    "sent": "Actually, there has been quite a lot of work on that because when you have it.",
                    "label": 0
                },
                {
                    "sent": "He joins when you're essentially doing convolution over.",
                    "label": 0
                },
                {
                    "sent": "So that's right, that's right.",
                    "label": 0
                }
            ]
        }
    }
}