{
    "id": "rr3pwwwey3cu2eda2y2weh6o7unzxbrf",
    "title": "Should all Machine Learning be Bayesian? Should all Bayesian models be non-parametric?",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/bark08_ghahramani_samlbb/",
    "segmentation": [
        [
            "Alright, so most of the conversation at dinner actually revolves around people falling asleep during talks after dinner, so it's making me very nervous.",
            "But you know, I notice some of you have beer and some of you have coffee, so I can just keep keep an eye on whether there's any correlation between who's actually falling asleep.",
            "Anyway, I was in a funny mood when I wrote the abstract to this and this is what's gotten me in this position here.",
            "I just thought, well, what would I want to talk about at a small workshop among friends, right?",
            "And I don't really want to talk about my own research, 'cause, you know, I know about it and it bores me to talk about my research.",
            "But it's fun to talk about, you know, kind of the big picture a little bit so."
        ],
        [
            "Here's what I'm going to do.",
            "I'm going to.",
            "I'm going to run through, sort of.",
            "The Canonical story we could give to motivate people for why they would want to be Bayesian OK. And then I'm going to try to critique that a bit, and I'm going to say, well, do we really believe this?",
            "And then I'm just going to jump around and talk about a few other questions that I think should really be on our minds and certainly are on my mind.",
            "So you know, a lot of machine learning involves classification, regression, clustering, and things like that.",
            "And we all know."
        ],
        [
            "So that basically we can take any Canonical machine learning problem and write it down in terms of a few components.",
            "We have some data.",
            "We have some model with some parameters and then we have some gold that as Bayesians we can write out as an inference problem and most people don't really think about things exactly in that way.",
            "But really, you know, take a Canonical problem like linear classification.",
            "You have some data set in these crosses, a knows you have some model some.",
            "Linear boundary, and rather than thinking of it as some sort of optimization problem, we can think about it as the problem of inferring some unknown quantity like these parameters from the data so as to predict future labels.",
            "So this is."
        ],
        [
            "Sort of like a Canonical tutorial.",
            "You would give an, you know, the motivation I and I'm sure all of you also give for doing this is that we can apply the basic rules of probability to doing this sort of learning.",
            "We all know what these."
        ],
        [
            "Rules are we apply it in the context of machine learning and we think of computing posteriors over unknown quantities given the data by multiplying some likelihood by some prior an renormalizing an we can do model comparison.",
            "We can do prediction.",
            "So this is part of this.",
            "I'm blasting through the Canonical story so that we can critique it."
        ],
        [
            "Later, OK, so that's it.",
            "That's like, really, you know.",
            "In some ways I kind of feel like that's all we need to know about machine learning.",
            "The rest is just sort of mechanics, algorithmics and so on.",
            "OK, but that's not how most people in the field of machine learning think about this, which surprises me still."
        ],
        [
            "So let's now ask some of these questions well.",
            "Should all machine learning be Bayesian an you know if we think the answer is yes, then why isn't it that so they will come to that later?",
            "Like you know, if we really think that this basic framework is really elegant, why?",
            "If I go to a typical conference like ICM, LYR, Bayesian papers, maybe like 10% of papers at most.",
            "So again, what I'll do is I'll blast through some of the arguments that that at least I have given in tutorials.",
            "These are clearly not things I've come up with.",
            "These are things I've picked up from other places I give in tutorials for why I you know how I try to convince people that Bayesianism is the right way to go.",
            "OK, and then we can go and critique those 'cause we're in in small company here, and nobody on video lectures is really going to watch this right, so?"
        ],
        [
            "So alright, so why should we be Bayesian?",
            "So one of the arguments is that if we want a system to behave intelligently, it should represent beliefs about propositions in the world, and we want to represent the strength of those beliefs numerically in the brain of some artificial agent, like a robot for example.",
            "And we want some set of rules or calculus for manipulating those trends of beliefs so."
        ],
        [
            "Of the standard arguments you could give from the artificial intelligence point of view, is this sort of arguments based on the Cox axioms that are described in James?",
            "Is book an?",
            "There's a really nice paper by Kevin Van Horn.",
            "Actually that is sort of a more modern treatment of this and discuss some of the critiques of those arguments in sort of dispels those critiques.",
            "So the arguments are basically that if we want to represent strength of belief and propositions about the world.",
            "Anne.",
            "Assume that we want to.",
            "The strength of belief to be somewhere between zero and one where zero is that X is definitely not true, whereas one is that X is definitely true and we write down conditional beliefs.",
            "What is your strength of belief in X given Y?",
            "Then if we accept certain axioms, you can read about them if you haven't already, then the consequences of these axioms is that belief functions this B of X.",
            "B of X given Y, etc.",
            "They must satisfy the rules of probability theory including Bayes rule, so this is one way we can justify.",
            "That if we want artificial systems, have beliefs about the world, then we should use the language of probability like we do in normal discourse, right?",
            "Like you know, we say I think it's probable that you know the sun will rise tomorrow.",
            "That's a statement of 1's belief about something that's going to happen in the future.",
            "Certainly not a repeatable experiment.",
            "OK, so."
        ],
        [
            "That's one classic argument for Bayesian thinking.",
            "Another classic argument is this Dutch book theorem, where now we're assuming that we have strengths of beliefs, and we're also gambolling people, so we're willing to bet.",
            "And if our strength of belief is .9, then we're willing to accept a bet where if X is true, we win anything greater than or equal to $1.",
            "If X is false, we lose $9 or something like that, and the Dutch book argument says that.",
            "Unless our beliefs, our strength abli satisfied the rules of probability theory, including Bayes rule, then there exists a set of simultaneous bets called a Dutch book which you would be willing to accept, but which for which you're guaranteed to lose money no matter what the outcome is.",
            "So essentially the only way to guard against these Dutch books is for your beliefs to cohere with the rules of probability.",
            "They don't have to be correct.",
            "You don't have to have.",
            "The the right beliefs about the real world.",
            "They just have to be coherent with each other.",
            "You can't be inconsistent.",
            "That's sort of a consistency argument.",
            "It's another argument that you know we can standard."
        ],
        [
            "A gift for Bayesian thinking and then there are other arguments like asymptotic certainty.",
            "Now we're considering an IID cases sort of statistical argument.",
            "We assume we have a data set D sub N consisting of N data points and it was generated from some true parameter value data star.",
            "Then, under some regularity conditions, as long as the probability density associated with Theta star is.",
            "Non zero, and really what the regularity conditions involve having some probability mass around Theta star.",
            "Then what happens is in the limit, as the number of data points goes to Infinity, your posterior over Theta given the data, will converge to a Delta function around Theta star.",
            "So this is a nice argument if you want to convince somebody who cares about asymptotics or who's trained in statistics, you can say look if the stuff came from some truth data star, we're going to converge to that.",
            "The only condition we need on the prior is to put some mass around the truth.",
            "It's not that, and I've heard this phrase and it really, really annoys me.",
            "I've heard people talking about how difficult it is to come up with the right prior, and I think that's a completely crazy way of misunderstanding Bayesian thinking.",
            "There isn't a right prior you have to have whatever prior you believe in.",
            "The only condition you need on the prior really is that it puts some probability mass close to the truth, and that might be a very big condition, but.",
            "You know any prior with this property will work in this sense, Yep.",
            "Critique.",
            "You have to be careful about dimensional right?",
            "So the caveat here.",
            "There are many caveats, and this is actually for finite dimensional models.",
            "Yes, good point.",
            "So for example, for Gaussian process is hard to prove these things and people have spent some time to think about these.",
            "They have, yeah, but but it's not trivial.",
            "Counter example.",
            "Yeah, it's a widely cited paper that you know has made many people not be Bayesian and not like nonparametric Bayesian methods.",
            "And I think it's probably abused, but but I don't really understand it well enough to know why it's abuse.",
            "I just think my priors that it's been abused.",
            "Alright, yeah?",
            "Things that are reasonable yeah.",
            "So so be be, you know, give give a little chance to all the different options that you can think of and now the yeah go ahead, thanks.",
            "I mean, it's never Infinity like I'll show tomorrow.",
            "There cases where the prior covers the true thing and you still get very bad performance for realistic.",
            "Sorry yeah yeah.",
            "Like any like any asymptotic statement you're living in SM Topia, which is a fantasy land, right?",
            "So so you know.",
            "So clearly you know this will satisfy the people who like to live in that fantasy land, but North is always finite.",
            "You know this statement is geared towards a certain type of person who cares about these things.",
            "They're called statisticians.",
            "Yeah, well, some Bayesian statisticians too.",
            "Alright, So what about if it's UN realizable in the UN realizable case?",
            "That means you know if the data was generated from some distribution which cannot be modeled by any Theta, then what you'll converge is to a Delta function around some Theta hat, and reassuringly, that Theta hat is the one that minimizes the KL divergent between the true distribution P star of X&P of X given Theta, and in fact that's actually the maximum likelihood where you'll converge.",
            "Two essentially, under the regularity conditions under the prior, you'll converge the maximum likelihood estimate.",
            "Theta had a Delta function on the maximum likelihood estimate, so that should also reassure some people in this UN realizable case.",
            "And for using a uniform distribution is bigger box.",
            "Yeah, you know, if you take this stuff literally then you really you really?",
            "Missed the point of the beauty of doing Bayesian inference in that you know, these are asymptotically statements.",
            "And yeah, if you use a uniform distribution over all possible thetas that you could think of, you'll converge.",
            "In either of these cases.",
            "But the rate at which you converge, how quickly you converge, etc.",
            "If that uniform distribution is even well defined, is heavily governed by how much probability mass you put on the truth.",
            "So it's a betting game you want to put.",
            "As much mass as you can on all the good bets, but you want to put a little bit of mass on the bad bets as well, just in case they're the right thing 'cause you don't want to be penalized infinitely for getting things wrong, lucky.",
            "Words.",
            "Yeah, I mean this is what this would say, right?",
            "'cause you're doing maximum likelihood anyway.",
            "He just converges to a Delta function on the maximum likelihood, yeah?",
            "So yeah, I mean it's this is a good question.",
            "I didn't actually bring this up, but.",
            "You know, we can also ask ourselves in this small room here.",
            "You know, in what regime?",
            "Are Bayesian methods really good right?",
            "Where are they most useful and if we have a finite dimensional parameter?",
            "Like some D dimensional parameter space, and we're tending N to Infinity.",
            "Then if N gets big enough.",
            "We're going to be fine using maximum likelihood methods, right?",
            "I mean, I think at least the interesting thing is a lot of real world applications and you working at Microsoft will know about some of these applications have both incredibly large and D. The dimensionality of your parameters incredibly large, and in those cases you know the asymptotic that you do when N goes to Infinity is not really that relevant because the data is incredibly sparse and the number of.",
            "Dimensions of your parameter space is so huge you don't ever have enough data.",
            "So yeah.",
            "Alright, so."
        ],
        [
            "Let's move through this.",
            "This is a nice argument.",
            "The asymptotic consensus argument, where all Bayesians I'll end up happily agreeing with each other.",
            "I kind of like this, so you know you have two Bayesians.",
            "Pick two people randomly from this room.",
            "We have different priors, P1 of Theta and P2 of Theta.",
            "But we observe the same data now.",
            "Assume that we both agree on the set of possible and impossible values of Theta.",
            "So the support in terms of Theta space for P1 of Theta is the same as the support for P2 of Theta.",
            "Then under that condition, in the limit as N goes to Infinity, the posteriors will converge in some measure of the distance between these distributions, like any over any set.",
            "the Super any set of the probability mass assigned to that set is the same as the probability mass assigned to that set by the other posterior, so.",
            "So, so that's nice.",
            "We can have different priors if we observe enough data will converge the same posterior.",
            "Function.",
            "So that I think you know.",
            "And again I haven't worked through all of the mathematics.",
            "I've just sort of, you know, found this result and used it.",
            "The argument of the.",
            "The mass, sorry, the support being the same.",
            "I don't know whether it requires other conditions on being a probability density or having Delta functions in the same place and so on, right?",
            "I mean, it might be that there are other conditions.",
            "I would.",
            "I wouldn't be surprised."
        ],
        [
            "OK, so then we have this nice Occam's razor thing where we talk about model selection using the marginal likelihood.",
            "So the idea that one hopes that when we observe some data set, D will be able to use this marginal likelihood or integrated likelihood or evidence too.",
            "Select a model class is neither too simple nor too complex, so we can.",
            "Hopefully reject models.",
            "That are either too simple or too complex just by computing the marginal likelihood and the argument comes from the conservation argument where any any probabilistic model with some.",
            "Set of parameters like let's say M corresponds to the number of parameters in your model.",
            "For example, which is one way of measuring complexity.",
            "Although it's a very simplistic way of measuring complexity.",
            "Any model will assign some amount of probability mass to different possible datasets, and we can think of simple models assigning lots of probability mass to certain simple datasets and no mass or very little mass to complicated datasets and vice versa for complex models.",
            "But because there is this sort of conservation idea that each model has to spread out this mass, if we observe a particular data set, then it's not the case that the more complex model will always win.",
            "So unlike maximum likelihood, we're not going to get the more complex model to win."
        ],
        [
            "OK, so this is the marginal likelihood idea.",
            "So here are some.",
            "You know potential advantages we could use to try to sell Bayesian methods to other people.",
            "And I kind of believe these things.",
            "I think at least I believe them when I wrote this slide last night.",
            "So, you know, we're trying to be coherent and honest about uncertainty, right?",
            "That's part of the goal.",
            "And it's easy to do model comparison and model selection.",
            "At least we know in theory what we should be doing right.",
            "Although in practice it might be hard.",
            "There is a really nice rational process for model building and for adding domain knowledge, and that's not true if you.",
            "If you look at like.",
            "Other standard machine learning methods, you know, let's say you're boosting decision trees and you have some domain knowledge.",
            "It's really hard to know like you know, should you change the way you build the decision trees, should you change your boosting algorithm?",
            "Should we change the features that we use etc.",
            "And I think the basic method gives us nice way of adding domain knowledge and building our models and it's really easy to handle missing and hidden data because it's all about missing in hidden data.",
            "So there's no question about that, whereas if you take a standard method like a support vector machine for classification, and you say you know what does it take to handle missing inputs in a support vector machine, will there be probably half a dozen answers, none of which are particularly satisfactory, OK?",
            "The disadvantages are basic methods.",
            "We can talk about later.",
            "Thank you."
        ],
        [
            "So one of the things people obsess about a lot is where does the prior come from?",
            "And there obviously these different schools objective Bayesians.",
            "This is a troubling thing.",
            "If you go to a standard Bayesian statistics conference like the Valencia meeting, my experience, at least this is for the cameras.",
            "My experience was a bit disappointing in that I thought I was going to the Mecca Bayesianism, but I heard about 50% of the talks where they were trying to come up with priors that had some good frequentist properties and there are only a few people who were really treating Bayesian inference as a subjective process and so I found that a bit disappointing.",
            "Because I think it's a it's sort of a completely flawed endeavour.",
            "Basically, you're never going to find completely noninformative priors.",
            "The methods out there for finding noninformative priors don't really scale to multidimensional systems as far as I understand.",
            "And you know it's why are we chasing after this silly ideal anyway.",
            "Um, OK, then there's a subjective Bayesian approach where we're trying to capture our beliefs as well as possible.",
            "Then there are hierarchical priors, which I mean this is not mutually exclusive, but hierarchical priors.",
            "Basically, you build a model and then you think, Oh well, I've got these parameters here.",
            "Let's put hyper parameters to define the priors on those you need.",
            "Go up some number of levels.",
            "People think that you can't stop this, but of course you can.",
            "A silly thing to think.",
            "You can stop this at the point where it doesn't really matter if you add more levels or not.",
            "And there are empirical Bayesians where they're sort of being Bayesian, except they've got a few parameters and they try to optimize those parameters.",
            "To fit the data, for example, that's one way of being an empirical Bayesian.",
            "Somehow the hyperparameters depend on the data.",
            "So there are these."
        ],
        [
            "Different schools of thought.",
            "Um?",
            "You know the most coherent one is definitely the subjective prior one, but that's also the most controversial One South priors should capture our beliefs, otherwise we're not coherent.",
            "Question people might ask is how do we know our beliefs?",
            "Well, this is a difficult question, but you know really, it involves thinking about the problem domain.",
            "So this is going to come to a point I'm about to make so we really have no black box view of machine learning if we have to think about each problem domain separately.",
            "And then I think a nice way of knowing our beliefs is to generate data from the prior, see if it matches our expectations.",
            "If it doesn't, try to figure out how to improve our prior.",
            "So play around with the prior for awhile before you actually expose it to the data.",
            "I think that seems a rational way of doing things, and I feel that even vague beliefs you know you might be shy about using them, but they'll actually be very useful."
        ],
        [
            "So.",
            "So let me talk about this sort of black box view of machine learning, and I think you know there really are two different at least two different schools of machine learning out there.",
            "Oh most of the field of machine learning I would characterize as being in this black box view.",
            "So the goal of machine learning is to produce general purpose algorithms for learning.",
            "I should be able to put my algorithm online.",
            "You know, many of us do this, so lots of people can download it.",
            "If people want to apply it to problems ABC and D, then it should work regardless of the problem.",
            "Basically like it should be robust to what you know.",
            "Classification or regression or clustering problem you throw at it 'cause we don't know what people are going to do.",
            "And the user should not have to think too much, right?",
            "We've provided a tool, the user downloads, it, runs it on their data and This is why most machine learning papers have a table of results at the end of them where they take some algorithm and they apply it.",
            "In fact, the problems ABC and D and they show that it works better problems ABC and D then some algorithm.",
            "Ex Prime, which is whatever the.",
            "No straw man algorithm is out there.",
            "OK, so this is how.",
            "Most machine learning papers are.",
            "Now most statistics papers.",
            "I feel at least followed this approach.",
            "There more case studies.",
            "Right, and so the idea here is, if I want to solve problem A, it seems silly to use some general purpose method that was never designed for a, so I should really try to understand what problem A is, learn about the properties of the data, uses much expert knowledge as I can only then should I think of designing a method to solve a. OK.",
            "So here is the question that we can think about.",
            "Um?",
            "You know, maybe this is, yeah, sure.",
            "Something that flies in birthdays and that is that actually.",
            "Most statisticians wouldn't do that.",
            "Most statisticians would look at their data 1st, and so actually realistically in a lot of real problem cases there in your talking about this.",
            "Not so much of business is thinking about what what is involved in solving the problem.",
            "That's part of it.",
            "But a lot of that part of it is actually getting hold of the beta and spending a lot of time getting to know it.",
            "Yeah.",
            "And then after after after you get to know it then you develop a model and you say hello.",
            "And behold, I can model my data.",
            "Yeah, that's true.",
            "I think it's even true for Bayesians, right?",
            "You know, I, I would be impressed to find a Bayesian who has a disc full of data and you know it is strong enough not to peek at the data, right?",
            "You know, if you're that Bayesian, raise your hand right now.",
            "I think most of us speak at the data, but you know, we try to quickly forget what we've seen.",
            "It's a gentleman, right?",
            "Yeah, I mean.",
            "Yeah, I it's probably you know if we want to solve the problem well.",
            "Then you know picking up the data.",
            "You know probably helps, right?",
            "It might point out things that are grossly wrong about our prior right, and I think something that may be driving at is that something he said at lunch we had with Tony O'hagan is what's the difference between a statistician and machine learning person?",
            "And Tony was saying there's not much difference and will do the pointing out with fundamental Tony was saying I always believe that human has to be involved with data analysis and have been said well.",
            "But unless you believe the human feeling something.",
            "Extraordinary we we can do that part as well so that I think it's more of a philosophical idea, but I think statisticians always believe that you'll have to do the second thing they want to be in business, right?",
            "They don't want to be, you know they don't want to be replaced by machines, right?",
            "Whereas we're trying to build machines to replace statisticians.",
            "And so that's why this black box view, if we can get Tony Ohagan inside that black box, then we will have a pretty good black box out there, you know.",
            "Were quite as well, no, no, but but you know in that sense you can see where you know these two views can get blurred, right in that if we have clever enough black box then it should be.",
            "You know it should be able to come up with rich enough set of models.",
            "That it might be able to work on lots of different problems, but you know, that's like back to the AI problem, really.",
            "And also it's hard to put in domain knowledge into that black box, right?",
            "'cause usually people just apply the data set to some matrix of data, whatever.",
            "OK.",
            "But this is interesting.",
            "I mean, I think about this quite a lot.",
            "'cause you know, if I'm really trying to solve a problem, I'm not just going to take a, you know if I really care about the result.",
            "I have a client or somebody who really needs.",
            "You know good answers.",
            "I don't really want to apply some, just standard algorithm unless that's you know, unless I don't really want to get, you know, unless I don't care about getting really good answers.",
            "I just want to get answers quickly to that person.",
            "But then this is much more satisfying.",
            "You want to produce an algorithm, put it out there, and have everybody use it, right?",
            "You don't want to have to solve every case study separately.",
            "I mean, do we really need a separate machine learning researcher statistician for every problem out there?",
            "Then we don't.",
            "We're not enough of us for that.",
            "Right?"
        ],
        [
            "So I mean, how do we build the Bayesian black box you know, can we meaningfully create Bayesian black boxes?",
            "I mean, I don't know like if So what would the prior be right if I'm going to take a Bayesian method and we do this right?",
            "You know some of us have some of you guys have Gaussian process code online, right?",
            "Where what's the?",
            "What's the prior there?",
            "Oh it's meant for any you know you can do any regression problem, right?",
            "But but.",
            "You know what should the prior be?",
            "It should be some sort of reflection of what different kinds of problems people are going to apply to apply it to.",
            "So we can clearly create black boxes.",
            "We put code online, but how can we advocate people blindly using them?",
            "We can write.",
            "We can't advocate people using them blindly, but unless we're just thinking incrementally will say, well, I don't really think this Gaussian process captures the correct prior for this person's application, but I really think it will do better than that crappy SVM code out there.",
            "Right, so maybe incrementally will providing a service even though we're not actually advocating you know we're not actually doing?",
            "You know, Bayesian proper Bayesian inference for them, right?",
            "So we can't require every practitioner to be well trained Bayesian statistician, so we're sort of stuck with this sort of conundrum of having to sell out.",
            "Because I think that I've gotten processes kind of prior that says something about smoothness, right?",
            "Pryor in many places so I don't see what you're missing there, I think.",
            "It's not.",
            "Yeah, I mean, most problems are probably smooth.",
            "That's why it works, right?",
            "Yeah, I mean maybe people with do you want to comment on that or follow or?",
            "Say that if you use, you know one of the vanilla RBF kernels that then you get really.",
            "Overconfidence error bars and things like that should be too small.",
            "No, but I mean since it works, it means that it has captured something.",
            "That is, I mean I. I mean, I'm also a little bit afraid if they would kind of look at the data and then it's no longer then snow then.",
            "So like he rankled based almost like.",
            "The one listed, so it's about what is the right they contracted.",
            "So I mean the right Bayesian practice I think is this case study view where you know you really try to understand your problem as well as possible and you develop.",
            "Or maybe you can like you.",
            "You peek at that.",
            "You know you can.",
            "You're allowed to look at at whatever 10% of your data as much as you want.",
            "You come up with whatever prior you can from that.",
            "And then you do your inference on their other 90%.",
            "I think that's actually valid.",
            "I think that's perfectly valid, right?",
            "Sorry, I mean, let's go through this kind of regression problem.",
            "Sure, maybe you have solved 100 regression problem, right?",
            "The parking lot was through the RBF kernel.",
            "Worked fine, right?",
            "It was really nice too, yeah.",
            "As my prior in front of the new regression problem would be a mixture of these two pages, that seems sensible, right?",
            "Because?",
            "Yeah, that would be."
        ],
        [
            "Yep.",
            "You're basically your prior than is a prior on the kinds of problems your code is going to be run on.",
            "So if I put code online, I have some beliefs about the kind of people who are going to download that and the kind of datasets they're going to run it on.",
            "And that's actually, I mean, I think that you know for most of us and I know you know.",
            "Like for example, you guys put some code online for the GP book, a bunch of you have put all of us have probably put some code online at some point.",
            "Right, we kind of think a little bit about you know what sort of crazy things are people going to do with this?",
            "So we do have prior expectations about.",
            "The uses of this code, and that's probably the rational way of going about it.",
            "Yeah, metric.",
            "If you say I believe in accounting processes in 100 dimensional space, then I really you know this is such a big thing.",
            "I don't know what is it.",
            "Yeah, well it is a little like joining a church, yeah?",
            "More complex than I actually thought if.",
            "Yeah.",
            "Yeah, I mean you know their ways of couching it?",
            "Don't sound so horrible like you could say, I believe that the function is kind of smooth.",
            "OK yeah, and you know I and and you know that doesn't sound as scary is.",
            "I believe that it comes from a Gaussian process with this covariance function.",
            "Yeah.",
            "So I mean it's something smooth and nice.",
            "Thank you Mr. 90 percent, 10% but.",
            "Stationary, non stationary I think is a bigger problem, so you have a smooth function, but it has a different scale different places.",
            "There's many ways to have a different lens going different places.",
            "I don't have a prior over all the different ways to have a different lens going different places.",
            "I don't really want to build so far.",
            "I'll look at the data, I'll see that it has a length scale like this here and a short let's go here.",
            "I can see that despite visualizing it.",
            "Why is the prior then?",
            "Once I've seen that, then I can maybe build a prior which so so I don't really see how I can have this huge hierarchy of nonstationary priors.",
            "Yeah, I mean that's yeah.",
            "But that's fine, that's fine for the case study view where you're going to peek at the data.",
            "Maybe you peek.",
            "I mean, I like the idea of peaking at 10% of the data, saying I think I see this sort of structure in here and then running it on the rest or something.",
            "I mean, people might object, but at least, but that's different from the black box view where you just put your code online and somebody else is running your code.",
            "So I mean.",
            "God knows what they're going to do with it, right?",
            "It's hard to predict.",
            "New email Yeah no, that's the worst thing they could do.",
            "Sure.",
            "The fact that you had the idea to look.",
            "The different landscapes in different parts of your face.",
            "It means actually that you have a prior expectation that this could happen.",
            "Yeah, I would say that already.",
            "Do this ask this kind of person you have a prior.",
            "10 for this case.",
            "So yeah.",
            "Yeah yeah yeah, yeah.",
            "So let's I mean, this is.",
            "Unfortunately, this isn't the only problem we're going to encounter.",
            "So let me just move on to other unless quite keen you want to.",
            "I I don't know.",
            "Between.",
            "Amazing way of life and.",
            "Understand method in some way you know.",
            "Indeed.",
            "The best thing that can happen to you is.",
            "You're able to throw a hypothesis.",
            "That is.",
            "As restrictive as possible, and then you still manage to fortify it.",
            "Get as much information as you could possibly hope.",
            "From the experiments conducted.",
            "So every time we have to revert to the most general prior with this thing called, then we're not making progress in some way.",
            "Discovery seems that we, you know, we try to learn from things that you know.",
            "Centuries.",
            "And in some ways, it seems that the idea of things they could make our choir is a little bit more and more informative as it works, right, yeah?",
            "Earlier was that the big thing was not assigned probabilities.",
            "Remain alive.",
            "Especially in the.",
            "Small amount.",
            "I guess that you know.",
            "Extended that to go against the Black box viewer memory, committing to whatever we do.",
            "I know this is like, yeah.",
            "Yeah, I mean, I think I think that's a valid point.",
            "In that we don't want to be just sort of blandly uniform over all possibilities.",
            "We want to learn from and I think as a community we do this.",
            "We learn from things that seem to work and then we develop new models that have more structure or maybe concentrate mass in more interesting places where datasets might actually live.",
            "No, I guess The thing is that consumers you'll take this point about that.",
            "Is it about where you learn experience?",
            "You don't approach problems that have their other fashion, but you might think that not only do you learn that out of their minds percent or problems, I solve this move in the same standardized, but you might think you actually can't can characterize those those problems in some ways about.",
            "In fact, meta learning is like how long to bake, but it's about saying you know we want to learn from my past experience, but it won't just be in some sort of average sense.",
            "It might be that.",
            "Yeah yeah, I mean there we would like to what we would like to do is to be able to characterize the space of possible problems so that new method can look at properties of the problem and know what kind of prior to use in that case.",
            "But let's just let's just move on 'cause I think there are other issues as well that."
        ],
        [
            "Worth talking about."
        ],
        [
            "So.",
            "So one of them is this sort of nonparametric versus parametric.",
            "Division and, you know, just roughly speaking, we can think of parametric models as having a finite dimensional or finite and fixed number of parameters Theta.",
            "And So what happens is, given those parameters, the predictions are independent of the data, so the parameters in a parametric model capture all the structure in the data.",
            "That's useful for doing predictions about new data points.",
            "OK, and we can think of this really as model based learning 'cause you know you've captured in that finite dimensional parameter model of the data.",
            "Nonparametric model is sort of, roughly speaking, allow the number of parameters or to grow with the data set size or the effective number of parameters grows with the data set size.",
            "Or Alternatively we can think of them as having predictions that depend on the data.",
            "And possibly usually small number of what we would probably call hyperparameters Alpha as well.",
            "OK, so the predictions then depend on the data and these other hyperparameters.",
            "There's not, like nonparametric models, don't have any parameters in them, but but the predictions depend somehow on the data.",
            "So we can call this memory based learning.",
            "For example, right?",
            "'cause you have to remember the data."
        ],
        [
            "And you know, we all know the example if you take."
        ],
        [
            "Clustering, then we can think of the nonparametric version of clustering.",
            "Most classical form of that is a Dirichlet process mixture where you assume that you have countably infinitely many components in your cluster model and as you observe more and more data you observe more and more examples of your different components and clusters and the arguments.",
            "But why you might want to do that is because you probably don't actually believe.",
            "That your data comes from a finite number of mixture components.",
            "So weird world out there.",
            "When your data comes from 5 Gaussians right in the real world, your data doesn't come from 5 Gaussians.",
            "And you also want to have good predictions, and if your model is inflexible, if you've stuck yourself with five or six Gaussians, no matter how much data you observe, you're stuck with that representation for prediction.",
            "So your predictions are not going to be as good either.",
            "So this is kind of a scientific or philosophical reason.",
            "This is a more practical reason why you might want to be nonparametric.",
            "And also practically, for many kinds of data, as you observe more data points you imagine observing more different kinds of data out there.",
            "More kinds of clusters.",
            "So these are sort of arguments we."
        ],
        [
            "Classically, gives for nonparametric methods.",
            "So here's my question.",
            "If we're really going to buy the Bayesian, you know the.",
            "Bayesian book here.",
            "Is nonparametric's the only way to go?",
            "Should we all be nonparametric Bayesians?",
            "Under what conditions should we do parametric Bayesian learning?",
            "So when do we really believe our parametric model?",
            "And that challenge you to come up with a parametric model that that you can convince me you really believe in.",
            "Believe in particular, parametric models and nonparametric model.",
            "You said that, but I want an example of you give me a data set and a model and say I believe this.",
            "Data set where there are particular theories about which they believe there are good physical reasons why one or the other should be the case.",
            "Parameterized those work with them and make assessments as to whether or not.",
            "Yeah, I could.",
            "I could buy that in certain cases.",
            "Like Newton's laws, you know that you can write them down as an equation of some kind.",
            "That's a parametric equation, right?",
            "But you know, physics is maybe like one of the only domains where I would agree with you on that, yeah?",
            "OK, I'll give you that one.",
            "Give me another one.",
            "OK, we will move on from there.",
            "You can ignore it.",
            "Aim assist.",
            "Only applies to physics.",
            "Give me give me an example from biology where you believe in parametric model.",
            "OK, do it so now.",
            "But now we can ask ourselves well, but when do we really believe our nonparametric model, right?",
            "Or you know how much should how much weight should we put on this idea of actually believing in our models or not?",
            "You know is that is that the right question to ask?",
            "And also sometimes people ask me pesky questions.",
            "Like you know, oh, so you did this nonparametric Bayesian thing?",
            "Couldn't you have done just a large parametric model and wouldn't have been just as good?",
            "And probably the answer is yes, right?",
            "You know most cases you know you did the nonparametric thing.",
            "It was pretty elegant.",
            "But you know, if you had a large enough parametric model performance wise it would have done just just as well.",
            "Probably are almost as well.",
            "But you know, philosophically.",
            "Many of us like the nonparametric models and certainly as their data set gets larger than the nonparametric model, seems to make more sense.",
            "But the computations get heavier as well.",
            "So more things to think about."
        ],
        [
            "Um?",
            "So now that now we have the other real problem, which is the approximate inference conundrum.",
            "And that is that all interesting models, as far as I can tell, are intractable.",
            "Right?",
            "So we use approximate inference.",
            "You know, MCMC being the most widely used approximate inference algorithm.",
            "I've heard people say, well, it's exact exact.",
            "If you wait the age of the universe, but it's approximate if you run it for finite amounts of time.",
            "So this is an approximate algorithm as much as variational Bayes or EP or any other approximate algorithm there.",
            "All speed accuracy tradeoffs, right?",
            "But we have to use one of these Kezar things are going to be intractable, mostly.",
            "But the problem is we often can't control the effect of our approximate inference.",
            "So you know all this coherence arguments I gave at the beginning.",
            "Um?",
            "You know, do they hold under the fact that we're going to use approximate inference?",
            "Um?",
            "And so you know it's subjective.",
            "Bayesianism.",
            "You know this whole paradigm based on this idea of being coherent and honest about uncertainty.",
            "Does it fall apart the moment we use approximate inference?",
            "Um?",
            "So any thoughts on that?",
            "Does fall apart?",
            "Did you have to pick on variational Bayes?",
            "Could you pick?",
            "Can you pick on something else?",
            "Just 'cause I mean.",
            "Yeah, I mean they all they can get them all to fall apart, right?",
            "Yeah, but you know I think they I mean the nice thing about variational Bayes is a formalism and you just do it and you basically know what you're doing.",
            "You know you decompose your model, but that's also the nasty thing about it.",
            "The nice thing about MCMC is that no one can ever tell you that there's no MCMC method that will give you.",
            "Good are they won't give you a good answer within a certain amount of time, so you've got a massive space to explore and you can be really, really creative.",
            "But then you have to have someone who's really creative and imaginative to do it.",
            "Yeah, but I mean it's still like.",
            "It's still.",
            "Troublesome from the philosophical point of view, right?",
            "In that whatever you do.",
            "You get our approximate answer and so you know should we really worry about.",
            "Figuring out how good our approximate answer is as compared to the truth, but the problem is, the truth is intractable to compute, so it's actually usually very hard to know how well we're doing compared to the truth.",
            "Right?",
            "Yep.",
            "Yeah.",
            "Yeah.",
            "Yeah.",
            "Yeah so.",
            "My Mike.",
            "A lot of time on.",
            "During is correct, right?",
            "Like but I mean.",
            "Maybe eight years ago.",
            "Thinking about nice performance on the end.",
            "I really did.",
            "Yeah.",
            "Committees, I mean and.",
            "I think that's the.",
            "Yeah, I mean, the philosophical arguments go away when we just care about performance as compared to other methods, right?",
            "I think this is a bigger problem that often we choose a prize in order to make it tractable.",
            "Is this good?",
            "Approximate inference is a problem, but often you also mess with the method you mess with the model.",
            "Yeah, so you know, I'm particularly fond of conjugate exponential models, right?",
            "Combining conjugate priors with exponential families, and some people ask me, well, you know.",
            "You've chosen that clearly.",
            "'cause you can do your algorithm easily on these conjugate exponential models.",
            "Yeah.",
            "Yeah.",
            "But maybe that is the rational thing to do given finite amounts of time and finite amounts of patience by the part of the programmer.",
            "You know finite payoff for getting the right answer.",
            "All these actually rational real world considerations.",
            "I like this, might not be the best thing to use to choose your hyperparameters, and then maybe yeah.",
            "About result maybe yeah.",
            "Yeah hockey.",
            "Maybe?",
            "Clearly.",
            "After you put in your approximate inference in the middle, now your prior is something else.",
            "Yeah, you told us earlier that the big thing was.",
            "You know, again, do not do very well, like too low probability, reasonable things, but maybe one should sort of systematically do some signing checks after one does, yeah, and maybe maybe that's another good reason why we shouldn't obsess too much about the prior 'cause we're going to muck it up with the approximate inference algorithm anyway.",
            "So what it does matter that we got the prior just as we wanted, right?",
            "So I think there's another interesting issue with.",
            "I mean it depends on the application where you actually care about the truth.",
            "So, um.",
            "Mark over there hiding in the corner has done some stuff recently where he's using Bayes factors to compare two hypothesize models of the way a biological system works, and then you genuinely care about estimating those based practice because you're trying to say is that model more correct than the other.",
            "And that's I think a really classical Bayesian statistics use of these things that you're doing science science, but I think actually use that.",
            "I mean, there's a difference between science.",
            "How many years did it take us to come up with the scientific method?",
            "Ray and people were dicking around and surviving just doing not something optimal but doing something not completely stupid, right?",
            "I think in machine learning we've got away with a lot of our approximate inference things, because genuinely in most applications we're looking at, it's not necessary to do the optimal thing is just necessary to do something better.",
            "Yeah, better or not idiotic, yeah.",
            "The reason we fight with these alternative methods is 'cause neither of us is doing something optimal.",
            "If we were, then it wouldn't be a fight.",
            "We just like can we when we.",
            "Not when we're not doing when we're being suboptimal, are we being less about tomorrow somewhere else?",
            "Yeah, I think there is a really lovely distinction.",
            "This was pointed out by a paper by I think Heckerman maybe Heckerman chicory.",
            "There's a lovely distinction between scientific and engineering approaches to model comparison, and if you really care about the science, you really care about the priors that you use an you really care about figuring out what the truth is between two competing hypothesis.",
            "Whereas if you just care about engineering performance, you just want your method to perform better than the alternatives, and you're not searching for the truth, so the prior doesn't really matter that much, so I think that's a really good point.",
            "Let me move on.",
            "I think there are a couple of other points actually."
        ],
        [
            "So there's a sort of idea of reconciling Bayesian and frequentist views.",
            "You know there's a lot of texts on this slide, but you know the frequentist theory is based on different different sorts of things than Bayesian theory.",
            "You look at sampling properties of estimators.",
            "You tend to look at things like Mini Max performance.",
            "You often optimizing some penalized cost function.",
            "Bayesian methods are tend to look at things like expected loss under the posterior.",
            "You generally don't make use of optimization except with the point at which decisions are made, and there are some reasons why frequentist procedures are useful for Bayesians.",
            "Like for example, you might want to.",
            "Communicate.",
            "Convert base in a.",
            "Might want to convince Bayesians, BC and D of the validity of some inference, and then you could show that your prior, if your prior had good frequentist properties, then you know it's sort of.",
            "Your inferences might be valid under many different.",
            "Alternative priors as well.",
            "So, um.",
            "You know robustness issues might be another reason why we might care to look at some of the frequentist methods.",
            "So also there are nice ways of having your cake and eating it too, so you can be Bayesian and then say, Oh well, we can do.",
            "We can provide pack bounds on performance by using these PAC Bayesian bounds so that you know there are some worst case guarantees that we can keep, even though we've done Bayesian inference all along.",
            "So these are sort of nice ways."
        ],
        [
            "Combining these two.",
            "Cons and pros of Bayesian methods.",
            "I sort of talked about these things so limitations of classical limitations.",
            "We often hear that their subjective people hate that.",
            "It's hard to come up with the prior.",
            "The assumptions are usually wrong.",
            "People often say that I think it's hard to come up with the prior because people don't have much experience at that and they are not very confident that you know they think that if they get the wrong prior, their method will just not work at all.",
            "You know the assumptions are usually wrong.",
            "Well, that's true for any modeling, and all modeling involves some assumptions.",
            "The only thing that involves no assumptions is if you take the data and you spit it back at the user, right?",
            "So the identity function has no assumptions in it I guess.",
            "So the closed world assumption is bit problematic when you're writing down your prior.",
            "You need to consider all possible hypothesis for the data before observing the data.",
            "That seems a little hard to deal with.",
            "We're limited by our creativity.",
            "Basically an important limitation is that they can be computationally very demanding.",
            "You know there are methods are sometimes very slow.",
            "That's why we do approximate inference.",
            "But then we end up with the approximate inference conundrum, so it's a little tricky.",
            "And this is the point that I was trying to make approximate inference.",
            "Weekends of coherence argument.",
            "So the advantages are when we can.",
            "We're trying to be as coherent as possible, is conceptually incredibly straightforward.",
            "You just have to learn a couple of things at a high level and then turn the crank.",
            "As I'm seeing signals in the crowd.",
            "They're very modular.",
            "You can have bits and pieces of models and piece them together and very, very elegant way that you can't necessarily with other methods.",
            "The beauty of it is that you're using the language of probability to transmit uncertainty from one module to the next.",
            "So if you're trying to build a robot that's taking in sensory information from lots of sources and making some decisions and taking some actions, it's nice to have these little modules, each of which transmits.",
            "Beliefs with some probabilities around it.",
            "Very hard to think of.",
            "Doing that with just classical machine learning methods that don't represent uncertainty and often we get very good performance."
        ],
        [
            "So this is a question I really care about.",
            "So how can we convert the Pagan majority of machine learning researchers to bayesianism, right?",
            "What do we do?",
            "Should go in with our swords, and you know, force them to convert.",
            "Or you know, can we do it in a little more gentle way?",
            "So you know, I welcome suggestions, but here are some thoughts I just wrote down some things I think might help.",
            "So we need more killer applications.",
            "You know we need more amazing applications out there that really work and that are clearly branded as using Bayesian inference.",
            "And I think there's some good ones out there now.",
            "But you know, maybe we need more of those.",
            "'cause you know a lot of people just care about results.",
            "So you know they'll be basion.",
            "If it's going to get them better results and they'll you know those same people will probably switch to doing SVM's or whatever, getting better results.",
            "So we need to win more competitions.",
            "Some people have done pretty well there, but.",
            "Maybe that's a useful thing to do.",
            "We need Radford tenther Martin.",
            "We need more radfords.",
            "Yeah.",
            "We need to go around dispelling some of these myths.",
            "We can't sit there patiently listening to people, say completely silly things.",
            "Maybe be more proactive, more aggressive, so often when I'm in a bad mood, I fantasize about writing papers where I basically, you know, describe all the reasons I hate, you know.",
            "The following ten very, very popular algorithms out there, right?",
            "So David Mackay and I have sat around talking about all the things that we would put in a paper like that, and the list quickly grew from like 10 to 25 or something like that and then then we decided to be easier to just write a paper on what we like rather than what we hate.",
            "And then we decided we do that all the time anyway.",
            "'cause we write papers about Bayesian methods.",
            "So.",
            "So we came around full circle and ended up not doing anything about it so, but there are certainly things out there that just seem really, really weird and you know it might be good.",
            "To just go after those things, I think it would be very useful to release good, easy to use Black box code, even if there's some philosophically, you know a little shaky.",
            "Convince people to download it and use it, and you know before they know it.",
            "They're they're being Bayesian.",
            "So.",
            "So I think that's it anyway.",
            "I hope that with you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so most of the conversation at dinner actually revolves around people falling asleep during talks after dinner, so it's making me very nervous.",
                    "label": 0
                },
                {
                    "sent": "But you know, I notice some of you have beer and some of you have coffee, so I can just keep keep an eye on whether there's any correlation between who's actually falling asleep.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I was in a funny mood when I wrote the abstract to this and this is what's gotten me in this position here.",
                    "label": 0
                },
                {
                    "sent": "I just thought, well, what would I want to talk about at a small workshop among friends, right?",
                    "label": 0
                },
                {
                    "sent": "And I don't really want to talk about my own research, 'cause, you know, I know about it and it bores me to talk about my research.",
                    "label": 0
                },
                {
                    "sent": "But it's fun to talk about, you know, kind of the big picture a little bit so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to run through, sort of.",
                    "label": 0
                },
                {
                    "sent": "The Canonical story we could give to motivate people for why they would want to be Bayesian OK. And then I'm going to try to critique that a bit, and I'm going to say, well, do we really believe this?",
                    "label": 0
                },
                {
                    "sent": "And then I'm just going to jump around and talk about a few other questions that I think should really be on our minds and certainly are on my mind.",
                    "label": 0
                },
                {
                    "sent": "So you know, a lot of machine learning involves classification, regression, clustering, and things like that.",
                    "label": 1
                },
                {
                    "sent": "And we all know.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that basically we can take any Canonical machine learning problem and write it down in terms of a few components.",
                    "label": 0
                },
                {
                    "sent": "We have some data.",
                    "label": 0
                },
                {
                    "sent": "We have some model with some parameters and then we have some gold that as Bayesians we can write out as an inference problem and most people don't really think about things exactly in that way.",
                    "label": 0
                },
                {
                    "sent": "But really, you know, take a Canonical problem like linear classification.",
                    "label": 1
                },
                {
                    "sent": "You have some data set in these crosses, a knows you have some model some.",
                    "label": 0
                },
                {
                    "sent": "Linear boundary, and rather than thinking of it as some sort of optimization problem, we can think about it as the problem of inferring some unknown quantity like these parameters from the data so as to predict future labels.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of like a Canonical tutorial.",
                    "label": 0
                },
                {
                    "sent": "You would give an, you know, the motivation I and I'm sure all of you also give for doing this is that we can apply the basic rules of probability to doing this sort of learning.",
                    "label": 1
                },
                {
                    "sent": "We all know what these.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rules are we apply it in the context of machine learning and we think of computing posteriors over unknown quantities given the data by multiplying some likelihood by some prior an renormalizing an we can do model comparison.",
                    "label": 1
                },
                {
                    "sent": "We can do prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is part of this.",
                    "label": 0
                },
                {
                    "sent": "I'm blasting through the Canonical story so that we can critique it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later, OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "That's like, really, you know.",
                    "label": 0
                },
                {
                    "sent": "In some ways I kind of feel like that's all we need to know about machine learning.",
                    "label": 0
                },
                {
                    "sent": "The rest is just sort of mechanics, algorithmics and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's not how most people in the field of machine learning think about this, which surprises me still.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's now ask some of these questions well.",
                    "label": 0
                },
                {
                    "sent": "Should all machine learning be Bayesian an you know if we think the answer is yes, then why isn't it that so they will come to that later?",
                    "label": 1
                },
                {
                    "sent": "Like you know, if we really think that this basic framework is really elegant, why?",
                    "label": 0
                },
                {
                    "sent": "If I go to a typical conference like ICM, LYR, Bayesian papers, maybe like 10% of papers at most.",
                    "label": 0
                },
                {
                    "sent": "So again, what I'll do is I'll blast through some of the arguments that that at least I have given in tutorials.",
                    "label": 0
                },
                {
                    "sent": "These are clearly not things I've come up with.",
                    "label": 0
                },
                {
                    "sent": "These are things I've picked up from other places I give in tutorials for why I you know how I try to convince people that Bayesianism is the right way to go.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we can go and critique those 'cause we're in in small company here, and nobody on video lectures is really going to watch this right, so?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So alright, so why should we be Bayesian?",
                    "label": 0
                },
                {
                    "sent": "So one of the arguments is that if we want a system to behave intelligently, it should represent beliefs about propositions in the world, and we want to represent the strength of those beliefs numerically in the brain of some artificial agent, like a robot for example.",
                    "label": 1
                },
                {
                    "sent": "And we want some set of rules or calculus for manipulating those trends of beliefs so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the standard arguments you could give from the artificial intelligence point of view, is this sort of arguments based on the Cox axioms that are described in James?",
                    "label": 0
                },
                {
                    "sent": "Is book an?",
                    "label": 0
                },
                {
                    "sent": "There's a really nice paper by Kevin Van Horn.",
                    "label": 0
                },
                {
                    "sent": "Actually that is sort of a more modern treatment of this and discuss some of the critiques of those arguments in sort of dispels those critiques.",
                    "label": 0
                },
                {
                    "sent": "So the arguments are basically that if we want to represent strength of belief and propositions about the world.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Assume that we want to.",
                    "label": 0
                },
                {
                    "sent": "The strength of belief to be somewhere between zero and one where zero is that X is definitely not true, whereas one is that X is definitely true and we write down conditional beliefs.",
                    "label": 1
                },
                {
                    "sent": "What is your strength of belief in X given Y?",
                    "label": 0
                },
                {
                    "sent": "Then if we accept certain axioms, you can read about them if you haven't already, then the consequences of these axioms is that belief functions this B of X.",
                    "label": 0
                },
                {
                    "sent": "B of X given Y, etc.",
                    "label": 0
                },
                {
                    "sent": "They must satisfy the rules of probability theory including Bayes rule, so this is one way we can justify.",
                    "label": 1
                },
                {
                    "sent": "That if we want artificial systems, have beliefs about the world, then we should use the language of probability like we do in normal discourse, right?",
                    "label": 0
                },
                {
                    "sent": "Like you know, we say I think it's probable that you know the sun will rise tomorrow.",
                    "label": 0
                },
                {
                    "sent": "That's a statement of 1's belief about something that's going to happen in the future.",
                    "label": 0
                },
                {
                    "sent": "Certainly not a repeatable experiment.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's one classic argument for Bayesian thinking.",
                    "label": 0
                },
                {
                    "sent": "Another classic argument is this Dutch book theorem, where now we're assuming that we have strengths of beliefs, and we're also gambolling people, so we're willing to bet.",
                    "label": 0
                },
                {
                    "sent": "And if our strength of belief is .9, then we're willing to accept a bet where if X is true, we win anything greater than or equal to $1.",
                    "label": 1
                },
                {
                    "sent": "If X is false, we lose $9 or something like that, and the Dutch book argument says that.",
                    "label": 0
                },
                {
                    "sent": "Unless our beliefs, our strength abli satisfied the rules of probability theory, including Bayes rule, then there exists a set of simultaneous bets called a Dutch book which you would be willing to accept, but which for which you're guaranteed to lose money no matter what the outcome is.",
                    "label": 1
                },
                {
                    "sent": "So essentially the only way to guard against these Dutch books is for your beliefs to cohere with the rules of probability.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be correct.",
                    "label": 0
                },
                {
                    "sent": "You don't have to have.",
                    "label": 0
                },
                {
                    "sent": "The the right beliefs about the real world.",
                    "label": 0
                },
                {
                    "sent": "They just have to be coherent with each other.",
                    "label": 0
                },
                {
                    "sent": "You can't be inconsistent.",
                    "label": 0
                },
                {
                    "sent": "That's sort of a consistency argument.",
                    "label": 0
                },
                {
                    "sent": "It's another argument that you know we can standard.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A gift for Bayesian thinking and then there are other arguments like asymptotic certainty.",
                    "label": 0
                },
                {
                    "sent": "Now we're considering an IID cases sort of statistical argument.",
                    "label": 0
                },
                {
                    "sent": "We assume we have a data set D sub N consisting of N data points and it was generated from some true parameter value data star.",
                    "label": 1
                },
                {
                    "sent": "Then, under some regularity conditions, as long as the probability density associated with Theta star is.",
                    "label": 0
                },
                {
                    "sent": "Non zero, and really what the regularity conditions involve having some probability mass around Theta star.",
                    "label": 0
                },
                {
                    "sent": "Then what happens is in the limit, as the number of data points goes to Infinity, your posterior over Theta given the data, will converge to a Delta function around Theta star.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice argument if you want to convince somebody who cares about asymptotics or who's trained in statistics, you can say look if the stuff came from some truth data star, we're going to converge to that.",
                    "label": 0
                },
                {
                    "sent": "The only condition we need on the prior is to put some mass around the truth.",
                    "label": 0
                },
                {
                    "sent": "It's not that, and I've heard this phrase and it really, really annoys me.",
                    "label": 0
                },
                {
                    "sent": "I've heard people talking about how difficult it is to come up with the right prior, and I think that's a completely crazy way of misunderstanding Bayesian thinking.",
                    "label": 0
                },
                {
                    "sent": "There isn't a right prior you have to have whatever prior you believe in.",
                    "label": 0
                },
                {
                    "sent": "The only condition you need on the prior really is that it puts some probability mass close to the truth, and that might be a very big condition, but.",
                    "label": 0
                },
                {
                    "sent": "You know any prior with this property will work in this sense, Yep.",
                    "label": 0
                },
                {
                    "sent": "Critique.",
                    "label": 0
                },
                {
                    "sent": "You have to be careful about dimensional right?",
                    "label": 0
                },
                {
                    "sent": "So the caveat here.",
                    "label": 0
                },
                {
                    "sent": "There are many caveats, and this is actually for finite dimensional models.",
                    "label": 0
                },
                {
                    "sent": "Yes, good point.",
                    "label": 0
                },
                {
                    "sent": "So for example, for Gaussian process is hard to prove these things and people have spent some time to think about these.",
                    "label": 0
                },
                {
                    "sent": "They have, yeah, but but it's not trivial.",
                    "label": 0
                },
                {
                    "sent": "Counter example.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a widely cited paper that you know has made many people not be Bayesian and not like nonparametric Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "And I think it's probably abused, but but I don't really understand it well enough to know why it's abuse.",
                    "label": 0
                },
                {
                    "sent": "I just think my priors that it's been abused.",
                    "label": 0
                },
                {
                    "sent": "Alright, yeah?",
                    "label": 0
                },
                {
                    "sent": "Things that are reasonable yeah.",
                    "label": 0
                },
                {
                    "sent": "So so be be, you know, give give a little chance to all the different options that you can think of and now the yeah go ahead, thanks.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's never Infinity like I'll show tomorrow.",
                    "label": 0
                },
                {
                    "sent": "There cases where the prior covers the true thing and you still get very bad performance for realistic.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Like any like any asymptotic statement you're living in SM Topia, which is a fantasy land, right?",
                    "label": 0
                },
                {
                    "sent": "So so you know.",
                    "label": 0
                },
                {
                    "sent": "So clearly you know this will satisfy the people who like to live in that fantasy land, but North is always finite.",
                    "label": 0
                },
                {
                    "sent": "You know this statement is geared towards a certain type of person who cares about these things.",
                    "label": 0
                },
                {
                    "sent": "They're called statisticians.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, some Bayesian statisticians too.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what about if it's UN realizable in the UN realizable case?",
                    "label": 0
                },
                {
                    "sent": "That means you know if the data was generated from some distribution which cannot be modeled by any Theta, then what you'll converge is to a Delta function around some Theta hat, and reassuringly, that Theta hat is the one that minimizes the KL divergent between the true distribution P star of X&P of X given Theta, and in fact that's actually the maximum likelihood where you'll converge.",
                    "label": 0
                },
                {
                    "sent": "Two essentially, under the regularity conditions under the prior, you'll converge the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "Theta had a Delta function on the maximum likelihood estimate, so that should also reassure some people in this UN realizable case.",
                    "label": 0
                },
                {
                    "sent": "And for using a uniform distribution is bigger box.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you know, if you take this stuff literally then you really you really?",
                    "label": 0
                },
                {
                    "sent": "Missed the point of the beauty of doing Bayesian inference in that you know, these are asymptotically statements.",
                    "label": 0
                },
                {
                    "sent": "And yeah, if you use a uniform distribution over all possible thetas that you could think of, you'll converge.",
                    "label": 0
                },
                {
                    "sent": "In either of these cases.",
                    "label": 0
                },
                {
                    "sent": "But the rate at which you converge, how quickly you converge, etc.",
                    "label": 0
                },
                {
                    "sent": "If that uniform distribution is even well defined, is heavily governed by how much probability mass you put on the truth.",
                    "label": 0
                },
                {
                    "sent": "So it's a betting game you want to put.",
                    "label": 0
                },
                {
                    "sent": "As much mass as you can on all the good bets, but you want to put a little bit of mass on the bad bets as well, just in case they're the right thing 'cause you don't want to be penalized infinitely for getting things wrong, lucky.",
                    "label": 0
                },
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean this is what this would say, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you're doing maximum likelihood anyway.",
                    "label": 0
                },
                {
                    "sent": "He just converges to a Delta function on the maximum likelihood, yeah?",
                    "label": 0
                },
                {
                    "sent": "So yeah, I mean it's this is a good question.",
                    "label": 0
                },
                {
                    "sent": "I didn't actually bring this up, but.",
                    "label": 0
                },
                {
                    "sent": "You know, we can also ask ourselves in this small room here.",
                    "label": 0
                },
                {
                    "sent": "You know, in what regime?",
                    "label": 0
                },
                {
                    "sent": "Are Bayesian methods really good right?",
                    "label": 0
                },
                {
                    "sent": "Where are they most useful and if we have a finite dimensional parameter?",
                    "label": 0
                },
                {
                    "sent": "Like some D dimensional parameter space, and we're tending N to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then if N gets big enough.",
                    "label": 0
                },
                {
                    "sent": "We're going to be fine using maximum likelihood methods, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think at least the interesting thing is a lot of real world applications and you working at Microsoft will know about some of these applications have both incredibly large and D. The dimensionality of your parameters incredibly large, and in those cases you know the asymptotic that you do when N goes to Infinity is not really that relevant because the data is incredibly sparse and the number of.",
                    "label": 0
                },
                {
                    "sent": "Dimensions of your parameter space is so huge you don't ever have enough data.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's move through this.",
                    "label": 0
                },
                {
                    "sent": "This is a nice argument.",
                    "label": 0
                },
                {
                    "sent": "The asymptotic consensus argument, where all Bayesians I'll end up happily agreeing with each other.",
                    "label": 0
                },
                {
                    "sent": "I kind of like this, so you know you have two Bayesians.",
                    "label": 0
                },
                {
                    "sent": "Pick two people randomly from this room.",
                    "label": 0
                },
                {
                    "sent": "We have different priors, P1 of Theta and P2 of Theta.",
                    "label": 0
                },
                {
                    "sent": "But we observe the same data now.",
                    "label": 1
                },
                {
                    "sent": "Assume that we both agree on the set of possible and impossible values of Theta.",
                    "label": 1
                },
                {
                    "sent": "So the support in terms of Theta space for P1 of Theta is the same as the support for P2 of Theta.",
                    "label": 0
                },
                {
                    "sent": "Then under that condition, in the limit as N goes to Infinity, the posteriors will converge in some measure of the distance between these distributions, like any over any set.",
                    "label": 0
                },
                {
                    "sent": "the Super any set of the probability mass assigned to that set is the same as the probability mass assigned to that set by the other posterior, so.",
                    "label": 0
                },
                {
                    "sent": "So, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "We can have different priors if we observe enough data will converge the same posterior.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "So that I think you know.",
                    "label": 0
                },
                {
                    "sent": "And again I haven't worked through all of the mathematics.",
                    "label": 0
                },
                {
                    "sent": "I've just sort of, you know, found this result and used it.",
                    "label": 0
                },
                {
                    "sent": "The argument of the.",
                    "label": 0
                },
                {
                    "sent": "The mass, sorry, the support being the same.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether it requires other conditions on being a probability density or having Delta functions in the same place and so on, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it might be that there are other conditions.",
                    "label": 0
                },
                {
                    "sent": "I would.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't be surprised.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then we have this nice Occam's razor thing where we talk about model selection using the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the idea that one hopes that when we observe some data set, D will be able to use this marginal likelihood or integrated likelihood or evidence too.",
                    "label": 0
                },
                {
                    "sent": "Select a model class is neither too simple nor too complex, so we can.",
                    "label": 1
                },
                {
                    "sent": "Hopefully reject models.",
                    "label": 0
                },
                {
                    "sent": "That are either too simple or too complex just by computing the marginal likelihood and the argument comes from the conservation argument where any any probabilistic model with some.",
                    "label": 1
                },
                {
                    "sent": "Set of parameters like let's say M corresponds to the number of parameters in your model.",
                    "label": 0
                },
                {
                    "sent": "For example, which is one way of measuring complexity.",
                    "label": 0
                },
                {
                    "sent": "Although it's a very simplistic way of measuring complexity.",
                    "label": 0
                },
                {
                    "sent": "Any model will assign some amount of probability mass to different possible datasets, and we can think of simple models assigning lots of probability mass to certain simple datasets and no mass or very little mass to complicated datasets and vice versa for complex models.",
                    "label": 0
                },
                {
                    "sent": "But because there is this sort of conservation idea that each model has to spread out this mass, if we observe a particular data set, then it's not the case that the more complex model will always win.",
                    "label": 0
                },
                {
                    "sent": "So unlike maximum likelihood, we're not going to get the more complex model to win.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the marginal likelihood idea.",
                    "label": 0
                },
                {
                    "sent": "So here are some.",
                    "label": 0
                },
                {
                    "sent": "You know potential advantages we could use to try to sell Bayesian methods to other people.",
                    "label": 0
                },
                {
                    "sent": "And I kind of believe these things.",
                    "label": 0
                },
                {
                    "sent": "I think at least I believe them when I wrote this slide last night.",
                    "label": 0
                },
                {
                    "sent": "So, you know, we're trying to be coherent and honest about uncertainty, right?",
                    "label": 1
                },
                {
                    "sent": "That's part of the goal.",
                    "label": 1
                },
                {
                    "sent": "And it's easy to do model comparison and model selection.",
                    "label": 0
                },
                {
                    "sent": "At least we know in theory what we should be doing right.",
                    "label": 0
                },
                {
                    "sent": "Although in practice it might be hard.",
                    "label": 0
                },
                {
                    "sent": "There is a really nice rational process for model building and for adding domain knowledge, and that's not true if you.",
                    "label": 1
                },
                {
                    "sent": "If you look at like.",
                    "label": 0
                },
                {
                    "sent": "Other standard machine learning methods, you know, let's say you're boosting decision trees and you have some domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to know like you know, should you change the way you build the decision trees, should you change your boosting algorithm?",
                    "label": 0
                },
                {
                    "sent": "Should we change the features that we use etc.",
                    "label": 0
                },
                {
                    "sent": "And I think the basic method gives us nice way of adding domain knowledge and building our models and it's really easy to handle missing and hidden data because it's all about missing in hidden data.",
                    "label": 0
                },
                {
                    "sent": "So there's no question about that, whereas if you take a standard method like a support vector machine for classification, and you say you know what does it take to handle missing inputs in a support vector machine, will there be probably half a dozen answers, none of which are particularly satisfactory, OK?",
                    "label": 0
                },
                {
                    "sent": "The disadvantages are basic methods.",
                    "label": 0
                },
                {
                    "sent": "We can talk about later.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the things people obsess about a lot is where does the prior come from?",
                    "label": 1
                },
                {
                    "sent": "And there obviously these different schools objective Bayesians.",
                    "label": 0
                },
                {
                    "sent": "This is a troubling thing.",
                    "label": 0
                },
                {
                    "sent": "If you go to a standard Bayesian statistics conference like the Valencia meeting, my experience, at least this is for the cameras.",
                    "label": 0
                },
                {
                    "sent": "My experience was a bit disappointing in that I thought I was going to the Mecca Bayesianism, but I heard about 50% of the talks where they were trying to come up with priors that had some good frequentist properties and there are only a few people who were really treating Bayesian inference as a subjective process and so I found that a bit disappointing.",
                    "label": 0
                },
                {
                    "sent": "Because I think it's a it's sort of a completely flawed endeavour.",
                    "label": 0
                },
                {
                    "sent": "Basically, you're never going to find completely noninformative priors.",
                    "label": 0
                },
                {
                    "sent": "The methods out there for finding noninformative priors don't really scale to multidimensional systems as far as I understand.",
                    "label": 0
                },
                {
                    "sent": "And you know it's why are we chasing after this silly ideal anyway.",
                    "label": 0
                },
                {
                    "sent": "Um, OK, then there's a subjective Bayesian approach where we're trying to capture our beliefs as well as possible.",
                    "label": 1
                },
                {
                    "sent": "Then there are hierarchical priors, which I mean this is not mutually exclusive, but hierarchical priors.",
                    "label": 0
                },
                {
                    "sent": "Basically, you build a model and then you think, Oh well, I've got these parameters here.",
                    "label": 0
                },
                {
                    "sent": "Let's put hyper parameters to define the priors on those you need.",
                    "label": 0
                },
                {
                    "sent": "Go up some number of levels.",
                    "label": 0
                },
                {
                    "sent": "People think that you can't stop this, but of course you can.",
                    "label": 0
                },
                {
                    "sent": "A silly thing to think.",
                    "label": 0
                },
                {
                    "sent": "You can stop this at the point where it doesn't really matter if you add more levels or not.",
                    "label": 0
                },
                {
                    "sent": "And there are empirical Bayesians where they're sort of being Bayesian, except they've got a few parameters and they try to optimize those parameters.",
                    "label": 1
                },
                {
                    "sent": "To fit the data, for example, that's one way of being an empirical Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Somehow the hyperparameters depend on the data.",
                    "label": 0
                },
                {
                    "sent": "So there are these.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different schools of thought.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You know the most coherent one is definitely the subjective prior one, but that's also the most controversial One South priors should capture our beliefs, otherwise we're not coherent.",
                    "label": 0
                },
                {
                    "sent": "Question people might ask is how do we know our beliefs?",
                    "label": 1
                },
                {
                    "sent": "Well, this is a difficult question, but you know really, it involves thinking about the problem domain.",
                    "label": 1
                },
                {
                    "sent": "So this is going to come to a point I'm about to make so we really have no black box view of machine learning if we have to think about each problem domain separately.",
                    "label": 1
                },
                {
                    "sent": "And then I think a nice way of knowing our beliefs is to generate data from the prior, see if it matches our expectations.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't, try to figure out how to improve our prior.",
                    "label": 0
                },
                {
                    "sent": "So play around with the prior for awhile before you actually expose it to the data.",
                    "label": 0
                },
                {
                    "sent": "I think that seems a rational way of doing things, and I feel that even vague beliefs you know you might be shy about using them, but they'll actually be very useful.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So let me talk about this sort of black box view of machine learning, and I think you know there really are two different at least two different schools of machine learning out there.",
                    "label": 0
                },
                {
                    "sent": "Oh most of the field of machine learning I would characterize as being in this black box view.",
                    "label": 0
                },
                {
                    "sent": "So the goal of machine learning is to produce general purpose algorithms for learning.",
                    "label": 0
                },
                {
                    "sent": "I should be able to put my algorithm online.",
                    "label": 0
                },
                {
                    "sent": "You know, many of us do this, so lots of people can download it.",
                    "label": 0
                },
                {
                    "sent": "If people want to apply it to problems ABC and D, then it should work regardless of the problem.",
                    "label": 0
                },
                {
                    "sent": "Basically like it should be robust to what you know.",
                    "label": 0
                },
                {
                    "sent": "Classification or regression or clustering problem you throw at it 'cause we don't know what people are going to do.",
                    "label": 0
                },
                {
                    "sent": "And the user should not have to think too much, right?",
                    "label": 0
                },
                {
                    "sent": "We've provided a tool, the user downloads, it, runs it on their data and This is why most machine learning papers have a table of results at the end of them where they take some algorithm and they apply it.",
                    "label": 0
                },
                {
                    "sent": "In fact, the problems ABC and D and they show that it works better problems ABC and D then some algorithm.",
                    "label": 0
                },
                {
                    "sent": "Ex Prime, which is whatever the.",
                    "label": 0
                },
                {
                    "sent": "No straw man algorithm is out there.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how.",
                    "label": 0
                },
                {
                    "sent": "Most machine learning papers are.",
                    "label": 0
                },
                {
                    "sent": "Now most statistics papers.",
                    "label": 0
                },
                {
                    "sent": "I feel at least followed this approach.",
                    "label": 0
                },
                {
                    "sent": "There more case studies.",
                    "label": 0
                },
                {
                    "sent": "Right, and so the idea here is, if I want to solve problem A, it seems silly to use some general purpose method that was never designed for a, so I should really try to understand what problem A is, learn about the properties of the data, uses much expert knowledge as I can only then should I think of designing a method to solve a. OK.",
                    "label": 1
                },
                {
                    "sent": "So here is the question that we can think about.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You know, maybe this is, yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Something that flies in birthdays and that is that actually.",
                    "label": 0
                },
                {
                    "sent": "Most statisticians wouldn't do that.",
                    "label": 0
                },
                {
                    "sent": "Most statisticians would look at their data 1st, and so actually realistically in a lot of real problem cases there in your talking about this.",
                    "label": 0
                },
                {
                    "sent": "Not so much of business is thinking about what what is involved in solving the problem.",
                    "label": 0
                },
                {
                    "sent": "That's part of it.",
                    "label": 0
                },
                {
                    "sent": "But a lot of that part of it is actually getting hold of the beta and spending a lot of time getting to know it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And then after after after you get to know it then you develop a model and you say hello.",
                    "label": 0
                },
                {
                    "sent": "And behold, I can model my data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "I think it's even true for Bayesians, right?",
                    "label": 0
                },
                {
                    "sent": "You know, I, I would be impressed to find a Bayesian who has a disc full of data and you know it is strong enough not to peek at the data, right?",
                    "label": 0
                },
                {
                    "sent": "You know, if you're that Bayesian, raise your hand right now.",
                    "label": 0
                },
                {
                    "sent": "I think most of us speak at the data, but you know, we try to quickly forget what we've seen.",
                    "label": 0
                },
                {
                    "sent": "It's a gentleman, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I it's probably you know if we want to solve the problem well.",
                    "label": 0
                },
                {
                    "sent": "Then you know picking up the data.",
                    "label": 0
                },
                {
                    "sent": "You know probably helps, right?",
                    "label": 0
                },
                {
                    "sent": "It might point out things that are grossly wrong about our prior right, and I think something that may be driving at is that something he said at lunch we had with Tony O'hagan is what's the difference between a statistician and machine learning person?",
                    "label": 0
                },
                {
                    "sent": "And Tony was saying there's not much difference and will do the pointing out with fundamental Tony was saying I always believe that human has to be involved with data analysis and have been said well.",
                    "label": 0
                },
                {
                    "sent": "But unless you believe the human feeling something.",
                    "label": 0
                },
                {
                    "sent": "Extraordinary we we can do that part as well so that I think it's more of a philosophical idea, but I think statisticians always believe that you'll have to do the second thing they want to be in business, right?",
                    "label": 0
                },
                {
                    "sent": "They don't want to be, you know they don't want to be replaced by machines, right?",
                    "label": 0
                },
                {
                    "sent": "Whereas we're trying to build machines to replace statisticians.",
                    "label": 0
                },
                {
                    "sent": "And so that's why this black box view, if we can get Tony Ohagan inside that black box, then we will have a pretty good black box out there, you know.",
                    "label": 0
                },
                {
                    "sent": "Were quite as well, no, no, but but you know in that sense you can see where you know these two views can get blurred, right in that if we have clever enough black box then it should be.",
                    "label": 0
                },
                {
                    "sent": "You know it should be able to come up with rich enough set of models.",
                    "label": 0
                },
                {
                    "sent": "That it might be able to work on lots of different problems, but you know, that's like back to the AI problem, really.",
                    "label": 0
                },
                {
                    "sent": "And also it's hard to put in domain knowledge into that black box, right?",
                    "label": 0
                },
                {
                    "sent": "'cause usually people just apply the data set to some matrix of data, whatever.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But this is interesting.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think about this quite a lot.",
                    "label": 0
                },
                {
                    "sent": "'cause you know, if I'm really trying to solve a problem, I'm not just going to take a, you know if I really care about the result.",
                    "label": 0
                },
                {
                    "sent": "I have a client or somebody who really needs.",
                    "label": 0
                },
                {
                    "sent": "You know good answers.",
                    "label": 0
                },
                {
                    "sent": "I don't really want to apply some, just standard algorithm unless that's you know, unless I don't really want to get, you know, unless I don't care about getting really good answers.",
                    "label": 0
                },
                {
                    "sent": "I just want to get answers quickly to that person.",
                    "label": 0
                },
                {
                    "sent": "But then this is much more satisfying.",
                    "label": 0
                },
                {
                    "sent": "You want to produce an algorithm, put it out there, and have everybody use it, right?",
                    "label": 0
                },
                {
                    "sent": "You don't want to have to solve every case study separately.",
                    "label": 0
                },
                {
                    "sent": "I mean, do we really need a separate machine learning researcher statistician for every problem out there?",
                    "label": 0
                },
                {
                    "sent": "Then we don't.",
                    "label": 0
                },
                {
                    "sent": "We're not enough of us for that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I mean, how do we build the Bayesian black box you know, can we meaningfully create Bayesian black boxes?",
                    "label": 1
                },
                {
                    "sent": "I mean, I don't know like if So what would the prior be right if I'm going to take a Bayesian method and we do this right?",
                    "label": 0
                },
                {
                    "sent": "You know some of us have some of you guys have Gaussian process code online, right?",
                    "label": 0
                },
                {
                    "sent": "Where what's the?",
                    "label": 0
                },
                {
                    "sent": "What's the prior there?",
                    "label": 0
                },
                {
                    "sent": "Oh it's meant for any you know you can do any regression problem, right?",
                    "label": 0
                },
                {
                    "sent": "But but.",
                    "label": 1
                },
                {
                    "sent": "You know what should the prior be?",
                    "label": 1
                },
                {
                    "sent": "It should be some sort of reflection of what different kinds of problems people are going to apply to apply it to.",
                    "label": 0
                },
                {
                    "sent": "So we can clearly create black boxes.",
                    "label": 0
                },
                {
                    "sent": "We put code online, but how can we advocate people blindly using them?",
                    "label": 1
                },
                {
                    "sent": "We can write.",
                    "label": 0
                },
                {
                    "sent": "We can't advocate people using them blindly, but unless we're just thinking incrementally will say, well, I don't really think this Gaussian process captures the correct prior for this person's application, but I really think it will do better than that crappy SVM code out there.",
                    "label": 0
                },
                {
                    "sent": "Right, so maybe incrementally will providing a service even though we're not actually advocating you know we're not actually doing?",
                    "label": 0
                },
                {
                    "sent": "You know, Bayesian proper Bayesian inference for them, right?",
                    "label": 0
                },
                {
                    "sent": "So we can't require every practitioner to be well trained Bayesian statistician, so we're sort of stuck with this sort of conundrum of having to sell out.",
                    "label": 0
                },
                {
                    "sent": "Because I think that I've gotten processes kind of prior that says something about smoothness, right?",
                    "label": 0
                },
                {
                    "sent": "Pryor in many places so I don't see what you're missing there, I think.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, most problems are probably smooth.",
                    "label": 0
                },
                {
                    "sent": "That's why it works, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean maybe people with do you want to comment on that or follow or?",
                    "label": 0
                },
                {
                    "sent": "Say that if you use, you know one of the vanilla RBF kernels that then you get really.",
                    "label": 0
                },
                {
                    "sent": "Overconfidence error bars and things like that should be too small.",
                    "label": 0
                },
                {
                    "sent": "No, but I mean since it works, it means that it has captured something.",
                    "label": 0
                },
                {
                    "sent": "That is, I mean I. I mean, I'm also a little bit afraid if they would kind of look at the data and then it's no longer then snow then.",
                    "label": 0
                },
                {
                    "sent": "So like he rankled based almost like.",
                    "label": 0
                },
                {
                    "sent": "The one listed, so it's about what is the right they contracted.",
                    "label": 0
                },
                {
                    "sent": "So I mean the right Bayesian practice I think is this case study view where you know you really try to understand your problem as well as possible and you develop.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you can like you.",
                    "label": 0
                },
                {
                    "sent": "You peek at that.",
                    "label": 0
                },
                {
                    "sent": "You know you can.",
                    "label": 0
                },
                {
                    "sent": "You're allowed to look at at whatever 10% of your data as much as you want.",
                    "label": 0
                },
                {
                    "sent": "You come up with whatever prior you can from that.",
                    "label": 0
                },
                {
                    "sent": "And then you do your inference on their other 90%.",
                    "label": 0
                },
                {
                    "sent": "I think that's actually valid.",
                    "label": 0
                },
                {
                    "sent": "I think that's perfectly valid, right?",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mean, let's go through this kind of regression problem.",
                    "label": 0
                },
                {
                    "sent": "Sure, maybe you have solved 100 regression problem, right?",
                    "label": 0
                },
                {
                    "sent": "The parking lot was through the RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "Worked fine, right?",
                    "label": 0
                },
                {
                    "sent": "It was really nice too, yeah.",
                    "label": 0
                },
                {
                    "sent": "As my prior in front of the new regression problem would be a mixture of these two pages, that seems sensible, right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "You're basically your prior than is a prior on the kinds of problems your code is going to be run on.",
                    "label": 0
                },
                {
                    "sent": "So if I put code online, I have some beliefs about the kind of people who are going to download that and the kind of datasets they're going to run it on.",
                    "label": 1
                },
                {
                    "sent": "And that's actually, I mean, I think that you know for most of us and I know you know.",
                    "label": 0
                },
                {
                    "sent": "Like for example, you guys put some code online for the GP book, a bunch of you have put all of us have probably put some code online at some point.",
                    "label": 0
                },
                {
                    "sent": "Right, we kind of think a little bit about you know what sort of crazy things are people going to do with this?",
                    "label": 0
                },
                {
                    "sent": "So we do have prior expectations about.",
                    "label": 0
                },
                {
                    "sent": "The uses of this code, and that's probably the rational way of going about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, metric.",
                    "label": 0
                },
                {
                    "sent": "If you say I believe in accounting processes in 100 dimensional space, then I really you know this is such a big thing.",
                    "label": 0
                },
                {
                    "sent": "I don't know what is it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well it is a little like joining a church, yeah?",
                    "label": 0
                },
                {
                    "sent": "More complex than I actually thought if.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean you know their ways of couching it?",
                    "label": 0
                },
                {
                    "sent": "Don't sound so horrible like you could say, I believe that the function is kind of smooth.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, and you know I and and you know that doesn't sound as scary is.",
                    "label": 0
                },
                {
                    "sent": "I believe that it comes from a Gaussian process with this covariance function.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I mean it's something smooth and nice.",
                    "label": 0
                },
                {
                    "sent": "Thank you Mr. 90 percent, 10% but.",
                    "label": 0
                },
                {
                    "sent": "Stationary, non stationary I think is a bigger problem, so you have a smooth function, but it has a different scale different places.",
                    "label": 0
                },
                {
                    "sent": "There's many ways to have a different lens going different places.",
                    "label": 0
                },
                {
                    "sent": "I don't have a prior over all the different ways to have a different lens going different places.",
                    "label": 0
                },
                {
                    "sent": "I don't really want to build so far.",
                    "label": 0
                },
                {
                    "sent": "I'll look at the data, I'll see that it has a length scale like this here and a short let's go here.",
                    "label": 0
                },
                {
                    "sent": "I can see that despite visualizing it.",
                    "label": 0
                },
                {
                    "sent": "Why is the prior then?",
                    "label": 0
                },
                {
                    "sent": "Once I've seen that, then I can maybe build a prior which so so I don't really see how I can have this huge hierarchy of nonstationary priors.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean that's yeah.",
                    "label": 0
                },
                {
                    "sent": "But that's fine, that's fine for the case study view where you're going to peek at the data.",
                    "label": 1
                },
                {
                    "sent": "Maybe you peek.",
                    "label": 0
                },
                {
                    "sent": "I mean, I like the idea of peaking at 10% of the data, saying I think I see this sort of structure in here and then running it on the rest or something.",
                    "label": 0
                },
                {
                    "sent": "I mean, people might object, but at least, but that's different from the black box view where you just put your code online and somebody else is running your code.",
                    "label": 0
                },
                {
                    "sent": "So I mean.",
                    "label": 0
                },
                {
                    "sent": "God knows what they're going to do with it, right?",
                    "label": 0
                },
                {
                    "sent": "It's hard to predict.",
                    "label": 0
                },
                {
                    "sent": "New email Yeah no, that's the worst thing they could do.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "The fact that you had the idea to look.",
                    "label": 0
                },
                {
                    "sent": "The different landscapes in different parts of your face.",
                    "label": 0
                },
                {
                    "sent": "It means actually that you have a prior expectation that this could happen.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I would say that already.",
                    "label": 0
                },
                {
                    "sent": "Do this ask this kind of person you have a prior.",
                    "label": 0
                },
                {
                    "sent": "10 for this case.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So let's I mean, this is.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this isn't the only problem we're going to encounter.",
                    "label": 0
                },
                {
                    "sent": "So let me just move on to other unless quite keen you want to.",
                    "label": 0
                },
                {
                    "sent": "I I don't know.",
                    "label": 0
                },
                {
                    "sent": "Between.",
                    "label": 0
                },
                {
                    "sent": "Amazing way of life and.",
                    "label": 0
                },
                {
                    "sent": "Understand method in some way you know.",
                    "label": 0
                },
                {
                    "sent": "Indeed.",
                    "label": 0
                },
                {
                    "sent": "The best thing that can happen to you is.",
                    "label": 0
                },
                {
                    "sent": "You're able to throw a hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That is.",
                    "label": 0
                },
                {
                    "sent": "As restrictive as possible, and then you still manage to fortify it.",
                    "label": 0
                },
                {
                    "sent": "Get as much information as you could possibly hope.",
                    "label": 0
                },
                {
                    "sent": "From the experiments conducted.",
                    "label": 0
                },
                {
                    "sent": "So every time we have to revert to the most general prior with this thing called, then we're not making progress in some way.",
                    "label": 0
                },
                {
                    "sent": "Discovery seems that we, you know, we try to learn from things that you know.",
                    "label": 0
                },
                {
                    "sent": "Centuries.",
                    "label": 0
                },
                {
                    "sent": "And in some ways, it seems that the idea of things they could make our choir is a little bit more and more informative as it works, right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Earlier was that the big thing was not assigned probabilities.",
                    "label": 0
                },
                {
                    "sent": "Remain alive.",
                    "label": 0
                },
                {
                    "sent": "Especially in the.",
                    "label": 0
                },
                {
                    "sent": "Small amount.",
                    "label": 0
                },
                {
                    "sent": "I guess that you know.",
                    "label": 0
                },
                {
                    "sent": "Extended that to go against the Black box viewer memory, committing to whatever we do.",
                    "label": 0
                },
                {
                    "sent": "I know this is like, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I think I think that's a valid point.",
                    "label": 1
                },
                {
                    "sent": "In that we don't want to be just sort of blandly uniform over all possibilities.",
                    "label": 0
                },
                {
                    "sent": "We want to learn from and I think as a community we do this.",
                    "label": 0
                },
                {
                    "sent": "We learn from things that seem to work and then we develop new models that have more structure or maybe concentrate mass in more interesting places where datasets might actually live.",
                    "label": 0
                },
                {
                    "sent": "No, I guess The thing is that consumers you'll take this point about that.",
                    "label": 0
                },
                {
                    "sent": "Is it about where you learn experience?",
                    "label": 0
                },
                {
                    "sent": "You don't approach problems that have their other fashion, but you might think that not only do you learn that out of their minds percent or problems, I solve this move in the same standardized, but you might think you actually can't can characterize those those problems in some ways about.",
                    "label": 0
                },
                {
                    "sent": "In fact, meta learning is like how long to bake, but it's about saying you know we want to learn from my past experience, but it won't just be in some sort of average sense.",
                    "label": 0
                },
                {
                    "sent": "It might be that.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, I mean there we would like to what we would like to do is to be able to characterize the space of possible problems so that new method can look at properties of the problem and know what kind of prior to use in that case.",
                    "label": 1
                },
                {
                    "sent": "But let's just let's just move on 'cause I think there are other issues as well that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worth talking about.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So one of them is this sort of nonparametric versus parametric.",
                    "label": 0
                },
                {
                    "sent": "Division and, you know, just roughly speaking, we can think of parametric models as having a finite dimensional or finite and fixed number of parameters Theta.",
                    "label": 1
                },
                {
                    "sent": "And So what happens is, given those parameters, the predictions are independent of the data, so the parameters in a parametric model capture all the structure in the data.",
                    "label": 1
                },
                {
                    "sent": "That's useful for doing predictions about new data points.",
                    "label": 0
                },
                {
                    "sent": "OK, and we can think of this really as model based learning 'cause you know you've captured in that finite dimensional parameter model of the data.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric model is sort of, roughly speaking, allow the number of parameters or to grow with the data set size or the effective number of parameters grows with the data set size.",
                    "label": 1
                },
                {
                    "sent": "Or Alternatively we can think of them as having predictions that depend on the data.",
                    "label": 1
                },
                {
                    "sent": "And possibly usually small number of what we would probably call hyperparameters Alpha as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so the predictions then depend on the data and these other hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "There's not, like nonparametric models, don't have any parameters in them, but but the predictions depend somehow on the data.",
                    "label": 0
                },
                {
                    "sent": "So we can call this memory based learning.",
                    "label": 0
                },
                {
                    "sent": "For example, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you have to remember the data.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know, we all know the example if you take.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clustering, then we can think of the nonparametric version of clustering.",
                    "label": 0
                },
                {
                    "sent": "Most classical form of that is a Dirichlet process mixture where you assume that you have countably infinitely many components in your cluster model and as you observe more and more data you observe more and more examples of your different components and clusters and the arguments.",
                    "label": 0
                },
                {
                    "sent": "But why you might want to do that is because you probably don't actually believe.",
                    "label": 0
                },
                {
                    "sent": "That your data comes from a finite number of mixture components.",
                    "label": 1
                },
                {
                    "sent": "So weird world out there.",
                    "label": 0
                },
                {
                    "sent": "When your data comes from 5 Gaussians right in the real world, your data doesn't come from 5 Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And you also want to have good predictions, and if your model is inflexible, if you've stuck yourself with five or six Gaussians, no matter how much data you observe, you're stuck with that representation for prediction.",
                    "label": 0
                },
                {
                    "sent": "So your predictions are not going to be as good either.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a scientific or philosophical reason.",
                    "label": 1
                },
                {
                    "sent": "This is a more practical reason why you might want to be nonparametric.",
                    "label": 0
                },
                {
                    "sent": "And also practically, for many kinds of data, as you observe more data points you imagine observing more different kinds of data out there.",
                    "label": 0
                },
                {
                    "sent": "More kinds of clusters.",
                    "label": 0
                },
                {
                    "sent": "So these are sort of arguments we.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classically, gives for nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "So here's my question.",
                    "label": 0
                },
                {
                    "sent": "If we're really going to buy the Bayesian, you know the.",
                    "label": 0
                },
                {
                    "sent": "Bayesian book here.",
                    "label": 0
                },
                {
                    "sent": "Is nonparametric's the only way to go?",
                    "label": 1
                },
                {
                    "sent": "Should we all be nonparametric Bayesians?",
                    "label": 0
                },
                {
                    "sent": "Under what conditions should we do parametric Bayesian learning?",
                    "label": 0
                },
                {
                    "sent": "So when do we really believe our parametric model?",
                    "label": 1
                },
                {
                    "sent": "And that challenge you to come up with a parametric model that that you can convince me you really believe in.",
                    "label": 0
                },
                {
                    "sent": "Believe in particular, parametric models and nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "You said that, but I want an example of you give me a data set and a model and say I believe this.",
                    "label": 0
                },
                {
                    "sent": "Data set where there are particular theories about which they believe there are good physical reasons why one or the other should be the case.",
                    "label": 0
                },
                {
                    "sent": "Parameterized those work with them and make assessments as to whether or not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I could.",
                    "label": 0
                },
                {
                    "sent": "I could buy that in certain cases.",
                    "label": 0
                },
                {
                    "sent": "Like Newton's laws, you know that you can write them down as an equation of some kind.",
                    "label": 0
                },
                {
                    "sent": "That's a parametric equation, right?",
                    "label": 0
                },
                {
                    "sent": "But you know, physics is maybe like one of the only domains where I would agree with you on that, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll give you that one.",
                    "label": 0
                },
                {
                    "sent": "Give me another one.",
                    "label": 0
                },
                {
                    "sent": "OK, we will move on from there.",
                    "label": 0
                },
                {
                    "sent": "You can ignore it.",
                    "label": 0
                },
                {
                    "sent": "Aim assist.",
                    "label": 0
                },
                {
                    "sent": "Only applies to physics.",
                    "label": 0
                },
                {
                    "sent": "Give me give me an example from biology where you believe in parametric model.",
                    "label": 0
                },
                {
                    "sent": "OK, do it so now.",
                    "label": 0
                },
                {
                    "sent": "But now we can ask ourselves well, but when do we really believe our nonparametric model, right?",
                    "label": 0
                },
                {
                    "sent": "Or you know how much should how much weight should we put on this idea of actually believing in our models or not?",
                    "label": 0
                },
                {
                    "sent": "You know is that is that the right question to ask?",
                    "label": 0
                },
                {
                    "sent": "And also sometimes people ask me pesky questions.",
                    "label": 0
                },
                {
                    "sent": "Like you know, oh, so you did this nonparametric Bayesian thing?",
                    "label": 0
                },
                {
                    "sent": "Couldn't you have done just a large parametric model and wouldn't have been just as good?",
                    "label": 0
                },
                {
                    "sent": "And probably the answer is yes, right?",
                    "label": 0
                },
                {
                    "sent": "You know most cases you know you did the nonparametric thing.",
                    "label": 0
                },
                {
                    "sent": "It was pretty elegant.",
                    "label": 0
                },
                {
                    "sent": "But you know, if you had a large enough parametric model performance wise it would have done just just as well.",
                    "label": 0
                },
                {
                    "sent": "Probably are almost as well.",
                    "label": 0
                },
                {
                    "sent": "But you know, philosophically.",
                    "label": 0
                },
                {
                    "sent": "Many of us like the nonparametric models and certainly as their data set gets larger than the nonparametric model, seems to make more sense.",
                    "label": 0
                },
                {
                    "sent": "But the computations get heavier as well.",
                    "label": 0
                },
                {
                    "sent": "So more things to think about.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So now that now we have the other real problem, which is the approximate inference conundrum.",
                    "label": 0
                },
                {
                    "sent": "And that is that all interesting models, as far as I can tell, are intractable.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So we use approximate inference.",
                    "label": 1
                },
                {
                    "sent": "You know, MCMC being the most widely used approximate inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "I've heard people say, well, it's exact exact.",
                    "label": 0
                },
                {
                    "sent": "If you wait the age of the universe, but it's approximate if you run it for finite amounts of time.",
                    "label": 0
                },
                {
                    "sent": "So this is an approximate algorithm as much as variational Bayes or EP or any other approximate algorithm there.",
                    "label": 0
                },
                {
                    "sent": "All speed accuracy tradeoffs, right?",
                    "label": 0
                },
                {
                    "sent": "But we have to use one of these Kezar things are going to be intractable, mostly.",
                    "label": 0
                },
                {
                    "sent": "But the problem is we often can't control the effect of our approximate inference.",
                    "label": 1
                },
                {
                    "sent": "So you know all this coherence arguments I gave at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You know, do they hold under the fact that we're going to use approximate inference?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so you know it's subjective.",
                    "label": 0
                },
                {
                    "sent": "Bayesianism.",
                    "label": 0
                },
                {
                    "sent": "You know this whole paradigm based on this idea of being coherent and honest about uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Does it fall apart the moment we use approximate inference?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So any thoughts on that?",
                    "label": 0
                },
                {
                    "sent": "Does fall apart?",
                    "label": 0
                },
                {
                    "sent": "Did you have to pick on variational Bayes?",
                    "label": 0
                },
                {
                    "sent": "Could you pick?",
                    "label": 0
                },
                {
                    "sent": "Can you pick on something else?",
                    "label": 0
                },
                {
                    "sent": "Just 'cause I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean they all they can get them all to fall apart, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you know I think they I mean the nice thing about variational Bayes is a formalism and you just do it and you basically know what you're doing.",
                    "label": 0
                },
                {
                    "sent": "You know you decompose your model, but that's also the nasty thing about it.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about MCMC is that no one can ever tell you that there's no MCMC method that will give you.",
                    "label": 0
                },
                {
                    "sent": "Good are they won't give you a good answer within a certain amount of time, so you've got a massive space to explore and you can be really, really creative.",
                    "label": 0
                },
                {
                    "sent": "But then you have to have someone who's really creative and imaginative to do it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I mean it's still like.",
                    "label": 0
                },
                {
                    "sent": "It's still.",
                    "label": 0
                },
                {
                    "sent": "Troublesome from the philosophical point of view, right?",
                    "label": 0
                },
                {
                    "sent": "In that whatever you do.",
                    "label": 0
                },
                {
                    "sent": "You get our approximate answer and so you know should we really worry about.",
                    "label": 0
                },
                {
                    "sent": "Figuring out how good our approximate answer is as compared to the truth, but the problem is, the truth is intractable to compute, so it's actually usually very hard to know how well we're doing compared to the truth.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "My Mike.",
                    "label": 0
                },
                {
                    "sent": "A lot of time on.",
                    "label": 0
                },
                {
                    "sent": "During is correct, right?",
                    "label": 0
                },
                {
                    "sent": "Like but I mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe eight years ago.",
                    "label": 0
                },
                {
                    "sent": "Thinking about nice performance on the end.",
                    "label": 0
                },
                {
                    "sent": "I really did.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Committees, I mean and.",
                    "label": 0
                },
                {
                    "sent": "I think that's the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, the philosophical arguments go away when we just care about performance as compared to other methods, right?",
                    "label": 0
                },
                {
                    "sent": "I think this is a bigger problem that often we choose a prize in order to make it tractable.",
                    "label": 0
                },
                {
                    "sent": "Is this good?",
                    "label": 0
                },
                {
                    "sent": "Approximate inference is a problem, but often you also mess with the method you mess with the model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you know, I'm particularly fond of conjugate exponential models, right?",
                    "label": 0
                },
                {
                    "sent": "Combining conjugate priors with exponential families, and some people ask me, well, you know.",
                    "label": 0
                },
                {
                    "sent": "You've chosen that clearly.",
                    "label": 0
                },
                {
                    "sent": "'cause you can do your algorithm easily on these conjugate exponential models.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But maybe that is the rational thing to do given finite amounts of time and finite amounts of patience by the part of the programmer.",
                    "label": 0
                },
                {
                    "sent": "You know finite payoff for getting the right answer.",
                    "label": 0
                },
                {
                    "sent": "All these actually rational real world considerations.",
                    "label": 0
                },
                {
                    "sent": "I like this, might not be the best thing to use to choose your hyperparameters, and then maybe yeah.",
                    "label": 0
                },
                {
                    "sent": "About result maybe yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah hockey.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Clearly.",
                    "label": 0
                },
                {
                    "sent": "After you put in your approximate inference in the middle, now your prior is something else.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you told us earlier that the big thing was.",
                    "label": 0
                },
                {
                    "sent": "You know, again, do not do very well, like too low probability, reasonable things, but maybe one should sort of systematically do some signing checks after one does, yeah, and maybe maybe that's another good reason why we shouldn't obsess too much about the prior 'cause we're going to muck it up with the approximate inference algorithm anyway.",
                    "label": 0
                },
                {
                    "sent": "So what it does matter that we got the prior just as we wanted, right?",
                    "label": 0
                },
                {
                    "sent": "So I think there's another interesting issue with.",
                    "label": 0
                },
                {
                    "sent": "I mean it depends on the application where you actually care about the truth.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Mark over there hiding in the corner has done some stuff recently where he's using Bayes factors to compare two hypothesize models of the way a biological system works, and then you genuinely care about estimating those based practice because you're trying to say is that model more correct than the other.",
                    "label": 0
                },
                {
                    "sent": "And that's I think a really classical Bayesian statistics use of these things that you're doing science science, but I think actually use that.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a difference between science.",
                    "label": 0
                },
                {
                    "sent": "How many years did it take us to come up with the scientific method?",
                    "label": 0
                },
                {
                    "sent": "Ray and people were dicking around and surviving just doing not something optimal but doing something not completely stupid, right?",
                    "label": 0
                },
                {
                    "sent": "I think in machine learning we've got away with a lot of our approximate inference things, because genuinely in most applications we're looking at, it's not necessary to do the optimal thing is just necessary to do something better.",
                    "label": 0
                },
                {
                    "sent": "Yeah, better or not idiotic, yeah.",
                    "label": 0
                },
                {
                    "sent": "The reason we fight with these alternative methods is 'cause neither of us is doing something optimal.",
                    "label": 0
                },
                {
                    "sent": "If we were, then it wouldn't be a fight.",
                    "label": 0
                },
                {
                    "sent": "We just like can we when we.",
                    "label": 0
                },
                {
                    "sent": "Not when we're not doing when we're being suboptimal, are we being less about tomorrow somewhere else?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there is a really lovely distinction.",
                    "label": 0
                },
                {
                    "sent": "This was pointed out by a paper by I think Heckerman maybe Heckerman chicory.",
                    "label": 0
                },
                {
                    "sent": "There's a lovely distinction between scientific and engineering approaches to model comparison, and if you really care about the science, you really care about the priors that you use an you really care about figuring out what the truth is between two competing hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you just care about engineering performance, you just want your method to perform better than the alternatives, and you're not searching for the truth, so the prior doesn't really matter that much, so I think that's a really good point.",
                    "label": 0
                },
                {
                    "sent": "Let me move on.",
                    "label": 0
                },
                {
                    "sent": "I think there are a couple of other points actually.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a sort of idea of reconciling Bayesian and frequentist views.",
                    "label": 1
                },
                {
                    "sent": "You know there's a lot of texts on this slide, but you know the frequentist theory is based on different different sorts of things than Bayesian theory.",
                    "label": 1
                },
                {
                    "sent": "You look at sampling properties of estimators.",
                    "label": 1
                },
                {
                    "sent": "You tend to look at things like Mini Max performance.",
                    "label": 0
                },
                {
                    "sent": "You often optimizing some penalized cost function.",
                    "label": 0
                },
                {
                    "sent": "Bayesian methods are tend to look at things like expected loss under the posterior.",
                    "label": 1
                },
                {
                    "sent": "You generally don't make use of optimization except with the point at which decisions are made, and there are some reasons why frequentist procedures are useful for Bayesians.",
                    "label": 1
                },
                {
                    "sent": "Like for example, you might want to.",
                    "label": 0
                },
                {
                    "sent": "Communicate.",
                    "label": 1
                },
                {
                    "sent": "Convert base in a.",
                    "label": 1
                },
                {
                    "sent": "Might want to convince Bayesians, BC and D of the validity of some inference, and then you could show that your prior, if your prior had good frequentist properties, then you know it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Your inferences might be valid under many different.",
                    "label": 0
                },
                {
                    "sent": "Alternative priors as well.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "You know robustness issues might be another reason why we might care to look at some of the frequentist methods.",
                    "label": 0
                },
                {
                    "sent": "So also there are nice ways of having your cake and eating it too, so you can be Bayesian and then say, Oh well, we can do.",
                    "label": 0
                },
                {
                    "sent": "We can provide pack bounds on performance by using these PAC Bayesian bounds so that you know there are some worst case guarantees that we can keep, even though we've done Bayesian inference all along.",
                    "label": 0
                },
                {
                    "sent": "So these are sort of nice ways.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Combining these two.",
                    "label": 0
                },
                {
                    "sent": "Cons and pros of Bayesian methods.",
                    "label": 1
                },
                {
                    "sent": "I sort of talked about these things so limitations of classical limitations.",
                    "label": 0
                },
                {
                    "sent": "We often hear that their subjective people hate that.",
                    "label": 1
                },
                {
                    "sent": "It's hard to come up with the prior.",
                    "label": 1
                },
                {
                    "sent": "The assumptions are usually wrong.",
                    "label": 0
                },
                {
                    "sent": "People often say that I think it's hard to come up with the prior because people don't have much experience at that and they are not very confident that you know they think that if they get the wrong prior, their method will just not work at all.",
                    "label": 0
                },
                {
                    "sent": "You know the assumptions are usually wrong.",
                    "label": 0
                },
                {
                    "sent": "Well, that's true for any modeling, and all modeling involves some assumptions.",
                    "label": 1
                },
                {
                    "sent": "The only thing that involves no assumptions is if you take the data and you spit it back at the user, right?",
                    "label": 0
                },
                {
                    "sent": "So the identity function has no assumptions in it I guess.",
                    "label": 0
                },
                {
                    "sent": "So the closed world assumption is bit problematic when you're writing down your prior.",
                    "label": 1
                },
                {
                    "sent": "You need to consider all possible hypothesis for the data before observing the data.",
                    "label": 1
                },
                {
                    "sent": "That seems a little hard to deal with.",
                    "label": 0
                },
                {
                    "sent": "We're limited by our creativity.",
                    "label": 0
                },
                {
                    "sent": "Basically an important limitation is that they can be computationally very demanding.",
                    "label": 0
                },
                {
                    "sent": "You know there are methods are sometimes very slow.",
                    "label": 0
                },
                {
                    "sent": "That's why we do approximate inference.",
                    "label": 0
                },
                {
                    "sent": "But then we end up with the approximate inference conundrum, so it's a little tricky.",
                    "label": 0
                },
                {
                    "sent": "And this is the point that I was trying to make approximate inference.",
                    "label": 0
                },
                {
                    "sent": "Weekends of coherence argument.",
                    "label": 0
                },
                {
                    "sent": "So the advantages are when we can.",
                    "label": 0
                },
                {
                    "sent": "We're trying to be as coherent as possible, is conceptually incredibly straightforward.",
                    "label": 0
                },
                {
                    "sent": "You just have to learn a couple of things at a high level and then turn the crank.",
                    "label": 0
                },
                {
                    "sent": "As I'm seeing signals in the crowd.",
                    "label": 0
                },
                {
                    "sent": "They're very modular.",
                    "label": 0
                },
                {
                    "sent": "You can have bits and pieces of models and piece them together and very, very elegant way that you can't necessarily with other methods.",
                    "label": 0
                },
                {
                    "sent": "The beauty of it is that you're using the language of probability to transmit uncertainty from one module to the next.",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to build a robot that's taking in sensory information from lots of sources and making some decisions and taking some actions, it's nice to have these little modules, each of which transmits.",
                    "label": 0
                },
                {
                    "sent": "Beliefs with some probabilities around it.",
                    "label": 0
                },
                {
                    "sent": "Very hard to think of.",
                    "label": 0
                },
                {
                    "sent": "Doing that with just classical machine learning methods that don't represent uncertainty and often we get very good performance.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a question I really care about.",
                    "label": 0
                },
                {
                    "sent": "So how can we convert the Pagan majority of machine learning researchers to bayesianism, right?",
                    "label": 1
                },
                {
                    "sent": "What do we do?",
                    "label": 0
                },
                {
                    "sent": "Should go in with our swords, and you know, force them to convert.",
                    "label": 0
                },
                {
                    "sent": "Or you know, can we do it in a little more gentle way?",
                    "label": 0
                },
                {
                    "sent": "So you know, I welcome suggestions, but here are some thoughts I just wrote down some things I think might help.",
                    "label": 0
                },
                {
                    "sent": "So we need more killer applications.",
                    "label": 0
                },
                {
                    "sent": "You know we need more amazing applications out there that really work and that are clearly branded as using Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "And I think there's some good ones out there now.",
                    "label": 0
                },
                {
                    "sent": "But you know, maybe we need more of those.",
                    "label": 0
                },
                {
                    "sent": "'cause you know a lot of people just care about results.",
                    "label": 0
                },
                {
                    "sent": "So you know they'll be basion.",
                    "label": 0
                },
                {
                    "sent": "If it's going to get them better results and they'll you know those same people will probably switch to doing SVM's or whatever, getting better results.",
                    "label": 0
                },
                {
                    "sent": "So we need to win more competitions.",
                    "label": 0
                },
                {
                    "sent": "Some people have done pretty well there, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's a useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "We need Radford tenther Martin.",
                    "label": 0
                },
                {
                    "sent": "We need more radfords.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We need to go around dispelling some of these myths.",
                    "label": 0
                },
                {
                    "sent": "We can't sit there patiently listening to people, say completely silly things.",
                    "label": 0
                },
                {
                    "sent": "Maybe be more proactive, more aggressive, so often when I'm in a bad mood, I fantasize about writing papers where I basically, you know, describe all the reasons I hate, you know.",
                    "label": 0
                },
                {
                    "sent": "The following ten very, very popular algorithms out there, right?",
                    "label": 0
                },
                {
                    "sent": "So David Mackay and I have sat around talking about all the things that we would put in a paper like that, and the list quickly grew from like 10 to 25 or something like that and then then we decided to be easier to just write a paper on what we like rather than what we hate.",
                    "label": 0
                },
                {
                    "sent": "And then we decided we do that all the time anyway.",
                    "label": 0
                },
                {
                    "sent": "'cause we write papers about Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we came around full circle and ended up not doing anything about it so, but there are certainly things out there that just seem really, really weird and you know it might be good.",
                    "label": 0
                },
                {
                    "sent": "To just go after those things, I think it would be very useful to release good, easy to use Black box code, even if there's some philosophically, you know a little shaky.",
                    "label": 0
                },
                {
                    "sent": "Convince people to download it and use it, and you know before they know it.",
                    "label": 0
                },
                {
                    "sent": "They're they're being Bayesian.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I think that's it anyway.",
                    "label": 0
                },
                {
                    "sent": "I hope that with you.",
                    "label": 0
                }
            ]
        }
    }
}