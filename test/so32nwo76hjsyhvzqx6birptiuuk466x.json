{
    "id": "so32nwo76hjsyhvzqx6birptiuuk466x",
    "title": "Grammatical Inference",
    "info": {
        "author": [
            "Colin de la Higuera, LINA, University of Nantes"
        ],
        "published": "July 16, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bootcamp2010_higuera_gi/",
    "segmentation": [
        [
            "Good morning everyone, so I hope my my voice will will keep the whole day.",
            "I mean last night there was a lot of shouting and it was a bit difficult.",
            "So I mean I I'm trying to speak slowly, the talk or the lecture is about grammatical inference.",
            "Grammatical inference is not one of the central topics inside machine learning.",
            "Nevertheless there are in machine learning conferences or in machine learning groups, all with some people for who for some reason or another are interested in this particular idea.",
            "Of learning grammars or learning finite state machines, and in a way, I'd say it's more and more the case will cause of the interest of having complex outputs or of having finite state machines which actually can represent a variety of things.",
            "Um, so I'm calling leggera.",
            "I'm from University of not not is in France.",
            "It's at the at the end of the lower Valley, so going into the Atlantic somewhere and I work on."
        ],
        [
            "In grammatical inference.",
            "These slides will correspond to work that have been done has been done for a long time with the number of people whose names are here and this work has allowed me to do a bit of advertising, right has allowed me to come out with a book which has been published now in March or April I'll pass it around if people want to have a look, and if it's around in your labs, you might want to look at it."
        ],
        [
            "Anyhow, this is the outline of the talk.",
            "I should say this is the outline of the talk as it is in these roughly 200 slides which are here, which are also the ones that are photocopied there.",
            "I've actually got some other slides about another topic related to it, perhaps during the break will see what sort of topics you'd be more interested in, and then I can shift towards more active learning sort of stuff, or more probabilistic finite state machines.",
            "Sort of stuff.",
            "Those are the really the two alternatives we've got.",
            "So the outline of the talk is what is learning automata about.",
            "We go through a very long introductory example to get the ideas of the sort of algorithms that are involved.",
            "Here we talk about validation issues, validation issues is the question of knowing.",
            "Well, when can I say that I have learned or when can I say that I am learning, so this is not just in this specific theme, but since learning languages is very much linked with upcycle cognitive aspects of a child.",
            "Acquiring a language well, there are other questions of how do you learn or when do you stop learning or when you say that you're learning correctly.",
            "All this allows us to have another perspective, which can be of interest even if your theme is going to be very different from this one.",
            "So we talk about some criteria that allow us to say that yes, we're learning correctly or not, and then we'll go through some algorithms.",
            "It's really going to be a talk about algorithms.",
            "I'm not going to give many maths.",
            "I'm going to give the most maths.",
            "I'm going to be giving is algorithms studying the algorithms, seeing what are the reasons for which these algorithms are working.",
            "So we be talking learning from an informant, an informant in our context, is saying that we're learning from data which is both positive and negative.",
            "We've got examples and counterexamples.",
            "The typical classification task.",
            "Now there's a second idea of learning here, which is actually.",
            "Could be related to clustering, but isn't in the sense that we've only got one sort of examples.",
            "We've only got text, so we've only got examples of the language that they are arriving, and somehow we still have to build a model of the language that's supposed to be what a child is doing.",
            "He's only receiving positive input, is only receiving strings that are arriving, or sentences, and from these sentences he has to somehow build a grammar.",
            "So that's what we're going to be interested in there.",
            "Learning by observing is one way of doing things, which is just I observed the data.",
            "I reorganize it in somewhere and say look, this is what I've observed learning actively.",
            "So you've had some online learning?",
            "I think, so this is a bit like online learning in the sense, or that we're going to be learning on the fly as new and new data arrives.",
            "The only thing being that the data we're actually going to choose ourselves so active learning in this sense.",
            "Is learning via an Oracle in the case where the algorithm the learning algorithm is going to be asking questions and through these questions is going to try and build the.",
            "Whatever he's trying to do is automaton or grammar, or something like that very quickly talk about extensions and."
        ],
        [
            "Solution.",
            "OK, so definition of grammatical inference.",
            "It's about learning a grammar given information about language.",
            "OK, so the information is about a language, but we're going to try and learn is a grammar that's for just for obvious reasons.",
            "I mean, you don't learn a language per, say.",
            "Language is usually an infinite set, so you have to have some representation of this infinite set.",
            "And this representation is going to be a grammar or an automaton, or something like that.",
            "What is the information we're going to be learning from?",
            "Well, typically strings or sequences?",
            "But also trees are graphs, so there are now interesting papers coming out about learning graph grammars.",
            "So where the data are going to be graphs and we're going to try and find some graph formalism to represent these graphs.",
            "And as I said before, information typically can be the text only one class or informant.",
            "Two classes, positive examples, negative examples, or actively sort, which is what we call query learning based related to teaching in the sense that, OK, well, perhaps I can feed to the learners some good examples for him to learn."
        ],
        [
            "The goals of the things we're trying to learn are going to be related with what is known as the Chomsky hierarchy, so I take it that most of you are trained in computer science, so you all have done some course somewhere of formal language theory.",
            "As you remember, bit about grammars and Automata and that sort of stuff.",
            "No, well, if not we'll have to try and go through them quickly.",
            "Learning how these objects are, the ones we're going to be interested in with extensions extensions which may be putting probabilities on these objects, leading to things like hidden Markov models or probabilistic finite state automata, context free grammars with probabilities.",
            "So that's a logical idea of saying, well, something purely deterministic isn't really going to suit the real life applications, so we can put probabilities on them.",
            "And then they said things like pattern languages or an exciting theme, which is the one of transducer learning transducers or like automata.",
            "But they have gotten input and they got an output.",
            "So the fact of having an output means that you can use them for a lot of other tasks.",
            "For example in translation tasks.",
            "And you probably had a talk talking a bit about transducers this week.",
            "Now.",
            "Didn't somebody talk about that?"
        ],
        [
            "OK, some examples of strings just very quickly.",
            "So well translation example this is.",
            "I actually picked up when I went to Scotland.",
            "It was meant to be a Scottish insult, so it's a very elaborate insult.",
            "But I mean, it's so this was the case where you'd be having sentences in two languages.",
            "I mean, the first thing you'll probably get in the second you if you compute, you get it right.",
            "So the translation from one to the other, you're going to have pairs of sequences, and from these pairs of sequences you are actually going to try and build some sort of a trial."
        ],
        [
            "Do something like that time series.",
            "Obviously you need to discretize in some way because if you just keep it with real values, you're not going to be able to do much with the sort of technique."
        ],
        [
            "And would be talking about biological data.",
            "So people working in bioinformatics have been using grammars and Hmm's to represent things.",
            "Actually, if you look through it you can see something that looks like a pass tree.",
            "So I mean you can use a pastrie to just represent the sort of stuff you want."
        ],
        [
            "To go with.",
            "Just this is just to remind me and remind you that even if a lot of the examples we're going to be looking at during this course are going to be very small examples with, you know, sort of strings or five, perhaps 10 symbols if you're working in biology, the strings are a little bit longer than that."
        ],
        [
            "So well, just remind this music so there are people working in music that I think there are people here from Alicante.",
            "Yes no.",
            "OK, well in Alicante there's a group working trying to use these sort of techniques with music.",
            "It's not the only group but there are other groups working with that.",
            "So music you can just take them as media files and then you've got an alphabet and from that alphabet you're trying to also learn patterns or to learn grammars or learn machines that somehow represent the melodies."
        ],
        [
            "Or represent something?",
            "Trees, so you've got lots of trees.",
            "You got family trees."
        ],
        [
            "I've got classification tree."
        ],
        [
            "And you've got all the trees that come with XML, so one of the big applications today of grammatical inference is to try and use XML data and to try and sort of sort out relationships in XML.",
            "So obviously in that case you wouldn't be trying to find finite state automata, but things like tree automata, which are a natural even if complex, at least technically complex extension of what you can do on strings."
        ],
        [
            "More."
        ],
        [
            "XML on all the anger.",
            "Natural language processing applications.",
            "So people working in natural language processing have been using trees to represent data or to obtain by tagging the data.",
            "Trees of this sort.",
            "So it's clearly being one of the fields in which there's been most of the work I'll be talking."
        ],
        [
            "And more things everyday the other day I was contacted by people in Germany or actually interested in using these algorithms for.",
            "Controlling reactive systems of some sort sort of in model checking applications.",
            "So becausw these finite state machines you find a bit of everywhere in computer science and in all sorts of different fields.",
            "People are having the same type of problem, which is.",
            "I've got some rough data I've got.",
            "I'd like to, you know, for the moment I'm hand building these finite state machines.",
            "Could I do it instead of doing it myself by hand?",
            "Could I get the machine to do it for me?",
            "And then there's all the variants of questions around this.",
            "Robots, web services, images etc."
        ],
        [
            "So let's go through an example that's got nothing to do with all what we've seen up to now.",
            "It's an example in the case of the Agent world, so it's agents, OK.",
            "Agents is the idea of an ice machine in which, while basically everybody, would be putting their little agent on this machine, and all these agents somehow interact and do something.",
            "So you can think of a situation like the one where machine would be the New York Stock Exchange, and we all have our little agents there trying to buy and sell shares.",
            "And somehow we're hoping that these agents are going to buy."
        ],
        [
            "And sell shares intelligently.",
            "So it's sort of cooperative situations, but well, it's not 100% property.",
            "I'm obviously you're trying to do the best deal possible, so what's my agent going to do when he's going to be on this machine?",
            "He's going to have to take a decision based on two things.",
            "First, just totally, mathematically economical gain functions saying right.",
            "This is how much I am prepared to buy the shares for or to sell the shares for if I can get this price, then I do it, but you should.",
            "Or they would these agents do something a bit better, which is saying, well, you know, if I'm buying the shares of this price and OK, that's the price I was meant to buy it from.",
            "But if I can see that my the person who's selling the shares is actually prepared to sell them cheaper, then perhaps I should wait.",
            "OK, so you introduce the notion of strategy in the sense of saying, well, if I can sort of understand what the strategy of the agent is, then I can get a better deal.",
            "I can, you know I can buy cheaper or I can sell more expensive so I have to somehow analyze the actions of the other age."
        ],
        [
            "So let's suppose for a very simple case that the agent is described by a finite state machine just like this one.",
            "Actually, let's suppose before going to the Stock Exchange that this is you as an agent.",
            "OK, so you're sitting there and you're listening to me talking, and you're actually going to have two attitudes.",
            "The one of listening or the one of dozing now after last night I suppose dozing is an option so you can see else and ease indicating what state you're in, what you're going to do.",
            "Are you going to be listening or you're going to be dozing?",
            "On the other hand, I I'm going to be giving the lecture and I've got two options.",
            "I can pass equations or I can pass pictures.",
            "So I do know that if I pass equations after awhile, you probably doze off.",
            "And if I pass pictures then you probably like pictures.",
            "Now I'd be able to catch up and understand what's happening, so my strategy I'm going to be passing pictures and equations here and depending what I pass, well, you're going to be one state or another.",
            "So notice that, for example, if I just keep on passing pictures, well, you just keep on listening.",
            "If I put in one equation then you still keep on listening.",
            "One equation is OK, right?",
            "But if I pass on a second equation then you might reach it, sort of dozing.",
            "State out of which you will have difficulties in getting out of because it's over there and it's.",
            "Technology will not be able to.",
            "I think I'll point.",
            "OK, so if you get to the right hand state, you'll be dozing off.",
            "I can get your attention again by following a P and getting onto a listing state and then equation, so I mean, you know there's different strategies.",
            "In this case, it's me that's directing the game.",
            "I decide, I know what your strategy is so I can decide what my sequence of slides is going to be in order to reach whatever I'm interested in.",
            "Perhaps to get you asleep, or perhaps to keep you at least partly away.",
            "OK, so this is a finite state machine, right?",
            "Just very simple finite state machine and it's also what is used for people doing what they call rational strategies.",
            "I mean rational strategies or deterministic strategies built in this sense.",
            "I will probably be able to argue that perhaps this more intelligent strategies than these, but these are actually quite nicely used in a number of cases."
        ],
        [
            "So to understand the.",
            "Rest, or at least well where I'm trying to get you.",
            "I've gotta tell another story, so it's like a picture.",
            "This one.",
            "It's the prisoners dilemma.",
            "Hands up.",
            "Who knows the prisoners dilemma.",
            "It's about 50% for us, so having to say a word about it right, prisoners dilemma is the the just to get it quickly.",
            "So nice of parroting to know about anyhow, because you can apply to other cases.",
            "It's typically from game theory.",
            "So the idea is that you have got the police that have arrested two people during the night because there's been a lot of noise or some things happen and the police is going to interrogate both of these people A&B separately and to each one.",
            "The deal is the following is saying look.",
            "I mean we've, you know we've caught you out there in the street and at the same time or more or less the same time there was this this robbery an so we think you did it now I'm not sure that you did it well.",
            "We think you did.",
            "So basically we'd like you to plead guilty.",
            "So what happens if you admit?",
            "And your colleague is silent.",
            "Well, we're going to say right where you try to cooperate with justice, so you get out for free and he gets five years prison.",
            "OK, if it's another way around, if you decided to remain silent, which he admits, then you're going to get five years prison and he's going to be able to get out for free.",
            "Second option is that basically you both admit in that case where you get three years prison each.",
            "And the last possibility is that in the very unlikely event that you both decide not to help us and to actually not say anything well, in this case you both get when your prison each because, OK, we can't charge you for the robbery, but you know, we found you at night and it's not very good and you should be so one year prison is.",
            "So that's the deal."
        ],
        [
            "So this deal is actually used an represented in this sort of table, so this is a matrix in which we can read the inputs and outputs and inputs of both players and the gains of both players.",
            "In this case, the gains means really the losses, so I can see the player A is playing against player B and that in the case where.",
            "I don't know it's let's say this is the situation where player A is silent and player B admits.",
            "So in this case we said that in he gets five years prison and he gets out for free.",
            "So anyway, I was really I should say a word about this just just for the sake of that's got nothing to do with grammatical inference, just for the sake of it.",
            "If it, if you look at this metrics, you're actually going to find out if you analyze it carefully that the correct strategy consists in.",
            "Admitting.",
            "OK, the correct strategy if you just looking at typically expectancy values is admitting because you know if why because?",
            "Well, let's see what happens.",
            "Suppose my my opponent here admits.",
            "Well, if I admit I get three years prison and if I'm silence, I get five years prison.",
            "So you know, I should admit.",
            "And if, on the other hand, my opponent is silent, well, if I admit I get 0 here, I get out for free, and if I'm silent I get 20 years prison.",
            "So in both cases my good strategy is to admit and both players taking the same decision will end up in this stupid situation here, which is obviously not the best common case for the two.",
            "So this has been analyzed in a number of research papers in a number of things, but it's not very interesting as it is for what we're interested in."
        ],
        [
            "What we're interested in is an iterated version, so an iterated version is.",
            "Suppose that we don't have to spend five years present.",
            "We just pay 50 rows or something, and so we're going to just keep on playing this game many times against the same adversary and seeing on the very long run, what's going to happen.",
            "So what does under very long run mean what it means?",
            "I'm going to actually do a limit of means, so I want to see just the average over a very, very long run, so potentially infinite light run.",
            "Right of how much am I going to win per move so we should notice also that in this game I'm not very interested about how well my opponent is doing.",
            "That's not very relevant.",
            "I'm only interested in my own gay OK, and I'd like to be able to do as well as possible."
        ],
        [
            "In this situation.",
            "Now suppose.",
            "To relate with what we were saying before, suppose that my adversary is actually using a strategy like the one we saw before.",
            "OK, so he's playing with a rational strategy.",
            "So how could I beat him?",
            "How can I do the best possible against such a rational strategy?",
            "Right, so first case."
        ],
        [
            "This is, I know the strategy.",
            "For some reason.",
            "You know I've been doing some spying, so I've got hold of his strategy and I look at this strategy and I think how with this strategy can I do, which is my best moves, given the fact that his strategy is whatever it is.",
            "Well, that's a little bit of graph theory.",
            "It's not very difficult."
        ],
        [
            "What you do is something like this.",
            "So let's suppose that this is my adversaries strategy.",
            "So remember that what he's going to do is what's inside the states.",
            "So basically, his strategy consists in first, he's going to admit you know he doesn't trust me at all.",
            "He starts by admitting and then if I'm silent, he decides to be silent.",
            "And if I'm silent again, he decides to be silent again.",
            "And if I keep on being silent, he's going to be silent, which is, you know, the good case where we're both just getting or paying 1.",
            "Oh, rule.",
            "If I had met once, while you know his shows, little of mistrust, but he says, look, you know I'll forgive you if it's just a one off, so he'll be silent.",
            "But of course, if I admit a second time, then we get into this bad state where he's not going to trust me and he's going to have to admit.",
            "So what should I do in this case?",
            "What's my best strategy?",
            "Will I look at my game function?",
            "I put some weights over the edges just to indicate how much I'd be winning in each one of these situations.",
            "Notice that winning correspond somehow to losing and I try and find the cycle with.",
            "The minimum mean wait, so here I've got a cycle where if we got into this cyclic situation then I'd be losing one horu every move.",
            "But if I manage to get into this cycle here then I'd be losing one or oh zero 1 / 0 and so on.",
            "So I am interested.",
            "I am interested in reaching circular situation in which I am going to be paying the least possible.",
            "In terms of mean, so this is the one where I want to get you.",
            "And how do I get there when I get there by choosing the cheapest path?",
            "In this case, the cheapest path consists in actually doing this.",
            "So this is just some very simple maths and graph theory and it's just going to say that this is how I actually reach the best for me.",
            "So I mean it makes sense.",
            "I mean the best strategy for me consists in actually starting here, being a silent once an accepting to pay full penalty and then getting into this lovely cycle of saying, OK, you're trusting me.",
            "OK, I'm silent.",
            "And now I admit once.",
            "You mistrust OK, I'm silent again admits silent admits.",
            "Silence serves the best situation for me.",
            "OK, so that's again will just."
        ],
        [
            "All theory.",
            "The question is.",
            "This is what we've just done."
        ],
        [
            "The question is, suppose I'm not given the strategy.",
            "Can I learn it?",
            "So I had to reach learning.",
            "So what's learning about here?",
            "It's saying, OK, the strategy isn't there.",
            "What I've got is the opponent.",
            "Perhaps I can play a game against this opponent and from the game I'm playing against this opponent I can reach an idea of what the strategy is.",
            "OK, so this is what we're going to do."
        ],
        [
            "So we'll talk about the question of saying, well, perhaps I should be playing the game and learning the strategy at the same time later.",
            "For the moment, I want to suppose that I'm given a game, so I'm giving this game, which is here my moves and his moves, and from this game that has been played, I'm going to try and reach the strategy.",
            "So how do I interpret this?",
            "This game, which is here?",
            "Which are the columns of moves?",
            "First move for him, first move for me.",
            "First move for him, the second for him, second for me and so onwards.",
            "How can I interpret this?",
            "Why can you interpret it as strings?",
            "Because the machine we saw earlier is deterministic.",
            "It means that in every game I'm going to play, his first move is always going to be to admit, but also in any game I play.",
            "If I start by playing ASA, then his move is going to be a.",
            "That's just because the machine is deterministic.",
            "He's not making any sort of random choices or nondeterministic choices or probabilistic choices.",
            "He's just, you know, applying a strategy so I can get this data and transform it into strings as it's on the right.",
            "So the logic of the algorithm is simple is I'm going to have got these strings and I'm going to try and create a machine that can pass.",
            "Past means to run the different strings and to try and reach conclusions that are consistent with what I can see, so the algorithm isn't a given algorithm in grammatical inference.",
            "It's sort of uses many of the ideas that you can find in the literature an since for that reason, why is it also addresses a number of the questions people are interested in?"
        ],
        [
            "So the first thing we saw was that the algorithm starts by saying after having seen nothing.",
            "So my first move says my opponent is to do an A and then I'm going to play in a.",
            "So that means that we know that the automaton looks like this.",
            "OK, there's a first state, and this first state is labeled with an A.",
            "Can't really see it when there is an A in the middle of that.",
            "And what then?",
            "We're left with is we're going to have to deal with this dangling edge here or this dangling transition.",
            "It's as it's called in the case of an automaton which we're going to have to do something with, but for the moment, since I haven't tried to see what's coming next, this is the situation in which I'm in.",
            "So let's try."
        ],
        [
            "And deal with this a so.",
            "What we're going to try and do is we're building this transition here.",
            "So why am I building this transition here, so I'm building upon the hypothesis I had on the move before, and I'm building the transition here, basically because I had two choices.",
            "One choice was doing this, and the other choice would have been to build an entirely new state and say, well, you know, I go to this new state within a.",
            "So the reason for not doing that is typical greediness reasons.",
            "I mean, I've got to have some sort of a strategy.",
            "Sort of trying to get some way in in a greedy way, so go for the go for the easiest.",
            "The second argument is going to be the one of size.",
            "The idea is that, like in any other machine learning applications, we think that going for some simple solution is better than going for some complex solution, meaning that I've got in mind that I want to find a small automaton rather than a large automaton.",
            "Now this makes sense under a lot of reasons, it makes sense under reasons dealing with something that's called Occam's razor.",
            "Is anyone been talking bout Occam's razor these days?",
            "Which is, you know if there's a variety of explanation.",
            "Choose the simplest.",
            "This is an explanation that comes from the 12th or 13th century, and it's nice for philosophical reasons, but it's actually good for mathematical reasons.",
            "It makes sense you can actually prove that an orcam algorithm, one that would be trying to find a small solution as small.",
            "Classifier is actually going to be better, is going to be better than one that is probably trying to find something large.",
            "It's explains also why in machine learning where putting so much attention to pruning problems to simplifying, there's a lot of issues related with this.",
            "So anyhow, logically we try and do this, and since this work because it's actually consistent with what I can see in the datas, I keep it.",
            "My next symbol is an S, so I'm now left with a dangling S here, which I've got to do something about.",
            "So what?"
        ],
        [
            "I do with it.",
            "Well, logically I try and do the same.",
            "You know what worked once might work a second time.",
            "I again try and just do a loop on the state saying when I come back to the same place with an S. But here I reach something that is inconsistent with the data seen so far.",
            "I can pass.",
            "I do a followed by an S and the airport says the automaton is an A.",
            "But what my data is saying is that the output is an S. Some means this is wrong, so since this is wrong, well I've got to do something different.",
            "So what do I do when I undo the last thing I've done?",
            "Which is this S and I try something different.",
            "The only possibility of something different here is creating this new state.",
            "So we may argue and say, well, you know, perhaps if we'd actually done something with the A the first move, then I could do something different now.",
            "Which is true.",
            "The only reason for not undoing everything is economically it's on the basis of combinatorics.",
            "I mean, you can't just try everything out because the number of possible automata grows in an exponential way, so you've got to take some sort of decision greedy decision, whatever had done before.",
            "I keep and it's just the last decision that I that I modify.",
            "So now what I have to do when I could pass all this so I can pass a S with the A that is here I cannot pass, meaning I can't run it through my automaton.",
            "So I'm going to do some."
        ],
        [
            "About that so the fourth decision I take is to bring back this a back to this state here.",
            "So why did I do that again?",
            "I had three possibilities.",
            "One of them was doing this, the second one would have been to actually do a loop with a here and the third one to create a state for greedy reason.",
            "I tried first to look for the initial state, then the second state.",
            "I created an if not, I would have looked at the Thursday since the first one actually works.",
            "Everything is OK. Notice one up over one thing that is nice here is that actually by doing this I've been able to pass not just one string.",
            "But three extra moves, which means, which is something that's actually going to happen in practice.",
            "When we write this sort of algorithm, is that we're going to have an enormous amount of data, but my automaton, little by little, rule by rule, is actually going to get rid of many packets of data at the same time.",
            "OK, you don't have one rule per piece of data, but it sounds reasonable.",
            "OK, so up to here it's alright, and now my.",
            "Next rule, the game was saying 1234566 symbol is an S."
        ],
        [
            "What do I do with it?",
            "So well, same argument as before.",
            "I try and bring the S here, but this doesn't work, so this doesn't work because of this rule here where we can see that IIS aass should buy the says the data allows the opponent to output an S, But my automaton, my theory I am building, says it's may, so that doesn't work.",
            "So the second possibility was doing a loop on the S state.",
            "Right, and that doesn't work either.",
            "But you've actually got go further in the data to notice that that doesn't work, so if that doesn't work, it means why."
        ],
        [
            "It means that we've actually got to create a more complex model with a third state, so it's just again that last transition with an S which is just passed on to a new state S. OK etc etc."
        ],
        [
            "We just go and you've got this.",
            "If you want to follow in the slide."
        ],
        [
            "And at the end of these decisions were left with."
        ],
        [
            "This may."
        ],
        [
            "In here, which is the result of the algorithm.",
            "So this was a deterministic even if, let's say freely written algorithm where we're just taking the data little by little, trying to see what rule we make and at the end of the day we reach.",
            "We reached this this machine.",
            "What should I say about this machine?",
            "Well, first of all, it is the machine we had in mind at the beginning.",
            "If you remember, this was the one that we were hoping to have.",
            "Obviously, in order to get this, I chose very nicely the example.",
            "In practice it doesn't work as well in practice for us to be able to have an example game that would allow us to bring this well clearly you need a game that is actually going to use all the information inside here.",
            "If you've only got a game, this is only using part of the graph, then you're not going to be able to learn.",
            "Let's say this transition here if you're actually never using it.",
            "So you need to invent a game that is actually using a bit of everything, and that allows you in a way to take the right decisions.",
            "To conclude on the."
        ],
        [
            "This two things.",
            "First one is that.",
            "This sounded easy.",
            "I was just sitting there.",
            "Was looking at a game, you know I was like seeing how he was losing the money against their opponent and once he lost the money then I would actually come in and say OK.",
            "I know the strategy I can play in practice.",
            "Obviously you probably have to pay for that data and you find yourself in an expert in an active learning setting.",
            "You also find yourself in a curious setting with these sort of game.",
            "Where suppose I have learned the automaton we saw five minutes ago.",
            "I'm satisfied with it.",
            "Because thanks to that I managed to find a way to get into this nice loop.",
            "You know where I was actually winning, or let's say only losing 50 while one oh row every two moves.",
            "But who tells me that if I done a bit more exploring and tried some more complex series of moves that we wouldn't find ourselves in this situation where there's a much better possibility?",
            "I never can tell with this sort of algorithm that I have learned the only thing I can tell, right is that OK, I've reached something for which now whatever I'm doing is consistent with what's happening, but perhaps there's something completely knew that I should go and explore, so this is what they call exploration versus exploitation.",
            "Of dilemna, and it's a complicated question, including in bandit problems.",
            "I think you've also had some bandit problems the other day.",
            "Yeah you can."
        ],
        [
            "That sort of question.",
            "Second question which is open is what happens if the machine, instead of being deterministic, is what we've seen is actually probabilistic at, which seems a little bit fairer.",
            "So you know the output of the opponent isn't always yes or always know.",
            "It's mostly yes or mostly know this sort of machine actually is related to things called Markov decision process is which exists in a number of fields.",
            "Robotics being probably the most important one.",
            "And I think in this years I CML, there's been the 1st paper attempting to try and learn this sort of machines."
        ],
        [
            "I guess just for the just for the sake of knowing for this competition of this sorry this game of playing the iterated version of the prisoners dilemma has actually taken place.",
            "There's been a competition where people sent their algorithms and played win against the other, and curiously enough, the one that one was using this rational strategy here.",
            "OK, this is tit for tat which is just.",
            "You know if you admit, I'll admit, if you're silent, I'll be silent.",
            "So if this is the best strategy overall at this game.",
            "And you can't beat it.",
            "The only way to do well against it is actually being silent, right?",
            "Because if not, he's immediately going to retaliate."
        ],
        [
            "OK, so we've seen through this this example what the algorithms of the algorithmics of the problem are.",
            "Let's now and see what what.",
            "Let's say about the results.",
            "So suppose somehow I've written an algorithm like the one we've seen 2 seconds ago, and.",
            "Once I've written this algorithm or my done, what can I say about it?",
            "Can I?",
            "Can I say yes, this is a good algorithm?",
            "Can I say this algorithm is actually going to allow me to always learn something right?",
            "So first question is, why bother?",
            "You know, so this is what, let's say machine learning in the 80s and 90s was about.",
            "You found some really nice algorithms and it seemed to work nicely.",
            "And this was cool, and it was nice.",
            "It was the good old days before you had to do maths in the in the field.",
            "So it's so well that question today doesn't doesn't happen.",
            "You actually do have to give an answer in most cases.",
            "And so then, why should we do something whenever researchers and machine learning or not?",
            "But that's not entirely true.",
            "OK, so at least you might have the feeling that other researchers are not having to prove that their algorithm is going to converge in some sense.",
            "So actually true.",
            "Usually the theory is just done in a very different way."
        ],
        [
            "So just to answer this, let let let's have a three motivating questions.",
            "The first one is.",
            "Suppose that our task was not to learn algorithms for.",
            "Learning automata, but algorithms for generating random numbers so you know you've come out with a random number generator, and this random jumman random number generator generates 17.",
            "So you can say you know this is good.",
            "I mean, there's 17 years this is random, or this sequence.",
            "Here is a random sequence.",
            "But somehow what I'm trying to express here is the fact that you have built a learning algorithm and you look at the result and then you're looking at the results.",
            "And you say, yeah, this is good.",
            "You know I like the result.",
            "The result isn't really telling us much about the algorithm.",
            "The result is saying something about the result.",
            "I can't say that 17 is random or not.",
            "I mean I can say something about the algorithm that is produced 17 but not about the number 17 itself.",
            "And same for the learning algorithm.",
            "I mean I can look at the learning algorithms result and say you know this has learned this DFA.",
            "Yeah, I think this is a cool DFL thing.",
            "It's a nice one, but it's not going to really be telling me if I'm learning or if I'm not."
        ],
        [
            "Second question is to say.",
            "In the case of languages, learning is an ongoing process, so that's clear for the kids, but it's clear for us.",
            "We've never learned we're always learning.",
            "There's no moment where you can really say.",
            "OK, I've learned a language.",
            "Right, so learning is a process in itself.",
            "It's you know it's the process of learning that we're actually trying to discuss.",
            "It's not the result of the learning itself."
        ],
        [
            "And the third question related to this one is that I don't even think that the statement I have learned makes sense.",
            "OK, or the algorithm is learned.",
            "I can say something.",
            "Intelligence about I am learning or my algorithm is learning.",
            "But how can I say that this algorithm has learned something?",
            "All I can say is that my algorithm has perhaps solved some combinatorial problem or some optimization problem.",
            "I hate hasn't learned right?",
            "Learning something much more complex and you never.",
            "You can't say it has learned unless you're really saying, you know I had something hidden here and it has found what was hidden in which case.",
            "Well, it's then something a little bit different, which we call identify."
        ],
        [
            "Nation.",
            "OK, So what usually is called having learned is just to say that well, perhaps whatever grammar automaton or any other function that we're trying to find, where is the smallest or the best for a given score or has solved some sort of a combinatorial optimization or comedy oratorial characterization issue?",
            "So in other cases we also say that perhaps the learning algorithm has converged, converged in the sense of, say, EMEM.",
            "You run the algorithm at one moment, it can't do any better, so you know it's reached some sort of a local maximum or local minimum depending which way around you're looking at the question, but you you know you haven't learned you.",
            "All you've done is you've solved something.",
            "Some nice problem instead, which is good.",
            "I mean, which is what we're usually doing.",
            "We're usually trying to transform or learning problem into an optimization problem.",
            "Say it slow."
        ],
        [
            "You could find it that way.",
            "So there are always some very nice reasons, very good mathematical reasons related to our Cam.",
            "We talked about compression issues.",
            "We know that an algorithm that compresses the data is an algorithm that is learning.",
            "We also know that an algorithm that achieves some MDL result, MDL is minimum description length, so that is also theories that helps us to understand the learning that learning has been achieved.",
            "Kolmogorov complexity is also in there.",
            "So all these are mathematical Arg."
        ],
        [
            "That's why we're interested in convergence.",
            "We're interested in convergence to be able to say something about the algorithm.",
            "Say, OK, my algorithm has too.",
            "It's not just good enough that it returns the DFA.",
            "We're actually going to have to say, well, why is it getting somewhere?",
            "Why is it going in a good direction?",
            "Why should I trust it?",
            "So what are the sort of convergence criteria were interested in?",
            "Well, we'd like to be able to say that I've I've learned and thanks to this learning issue, I will be able to predict so I can predict, for example, the next string somebody gives me.",
            "I can label saying, well, this is in the language of.",
            "This is not or, like earlier on in the game saying, well, his next move is going to be to admit or to be silent, so I know that if I can predict in a certain way, that's a way of saying, well, that I have learned.",
            "Obviously, if I can.",
            "That's some money on the outcome of this result.",
            "Betting money means that I've got probably a distribution of probabilities somewhere.",
            "If not, I wouldn't be doing it then, that's even better."
        ],
        [
            "Now that's nice if I said if I had a distribution of probability, is there some material, some probabilistic world I'd be able to say?",
            "Well, I can predict the outcome, so that's very strong, and in practice it's not always that easy.",
            "I don't always have this distribution of probabilities, so another way of saying things is saying, look, I'm learning and basically given the data I was given.",
            "I've done as well as possible.",
            "Or in other words, saying.",
            "Well, if there was something to learn and I haven't learned it, it was because the data was insufficient.",
            "It's a bit like the example I gave earlier, where I said, you know, if one edge hadn't been used during the learning data, there's no way I can learn it, so I can't really talk about learning if I've got insufficient data.",
            "This may seem very weak hypothesis.",
            "The one of saying blamed the data, not the algorithm.",
            "It's actually much stronger than one things.",
            "It's really means sometimes you can't just blame the data.",
            "Sometimes you've got a bad algorithm and whatever the data, it's actually never going to learn something that is supposed to learn, or something that you're claiming it should be learning.",
            "Will give an example."
        ],
        [
            "2nd.",
            "So.",
            "Well, this is the example.",
            "This is a moment.",
            "So suppose I've got an algorithm that is actually trying to learn context free grammars, so that's what it's supposed to do it we designed to learn context free grammars so income, some data and from this data my algorithm is returning a context free grammar.",
            "But if my algorithm contains what we call hidden bias, so hidden biases that fact that in fact the grammar, because the algorithm because of its design is only capable of learning biased grammars, grammars that can only do special things.",
            "For example, regular grammars are the ones that are equivalent to finite state automata.",
            "In this case, I'm claiming that my algorithm is capable of learning more or less anything in this class, which is always biased towards the same type of grammars.",
            "Now then, if I find myself in the situation I had two seconds before let's go."
        ],
        [
            "Back to the slide.",
            "Blame the data, not the algorithm.",
            "I that's not going to be the case because I mean I should always be able to converge towards the correct grammar.",
            "It's just a question of time or it's just a question of data, so it's not exactly."
        ],
        [
            "So two settings the non probabilistic setting and the probabilistic setting non probabilistic setting.",
            "Is called identification in the limit, so identification in the limit is a famous setting which is really used in a field called inductive inference.",
            "Which is related to machine learning, which is a field that goes back to the 50s.",
            "And it's the idea that it's the idea of a field way.",
            "Basically, you're given numbers you know you're given #2, then #3 the number 5 and #7, and you know the game.",
            "Are you trying to guess which is the next number?",
            "So this is inductive inference.",
            "So, edification in the limit is 1, we're going to talk about and of course, because we're computer scientists were just trying to put some resource bounds, saying, well, we'd like to be able to identify given not too much data.",
            "And then active learning, which we perhaps talk about after."
        ],
        [
            "So the chief papers are from this person called Mark Gold E Mark Gold.",
            "I've never known what the E stands for.",
            "E Mark goldran.",
            "So in the 60s and 70s he came out with these ideas."
        ],
        [
            "So let's see what the ideas are.",
            "General idea is like the online learning.",
            "The data is presented.",
            "One item of data at the time, and after each item of data we have as a learner to produce a hypothesis.",
            "So pretty sure hypothesis then the new item of data arrives.",
            "We may have to update this apophysis or we may keep it and so on.",
            "With now identification, the limit says the following.",
            "It says that at one moment or another I'm going to stop changing my opinion.",
            "OK, my algorithm can't go on forever and ever modifying, its modifying its epatha sis.",
            "So I would say that I identify in the limit when there is a limit point.",
            "There is a moment in which my algorithm is going to identify correctly.",
            "That's your name."
        ],
        [
            "Example, so this is just with numbers and not with grammars and automata.",
            "The first I item of data arrives is number 2.",
            "My hypothesis saying, OK, well, this is just a single language.",
            "I reckon it's just a set containing one element #2.",
            "So #3 arrives.",
            "So I've got #2 and #3 where clearly my previous hypothesis is bad.",
            "So I have to change it.",
            "So I change it to double time two and three.",
            "My next example arrive and it's #5, so here I could again just keep on forever and ever saying 235 and every time a new example arrives I just added to my set, but clearly that strategy is not going to allow me to identify in the limit because I'll be changing apophysis all the time if my set is an infinite set.",
            "So I've decided my algorithm is going to actually come out with something braver, which is going to say OK, 235.",
            "Those are three fever Nachi numbers, so I believe 2, three and five other fever Nachi numbers.",
            "So that's my heavy current hypostasis and arrives #7.",
            "#7 is not.",
            "If you weren't actually number, so I have to change my hypothesis and I have now thought 2357 or how this looks better.",
            "I think it's the prime numbers.",
            "So 2357 Next Element is 11.",
            "I don't have to update.",
            "Hundred 303 is prime don't update.",
            "And so on.",
            "And so on."
        ],
        [
            "OK, so this would be an algorithm, so in that case obviously you've notice my example had to stop after six or seven elements, not just because I didn't have any enough size on my slide or that it will be boring.",
            "But because there are an infinite number of possible examples, so it really means that even if I had kept on doing this example for about an hour and a half, just passing numbers at no moment can I really be sure that I am not going to have to change apophysis just the next move.",
            "So this is a mix, a little bit complicated, all I know.",
            "If my algorithm does identify in the limit, is that a one moment or another I will stick to the same epatha sis, even if I can't know when, or even if when I'm actually running the algorithm.",
            "I don't even know if this is the correct if offices or not.",
            "It's a bit like in my game earlier on, all I knew through my algorithm was that I would find eventually the good solution, but in how long when I don't know?",
            "OK, so to be able to learn, we said there was there was a presentation of data, so presentation is just.",
            "This is just a function presenting data, so data is anything it could be numbers or it could be strings, or it could be examples, counterexamples, or if you're talking about strings, you could be presenting prefixes or subsequences, but some information that is arriving one item at the time.",
            "Obviously, when you've got a presentation of the language, well, somehow this presentation is related to a language, so we've written a function called yields which associate's to a given presentation.",
            "The language which is the one we supposed to try and learn.",
            "And obviously, while the order in which we present the elements doesn't matter, if at the end of days two presentations present the same data, then logically it corresponds to the same language, and that's where the slide says."
        ],
        [
            "So typically one is the text presentation, protects presentation of the language, is just a function of fine like the one we had two seconds ago, such that at the end of the day and then of today means in an infinite number of steps.",
            "While we have seen all the strings of the language, if you want to be able to learn a language, you're going to have to suppose that all the strings will eventually appear.",
            "So if I'm trying to learn a language and only some strings in a biased way are going to appear by quite logically, I'm only going to hope to learn for the.",
            "Strings of the language corresponding to the bias strings, not to all the strings."
        ],
        [
            "And inform presentation is not just the strings of the language, it's label string, so it's a string with a one thing or a plus saying yes.",
            "The string is in the language or a string with a minus indicating that no, this string is not in the language."
        ],
        [
            "So typically presentations.",
            "Let's say the language is a MVN.",
            "The typical language used to prove that this language is not regular.",
            "So, well, you could have a legal presentation from text which would correspond well, all to all the pairs A2B2A7B7A4B four.",
            "You can have repetitions in our presentation.",
            "And an illegal presentation would be one way you would only get the same string repeated up to Infinity, so you wouldn't have the whole language which would appear.",
            "Legal presentation from an informant.",
            "Well, it's pairs like this.",
            "ABAB is a negative examples.",
            "A2B2 is a positive example, and so on."
        ],
        [
            "So there is a.",
            "No, this is not the learning function.",
            "This is a this is a mistake.",
            "I don't know why I've put it there.",
            "OK, so given the presentation, the end first elements of the presentation or.",
            "The N 1st elements of the presentation we just denote by fight some N. And we've got a learning algorithm which does what way just takes.",
            "Then first elements of a presentation, and given those N 1st elements, bills or returns or grammar.",
            "And given a grammar, of course we've got a naming function and not a learning function naming function which is LG, which is just the language generated, recognized, represented by the grammar."
        ],
        [
            "I don't get this one, so identification the limit is the little picture which is here.",
            "Let's give the whole picture.",
            "OK, so the whole picture here is just saying that there is a little class of languages where we're going to get information presentations which are going to come from this presentations of this class of languages and we're trying to work on grammars, each grammar corresponding to one language of their.",
            "I mean clearly they can be equivalent guar grammars for the same language, so that's her problem also.",
            "And we've got a learning algorithm which is trying to, given the presentation learner grammar.",
            "And of course we'd like this triangle to be.",
            "Work nicely, which means that the learned grammar at some point or another has to be the grammar corresponding to a language, which is the one corresponding to the presentation.",
            "OK, so that sounds fair.",
            "Let's skip these."
        ],
        [
            "Also.",
            "So this is what identification in the limit is about is just about saying that there is a step N at which all was for which my nice little triangle closes down neatly, and whatever I was supposed to learn, I have learned so when a class of grammars are a class of languages is identifiable.",
            "In the limit, it means that I have gotten algorithm can do that.",
            "Now, obviously that's a very just nice mathematical setting.",
            "As I said before, this is inductive inference, but if we're going to use these practically, we're going to have to put some limits related with.",
            "Well, typically with polynomial bounds.",
            "So what should we bound?",
            "What should we put a bound on when we're trying to learn?",
            "OK, so we've got, you know, we've got these presentations.",
            "We've got this learning algorithm, so we obviously can put a bound on the time, but I can't really put a bound on the global time because I don't know how many data I don't have any control really over my presentation, so I can't say, you know, after 200 examples I have learned because I don't control the presentation, I just know the one day or another.",
            "All the exalta interesting examples will have appeared, but I don't know when they will have appeared.",
            "I can put about on update time.",
            "Update Time is since we're on an online learning process.",
            "Is the time I need to go from hypothesis I2 hypothesis I plus one?",
            "Well, actually this has been proved not to give any advantage at all.",
            "It's very easy to do an algorithm which has got, say, polynomial update time, but all it does is really says, well, I used my time to try and update.",
            "Oh dear, I haven't got any time when I just returned the previous hypothesis and I just, you know, sort of keep this extra time or when I get more time.",
            "I'll be able to run my examples so update time doesn't really help us much.",
            "There's a very nice model as they called.",
            "IPE IPE is implicit prediction errors in an online learning model.",
            "I am learning hypothesis, and I've got a new example that arrives.",
            "So if my bath ASIS is capable of classifying correctly the very next example that arrives, then I'm going to say, OK, that's that's good.",
            "You know that doesn't score negatively, but if my hypothesis is actually misclassifying the next example, it arrives.",
            "I call that an implicit prediction error.",
            "Obviously a good learning process would be 1 where the number of implicit prediction errors would be small.",
            "Meaning the number of times that you know that my hypothesis is making mistakes is going to be small.",
            "Clearly it's going to be related to the number of times I'm going to have to change the Spartacist, so keeping this small in an online learning process seems like a good idea.",
            "I just mentioned that no notion of having to change my hypothesis well, this called mind changes.",
            "So you can also put a bound on the number of mind changes.",
            "Mind changes is how many times have I got to change my hypothesis?",
            "I don't really have to if I don't want to.",
            "I mean, I might have something that is misclassified classifying.",
            "And still I keep it, but it's not usually very good idea.",
            "So how many times do I have to change my mind and online?",
            "Learning process that doesn't change its mind too often is a good online learning process.",
            "And then this queries or the number of good examples needed.",
            "So there's a lot of papers and a lot of ideas out there in comparison of these, none of which is entirely convincing.",
            "It's more complicated than it looks like.",
            "I mean, you don't have one good definition of learning in efficient way.",
            "It's not just saying I've got a polynomial time algorithm.",
            "If my algorithm is polynomial time, but it's just returning a bad solution all the time.",
            "We're not very happy about this.",
            "So there's still room, and there's still people working on these things.",
            "It's not all that simple.",
            "OK."
        ],
        [
            "Now then, that was the case where we don't have probabilities when there are cases where you can have probabilities.",
            "So you have really good probabilities for at least two reasons, either becausw, you're better distribution over the examples and you suppose you're working with this distribution or and this is a specific thing in this field, because what you're actually going to learn is a distribution.",
            "When I'm seeing specific know, there's other people learning a distributions, but in fact what happens is that the distributions are themselves defined through probabilistic finite automata, or through context free grammars, so those are what you call probabilistic automata or probabilistic context free grammars.",
            "How are we doing for time?"
        ],
        [
            "I'm going to skip the."
        ],
        [
            "Pak results I don't know.",
            "Has anyone been talking about Pak results?",
            "Pak Pak learning well should then OK let me just say a word about them then."
        ],
        [
            "Sorry, go back to this.",
            "OK so the idea is the following.",
            "Is this actually you can apply in other fields in machine learning so I should talk about it.",
            "These are actually questions that have been looked into a long time ago, probably about.",
            "I'd say 30 years ago and I had a number of results were known and a lot of people are now including people working in beige and learning.",
            "Trying to reintroduce in one way or another, this type of mechanisms so the idea is the following, so you know we've got this data that we don't know anything about.",
            "All we know is that there is an unknown distribution, but a stable distribution is not changing overtime.",
            "OK, there's a distribution over the whatever examples we're trying to classify.",
            "And in order to measure the quality of our learning process, we're going to sample twice.",
            "We should sample the first one to be able to get hold off for training data, and we should sample a second time in order to look at the testing data.",
            "So one thing is just to learn, and the other thing is to measure how well we've learned, and this is the pack setting."
        ],
        [
            "So to be able to pack learn.",
            "That was unfair.",
            "It wasn't 30.",
            "In order to pack learn without class of languages with custom class of grammars and we've got two rule para meters epsilon and Delta.",
            "Subcell and Delta are really going to tell us know how close I want to be to the truth.",
            "I'll explain how in a second and then there's two sort of numerical data is explaining well because of technical things I'm not going to go into here.",
            "We're going to have a maximal size over the number of states of the machines were trying to learn, or the number of rules in the grammar.",
            "And we're going to have also some sort of maximum length over the strings, but it's not really essential to understand this now."
        ],
        [
            "So the idea is somehow supposed we've learned and we come out with a hypothesis which is a classifier.",
            "We're going to say that our hypothesis is epsilon approximately correct whenever the error made by my hypothesis in comparison to the ideal.",
            "Grammar so there is a notion of an ideal grammar.",
            "There the error is less Ness."
        ],
        [
            "In a picture it means this.",
            "You know you've got, you know the rule grammar and you've got your hypothesis an in between the two.",
            "Well, there's some errors, some places where you know your grammar is actually saying that this is in the language and it's actually not, or your grammar is saying this is not in the language and it actually is.",
            "So all these little spots that are in this zone defined by the two moons, all these are errors.",
            "And we can measure these errors as a surface because we've got a distribution of probabilities.",
            "So it's really what is the probability of landing when testing of landing exactly in those zones.",
            "So we want that to be kept very small.",
            "So that's that.",
            "Sounds like a fair idea, so why do I need a second parimeter?",
            "Why do I need the Delta?"
        ],
        [
            "Because, well, this is the argument that the two years ago there was a French election and they were announcing that unless there is a surprise, there should be no surprise.",
            "So in this sentence, we're just saying that there's actually 2.",
            "A levels to measure errors.",
            "One is the second level.",
            "Seems there should be no surprise, so you know that's my epsilon.",
            "Epsilon is the surprise factor.",
            "But then there's unless there is a surprise which means, well, you know we may have made a really big mess through our sampling or through our choosing RF examples or something could have a big and then been considering long notice that the examples could be the errors could be on the examples from the training set or even on the testing set if.",
            "If you completely choose wrongly becausw.",
            "Unluckily the examples, then you would be wrong.",
            "So there you need your second parimeter."
        ],
        [
            "Need your Delta.",
            "OK."
        ],
        [
            "So."
        ],
        [
            "I'm not giving the maths but the maths is saying that we want with probability.",
            "At least one minus Delta.",
            "The error to be at most epsilon.",
            "OK, so you've got 1D.",
            "I want epsilon and there's all sorts of really nice results or nice theory."
        ],
        [
            "Written about these things, the problem being that when we're working with things like DFA, so deterministic finite automata and same applies for nondeterminism finite automata or context free grammars where all the pack type of results are negative, they're saying we can't learn.",
            "Or we can't learn something in polynomial time.",
            "We need much more than polynomial resources in order to learn, so that's bad news.",
            "That's bad news because Becausw, because that's what we like to do, actually.",
            "It's curious the reasons for doing this.",
            "The reasons for doing this depends not on sort of typical PNP problems.",
            "It depends on cryptographic assumptions.",
            "The idea is saying that.",
            "Well, if I could learn DFA, I could probably break things like RSA codes.",
            "So why would I break codes by learning well becausw?",
            "I'm allowed to produce examples so I can produce examples of messages and messages the way they've been decoded, so I can really produce the examples I want, and so if by producing these examples I was then able to learn the DFA and what the DFA is doing is really decoding.",
            "Bit by bit, but it still is decoding then in a way where my problem of solving of learning a DFA is at least as hard as the one is breaking RSA.",
            "And you don't really believe that breaking error says that simple so OK.",
            "So it's related, very good, very directly to that sort of risk."
        ],
        [
            "So in Alternatively, since we've just seen that we're learning DFA is not going to be a very easy or learning context.",
            "Free grammars, why don't we learn directly the distributions?",
            "So learning directly the distributions is making the processes that I'm not trying to learn a classifier any longer, but instead of learning a classifier what I'm going to learn is a probabilistic finite state machine or probabilistic context free grammars."
        ],
        [
            "So suppose I've got an algorithm that learns that what then can I say about the type of results or the type of algorithms that I've done where I can obtain something that is called identification in the limit with probability one?",
            "So earlier on we saw identification in the limit, which was saying that I am sure that one day or another my algorithm is going to be returning the correct solution.",
            "Identification in the limit with probably what the probability one is saying.",
            "I am sure with probability one or better said it's not impossible because impossibility is not defined that way from power probabilities, I am the probability that the data arrives so absurdly that I can't learn correctly.",
            "This probability is equal to 0.",
            "If people haven't looked into probabilistic learning, I mean, this doesn't make much sense, but it's just the idea of saying, well, you know, I can write out the the series of elements that would make my algorithm fail.",
            "It just happens that the probability corresponding to this series of events is equal to 0."
        ],
        [
            "Which doesn't mean the same as being impossible.",
            "So what can we do?",
            "Well, if we can compute probabilities, obviously we suppose the probabilities are given by rational numbers or something.",
            "Then we do have algorithms that can learn with probability one finite state automata.",
            "But then we've got problems with boundedness, again resources, so broadness of resources if we're trying to do it with just a polynomial number of data, then that becomes nearly impossible.",
            "I say it becomes very tricky.",
            "You've actually got to add some sort of information in order to be able to get somewhere, but I mean there are algorithms out there, which perhaps I'll be passing."
        ],
        [
            "Second possibility, if I'm trying to learn probabilities.",
            "Sorry, distribution of probabilities is to admit we've got errors, so probability one identification in the limit means I've actually got to learn the exact target distribution.",
            "Perhaps you know, mainly because we're in a probabilistic world, we'd be happy if we nearly learn the distribution.",
            "The whispers to learn.",
            "If we approximate whatever were supposed to be learning.",
            "So you can also have pack definitions of this.",
            "You can have sort of definitions saying that while the distance or the error between what I've learned and what the target was is going to be very small, I'm going to keep it as small as I can.",
            "So what does the error between two distributions means while introduces the notion of distances?",
            "So you're going to have distances between two distributions distances based on.",
            "Well, there's all sorts of possibilities of defining distances.",
            "Between distributions corresponding to norms corresponding to entropies corresponding to things like that, if you are interested with that, I can say a word just brings us a little bit far away."
        ],
        [
            "So yeah, depending on what distance you use, the problem is either too simple or too hard and in between the middle where we're not really sure what the state is and there's some nice algorithms out there for different sorts of distributions, and perhaps I'll be talking about that."
        ],
        [
            "Later.",
            "So conclusion about this, I must say it's a difficult part because without having the algorithms to look into well, one might not follow exactly where is this guy getting too well.",
            "I'm trying to get to is saying we're trying to learn.",
            "We're going to learn Automata.",
            "We're going to learn Grammar's an we're going to have to keep in mind that we don't.",
            "We can't be happy with just saying.",
            "Well, my algorithm does something I need to say something intelligent about it.",
            "So what I was trying to say is.",
            "Well, if I can say something intelligent like saying, well, you know, give me more data and I'm sure to learn.",
            "Or in other words, I'm doing as well as I can with the data I have, but that's already something.",
            "And then there's all the distribution world in which in the probabilities probabilistic world where I'm going to also be able to say things about when I have learned or when I have not learned."
        ],
        [
            "And I think this is going to be a good moment to have a coffee break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone, so I hope my my voice will will keep the whole day.",
                    "label": 0
                },
                {
                    "sent": "I mean last night there was a lot of shouting and it was a bit difficult.",
                    "label": 0
                },
                {
                    "sent": "So I mean I I'm trying to speak slowly, the talk or the lecture is about grammatical inference.",
                    "label": 0
                },
                {
                    "sent": "Grammatical inference is not one of the central topics inside machine learning.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless there are in machine learning conferences or in machine learning groups, all with some people for who for some reason or another are interested in this particular idea.",
                    "label": 0
                },
                {
                    "sent": "Of learning grammars or learning finite state machines, and in a way, I'd say it's more and more the case will cause of the interest of having complex outputs or of having finite state machines which actually can represent a variety of things.",
                    "label": 0
                },
                {
                    "sent": "Um, so I'm calling leggera.",
                    "label": 0
                },
                {
                    "sent": "I'm from University of not not is in France.",
                    "label": 1
                },
                {
                    "sent": "It's at the at the end of the lower Valley, so going into the Atlantic somewhere and I work on.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In grammatical inference.",
                    "label": 0
                },
                {
                    "sent": "These slides will correspond to work that have been done has been done for a long time with the number of people whose names are here and this work has allowed me to do a bit of advertising, right has allowed me to come out with a book which has been published now in March or April I'll pass it around if people want to have a look, and if it's around in your labs, you might want to look at it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyhow, this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I should say this is the outline of the talk as it is in these roughly 200 slides which are here, which are also the ones that are photocopied there.",
                    "label": 0
                },
                {
                    "sent": "I've actually got some other slides about another topic related to it, perhaps during the break will see what sort of topics you'd be more interested in, and then I can shift towards more active learning sort of stuff, or more probabilistic finite state machines.",
                    "label": 0
                },
                {
                    "sent": "Sort of stuff.",
                    "label": 0
                },
                {
                    "sent": "Those are the really the two alternatives we've got.",
                    "label": 0
                },
                {
                    "sent": "So the outline of the talk is what is learning automata about.",
                    "label": 1
                },
                {
                    "sent": "We go through a very long introductory example to get the ideas of the sort of algorithms that are involved.",
                    "label": 0
                },
                {
                    "sent": "Here we talk about validation issues, validation issues is the question of knowing.",
                    "label": 0
                },
                {
                    "sent": "Well, when can I say that I have learned or when can I say that I am learning, so this is not just in this specific theme, but since learning languages is very much linked with upcycle cognitive aspects of a child.",
                    "label": 0
                },
                {
                    "sent": "Acquiring a language well, there are other questions of how do you learn or when do you stop learning or when you say that you're learning correctly.",
                    "label": 0
                },
                {
                    "sent": "All this allows us to have another perspective, which can be of interest even if your theme is going to be very different from this one.",
                    "label": 0
                },
                {
                    "sent": "So we talk about some criteria that allow us to say that yes, we're learning correctly or not, and then we'll go through some algorithms.",
                    "label": 0
                },
                {
                    "sent": "It's really going to be a talk about algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to give many maths.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give the most maths.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be giving is algorithms studying the algorithms, seeing what are the reasons for which these algorithms are working.",
                    "label": 1
                },
                {
                    "sent": "So we be talking learning from an informant, an informant in our context, is saying that we're learning from data which is both positive and negative.",
                    "label": 0
                },
                {
                    "sent": "We've got examples and counterexamples.",
                    "label": 0
                },
                {
                    "sent": "The typical classification task.",
                    "label": 0
                },
                {
                    "sent": "Now there's a second idea of learning here, which is actually.",
                    "label": 0
                },
                {
                    "sent": "Could be related to clustering, but isn't in the sense that we've only got one sort of examples.",
                    "label": 0
                },
                {
                    "sent": "We've only got text, so we've only got examples of the language that they are arriving, and somehow we still have to build a model of the language that's supposed to be what a child is doing.",
                    "label": 0
                },
                {
                    "sent": "He's only receiving positive input, is only receiving strings that are arriving, or sentences, and from these sentences he has to somehow build a grammar.",
                    "label": 1
                },
                {
                    "sent": "So that's what we're going to be interested in there.",
                    "label": 0
                },
                {
                    "sent": "Learning by observing is one way of doing things, which is just I observed the data.",
                    "label": 0
                },
                {
                    "sent": "I reorganize it in somewhere and say look, this is what I've observed learning actively.",
                    "label": 0
                },
                {
                    "sent": "So you've had some online learning?",
                    "label": 0
                },
                {
                    "sent": "I think, so this is a bit like online learning in the sense, or that we're going to be learning on the fly as new and new data arrives.",
                    "label": 0
                },
                {
                    "sent": "The only thing being that the data we're actually going to choose ourselves so active learning in this sense.",
                    "label": 0
                },
                {
                    "sent": "Is learning via an Oracle in the case where the algorithm the learning algorithm is going to be asking questions and through these questions is going to try and build the.",
                    "label": 0
                },
                {
                    "sent": "Whatever he's trying to do is automaton or grammar, or something like that very quickly talk about extensions and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so definition of grammatical inference.",
                    "label": 1
                },
                {
                    "sent": "It's about learning a grammar given information about language.",
                    "label": 1
                },
                {
                    "sent": "OK, so the information is about a language, but we're going to try and learn is a grammar that's for just for obvious reasons.",
                    "label": 0
                },
                {
                    "sent": "I mean, you don't learn a language per, say.",
                    "label": 0
                },
                {
                    "sent": "Language is usually an infinite set, so you have to have some representation of this infinite set.",
                    "label": 0
                },
                {
                    "sent": "And this representation is going to be a grammar or an automaton, or something like that.",
                    "label": 0
                },
                {
                    "sent": "What is the information we're going to be learning from?",
                    "label": 0
                },
                {
                    "sent": "Well, typically strings or sequences?",
                    "label": 0
                },
                {
                    "sent": "But also trees are graphs, so there are now interesting papers coming out about learning graph grammars.",
                    "label": 0
                },
                {
                    "sent": "So where the data are going to be graphs and we're going to try and find some graph formalism to represent these graphs.",
                    "label": 0
                },
                {
                    "sent": "And as I said before, information typically can be the text only one class or informant.",
                    "label": 0
                },
                {
                    "sent": "Two classes, positive examples, negative examples, or actively sort, which is what we call query learning based related to teaching in the sense that, OK, well, perhaps I can feed to the learners some good examples for him to learn.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goals of the things we're trying to learn are going to be related with what is known as the Chomsky hierarchy, so I take it that most of you are trained in computer science, so you all have done some course somewhere of formal language theory.",
                    "label": 0
                },
                {
                    "sent": "As you remember, bit about grammars and Automata and that sort of stuff.",
                    "label": 0
                },
                {
                    "sent": "No, well, if not we'll have to try and go through them quickly.",
                    "label": 0
                },
                {
                    "sent": "Learning how these objects are, the ones we're going to be interested in with extensions extensions which may be putting probabilities on these objects, leading to things like hidden Markov models or probabilistic finite state automata, context free grammars with probabilities.",
                    "label": 1
                },
                {
                    "sent": "So that's a logical idea of saying, well, something purely deterministic isn't really going to suit the real life applications, so we can put probabilities on them.",
                    "label": 0
                },
                {
                    "sent": "And then they said things like pattern languages or an exciting theme, which is the one of transducer learning transducers or like automata.",
                    "label": 0
                },
                {
                    "sent": "But they have gotten input and they got an output.",
                    "label": 0
                },
                {
                    "sent": "So the fact of having an output means that you can use them for a lot of other tasks.",
                    "label": 0
                },
                {
                    "sent": "For example in translation tasks.",
                    "label": 0
                },
                {
                    "sent": "And you probably had a talk talking a bit about transducers this week.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Didn't somebody talk about that?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, some examples of strings just very quickly.",
                    "label": 1
                },
                {
                    "sent": "So well translation example this is.",
                    "label": 0
                },
                {
                    "sent": "I actually picked up when I went to Scotland.",
                    "label": 0
                },
                {
                    "sent": "It was meant to be a Scottish insult, so it's a very elaborate insult.",
                    "label": 0
                },
                {
                    "sent": "But I mean, it's so this was the case where you'd be having sentences in two languages.",
                    "label": 0
                },
                {
                    "sent": "I mean, the first thing you'll probably get in the second you if you compute, you get it right.",
                    "label": 1
                },
                {
                    "sent": "So the translation from one to the other, you're going to have pairs of sequences, and from these pairs of sequences you are actually going to try and build some sort of a trial.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do something like that time series.",
                    "label": 0
                },
                {
                    "sent": "Obviously you need to discretize in some way because if you just keep it with real values, you're not going to be able to do much with the sort of technique.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And would be talking about biological data.",
                    "label": 0
                },
                {
                    "sent": "So people working in bioinformatics have been using grammars and Hmm's to represent things.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you look through it you can see something that looks like a pass tree.",
                    "label": 0
                },
                {
                    "sent": "So I mean you can use a pastrie to just represent the sort of stuff you want.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To go with.",
                    "label": 0
                },
                {
                    "sent": "Just this is just to remind me and remind you that even if a lot of the examples we're going to be looking at during this course are going to be very small examples with, you know, sort of strings or five, perhaps 10 symbols if you're working in biology, the strings are a little bit longer than that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So well, just remind this music so there are people working in music that I think there are people here from Alicante.",
                    "label": 0
                },
                {
                    "sent": "Yes no.",
                    "label": 0
                },
                {
                    "sent": "OK, well in Alicante there's a group working trying to use these sort of techniques with music.",
                    "label": 0
                },
                {
                    "sent": "It's not the only group but there are other groups working with that.",
                    "label": 0
                },
                {
                    "sent": "So music you can just take them as media files and then you've got an alphabet and from that alphabet you're trying to also learn patterns or to learn grammars or learn machines that somehow represent the melodies.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or represent something?",
                    "label": 0
                },
                {
                    "sent": "Trees, so you've got lots of trees.",
                    "label": 0
                },
                {
                    "sent": "You got family trees.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've got classification tree.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you've got all the trees that come with XML, so one of the big applications today of grammatical inference is to try and use XML data and to try and sort of sort out relationships in XML.",
                    "label": 0
                },
                {
                    "sent": "So obviously in that case you wouldn't be trying to find finite state automata, but things like tree automata, which are a natural even if complex, at least technically complex extension of what you can do on strings.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "XML on all the anger.",
                    "label": 0
                },
                {
                    "sent": "Natural language processing applications.",
                    "label": 0
                },
                {
                    "sent": "So people working in natural language processing have been using trees to represent data or to obtain by tagging the data.",
                    "label": 0
                },
                {
                    "sent": "Trees of this sort.",
                    "label": 0
                },
                {
                    "sent": "So it's clearly being one of the fields in which there's been most of the work I'll be talking.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more things everyday the other day I was contacted by people in Germany or actually interested in using these algorithms for.",
                    "label": 0
                },
                {
                    "sent": "Controlling reactive systems of some sort sort of in model checking applications.",
                    "label": 0
                },
                {
                    "sent": "So becausw these finite state machines you find a bit of everywhere in computer science and in all sorts of different fields.",
                    "label": 0
                },
                {
                    "sent": "People are having the same type of problem, which is.",
                    "label": 0
                },
                {
                    "sent": "I've got some rough data I've got.",
                    "label": 0
                },
                {
                    "sent": "I'd like to, you know, for the moment I'm hand building these finite state machines.",
                    "label": 0
                },
                {
                    "sent": "Could I do it instead of doing it myself by hand?",
                    "label": 0
                },
                {
                    "sent": "Could I get the machine to do it for me?",
                    "label": 0
                },
                {
                    "sent": "And then there's all the variants of questions around this.",
                    "label": 0
                },
                {
                    "sent": "Robots, web services, images etc.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go through an example that's got nothing to do with all what we've seen up to now.",
                    "label": 0
                },
                {
                    "sent": "It's an example in the case of the Agent world, so it's agents, OK.",
                    "label": 0
                },
                {
                    "sent": "Agents is the idea of an ice machine in which, while basically everybody, would be putting their little agent on this machine, and all these agents somehow interact and do something.",
                    "label": 0
                },
                {
                    "sent": "So you can think of a situation like the one where machine would be the New York Stock Exchange, and we all have our little agents there trying to buy and sell shares.",
                    "label": 0
                },
                {
                    "sent": "And somehow we're hoping that these agents are going to buy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And sell shares intelligently.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of cooperative situations, but well, it's not 100% property.",
                    "label": 0
                },
                {
                    "sent": "I'm obviously you're trying to do the best deal possible, so what's my agent going to do when he's going to be on this machine?",
                    "label": 0
                },
                {
                    "sent": "He's going to have to take a decision based on two things.",
                    "label": 0
                },
                {
                    "sent": "First, just totally, mathematically economical gain functions saying right.",
                    "label": 0
                },
                {
                    "sent": "This is how much I am prepared to buy the shares for or to sell the shares for if I can get this price, then I do it, but you should.",
                    "label": 0
                },
                {
                    "sent": "Or they would these agents do something a bit better, which is saying, well, you know, if I'm buying the shares of this price and OK, that's the price I was meant to buy it from.",
                    "label": 0
                },
                {
                    "sent": "But if I can see that my the person who's selling the shares is actually prepared to sell them cheaper, then perhaps I should wait.",
                    "label": 0
                },
                {
                    "sent": "OK, so you introduce the notion of strategy in the sense of saying, well, if I can sort of understand what the strategy of the agent is, then I can get a better deal.",
                    "label": 0
                },
                {
                    "sent": "I can, you know I can buy cheaper or I can sell more expensive so I have to somehow analyze the actions of the other age.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's suppose for a very simple case that the agent is described by a finite state machine just like this one.",
                    "label": 1
                },
                {
                    "sent": "Actually, let's suppose before going to the Stock Exchange that this is you as an agent.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're sitting there and you're listening to me talking, and you're actually going to have two attitudes.",
                    "label": 0
                },
                {
                    "sent": "The one of listening or the one of dozing now after last night I suppose dozing is an option so you can see else and ease indicating what state you're in, what you're going to do.",
                    "label": 0
                },
                {
                    "sent": "Are you going to be listening or you're going to be dozing?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I I'm going to be giving the lecture and I've got two options.",
                    "label": 0
                },
                {
                    "sent": "I can pass equations or I can pass pictures.",
                    "label": 1
                },
                {
                    "sent": "So I do know that if I pass equations after awhile, you probably doze off.",
                    "label": 0
                },
                {
                    "sent": "And if I pass pictures then you probably like pictures.",
                    "label": 0
                },
                {
                    "sent": "Now I'd be able to catch up and understand what's happening, so my strategy I'm going to be passing pictures and equations here and depending what I pass, well, you're going to be one state or another.",
                    "label": 0
                },
                {
                    "sent": "So notice that, for example, if I just keep on passing pictures, well, you just keep on listening.",
                    "label": 0
                },
                {
                    "sent": "If I put in one equation then you still keep on listening.",
                    "label": 0
                },
                {
                    "sent": "One equation is OK, right?",
                    "label": 0
                },
                {
                    "sent": "But if I pass on a second equation then you might reach it, sort of dozing.",
                    "label": 0
                },
                {
                    "sent": "State out of which you will have difficulties in getting out of because it's over there and it's.",
                    "label": 0
                },
                {
                    "sent": "Technology will not be able to.",
                    "label": 0
                },
                {
                    "sent": "I think I'll point.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you get to the right hand state, you'll be dozing off.",
                    "label": 0
                },
                {
                    "sent": "I can get your attention again by following a P and getting onto a listing state and then equation, so I mean, you know there's different strategies.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's me that's directing the game.",
                    "label": 0
                },
                {
                    "sent": "I decide, I know what your strategy is so I can decide what my sequence of slides is going to be in order to reach whatever I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "Perhaps to get you asleep, or perhaps to keep you at least partly away.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a finite state machine, right?",
                    "label": 0
                },
                {
                    "sent": "Just very simple finite state machine and it's also what is used for people doing what they call rational strategies.",
                    "label": 0
                },
                {
                    "sent": "I mean rational strategies or deterministic strategies built in this sense.",
                    "label": 0
                },
                {
                    "sent": "I will probably be able to argue that perhaps this more intelligent strategies than these, but these are actually quite nicely used in a number of cases.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to understand the.",
                    "label": 0
                },
                {
                    "sent": "Rest, or at least well where I'm trying to get you.",
                    "label": 0
                },
                {
                    "sent": "I've gotta tell another story, so it's like a picture.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "It's the prisoners dilemma.",
                    "label": 0
                },
                {
                    "sent": "Hands up.",
                    "label": 0
                },
                {
                    "sent": "Who knows the prisoners dilemma.",
                    "label": 1
                },
                {
                    "sent": "It's about 50% for us, so having to say a word about it right, prisoners dilemma is the the just to get it quickly.",
                    "label": 0
                },
                {
                    "sent": "So nice of parroting to know about anyhow, because you can apply to other cases.",
                    "label": 0
                },
                {
                    "sent": "It's typically from game theory.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you have got the police that have arrested two people during the night because there's been a lot of noise or some things happen and the police is going to interrogate both of these people A&B separately and to each one.",
                    "label": 0
                },
                {
                    "sent": "The deal is the following is saying look.",
                    "label": 0
                },
                {
                    "sent": "I mean we've, you know we've caught you out there in the street and at the same time or more or less the same time there was this this robbery an so we think you did it now I'm not sure that you did it well.",
                    "label": 0
                },
                {
                    "sent": "We think you did.",
                    "label": 0
                },
                {
                    "sent": "So basically we'd like you to plead guilty.",
                    "label": 0
                },
                {
                    "sent": "So what happens if you admit?",
                    "label": 0
                },
                {
                    "sent": "And your colleague is silent.",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to say right where you try to cooperate with justice, so you get out for free and he gets five years prison.",
                    "label": 0
                },
                {
                    "sent": "OK, if it's another way around, if you decided to remain silent, which he admits, then you're going to get five years prison and he's going to be able to get out for free.",
                    "label": 1
                },
                {
                    "sent": "Second option is that basically you both admit in that case where you get three years prison each.",
                    "label": 0
                },
                {
                    "sent": "And the last possibility is that in the very unlikely event that you both decide not to help us and to actually not say anything well, in this case you both get when your prison each because, OK, we can't charge you for the robbery, but you know, we found you at night and it's not very good and you should be so one year prison is.",
                    "label": 0
                },
                {
                    "sent": "So that's the deal.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this deal is actually used an represented in this sort of table, so this is a matrix in which we can read the inputs and outputs and inputs of both players and the gains of both players.",
                    "label": 0
                },
                {
                    "sent": "In this case, the gains means really the losses, so I can see the player A is playing against player B and that in the case where.",
                    "label": 0
                },
                {
                    "sent": "I don't know it's let's say this is the situation where player A is silent and player B admits.",
                    "label": 0
                },
                {
                    "sent": "So in this case we said that in he gets five years prison and he gets out for free.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I was really I should say a word about this just just for the sake of that's got nothing to do with grammatical inference, just for the sake of it.",
                    "label": 0
                },
                {
                    "sent": "If it, if you look at this metrics, you're actually going to find out if you analyze it carefully that the correct strategy consists in.",
                    "label": 0
                },
                {
                    "sent": "Admitting.",
                    "label": 0
                },
                {
                    "sent": "OK, the correct strategy if you just looking at typically expectancy values is admitting because you know if why because?",
                    "label": 0
                },
                {
                    "sent": "Well, let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "Suppose my my opponent here admits.",
                    "label": 0
                },
                {
                    "sent": "Well, if I admit I get three years prison and if I'm silence, I get five years prison.",
                    "label": 0
                },
                {
                    "sent": "So you know, I should admit.",
                    "label": 0
                },
                {
                    "sent": "And if, on the other hand, my opponent is silent, well, if I admit I get 0 here, I get out for free, and if I'm silent I get 20 years prison.",
                    "label": 0
                },
                {
                    "sent": "So in both cases my good strategy is to admit and both players taking the same decision will end up in this stupid situation here, which is obviously not the best common case for the two.",
                    "label": 0
                },
                {
                    "sent": "So this has been analyzed in a number of research papers in a number of things, but it's not very interesting as it is for what we're interested in.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we're interested in is an iterated version, so an iterated version is.",
                    "label": 1
                },
                {
                    "sent": "Suppose that we don't have to spend five years present.",
                    "label": 0
                },
                {
                    "sent": "We just pay 50 rows or something, and so we're going to just keep on playing this game many times against the same adversary and seeing on the very long run, what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "So what does under very long run mean what it means?",
                    "label": 1
                },
                {
                    "sent": "I'm going to actually do a limit of means, so I want to see just the average over a very, very long run, so potentially infinite light run.",
                    "label": 0
                },
                {
                    "sent": "Right of how much am I going to win per move so we should notice also that in this game I'm not very interested about how well my opponent is doing.",
                    "label": 0
                },
                {
                    "sent": "That's not very relevant.",
                    "label": 0
                },
                {
                    "sent": "I'm only interested in my own gay OK, and I'd like to be able to do as well as possible.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this situation.",
                    "label": 0
                },
                {
                    "sent": "Now suppose.",
                    "label": 0
                },
                {
                    "sent": "To relate with what we were saying before, suppose that my adversary is actually using a strategy like the one we saw before.",
                    "label": 0
                },
                {
                    "sent": "OK, so he's playing with a rational strategy.",
                    "label": 0
                },
                {
                    "sent": "So how could I beat him?",
                    "label": 0
                },
                {
                    "sent": "How can I do the best possible against such a rational strategy?",
                    "label": 0
                },
                {
                    "sent": "Right, so first case.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is, I know the strategy.",
                    "label": 1
                },
                {
                    "sent": "For some reason.",
                    "label": 0
                },
                {
                    "sent": "You know I've been doing some spying, so I've got hold of his strategy and I look at this strategy and I think how with this strategy can I do, which is my best moves, given the fact that his strategy is whatever it is.",
                    "label": 0
                },
                {
                    "sent": "Well, that's a little bit of graph theory.",
                    "label": 0
                },
                {
                    "sent": "It's not very difficult.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you do is something like this.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that this is my adversaries strategy.",
                    "label": 0
                },
                {
                    "sent": "So remember that what he's going to do is what's inside the states.",
                    "label": 0
                },
                {
                    "sent": "So basically, his strategy consists in first, he's going to admit you know he doesn't trust me at all.",
                    "label": 0
                },
                {
                    "sent": "He starts by admitting and then if I'm silent, he decides to be silent.",
                    "label": 0
                },
                {
                    "sent": "And if I'm silent again, he decides to be silent again.",
                    "label": 0
                },
                {
                    "sent": "And if I keep on being silent, he's going to be silent, which is, you know, the good case where we're both just getting or paying 1.",
                    "label": 0
                },
                {
                    "sent": "Oh, rule.",
                    "label": 0
                },
                {
                    "sent": "If I had met once, while you know his shows, little of mistrust, but he says, look, you know I'll forgive you if it's just a one off, so he'll be silent.",
                    "label": 0
                },
                {
                    "sent": "But of course, if I admit a second time, then we get into this bad state where he's not going to trust me and he's going to have to admit.",
                    "label": 0
                },
                {
                    "sent": "So what should I do in this case?",
                    "label": 0
                },
                {
                    "sent": "What's my best strategy?",
                    "label": 0
                },
                {
                    "sent": "Will I look at my game function?",
                    "label": 0
                },
                {
                    "sent": "I put some weights over the edges just to indicate how much I'd be winning in each one of these situations.",
                    "label": 0
                },
                {
                    "sent": "Notice that winning correspond somehow to losing and I try and find the cycle with.",
                    "label": 0
                },
                {
                    "sent": "The minimum mean wait, so here I've got a cycle where if we got into this cyclic situation then I'd be losing one horu every move.",
                    "label": 0
                },
                {
                    "sent": "But if I manage to get into this cycle here then I'd be losing one or oh zero 1 / 0 and so on.",
                    "label": 0
                },
                {
                    "sent": "So I am interested.",
                    "label": 0
                },
                {
                    "sent": "I am interested in reaching circular situation in which I am going to be paying the least possible.",
                    "label": 0
                },
                {
                    "sent": "In terms of mean, so this is the one where I want to get you.",
                    "label": 0
                },
                {
                    "sent": "And how do I get there when I get there by choosing the cheapest path?",
                    "label": 0
                },
                {
                    "sent": "In this case, the cheapest path consists in actually doing this.",
                    "label": 0
                },
                {
                    "sent": "So this is just some very simple maths and graph theory and it's just going to say that this is how I actually reach the best for me.",
                    "label": 0
                },
                {
                    "sent": "So I mean it makes sense.",
                    "label": 0
                },
                {
                    "sent": "I mean the best strategy for me consists in actually starting here, being a silent once an accepting to pay full penalty and then getting into this lovely cycle of saying, OK, you're trusting me.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm silent.",
                    "label": 0
                },
                {
                    "sent": "And now I admit once.",
                    "label": 0
                },
                {
                    "sent": "You mistrust OK, I'm silent again admits silent admits.",
                    "label": 0
                },
                {
                    "sent": "Silence serves the best situation for me.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's again will just.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All theory.",
                    "label": 0
                },
                {
                    "sent": "The question is.",
                    "label": 0
                },
                {
                    "sent": "This is what we've just done.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question is, suppose I'm not given the strategy.",
                    "label": 0
                },
                {
                    "sent": "Can I learn it?",
                    "label": 0
                },
                {
                    "sent": "So I had to reach learning.",
                    "label": 0
                },
                {
                    "sent": "So what's learning about here?",
                    "label": 0
                },
                {
                    "sent": "It's saying, OK, the strategy isn't there.",
                    "label": 0
                },
                {
                    "sent": "What I've got is the opponent.",
                    "label": 0
                },
                {
                    "sent": "Perhaps I can play a game against this opponent and from the game I'm playing against this opponent I can reach an idea of what the strategy is.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is what we're going to do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll talk about the question of saying, well, perhaps I should be playing the game and learning the strategy at the same time later.",
                    "label": 0
                },
                {
                    "sent": "For the moment, I want to suppose that I'm given a game, so I'm giving this game, which is here my moves and his moves, and from this game that has been played, I'm going to try and reach the strategy.",
                    "label": 0
                },
                {
                    "sent": "So how do I interpret this?",
                    "label": 0
                },
                {
                    "sent": "This game, which is here?",
                    "label": 0
                },
                {
                    "sent": "Which are the columns of moves?",
                    "label": 0
                },
                {
                    "sent": "First move for him, first move for me.",
                    "label": 0
                },
                {
                    "sent": "First move for him, the second for him, second for me and so onwards.",
                    "label": 0
                },
                {
                    "sent": "How can I interpret this?",
                    "label": 0
                },
                {
                    "sent": "Why can you interpret it as strings?",
                    "label": 0
                },
                {
                    "sent": "Because the machine we saw earlier is deterministic.",
                    "label": 0
                },
                {
                    "sent": "It means that in every game I'm going to play, his first move is always going to be to admit, but also in any game I play.",
                    "label": 1
                },
                {
                    "sent": "If I start by playing ASA, then his move is going to be a.",
                    "label": 1
                },
                {
                    "sent": "That's just because the machine is deterministic.",
                    "label": 0
                },
                {
                    "sent": "He's not making any sort of random choices or nondeterministic choices or probabilistic choices.",
                    "label": 0
                },
                {
                    "sent": "He's just, you know, applying a strategy so I can get this data and transform it into strings as it's on the right.",
                    "label": 0
                },
                {
                    "sent": "So the logic of the algorithm is simple is I'm going to have got these strings and I'm going to try and create a machine that can pass.",
                    "label": 0
                },
                {
                    "sent": "Past means to run the different strings and to try and reach conclusions that are consistent with what I can see, so the algorithm isn't a given algorithm in grammatical inference.",
                    "label": 0
                },
                {
                    "sent": "It's sort of uses many of the ideas that you can find in the literature an since for that reason, why is it also addresses a number of the questions people are interested in?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first thing we saw was that the algorithm starts by saying after having seen nothing.",
                    "label": 0
                },
                {
                    "sent": "So my first move says my opponent is to do an A and then I'm going to play in a.",
                    "label": 0
                },
                {
                    "sent": "So that means that we know that the automaton looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a first state, and this first state is labeled with an A.",
                    "label": 1
                },
                {
                    "sent": "Can't really see it when there is an A in the middle of that.",
                    "label": 0
                },
                {
                    "sent": "And what then?",
                    "label": 0
                },
                {
                    "sent": "We're left with is we're going to have to deal with this dangling edge here or this dangling transition.",
                    "label": 1
                },
                {
                    "sent": "It's as it's called in the case of an automaton which we're going to have to do something with, but for the moment, since I haven't tried to see what's coming next, this is the situation in which I'm in.",
                    "label": 0
                },
                {
                    "sent": "So let's try.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And deal with this a so.",
                    "label": 1
                },
                {
                    "sent": "What we're going to try and do is we're building this transition here.",
                    "label": 0
                },
                {
                    "sent": "So why am I building this transition here, so I'm building upon the hypothesis I had on the move before, and I'm building the transition here, basically because I had two choices.",
                    "label": 0
                },
                {
                    "sent": "One choice was doing this, and the other choice would have been to build an entirely new state and say, well, you know, I go to this new state within a.",
                    "label": 0
                },
                {
                    "sent": "So the reason for not doing that is typical greediness reasons.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've got to have some sort of a strategy.",
                    "label": 0
                },
                {
                    "sent": "Sort of trying to get some way in in a greedy way, so go for the go for the easiest.",
                    "label": 0
                },
                {
                    "sent": "The second argument is going to be the one of size.",
                    "label": 0
                },
                {
                    "sent": "The idea is that, like in any other machine learning applications, we think that going for some simple solution is better than going for some complex solution, meaning that I've got in mind that I want to find a small automaton rather than a large automaton.",
                    "label": 0
                },
                {
                    "sent": "Now this makes sense under a lot of reasons, it makes sense under reasons dealing with something that's called Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "Is anyone been talking bout Occam's razor these days?",
                    "label": 0
                },
                {
                    "sent": "Which is, you know if there's a variety of explanation.",
                    "label": 0
                },
                {
                    "sent": "Choose the simplest.",
                    "label": 0
                },
                {
                    "sent": "This is an explanation that comes from the 12th or 13th century, and it's nice for philosophical reasons, but it's actually good for mathematical reasons.",
                    "label": 0
                },
                {
                    "sent": "It makes sense you can actually prove that an orcam algorithm, one that would be trying to find a small solution as small.",
                    "label": 0
                },
                {
                    "sent": "Classifier is actually going to be better, is going to be better than one that is probably trying to find something large.",
                    "label": 0
                },
                {
                    "sent": "It's explains also why in machine learning where putting so much attention to pruning problems to simplifying, there's a lot of issues related with this.",
                    "label": 0
                },
                {
                    "sent": "So anyhow, logically we try and do this, and since this work because it's actually consistent with what I can see in the datas, I keep it.",
                    "label": 0
                },
                {
                    "sent": "My next symbol is an S, so I'm now left with a dangling S here, which I've got to do something about.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I do with it.",
                    "label": 0
                },
                {
                    "sent": "Well, logically I try and do the same.",
                    "label": 0
                },
                {
                    "sent": "You know what worked once might work a second time.",
                    "label": 0
                },
                {
                    "sent": "I again try and just do a loop on the state saying when I come back to the same place with an S. But here I reach something that is inconsistent with the data seen so far.",
                    "label": 0
                },
                {
                    "sent": "I can pass.",
                    "label": 0
                },
                {
                    "sent": "I do a followed by an S and the airport says the automaton is an A.",
                    "label": 0
                },
                {
                    "sent": "But what my data is saying is that the output is an S. Some means this is wrong, so since this is wrong, well I've got to do something different.",
                    "label": 0
                },
                {
                    "sent": "So what do I do when I undo the last thing I've done?",
                    "label": 0
                },
                {
                    "sent": "Which is this S and I try something different.",
                    "label": 0
                },
                {
                    "sent": "The only possibility of something different here is creating this new state.",
                    "label": 0
                },
                {
                    "sent": "So we may argue and say, well, you know, perhaps if we'd actually done something with the A the first move, then I could do something different now.",
                    "label": 0
                },
                {
                    "sent": "Which is true.",
                    "label": 0
                },
                {
                    "sent": "The only reason for not undoing everything is economically it's on the basis of combinatorics.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can't just try everything out because the number of possible automata grows in an exponential way, so you've got to take some sort of decision greedy decision, whatever had done before.",
                    "label": 0
                },
                {
                    "sent": "I keep and it's just the last decision that I that I modify.",
                    "label": 0
                },
                {
                    "sent": "So now what I have to do when I could pass all this so I can pass a S with the A that is here I cannot pass, meaning I can't run it through my automaton.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to do some.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About that so the fourth decision I take is to bring back this a back to this state here.",
                    "label": 0
                },
                {
                    "sent": "So why did I do that again?",
                    "label": 0
                },
                {
                    "sent": "I had three possibilities.",
                    "label": 0
                },
                {
                    "sent": "One of them was doing this, the second one would have been to actually do a loop with a here and the third one to create a state for greedy reason.",
                    "label": 0
                },
                {
                    "sent": "I tried first to look for the initial state, then the second state.",
                    "label": 0
                },
                {
                    "sent": "I created an if not, I would have looked at the Thursday since the first one actually works.",
                    "label": 0
                },
                {
                    "sent": "Everything is OK. Notice one up over one thing that is nice here is that actually by doing this I've been able to pass not just one string.",
                    "label": 0
                },
                {
                    "sent": "But three extra moves, which means, which is something that's actually going to happen in practice.",
                    "label": 0
                },
                {
                    "sent": "When we write this sort of algorithm, is that we're going to have an enormous amount of data, but my automaton, little by little, rule by rule, is actually going to get rid of many packets of data at the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't have one rule per piece of data, but it sounds reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so up to here it's alright, and now my.",
                    "label": 0
                },
                {
                    "sent": "Next rule, the game was saying 1234566 symbol is an S.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do I do with it?",
                    "label": 0
                },
                {
                    "sent": "So well, same argument as before.",
                    "label": 0
                },
                {
                    "sent": "I try and bring the S here, but this doesn't work, so this doesn't work because of this rule here where we can see that IIS aass should buy the says the data allows the opponent to output an S, But my automaton, my theory I am building, says it's may, so that doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So the second possibility was doing a loop on the S state.",
                    "label": 0
                },
                {
                    "sent": "Right, and that doesn't work either.",
                    "label": 0
                },
                {
                    "sent": "But you've actually got go further in the data to notice that that doesn't work, so if that doesn't work, it means why.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It means that we've actually got to create a more complex model with a third state, so it's just again that last transition with an S which is just passed on to a new state S. OK etc etc.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just go and you've got this.",
                    "label": 0
                },
                {
                    "sent": "If you want to follow in the slide.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at the end of these decisions were left with.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This may.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In here, which is the result of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this was a deterministic even if, let's say freely written algorithm where we're just taking the data little by little, trying to see what rule we make and at the end of the day we reach.",
                    "label": 0
                },
                {
                    "sent": "We reached this this machine.",
                    "label": 0
                },
                {
                    "sent": "What should I say about this machine?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, it is the machine we had in mind at the beginning.",
                    "label": 0
                },
                {
                    "sent": "If you remember, this was the one that we were hoping to have.",
                    "label": 0
                },
                {
                    "sent": "Obviously, in order to get this, I chose very nicely the example.",
                    "label": 0
                },
                {
                    "sent": "In practice it doesn't work as well in practice for us to be able to have an example game that would allow us to bring this well clearly you need a game that is actually going to use all the information inside here.",
                    "label": 0
                },
                {
                    "sent": "If you've only got a game, this is only using part of the graph, then you're not going to be able to learn.",
                    "label": 0
                },
                {
                    "sent": "Let's say this transition here if you're actually never using it.",
                    "label": 0
                },
                {
                    "sent": "So you need to invent a game that is actually using a bit of everything, and that allows you in a way to take the right decisions.",
                    "label": 0
                },
                {
                    "sent": "To conclude on the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This two things.",
                    "label": 0
                },
                {
                    "sent": "First one is that.",
                    "label": 0
                },
                {
                    "sent": "This sounded easy.",
                    "label": 0
                },
                {
                    "sent": "I was just sitting there.",
                    "label": 0
                },
                {
                    "sent": "Was looking at a game, you know I was like seeing how he was losing the money against their opponent and once he lost the money then I would actually come in and say OK.",
                    "label": 0
                },
                {
                    "sent": "I know the strategy I can play in practice.",
                    "label": 0
                },
                {
                    "sent": "Obviously you probably have to pay for that data and you find yourself in an expert in an active learning setting.",
                    "label": 0
                },
                {
                    "sent": "You also find yourself in a curious setting with these sort of game.",
                    "label": 0
                },
                {
                    "sent": "Where suppose I have learned the automaton we saw five minutes ago.",
                    "label": 0
                },
                {
                    "sent": "I'm satisfied with it.",
                    "label": 0
                },
                {
                    "sent": "Because thanks to that I managed to find a way to get into this nice loop.",
                    "label": 0
                },
                {
                    "sent": "You know where I was actually winning, or let's say only losing 50 while one oh row every two moves.",
                    "label": 0
                },
                {
                    "sent": "But who tells me that if I done a bit more exploring and tried some more complex series of moves that we wouldn't find ourselves in this situation where there's a much better possibility?",
                    "label": 0
                },
                {
                    "sent": "I never can tell with this sort of algorithm that I have learned the only thing I can tell, right is that OK, I've reached something for which now whatever I'm doing is consistent with what's happening, but perhaps there's something completely knew that I should go and explore, so this is what they call exploration versus exploitation.",
                    "label": 0
                },
                {
                    "sent": "Of dilemna, and it's a complicated question, including in bandit problems.",
                    "label": 0
                },
                {
                    "sent": "I think you've also had some bandit problems the other day.",
                    "label": 0
                },
                {
                    "sent": "Yeah you can.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That sort of question.",
                    "label": 0
                },
                {
                    "sent": "Second question which is open is what happens if the machine, instead of being deterministic, is what we've seen is actually probabilistic at, which seems a little bit fairer.",
                    "label": 0
                },
                {
                    "sent": "So you know the output of the opponent isn't always yes or always know.",
                    "label": 0
                },
                {
                    "sent": "It's mostly yes or mostly know this sort of machine actually is related to things called Markov decision process is which exists in a number of fields.",
                    "label": 0
                },
                {
                    "sent": "Robotics being probably the most important one.",
                    "label": 0
                },
                {
                    "sent": "And I think in this years I CML, there's been the 1st paper attempting to try and learn this sort of machines.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess just for the just for the sake of knowing for this competition of this sorry this game of playing the iterated version of the prisoners dilemma has actually taken place.",
                    "label": 0
                },
                {
                    "sent": "There's been a competition where people sent their algorithms and played win against the other, and curiously enough, the one that one was using this rational strategy here.",
                    "label": 0
                },
                {
                    "sent": "OK, this is tit for tat which is just.",
                    "label": 1
                },
                {
                    "sent": "You know if you admit, I'll admit, if you're silent, I'll be silent.",
                    "label": 0
                },
                {
                    "sent": "So if this is the best strategy overall at this game.",
                    "label": 0
                },
                {
                    "sent": "And you can't beat it.",
                    "label": 0
                },
                {
                    "sent": "The only way to do well against it is actually being silent, right?",
                    "label": 0
                },
                {
                    "sent": "Because if not, he's immediately going to retaliate.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've seen through this this example what the algorithms of the algorithmics of the problem are.",
                    "label": 0
                },
                {
                    "sent": "Let's now and see what what.",
                    "label": 0
                },
                {
                    "sent": "Let's say about the results.",
                    "label": 0
                },
                {
                    "sent": "So suppose somehow I've written an algorithm like the one we've seen 2 seconds ago, and.",
                    "label": 0
                },
                {
                    "sent": "Once I've written this algorithm or my done, what can I say about it?",
                    "label": 0
                },
                {
                    "sent": "Can I?",
                    "label": 0
                },
                {
                    "sent": "Can I say yes, this is a good algorithm?",
                    "label": 0
                },
                {
                    "sent": "Can I say this algorithm is actually going to allow me to always learn something right?",
                    "label": 0
                },
                {
                    "sent": "So first question is, why bother?",
                    "label": 1
                },
                {
                    "sent": "You know, so this is what, let's say machine learning in the 80s and 90s was about.",
                    "label": 0
                },
                {
                    "sent": "You found some really nice algorithms and it seemed to work nicely.",
                    "label": 0
                },
                {
                    "sent": "And this was cool, and it was nice.",
                    "label": 0
                },
                {
                    "sent": "It was the good old days before you had to do maths in the in the field.",
                    "label": 0
                },
                {
                    "sent": "So it's so well that question today doesn't doesn't happen.",
                    "label": 0
                },
                {
                    "sent": "You actually do have to give an answer in most cases.",
                    "label": 0
                },
                {
                    "sent": "And so then, why should we do something whenever researchers and machine learning or not?",
                    "label": 1
                },
                {
                    "sent": "But that's not entirely true.",
                    "label": 0
                },
                {
                    "sent": "OK, so at least you might have the feeling that other researchers are not having to prove that their algorithm is going to converge in some sense.",
                    "label": 0
                },
                {
                    "sent": "So actually true.",
                    "label": 0
                },
                {
                    "sent": "Usually the theory is just done in a very different way.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to answer this, let let let's have a three motivating questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is.",
                    "label": 0
                },
                {
                    "sent": "Suppose that our task was not to learn algorithms for.",
                    "label": 0
                },
                {
                    "sent": "Learning automata, but algorithms for generating random numbers so you know you've come out with a random number generator, and this random jumman random number generator generates 17.",
                    "label": 1
                },
                {
                    "sent": "So you can say you know this is good.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's 17 years this is random, or this sequence.",
                    "label": 0
                },
                {
                    "sent": "Here is a random sequence.",
                    "label": 1
                },
                {
                    "sent": "But somehow what I'm trying to express here is the fact that you have built a learning algorithm and you look at the result and then you're looking at the results.",
                    "label": 0
                },
                {
                    "sent": "And you say, yeah, this is good.",
                    "label": 0
                },
                {
                    "sent": "You know I like the result.",
                    "label": 0
                },
                {
                    "sent": "The result isn't really telling us much about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The result is saying something about the result.",
                    "label": 0
                },
                {
                    "sent": "I can't say that 17 is random or not.",
                    "label": 0
                },
                {
                    "sent": "I mean I can say something about the algorithm that is produced 17 but not about the number 17 itself.",
                    "label": 0
                },
                {
                    "sent": "And same for the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean I can look at the learning algorithms result and say you know this has learned this DFA.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think this is a cool DFL thing.",
                    "label": 0
                },
                {
                    "sent": "It's a nice one, but it's not going to really be telling me if I'm learning or if I'm not.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second question is to say.",
                    "label": 0
                },
                {
                    "sent": "In the case of languages, learning is an ongoing process, so that's clear for the kids, but it's clear for us.",
                    "label": 0
                },
                {
                    "sent": "We've never learned we're always learning.",
                    "label": 0
                },
                {
                    "sent": "There's no moment where you can really say.",
                    "label": 0
                },
                {
                    "sent": "OK, I've learned a language.",
                    "label": 0
                },
                {
                    "sent": "Right, so learning is a process in itself.",
                    "label": 0
                },
                {
                    "sent": "It's you know it's the process of learning that we're actually trying to discuss.",
                    "label": 0
                },
                {
                    "sent": "It's not the result of the learning itself.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the third question related to this one is that I don't even think that the statement I have learned makes sense.",
                    "label": 0
                },
                {
                    "sent": "OK, or the algorithm is learned.",
                    "label": 0
                },
                {
                    "sent": "I can say something.",
                    "label": 0
                },
                {
                    "sent": "Intelligence about I am learning or my algorithm is learning.",
                    "label": 0
                },
                {
                    "sent": "But how can I say that this algorithm has learned something?",
                    "label": 0
                },
                {
                    "sent": "All I can say is that my algorithm has perhaps solved some combinatorial problem or some optimization problem.",
                    "label": 0
                },
                {
                    "sent": "I hate hasn't learned right?",
                    "label": 0
                },
                {
                    "sent": "Learning something much more complex and you never.",
                    "label": 0
                },
                {
                    "sent": "You can't say it has learned unless you're really saying, you know I had something hidden here and it has found what was hidden in which case.",
                    "label": 0
                },
                {
                    "sent": "Well, it's then something a little bit different, which we call identify.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "OK, So what usually is called having learned is just to say that well, perhaps whatever grammar automaton or any other function that we're trying to find, where is the smallest or the best for a given score or has solved some sort of a combinatorial optimization or comedy oratorial characterization issue?",
                    "label": 0
                },
                {
                    "sent": "So in other cases we also say that perhaps the learning algorithm has converged, converged in the sense of, say, EMEM.",
                    "label": 0
                },
                {
                    "sent": "You run the algorithm at one moment, it can't do any better, so you know it's reached some sort of a local maximum or local minimum depending which way around you're looking at the question, but you you know you haven't learned you.",
                    "label": 0
                },
                {
                    "sent": "All you've done is you've solved something.",
                    "label": 0
                },
                {
                    "sent": "Some nice problem instead, which is good.",
                    "label": 0
                },
                {
                    "sent": "I mean, which is what we're usually doing.",
                    "label": 0
                },
                {
                    "sent": "We're usually trying to transform or learning problem into an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Say it slow.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You could find it that way.",
                    "label": 0
                },
                {
                    "sent": "So there are always some very nice reasons, very good mathematical reasons related to our Cam.",
                    "label": 0
                },
                {
                    "sent": "We talked about compression issues.",
                    "label": 0
                },
                {
                    "sent": "We know that an algorithm that compresses the data is an algorithm that is learning.",
                    "label": 0
                },
                {
                    "sent": "We also know that an algorithm that achieves some MDL result, MDL is minimum description length, so that is also theories that helps us to understand the learning that learning has been achieved.",
                    "label": 1
                },
                {
                    "sent": "Kolmogorov complexity is also in there.",
                    "label": 0
                },
                {
                    "sent": "So all these are mathematical Arg.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's why we're interested in convergence.",
                    "label": 0
                },
                {
                    "sent": "We're interested in convergence to be able to say something about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Say, OK, my algorithm has too.",
                    "label": 0
                },
                {
                    "sent": "It's not just good enough that it returns the DFA.",
                    "label": 0
                },
                {
                    "sent": "We're actually going to have to say, well, why is it getting somewhere?",
                    "label": 0
                },
                {
                    "sent": "Why is it going in a good direction?",
                    "label": 0
                },
                {
                    "sent": "Why should I trust it?",
                    "label": 0
                },
                {
                    "sent": "So what are the sort of convergence criteria were interested in?",
                    "label": 0
                },
                {
                    "sent": "Well, we'd like to be able to say that I've I've learned and thanks to this learning issue, I will be able to predict so I can predict, for example, the next string somebody gives me.",
                    "label": 1
                },
                {
                    "sent": "I can label saying, well, this is in the language of.",
                    "label": 0
                },
                {
                    "sent": "This is not or, like earlier on in the game saying, well, his next move is going to be to admit or to be silent, so I know that if I can predict in a certain way, that's a way of saying, well, that I have learned.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if I can.",
                    "label": 0
                },
                {
                    "sent": "That's some money on the outcome of this result.",
                    "label": 0
                },
                {
                    "sent": "Betting money means that I've got probably a distribution of probabilities somewhere.",
                    "label": 0
                },
                {
                    "sent": "If not, I wouldn't be doing it then, that's even better.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that's nice if I said if I had a distribution of probability, is there some material, some probabilistic world I'd be able to say?",
                    "label": 1
                },
                {
                    "sent": "Well, I can predict the outcome, so that's very strong, and in practice it's not always that easy.",
                    "label": 0
                },
                {
                    "sent": "I don't always have this distribution of probabilities, so another way of saying things is saying, look, I'm learning and basically given the data I was given.",
                    "label": 0
                },
                {
                    "sent": "I've done as well as possible.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, saying.",
                    "label": 0
                },
                {
                    "sent": "Well, if there was something to learn and I haven't learned it, it was because the data was insufficient.",
                    "label": 0
                },
                {
                    "sent": "It's a bit like the example I gave earlier, where I said, you know, if one edge hadn't been used during the learning data, there's no way I can learn it, so I can't really talk about learning if I've got insufficient data.",
                    "label": 0
                },
                {
                    "sent": "This may seem very weak hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The one of saying blamed the data, not the algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's actually much stronger than one things.",
                    "label": 0
                },
                {
                    "sent": "It's really means sometimes you can't just blame the data.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you've got a bad algorithm and whatever the data, it's actually never going to learn something that is supposed to learn, or something that you're claiming it should be learning.",
                    "label": 0
                },
                {
                    "sent": "Will give an example.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the example.",
                    "label": 0
                },
                {
                    "sent": "This is a moment.",
                    "label": 0
                },
                {
                    "sent": "So suppose I've got an algorithm that is actually trying to learn context free grammars, so that's what it's supposed to do it we designed to learn context free grammars so income, some data and from this data my algorithm is returning a context free grammar.",
                    "label": 0
                },
                {
                    "sent": "But if my algorithm contains what we call hidden bias, so hidden biases that fact that in fact the grammar, because the algorithm because of its design is only capable of learning biased grammars, grammars that can only do special things.",
                    "label": 0
                },
                {
                    "sent": "For example, regular grammars are the ones that are equivalent to finite state automata.",
                    "label": 0
                },
                {
                    "sent": "In this case, I'm claiming that my algorithm is capable of learning more or less anything in this class, which is always biased towards the same type of grammars.",
                    "label": 0
                },
                {
                    "sent": "Now then, if I find myself in the situation I had two seconds before let's go.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back to the slide.",
                    "label": 0
                },
                {
                    "sent": "Blame the data, not the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I that's not going to be the case because I mean I should always be able to converge towards the correct grammar.",
                    "label": 0
                },
                {
                    "sent": "It's just a question of time or it's just a question of data, so it's not exactly.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So two settings the non probabilistic setting and the probabilistic setting non probabilistic setting.",
                    "label": 0
                },
                {
                    "sent": "Is called identification in the limit, so identification in the limit is a famous setting which is really used in a field called inductive inference.",
                    "label": 0
                },
                {
                    "sent": "Which is related to machine learning, which is a field that goes back to the 50s.",
                    "label": 0
                },
                {
                    "sent": "And it's the idea that it's the idea of a field way.",
                    "label": 0
                },
                {
                    "sent": "Basically, you're given numbers you know you're given #2, then #3 the number 5 and #7, and you know the game.",
                    "label": 0
                },
                {
                    "sent": "Are you trying to guess which is the next number?",
                    "label": 0
                },
                {
                    "sent": "So this is inductive inference.",
                    "label": 0
                },
                {
                    "sent": "So, edification in the limit is 1, we're going to talk about and of course, because we're computer scientists were just trying to put some resource bounds, saying, well, we'd like to be able to identify given not too much data.",
                    "label": 0
                },
                {
                    "sent": "And then active learning, which we perhaps talk about after.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the chief papers are from this person called Mark Gold E Mark Gold.",
                    "label": 0
                },
                {
                    "sent": "I've never known what the E stands for.",
                    "label": 0
                },
                {
                    "sent": "E Mark goldran.",
                    "label": 0
                },
                {
                    "sent": "So in the 60s and 70s he came out with these ideas.",
                    "label": 1
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see what the ideas are.",
                    "label": 0
                },
                {
                    "sent": "General idea is like the online learning.",
                    "label": 0
                },
                {
                    "sent": "The data is presented.",
                    "label": 0
                },
                {
                    "sent": "One item of data at the time, and after each item of data we have as a learner to produce a hypothesis.",
                    "label": 1
                },
                {
                    "sent": "So pretty sure hypothesis then the new item of data arrives.",
                    "label": 0
                },
                {
                    "sent": "We may have to update this apophysis or we may keep it and so on.",
                    "label": 0
                },
                {
                    "sent": "With now identification, the limit says the following.",
                    "label": 0
                },
                {
                    "sent": "It says that at one moment or another I'm going to stop changing my opinion.",
                    "label": 0
                },
                {
                    "sent": "OK, my algorithm can't go on forever and ever modifying, its modifying its epatha sis.",
                    "label": 0
                },
                {
                    "sent": "So I would say that I identify in the limit when there is a limit point.",
                    "label": 1
                },
                {
                    "sent": "There is a moment in which my algorithm is going to identify correctly.",
                    "label": 0
                },
                {
                    "sent": "That's your name.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example, so this is just with numbers and not with grammars and automata.",
                    "label": 1
                },
                {
                    "sent": "The first I item of data arrives is number 2.",
                    "label": 1
                },
                {
                    "sent": "My hypothesis saying, OK, well, this is just a single language.",
                    "label": 0
                },
                {
                    "sent": "I reckon it's just a set containing one element #2.",
                    "label": 0
                },
                {
                    "sent": "So #3 arrives.",
                    "label": 0
                },
                {
                    "sent": "So I've got #2 and #3 where clearly my previous hypothesis is bad.",
                    "label": 0
                },
                {
                    "sent": "So I have to change it.",
                    "label": 0
                },
                {
                    "sent": "So I change it to double time two and three.",
                    "label": 0
                },
                {
                    "sent": "My next example arrive and it's #5, so here I could again just keep on forever and ever saying 235 and every time a new example arrives I just added to my set, but clearly that strategy is not going to allow me to identify in the limit because I'll be changing apophysis all the time if my set is an infinite set.",
                    "label": 0
                },
                {
                    "sent": "So I've decided my algorithm is going to actually come out with something braver, which is going to say OK, 235.",
                    "label": 0
                },
                {
                    "sent": "Those are three fever Nachi numbers, so I believe 2, three and five other fever Nachi numbers.",
                    "label": 0
                },
                {
                    "sent": "So that's my heavy current hypostasis and arrives #7.",
                    "label": 0
                },
                {
                    "sent": "#7 is not.",
                    "label": 0
                },
                {
                    "sent": "If you weren't actually number, so I have to change my hypothesis and I have now thought 2357 or how this looks better.",
                    "label": 0
                },
                {
                    "sent": "I think it's the prime numbers.",
                    "label": 0
                },
                {
                    "sent": "So 2357 Next Element is 11.",
                    "label": 0
                },
                {
                    "sent": "I don't have to update.",
                    "label": 0
                },
                {
                    "sent": "Hundred 303 is prime don't update.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this would be an algorithm, so in that case obviously you've notice my example had to stop after six or seven elements, not just because I didn't have any enough size on my slide or that it will be boring.",
                    "label": 0
                },
                {
                    "sent": "But because there are an infinite number of possible examples, so it really means that even if I had kept on doing this example for about an hour and a half, just passing numbers at no moment can I really be sure that I am not going to have to change apophysis just the next move.",
                    "label": 0
                },
                {
                    "sent": "So this is a mix, a little bit complicated, all I know.",
                    "label": 0
                },
                {
                    "sent": "If my algorithm does identify in the limit, is that a one moment or another I will stick to the same epatha sis, even if I can't know when, or even if when I'm actually running the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I don't even know if this is the correct if offices or not.",
                    "label": 0
                },
                {
                    "sent": "It's a bit like in my game earlier on, all I knew through my algorithm was that I would find eventually the good solution, but in how long when I don't know?",
                    "label": 0
                },
                {
                    "sent": "OK, so to be able to learn, we said there was there was a presentation of data, so presentation is just.",
                    "label": 0
                },
                {
                    "sent": "This is just a function presenting data, so data is anything it could be numbers or it could be strings, or it could be examples, counterexamples, or if you're talking about strings, you could be presenting prefixes or subsequences, but some information that is arriving one item at the time.",
                    "label": 0
                },
                {
                    "sent": "Obviously, when you've got a presentation of the language, well, somehow this presentation is related to a language, so we've written a function called yields which associate's to a given presentation.",
                    "label": 0
                },
                {
                    "sent": "The language which is the one we supposed to try and learn.",
                    "label": 0
                },
                {
                    "sent": "And obviously, while the order in which we present the elements doesn't matter, if at the end of days two presentations present the same data, then logically it corresponds to the same language, and that's where the slide says.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So typically one is the text presentation, protects presentation of the language, is just a function of fine like the one we had two seconds ago, such that at the end of the day and then of today means in an infinite number of steps.",
                    "label": 1
                },
                {
                    "sent": "While we have seen all the strings of the language, if you want to be able to learn a language, you're going to have to suppose that all the strings will eventually appear.",
                    "label": 1
                },
                {
                    "sent": "So if I'm trying to learn a language and only some strings in a biased way are going to appear by quite logically, I'm only going to hope to learn for the.",
                    "label": 0
                },
                {
                    "sent": "Strings of the language corresponding to the bias strings, not to all the strings.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And inform presentation is not just the strings of the language, it's label string, so it's a string with a one thing or a plus saying yes.",
                    "label": 0
                },
                {
                    "sent": "The string is in the language or a string with a minus indicating that no, this string is not in the language.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So typically presentations.",
                    "label": 0
                },
                {
                    "sent": "Let's say the language is a MVN.",
                    "label": 0
                },
                {
                    "sent": "The typical language used to prove that this language is not regular.",
                    "label": 0
                },
                {
                    "sent": "So, well, you could have a legal presentation from text which would correspond well, all to all the pairs A2B2A7B7A4B four.",
                    "label": 1
                },
                {
                    "sent": "You can have repetitions in our presentation.",
                    "label": 0
                },
                {
                    "sent": "And an illegal presentation would be one way you would only get the same string repeated up to Infinity, so you wouldn't have the whole language which would appear.",
                    "label": 0
                },
                {
                    "sent": "Legal presentation from an informant.",
                    "label": 1
                },
                {
                    "sent": "Well, it's pairs like this.",
                    "label": 1
                },
                {
                    "sent": "ABAB is a negative examples.",
                    "label": 0
                },
                {
                    "sent": "A2B2 is a positive example, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "No, this is not the learning function.",
                    "label": 0
                },
                {
                    "sent": "This is a this is a mistake.",
                    "label": 0
                },
                {
                    "sent": "I don't know why I've put it there.",
                    "label": 0
                },
                {
                    "sent": "OK, so given the presentation, the end first elements of the presentation or.",
                    "label": 0
                },
                {
                    "sent": "The N 1st elements of the presentation we just denote by fight some N. And we've got a learning algorithm which does what way just takes.",
                    "label": 0
                },
                {
                    "sent": "Then first elements of a presentation, and given those N 1st elements, bills or returns or grammar.",
                    "label": 0
                },
                {
                    "sent": "And given a grammar, of course we've got a naming function and not a learning function naming function which is LG, which is just the language generated, recognized, represented by the grammar.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't get this one, so identification the limit is the little picture which is here.",
                    "label": 0
                },
                {
                    "sent": "Let's give the whole picture.",
                    "label": 0
                },
                {
                    "sent": "OK, so the whole picture here is just saying that there is a little class of languages where we're going to get information presentations which are going to come from this presentations of this class of languages and we're trying to work on grammars, each grammar corresponding to one language of their.",
                    "label": 0
                },
                {
                    "sent": "I mean clearly they can be equivalent guar grammars for the same language, so that's her problem also.",
                    "label": 0
                },
                {
                    "sent": "And we've got a learning algorithm which is trying to, given the presentation learner grammar.",
                    "label": 0
                },
                {
                    "sent": "And of course we'd like this triangle to be.",
                    "label": 0
                },
                {
                    "sent": "Work nicely, which means that the learned grammar at some point or another has to be the grammar corresponding to a language, which is the one corresponding to the presentation.",
                    "label": 1
                },
                {
                    "sent": "OK, so that sounds fair.",
                    "label": 0
                },
                {
                    "sent": "Let's skip these.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "So this is what identification in the limit is about is just about saying that there is a step N at which all was for which my nice little triangle closes down neatly, and whatever I was supposed to learn, I have learned so when a class of grammars are a class of languages is identifiable.",
                    "label": 1
                },
                {
                    "sent": "In the limit, it means that I have gotten algorithm can do that.",
                    "label": 0
                },
                {
                    "sent": "Now, obviously that's a very just nice mathematical setting.",
                    "label": 0
                },
                {
                    "sent": "As I said before, this is inductive inference, but if we're going to use these practically, we're going to have to put some limits related with.",
                    "label": 0
                },
                {
                    "sent": "Well, typically with polynomial bounds.",
                    "label": 0
                },
                {
                    "sent": "So what should we bound?",
                    "label": 0
                },
                {
                    "sent": "What should we put a bound on when we're trying to learn?",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got, you know, we've got these presentations.",
                    "label": 0
                },
                {
                    "sent": "We've got this learning algorithm, so we obviously can put a bound on the time, but I can't really put a bound on the global time because I don't know how many data I don't have any control really over my presentation, so I can't say, you know, after 200 examples I have learned because I don't control the presentation, I just know the one day or another.",
                    "label": 0
                },
                {
                    "sent": "All the exalta interesting examples will have appeared, but I don't know when they will have appeared.",
                    "label": 0
                },
                {
                    "sent": "I can put about on update time.",
                    "label": 0
                },
                {
                    "sent": "Update Time is since we're on an online learning process.",
                    "label": 0
                },
                {
                    "sent": "Is the time I need to go from hypothesis I2 hypothesis I plus one?",
                    "label": 0
                },
                {
                    "sent": "Well, actually this has been proved not to give any advantage at all.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to do an algorithm which has got, say, polynomial update time, but all it does is really says, well, I used my time to try and update.",
                    "label": 0
                },
                {
                    "sent": "Oh dear, I haven't got any time when I just returned the previous hypothesis and I just, you know, sort of keep this extra time or when I get more time.",
                    "label": 0
                },
                {
                    "sent": "I'll be able to run my examples so update time doesn't really help us much.",
                    "label": 0
                },
                {
                    "sent": "There's a very nice model as they called.",
                    "label": 0
                },
                {
                    "sent": "IPE IPE is implicit prediction errors in an online learning model.",
                    "label": 0
                },
                {
                    "sent": "I am learning hypothesis, and I've got a new example that arrives.",
                    "label": 0
                },
                {
                    "sent": "So if my bath ASIS is capable of classifying correctly the very next example that arrives, then I'm going to say, OK, that's that's good.",
                    "label": 0
                },
                {
                    "sent": "You know that doesn't score negatively, but if my hypothesis is actually misclassifying the next example, it arrives.",
                    "label": 0
                },
                {
                    "sent": "I call that an implicit prediction error.",
                    "label": 0
                },
                {
                    "sent": "Obviously a good learning process would be 1 where the number of implicit prediction errors would be small.",
                    "label": 0
                },
                {
                    "sent": "Meaning the number of times that you know that my hypothesis is making mistakes is going to be small.",
                    "label": 0
                },
                {
                    "sent": "Clearly it's going to be related to the number of times I'm going to have to change the Spartacist, so keeping this small in an online learning process seems like a good idea.",
                    "label": 0
                },
                {
                    "sent": "I just mentioned that no notion of having to change my hypothesis well, this called mind changes.",
                    "label": 0
                },
                {
                    "sent": "So you can also put a bound on the number of mind changes.",
                    "label": 0
                },
                {
                    "sent": "Mind changes is how many times have I got to change my hypothesis?",
                    "label": 0
                },
                {
                    "sent": "I don't really have to if I don't want to.",
                    "label": 0
                },
                {
                    "sent": "I mean, I might have something that is misclassified classifying.",
                    "label": 0
                },
                {
                    "sent": "And still I keep it, but it's not usually very good idea.",
                    "label": 0
                },
                {
                    "sent": "So how many times do I have to change my mind and online?",
                    "label": 0
                },
                {
                    "sent": "Learning process that doesn't change its mind too often is a good online learning process.",
                    "label": 0
                },
                {
                    "sent": "And then this queries or the number of good examples needed.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of papers and a lot of ideas out there in comparison of these, none of which is entirely convincing.",
                    "label": 0
                },
                {
                    "sent": "It's more complicated than it looks like.",
                    "label": 0
                },
                {
                    "sent": "I mean, you don't have one good definition of learning in efficient way.",
                    "label": 0
                },
                {
                    "sent": "It's not just saying I've got a polynomial time algorithm.",
                    "label": 0
                },
                {
                    "sent": "If my algorithm is polynomial time, but it's just returning a bad solution all the time.",
                    "label": 0
                },
                {
                    "sent": "We're not very happy about this.",
                    "label": 0
                },
                {
                    "sent": "So there's still room, and there's still people working on these things.",
                    "label": 0
                },
                {
                    "sent": "It's not all that simple.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now then, that was the case where we don't have probabilities when there are cases where you can have probabilities.",
                    "label": 0
                },
                {
                    "sent": "So you have really good probabilities for at least two reasons, either becausw, you're better distribution over the examples and you suppose you're working with this distribution or and this is a specific thing in this field, because what you're actually going to learn is a distribution.",
                    "label": 0
                },
                {
                    "sent": "When I'm seeing specific know, there's other people learning a distributions, but in fact what happens is that the distributions are themselves defined through probabilistic finite automata, or through context free grammars, so those are what you call probabilistic automata or probabilistic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "How are we doing for time?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip the.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pak results I don't know.",
                    "label": 0
                },
                {
                    "sent": "Has anyone been talking about Pak results?",
                    "label": 0
                },
                {
                    "sent": "Pak Pak learning well should then OK let me just say a word about them then.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, go back to this.",
                    "label": 0
                },
                {
                    "sent": "OK so the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Is this actually you can apply in other fields in machine learning so I should talk about it.",
                    "label": 0
                },
                {
                    "sent": "These are actually questions that have been looked into a long time ago, probably about.",
                    "label": 0
                },
                {
                    "sent": "I'd say 30 years ago and I had a number of results were known and a lot of people are now including people working in beige and learning.",
                    "label": 0
                },
                {
                    "sent": "Trying to reintroduce in one way or another, this type of mechanisms so the idea is the following, so you know we've got this data that we don't know anything about.",
                    "label": 0
                },
                {
                    "sent": "All we know is that there is an unknown distribution, but a stable distribution is not changing overtime.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a distribution over the whatever examples we're trying to classify.",
                    "label": 0
                },
                {
                    "sent": "And in order to measure the quality of our learning process, we're going to sample twice.",
                    "label": 0
                },
                {
                    "sent": "We should sample the first one to be able to get hold off for training data, and we should sample a second time in order to look at the testing data.",
                    "label": 0
                },
                {
                    "sent": "So one thing is just to learn, and the other thing is to measure how well we've learned, and this is the pack setting.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to be able to pack learn.",
                    "label": 0
                },
                {
                    "sent": "That was unfair.",
                    "label": 0
                },
                {
                    "sent": "It wasn't 30.",
                    "label": 0
                },
                {
                    "sent": "In order to pack learn without class of languages with custom class of grammars and we've got two rule para meters epsilon and Delta.",
                    "label": 0
                },
                {
                    "sent": "Subcell and Delta are really going to tell us know how close I want to be to the truth.",
                    "label": 0
                },
                {
                    "sent": "I'll explain how in a second and then there's two sort of numerical data is explaining well because of technical things I'm not going to go into here.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a maximal size over the number of states of the machines were trying to learn, or the number of rules in the grammar.",
                    "label": 1
                },
                {
                    "sent": "And we're going to have also some sort of maximum length over the strings, but it's not really essential to understand this now.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is somehow supposed we've learned and we come out with a hypothesis which is a classifier.",
                    "label": 0
                },
                {
                    "sent": "We're going to say that our hypothesis is epsilon approximately correct whenever the error made by my hypothesis in comparison to the ideal.",
                    "label": 0
                },
                {
                    "sent": "Grammar so there is a notion of an ideal grammar.",
                    "label": 0
                },
                {
                    "sent": "There the error is less Ness.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a picture it means this.",
                    "label": 0
                },
                {
                    "sent": "You know you've got, you know the rule grammar and you've got your hypothesis an in between the two.",
                    "label": 0
                },
                {
                    "sent": "Well, there's some errors, some places where you know your grammar is actually saying that this is in the language and it's actually not, or your grammar is saying this is not in the language and it actually is.",
                    "label": 0
                },
                {
                    "sent": "So all these little spots that are in this zone defined by the two moons, all these are errors.",
                    "label": 0
                },
                {
                    "sent": "And we can measure these errors as a surface because we've got a distribution of probabilities.",
                    "label": 0
                },
                {
                    "sent": "So it's really what is the probability of landing when testing of landing exactly in those zones.",
                    "label": 0
                },
                {
                    "sent": "So we want that to be kept very small.",
                    "label": 0
                },
                {
                    "sent": "So that's that.",
                    "label": 0
                },
                {
                    "sent": "Sounds like a fair idea, so why do I need a second parimeter?",
                    "label": 0
                },
                {
                    "sent": "Why do I need the Delta?",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because, well, this is the argument that the two years ago there was a French election and they were announcing that unless there is a surprise, there should be no surprise.",
                    "label": 0
                },
                {
                    "sent": "So in this sentence, we're just saying that there's actually 2.",
                    "label": 0
                },
                {
                    "sent": "A levels to measure errors.",
                    "label": 0
                },
                {
                    "sent": "One is the second level.",
                    "label": 0
                },
                {
                    "sent": "Seems there should be no surprise, so you know that's my epsilon.",
                    "label": 0
                },
                {
                    "sent": "Epsilon is the surprise factor.",
                    "label": 0
                },
                {
                    "sent": "But then there's unless there is a surprise which means, well, you know we may have made a really big mess through our sampling or through our choosing RF examples or something could have a big and then been considering long notice that the examples could be the errors could be on the examples from the training set or even on the testing set if.",
                    "label": 0
                },
                {
                    "sent": "If you completely choose wrongly becausw.",
                    "label": 0
                },
                {
                    "sent": "Unluckily the examples, then you would be wrong.",
                    "label": 0
                },
                {
                    "sent": "So there you need your second parimeter.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need your Delta.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not giving the maths but the maths is saying that we want with probability.",
                    "label": 0
                },
                {
                    "sent": "At least one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "The error to be at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so you've got 1D.",
                    "label": 0
                },
                {
                    "sent": "I want epsilon and there's all sorts of really nice results or nice theory.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Written about these things, the problem being that when we're working with things like DFA, so deterministic finite automata and same applies for nondeterminism finite automata or context free grammars where all the pack type of results are negative, they're saying we can't learn.",
                    "label": 0
                },
                {
                    "sent": "Or we can't learn something in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "We need much more than polynomial resources in order to learn, so that's bad news.",
                    "label": 0
                },
                {
                    "sent": "That's bad news because Becausw, because that's what we like to do, actually.",
                    "label": 0
                },
                {
                    "sent": "It's curious the reasons for doing this.",
                    "label": 0
                },
                {
                    "sent": "The reasons for doing this depends not on sort of typical PNP problems.",
                    "label": 0
                },
                {
                    "sent": "It depends on cryptographic assumptions.",
                    "label": 0
                },
                {
                    "sent": "The idea is saying that.",
                    "label": 0
                },
                {
                    "sent": "Well, if I could learn DFA, I could probably break things like RSA codes.",
                    "label": 0
                },
                {
                    "sent": "So why would I break codes by learning well becausw?",
                    "label": 0
                },
                {
                    "sent": "I'm allowed to produce examples so I can produce examples of messages and messages the way they've been decoded, so I can really produce the examples I want, and so if by producing these examples I was then able to learn the DFA and what the DFA is doing is really decoding.",
                    "label": 0
                },
                {
                    "sent": "Bit by bit, but it still is decoding then in a way where my problem of solving of learning a DFA is at least as hard as the one is breaking RSA.",
                    "label": 0
                },
                {
                    "sent": "And you don't really believe that breaking error says that simple so OK.",
                    "label": 0
                },
                {
                    "sent": "So it's related, very good, very directly to that sort of risk.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in Alternatively, since we've just seen that we're learning DFA is not going to be a very easy or learning context.",
                    "label": 0
                },
                {
                    "sent": "Free grammars, why don't we learn directly the distributions?",
                    "label": 0
                },
                {
                    "sent": "So learning directly the distributions is making the processes that I'm not trying to learn a classifier any longer, but instead of learning a classifier what I'm going to learn is a probabilistic finite state machine or probabilistic context free grammars.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose I've got an algorithm that learns that what then can I say about the type of results or the type of algorithms that I've done where I can obtain something that is called identification in the limit with probability one?",
                    "label": 0
                },
                {
                    "sent": "So earlier on we saw identification in the limit, which was saying that I am sure that one day or another my algorithm is going to be returning the correct solution.",
                    "label": 0
                },
                {
                    "sent": "Identification in the limit with probably what the probability one is saying.",
                    "label": 0
                },
                {
                    "sent": "I am sure with probability one or better said it's not impossible because impossibility is not defined that way from power probabilities, I am the probability that the data arrives so absurdly that I can't learn correctly.",
                    "label": 0
                },
                {
                    "sent": "This probability is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "If people haven't looked into probabilistic learning, I mean, this doesn't make much sense, but it's just the idea of saying, well, you know, I can write out the the series of elements that would make my algorithm fail.",
                    "label": 0
                },
                {
                    "sent": "It just happens that the probability corresponding to this series of events is equal to 0.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which doesn't mean the same as being impossible.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, if we can compute probabilities, obviously we suppose the probabilities are given by rational numbers or something.",
                    "label": 0
                },
                {
                    "sent": "Then we do have algorithms that can learn with probability one finite state automata.",
                    "label": 1
                },
                {
                    "sent": "But then we've got problems with boundedness, again resources, so broadness of resources if we're trying to do it with just a polynomial number of data, then that becomes nearly impossible.",
                    "label": 0
                },
                {
                    "sent": "I say it becomes very tricky.",
                    "label": 0
                },
                {
                    "sent": "You've actually got to add some sort of information in order to be able to get somewhere, but I mean there are algorithms out there, which perhaps I'll be passing.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second possibility, if I'm trying to learn probabilities.",
                    "label": 0
                },
                {
                    "sent": "Sorry, distribution of probabilities is to admit we've got errors, so probability one identification in the limit means I've actually got to learn the exact target distribution.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you know, mainly because we're in a probabilistic world, we'd be happy if we nearly learn the distribution.",
                    "label": 0
                },
                {
                    "sent": "The whispers to learn.",
                    "label": 0
                },
                {
                    "sent": "If we approximate whatever were supposed to be learning.",
                    "label": 0
                },
                {
                    "sent": "So you can also have pack definitions of this.",
                    "label": 0
                },
                {
                    "sent": "You can have sort of definitions saying that while the distance or the error between what I've learned and what the target was is going to be very small, I'm going to keep it as small as I can.",
                    "label": 0
                },
                {
                    "sent": "So what does the error between two distributions means while introduces the notion of distances?",
                    "label": 0
                },
                {
                    "sent": "So you're going to have distances between two distributions distances based on.",
                    "label": 0
                },
                {
                    "sent": "Well, there's all sorts of possibilities of defining distances.",
                    "label": 0
                },
                {
                    "sent": "Between distributions corresponding to norms corresponding to entropies corresponding to things like that, if you are interested with that, I can say a word just brings us a little bit far away.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, depending on what distance you use, the problem is either too simple or too hard and in between the middle where we're not really sure what the state is and there's some nice algorithms out there for different sorts of distributions, and perhaps I'll be talking about that.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "So conclusion about this, I must say it's a difficult part because without having the algorithms to look into well, one might not follow exactly where is this guy getting too well.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to get to is saying we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn Automata.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn Grammar's an we're going to have to keep in mind that we don't.",
                    "label": 0
                },
                {
                    "sent": "We can't be happy with just saying.",
                    "label": 0
                },
                {
                    "sent": "Well, my algorithm does something I need to say something intelligent about it.",
                    "label": 0
                },
                {
                    "sent": "So what I was trying to say is.",
                    "label": 0
                },
                {
                    "sent": "Well, if I can say something intelligent like saying, well, you know, give me more data and I'm sure to learn.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, I'm doing as well as I can with the data I have, but that's already something.",
                    "label": 0
                },
                {
                    "sent": "And then there's all the distribution world in which in the probabilities probabilistic world where I'm going to also be able to say things about when I have learned or when I have not learned.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think this is going to be a good moment to have a coffee break.",
                    "label": 0
                }
            ]
        }
    }
}