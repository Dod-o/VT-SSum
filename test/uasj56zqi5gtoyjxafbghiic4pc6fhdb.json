{
    "id": "uasj56zqi5gtoyjxafbghiic4pc6fhdb",
    "title": "Subspace-based Learning with Grassmann Kernels",
    "info": {
        "author": [
            "Jihun Hamm, GRASP Laboratory, University of Pennsylvania"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_hamm_sbl/",
    "segmentation": [
        [
            "So I change the title of the talk a little.",
            "It'll be different from the preceding.",
            "Since this submission we had a little bit of additional materials added to the talk, so this is a new title and this is joint work with Daniel Lee and my name is Jihan.",
            "Yeah."
        ],
        [
            "So let me just jump into the motivation for this work.",
            "Suppose you have this set of images.",
            "This is a face image of a same person bearing only in the illumination condition.",
            "Suppose you do PCA on this set of images represented at a restaurant vector.",
            "For each image, then this is eigenvector and eigenvalue you get from PCA.",
            "Similarly, this is a another set of image for different phase an.",
            "This is a result of PCA on this set of image.",
            "And as you can see, if you just ignore the first component, which is basically a mean of the data.",
            "The other eigenvectors are very typical of the face images that are bearing in illumination conditions, regardless of subject identity.",
            "Identity of this data and also you can see these are the cumulative spectrogram of the eigenvalues, and you see this increase to one very fast, so there are other people who observe this low dimensionality of illumination bearing images.",
            "Publishing the paper by Helena Stein and so on.",
            "And there are also other people who have some theoretical backgrounds for why this is the case for elimination.",
            "Pairing images."
        ],
        [
            "And here is another example for subspace structure in the data.",
            "You have a set of image of images of an object taken on the different viewpoints.",
            "He applies PCA through this set of images similarly, and you see this eigenvalues eigenvectors.",
            "You see that these eigenvalues also increases quite fast.",
            "Ann, this is a example with another set of data with different object.",
            "You can apply this simple idea to pose expression and all the other kinds of variations and this.",
            "Low low approximation is quite worked quite well for many kinds of different variabilities in the images, and this is known as eigenfaces.",
            "Which is one of the simplest and.",
            "Effective algorithms that use since 80s.",
            "So if you have a collection of if your data consists of collection images with say different views of different views of an object and you have multiple object in your database, you can.",
            "You might want to think your data set as a collection of subspaces."
        ],
        [
            "This is a little bit different subject, but this is another example of linear subspace structure in your data.",
            "Suppose you have a sequence sequence of data doesn't have to be an image data, but you have a sequence of data in vector space and you can use the simplest linear dynamical model such as autoregressive moving average model to model your sequence.",
            "In this model, your your state space evolves simply linearly and your output is also simple linear function.",
            "Instantaneous function of your current state X.",
            "And V NWR noise, noise added here.",
            "Your data.",
            "And this model has been used for for example, in dynamic textures and modeling human actions.",
            "By researchers in computer building.",
            "Now, so for each sequence you can try to estimate the parameters of this armor model, which is matrix A&C.",
            "But the problem is this is not a unique representation of a sequence especially.",
            "The Matrix A depends on how you define your coordinate system for your states.",
            "So on alternative to representing a dynamical system is to use something called observability matrix which is a concatenation of a matrix C. See times 80 times a square and so on along the road.",
            "Say this is a.",
            "This is a big, very tall matrix of size D by M. Now, if you consider the span of this observation matrix from the parameters estimated from your for your sequence.",
            "Then this becomes a unique signature that doesn't depend on the your choice of coordinate system.",
            "By the way, the span of a matrix means you know just the arbitrary linear combination of this column vectors in the matrix."
        ],
        [
            "So if you if your database consists of a sequence of actions such as checking, watch, scratching head and so on.",
            "Then you can estimate the parameters A&C for each sequence you have in your data set.",
            "And from the partners you have a unique subspace called observability span of the observer two matrices and you can also consider your data set consists of these sub spaces from the observed matrices."
        ],
        [
            "So.",
            "The subspace based learning is what we want to propose.",
            "In this setting you assume your data consists of linear subspaces, which are all familiar.",
            "The difference between the usual settings data is not consisting of.",
            "Vectors is consistent linear subspaces.",
            "For example, this is a picture, a schematic picture of your data set.",
            "But we want to do with this approach is that you want to model the known variability as a variability within each subspace you want to absorb.",
            "The variable that you know by the subspace structure, an focus on the unknown and therefore more interesting variability between these subspaces."
        ],
        [
            "So the most appropriate framework for subspace based learning is the grassman manifold.",
            "The grassman manifold, denoted by G small income capital D is by definition that set of all small M dimensional linear subspaces of capital D dimensional vector space.",
            "If you have.",
            "A subspace is in your original vector space, then they are represented as single points on this grassman manifold.",
            "Applications of Grassmann manifold has been, you know, can be found in signal processing control theory optimization, computer vision and so on.",
            "But there are also many, many applications of the concept of Grassmann manifold but didn't explicitly refer to this as a subspace of learning."
        ],
        [
            "So Grassman manifold is.",
            "It says moves Romanian compact manifold, which can be formally drive as a quotient space of orthogonal groups, but what's really good about this manifold is it has a global coordinate, especially global Euclidean coordinate, and you only need some knowledge of linear algebra to understand this nonlinear manifold.",
            "This is a great advantage to Grassman manifolds to other manifold to represent data.",
            "Simply speaking on an element, you can represent an element of a Grassmann manifold with an orthonormal matrix.",
            "Of the size capital D by M. So these large then larger than M is a tall matrix.",
            "And it has to be orthogonal.",
            "But the thing you have to keep to keep in mind is that there is an equivalent simulation.",
            "This says two represent two matrices should be considered equivalent if they spend the same subspace, because this is the definition of the grassman manifold.",
            "Or in other words.",
            "Two matrices.",
            "Can be represented.",
            "Why won't equals Y2 if there's some?",
            "Orthogonal matrix R. Then these two matrix has to be considered same."
        ],
        [
            "Anne.",
            "Let me talk about a principle, angles and Canonical correlations, so I guess this is a different take on the Canonical correlation from the first speaker.",
            "Say you have two matrices Y1Y2, which are orthonormal and consider the span of those two basis matrices.",
            "And let you be a vector line in the first subspace, and VV the second vector, second unit vector lying on the other subjects.",
            "The first principle, angle or Canonical correlation, is.",
            "Between these subspaces is defined as the maximum of the.",
            "Inner product between 2 unit vectors lying on each subspace.",
            "This and this value itself is known as a Canonical correlation and this angle Theta one is known as principle angle.",
            "So this this coincide with the definition, it's just like different different definition from the definition from the first talk, but."
        ],
        [
            "They should coincide.",
            "You can similarly compute other principle angles at other than the first one with the same recursive procedure.",
            "So The Cave cosine angle is similarly defined as this maximization problem with an additional constraint that this direction unv has to be orthogonal to the previously found direction from 1 two K -- 1.",
            "So if you have small M dimensional subspaces then you have principle angles.",
            "The first principle language that I want to the last one said I am in a nondecreasing sequence, and equivalently you have a Canonical correlation decreased in and an increasing sequence.",
            "But you don't actually have to solve this maximization problem.",
            "Good luck showed in his book that you can use SVD simply SVD to compute all the principle angles at once.",
            "For example, you have two orthonormal matrices, Y&Y two take the inner product of it and apply SVD on it.",
            "Then this matrix is.",
            "Other matrix is to define in this definition of principle angle and this dialogue matrix happened to have the option to be the Canonical correlation."
        ],
        [
            "So.",
            "So we do have.",
            "We have seen how to compute principle angles when you're given the two subspaces.",
            "So that's how we go about defining a nice distance between two subspaces based on this angles.",
            "And there is a Canonical distance for this known as arc length distance, which is.",
            "The L2.",
            "Elton Elton, Norm of this principle angles.",
            "And this is a Canonical distance, because if you remember, the grassman manifold is a non manifold and this happens to coincide with the length of geodesic paths connecting these two points on the grassman manifold.",
            "But this is not the only distance.",
            "Of course."
        ],
        [
            "Let me give you an example of some other distances defined on this grassman manifold.",
            "The first one is known as projection distance, which is the sum of.",
            "Science and I ^2.",
            "And the other one is known as being a cautious distance.",
            "Which is 1 minus product of cosine satirize.",
            "An Interestingly, both of them can be directly computed, either from the principle angles or just the matrix representation of each subspaces.",
            "There are also other distances such as Max correlation, minimum correlation, procrustes and so on in the literature would let me just skip those."
        ],
        [
            "Anne.",
            "These distances do have some interesting characteristics and their group into a several groups, but let me discuss it in the poster section if you're interested."
        ],
        [
            "Let's consider the application of this distance.",
            "When you have this distance defined for subspaces, you can use K nearest neighbors.",
            "To classify as a simplest approach to classification problems with subspaces.",
            "And this has been used in a work by Yamaguchi and was called Mutual Subspace Method.",
            "But we definitely want to do you want to use better algorithm than K nearest neighbors?",
            "And especially in this talk I want to use.",
            "I won't take this current analysis approach.",
            "With the subspace structure data.",
            "And these two are the previous method using this idea.",
            "Constraint, mutual subscript method or discriminate analysis Canonical correlations.",
            "Let me briefly discuss what these algorithms."
        ],
        [
            "Do.",
            "So the idea of discriminant analysis with subspaces is that.",
            "You want to find direction, a discriminative direction in your original data space.",
            "Such that.",
            "When you project your sets along those directions and compute the subspace distances.",
            "Then you want the distance to be small if they belong to the same class and one you want those distance to be larger if they have a different class labels.",
            "So idea itself is sound, but the problem is in the implementation.",
            "As you can see, this is a very difficult optimization an the formulation itself is.",
            "Iterative.",
            "You're never going to get a closed form solution with this approach.",
            "And that was the main disadvantage comes from the fact you're performing projection in your data space.",
            "You're originally collision space, but then you are measuring the distances using the concept from grassman space, and because of this inconsistency there's a difficulty in implementing these ideas."
        ],
        [
            "An R solution are easier solution is just simply use kernel method.",
            "If you can define a good kernel ingress and manifold, you can use the family clean algorithms for this in this Hilbert space and you don't have to project the data and measure distance separately as the previously used method."
        ],
        [
            "So let me give you the definition of a grassman kernel.",
            "Say AK is.",
            "A real valued symmetric function which take.",
            "Which takes matrices as arguments.",
            "It has identified two conditions.",
            "First is invariant to different representation of your subspace.",
            "The value of this current has to be same whether regardless of how you rotate your matrix representation, because.",
            "By definition, these two matrices represent the same subspace.",
            "2nd is the usual positive definite condition and.",
            "Anne.",
            "Among the distance that we talk so far, the projection distance in being equal distance do have the corresponding grassman kernels, and we will look at those two forms in the coming slides."
        ],
        [
            "So the first kernel is a projection kernel.",
            "You can understand it through.",
            "Associating a subspace with.",
            "Product out of product Y with yourself Y transpose.",
            "So why, why transpose is a big dividing matrix matrix of rank small M and this is the orthogonal projection matrix.",
            "That's where this name derives from.",
            "So the space of all orthonormal projection.",
            "Sorry, the space of D by the orthonormal projection matrices is a subset of Euclidean space.",
            "An natural inner product in that equation spaces trace of two matrices.",
            "As we all know.",
            "So this defines the projection kernel.",
            "Projection of kernel.",
            "Why, Why two is defined as the trace of these?",
            "Why why, why interest both white white transpose, or equivalently the Frobenius norm squared awhile and transpose Y 2?",
            "Is the Berry has a very simple form and easy to evaluate?",
            "So this is the main kernel we want to promote to use."
        ],
        [
            "And there's another kernel called unique wash kernel.",
            "Proposed by Wolf and Viswanathan and others.",
            "But let me just skip the construction of this kernel.",
            "An that the final form of this kernel is.",
            "K Why won't come on Y2 is determinant of wine, transpose Y 2 squared.",
            "This has a different form, but it's also relatively easy to evaluate and is also useful kernel, but I'll compare the results with projection corner in the experimental section."
        ],
        [
            "So advantage of having a valid kernel on grass and manifold is obvious.",
            "We can use all the kernel machines for arbitrary task we want to do.",
            "And once you have some nice, well defined kernel for this space, you can combine them to generate new kernels.",
            "If you have new requirements."
        ],
        [
            "There's a.",
            "Another extension, the final extension to the original idea.",
            "You can extend idea of subspace to non linear subspace.",
            "Say.",
            "So this part of the figure is something I already explained.",
            "This silver Spade is defined through a grassman kernel.",
            "But now if you add another stage in the front end.",
            "And assume your data domain is not a directly vector space.",
            "But it's some arbitrary set with some kernel, and this kernel defines a map to the Hilbert space.",
            "So if you compute the principle angles, distances, kernels on this in this Hilbert space, that effectively defines.",
            "The nonlinear subspace in the original data set.",
            "So in this way we can extend our work to non linear subspaces as well.",
            "Just to be clear, so the kernel for this part under this part of it is to be distinguished.",
            "There are different kernels."
        ],
        [
            "So as an application we're going to use Fisher discriminant analysis in conjunction with this grassman kernel."
        ],
        [
            "And we're going to contrast our method with the baseline algorithm, which is a simple Euclidean Fisher discriminant analysis.",
            "So with the same data we try to view it as a subspace and apply subspace based algorithms and also viewed as this simple Euclidean vectors and tried to do equally in FT algorithm on that.",
            "And we're comparing our method with other methods.",
            "Previous methods.",
            "Let me call our approach, which is to use kernel efficiency analysis with projection or even a question kernel as Grassmann discriminant analysis.",
            "To contrast it from the previous method, they use some heuristics to going back and forth from the data space and the grassman concept.",
            "So."
        ],
        [
            "Let me explain the data set.",
            "We're going to use.",
            "This.",
            "The Yale Face database consists of 80 three different persons, nine different poses and 45 different illumination conditions.",
            "We're going to apply PCA along the direction of this illumination direction, so we want to make this problem this recognition problem, illumination invariant basically.",
            "Is it in the training, training and test phase?",
            "I'm going to hold out.",
            "One pose of all the persons as a test set and use the remaining data set for training.",
            "And then I'm going to repeat this for 9 fold cross validation.",
            "The CMU PIE data set is all very similarly structured face database."
        ],
        [
            "There's another data set called E TH80 database, which consists of eight different categories, and each category has 10 objects, and each object is viewed 441 different poses.",
            "We're going to apply a PCA along the post direction.",
            "So in order to make this categorisation problem invariant to viewpoints.",
            "We're also going to similarly apply tenfold cross validation by taking out one vertical slides from this data set."
        ],
        [
            "This is the last data set we're going to use.",
            "The exmas databases.",
            "Database for human actions.",
            "OK.",
            "Which consists of 11 different actions and 11 different actors.",
            "If you remember, I'm going to estimate the linear dynamical system models for each time frames of these actions and uses for our test."
        ],
        [
            "And this is actually our final page.",
            "This is original from the four databases.",
            "Anne.",
            "This the bars represent the recognition rate for different algorithms.",
            "If you look at this group of bar, the first one is the result from the conventional Euclidean approach.",
            "The second one is the proposed method and the others are competing algorithms and you can see all the rates vary.",
            "Rates are different overall for different databases.",
            "The second column, the light blue one, is achieved the best rates for all these databases.",
            "By far, and one interesting point is.",
            "The same algorithm with Vinny Cash kernel degrade very fast.",
            "These are this Sky blue bars.",
            "They all decrease fast as a subspace dimension increases and I have a good explanation for this.",
            "But let me talk this in the."
        ],
        [
            "Recession.",
            "So let me summarize.",
            "We're proposing a subspace based learning paradigm to exploit the data that has inherent linear structures in it.",
            "And we use Grassman manifold as a common framework for this problem.",
            "And the projection kernels expose shows very.",
            "Nice performance compared to other previous known methods.",
            "And this is a kernel method.",
            "This is general building block for arbitrary.",
            "Datasets, arbitrary method and arbitrary learning paradigm.",
            "Let me finish with just one remaining question.",
            "Which is if the projection kernel and any question kernel are the only only kernels for the grassman manifolds, and this is a remaining question where currently working on.",
            "Thank you very much.",
            "Spaces.",
            "Yes, yes.",
            "So the simplest extension to affine subspaces is to use homogeneous coordinate to absorb this.",
            "Bias into this differentation.",
            "Actually, this call is some problems for definition and we have some work in unpublished work.",
            "Thank you for the nice question.",
            "2 dimensional, did you say?",
            "It's limited linear subspaces.",
            "I'm sorry.",
            "Yes, the dimensionality is not a factor, yeah?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I change the title of the talk a little.",
                    "label": 0
                },
                {
                    "sent": "It'll be different from the preceding.",
                    "label": 0
                },
                {
                    "sent": "Since this submission we had a little bit of additional materials added to the talk, so this is a new title and this is joint work with Daniel Lee and my name is Jihan.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just jump into the motivation for this work.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have this set of images.",
                    "label": 1
                },
                {
                    "sent": "This is a face image of a same person bearing only in the illumination condition.",
                    "label": 0
                },
                {
                    "sent": "Suppose you do PCA on this set of images represented at a restaurant vector.",
                    "label": 0
                },
                {
                    "sent": "For each image, then this is eigenvector and eigenvalue you get from PCA.",
                    "label": 0
                },
                {
                    "sent": "Similarly, this is a another set of image for different phase an.",
                    "label": 0
                },
                {
                    "sent": "This is a result of PCA on this set of image.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, if you just ignore the first component, which is basically a mean of the data.",
                    "label": 0
                },
                {
                    "sent": "The other eigenvectors are very typical of the face images that are bearing in illumination conditions, regardless of subject identity.",
                    "label": 0
                },
                {
                    "sent": "Identity of this data and also you can see these are the cumulative spectrogram of the eigenvalues, and you see this increase to one very fast, so there are other people who observe this low dimensionality of illumination bearing images.",
                    "label": 0
                },
                {
                    "sent": "Publishing the paper by Helena Stein and so on.",
                    "label": 0
                },
                {
                    "sent": "And there are also other people who have some theoretical backgrounds for why this is the case for elimination.",
                    "label": 0
                },
                {
                    "sent": "Pairing images.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is another example for subspace structure in the data.",
                    "label": 0
                },
                {
                    "sent": "You have a set of image of images of an object taken on the different viewpoints.",
                    "label": 0
                },
                {
                    "sent": "He applies PCA through this set of images similarly, and you see this eigenvalues eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "You see that these eigenvalues also increases quite fast.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is a example with another set of data with different object.",
                    "label": 1
                },
                {
                    "sent": "You can apply this simple idea to pose expression and all the other kinds of variations and this.",
                    "label": 0
                },
                {
                    "sent": "Low low approximation is quite worked quite well for many kinds of different variabilities in the images, and this is known as eigenfaces.",
                    "label": 0
                },
                {
                    "sent": "Which is one of the simplest and.",
                    "label": 0
                },
                {
                    "sent": "Effective algorithms that use since 80s.",
                    "label": 0
                },
                {
                    "sent": "So if you have a collection of if your data consists of collection images with say different views of different views of an object and you have multiple object in your database, you can.",
                    "label": 0
                },
                {
                    "sent": "You might want to think your data set as a collection of subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a little bit different subject, but this is another example of linear subspace structure in your data.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a sequence sequence of data doesn't have to be an image data, but you have a sequence of data in vector space and you can use the simplest linear dynamical model such as autoregressive moving average model to model your sequence.",
                    "label": 0
                },
                {
                    "sent": "In this model, your your state space evolves simply linearly and your output is also simple linear function.",
                    "label": 0
                },
                {
                    "sent": "Instantaneous function of your current state X.",
                    "label": 0
                },
                {
                    "sent": "And V NWR noise, noise added here.",
                    "label": 0
                },
                {
                    "sent": "Your data.",
                    "label": 0
                },
                {
                    "sent": "And this model has been used for for example, in dynamic textures and modeling human actions.",
                    "label": 1
                },
                {
                    "sent": "By researchers in computer building.",
                    "label": 0
                },
                {
                    "sent": "Now, so for each sequence you can try to estimate the parameters of this armor model, which is matrix A&C.",
                    "label": 0
                },
                {
                    "sent": "But the problem is this is not a unique representation of a sequence especially.",
                    "label": 1
                },
                {
                    "sent": "The Matrix A depends on how you define your coordinate system for your states.",
                    "label": 0
                },
                {
                    "sent": "So on alternative to representing a dynamical system is to use something called observability matrix which is a concatenation of a matrix C. See times 80 times a square and so on along the road.",
                    "label": 0
                },
                {
                    "sent": "Say this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a big, very tall matrix of size D by M. Now, if you consider the span of this observation matrix from the parameters estimated from your for your sequence.",
                    "label": 0
                },
                {
                    "sent": "Then this becomes a unique signature that doesn't depend on the your choice of coordinate system.",
                    "label": 0
                },
                {
                    "sent": "By the way, the span of a matrix means you know just the arbitrary linear combination of this column vectors in the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you if your database consists of a sequence of actions such as checking, watch, scratching head and so on.",
                    "label": 0
                },
                {
                    "sent": "Then you can estimate the parameters A&C for each sequence you have in your data set.",
                    "label": 0
                },
                {
                    "sent": "And from the partners you have a unique subspace called observability span of the observer two matrices and you can also consider your data set consists of these sub spaces from the observed matrices.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The subspace based learning is what we want to propose.",
                    "label": 0
                },
                {
                    "sent": "In this setting you assume your data consists of linear subspaces, which are all familiar.",
                    "label": 1
                },
                {
                    "sent": "The difference between the usual settings data is not consisting of.",
                    "label": 1
                },
                {
                    "sent": "Vectors is consistent linear subspaces.",
                    "label": 0
                },
                {
                    "sent": "For example, this is a picture, a schematic picture of your data set.",
                    "label": 0
                },
                {
                    "sent": "But we want to do with this approach is that you want to model the known variability as a variability within each subspace you want to absorb.",
                    "label": 1
                },
                {
                    "sent": "The variable that you know by the subspace structure, an focus on the unknown and therefore more interesting variability between these subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the most appropriate framework for subspace based learning is the grassman manifold.",
                    "label": 1
                },
                {
                    "sent": "The grassman manifold, denoted by G small income capital D is by definition that set of all small M dimensional linear subspaces of capital D dimensional vector space.",
                    "label": 1
                },
                {
                    "sent": "If you have.",
                    "label": 1
                },
                {
                    "sent": "A subspace is in your original vector space, then they are represented as single points on this grassman manifold.",
                    "label": 0
                },
                {
                    "sent": "Applications of Grassmann manifold has been, you know, can be found in signal processing control theory optimization, computer vision and so on.",
                    "label": 1
                },
                {
                    "sent": "But there are also many, many applications of the concept of Grassmann manifold but didn't explicitly refer to this as a subspace of learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Grassman manifold is.",
                    "label": 0
                },
                {
                    "sent": "It says moves Romanian compact manifold, which can be formally drive as a quotient space of orthogonal groups, but what's really good about this manifold is it has a global coordinate, especially global Euclidean coordinate, and you only need some knowledge of linear algebra to understand this nonlinear manifold.",
                    "label": 0
                },
                {
                    "sent": "This is a great advantage to Grassman manifolds to other manifold to represent data.",
                    "label": 0
                },
                {
                    "sent": "Simply speaking on an element, you can represent an element of a Grassmann manifold with an orthonormal matrix.",
                    "label": 1
                },
                {
                    "sent": "Of the size capital D by M. So these large then larger than M is a tall matrix.",
                    "label": 0
                },
                {
                    "sent": "And it has to be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "But the thing you have to keep to keep in mind is that there is an equivalent simulation.",
                    "label": 0
                },
                {
                    "sent": "This says two represent two matrices should be considered equivalent if they spend the same subspace, because this is the definition of the grassman manifold.",
                    "label": 0
                },
                {
                    "sent": "Or in other words.",
                    "label": 0
                },
                {
                    "sent": "Two matrices.",
                    "label": 0
                },
                {
                    "sent": "Can be represented.",
                    "label": 0
                },
                {
                    "sent": "Why won't equals Y2 if there's some?",
                    "label": 0
                },
                {
                    "sent": "Orthogonal matrix R. Then these two matrix has to be considered same.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Let me talk about a principle, angles and Canonical correlations, so I guess this is a different take on the Canonical correlation from the first speaker.",
                    "label": 0
                },
                {
                    "sent": "Say you have two matrices Y1Y2, which are orthonormal and consider the span of those two basis matrices.",
                    "label": 0
                },
                {
                    "sent": "And let you be a vector line in the first subspace, and VV the second vector, second unit vector lying on the other subjects.",
                    "label": 0
                },
                {
                    "sent": "The first principle, angle or Canonical correlation, is.",
                    "label": 0
                },
                {
                    "sent": "Between these subspaces is defined as the maximum of the.",
                    "label": 0
                },
                {
                    "sent": "Inner product between 2 unit vectors lying on each subspace.",
                    "label": 0
                },
                {
                    "sent": "This and this value itself is known as a Canonical correlation and this angle Theta one is known as principle angle.",
                    "label": 0
                },
                {
                    "sent": "So this this coincide with the definition, it's just like different different definition from the definition from the first talk, but.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They should coincide.",
                    "label": 0
                },
                {
                    "sent": "You can similarly compute other principle angles at other than the first one with the same recursive procedure.",
                    "label": 0
                },
                {
                    "sent": "So The Cave cosine angle is similarly defined as this maximization problem with an additional constraint that this direction unv has to be orthogonal to the previously found direction from 1 two K -- 1.",
                    "label": 0
                },
                {
                    "sent": "So if you have small M dimensional subspaces then you have principle angles.",
                    "label": 0
                },
                {
                    "sent": "The first principle language that I want to the last one said I am in a nondecreasing sequence, and equivalently you have a Canonical correlation decreased in and an increasing sequence.",
                    "label": 0
                },
                {
                    "sent": "But you don't actually have to solve this maximization problem.",
                    "label": 0
                },
                {
                    "sent": "Good luck showed in his book that you can use SVD simply SVD to compute all the principle angles at once.",
                    "label": 0
                },
                {
                    "sent": "For example, you have two orthonormal matrices, Y&Y two take the inner product of it and apply SVD on it.",
                    "label": 0
                },
                {
                    "sent": "Then this matrix is.",
                    "label": 0
                },
                {
                    "sent": "Other matrix is to define in this definition of principle angle and this dialogue matrix happened to have the option to be the Canonical correlation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we do have.",
                    "label": 0
                },
                {
                    "sent": "We have seen how to compute principle angles when you're given the two subspaces.",
                    "label": 1
                },
                {
                    "sent": "So that's how we go about defining a nice distance between two subspaces based on this angles.",
                    "label": 1
                },
                {
                    "sent": "And there is a Canonical distance for this known as arc length distance, which is.",
                    "label": 0
                },
                {
                    "sent": "The L2.",
                    "label": 0
                },
                {
                    "sent": "Elton Elton, Norm of this principle angles.",
                    "label": 0
                },
                {
                    "sent": "And this is a Canonical distance, because if you remember, the grassman manifold is a non manifold and this happens to coincide with the length of geodesic paths connecting these two points on the grassman manifold.",
                    "label": 0
                },
                {
                    "sent": "But this is not the only distance.",
                    "label": 1
                },
                {
                    "sent": "Of course.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me give you an example of some other distances defined on this grassman manifold.",
                    "label": 0
                },
                {
                    "sent": "The first one is known as projection distance, which is the sum of.",
                    "label": 0
                },
                {
                    "sent": "Science and I ^2.",
                    "label": 0
                },
                {
                    "sent": "And the other one is known as being a cautious distance.",
                    "label": 0
                },
                {
                    "sent": "Which is 1 minus product of cosine satirize.",
                    "label": 0
                },
                {
                    "sent": "An Interestingly, both of them can be directly computed, either from the principle angles or just the matrix representation of each subspaces.",
                    "label": 0
                },
                {
                    "sent": "There are also other distances such as Max correlation, minimum correlation, procrustes and so on in the literature would let me just skip those.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "These distances do have some interesting characteristics and their group into a several groups, but let me discuss it in the poster section if you're interested.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's consider the application of this distance.",
                    "label": 0
                },
                {
                    "sent": "When you have this distance defined for subspaces, you can use K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "To classify as a simplest approach to classification problems with subspaces.",
                    "label": 0
                },
                {
                    "sent": "And this has been used in a work by Yamaguchi and was called Mutual Subspace Method.",
                    "label": 1
                },
                {
                    "sent": "But we definitely want to do you want to use better algorithm than K nearest neighbors?",
                    "label": 0
                },
                {
                    "sent": "And especially in this talk I want to use.",
                    "label": 0
                },
                {
                    "sent": "I won't take this current analysis approach.",
                    "label": 0
                },
                {
                    "sent": "With the subspace structure data.",
                    "label": 0
                },
                {
                    "sent": "And these two are the previous method using this idea.",
                    "label": 0
                },
                {
                    "sent": "Constraint, mutual subscript method or discriminate analysis Canonical correlations.",
                    "label": 0
                },
                {
                    "sent": "Let me briefly discuss what these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "So the idea of discriminant analysis with subspaces is that.",
                    "label": 0
                },
                {
                    "sent": "You want to find direction, a discriminative direction in your original data space.",
                    "label": 1
                },
                {
                    "sent": "Such that.",
                    "label": 0
                },
                {
                    "sent": "When you project your sets along those directions and compute the subspace distances.",
                    "label": 0
                },
                {
                    "sent": "Then you want the distance to be small if they belong to the same class and one you want those distance to be larger if they have a different class labels.",
                    "label": 0
                },
                {
                    "sent": "So idea itself is sound, but the problem is in the implementation.",
                    "label": 1
                },
                {
                    "sent": "As you can see, this is a very difficult optimization an the formulation itself is.",
                    "label": 0
                },
                {
                    "sent": "Iterative.",
                    "label": 0
                },
                {
                    "sent": "You're never going to get a closed form solution with this approach.",
                    "label": 0
                },
                {
                    "sent": "And that was the main disadvantage comes from the fact you're performing projection in your data space.",
                    "label": 0
                },
                {
                    "sent": "You're originally collision space, but then you are measuring the distances using the concept from grassman space, and because of this inconsistency there's a difficulty in implementing these ideas.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An R solution are easier solution is just simply use kernel method.",
                    "label": 0
                },
                {
                    "sent": "If you can define a good kernel ingress and manifold, you can use the family clean algorithms for this in this Hilbert space and you don't have to project the data and measure distance separately as the previously used method.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give you the definition of a grassman kernel.",
                    "label": 0
                },
                {
                    "sent": "Say AK is.",
                    "label": 0
                },
                {
                    "sent": "A real valued symmetric function which take.",
                    "label": 0
                },
                {
                    "sent": "Which takes matrices as arguments.",
                    "label": 0
                },
                {
                    "sent": "It has identified two conditions.",
                    "label": 0
                },
                {
                    "sent": "First is invariant to different representation of your subspace.",
                    "label": 0
                },
                {
                    "sent": "The value of this current has to be same whether regardless of how you rotate your matrix representation, because.",
                    "label": 0
                },
                {
                    "sent": "By definition, these two matrices represent the same subspace.",
                    "label": 0
                },
                {
                    "sent": "2nd is the usual positive definite condition and.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Among the distance that we talk so far, the projection distance in being equal distance do have the corresponding grassman kernels, and we will look at those two forms in the coming slides.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first kernel is a projection kernel.",
                    "label": 1
                },
                {
                    "sent": "You can understand it through.",
                    "label": 0
                },
                {
                    "sent": "Associating a subspace with.",
                    "label": 0
                },
                {
                    "sent": "Product out of product Y with yourself Y transpose.",
                    "label": 0
                },
                {
                    "sent": "So why, why transpose is a big dividing matrix matrix of rank small M and this is the orthogonal projection matrix.",
                    "label": 0
                },
                {
                    "sent": "That's where this name derives from.",
                    "label": 0
                },
                {
                    "sent": "So the space of all orthonormal projection.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the space of D by the orthonormal projection matrices is a subset of Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "An natural inner product in that equation spaces trace of two matrices.",
                    "label": 1
                },
                {
                    "sent": "As we all know.",
                    "label": 0
                },
                {
                    "sent": "So this defines the projection kernel.",
                    "label": 0
                },
                {
                    "sent": "Projection of kernel.",
                    "label": 0
                },
                {
                    "sent": "Why, Why two is defined as the trace of these?",
                    "label": 0
                },
                {
                    "sent": "Why why, why interest both white white transpose, or equivalently the Frobenius norm squared awhile and transpose Y 2?",
                    "label": 0
                },
                {
                    "sent": "Is the Berry has a very simple form and easy to evaluate?",
                    "label": 1
                },
                {
                    "sent": "So this is the main kernel we want to promote to use.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's another kernel called unique wash kernel.",
                    "label": 0
                },
                {
                    "sent": "Proposed by Wolf and Viswanathan and others.",
                    "label": 0
                },
                {
                    "sent": "But let me just skip the construction of this kernel.",
                    "label": 0
                },
                {
                    "sent": "An that the final form of this kernel is.",
                    "label": 0
                },
                {
                    "sent": "K Why won't come on Y2 is determinant of wine, transpose Y 2 squared.",
                    "label": 0
                },
                {
                    "sent": "This has a different form, but it's also relatively easy to evaluate and is also useful kernel, but I'll compare the results with projection corner in the experimental section.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So advantage of having a valid kernel on grass and manifold is obvious.",
                    "label": 0
                },
                {
                    "sent": "We can use all the kernel machines for arbitrary task we want to do.",
                    "label": 0
                },
                {
                    "sent": "And once you have some nice, well defined kernel for this space, you can combine them to generate new kernels.",
                    "label": 0
                },
                {
                    "sent": "If you have new requirements.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "Another extension, the final extension to the original idea.",
                    "label": 1
                },
                {
                    "sent": "You can extend idea of subspace to non linear subspace.",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "So this part of the figure is something I already explained.",
                    "label": 0
                },
                {
                    "sent": "This silver Spade is defined through a grassman kernel.",
                    "label": 0
                },
                {
                    "sent": "But now if you add another stage in the front end.",
                    "label": 0
                },
                {
                    "sent": "And assume your data domain is not a directly vector space.",
                    "label": 0
                },
                {
                    "sent": "But it's some arbitrary set with some kernel, and this kernel defines a map to the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So if you compute the principle angles, distances, kernels on this in this Hilbert space, that effectively defines.",
                    "label": 0
                },
                {
                    "sent": "The nonlinear subspace in the original data set.",
                    "label": 1
                },
                {
                    "sent": "So in this way we can extend our work to non linear subspaces as well.",
                    "label": 0
                },
                {
                    "sent": "Just to be clear, so the kernel for this part under this part of it is to be distinguished.",
                    "label": 0
                },
                {
                    "sent": "There are different kernels.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as an application we're going to use Fisher discriminant analysis in conjunction with this grassman kernel.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to contrast our method with the baseline algorithm, which is a simple Euclidean Fisher discriminant analysis.",
                    "label": 0
                },
                {
                    "sent": "So with the same data we try to view it as a subspace and apply subspace based algorithms and also viewed as this simple Euclidean vectors and tried to do equally in FT algorithm on that.",
                    "label": 0
                },
                {
                    "sent": "And we're comparing our method with other methods.",
                    "label": 0
                },
                {
                    "sent": "Previous methods.",
                    "label": 0
                },
                {
                    "sent": "Let me call our approach, which is to use kernel efficiency analysis with projection or even a question kernel as Grassmann discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "To contrast it from the previous method, they use some heuristics to going back and forth from the data space and the grassman concept.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me explain the data set.",
                    "label": 0
                },
                {
                    "sent": "We're going to use.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "The Yale Face database consists of 80 three different persons, nine different poses and 45 different illumination conditions.",
                    "label": 1
                },
                {
                    "sent": "We're going to apply PCA along the direction of this illumination direction, so we want to make this problem this recognition problem, illumination invariant basically.",
                    "label": 0
                },
                {
                    "sent": "Is it in the training, training and test phase?",
                    "label": 0
                },
                {
                    "sent": "I'm going to hold out.",
                    "label": 0
                },
                {
                    "sent": "One pose of all the persons as a test set and use the remaining data set for training.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to repeat this for 9 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "The CMU PIE data set is all very similarly structured face database.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's another data set called E TH80 database, which consists of eight different categories, and each category has 10 objects, and each object is viewed 441 different poses.",
                    "label": 0
                },
                {
                    "sent": "We're going to apply a PCA along the post direction.",
                    "label": 1
                },
                {
                    "sent": "So in order to make this categorisation problem invariant to viewpoints.",
                    "label": 0
                },
                {
                    "sent": "We're also going to similarly apply tenfold cross validation by taking out one vertical slides from this data set.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the last data set we're going to use.",
                    "label": 0
                },
                {
                    "sent": "The exmas databases.",
                    "label": 0
                },
                {
                    "sent": "Database for human actions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Which consists of 11 different actions and 11 different actors.",
                    "label": 0
                },
                {
                    "sent": "If you remember, I'm going to estimate the linear dynamical system models for each time frames of these actions and uses for our test.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is actually our final page.",
                    "label": 0
                },
                {
                    "sent": "This is original from the four databases.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This the bars represent the recognition rate for different algorithms.",
                    "label": 0
                },
                {
                    "sent": "If you look at this group of bar, the first one is the result from the conventional Euclidean approach.",
                    "label": 0
                },
                {
                    "sent": "The second one is the proposed method and the others are competing algorithms and you can see all the rates vary.",
                    "label": 0
                },
                {
                    "sent": "Rates are different overall for different databases.",
                    "label": 0
                },
                {
                    "sent": "The second column, the light blue one, is achieved the best rates for all these databases.",
                    "label": 0
                },
                {
                    "sent": "By far, and one interesting point is.",
                    "label": 0
                },
                {
                    "sent": "The same algorithm with Vinny Cash kernel degrade very fast.",
                    "label": 0
                },
                {
                    "sent": "These are this Sky blue bars.",
                    "label": 0
                },
                {
                    "sent": "They all decrease fast as a subspace dimension increases and I have a good explanation for this.",
                    "label": 0
                },
                {
                    "sent": "But let me talk this in the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recession.",
                    "label": 0
                },
                {
                    "sent": "So let me summarize.",
                    "label": 0
                },
                {
                    "sent": "We're proposing a subspace based learning paradigm to exploit the data that has inherent linear structures in it.",
                    "label": 1
                },
                {
                    "sent": "And we use Grassman manifold as a common framework for this problem.",
                    "label": 0
                },
                {
                    "sent": "And the projection kernels expose shows very.",
                    "label": 0
                },
                {
                    "sent": "Nice performance compared to other previous known methods.",
                    "label": 0
                },
                {
                    "sent": "And this is a kernel method.",
                    "label": 0
                },
                {
                    "sent": "This is general building block for arbitrary.",
                    "label": 0
                },
                {
                    "sent": "Datasets, arbitrary method and arbitrary learning paradigm.",
                    "label": 0
                },
                {
                    "sent": "Let me finish with just one remaining question.",
                    "label": 0
                },
                {
                    "sent": "Which is if the projection kernel and any question kernel are the only only kernels for the grassman manifolds, and this is a remaining question where currently working on.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Spaces.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So the simplest extension to affine subspaces is to use homogeneous coordinate to absorb this.",
                    "label": 0
                },
                {
                    "sent": "Bias into this differentation.",
                    "label": 0
                },
                {
                    "sent": "Actually, this call is some problems for definition and we have some work in unpublished work.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the nice question.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional, did you say?",
                    "label": 0
                },
                {
                    "sent": "It's limited linear subspaces.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, the dimensionality is not a factor, yeah?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}