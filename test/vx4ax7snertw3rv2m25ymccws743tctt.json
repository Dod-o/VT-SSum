{
    "id": "vx4ax7snertw3rv2m25ymccws743tctt",
    "title": "Structured Output Prediction of Enzyme Function via Reaction Kernels",
    "info": {
        "author": [
            "Juho Rousu, Department of Computer Science, University of Helsinki"
        ],
        "published": "Oct. 14, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/solomon_rousu_sopef/",
    "segmentation": [
        [
            "It is my pleasure to do so.",
            "I first met you, I think about two years ago in Helsinki very early January, and the thing I remember at that point is that he was handling the MSE in Vaio informatics at at Helsinki Helsinki University.",
            "He's doing very interesting research on developing machine learning methods and their applications to bioinformatics problems and also.",
            "Bing assistance biology, and that's the official excuse for his coming here.",
            "He's also program Co.",
            "Chair of the Workshop on Machine Learning and Systems Biology, which will take place here at the Institute tomorrow and then on Sunday.",
            "Without much further ado, it's my great pleasure to welcome for also, OK, thank you so.",
            "Right so.",
            "This is a story.",
            "That about.",
            "Project that has been going on for quite a long time already.",
            "I think something like we started something like 2006 and.",
            "We trying to.",
            "Do new methods for prediction of enzyme function.",
            "So there is a.",
            "Hence there is a long list of people that have been involved in this already, but I'll test the mention.",
            "Got the astic on and who is the VSDS student who has done all the hard work.",
            "All the experiments going to show here.",
            "Right so."
        ],
        [
            "Oh I don't know how much you know about structured output prediction.",
            "So I'm going to give you kind of a short introduction who many how many of you have heard about it.",
            "Half 5050 OK so this should be.",
            "OK, so so structured output prediction is a family of matching learning methods where the aim is to predict complex object in kind of one shot rather than trying to predict the components of the object.",
            "Individually and then put these predictions together.",
            "Kind of in a post processing stuff.",
            "The point is that.",
            "Why this would be more kind of beneficial for predicting this complex complex objects is that the idea is that we try to utilize the structure of the object.",
            "Both to improve the accuracy.",
            "So if there are some dependencies in this complex object, whatever it is.",
            "These predictions shouldn't be independent, but they they should suit.",
            "Suit kind of benefit from each other and also we turn out.",
            "Turns out one can do this more more.",
            "Efficiently then?",
            "Kind of doing this kind of component predictions and and then putting them together.",
            "So.",
            "Kind of them.",
            "Kind of the breeding ground of this.",
            "Kind of research.",
            "Is this SVM support vector machine research where some aspects have been.",
            "Adopted.",
            "In particular.",
            "We are using same kind of optimization techniques as in support vector machines.",
            "So we have convex optimization problems.",
            "It can do things in dual representation so we can use kernels.",
            "It's not, you don't need to do it, so you can use your features, but you can also use use kernels.",
            "And we also have typically have some kind of margin maximization.",
            "Going on, not always.",
            "I will one of the methods I'm going to show.",
            "In this talk, does not have any.",
            "Anything like this at all?",
            "OK, this kind of methods have been applied to many kinds of problems already, so sequence, annotation, statistical machine translation, image segmentation, hierarchical classification, you name it.",
            "Very many very diverse drivers.",
            "Set of applications."
        ],
        [
            "OK so I'm just giving you couple of.",
            "Examples what we can try to do with this kind of techniques?",
            "So kind of, I guess the simplest structure again think of is a sequence, so at least to me it's hard to think of anything simpler.",
            "So so you have different kinds of problems that are can be defined as sequence to sequence learning.",
            "Statistical machine translation obviously is one of those problems.",
            "Very trust want to learn these correspondences between two sentences.",
            "So this is very challenging, challenging problem.",
            "We have actually have had this evil project on this.",
            "This topic called Smart.",
            "You may have heard of it may not.",
            "It's very, very challenging problem.",
            "To work on.",
            "So.",
            "Coming closer, the biology or kind of sequence annotation problems can be defined of this sequence to sequence learning.",
            "Something like this.",
            "Your input DNA sequence on your outlook to annotation.",
            "So what kind of components you find in this sequence?",
            "OK, so this is."
        ],
        [
            "One example, then this is the.",
            "So.",
            "Our pet pet problem of the past hierarchical multi label classification.",
            "Where we have documents?",
            "For example, like like this, there's this news document from BBC.",
            "Website.",
            "And there is kind of.",
            "Plus some hierarchy, some traction on me.",
            "According to which we are classifying this article, so this is supposedly about music and football.",
            "On all of the parent.",
            "Parent classes.",
            "And so this is the learning learning task that given the object like this, we are.",
            "Output in this kind of.",
            "Subtree of the whole hierarchy.",
            "So to speak."
        ],
        [
            "OK, so.",
            "The thieves were just going over warmup problems.",
            "So now we are going to kind of shows you the real topic of this talk.",
            "So our task is the following.",
            "We tried to.",
            "Predict the function of an enzyme from the from the sequence sequence of the of the inside so so we have as input.",
            "We have objects like this, so this is.",
            "I mean this kind of piece of amino acid sequence of a protein that works as a as an enzyme.",
            "So it catalyzes some chemical reaction.",
            "Inside cell.",
            "This up this piece happens to be from a protein that catalyzes this kind of reaction.",
            "OK, so this is of course a problem that biologists have been solving like.",
            "Seems to 1st 1st.",
            "Jeanswear sequenced.",
            "And they have of course, their favorite technique.",
            "How they do it.",
            "So it's called annotation transfer.",
            "They have a database of sequences and this this functions.",
            "When they acquire new sequence, they go through the database, look for.",
            "Very similar looking sequence and then they test Kopitar annotation and say OK this is the positive function of this movie new sequence.",
            "They may or may not go to the wet lab and verified.",
            "But but this is, this is how they.",
            "Typically operate.",
            "OK, so so machine learning people have been interested in this.",
            "Problem for some years already.",
            "So there are.",
            "Quite a many of different classifier systems that try to.",
            "Do something more.",
            "Clever or more general?",
            "So we have systems that try to try to.",
            "Predictor class.",
            "It can be.",
            "For example, this multi hierarchical classification task.",
            "So here you want to predict the bar.",
            "Label reactions.",
            "I train it.",
            "I train it with with this kind of information, so yeah.",
            "Yeah so.",
            "I think it will come clearer right later, but.",
            "Yes, please stop me if if it."
        ],
        [
            "If it doesn't.",
            "OK, So what are our goals here?",
            "So we still obviously improve the accuracy of existing.",
            "Tools.",
            "When we are classifying.",
            "Under remote homologies.",
            "So this is the thing with the protein function prediction that if you have a Houma logis sequence to sequence that is very close to your.",
            "New one, so your database already contains some very near neighbor of your sequence.",
            "Then this learning task is very easy and it's there's nothing to it.",
            "But if you don't, if your sequence has diverged.",
            "So by mutations from so that there is no near counterpart in the in the database then then it becomes challenging.",
            "And this is where we were concentrating.",
            "We're not interested in this home Ologist case because it's really this annotation transfer is it works good enough so there's nothing you can.",
            "You cannot even beat the nearest neighbor classifier in that setup.",
            "OK. And of course.",
            "We also have this kind of more far reaching goal.",
            "We would like to build models that.",
            "We could even predict hypothetical functions, so even though we there is a function that has, some has not been described at all.",
            "Our tool could somehow say OK, this sequence probably does something like this.",
            "With some some confidence, so we are not very close to this point, but this is kind of the long term.",
            "Long term call, like I say here, second goal is very ambitious, but reaching for it can give us.",
            "Improvement in this kind of easier and easier task.",
            "So this is kind of the motivation."
        ],
        [
            "Right OK so.",
            "Now that.",
            "Kind of the task has been set up I.",
            "Show you a little bit about what kind of systems the structure out of production.",
            "Models are.",
            "So this is.",
            "On one slide.",
            "Kind of covering almost all of the kernel based.",
            "Or let's say margin based structured output prediction models.",
            "So almost all of them do something like this.",
            "So you have your inputs, you have your outputs, you map them into joint feed space, so you have some kind of feature space defined for the Co occurrence of your inputs and outputs.",
            "And then in this space you'll have simple model.",
            "So typically a linear model.",
            "So you're just waiting with features of in that joint field suppress.",
            "There can be some offset sometimes.",
            "You can use kernels so you can.",
            "If you have kernels, you can have this take this inner product of this feature Maps instead of this.",
            "Originel feature Maps.",
            "In this talk, we will do this so we will have fully kernelized.",
            "Representation of the of the problem.",
            "To understand this whole.",
            "Both.",
            "He in boots yes yeah.",
            "The second part, which decomposes the kernel.",
            "Text my ex prime wife.",
            "Oh well this doesn't.",
            "OK this doesn't.",
            "Actually decompose, it's it's just the inner product between.",
            "OK, you have one sequence.",
            "One function I have another sequence, another function mapped into complex space and then I ask what is the inner product between these so it's just.",
            "So this doesn't.",
            "It's just the similarity of two pairs.",
            "Basically what this is.",
            "So sequence function, sequence function.",
            "And this is just the inner product.",
            "It's tells how close these two pairs are.",
            "Each to each other in this joint feed this place.",
            "Yeah, it's a similarity in this.",
            "Yeah yeah, so similarity in this space of these guys.",
            "Right and then?",
            "If when we have this core function learned, the prediction is.",
            "The arc marks overall possible wise.",
            "Of this guy.",
            "So we want to find the Y that has the higher score when we couple it with our new.",
            "X So we have news.",
            "Let's say new sequence.",
            "We want to find the function that scores the highest.",
            "This is our prediction, predict that.",
            "Function.",
            "Yeah, yeah, so so it depends.",
            "Some sometimes you can do something more clever so you can.",
            "You can actually compute things.",
            "You can have some combinatorial algorithm that.",
            "In without enumeration finds you the best Y, but for example, in our case we need to resolve this kind of brute force.",
            "We feed in different ways here.",
            "So it can take time.",
            "You can have a large space of wise that you need to try out.",
            "Yes.",
            "Is a complex.",
            "Yeah, yeah yeah yeah, that's very very hard.",
            "Very hard program but.",
            "Yeah."
        ],
        [
            "OK.",
            "So.",
            "So this is kind of the standard optimization problem that most of these frameworks use, so if you know SVM, this is like the objective is exactly like the SVM objective.",
            "You have some.",
            "Norm of the weight vector that you minimize some slack.",
            "And you minimized.",
            "There are some.",
            "And this constraints are not now much more complex than in SVM.",
            "That that's kind of where the difference to SVM comes.",
            "Um?",
            "So you don't need to worry too much, but this is the intuitive story about what these constraints says.",
            "It tries to push the score of the.",
            "Reference pairs, so we have.",
            "Have our training data.",
            "And we have something that occurs there.",
            "There is sequence and function.",
            "Couples come through pairings.",
            "Of things and then we have.",
            "Have some incorrect or possibly incorrect pairings so so we have something that exist in our training set in X.",
            "Side, but something that.",
            "May be different from our training.",
            "What are training data successed so.",
            "What we do here?",
            "We've tried to push the score of the truth through pairing.",
            "Above any.",
            "Kind of incorrect.",
            "Pairing so.",
            "So we want to find weight vector that allows us to do it.",
            "So this is.",
            "Kind of the.",
            "Where where is?",
            "Kind of this margin based learning comes here so we have.",
            "Basically 5 CCS.",
            "Probably if I draw it so we have we have this.",
            "Joint feed space.",
            "For despairs and we have some correct pair.",
            "Like here and then we have a set of.",
            "Incorrect pairs.",
            "Why?",
            "And what this basically these constraints try to say is that we want to have this correct the weight vector in this space.",
            "The point in the direction that pushes this guy.",
            "Kind of farthest along with this.",
            "This Red Tractor hoops and.",
            "So V. And we want to establish some.",
            "Margin there as well.",
            "So like in SVM you want to have some margin between things.",
            "So the same thing.",
            "Is.",
            "They tried to do the same thing here.",
            "So I want the correct guide to be on top and all the incorrect guys.",
            "Somewhere lower.",
            "How much slower depends on how incorrect these guys are, so you have some loss function that measures how incorrect is Y is in respect to Y one and this.",
            "Is kind of the how much margin we require.",
            "Probably, but.",
            "Well, you have the you have some you have correct ones and everything else you know of that is not mentioned as correct.",
            "You think it's?",
            "Includes out yeah, yeah and depends what your data is about.",
            "Maybe you can do this kind of.",
            "You can go through all of them, or maybe you cannot so many times.",
            "You just need to have some set.",
            "Some representative set of voice that you're dealing with.",
            "Yes.",
            "How you get the incorrect ones really is a matter of domain knowledge.",
            "Yeah, yeah.",
            "Assumption, but.",
            "Yes, that's that's that's always a bit of a problem problem.",
            "In some, like this hiring classification, I think you can usually be quite confident if somebody has labeled your document to be something.",
            "OK, you take something that is different from it probably is incorrect, but in some other domains like this engine functional prediction.",
            "It's not that clear.",
            "We can say something is not the function of this enzyme, so this is.",
            "Yes, exactly exactly.",
            "Yeah, this is.",
            "Yes, this is a bit of a bit of a problem problem an I guess this is maybe one of the weaknesses of this standard scheme that people are using that it really.",
            "Assumes that you know about this negative negative things, and this actually now.",
            "OK, so there is.",
            "Obviously solving this arc Max in this training phase is takes a lot of time, so this is the efficiency bottleneck.",
            "And like we discussed, there is some kind of a problem.",
            "Whether we actually learn the correct thing or not when we do this kind of thing, so this kind of nicely brings me to."
        ],
        [
            "To the next method.",
            "It actually doesn't care about this negative things at all, so.",
            "So what it does tries to do, it tries to.",
            "It looks at your.",
            "Scores of your training data and try to.",
            "Actually only find the direction of the weight vector so that this.",
            "Kind of the scores of your training data gets as high as possible, so if you think about.",
            "OK, so here we are worrying about this incorrect guys, but but in this setup the only one to find.",
            "Direction for the W so that.",
            "Our training data.",
            "Gets as high score as possible, so we just think about rotating this weight vector in that way that all of our training data gets.",
            "Away from the origin of the.",
            "Pizza space.",
            "OK, so it's.",
            "Quite a bit simpler.",
            "Learning learning tasks.",
            "Right?",
            "So actually it's if you know about one class SVM, this can be seen.",
            "So one class SVM in this joint feature space.",
            "So one class SVM does this.",
            "It takes unlabeled data, so without any only X data and tries to push your ex data out of away from the origin of the feature space.",
            "Anne.",
            "Then everything that is kind of.",
            "Close to your origin or on the negative side is then kind of outlier and.",
            "Kind of in the prediction phase in the product as.",
            "As kind of a negative.",
            "So this.",
            "Does this same thing in this joint joint feeds West?",
            "OK, so you can see that this is much simpler looking.",
            "Optimization problem there is no arc Max in the in the constraint set, so this.",
            "Is actually we can solve this very, very fast and this is scalable.",
            "Um?",
            "Oh, this is test the feature.",
            "This is just the search the score, so we have weight vector.",
            "And the feature feature map.",
            "And this is just.",
            "So there is no kernel at this point, so so it's just.",
            "I'm working in the original features, but yes, you can write this in the kernel.",
            "You can make the dual representation and put the kernel in there, but I haven't done it.",
            "Done it here.",
            "Right?",
            "And it turns out that many times.",
            "This, although it's much simpler, it works just as well as to as this one.",
            "K. Right?"
        ],
        [
            "OK, so somehow my talk is going to enter opposite direction of usually how scientific research kaweco towards more complex and complex things all the time, But my talk kind of kind of course simply ran similar things.",
            "So.",
            "OK, so.",
            "This is kind of a.",
            "Philosophy, so fickle thing that.",
            "So what do we have actually need for learning in this kind of features?",
            "Feature spaces, spaces?",
            "So we have two components that affects how well we can learn first.",
            "What kind of features do we have?",
            "What is our joint feature space?",
            "How does it look?",
            "So good features obviously allow us to learn then well.",
            "Then we have this way to vector on different ways of saying what is the code direction for the weight vector.",
            "So what are the code feature weights?",
            "So, so there were tests are giving some important to some features over some.",
            "Some other features.",
            "In this score.",
            "OK, so then one kind of maybe a little bit heretic.",
            "Question is OK, well can we just?",
            "For get this weight vector business altogether and test, you use to use the features or actually the joint kernel."
        ],
        [
            "For prediction.",
            "So this is something that we have very recently tried out, so we just take.",
            "Um?",
            "Access core function.",
            "Some of some of the Colonel.",
            "Kernel values of the training set set coupled with our whole new pair.",
            "So.",
            "So what does this mean?",
            "I think I'll.",
            "Need to draw another picture.",
            "Um?",
            "So this is now.",
            "OK, I have some my training data here again.",
            "XKYK and then I put some.",
            "Kernel on top of it, so I have some similarity metric there so, so let's say about some kind of Gaussians on top of.",
            "My data.",
            "So now I think I have very simple feature space so that I have only two dimensional feature space which is of course stupid, but it allows me to draw.",
            "So this is my new X.",
            "How do I find the best Y if I only have this this?",
            "Map of kind of density map of of my very complex joint features place so.",
            "I guess.",
            "The only thing I can do with only this data is to.",
            "Go through my so this is the Y. Y axis go through and see which Y ranks the highest.",
            "So if I.",
            "So this is.",
            "Yeah.",
            "And this is, this is what we are trying to do, so I have my.",
            "XI have my.",
            "Why?",
            "So I like here and this is just a score is just.",
            "Score for this coupling is touched.",
            "What what is?",
            "Our neighboring point is it's giving me.",
            "So and then I.",
            "Go and choose the wire that.",
            "Gives me the best score so.",
            "OK, and this is if we.",
            "Give some interpretation.",
            "What this would mean in this classification?",
            "But this is essentially a Parson window classifier.",
            "So you have some class densities in this feature space, and you go and look with class.",
            "Gives you the highest.",
            "Highest density, so it's very.",
            "Very simple, very simple way of.",
            "And there is no no wait vector involved at all, so you don't need to.",
            "You just need the kernel.",
            "OK, so.",
            "Right, so I've shown you three different ways of doing doing this, structured output prediction.",
            "Kind of the setup, but I haven't told you anything about the features we're using, so I'm going to."
        ],
        [
            "Next tell you something about them.",
            "OK. We have been mostly using this kind of tensor product of feature map, so we have which means that we have individual features feature map from inputs and outputs.",
            "And then we couple them by taking the products of the features.",
            "So we have.",
            "OK, any kind of sequence feed so as you can think of and we couple them with.",
            "Output features and kind of back of all of these coupled features is our joint.",
            "Join feature map.",
            "Oh no, this should be.",
            "Fly.",
            "Obviously.",
            "Yeah, system.",
            "Yeah OK, yeah so.",
            "OK, So what does this kind of mean?",
            "It means that we don't have.",
            "Any prior alignment information between the inputs and outputs.",
            "In some applications this is not a good assumption.",
            "For example sequence annotation.",
            "This is a very bad assumption that if you want to annotate the sequence that your annotation in this end of the sequence is as important.",
            "Or let's say that whatever you have a stamina asset of this end of the sequence would be as important to annotation.",
            "The other other end of the sequence as to.",
            "Kind of things closer by.",
            "So OK, so this is something that is not always a good assumption to make, but in some applications it's it's very.",
            "Nothing wrong with it.",
            "OK, so this.",
            "Kind of pizza representation is actually quite nice in the kernelized formulation.",
            "So your joint kernel becomes test elementwise product of your input and output output kernel.",
            "So this means computationally things get much easier than having this whole beast.",
            "Alone.",
            "OK, so now we know that we have some some kernel for sequences, some kernel folder reactions, an.",
            "And we just multiply them together."
        ],
        [
            "OK, so.",
            "Then I'll call truth to some kind of candidates that we haven't tried.",
            "More or less successfully in this this.",
            "Application.",
            "Of course, string kernel is something you value.",
            "When you're working with sequences.",
            "It's kind of obvious first candidate, so you have.",
            "You basically thieves will be Max of different kind of substrings.",
            "Can contain gaps, can contain some.",
            "Special characters.",
            "Um?",
            "And there are many algorithms that compute this string kernels and you can combine them in different kinds.",
            "You can take some kind of polynomial kernel on top of them.",
            "Or whatever.",
            "That's kind of the sad story here is that none of this actually works in function prediction, so this kind of.",
            "Looking at currencies of of some substrings that match is not a good strategy, so there is the reason seems to be that this there's too much mutation, and evolution has kind of.",
            "Done his job and there is too much variety that you could actually spot.",
            "Meaningful substrings.",
            "Oh yeah, this is the input side.",
            "Is this just the inputs?",
            "How?",
            "How I encode?",
            "How to encode the sequences in this in this feature?"
        ],
        [
            "OK, so this is something that actually works much better.",
            "We have this kind of so called.",
            "GDG cobalt racecraft by Lisa home.",
            "That is able to predict conserved residues in the sequences.",
            "And it works by actually taking the whole.",
            "World of protein sequences.",
            "So the all all sequences that have been listed in databases and does this massive all against all alignment for these sequences.",
            "So you have this kind of.",
            "Sequence against sequence.",
            "Alignments.",
            "And they build a craft alignment craft of of the so you have some amino acid in some sequence.",
            "If it's aligned again, some other, I mean acid in some other sequence, you have an edge in the graph.",
            "And when you have this kind of all all against all matching, you have.",
            "Have a contest.",
            "This kind of quite complex complex alignment graphs and.",
            "Then what you do to find this concert residues you look for this kind of consistent alignment that that you know that OK X is aligned against OK, whatever.",
            "This is why this I don't even see what it is, but the point is, these are kind of consistently aligned, so axis aligned against Y.",
            "And this is like this and this.",
            "So you have this kind of tight cluster.",
            "That all at this kind of pairwise alignments align them consistently.",
            "And then you say that if you have.",
            "The big cluster of things that align consistently tends to say OK.",
            "This residue must be.",
            "Konserve into evolution Becauses there are so many sequences that align consistently.",
            "So this kind of alignment will keep one feature.",
            "So then we will have this kind of.",
            "Feature vector there.",
            "Where each of these?",
            "Clusters give one feature, so when you have a sequence, for example this one.",
            "Is member of this cluster.",
            "This is 1 bit.",
            "Open for this sequence.",
            "Typically you don't get too many of these.",
            "This kind of per sequence, so it's quite sparse but very high dimensional feature.",
            "OK, so this is how the inputs actually are encoded in our work.",
            "OK."
        ],
        [
            "So in on output side you can do different things so.",
            "One very obvious way for protein function is to have some kind of hierarchy in the output side.",
            "So for example this.",
            "This is the easy hierarchy.",
            "How you encode it you have.",
            "4 levels and.",
            "If you have.",
            "For example, this is an enzyme that has two different functions, so you have this kind of path that diverges at the leaves.",
            "And you test encode bits that are on.",
            "On enter.",
            "Kind of.",
            "Highlighted on the on the.",
            "OK, so you can do something else like you can encode the edges instead of the nodes, But this is kind of a technical detail.",
            "I don't want to spend too much time on."
        ],
        [
            "OK, so there are many hierarchies for protein function.",
            "Enzyme enzymes function easy.",
            "Hierarchized is the oldest one.",
            "The other one we have used this gold standard hierarchy which is supposedly little bit better quality than easy hierarchy.",
            "Then you have some other like OK Go has has this kind of hierarchical.",
            "Structure then meets also on.",
            "There are some others as well.",
            "So this is just one.",
            "I guess I will compare against this so so so this is the.",
            "That's one way to represent out."
        ],
        [
            "But this is the.",
            "This is the reaction side that we are actually kind of now actively.",
            "Developing so.",
            "So we are.",
            "So thinking about this kind of.",
            "Set up her some substrate molecules.",
            "That are converted into some.",
            "Products.",
            "So I have here I have two substrates, two products.",
            "And how do?",
            "Do a feature map out of them so I will do this so I have.",
            "Compute.",
            "Some of the feature feature.",
            "Representations of all my input molecules, so I have this kind of cumulative spectrum of my inputs.",
            "And then I have the same thing for my outputs.",
            "And then I do the do the danger product between them.",
            "So it means that.",
            "I have some kind of feature vector for the input side, some feature vector for the output side and I do this coupling so I have.",
            "Here I will then have kind of.",
            "Products of 1 input feature.",
            "One output feature will keep them.",
            "You're cuter.",
            "Kind of combined spectrum.",
            "Further for the reaction.",
            "Um?",
            "Right?",
            "So then I. I'm kind of assuming here I have some feature Maps for my molecule so have some feature representation for my molecules and so that I'm relying on.",
            "It can be some kind of soft ground spectrum of the molecule or what kind of walks you can find from the molecule.",
            "This is it, so it's very, very simply just combining this kind of information from the individual model molecules from the.",
            "From the reaction.",
            "Um?",
            "You can write this as a kernel.",
            "I didn't do it here but but if you have a kernel between your molecules you can.",
            "This is also very easy, too easy to kind of derive from that information."
        ],
        [
            "OK.",
            "So one thing in this prediction thus becomes very important is to using the kernels.",
            "To the best.",
            "Kind of a way on.",
            "It turns out that we actually need to use this kind of polynomial kernel trick, so.",
            "With kernels you can, you can have any base kernel bit theory between your data and you can.",
            "Raise it to some polynomial degree.",
            "What this does?",
            "Kind of underlying it takes.",
            "Groups takes kind of.",
            "Groups of your features and makes kind of gone by combined combined features out of your.",
            "Out of your feature, so this will be.",
            "In the.",
            "Input side this will be groups of conserved residues.",
            "So our features will be groups of constant residues instead of individual.",
            "Concept residues.",
            "And on the output side this will will.",
            "Correspond to groups of molecule subgraphs that exist in the reaction.",
            "OK.",
            "I mean normally.",
            "Yeah.",
            "You can, you can have.",
            "You can have this kind of.",
            "Red test by putting plus one here.",
            "You can simulate this kind of setting that you have this individual guys and then any groups to the power of D. You can do that.",
            "I don't remember if I. I think we tried that, but in this application it wasn't."
        ],
        [
            "Wasn't working.",
            "OK, so some experiments before I run out of time.",
            "OK, so we have used this kind of five fold stratified close across the Holidays and the idea is that we want to.",
            "Similar to situation where we have new enzyme function, so new EC codes for the enzymes.",
            "So are whatever we have in test fold.",
            "Should not appear in the training trading false, so we're separating the test test easy codes from the training.",
            "Easiest VC codes, and so we have this kind of fivefold setting where there is no.",
            "Kind of redundancy.",
            "Between the between the faults.",
            "It means that.",
            "So we will have two folds.",
            "I can have.",
            "The first 3 digits can be can be Co occur in both side, but there not the four complete 4 digit code cannot, so that is how we have done it.",
            "OK.",
            "So we have some things we compare.",
            "Compare accounts, so this is the baseline plus that you know most biologists would use.",
            "This is the same but using this concept residue information.",
            "And then we have this MMR, an OK I didn't exclude.",
            "We also have this kernel density estimates on there.",
            "In prediction we are using this kind of trivial primitive algorithms, so we enumerate the set of candidate functions.",
            "And look for the best best scoring Y in this set.",
            "Uh.",
            "On this set is has taken from all data, so we have all functions we have seen in the in the.",
            "In in our data, is this set in principle?",
            "This set could be much wider, so you could for example take or reactions listed in keg as this set.",
            "We haven't done that, but this is of course fully possible.",
            "You can go and.",
            "Go through this whole list of functions in keg and.",
            "Take the best one of them.",
            "OK, so this."
        ],
        [
            "Is the set up.",
            "So here I show how this polynomial kernel helps us.",
            "Don't worry about the figure figures, only worry about the color here is when you don't put any polynomial.",
            "Kernel on top of your data, so then you're just using individual concert residues against subgraphs in your.",
            "In your molecules and when you when you're raised, go.",
            "Along this direction, you're raising the decree of the input kernel, so you're taking more and larger, larger and combinations of conserved residues as the features.",
            "Here you're taking larger logical combinations of of.",
            "Sub crops that you find in your reaction molecules.",
            "As you can see, you have this kind of initial.",
            "Steph, things are not not that good but.",
            "Something from, let's say, something like.",
            "OK, this line onwards everything is cool so you have have to have some kind of some degree of polynomial kernel on top of your data for best performance, But after that it doesn't really matter.",
            "So it's pretty Even so the maximum is somewhere here so.",
            "Actually, it looks like the output kernel needs little bit more higher diamonds and then then the input, but.",
            "OK."
        ],
        [
            "Right, OK, and this is the.",
            "This is the story about the accuracy of our methods.",
            "So what this plot does it shows you the distribution of.",
            "Uh.",
            "Enzymes where we kept.",
            "123 or 4 digits, correct?",
            "Um?",
            "And these are different methods we have.",
            "Blast is the baseline.",
            "Then there is the nearest neighbour with our GT data console residue data.",
            "And then the last three are different.",
            "Structured output methods so.",
            "This is the hierarchical classifier.",
            "And this is the.",
            "This MMR method.",
            "So.",
            "This this method.",
            "With reaction kernel and this is the kernel density estimation.",
            "So this method on the reaction kernel.",
            "So one thing you can see is that.",
            "OK, blast is not.",
            "Up there and this is to be expected.",
            "We have by taking out the.",
            "The function same functions default 4 digit in the easy easy class.",
            "We're taking out a lot of home ologie from the data and blast.",
            "This really needs this homological to work.",
            "So everything that relies on the concert residues does much better, so this four other methods that have this concept residue in from my service is much, much better.",
            "OK, So what else can we see?",
            "The hierarchical classifier is.",
            "Quite good.",
            "Predicting 3 digits.",
            "3C code and obviously cannot do anymore because in training phase it never has seen any examples where the.",
            "4 digit is correctly assigned, so it cannot really get anything here, so.",
            "It's really tight, but it does whatever it does.",
            "It does very well.",
            "So these two reaction reaction kernel.",
            "Methods actually can sometimes get poor digits correct?",
            "Not very often, so it seems.",
            "See you can see it.",
            "It's quite quite tough tough task.",
            "But what is maybe most interesting is that.",
            "This very simple method here.",
            "Is actually the best method to get one or more easy, that's correct.",
            "In this setup.",
            "So this was very much a surprise to us.",
            "We didn't expect expect this.",
            "This method worked as well, but.",
            "Sometimes you get this kind of surprises.",
            "Right, so it's just."
        ],
        [
            "Yes, so such to conclude.",
            "So we have shown you some.",
            "Reaction kernels are means to improve accuracy in this remote homology enzyme function.",
            "Prediction.",
            "Um?",
            "Present result seems to success that good kernels and good features matter a lot.",
            "But this kind is how to learn this hyperplane in this joint feature space.",
            "Basically how to learn your feature wage?",
            "Seems to not to matter that much.",
            "OK, so it's not very conclusive.",
            "Have made few experiments but but somehow things point to that direction.",
            "Um, OK also.",
            "OK, it seems that this structure learning or structured output prediction.",
            "Can be actually tackled with.",
            "This kind of quite simple approaches.",
            "As opposed to this quite complicated business of optimizing things against negative examples that you actually may not have at all.",
            "Right, so this brings me to end, so I take any questions you have.",
            "Thank you.",
            "Perfect timing.",
            "Right?",
            "Brother killed everybody, right?",
            "Free.",
            "The central question to me here seems how we need to get any performance on labels that you have not seen, yeah.",
            "Obviously you were getting at least some.",
            "Example correctly, there wasn't no no no so.",
            "Very well then I don't know what this is.",
            "Hold on baby.",
            "Go about step.",
            "Problem seems to be central.",
            "Yeah I. I think we have to think we have to leave the sequences behind.",
            "We need to call the Treaty structures of the proteins.",
            "I kind of feel that it says not possible from the sequences alone but but yeah.",
            "Moreover yeah.",
            "My question was more, what's the methodology right?",
            "Right, so yeah, I guess the question is that how much interpolation you can do in this kind of very complex feature space.",
            "And maybe you cannot do too much.",
            "But so this is.",
            "So this might be application dependent, so this actually might be bad application for doing any kind of interpolation people because biologists tell me that OK somehow.",
            "The space of enzymes is so bumpy that you cannot really do interpolation successfully.",
            "So maybe some internal some other application you could actually deploy it in.",
            "I can work with your people said was this motivated by some specific problem or it is just in general?",
            "He said general is it's a sample from Chegg actually yeah.",
            "Co operators.",
            "While it depends how I don't have any vet lab collaborators, but but I do have some that are kind of protein bioinformatics people, so yeah.",
            "So, for example, Lisa Holm is one of these."
        ],
        [
            "Tease.",
            "This kind of paper.",
            "Yeah.",
            "And what other?",
            "Other structure prediction problem of exactly this type generates new labels for other examples of this.",
            "Well, for example the the machine translation I mentioned is one of these things that you don't in your training data.",
            "You don't have all the sentences that can be presented to you, so you need to generalize in that space.",
            "I mean there there it would seem to me and only make this one more common.",
            "At the beginning you said the point is structured prediction is.",
            "Come directly yeah yeah yeah.",
            "Now, if you want to do with unseen things, I feel actually need some combination of both the local or component prediction and the global structure.",
            "Because you might have seen the individual words right.",
            "So if you are able to, right yeah.",
            "Yes, OK, yeah, I agree with you and you can kind of.",
            "Smuggle in this kind of component predictors into your system as features, so you can have some tool that gives you some prediction and take that as a feature so that kind of thing you can embed here.",
            "And for example this.",
            "This GDT features are kind of the kind of thing that there is quite of heavy processing that gives you this predicted conserved residues that we are relying on South.",
            "So you can do all kinds of things, But yeah, so.",
            "Yeah, this kind of distinction between what is.",
            "OK, very structured prediction.",
            "What is just predicting components is OK.",
            "It's not that clear.",
            "Yeah.",
            "Yeah yeah yeah yeah.",
            "Thank you, call again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is my pleasure to do so.",
                    "label": 0
                },
                {
                    "sent": "I first met you, I think about two years ago in Helsinki very early January, and the thing I remember at that point is that he was handling the MSE in Vaio informatics at at Helsinki Helsinki University.",
                    "label": 0
                },
                {
                    "sent": "He's doing very interesting research on developing machine learning methods and their applications to bioinformatics problems and also.",
                    "label": 0
                },
                {
                    "sent": "Bing assistance biology, and that's the official excuse for his coming here.",
                    "label": 0
                },
                {
                    "sent": "He's also program Co.",
                    "label": 0
                },
                {
                    "sent": "Chair of the Workshop on Machine Learning and Systems Biology, which will take place here at the Institute tomorrow and then on Sunday.",
                    "label": 0
                },
                {
                    "sent": "Without much further ado, it's my great pleasure to welcome for also, OK, thank you so.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "This is a story.",
                    "label": 0
                },
                {
                    "sent": "That about.",
                    "label": 0
                },
                {
                    "sent": "Project that has been going on for quite a long time already.",
                    "label": 0
                },
                {
                    "sent": "I think something like we started something like 2006 and.",
                    "label": 0
                },
                {
                    "sent": "We trying to.",
                    "label": 0
                },
                {
                    "sent": "Do new methods for prediction of enzyme function.",
                    "label": 1
                },
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "Hence there is a long list of people that have been involved in this already, but I'll test the mention.",
                    "label": 0
                },
                {
                    "sent": "Got the astic on and who is the VSDS student who has done all the hard work.",
                    "label": 0
                },
                {
                    "sent": "All the experiments going to show here.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh I don't know how much you know about structured output prediction.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give you kind of a short introduction who many how many of you have heard about it.",
                    "label": 0
                },
                {
                    "sent": "Half 5050 OK so this should be.",
                    "label": 0
                },
                {
                    "sent": "OK, so so structured output prediction is a family of matching learning methods where the aim is to predict complex object in kind of one shot rather than trying to predict the components of the object.",
                    "label": 1
                },
                {
                    "sent": "Individually and then put these predictions together.",
                    "label": 0
                },
                {
                    "sent": "Kind of in a post processing stuff.",
                    "label": 0
                },
                {
                    "sent": "The point is that.",
                    "label": 0
                },
                {
                    "sent": "Why this would be more kind of beneficial for predicting this complex complex objects is that the idea is that we try to utilize the structure of the object.",
                    "label": 1
                },
                {
                    "sent": "Both to improve the accuracy.",
                    "label": 0
                },
                {
                    "sent": "So if there are some dependencies in this complex object, whatever it is.",
                    "label": 0
                },
                {
                    "sent": "These predictions shouldn't be independent, but they they should suit.",
                    "label": 0
                },
                {
                    "sent": "Suit kind of benefit from each other and also we turn out.",
                    "label": 0
                },
                {
                    "sent": "Turns out one can do this more more.",
                    "label": 0
                },
                {
                    "sent": "Efficiently then?",
                    "label": 0
                },
                {
                    "sent": "Kind of doing this kind of component predictions and and then putting them together.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Kind of them.",
                    "label": 0
                },
                {
                    "sent": "Kind of the breeding ground of this.",
                    "label": 1
                },
                {
                    "sent": "Kind of research.",
                    "label": 0
                },
                {
                    "sent": "Is this SVM support vector machine research where some aspects have been.",
                    "label": 0
                },
                {
                    "sent": "Adopted.",
                    "label": 1
                },
                {
                    "sent": "In particular.",
                    "label": 1
                },
                {
                    "sent": "We are using same kind of optimization techniques as in support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So we have convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "It can do things in dual representation so we can use kernels.",
                    "label": 0
                },
                {
                    "sent": "It's not, you don't need to do it, so you can use your features, but you can also use use kernels.",
                    "label": 0
                },
                {
                    "sent": "And we also have typically have some kind of margin maximization.",
                    "label": 0
                },
                {
                    "sent": "Going on, not always.",
                    "label": 0
                },
                {
                    "sent": "I will one of the methods I'm going to show.",
                    "label": 0
                },
                {
                    "sent": "In this talk, does not have any.",
                    "label": 0
                },
                {
                    "sent": "Anything like this at all?",
                    "label": 1
                },
                {
                    "sent": "OK, this kind of methods have been applied to many kinds of problems already, so sequence, annotation, statistical machine translation, image segmentation, hierarchical classification, you name it.",
                    "label": 0
                },
                {
                    "sent": "Very many very diverse drivers.",
                    "label": 0
                },
                {
                    "sent": "Set of applications.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I'm just giving you couple of.",
                    "label": 0
                },
                {
                    "sent": "Examples what we can try to do with this kind of techniques?",
                    "label": 0
                },
                {
                    "sent": "So kind of, I guess the simplest structure again think of is a sequence, so at least to me it's hard to think of anything simpler.",
                    "label": 0
                },
                {
                    "sent": "So so you have different kinds of problems that are can be defined as sequence to sequence learning.",
                    "label": 0
                },
                {
                    "sent": "Statistical machine translation obviously is one of those problems.",
                    "label": 1
                },
                {
                    "sent": "Very trust want to learn these correspondences between two sentences.",
                    "label": 0
                },
                {
                    "sent": "So this is very challenging, challenging problem.",
                    "label": 0
                },
                {
                    "sent": "We have actually have had this evil project on this.",
                    "label": 0
                },
                {
                    "sent": "This topic called Smart.",
                    "label": 0
                },
                {
                    "sent": "You may have heard of it may not.",
                    "label": 0
                },
                {
                    "sent": "It's very, very challenging problem.",
                    "label": 0
                },
                {
                    "sent": "To work on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Coming closer, the biology or kind of sequence annotation problems can be defined of this sequence to sequence learning.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "Your input DNA sequence on your outlook to annotation.",
                    "label": 0
                },
                {
                    "sent": "So what kind of components you find in this sequence?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One example, then this is the.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our pet pet problem of the past hierarchical multi label classification.",
                    "label": 0
                },
                {
                    "sent": "Where we have documents?",
                    "label": 0
                },
                {
                    "sent": "For example, like like this, there's this news document from BBC.",
                    "label": 0
                },
                {
                    "sent": "Website.",
                    "label": 0
                },
                {
                    "sent": "And there is kind of.",
                    "label": 0
                },
                {
                    "sent": "Plus some hierarchy, some traction on me.",
                    "label": 0
                },
                {
                    "sent": "According to which we are classifying this article, so this is supposedly about music and football.",
                    "label": 0
                },
                {
                    "sent": "On all of the parent.",
                    "label": 0
                },
                {
                    "sent": "Parent classes.",
                    "label": 0
                },
                {
                    "sent": "And so this is the learning learning task that given the object like this, we are.",
                    "label": 0
                },
                {
                    "sent": "Output in this kind of.",
                    "label": 0
                },
                {
                    "sent": "Subtree of the whole hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So to speak.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The thieves were just going over warmup problems.",
                    "label": 0
                },
                {
                    "sent": "So now we are going to kind of shows you the real topic of this talk.",
                    "label": 0
                },
                {
                    "sent": "So our task is the following.",
                    "label": 0
                },
                {
                    "sent": "We tried to.",
                    "label": 0
                },
                {
                    "sent": "Predict the function of an enzyme from the from the sequence sequence of the of the inside so so we have as input.",
                    "label": 1
                },
                {
                    "sent": "We have objects like this, so this is.",
                    "label": 0
                },
                {
                    "sent": "I mean this kind of piece of amino acid sequence of a protein that works as a as an enzyme.",
                    "label": 0
                },
                {
                    "sent": "So it catalyzes some chemical reaction.",
                    "label": 0
                },
                {
                    "sent": "Inside cell.",
                    "label": 0
                },
                {
                    "sent": "This up this piece happens to be from a protein that catalyzes this kind of reaction.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is of course a problem that biologists have been solving like.",
                    "label": 0
                },
                {
                    "sent": "Seems to 1st 1st.",
                    "label": 0
                },
                {
                    "sent": "Jeanswear sequenced.",
                    "label": 0
                },
                {
                    "sent": "And they have of course, their favorite technique.",
                    "label": 0
                },
                {
                    "sent": "How they do it.",
                    "label": 0
                },
                {
                    "sent": "So it's called annotation transfer.",
                    "label": 0
                },
                {
                    "sent": "They have a database of sequences and this this functions.",
                    "label": 0
                },
                {
                    "sent": "When they acquire new sequence, they go through the database, look for.",
                    "label": 0
                },
                {
                    "sent": "Very similar looking sequence and then they test Kopitar annotation and say OK this is the positive function of this movie new sequence.",
                    "label": 0
                },
                {
                    "sent": "They may or may not go to the wet lab and verified.",
                    "label": 0
                },
                {
                    "sent": "But but this is, this is how they.",
                    "label": 0
                },
                {
                    "sent": "Typically operate.",
                    "label": 0
                },
                {
                    "sent": "OK, so so machine learning people have been interested in this.",
                    "label": 0
                },
                {
                    "sent": "Problem for some years already.",
                    "label": 0
                },
                {
                    "sent": "So there are.",
                    "label": 0
                },
                {
                    "sent": "Quite a many of different classifier systems that try to.",
                    "label": 0
                },
                {
                    "sent": "Do something more.",
                    "label": 0
                },
                {
                    "sent": "Clever or more general?",
                    "label": 0
                },
                {
                    "sent": "So we have systems that try to try to.",
                    "label": 0
                },
                {
                    "sent": "Predictor class.",
                    "label": 0
                },
                {
                    "sent": "It can be.",
                    "label": 0
                },
                {
                    "sent": "For example, this multi hierarchical classification task.",
                    "label": 0
                },
                {
                    "sent": "So here you want to predict the bar.",
                    "label": 0
                },
                {
                    "sent": "Label reactions.",
                    "label": 0
                },
                {
                    "sent": "I train it.",
                    "label": 0
                },
                {
                    "sent": "I train it with with this kind of information, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "I think it will come clearer right later, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, please stop me if if it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If it doesn't.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are our goals here?",
                    "label": 0
                },
                {
                    "sent": "So we still obviously improve the accuracy of existing.",
                    "label": 0
                },
                {
                    "sent": "Tools.",
                    "label": 0
                },
                {
                    "sent": "When we are classifying.",
                    "label": 0
                },
                {
                    "sent": "Under remote homologies.",
                    "label": 0
                },
                {
                    "sent": "So this is the thing with the protein function prediction that if you have a Houma logis sequence to sequence that is very close to your.",
                    "label": 0
                },
                {
                    "sent": "New one, so your database already contains some very near neighbor of your sequence.",
                    "label": 0
                },
                {
                    "sent": "Then this learning task is very easy and it's there's nothing to it.",
                    "label": 0
                },
                {
                    "sent": "But if you don't, if your sequence has diverged.",
                    "label": 0
                },
                {
                    "sent": "So by mutations from so that there is no near counterpart in the in the database then then it becomes challenging.",
                    "label": 0
                },
                {
                    "sent": "And this is where we were concentrating.",
                    "label": 0
                },
                {
                    "sent": "We're not interested in this home Ologist case because it's really this annotation transfer is it works good enough so there's nothing you can.",
                    "label": 0
                },
                {
                    "sent": "You cannot even beat the nearest neighbor classifier in that setup.",
                    "label": 0
                },
                {
                    "sent": "OK. And of course.",
                    "label": 0
                },
                {
                    "sent": "We also have this kind of more far reaching goal.",
                    "label": 0
                },
                {
                    "sent": "We would like to build models that.",
                    "label": 1
                },
                {
                    "sent": "We could even predict hypothetical functions, so even though we there is a function that has, some has not been described at all.",
                    "label": 0
                },
                {
                    "sent": "Our tool could somehow say OK, this sequence probably does something like this.",
                    "label": 0
                },
                {
                    "sent": "With some some confidence, so we are not very close to this point, but this is kind of the long term.",
                    "label": 0
                },
                {
                    "sent": "Long term call, like I say here, second goal is very ambitious, but reaching for it can give us.",
                    "label": 1
                },
                {
                    "sent": "Improvement in this kind of easier and easier task.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the motivation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right OK so.",
                    "label": 0
                },
                {
                    "sent": "Now that.",
                    "label": 0
                },
                {
                    "sent": "Kind of the task has been set up I.",
                    "label": 0
                },
                {
                    "sent": "Show you a little bit about what kind of systems the structure out of production.",
                    "label": 0
                },
                {
                    "sent": "Models are.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "On one slide.",
                    "label": 0
                },
                {
                    "sent": "Kind of covering almost all of the kernel based.",
                    "label": 0
                },
                {
                    "sent": "Or let's say margin based structured output prediction models.",
                    "label": 1
                },
                {
                    "sent": "So almost all of them do something like this.",
                    "label": 0
                },
                {
                    "sent": "So you have your inputs, you have your outputs, you map them into joint feed space, so you have some kind of feature space defined for the Co occurrence of your inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "And then in this space you'll have simple model.",
                    "label": 0
                },
                {
                    "sent": "So typically a linear model.",
                    "label": 0
                },
                {
                    "sent": "So you're just waiting with features of in that joint field suppress.",
                    "label": 0
                },
                {
                    "sent": "There can be some offset sometimes.",
                    "label": 0
                },
                {
                    "sent": "You can use kernels so you can.",
                    "label": 0
                },
                {
                    "sent": "If you have kernels, you can have this take this inner product of this feature Maps instead of this.",
                    "label": 0
                },
                {
                    "sent": "Originel feature Maps.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we will do this so we will have fully kernelized.",
                    "label": 0
                },
                {
                    "sent": "Representation of the of the problem.",
                    "label": 0
                },
                {
                    "sent": "To understand this whole.",
                    "label": 0
                },
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "He in boots yes yeah.",
                    "label": 0
                },
                {
                    "sent": "The second part, which decomposes the kernel.",
                    "label": 0
                },
                {
                    "sent": "Text my ex prime wife.",
                    "label": 0
                },
                {
                    "sent": "Oh well this doesn't.",
                    "label": 0
                },
                {
                    "sent": "OK this doesn't.",
                    "label": 0
                },
                {
                    "sent": "Actually decompose, it's it's just the inner product between.",
                    "label": 0
                },
                {
                    "sent": "OK, you have one sequence.",
                    "label": 0
                },
                {
                    "sent": "One function I have another sequence, another function mapped into complex space and then I ask what is the inner product between these so it's just.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's just the similarity of two pairs.",
                    "label": 0
                },
                {
                    "sent": "Basically what this is.",
                    "label": 0
                },
                {
                    "sent": "So sequence function, sequence function.",
                    "label": 0
                },
                {
                    "sent": "And this is just the inner product.",
                    "label": 0
                },
                {
                    "sent": "It's tells how close these two pairs are.",
                    "label": 0
                },
                {
                    "sent": "Each to each other in this joint feed this place.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a similarity in this.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so similarity in this space of these guys.",
                    "label": 0
                },
                {
                    "sent": "Right and then?",
                    "label": 0
                },
                {
                    "sent": "If when we have this core function learned, the prediction is.",
                    "label": 0
                },
                {
                    "sent": "The arc marks overall possible wise.",
                    "label": 0
                },
                {
                    "sent": "Of this guy.",
                    "label": 0
                },
                {
                    "sent": "So we want to find the Y that has the higher score when we couple it with our new.",
                    "label": 0
                },
                {
                    "sent": "X So we have news.",
                    "label": 0
                },
                {
                    "sent": "Let's say new sequence.",
                    "label": 0
                },
                {
                    "sent": "We want to find the function that scores the highest.",
                    "label": 0
                },
                {
                    "sent": "This is our prediction, predict that.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so so it depends.",
                    "label": 0
                },
                {
                    "sent": "Some sometimes you can do something more clever so you can.",
                    "label": 0
                },
                {
                    "sent": "You can actually compute things.",
                    "label": 0
                },
                {
                    "sent": "You can have some combinatorial algorithm that.",
                    "label": 0
                },
                {
                    "sent": "In without enumeration finds you the best Y, but for example, in our case we need to resolve this kind of brute force.",
                    "label": 0
                },
                {
                    "sent": "We feed in different ways here.",
                    "label": 0
                },
                {
                    "sent": "So it can take time.",
                    "label": 0
                },
                {
                    "sent": "You can have a large space of wise that you need to try out.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is a complex.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah yeah, that's very very hard.",
                    "label": 0
                },
                {
                    "sent": "Very hard program but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the standard optimization problem that most of these frameworks use, so if you know SVM, this is like the objective is exactly like the SVM objective.",
                    "label": 0
                },
                {
                    "sent": "You have some.",
                    "label": 0
                },
                {
                    "sent": "Norm of the weight vector that you minimize some slack.",
                    "label": 0
                },
                {
                    "sent": "And you minimized.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "And this constraints are not now much more complex than in SVM.",
                    "label": 0
                },
                {
                    "sent": "That that's kind of where the difference to SVM comes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So you don't need to worry too much, but this is the intuitive story about what these constraints says.",
                    "label": 0
                },
                {
                    "sent": "It tries to push the score of the.",
                    "label": 1
                },
                {
                    "sent": "Reference pairs, so we have.",
                    "label": 0
                },
                {
                    "sent": "Have our training data.",
                    "label": 0
                },
                {
                    "sent": "And we have something that occurs there.",
                    "label": 0
                },
                {
                    "sent": "There is sequence and function.",
                    "label": 0
                },
                {
                    "sent": "Couples come through pairings.",
                    "label": 0
                },
                {
                    "sent": "Of things and then we have.",
                    "label": 0
                },
                {
                    "sent": "Have some incorrect or possibly incorrect pairings so so we have something that exist in our training set in X.",
                    "label": 0
                },
                {
                    "sent": "Side, but something that.",
                    "label": 0
                },
                {
                    "sent": "May be different from our training.",
                    "label": 0
                },
                {
                    "sent": "What are training data successed so.",
                    "label": 0
                },
                {
                    "sent": "What we do here?",
                    "label": 0
                },
                {
                    "sent": "We've tried to push the score of the truth through pairing.",
                    "label": 0
                },
                {
                    "sent": "Above any.",
                    "label": 0
                },
                {
                    "sent": "Kind of incorrect.",
                    "label": 0
                },
                {
                    "sent": "Pairing so.",
                    "label": 0
                },
                {
                    "sent": "So we want to find weight vector that allows us to do it.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Kind of the.",
                    "label": 0
                },
                {
                    "sent": "Where where is?",
                    "label": 0
                },
                {
                    "sent": "Kind of this margin based learning comes here so we have.",
                    "label": 0
                },
                {
                    "sent": "Basically 5 CCS.",
                    "label": 0
                },
                {
                    "sent": "Probably if I draw it so we have we have this.",
                    "label": 0
                },
                {
                    "sent": "Joint feed space.",
                    "label": 0
                },
                {
                    "sent": "For despairs and we have some correct pair.",
                    "label": 0
                },
                {
                    "sent": "Like here and then we have a set of.",
                    "label": 0
                },
                {
                    "sent": "Incorrect pairs.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "And what this basically these constraints try to say is that we want to have this correct the weight vector in this space.",
                    "label": 0
                },
                {
                    "sent": "The point in the direction that pushes this guy.",
                    "label": 0
                },
                {
                    "sent": "Kind of farthest along with this.",
                    "label": 0
                },
                {
                    "sent": "This Red Tractor hoops and.",
                    "label": 0
                },
                {
                    "sent": "So V. And we want to establish some.",
                    "label": 0
                },
                {
                    "sent": "Margin there as well.",
                    "label": 0
                },
                {
                    "sent": "So like in SVM you want to have some margin between things.",
                    "label": 0
                },
                {
                    "sent": "So the same thing.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "They tried to do the same thing here.",
                    "label": 0
                },
                {
                    "sent": "So I want the correct guide to be on top and all the incorrect guys.",
                    "label": 0
                },
                {
                    "sent": "Somewhere lower.",
                    "label": 0
                },
                {
                    "sent": "How much slower depends on how incorrect these guys are, so you have some loss function that measures how incorrect is Y is in respect to Y one and this.",
                    "label": 0
                },
                {
                    "sent": "Is kind of the how much margin we require.",
                    "label": 0
                },
                {
                    "sent": "Probably, but.",
                    "label": 0
                },
                {
                    "sent": "Well, you have the you have some you have correct ones and everything else you know of that is not mentioned as correct.",
                    "label": 0
                },
                {
                    "sent": "You think it's?",
                    "label": 0
                },
                {
                    "sent": "Includes out yeah, yeah and depends what your data is about.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can do this kind of.",
                    "label": 0
                },
                {
                    "sent": "You can go through all of them, or maybe you cannot so many times.",
                    "label": 0
                },
                {
                    "sent": "You just need to have some set.",
                    "label": 0
                },
                {
                    "sent": "Some representative set of voice that you're dealing with.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "How you get the incorrect ones really is a matter of domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Assumption, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's that's that's always a bit of a problem problem.",
                    "label": 0
                },
                {
                    "sent": "In some, like this hiring classification, I think you can usually be quite confident if somebody has labeled your document to be something.",
                    "label": 0
                },
                {
                    "sent": "OK, you take something that is different from it probably is incorrect, but in some other domains like this engine functional prediction.",
                    "label": 0
                },
                {
                    "sent": "It's not that clear.",
                    "label": 0
                },
                {
                    "sent": "We can say something is not the function of this enzyme, so this is.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is a bit of a bit of a problem problem an I guess this is maybe one of the weaknesses of this standard scheme that people are using that it really.",
                    "label": 0
                },
                {
                    "sent": "Assumes that you know about this negative negative things, and this actually now.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is.",
                    "label": 0
                },
                {
                    "sent": "Obviously solving this arc Max in this training phase is takes a lot of time, so this is the efficiency bottleneck.",
                    "label": 0
                },
                {
                    "sent": "And like we discussed, there is some kind of a problem.",
                    "label": 0
                },
                {
                    "sent": "Whether we actually learn the correct thing or not when we do this kind of thing, so this kind of nicely brings me to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the next method.",
                    "label": 0
                },
                {
                    "sent": "It actually doesn't care about this negative things at all, so.",
                    "label": 0
                },
                {
                    "sent": "So what it does tries to do, it tries to.",
                    "label": 0
                },
                {
                    "sent": "It looks at your.",
                    "label": 0
                },
                {
                    "sent": "Scores of your training data and try to.",
                    "label": 0
                },
                {
                    "sent": "Actually only find the direction of the weight vector so that this.",
                    "label": 0
                },
                {
                    "sent": "Kind of the scores of your training data gets as high as possible, so if you think about.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we are worrying about this incorrect guys, but but in this setup the only one to find.",
                    "label": 0
                },
                {
                    "sent": "Direction for the W so that.",
                    "label": 0
                },
                {
                    "sent": "Our training data.",
                    "label": 0
                },
                {
                    "sent": "Gets as high score as possible, so we just think about rotating this weight vector in that way that all of our training data gets.",
                    "label": 0
                },
                {
                    "sent": "Away from the origin of the.",
                    "label": 1
                },
                {
                    "sent": "Pizza space.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit simpler.",
                    "label": 0
                },
                {
                    "sent": "Learning learning tasks.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So actually it's if you know about one class SVM, this can be seen.",
                    "label": 0
                },
                {
                    "sent": "So one class SVM in this joint feature space.",
                    "label": 1
                },
                {
                    "sent": "So one class SVM does this.",
                    "label": 0
                },
                {
                    "sent": "It takes unlabeled data, so without any only X data and tries to push your ex data out of away from the origin of the feature space.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then everything that is kind of.",
                    "label": 0
                },
                {
                    "sent": "Close to your origin or on the negative side is then kind of outlier and.",
                    "label": 0
                },
                {
                    "sent": "Kind of in the prediction phase in the product as.",
                    "label": 0
                },
                {
                    "sent": "As kind of a negative.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Does this same thing in this joint joint feeds West?",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see that this is much simpler looking.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem there is no arc Max in the in the constraint set, so this.",
                    "label": 0
                },
                {
                    "sent": "Is actually we can solve this very, very fast and this is scalable.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Oh, this is test the feature.",
                    "label": 0
                },
                {
                    "sent": "This is just the search the score, so we have weight vector.",
                    "label": 0
                },
                {
                    "sent": "And the feature feature map.",
                    "label": 0
                },
                {
                    "sent": "And this is just.",
                    "label": 0
                },
                {
                    "sent": "So there is no kernel at this point, so so it's just.",
                    "label": 0
                },
                {
                    "sent": "I'm working in the original features, but yes, you can write this in the kernel.",
                    "label": 0
                },
                {
                    "sent": "You can make the dual representation and put the kernel in there, but I haven't done it.",
                    "label": 0
                },
                {
                    "sent": "Done it here.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that many times.",
                    "label": 0
                },
                {
                    "sent": "This, although it's much simpler, it works just as well as to as this one.",
                    "label": 0
                },
                {
                    "sent": "K. Right?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so somehow my talk is going to enter opposite direction of usually how scientific research kaweco towards more complex and complex things all the time, But my talk kind of kind of course simply ran similar things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a.",
                    "label": 0
                },
                {
                    "sent": "Philosophy, so fickle thing that.",
                    "label": 0
                },
                {
                    "sent": "So what do we have actually need for learning in this kind of features?",
                    "label": 0
                },
                {
                    "sent": "Feature spaces, spaces?",
                    "label": 0
                },
                {
                    "sent": "So we have two components that affects how well we can learn first.",
                    "label": 1
                },
                {
                    "sent": "What kind of features do we have?",
                    "label": 1
                },
                {
                    "sent": "What is our joint feature space?",
                    "label": 0
                },
                {
                    "sent": "How does it look?",
                    "label": 0
                },
                {
                    "sent": "So good features obviously allow us to learn then well.",
                    "label": 0
                },
                {
                    "sent": "Then we have this way to vector on different ways of saying what is the code direction for the weight vector.",
                    "label": 1
                },
                {
                    "sent": "So what are the code feature weights?",
                    "label": 0
                },
                {
                    "sent": "So, so there were tests are giving some important to some features over some.",
                    "label": 0
                },
                {
                    "sent": "Some other features.",
                    "label": 1
                },
                {
                    "sent": "In this score.",
                    "label": 0
                },
                {
                    "sent": "OK, so then one kind of maybe a little bit heretic.",
                    "label": 0
                },
                {
                    "sent": "Question is OK, well can we just?",
                    "label": 0
                },
                {
                    "sent": "For get this weight vector business altogether and test, you use to use the features or actually the joint kernel.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is something that we have very recently tried out, so we just take.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Access core function.",
                    "label": 0
                },
                {
                    "sent": "Some of some of the Colonel.",
                    "label": 0
                },
                {
                    "sent": "Kernel values of the training set set coupled with our whole new pair.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "I think I'll.",
                    "label": 0
                },
                {
                    "sent": "Need to draw another picture.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is now.",
                    "label": 0
                },
                {
                    "sent": "OK, I have some my training data here again.",
                    "label": 0
                },
                {
                    "sent": "XKYK and then I put some.",
                    "label": 0
                },
                {
                    "sent": "Kernel on top of it, so I have some similarity metric there so, so let's say about some kind of Gaussians on top of.",
                    "label": 0
                },
                {
                    "sent": "My data.",
                    "label": 0
                },
                {
                    "sent": "So now I think I have very simple feature space so that I have only two dimensional feature space which is of course stupid, but it allows me to draw.",
                    "label": 0
                },
                {
                    "sent": "So this is my new X.",
                    "label": 0
                },
                {
                    "sent": "How do I find the best Y if I only have this this?",
                    "label": 0
                },
                {
                    "sent": "Map of kind of density map of of my very complex joint features place so.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "The only thing I can do with only this data is to.",
                    "label": 0
                },
                {
                    "sent": "Go through my so this is the Y. Y axis go through and see which Y ranks the highest.",
                    "label": 0
                },
                {
                    "sent": "So if I.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And this is, this is what we are trying to do, so I have my.",
                    "label": 0
                },
                {
                    "sent": "XI have my.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "So I like here and this is just a score is just.",
                    "label": 0
                },
                {
                    "sent": "Score for this coupling is touched.",
                    "label": 0
                },
                {
                    "sent": "What what is?",
                    "label": 0
                },
                {
                    "sent": "Our neighboring point is it's giving me.",
                    "label": 0
                },
                {
                    "sent": "So and then I.",
                    "label": 0
                },
                {
                    "sent": "Go and choose the wire that.",
                    "label": 0
                },
                {
                    "sent": "Gives me the best score so.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is if we.",
                    "label": 0
                },
                {
                    "sent": "Give some interpretation.",
                    "label": 0
                },
                {
                    "sent": "What this would mean in this classification?",
                    "label": 0
                },
                {
                    "sent": "But this is essentially a Parson window classifier.",
                    "label": 1
                },
                {
                    "sent": "So you have some class densities in this feature space, and you go and look with class.",
                    "label": 0
                },
                {
                    "sent": "Gives you the highest.",
                    "label": 0
                },
                {
                    "sent": "Highest density, so it's very.",
                    "label": 0
                },
                {
                    "sent": "Very simple, very simple way of.",
                    "label": 0
                },
                {
                    "sent": "And there is no no wait vector involved at all, so you don't need to.",
                    "label": 0
                },
                {
                    "sent": "You just need the kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Right, so I've shown you three different ways of doing doing this, structured output prediction.",
                    "label": 0
                },
                {
                    "sent": "Kind of the setup, but I haven't told you anything about the features we're using, so I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next tell you something about them.",
                    "label": 0
                },
                {
                    "sent": "OK. We have been mostly using this kind of tensor product of feature map, so we have which means that we have individual features feature map from inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "And then we couple them by taking the products of the features.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "OK, any kind of sequence feed so as you can think of and we couple them with.",
                    "label": 0
                },
                {
                    "sent": "Output features and kind of back of all of these coupled features is our joint.",
                    "label": 0
                },
                {
                    "sent": "Join feature map.",
                    "label": 0
                },
                {
                    "sent": "Oh no, this should be.",
                    "label": 0
                },
                {
                    "sent": "Fly.",
                    "label": 0
                },
                {
                    "sent": "Obviously.",
                    "label": 0
                },
                {
                    "sent": "Yeah, system.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, yeah so.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does this kind of mean?",
                    "label": 0
                },
                {
                    "sent": "It means that we don't have.",
                    "label": 0
                },
                {
                    "sent": "Any prior alignment information between the inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "In some applications this is not a good assumption.",
                    "label": 0
                },
                {
                    "sent": "For example sequence annotation.",
                    "label": 0
                },
                {
                    "sent": "This is a very bad assumption that if you want to annotate the sequence that your annotation in this end of the sequence is as important.",
                    "label": 0
                },
                {
                    "sent": "Or let's say that whatever you have a stamina asset of this end of the sequence would be as important to annotation.",
                    "label": 0
                },
                {
                    "sent": "The other other end of the sequence as to.",
                    "label": 0
                },
                {
                    "sent": "Kind of things closer by.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this is something that is not always a good assumption to make, but in some applications it's it's very.",
                    "label": 0
                },
                {
                    "sent": "Nothing wrong with it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Kind of pizza representation is actually quite nice in the kernelized formulation.",
                    "label": 1
                },
                {
                    "sent": "So your joint kernel becomes test elementwise product of your input and output output kernel.",
                    "label": 0
                },
                {
                    "sent": "So this means computationally things get much easier than having this whole beast.",
                    "label": 0
                },
                {
                    "sent": "Alone.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we know that we have some some kernel for sequences, some kernel folder reactions, an.",
                    "label": 0
                },
                {
                    "sent": "And we just multiply them together.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Then I'll call truth to some kind of candidates that we haven't tried.",
                    "label": 0
                },
                {
                    "sent": "More or less successfully in this this.",
                    "label": 0
                },
                {
                    "sent": "Application.",
                    "label": 0
                },
                {
                    "sent": "Of course, string kernel is something you value.",
                    "label": 0
                },
                {
                    "sent": "When you're working with sequences.",
                    "label": 0
                },
                {
                    "sent": "It's kind of obvious first candidate, so you have.",
                    "label": 0
                },
                {
                    "sent": "You basically thieves will be Max of different kind of substrings.",
                    "label": 0
                },
                {
                    "sent": "Can contain gaps, can contain some.",
                    "label": 0
                },
                {
                    "sent": "Special characters.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And there are many algorithms that compute this string kernels and you can combine them in different kinds.",
                    "label": 1
                },
                {
                    "sent": "You can take some kind of polynomial kernel on top of them.",
                    "label": 1
                },
                {
                    "sent": "Or whatever.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the sad story here is that none of this actually works in function prediction, so this kind of.",
                    "label": 1
                },
                {
                    "sent": "Looking at currencies of of some substrings that match is not a good strategy, so there is the reason seems to be that this there's too much mutation, and evolution has kind of.",
                    "label": 0
                },
                {
                    "sent": "Done his job and there is too much variety that you could actually spot.",
                    "label": 0
                },
                {
                    "sent": "Meaningful substrings.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, this is the input side.",
                    "label": 0
                },
                {
                    "sent": "Is this just the inputs?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How I encode?",
                    "label": 0
                },
                {
                    "sent": "How to encode the sequences in this in this feature?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is something that actually works much better.",
                    "label": 0
                },
                {
                    "sent": "We have this kind of so called.",
                    "label": 0
                },
                {
                    "sent": "GDG cobalt racecraft by Lisa home.",
                    "label": 0
                },
                {
                    "sent": "That is able to predict conserved residues in the sequences.",
                    "label": 1
                },
                {
                    "sent": "And it works by actually taking the whole.",
                    "label": 0
                },
                {
                    "sent": "World of protein sequences.",
                    "label": 0
                },
                {
                    "sent": "So the all all sequences that have been listed in databases and does this massive all against all alignment for these sequences.",
                    "label": 0
                },
                {
                    "sent": "So you have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Sequence against sequence.",
                    "label": 0
                },
                {
                    "sent": "Alignments.",
                    "label": 0
                },
                {
                    "sent": "And they build a craft alignment craft of of the so you have some amino acid in some sequence.",
                    "label": 0
                },
                {
                    "sent": "If it's aligned again, some other, I mean acid in some other sequence, you have an edge in the graph.",
                    "label": 1
                },
                {
                    "sent": "And when you have this kind of all all against all matching, you have.",
                    "label": 0
                },
                {
                    "sent": "Have a contest.",
                    "label": 0
                },
                {
                    "sent": "This kind of quite complex complex alignment graphs and.",
                    "label": 0
                },
                {
                    "sent": "Then what you do to find this concert residues you look for this kind of consistent alignment that that you know that OK X is aligned against OK, whatever.",
                    "label": 0
                },
                {
                    "sent": "This is why this I don't even see what it is, but the point is, these are kind of consistently aligned, so axis aligned against Y.",
                    "label": 0
                },
                {
                    "sent": "And this is like this and this.",
                    "label": 0
                },
                {
                    "sent": "So you have this kind of tight cluster.",
                    "label": 1
                },
                {
                    "sent": "That all at this kind of pairwise alignments align them consistently.",
                    "label": 0
                },
                {
                    "sent": "And then you say that if you have.",
                    "label": 0
                },
                {
                    "sent": "The big cluster of things that align consistently tends to say OK.",
                    "label": 0
                },
                {
                    "sent": "This residue must be.",
                    "label": 0
                },
                {
                    "sent": "Konserve into evolution Becauses there are so many sequences that align consistently.",
                    "label": 0
                },
                {
                    "sent": "So this kind of alignment will keep one feature.",
                    "label": 0
                },
                {
                    "sent": "So then we will have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Feature vector there.",
                    "label": 0
                },
                {
                    "sent": "Where each of these?",
                    "label": 0
                },
                {
                    "sent": "Clusters give one feature, so when you have a sequence, for example this one.",
                    "label": 0
                },
                {
                    "sent": "Is member of this cluster.",
                    "label": 0
                },
                {
                    "sent": "This is 1 bit.",
                    "label": 0
                },
                {
                    "sent": "Open for this sequence.",
                    "label": 1
                },
                {
                    "sent": "Typically you don't get too many of these.",
                    "label": 0
                },
                {
                    "sent": "This kind of per sequence, so it's quite sparse but very high dimensional feature.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how the inputs actually are encoded in our work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in on output side you can do different things so.",
                    "label": 0
                },
                {
                    "sent": "One very obvious way for protein function is to have some kind of hierarchy in the output side.",
                    "label": 0
                },
                {
                    "sent": "So for example this.",
                    "label": 0
                },
                {
                    "sent": "This is the easy hierarchy.",
                    "label": 0
                },
                {
                    "sent": "How you encode it you have.",
                    "label": 0
                },
                {
                    "sent": "4 levels and.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "For example, this is an enzyme that has two different functions, so you have this kind of path that diverges at the leaves.",
                    "label": 0
                },
                {
                    "sent": "And you test encode bits that are on.",
                    "label": 0
                },
                {
                    "sent": "On enter.",
                    "label": 0
                },
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Highlighted on the on the.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do something else like you can encode the edges instead of the nodes, But this is kind of a technical detail.",
                    "label": 0
                },
                {
                    "sent": "I don't want to spend too much time on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there are many hierarchies for protein function.",
                    "label": 1
                },
                {
                    "sent": "Enzyme enzymes function easy.",
                    "label": 0
                },
                {
                    "sent": "Hierarchized is the oldest one.",
                    "label": 1
                },
                {
                    "sent": "The other one we have used this gold standard hierarchy which is supposedly little bit better quality than easy hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Then you have some other like OK Go has has this kind of hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Structure then meets also on.",
                    "label": 0
                },
                {
                    "sent": "There are some others as well.",
                    "label": 0
                },
                {
                    "sent": "So this is just one.",
                    "label": 0
                },
                {
                    "sent": "I guess I will compare against this so so so this is the.",
                    "label": 0
                },
                {
                    "sent": "That's one way to represent out.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the reaction side that we are actually kind of now actively.",
                    "label": 0
                },
                {
                    "sent": "Developing so.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "So thinking about this kind of.",
                    "label": 0
                },
                {
                    "sent": "Set up her some substrate molecules.",
                    "label": 0
                },
                {
                    "sent": "That are converted into some.",
                    "label": 0
                },
                {
                    "sent": "Products.",
                    "label": 0
                },
                {
                    "sent": "So I have here I have two substrates, two products.",
                    "label": 0
                },
                {
                    "sent": "And how do?",
                    "label": 0
                },
                {
                    "sent": "Do a feature map out of them so I will do this so I have.",
                    "label": 1
                },
                {
                    "sent": "Compute.",
                    "label": 0
                },
                {
                    "sent": "Some of the feature feature.",
                    "label": 0
                },
                {
                    "sent": "Representations of all my input molecules, so I have this kind of cumulative spectrum of my inputs.",
                    "label": 0
                },
                {
                    "sent": "And then I have the same thing for my outputs.",
                    "label": 0
                },
                {
                    "sent": "And then I do the do the danger product between them.",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "I have some kind of feature vector for the input side, some feature vector for the output side and I do this coupling so I have.",
                    "label": 0
                },
                {
                    "sent": "Here I will then have kind of.",
                    "label": 1
                },
                {
                    "sent": "Products of 1 input feature.",
                    "label": 0
                },
                {
                    "sent": "One output feature will keep them.",
                    "label": 0
                },
                {
                    "sent": "You're cuter.",
                    "label": 0
                },
                {
                    "sent": "Kind of combined spectrum.",
                    "label": 0
                },
                {
                    "sent": "Further for the reaction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So then I. I'm kind of assuming here I have some feature Maps for my molecule so have some feature representation for my molecules and so that I'm relying on.",
                    "label": 0
                },
                {
                    "sent": "It can be some kind of soft ground spectrum of the molecule or what kind of walks you can find from the molecule.",
                    "label": 0
                },
                {
                    "sent": "This is it, so it's very, very simply just combining this kind of information from the individual model molecules from the.",
                    "label": 0
                },
                {
                    "sent": "From the reaction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can write this as a kernel.",
                    "label": 0
                },
                {
                    "sent": "I didn't do it here but but if you have a kernel between your molecules you can.",
                    "label": 0
                },
                {
                    "sent": "This is also very easy, too easy to kind of derive from that information.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one thing in this prediction thus becomes very important is to using the kernels.",
                    "label": 0
                },
                {
                    "sent": "To the best.",
                    "label": 0
                },
                {
                    "sent": "Kind of a way on.",
                    "label": 0
                },
                {
                    "sent": "It turns out that we actually need to use this kind of polynomial kernel trick, so.",
                    "label": 1
                },
                {
                    "sent": "With kernels you can, you can have any base kernel bit theory between your data and you can.",
                    "label": 0
                },
                {
                    "sent": "Raise it to some polynomial degree.",
                    "label": 0
                },
                {
                    "sent": "What this does?",
                    "label": 0
                },
                {
                    "sent": "Kind of underlying it takes.",
                    "label": 0
                },
                {
                    "sent": "Groups takes kind of.",
                    "label": 0
                },
                {
                    "sent": "Groups of your features and makes kind of gone by combined combined features out of your.",
                    "label": 0
                },
                {
                    "sent": "Out of your feature, so this will be.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 1
                },
                {
                    "sent": "Input side this will be groups of conserved residues.",
                    "label": 0
                },
                {
                    "sent": "So our features will be groups of constant residues instead of individual.",
                    "label": 0
                },
                {
                    "sent": "Concept residues.",
                    "label": 0
                },
                {
                    "sent": "And on the output side this will will.",
                    "label": 0
                },
                {
                    "sent": "Correspond to groups of molecule subgraphs that exist in the reaction.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I mean normally.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You can, you can have.",
                    "label": 0
                },
                {
                    "sent": "You can have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Red test by putting plus one here.",
                    "label": 0
                },
                {
                    "sent": "You can simulate this kind of setting that you have this individual guys and then any groups to the power of D. You can do that.",
                    "label": 0
                },
                {
                    "sent": "I don't remember if I. I think we tried that, but in this application it wasn't.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wasn't working.",
                    "label": 0
                },
                {
                    "sent": "OK, so some experiments before I run out of time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have used this kind of five fold stratified close across the Holidays and the idea is that we want to.",
                    "label": 0
                },
                {
                    "sent": "Similar to situation where we have new enzyme function, so new EC codes for the enzymes.",
                    "label": 0
                },
                {
                    "sent": "So are whatever we have in test fold.",
                    "label": 1
                },
                {
                    "sent": "Should not appear in the training trading false, so we're separating the test test easy codes from the training.",
                    "label": 1
                },
                {
                    "sent": "Easiest VC codes, and so we have this kind of fivefold setting where there is no.",
                    "label": 0
                },
                {
                    "sent": "Kind of redundancy.",
                    "label": 0
                },
                {
                    "sent": "Between the between the faults.",
                    "label": 0
                },
                {
                    "sent": "It means that.",
                    "label": 0
                },
                {
                    "sent": "So we will have two folds.",
                    "label": 0
                },
                {
                    "sent": "I can have.",
                    "label": 0
                },
                {
                    "sent": "The first 3 digits can be can be Co occur in both side, but there not the four complete 4 digit code cannot, so that is how we have done it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have some things we compare.",
                    "label": 0
                },
                {
                    "sent": "Compare accounts, so this is the baseline plus that you know most biologists would use.",
                    "label": 0
                },
                {
                    "sent": "This is the same but using this concept residue information.",
                    "label": 0
                },
                {
                    "sent": "And then we have this MMR, an OK I didn't exclude.",
                    "label": 0
                },
                {
                    "sent": "We also have this kernel density estimates on there.",
                    "label": 1
                },
                {
                    "sent": "In prediction we are using this kind of trivial primitive algorithms, so we enumerate the set of candidate functions.",
                    "label": 0
                },
                {
                    "sent": "And look for the best best scoring Y in this set.",
                    "label": 1
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "On this set is has taken from all data, so we have all functions we have seen in the in the.",
                    "label": 1
                },
                {
                    "sent": "In in our data, is this set in principle?",
                    "label": 0
                },
                {
                    "sent": "This set could be much wider, so you could for example take or reactions listed in keg as this set.",
                    "label": 0
                },
                {
                    "sent": "We haven't done that, but this is of course fully possible.",
                    "label": 0
                },
                {
                    "sent": "You can go and.",
                    "label": 0
                },
                {
                    "sent": "Go through this whole list of functions in keg and.",
                    "label": 0
                },
                {
                    "sent": "Take the best one of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the set up.",
                    "label": 0
                },
                {
                    "sent": "So here I show how this polynomial kernel helps us.",
                    "label": 1
                },
                {
                    "sent": "Don't worry about the figure figures, only worry about the color here is when you don't put any polynomial.",
                    "label": 0
                },
                {
                    "sent": "Kernel on top of your data, so then you're just using individual concert residues against subgraphs in your.",
                    "label": 0
                },
                {
                    "sent": "In your molecules and when you when you're raised, go.",
                    "label": 0
                },
                {
                    "sent": "Along this direction, you're raising the decree of the input kernel, so you're taking more and larger, larger and combinations of conserved residues as the features.",
                    "label": 0
                },
                {
                    "sent": "Here you're taking larger logical combinations of of.",
                    "label": 0
                },
                {
                    "sent": "Sub crops that you find in your reaction molecules.",
                    "label": 0
                },
                {
                    "sent": "As you can see, you have this kind of initial.",
                    "label": 0
                },
                {
                    "sent": "Steph, things are not not that good but.",
                    "label": 0
                },
                {
                    "sent": "Something from, let's say, something like.",
                    "label": 0
                },
                {
                    "sent": "OK, this line onwards everything is cool so you have have to have some kind of some degree of polynomial kernel on top of your data for best performance, But after that it doesn't really matter.",
                    "label": 1
                },
                {
                    "sent": "So it's pretty Even so the maximum is somewhere here so.",
                    "label": 0
                },
                {
                    "sent": "Actually, it looks like the output kernel needs little bit more higher diamonds and then then the input, but.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, OK, and this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the story about the accuracy of our methods.",
                    "label": 0
                },
                {
                    "sent": "So what this plot does it shows you the distribution of.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Enzymes where we kept.",
                    "label": 0
                },
                {
                    "sent": "123 or 4 digits, correct?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And these are different methods we have.",
                    "label": 0
                },
                {
                    "sent": "Blast is the baseline.",
                    "label": 0
                },
                {
                    "sent": "Then there is the nearest neighbour with our GT data console residue data.",
                    "label": 0
                },
                {
                    "sent": "And then the last three are different.",
                    "label": 0
                },
                {
                    "sent": "Structured output methods so.",
                    "label": 0
                },
                {
                    "sent": "This is the hierarchical classifier.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "This MMR method.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This this method.",
                    "label": 0
                },
                {
                    "sent": "With reaction kernel and this is the kernel density estimation.",
                    "label": 0
                },
                {
                    "sent": "So this method on the reaction kernel.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can see is that.",
                    "label": 0
                },
                {
                    "sent": "OK, blast is not.",
                    "label": 0
                },
                {
                    "sent": "Up there and this is to be expected.",
                    "label": 0
                },
                {
                    "sent": "We have by taking out the.",
                    "label": 0
                },
                {
                    "sent": "The function same functions default 4 digit in the easy easy class.",
                    "label": 0
                },
                {
                    "sent": "We're taking out a lot of home ologie from the data and blast.",
                    "label": 0
                },
                {
                    "sent": "This really needs this homological to work.",
                    "label": 0
                },
                {
                    "sent": "So everything that relies on the concert residues does much better, so this four other methods that have this concept residue in from my service is much, much better.",
                    "label": 0
                },
                {
                    "sent": "OK, So what else can we see?",
                    "label": 0
                },
                {
                    "sent": "The hierarchical classifier is.",
                    "label": 0
                },
                {
                    "sent": "Quite good.",
                    "label": 0
                },
                {
                    "sent": "Predicting 3 digits.",
                    "label": 0
                },
                {
                    "sent": "3C code and obviously cannot do anymore because in training phase it never has seen any examples where the.",
                    "label": 1
                },
                {
                    "sent": "4 digit is correctly assigned, so it cannot really get anything here, so.",
                    "label": 0
                },
                {
                    "sent": "It's really tight, but it does whatever it does.",
                    "label": 0
                },
                {
                    "sent": "It does very well.",
                    "label": 0
                },
                {
                    "sent": "So these two reaction reaction kernel.",
                    "label": 0
                },
                {
                    "sent": "Methods actually can sometimes get poor digits correct?",
                    "label": 1
                },
                {
                    "sent": "Not very often, so it seems.",
                    "label": 0
                },
                {
                    "sent": "See you can see it.",
                    "label": 0
                },
                {
                    "sent": "It's quite quite tough tough task.",
                    "label": 0
                },
                {
                    "sent": "But what is maybe most interesting is that.",
                    "label": 0
                },
                {
                    "sent": "This very simple method here.",
                    "label": 1
                },
                {
                    "sent": "Is actually the best method to get one or more easy, that's correct.",
                    "label": 0
                },
                {
                    "sent": "In this setup.",
                    "label": 0
                },
                {
                    "sent": "So this was very much a surprise to us.",
                    "label": 0
                },
                {
                    "sent": "We didn't expect expect this.",
                    "label": 0
                },
                {
                    "sent": "This method worked as well, but.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you get this kind of surprises.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, so such to conclude.",
                    "label": 0
                },
                {
                    "sent": "So we have shown you some.",
                    "label": 0
                },
                {
                    "sent": "Reaction kernels are means to improve accuracy in this remote homology enzyme function.",
                    "label": 1
                },
                {
                    "sent": "Prediction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Present result seems to success that good kernels and good features matter a lot.",
                    "label": 0
                },
                {
                    "sent": "But this kind is how to learn this hyperplane in this joint feature space.",
                    "label": 0
                },
                {
                    "sent": "Basically how to learn your feature wage?",
                    "label": 0
                },
                {
                    "sent": "Seems to not to matter that much.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not very conclusive.",
                    "label": 0
                },
                {
                    "sent": "Have made few experiments but but somehow things point to that direction.",
                    "label": 0
                },
                {
                    "sent": "Um, OK also.",
                    "label": 0
                },
                {
                    "sent": "OK, it seems that this structure learning or structured output prediction.",
                    "label": 0
                },
                {
                    "sent": "Can be actually tackled with.",
                    "label": 0
                },
                {
                    "sent": "This kind of quite simple approaches.",
                    "label": 0
                },
                {
                    "sent": "As opposed to this quite complicated business of optimizing things against negative examples that you actually may not have at all.",
                    "label": 0
                },
                {
                    "sent": "Right, so this brings me to end, so I take any questions you have.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Perfect timing.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Brother killed everybody, right?",
                    "label": 0
                },
                {
                    "sent": "Free.",
                    "label": 0
                },
                {
                    "sent": "The central question to me here seems how we need to get any performance on labels that you have not seen, yeah.",
                    "label": 0
                },
                {
                    "sent": "Obviously you were getting at least some.",
                    "label": 0
                },
                {
                    "sent": "Example correctly, there wasn't no no no so.",
                    "label": 0
                },
                {
                    "sent": "Very well then I don't know what this is.",
                    "label": 0
                },
                {
                    "sent": "Hold on baby.",
                    "label": 0
                },
                {
                    "sent": "Go about step.",
                    "label": 0
                },
                {
                    "sent": "Problem seems to be central.",
                    "label": 0
                },
                {
                    "sent": "Yeah I. I think we have to think we have to leave the sequences behind.",
                    "label": 0
                },
                {
                    "sent": "We need to call the Treaty structures of the proteins.",
                    "label": 0
                },
                {
                    "sent": "I kind of feel that it says not possible from the sequences alone but but yeah.",
                    "label": 0
                },
                {
                    "sent": "Moreover yeah.",
                    "label": 0
                },
                {
                    "sent": "My question was more, what's the methodology right?",
                    "label": 0
                },
                {
                    "sent": "Right, so yeah, I guess the question is that how much interpolation you can do in this kind of very complex feature space.",
                    "label": 0
                },
                {
                    "sent": "And maybe you cannot do too much.",
                    "label": 0
                },
                {
                    "sent": "But so this is.",
                    "label": 0
                },
                {
                    "sent": "So this might be application dependent, so this actually might be bad application for doing any kind of interpolation people because biologists tell me that OK somehow.",
                    "label": 0
                },
                {
                    "sent": "The space of enzymes is so bumpy that you cannot really do interpolation successfully.",
                    "label": 0
                },
                {
                    "sent": "So maybe some internal some other application you could actually deploy it in.",
                    "label": 0
                },
                {
                    "sent": "I can work with your people said was this motivated by some specific problem or it is just in general?",
                    "label": 0
                },
                {
                    "sent": "He said general is it's a sample from Chegg actually yeah.",
                    "label": 0
                },
                {
                    "sent": "Co operators.",
                    "label": 0
                },
                {
                    "sent": "While it depends how I don't have any vet lab collaborators, but but I do have some that are kind of protein bioinformatics people, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So, for example, Lisa Holm is one of these.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tease.",
                    "label": 0
                },
                {
                    "sent": "This kind of paper.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And what other?",
                    "label": 0
                },
                {
                    "sent": "Other structure prediction problem of exactly this type generates new labels for other examples of this.",
                    "label": 0
                },
                {
                    "sent": "Well, for example the the machine translation I mentioned is one of these things that you don't in your training data.",
                    "label": 0
                },
                {
                    "sent": "You don't have all the sentences that can be presented to you, so you need to generalize in that space.",
                    "label": 0
                },
                {
                    "sent": "I mean there there it would seem to me and only make this one more common.",
                    "label": 0
                },
                {
                    "sent": "At the beginning you said the point is structured prediction is.",
                    "label": 0
                },
                {
                    "sent": "Come directly yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to do with unseen things, I feel actually need some combination of both the local or component prediction and the global structure.",
                    "label": 0
                },
                {
                    "sent": "Because you might have seen the individual words right.",
                    "label": 0
                },
                {
                    "sent": "So if you are able to, right yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, yeah, I agree with you and you can kind of.",
                    "label": 0
                },
                {
                    "sent": "Smuggle in this kind of component predictors into your system as features, so you can have some tool that gives you some prediction and take that as a feature so that kind of thing you can embed here.",
                    "label": 0
                },
                {
                    "sent": "And for example this.",
                    "label": 0
                },
                {
                    "sent": "This GDT features are kind of the kind of thing that there is quite of heavy processing that gives you this predicted conserved residues that we are relying on South.",
                    "label": 0
                },
                {
                    "sent": "So you can do all kinds of things, But yeah, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this kind of distinction between what is.",
                    "label": 0
                },
                {
                    "sent": "OK, very structured prediction.",
                    "label": 0
                },
                {
                    "sent": "What is just predicting components is OK.",
                    "label": 0
                },
                {
                    "sent": "It's not that clear.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you, call again.",
                    "label": 0
                }
            ]
        }
    }
}