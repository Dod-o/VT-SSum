{
    "id": "nwniklnsulq4ec6lz75annc3on5nerqu",
    "title": "Subspace Embeddings and \u2113p-Regression Using Exponential Random Variables",
    "info": {
        "author": [
            "Qin Zhang, IBM Almaden Research Center, IBM Research"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_zhang_subspace/",
    "segmentation": [
        [
            "So today I will talk about subspace imbedding and regressions.",
            "So this is joint work with David Woodruff.",
            "So what is?"
        ],
        [
            "Subspace embedding its distribution over linear mappings from North dimensional space to M dimensional space, so that for any fixed dimensional subspace, if we pick a random mapping from that distribution, then the P norm of all vectors in that subspace preserved after embedding so up to a distortion of Kappa.",
            "So there are three parameters we would like to minimize.",
            "The first is the.",
            "Dimension of the embedding second is the distortion of the embedding, and we also want to minimize the time to compute such embedding."
        ],
        [
            "So subspace eventing has a lot of applications for that can be used to compute the regression and I will talk about this in the next slides.",
            "It also finds applications in low rank approximation, quantile regression and so on.",
            "So here is example.",
            "Do you want this for all vectors X?",
            "Yeah X, so the vector in this 3 dimensional space is MX.",
            "So apply M to D. OK.",
            "Yes, yes, I mean.",
            "The details sorry is N * D matrix.",
            "Sorry, I should write it explicitly, but you will see so."
        ],
        [
            "So for example, we can use the LP subspace embedding to solve the regression problem.",
            "So for convenience, let em Bartleby matrix of N type N by D -- 1 and the matrix M is the combination of EM bond minus P and let.",
            "I be a subspace embedding which dimension M distortion cap and an invention time T. So Pi is the matrix of of M by N. So here is."
        ],
        [
            "Typical implementation, so the first step is to compute the embedding Pi M and then we use Pi M to compute change of basis matrix so that the MRI will have some good properties.",
            "Then use our.",
            "We can find the sampling matrix by one and then we end up solving a subsample.",
            "The problem defined on the sample matrix Pi one mbar and apply 1B and this is a much smaller.",
            "And that's why it's running.",
            "Time is much smaller than solving the original problem, and we can also show that the solution for this subsample problem is a good approximation of the solution of the original problem.",
            "So running time of the first step is basically T the time to do the embedding and running to the second step increase if the dimension of the embedding increase and the right type this so the first step increase if the dimension of the sampling matrix increase.",
            "Which is actually depends on the distortion of the embedding, so you can see that the total running time will increase if one of the three parameters increase, so that's why we want to minimize all these three vectors, three parameters.",
            "OK.",
            "So."
        ],
        [
            "LP regression is well studied problem.",
            "For example, when P is equal to 1, it can actually be solved by linear programming, but the running time is super linear in North, so click and get.",
            "Algorithm whose running time is linear in terms of North, but it has large multiplicative quality factor.",
            "So if we allow one plus epsilon approximation, we can further reduce the quality factor using the technique of subspace embedding.",
            "And recently Clarkson Woodruff also member only they use L1 subspace cementing get algorithm with spending time and the M log N plus.",
            "A polylog factor of D over epsilon, so here ends the M means the number of non zero entries of the matrix, so if the matrix is very sparse, that's the order has just North.",
            "Non zero entries then this algorithm is almost linear in North.",
            "OK, so.",
            "So."
        ],
        [
            "In this paper we further improve this album, subspace embedding in terms of these three parameters.",
            "Therefore, we also improved the running time of the L1 expression.",
            "So."
        ],
        [
            "Here is a summary of our results.",
            "So we first input all previous LP subspace embeddings for all P range from one to Infinity except the two.",
            "So why we don't consider 2 because two has already been made optimal by the recent paper by collection modules.",
            "The Q equals QQ is, so for P value from one to two, Q is equal to B and four P is larger than one.",
            "Q is actually Infinity.",
            "I will talk.",
            "I will mention that later, OK.",
            "So he."
        ],
        [
            "Sears in particular for people to one run inside of our subspace embedding, is almost optimal, and the distortion is the square if D is large, then log we actually can improve it to D to the power 1.5 Times Square root log and distortion of embedding is tied up to some point log factor.",
            "So here to to we hide log defectors not log in log D."
        ],
        [
            "So use this improved subspace embedding.",
            "We can improve all the regression results and our our algorithm for LP regression can also be implemented efficiently in the distributed setting, which I will talk more at the end of the talk.",
            "OK."
        ],
        [
            "So let me introduce our subspace embedding matrix.",
            "So we are going to use the L2 subspace imbedding as a black box.",
            "So here L2 subspace imagine is defined similarly as the general LP subspace.",
            "Imagine so the only difference is that here the distortion is fixed via constant.",
            "Actually I can replace this to be .99 and this to be 1.1.",
            "So then change anything and so there are two parameters of the L2.",
            "Space embedding the first is the dimension of the embedding and the 2nd is the maximum non zero entries of columns of South.",
            "So this actually relates to the running time of the embedding time."
        ],
        [
            "So here is our LP subspace embedding, so it's a product of two matrix.",
            "The first is the L2 subspace embedding and the 2nd is a diagonal matrix with inverse exponential to the power 1 / P on diagonal.",
            "So for different people we're going to, we use different L2 subspace embeddings, and those subspace embeddings actually from previous work, and in particular a recent paper by news and the wind.",
            "So one thing you can see from here is that we actually can compute this embedding in time and the end of North.",
            "So why this is true becausw?",
            "So this doesn't matter because this is just a diagonal matrix and in this matrix.",
            "The number of the maximum number of down zero entries of each column is a constant, so that's why the embedding time is and the OK.",
            "So here all those embedding matrix we are going to use.",
            "South is a constant.",
            "OK, so.",
            "Distortion necessary for L1 more.",
            "Can you get the same sort of constant?",
            "This one.",
            "Oh this.",
            "Giving distortion order of the right yes.",
            "You can do.",
            "Four out of the distortion is slightly higher.",
            "It's the square or something.",
            "But here it looks like.",
            "That you can achieve.",
            "Oh oh two sorry yeah L2L2 we can achieve constant out with the exception for any P not equal to two.",
            "The distortion depends on the.",
            "Sorry this is there some lower bound but you can make it 1 minus epsilon one plus epsilon so.",
            "Little different than two.",
            "Yeah yeah yeah.",
            "So this is open question.",
            "This is open question.",
            "In my last slides great.",
            "OK so yeah out with different cause the Ukrainian space has some distinctive features like rotation environments, invariant or something like that.",
            "OK so."
        ],
        [
            "So before going to the detailed analysis, let me introduce two distributions.",
            "The first is the exponential distribution which we used in our embedding matrix.",
            "So the exponential distribution has very good property called the Max stability.",
            "Saying that if you want UN exponentially distributed, then the Alpha is the end dimensional vector of positive values.",
            "Then the Max of Alpha over Uof I over UI.",
            "As this treated identically as the L1 norm of Alpha divided by UU is also exponential.",
            "OK, so."
        ],
        [
            "Another very close to the distribution I would like to mention is the P stable distribution.",
            "So this distribution was used widely previously for also for subspace schematic.",
            "So distribution is peace table.",
            "If for any vector in the end dimensional space and V1 to VM that is distributed according to this distribution, we have that this sum of IVI is distributed identically as the P norm of F * V. We we is also distributed according to this distribution.",
            "So here the difference is basically like we replace the Max with the sum.",
            "And here we use a pin down here we for we only use the one norm."
        ],
        [
            "So for P equal to two, this is basically the Gaussian distribution for P equal to 1.",
            "This is the this called cost distribution, so."
        ],
        [
            "If we want to use the stable distribution to construct the subspace embedding the matrixes will be of the same form.",
            "OK, so we just replace this that this diagonal matrix by putting the IDP stables on the diagonal and the rest are same.",
            "So."
        ],
        [
            "The natural question is why exponential distribution is better.",
            "So there are two reasons.",
            "The first is P stable distribution only exists for a narrow range of P. Basically, people want to do OK, so it cannot be used to construct subspace embedding matrix for P that is greater than two.",
            "But expressions can be used for constructing.",
            "Subspace menu for all P that is greater than one.",
            "So the second difference is that the lower tail of inverse exponential decreases much faster than the.",
            "Lower tail of peace table.",
            "OK, so This is why we gain some.",
            "We have some advantage on the distortion.",
            "The operators are similar, so we also will go.",
            "We also need to use this property in the analysis, so here's illustration.",
            "You can see Red one is the inverse exponential, the blue line is the blue curve is the cost distribution and you can see that the lower tail of the exponential inverse expression decreases much faster than the cost distribution.",
            "But the upper tails.",
            "They are basically similar up to a constant factor.",
            "What is the?",
            "So this is 1 / U U is distributed according to exponential.",
            "So yeah I'll say it was expansion of this.",
            "Basically the reciprocal of exponential.",
            "So that's a."
        ],
        [
            "So now let's look at a little bit about the analysis."
        ],
        [
            "So recall that our embedding matrix is in the form of the product of two matrix, so the first one is the L2 subspace imbedding, the second one is diagonal matrix with inverse exponential on the diagonal.",
            "So let's do a little bit the mess we first showed that the underestimation is small.",
            "So give any idea why in the subspace in vector Y in the subspace, the first line just rewrite of the matrix the first inequality.",
            "Street View so because the two normal vector is allowed at at mostly one of the vector and then we use the property of the L2 SE to get rid of the South and the third quality is again trivial becausw the fitted norm of vector is at most two normal vector and then this step is the key.",
            "We use the Max stability of exponentials so that we can say the L Infinity of the of the why?",
            "Is this Richard identically as the one one norm of Y over you?",
            "You is also exponential.",
            "Then we use the property of lower tail of the exponential to conclude that with very high probability this distortion responded by D log D. OK.",
            "So.",
            "So this proves like for any.",
            "Fix for any vector in the subspace.",
            "The underestimation is small, so we have to show that this holds for all the vectors.",
            "So what we do is we use standard net arguments plus union bound.",
            "So this is quite standard technique.",
            "So for D is larger than log N. The distortion.",
            "Can actually be improved 2 ^2, D log use?",
            "Use a better analysis.",
            "OK. And the similar arguments holds for RP that range from one to two.",
            "So that's conclude the underestimation.",
            "Oh, whatever.",
            "But in the exponential, errors in the same times in new media.",
            "So you have a medium.",
            "Yes, yes.",
            "Windows is this actually relates to the probability here.",
            "So we need to apply a union bound.",
            "So that's why we need this to be deal ugly becausw Internet arguments.",
            "We have to apply the Union bound basically 2 to the power D log D power, D log vectors.",
            "So that's how we.",
            "Mention anywhere any small in the.",
            "So you mean an is much, much bigger than the?",
            "It is much bigger, yeah?",
            "So."
        ],
        [
            "For over estimation, so the first lines Agama rewrite of the embedding matrix and the first inequality using the fact that the L2 SC only contracts the L1 norm.",
            "So this is part of the of the matrix.",
            "The third line we use the fact that the upper tail of the inverse exponential and the cost basically similar up to a constant factor.",
            "So basically we can replace the diagonal matrix D using another diagonal matrix D prime and then so the last step is we just use some previous known results and this holds for all vectors in that subspace with constant probability.",
            "So this is for results.",
            "And similar arguments also works for General P range from one to two.",
            "OK.",
            "So any questions?"
        ],
        [
            "So for LP for big P we still use the.",
            "The matrix is still off the same structure, but the L2 subspace subspace embedding matrix we're going to use have different parameters.",
            "And we actually can embed into the subspace into the L Infinity norm, so a good property of such kind of embedding is that the L in fitting regression can be solved efficiently by the linear programming.",
            "So that means we can solve basically all LP regression efficiently by linear program.",
            "So I think I'm not going to discuss the details of the analysis here.",
            "2 main technical Ingrid ingredients.",
            "So the first is.",
            "So to show that the UN estimation is small, we basically use the Marxist ability of the expressions, which is the same as before, but for overestimation the proof will be much more complicated.",
            "Essentially we have to use the fact that the leverage score of the matrix to upper bound the coordinates of the vector subspace.",
            "So this idea was not new IT was already used in a previous paper.",
            "But how to adapt this to our framework needs some nontrivial analysis.",
            "OK.",
            "So."
        ],
        [
            "So that's basically the analysis of the embedding.",
            "So finally I want to mention that our our subspace embedding and regression can be implemented efficiently in the distributed setting.",
            "So in the distributed setting we have K machines and one central servers, and each machine has a two way communication with the server.",
            "So each machine has a set of rows subset basically a subset, a submatrix softly of the matrix M bar.",
            "And also have the vector V. Courage to solve the LP regression.",
            "OK.",
            "So.",
            "I hope."
        ],
        [
            "I still remember the framework of solving to solve our progression using LP subspace of adding.",
            "So basically the first step to compute the embedding matrix can be done by the K machines.",
            "So basically each machine computes a local piece of spending of embedding, then send to the coordinator server and then server reconstruct the global embedding by those small local embeddings and then the server computes a matrix change of basis matrix R so that.",
            "Mr will have some good property, then give us so then the server sent out to all the K machines and then each machine tried to find a good local sample sample matrix by one and then.",
            "Yeah, and then the server solve the subsampled problems so that running time of the system will be the running time of decentralized version of the LP integration.",
            "Press the communication between the server and the case sites OK machines and we can show that most of the work actually is distributed into the K machines and the running time the server plus the total communication is sublinear in North so previous work either.",
            "Have like almost linear communication or it only works for P small narrow range of P. So this is the our improvement."
        ],
        [
            "So finally, let me quickly summarize our results.",
            "So basically we get improved subspace embedding.",
            "Using exponential random variables and this improves all the previous work on the embedding time, embedding distortion dimension given the optimal almost optimal running time and improve the subspace, emailing also gives the improved LP regression.",
            "Finally, our regression algorithm can be implemented in the distributed setting efficiently, so there are two open questions.",
            "So first is we don't know like what is the best distortion.",
            "Given the almost 4 hour one subspace immediacy, given the almost optimal embedding tie and the the embedding dimensions so.",
            "Our conjecture is maybe can be improved to do a power 1.5, so currently it's D point 1D to power point five times.",
            "Some log in factor and maybe it can even be improved to the second is can we show any nontrivial tradeoffs for the lower bounds?",
            "That's exactly the question asked previously."
        ],
        [
            "So thank you any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I will talk about subspace imbedding and regressions.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with David Woodruff.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subspace embedding its distribution over linear mappings from North dimensional space to M dimensional space, so that for any fixed dimensional subspace, if we pick a random mapping from that distribution, then the P norm of all vectors in that subspace preserved after embedding so up to a distortion of Kappa.",
                    "label": 0
                },
                {
                    "sent": "So there are three parameters we would like to minimize.",
                    "label": 0
                },
                {
                    "sent": "The first is the.",
                    "label": 0
                },
                {
                    "sent": "Dimension of the embedding second is the distortion of the embedding, and we also want to minimize the time to compute such embedding.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So subspace eventing has a lot of applications for that can be used to compute the regression and I will talk about this in the next slides.",
                    "label": 0
                },
                {
                    "sent": "It also finds applications in low rank approximation, quantile regression and so on.",
                    "label": 0
                },
                {
                    "sent": "So here is example.",
                    "label": 0
                },
                {
                    "sent": "Do you want this for all vectors X?",
                    "label": 1
                },
                {
                    "sent": "Yeah X, so the vector in this 3 dimensional space is MX.",
                    "label": 0
                },
                {
                    "sent": "So apply M to D. OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, I mean.",
                    "label": 0
                },
                {
                    "sent": "The details sorry is N * D matrix.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I should write it explicitly, but you will see so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, we can use the LP subspace embedding to solve the regression problem.",
                    "label": 0
                },
                {
                    "sent": "So for convenience, let em Bartleby matrix of N type N by D -- 1 and the matrix M is the combination of EM bond minus P and let.",
                    "label": 0
                },
                {
                    "sent": "I be a subspace embedding which dimension M distortion cap and an invention time T. So Pi is the matrix of of M by N. So here is.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Typical implementation, so the first step is to compute the embedding Pi M and then we use Pi M to compute change of basis matrix so that the MRI will have some good properties.",
                    "label": 1
                },
                {
                    "sent": "Then use our.",
                    "label": 1
                },
                {
                    "sent": "We can find the sampling matrix by one and then we end up solving a subsample.",
                    "label": 0
                },
                {
                    "sent": "The problem defined on the sample matrix Pi one mbar and apply 1B and this is a much smaller.",
                    "label": 0
                },
                {
                    "sent": "And that's why it's running.",
                    "label": 0
                },
                {
                    "sent": "Time is much smaller than solving the original problem, and we can also show that the solution for this subsample problem is a good approximation of the solution of the original problem.",
                    "label": 0
                },
                {
                    "sent": "So running time of the first step is basically T the time to do the embedding and running to the second step increase if the dimension of the embedding increase and the right type this so the first step increase if the dimension of the sampling matrix increase.",
                    "label": 1
                },
                {
                    "sent": "Which is actually depends on the distortion of the embedding, so you can see that the total running time will increase if one of the three parameters increase, so that's why we want to minimize all these three vectors, three parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "LP regression is well studied problem.",
                    "label": 0
                },
                {
                    "sent": "For example, when P is equal to 1, it can actually be solved by linear programming, but the running time is super linear in North, so click and get.",
                    "label": 1
                },
                {
                    "sent": "Algorithm whose running time is linear in terms of North, but it has large multiplicative quality factor.",
                    "label": 0
                },
                {
                    "sent": "So if we allow one plus epsilon approximation, we can further reduce the quality factor using the technique of subspace embedding.",
                    "label": 1
                },
                {
                    "sent": "And recently Clarkson Woodruff also member only they use L1 subspace cementing get algorithm with spending time and the M log N plus.",
                    "label": 0
                },
                {
                    "sent": "A polylog factor of D over epsilon, so here ends the M means the number of non zero entries of the matrix, so if the matrix is very sparse, that's the order has just North.",
                    "label": 0
                },
                {
                    "sent": "Non zero entries then this algorithm is almost linear in North.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this paper we further improve this album, subspace embedding in terms of these three parameters.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we also improved the running time of the L1 expression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a summary of our results.",
                    "label": 1
                },
                {
                    "sent": "So we first input all previous LP subspace embeddings for all P range from one to Infinity except the two.",
                    "label": 0
                },
                {
                    "sent": "So why we don't consider 2 because two has already been made optimal by the recent paper by collection modules.",
                    "label": 1
                },
                {
                    "sent": "The Q equals QQ is, so for P value from one to two, Q is equal to B and four P is larger than one.",
                    "label": 0
                },
                {
                    "sent": "Q is actually Infinity.",
                    "label": 0
                },
                {
                    "sent": "I will talk.",
                    "label": 0
                },
                {
                    "sent": "I will mention that later, OK.",
                    "label": 0
                },
                {
                    "sent": "So he.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sears in particular for people to one run inside of our subspace embedding, is almost optimal, and the distortion is the square if D is large, then log we actually can improve it to D to the power 1.5 Times Square root log and distortion of embedding is tied up to some point log factor.",
                    "label": 0
                },
                {
                    "sent": "So here to to we hide log defectors not log in log D.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So use this improved subspace embedding.",
                    "label": 0
                },
                {
                    "sent": "We can improve all the regression results and our our algorithm for LP regression can also be implemented efficiently in the distributed setting, which I will talk more at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me introduce our subspace embedding matrix.",
                    "label": 1
                },
                {
                    "sent": "So we are going to use the L2 subspace imbedding as a black box.",
                    "label": 0
                },
                {
                    "sent": "So here L2 subspace imagine is defined similarly as the general LP subspace.",
                    "label": 0
                },
                {
                    "sent": "Imagine so the only difference is that here the distortion is fixed via constant.",
                    "label": 0
                },
                {
                    "sent": "Actually I can replace this to be .99 and this to be 1.1.",
                    "label": 0
                },
                {
                    "sent": "So then change anything and so there are two parameters of the L2.",
                    "label": 1
                },
                {
                    "sent": "Space embedding the first is the dimension of the embedding and the 2nd is the maximum non zero entries of columns of South.",
                    "label": 0
                },
                {
                    "sent": "So this actually relates to the running time of the embedding time.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is our LP subspace embedding, so it's a product of two matrix.",
                    "label": 1
                },
                {
                    "sent": "The first is the L2 subspace embedding and the 2nd is a diagonal matrix with inverse exponential to the power 1 / P on diagonal.",
                    "label": 1
                },
                {
                    "sent": "So for different people we're going to, we use different L2 subspace embeddings, and those subspace embeddings actually from previous work, and in particular a recent paper by news and the wind.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can see from here is that we actually can compute this embedding in time and the end of North.",
                    "label": 0
                },
                {
                    "sent": "So why this is true becausw?",
                    "label": 0
                },
                {
                    "sent": "So this doesn't matter because this is just a diagonal matrix and in this matrix.",
                    "label": 1
                },
                {
                    "sent": "The number of the maximum number of down zero entries of each column is a constant, so that's why the embedding time is and the OK.",
                    "label": 0
                },
                {
                    "sent": "So here all those embedding matrix we are going to use.",
                    "label": 0
                },
                {
                    "sent": "South is a constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Distortion necessary for L1 more.",
                    "label": 0
                },
                {
                    "sent": "Can you get the same sort of constant?",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Oh this.",
                    "label": 0
                },
                {
                    "sent": "Giving distortion order of the right yes.",
                    "label": 0
                },
                {
                    "sent": "You can do.",
                    "label": 0
                },
                {
                    "sent": "Four out of the distortion is slightly higher.",
                    "label": 1
                },
                {
                    "sent": "It's the square or something.",
                    "label": 0
                },
                {
                    "sent": "But here it looks like.",
                    "label": 0
                },
                {
                    "sent": "That you can achieve.",
                    "label": 0
                },
                {
                    "sent": "Oh oh two sorry yeah L2L2 we can achieve constant out with the exception for any P not equal to two.",
                    "label": 0
                },
                {
                    "sent": "The distortion depends on the.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is there some lower bound but you can make it 1 minus epsilon one plus epsilon so.",
                    "label": 0
                },
                {
                    "sent": "Little different than two.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is open question.",
                    "label": 0
                },
                {
                    "sent": "This is open question.",
                    "label": 0
                },
                {
                    "sent": "In my last slides great.",
                    "label": 0
                },
                {
                    "sent": "OK so yeah out with different cause the Ukrainian space has some distinctive features like rotation environments, invariant or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before going to the detailed analysis, let me introduce two distributions.",
                    "label": 0
                },
                {
                    "sent": "The first is the exponential distribution which we used in our embedding matrix.",
                    "label": 0
                },
                {
                    "sent": "So the exponential distribution has very good property called the Max stability.",
                    "label": 1
                },
                {
                    "sent": "Saying that if you want UN exponentially distributed, then the Alpha is the end dimensional vector of positive values.",
                    "label": 0
                },
                {
                    "sent": "Then the Max of Alpha over Uof I over UI.",
                    "label": 0
                },
                {
                    "sent": "As this treated identically as the L1 norm of Alpha divided by UU is also exponential.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another very close to the distribution I would like to mention is the P stable distribution.",
                    "label": 0
                },
                {
                    "sent": "So this distribution was used widely previously for also for subspace schematic.",
                    "label": 0
                },
                {
                    "sent": "So distribution is peace table.",
                    "label": 0
                },
                {
                    "sent": "If for any vector in the end dimensional space and V1 to VM that is distributed according to this distribution, we have that this sum of IVI is distributed identically as the P norm of F * V. We we is also distributed according to this distribution.",
                    "label": 1
                },
                {
                    "sent": "So here the difference is basically like we replace the Max with the sum.",
                    "label": 0
                },
                {
                    "sent": "And here we use a pin down here we for we only use the one norm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for P equal to two, this is basically the Gaussian distribution for P equal to 1.",
                    "label": 0
                },
                {
                    "sent": "This is the this called cost distribution, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we want to use the stable distribution to construct the subspace embedding the matrixes will be of the same form.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just replace this that this diagonal matrix by putting the IDP stables on the diagonal and the rest are same.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The natural question is why exponential distribution is better.",
                    "label": 1
                },
                {
                    "sent": "So there are two reasons.",
                    "label": 0
                },
                {
                    "sent": "The first is P stable distribution only exists for a narrow range of P. Basically, people want to do OK, so it cannot be used to construct subspace embedding matrix for P that is greater than two.",
                    "label": 1
                },
                {
                    "sent": "But expressions can be used for constructing.",
                    "label": 0
                },
                {
                    "sent": "Subspace menu for all P that is greater than one.",
                    "label": 0
                },
                {
                    "sent": "So the second difference is that the lower tail of inverse exponential decreases much faster than the.",
                    "label": 1
                },
                {
                    "sent": "Lower tail of peace table.",
                    "label": 0
                },
                {
                    "sent": "OK, so This is why we gain some.",
                    "label": 0
                },
                {
                    "sent": "We have some advantage on the distortion.",
                    "label": 0
                },
                {
                    "sent": "The operators are similar, so we also will go.",
                    "label": 0
                },
                {
                    "sent": "We also need to use this property in the analysis, so here's illustration.",
                    "label": 0
                },
                {
                    "sent": "You can see Red one is the inverse exponential, the blue line is the blue curve is the cost distribution and you can see that the lower tail of the exponential inverse expression decreases much faster than the cost distribution.",
                    "label": 0
                },
                {
                    "sent": "But the upper tails.",
                    "label": 0
                },
                {
                    "sent": "They are basically similar up to a constant factor.",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "So this is 1 / U U is distributed according to exponential.",
                    "label": 1
                },
                {
                    "sent": "So yeah I'll say it was expansion of this.",
                    "label": 0
                },
                {
                    "sent": "Basically the reciprocal of exponential.",
                    "label": 0
                },
                {
                    "sent": "So that's a.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look at a little bit about the analysis.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So recall that our embedding matrix is in the form of the product of two matrix, so the first one is the L2 subspace imbedding, the second one is diagonal matrix with inverse exponential on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So let's do a little bit the mess we first showed that the underestimation is small.",
                    "label": 0
                },
                {
                    "sent": "So give any idea why in the subspace in vector Y in the subspace, the first line just rewrite of the matrix the first inequality.",
                    "label": 0
                },
                {
                    "sent": "Street View so because the two normal vector is allowed at at mostly one of the vector and then we use the property of the L2 SE to get rid of the South and the third quality is again trivial becausw the fitted norm of vector is at most two normal vector and then this step is the key.",
                    "label": 0
                },
                {
                    "sent": "We use the Max stability of exponentials so that we can say the L Infinity of the of the why?",
                    "label": 0
                },
                {
                    "sent": "Is this Richard identically as the one one norm of Y over you?",
                    "label": 0
                },
                {
                    "sent": "You is also exponential.",
                    "label": 0
                },
                {
                    "sent": "Then we use the property of lower tail of the exponential to conclude that with very high probability this distortion responded by D log D. OK.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this proves like for any.",
                    "label": 0
                },
                {
                    "sent": "Fix for any vector in the subspace.",
                    "label": 0
                },
                {
                    "sent": "The underestimation is small, so we have to show that this holds for all the vectors.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we use standard net arguments plus union bound.",
                    "label": 0
                },
                {
                    "sent": "So this is quite standard technique.",
                    "label": 0
                },
                {
                    "sent": "So for D is larger than log N. The distortion.",
                    "label": 0
                },
                {
                    "sent": "Can actually be improved 2 ^2, D log use?",
                    "label": 0
                },
                {
                    "sent": "Use a better analysis.",
                    "label": 0
                },
                {
                    "sent": "OK. And the similar arguments holds for RP that range from one to two.",
                    "label": 0
                },
                {
                    "sent": "So that's conclude the underestimation.",
                    "label": 0
                },
                {
                    "sent": "Oh, whatever.",
                    "label": 0
                },
                {
                    "sent": "But in the exponential, errors in the same times in new media.",
                    "label": 0
                },
                {
                    "sent": "So you have a medium.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Windows is this actually relates to the probability here.",
                    "label": 0
                },
                {
                    "sent": "So we need to apply a union bound.",
                    "label": 0
                },
                {
                    "sent": "So that's why we need this to be deal ugly becausw Internet arguments.",
                    "label": 1
                },
                {
                    "sent": "We have to apply the Union bound basically 2 to the power D log D power, D log vectors.",
                    "label": 0
                },
                {
                    "sent": "So that's how we.",
                    "label": 0
                },
                {
                    "sent": "Mention anywhere any small in the.",
                    "label": 0
                },
                {
                    "sent": "So you mean an is much, much bigger than the?",
                    "label": 0
                },
                {
                    "sent": "It is much bigger, yeah?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For over estimation, so the first lines Agama rewrite of the embedding matrix and the first inequality using the fact that the L2 SC only contracts the L1 norm.",
                    "label": 1
                },
                {
                    "sent": "So this is part of the of the matrix.",
                    "label": 0
                },
                {
                    "sent": "The third line we use the fact that the upper tail of the inverse exponential and the cost basically similar up to a constant factor.",
                    "label": 1
                },
                {
                    "sent": "So basically we can replace the diagonal matrix D using another diagonal matrix D prime and then so the last step is we just use some previous known results and this holds for all vectors in that subspace with constant probability.",
                    "label": 0
                },
                {
                    "sent": "So this is for results.",
                    "label": 0
                },
                {
                    "sent": "And similar arguments also works for General P range from one to two.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So any questions?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for LP for big P we still use the.",
                    "label": 0
                },
                {
                    "sent": "The matrix is still off the same structure, but the L2 subspace subspace embedding matrix we're going to use have different parameters.",
                    "label": 0
                },
                {
                    "sent": "And we actually can embed into the subspace into the L Infinity norm, so a good property of such kind of embedding is that the L in fitting regression can be solved efficiently by the linear programming.",
                    "label": 1
                },
                {
                    "sent": "So that means we can solve basically all LP regression efficiently by linear program.",
                    "label": 0
                },
                {
                    "sent": "So I think I'm not going to discuss the details of the analysis here.",
                    "label": 1
                },
                {
                    "sent": "2 main technical Ingrid ingredients.",
                    "label": 0
                },
                {
                    "sent": "So the first is.",
                    "label": 0
                },
                {
                    "sent": "So to show that the UN estimation is small, we basically use the Marxist ability of the expressions, which is the same as before, but for overestimation the proof will be much more complicated.",
                    "label": 1
                },
                {
                    "sent": "Essentially we have to use the fact that the leverage score of the matrix to upper bound the coordinates of the vector subspace.",
                    "label": 0
                },
                {
                    "sent": "So this idea was not new IT was already used in a previous paper.",
                    "label": 0
                },
                {
                    "sent": "But how to adapt this to our framework needs some nontrivial analysis.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's basically the analysis of the embedding.",
                    "label": 0
                },
                {
                    "sent": "So finally I want to mention that our our subspace embedding and regression can be implemented efficiently in the distributed setting.",
                    "label": 0
                },
                {
                    "sent": "So in the distributed setting we have K machines and one central servers, and each machine has a two way communication with the server.",
                    "label": 1
                },
                {
                    "sent": "So each machine has a set of rows subset basically a subset, a submatrix softly of the matrix M bar.",
                    "label": 0
                },
                {
                    "sent": "And also have the vector V. Courage to solve the LP regression.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I hope.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I still remember the framework of solving to solve our progression using LP subspace of adding.",
                    "label": 0
                },
                {
                    "sent": "So basically the first step to compute the embedding matrix can be done by the K machines.",
                    "label": 0
                },
                {
                    "sent": "So basically each machine computes a local piece of spending of embedding, then send to the coordinator server and then server reconstruct the global embedding by those small local embeddings and then the server computes a matrix change of basis matrix R so that.",
                    "label": 0
                },
                {
                    "sent": "Mr will have some good property, then give us so then the server sent out to all the K machines and then each machine tried to find a good local sample sample matrix by one and then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then the server solve the subsampled problems so that running time of the system will be the running time of decentralized version of the LP integration.",
                    "label": 1
                },
                {
                    "sent": "Press the communication between the server and the case sites OK machines and we can show that most of the work actually is distributed into the K machines and the running time the server plus the total communication is sublinear in North so previous work either.",
                    "label": 0
                },
                {
                    "sent": "Have like almost linear communication or it only works for P small narrow range of P. So this is the our improvement.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally, let me quickly summarize our results.",
                    "label": 0
                },
                {
                    "sent": "So basically we get improved subspace embedding.",
                    "label": 1
                },
                {
                    "sent": "Using exponential random variables and this improves all the previous work on the embedding time, embedding distortion dimension given the optimal almost optimal running time and improve the subspace, emailing also gives the improved LP regression.",
                    "label": 1
                },
                {
                    "sent": "Finally, our regression algorithm can be implemented in the distributed setting efficiently, so there are two open questions.",
                    "label": 0
                },
                {
                    "sent": "So first is we don't know like what is the best distortion.",
                    "label": 0
                },
                {
                    "sent": "Given the almost 4 hour one subspace immediacy, given the almost optimal embedding tie and the the embedding dimensions so.",
                    "label": 0
                },
                {
                    "sent": "Our conjecture is maybe can be improved to do a power 1.5, so currently it's D point 1D to power point five times.",
                    "label": 0
                },
                {
                    "sent": "Some log in factor and maybe it can even be improved to the second is can we show any nontrivial tradeoffs for the lower bounds?",
                    "label": 0
                },
                {
                    "sent": "That's exactly the question asked previously.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you any questions.",
                    "label": 0
                }
            ]
        }
    }
}