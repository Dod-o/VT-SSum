{
    "id": "zjdzmajnmpjupu5mhamdzpptofpurlu3",
    "title": "Learning Scale Free Networks by Reweighted L1 regularization",
    "info": {
        "author": [
            "Qiang Liu, Donald Bren School of Information and Computer Science, University of California, Irvine"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data",
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/aistats2011_liu_learning/",
    "segmentation": [
        [
            "OK, I'm telling you, a PhD student from UC Irvine.",
            "My advisor is Alex Isla and I'm going to talk a work about learning scale free networks using L1 type of methods."
        ],
        [
            "So the problem that I'm going to focus on is the high dimensional structure learning problem in which we want to estimate the dependency structure in Markov random field or Bayesian network."
        ],
        [
            "And application of this problem is very important in many areas, especially in biology.",
            "We are interested in a large body of work is on estimating the gene regulatory network for microarray data and many other examples in many other areas, and the challenge of this problem is also well recognized.",
            "It's this high dimension knowledge.",
            "We were the dimension of the models.",
            "For example in June at work it's the number of genes is much larger than the number of data.",
            "That we use it 1/2."
        ],
        [
            "It seems like for this high dimension problems we really need to find some prior or some low dimensional structure as professor you just talk about to simplify our problem and one type of this prior or recall system specific information is the detailed knowledge about the specific problem that you are working on, but this knowledge usually is very expensive.",
            "They can be very powerful, but it will.",
            "Require very deep domain knowledge is, on the other hand, computer sense test and statisticians more like this universal prior information like sparsity.",
            "This is like what the property that is common in Manning system."
        ],
        [
            "But today I'm going to present a another kind of universal prior information that appears in many types of practical system.",
            "These properties come from the research of complexity.",
            "Complicated network research it fact it suggests that most practical systems usually have some common properties.",
            "For example, many networks are found to be skill free, and they find that in these networks there usually exist some important hubs.",
            "Another example is in social networks, where we usually have this transitivity property based on the principle that friends.",
            "Of your friends are more likely to be your friend today.",
            "I'm going to specifically focus on encourages skillfulness as a prior when we learn we will end.",
            "Network structure."
        ],
        [
            "So what scale free networks is defined by the degree distribution which follows a power law?",
            "Here D is the degree of the nodes, which is the number of edges that it connects to, and one property of power law distribution is that the log log plot is a straight line, so this is a very good way to test whether distribution is really a power law.",
            "Also, another important property of parallel is that it's a heavy tail distribution.",
            "Because the tail probability actually decays polynomially, this means that it gives the chance to find some nodes that have extremely high degrees, and these nodes we call hops.",
            "They usually are usually very important for the network, and it's very important to identify them.",
            "So skillfully also appears in Manning areas like in biology.",
            "We know that some some.",
            "In biology we know that some some gene networks protein networks in social and many social networks, citation networks, Internet.",
            "They are all skill free and."
        ],
        [
            "To to explain why the scale free network are so widely exist, we have this per basic Albert model.",
            "It's a method to generate random scale free networks using this preferential attachment mechanism.",
            "It works like by beginning from some initial networks and then gradually and then at each time point new edges are added and it connects to the."
        ],
        [
            "Tasting notes according to a probability that is proportional to the current degree of the existing nodes.",
            "For example, the blue.",
            "The blue nodes are the existing network as a new node labeled in red.",
            "It has larger probability token."
        ],
        [
            "To the central node of the existing network and this mechanism.",
            "This rich, richer mechanism is very essential for the scale free network and we will show that algorithm later for learning the scale.",
            "Free network actually have also have this mechanism."
        ],
        [
            "So let's come back to the graphical model.",
            "In this work, I will assume the data that we get and follows a Gaussian Markov random field.",
            "And here Omega is the Invesco is matrix node at the distribution of multivariate Gaussian can be factorized as a product of many pairwise factors.",
            "Were the Omega IG equal to 0 or not?",
            "Can depend, decide whether the corresponding provides.",
            "Factor is yellow or not is uniform alot, so the Omega actually reflects the structure of graphical model.",
            "Gaussian Markov random field.",
            "I'm using the same figure that professor just used, so and then now knowing this factor, we can actually reform the network inference problem.",
            "As given some data X and we want to estimate an envelope patterns of the nonzero patterns.",
            "The nonzero patterns of Omega.",
            "But then the problem is still like.",
            "Usually we have very few samples and how to deal with that so?"
        ],
        [
            "So here is some useful property of Gaussian that we're going to use.",
            "First of all, the partial correlation, the partial correlation, which is the correlation of XI an extra given all the other variables is a constant times the Omega edgy, which again is the fact that Omega-3 reflects the structure of the Gaussian Markov random field.",
            "Another, more important factor is that any's I can be represented as a linear.",
            "Azzedine combination of all the other variables plus a Gaussian noise and the regression coefficient.",
            "Beta IG is also again a constant times the Omega IG.",
            "So now the Omega IG, Rd, IG, an beta IG they or their all the structural parameters.",
            "So you just meant you just need to learn one of them is enough.",
            "Um?",
            "Bush.",
            "So."
        ],
        [
            "So, um.",
            "So now there are many L1 type of method solving this problem and probably the simplest one is the neighborhood selection method by myself and Bowman, which simply which simply make use of the autoregression poverty.",
            "It requests every XI using order other variables using a standard lasso regression so that the beta IG which is the regression coefficient, simply tells you your structure of the network.",
            "This method is very computationally efficient, and that's consistency properties in the high dimensional setting.",
            "But the problem is that the beta edgy and beta GI may be inconsistent because they are taken from different independent regression problems.",
            "Usually we solve this inconsistency by using the ender or rose."
        ],
        [
            "Another another method which improve on the neighborhood selection is the joint sparse regression method, which which some solve the inconsistency problem by minimizing the order regression problems problems jointly at the constraint that the beta IG has to be equal to has to be consistent with Inspector GI, and this approach is actually very similar to show the likelihood approach.",
            "The difference is very minor.",
            "Basically, it's only on the diagonal.",
            "Elements, the difference is only on the diagonal diagonal elements."
        ],
        [
            "And finally we have.",
            "We have the graphical lasso problem which maximized L1 regularizer full likelihood, and it turns out that for this type of.",
            "Problem that very efficient algorithm that based on block coding dissent."
        ],
        [
            "So all of these three problems can be wrote in a form, or you have some score function and you want to regularize the and you minimize their one regularised.",
            "Call function and the SIT here is any structural parameter that we get before we're doing this big cause.",
            "The current method don't have consistency about whether OG or beta idea or measure should be regularised, so we just keep this inconsistency here because I'm going to compare all these methods using my method later and a little more about L1 regularization.",
            "So we can treat it as a convex and continuous tailgate.",
            "Of LSL along, which is which.",
            "Simply count how many nonzero elements in the theater in the vector Theta.",
            "But notice that LO is really empty.",
            "Hard, so transforming this replacing L1 is very important.",
            "Another treatment is that we can treat their city the L1 as a Laplacian prior distribution."
        ],
        [
            "So, however, there are one method is actually not very good for schooling scale free networks.",
            "Intuitively, as you use L1 regularization, you actually enforce the sparsity uniformly for all the edges those.",
            "So there's no hope to get some scale free networks so erratically we find that both neighborhood selection and graphical lasso or have a sample size that is polynomial to the maximum degree of the network, but.",
            "Owning logarithm to the degree of the whole system.",
            "So this is actually a very very bad useful L1 type of method.",
            "And experimentally we also find that both our neighborhood selection and the graph glasow doesn't work very well as the maximum degree increases."
        ],
        [
            "So to solve this problem, we introduced the power load regularization.",
            "So again, let's look at the parallel distribution.",
            "Now I'm wrote it in a more detailed away or die.",
            "What the eye is, the degree of the ice node.",
            "So we observe that the DI can be simply wrote as LO one of the ice column of Theta.",
            "Anne from Anne.",
            "Using a Bayesian perspective, we can simply add a lock lock prior into our school function and now we have this new regularised score function were where it's a mention over order rose and then take a log and then sum over all the other roles.",
            "But but this problem?"
        ],
        [
            "Still very hard cause the appearance of their hours.",
            "LOLO ROM what I'm going to do is."
        ],
        [
            "To relax there also I'm using our one warm and now we get a new regularised score function.",
            "And another way to look at this new."
        ],
        [
            "Oh oh, it's called function is that it can actually think as approximate lognormal distribution.",
            "Lognormal distribution is a distribution whose lock is actually along.",
            "So if we vote, its its distribution and assume the variance actually goes to infinite.",
            "So we can actually recover this power law like distribution and this is.",
            "Ann, usually this lock normal distribution is also used sometimes for modeling the degree of the scale free networks.",
            "So somehow we can just treat this L1 regularization as as a log prior on our degrees."
        ],
        [
            "We did some small modification here.",
            "We add a small epsilon to represent to to avoid the singularity and we also add some diagonal regularization.",
            "This is for just for consistent with current methods because graphical lasso usually have some regularization terms."
        ],
        [
            "And now we have this problem.",
            "How to solve this optimization problem first over it's it is non differentiable cause the appearance of the L1 regularization also is nonconvex.",
            "Now because of the log absolute value function, so I plot 1 dimensional case there.",
            "But it turns out that we can solve this problem by using a sequence of standard L1 regularization problem in which the coefficient of the L1.",
            "Can be updated using a way that mimic the preferential attachment mechanism of scale free networks."
        ],
        [
            "So we introduced the algorithm.",
            "I firstly introduced what's called EM algorithm.",
            "It's a generalization of EM algorithm.",
            "It's at each step it approximate the objective function using outbound that is tangent to the current point of your project project.",
            "If and then you minimize your outbound sequentially.",
            "By doing this, you can guarantee the monotonic decreasing of the."
        ],
        [
            "Objective function, so here is a simulation what happening starting from some in."
        ],
        [
            "Issue points it has a low.",
            "We approximate the function using our bound that is tangent at the current point."
        ],
        [
            "And then we minimize the upper bound together new points into one."
        ],
        [
            "And then we do this iteratively and we can make sure that it's very easy to see that the city has yellow is actually guaranteed to be no worse than Sita, Sita Solo."
        ],
        [
            "An impractical and in our case is.",
            "We know that our polo regularization can be actually up bounded by L1 regularization, where the number IG can be rolled in that way, and it's actually if you look at the figure, the number edgy is simply the slope of the of the log curve."
        ],
        [
            "So motivated by that, we can actually roll down a related L1 based optimization with first of all we reinitialize status yellow to be identity matrix and we use it again to represent the estimation at North iteration and then we and then we set set number IG to be a function that depends on the lost the data on the last iteration and then updates data using a standard.",
            "Allow optimization by doing this, we are guaranteed to monotonic decrease the function and what's in."
        ],
        [
            "Thing is that if you look at the formula of the number IG, it actually have preferential attachment mechanism, which means that if your degree if some nodes have very high degree distribution, you will get a low value of number IG which which which will decrease the ALS Elo penalty so that the degree of of this node will tends to increase and then you have this positive feedback.",
            "And this is exactly.",
            "What happens in Schofield Networks?"
        ],
        [
            "So some simple properties of our reweighting method fast over it, monotonic increase.",
            "As I told before, and this message is actually very simple to adaptive to any L1 type of method."
        ],
        [
            "Another thing is that notice that the first iteration of our related method is simply.",
            "Como L1 regularization with Lambda equal to 2A over epsilon, this can be seen from the formula or or the order.",
            "Some mention and of the diagonals are Zillow.",
            "If we initialize data I to identity and this is the reason we we do this initialization."
        ],
        [
            "And suddenly we we will find that later most improvement of the related processes actually in the first few iterations I will talk more in."
        ],
        [
            "Experiment part.",
            "And finally, we know that there are many similar methods that also used used this related procedure in some other places.",
            "For example, it's used to optimize the SADF penalty, which is a non convex.",
            "Non convex regularization.",
            "Also earlier we have this adaptive lasso method which user predefined.",
            "Low quality estimation to adjust the weights.",
            "Although this is only a 2 pass method and also distributed method is also used in the compressive sensing problem where they use it to encourage sparsity and get more accurate results."
        ],
        [
            "So this is our algorithm about before I'm going before I go to the experiment in section, I give a review of other structural regression problems that are based on L1 loss.",
            "Oh, so one of them.",
            "So from the for the very variable selection problems we have this group lasso, which encourage the structure inside each subgroup to be consistent.",
            "And we also have fuse last, which encourage neighborhood.",
            "Variables are consistent for structured learning.",
            "Most of them are many of them motivated actually by the group lasso which use this mix.",
            "Alarm regularization and usually we take the mix long 2B2B2BL2 one which is similar to group LASSO and L Infinity one.",
            "And specifically there are some work very recent work about the node sparsity.",
            "Our proposed by Peng and friend friend meant they actually actually tries to find very few nodes in the network that is very important, so this notation probably will relate to our notation of hops an now."
        ],
        [
            "I'm going to talk about the experiments, so in all my experiment section I will compare our power, low regularization wins the standard L1 regularization using all these different score functions that I used.",
            "The first experiment is about is using some simulated skillful network that is generated by the BA model and we generate the partial correlation inverse conference metric using some uniform.",
            "Using some uniform partial correlation an then we add the regularization coefficient Alpha so that we can get a regularization path.",
            "So this is."
        ],
        [
            "This is a curve that we change, choose different Alpha and Prada OC curve with respect to the true to network.",
            "The dashed lines is the regular graph graphical lasso and the solid line is our method with Paolo Regularization.",
            "So this is a.",
            "This is a similar."
        ],
        [
            "Same but using the joint sparse regression score function.",
            "The dashed line is still regular graphical lasso.",
            "The storylines are method and the magenta line is some method that is based on the joint sparse regression method, but they have some heuristic to actually encourage the scale free network so, but message definitely do much better than the heuristic method."
        ],
        [
            "And also this is the score function for neighborhood selection.",
            "Again, our message is better."
        ],
        [
            "And this is the order of functions.",
            "And then we have."
        ],
        [
            "If we plot the network that we estimated by using different methods, so the left one is the true true network and then the second one is the is the method that using L1 regularization and it looks like we also plot the log log of the of the degree distribution.",
            "It looks like their standard L1 really don't capture very good about the power law distribution and the heuristic that is used.",
            "In the joint regression model, does better than the regular regular one method.",
            "But then our method can actually looks better than their method.",
            "Also, if you look at the network.",
            "Our method, which is the rightmost one, actually have more edges that connects to the hops which are labeled by the red colors.",
            "We also."
        ],
        [
            "Actually plot the percentage of edges that are connected to the hops and the solid lines are all our methods, and dashed lines are L1 regularised methods.",
            "We obviously we will see like always our method can get more edges that connects to the hubs.",
            "We did so to show like how the."
        ],
        [
            "Formance improve over the iterations we show like so this is the two OC code that I showed before.",
            "If we, if I do the iteration iteration."
        ],
        [
            "The second iteration I can already get this good result and then if I."
        ],
        [
            "To further iterations, this is the third iteration."
        ],
        [
            "Forces vision."
        ],
        [
            "Andfifty iteration, so this without actually suggests that, at least in our problem, the most again is in the faster related process, so we actually don't need to do a lot of work, and the this means that our calculation is actually not much worse than the regular L1 method."
        ],
        [
            "We did the same thing, but for another network and this network is generated by.",
            "By using four hops that connects to several other edges, several other nodes, and among these other nodes, we joined them using a random network and the performance is basically the same, very similar as the skillful network."
        ],
        [
            "Also, we did some experiments on some microwave data set.",
            "The Left network is using standard graphical lasso and the right network is using our method, so definitely.",
            "Our method makes the network much like much more skillful like.",
            "In"
        ],
        [
            "Terms of future network future work.",
            "We definitely want to further verify our method in some real biological system.",
            "Especially probably we can.",
            "We can look at the hops that we predicted and go to biology to use some experiments to check it.",
            "Also, we are interested in whether there are some theoretical analysis for this type of method, especially in the case where the degree maximum degree are very high.",
            "And also we are interesting to incorporate more structure prior an probably using the idea that comes from the urban model model is a is a general framework for modeling complex network that have many structured informations.",
            "This is my present."
        ],
        [
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm telling you, a PhD student from UC Irvine.",
                    "label": 0
                },
                {
                    "sent": "My advisor is Alex Isla and I'm going to talk a work about learning scale free networks using L1 type of methods.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem that I'm going to focus on is the high dimensional structure learning problem in which we want to estimate the dependency structure in Markov random field or Bayesian network.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And application of this problem is very important in many areas, especially in biology.",
                    "label": 0
                },
                {
                    "sent": "We are interested in a large body of work is on estimating the gene regulatory network for microarray data and many other examples in many other areas, and the challenge of this problem is also well recognized.",
                    "label": 0
                },
                {
                    "sent": "It's this high dimension knowledge.",
                    "label": 0
                },
                {
                    "sent": "We were the dimension of the models.",
                    "label": 0
                },
                {
                    "sent": "For example in June at work it's the number of genes is much larger than the number of data.",
                    "label": 1
                },
                {
                    "sent": "That we use it 1/2.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It seems like for this high dimension problems we really need to find some prior or some low dimensional structure as professor you just talk about to simplify our problem and one type of this prior or recall system specific information is the detailed knowledge about the specific problem that you are working on, but this knowledge usually is very expensive.",
                    "label": 0
                },
                {
                    "sent": "They can be very powerful, but it will.",
                    "label": 0
                },
                {
                    "sent": "Require very deep domain knowledge is, on the other hand, computer sense test and statisticians more like this universal prior information like sparsity.",
                    "label": 0
                },
                {
                    "sent": "This is like what the property that is common in Manning system.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But today I'm going to present a another kind of universal prior information that appears in many types of practical system.",
                    "label": 0
                },
                {
                    "sent": "These properties come from the research of complexity.",
                    "label": 0
                },
                {
                    "sent": "Complicated network research it fact it suggests that most practical systems usually have some common properties.",
                    "label": 0
                },
                {
                    "sent": "For example, many networks are found to be skill free, and they find that in these networks there usually exist some important hubs.",
                    "label": 0
                },
                {
                    "sent": "Another example is in social networks, where we usually have this transitivity property based on the principle that friends.",
                    "label": 0
                },
                {
                    "sent": "Of your friends are more likely to be your friend today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to specifically focus on encourages skillfulness as a prior when we learn we will end.",
                    "label": 0
                },
                {
                    "sent": "Network structure.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what scale free networks is defined by the degree distribution which follows a power law?",
                    "label": 0
                },
                {
                    "sent": "Here D is the degree of the nodes, which is the number of edges that it connects to, and one property of power law distribution is that the log log plot is a straight line, so this is a very good way to test whether distribution is really a power law.",
                    "label": 1
                },
                {
                    "sent": "Also, another important property of parallel is that it's a heavy tail distribution.",
                    "label": 0
                },
                {
                    "sent": "Because the tail probability actually decays polynomially, this means that it gives the chance to find some nodes that have extremely high degrees, and these nodes we call hops.",
                    "label": 0
                },
                {
                    "sent": "They usually are usually very important for the network, and it's very important to identify them.",
                    "label": 0
                },
                {
                    "sent": "So skillfully also appears in Manning areas like in biology.",
                    "label": 0
                },
                {
                    "sent": "We know that some some.",
                    "label": 1
                },
                {
                    "sent": "In biology we know that some some gene networks protein networks in social and many social networks, citation networks, Internet.",
                    "label": 0
                },
                {
                    "sent": "They are all skill free and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To to explain why the scale free network are so widely exist, we have this per basic Albert model.",
                    "label": 0
                },
                {
                    "sent": "It's a method to generate random scale free networks using this preferential attachment mechanism.",
                    "label": 1
                },
                {
                    "sent": "It works like by beginning from some initial networks and then gradually and then at each time point new edges are added and it connects to the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tasting notes according to a probability that is proportional to the current degree of the existing nodes.",
                    "label": 1
                },
                {
                    "sent": "For example, the blue.",
                    "label": 0
                },
                {
                    "sent": "The blue nodes are the existing network as a new node labeled in red.",
                    "label": 0
                },
                {
                    "sent": "It has larger probability token.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the central node of the existing network and this mechanism.",
                    "label": 1
                },
                {
                    "sent": "This rich, richer mechanism is very essential for the scale free network and we will show that algorithm later for learning the scale.",
                    "label": 0
                },
                {
                    "sent": "Free network actually have also have this mechanism.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's come back to the graphical model.",
                    "label": 0
                },
                {
                    "sent": "In this work, I will assume the data that we get and follows a Gaussian Markov random field.",
                    "label": 1
                },
                {
                    "sent": "And here Omega is the Invesco is matrix node at the distribution of multivariate Gaussian can be factorized as a product of many pairwise factors.",
                    "label": 0
                },
                {
                    "sent": "Were the Omega IG equal to 0 or not?",
                    "label": 0
                },
                {
                    "sent": "Can depend, decide whether the corresponding provides.",
                    "label": 0
                },
                {
                    "sent": "Factor is yellow or not is uniform alot, so the Omega actually reflects the structure of graphical model.",
                    "label": 0
                },
                {
                    "sent": "Gaussian Markov random field.",
                    "label": 0
                },
                {
                    "sent": "I'm using the same figure that professor just used, so and then now knowing this factor, we can actually reform the network inference problem.",
                    "label": 0
                },
                {
                    "sent": "As given some data X and we want to estimate an envelope patterns of the nonzero patterns.",
                    "label": 0
                },
                {
                    "sent": "The nonzero patterns of Omega.",
                    "label": 1
                },
                {
                    "sent": "But then the problem is still like.",
                    "label": 0
                },
                {
                    "sent": "Usually we have very few samples and how to deal with that so?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is some useful property of Gaussian that we're going to use.",
                    "label": 1
                },
                {
                    "sent": "First of all, the partial correlation, the partial correlation, which is the correlation of XI an extra given all the other variables is a constant times the Omega edgy, which again is the fact that Omega-3 reflects the structure of the Gaussian Markov random field.",
                    "label": 1
                },
                {
                    "sent": "Another, more important factor is that any's I can be represented as a linear.",
                    "label": 0
                },
                {
                    "sent": "Azzedine combination of all the other variables plus a Gaussian noise and the regression coefficient.",
                    "label": 0
                },
                {
                    "sent": "Beta IG is also again a constant times the Omega IG.",
                    "label": 0
                },
                {
                    "sent": "So now the Omega IG, Rd, IG, an beta IG they or their all the structural parameters.",
                    "label": 0
                },
                {
                    "sent": "So you just meant you just need to learn one of them is enough.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Bush.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So now there are many L1 type of method solving this problem and probably the simplest one is the neighborhood selection method by myself and Bowman, which simply which simply make use of the autoregression poverty.",
                    "label": 1
                },
                {
                    "sent": "It requests every XI using order other variables using a standard lasso regression so that the beta IG which is the regression coefficient, simply tells you your structure of the network.",
                    "label": 0
                },
                {
                    "sent": "This method is very computationally efficient, and that's consistency properties in the high dimensional setting.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that the beta edgy and beta GI may be inconsistent because they are taken from different independent regression problems.",
                    "label": 0
                },
                {
                    "sent": "Usually we solve this inconsistency by using the ender or rose.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another another method which improve on the neighborhood selection is the joint sparse regression method, which which some solve the inconsistency problem by minimizing the order regression problems problems jointly at the constraint that the beta IG has to be equal to has to be consistent with Inspector GI, and this approach is actually very similar to show the likelihood approach.",
                    "label": 0
                },
                {
                    "sent": "The difference is very minor.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's only on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Elements, the difference is only on the diagonal diagonal elements.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally we have.",
                    "label": 0
                },
                {
                    "sent": "We have the graphical lasso problem which maximized L1 regularizer full likelihood, and it turns out that for this type of.",
                    "label": 0
                },
                {
                    "sent": "Problem that very efficient algorithm that based on block coding dissent.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all of these three problems can be wrote in a form, or you have some score function and you want to regularize the and you minimize their one regularised.",
                    "label": 0
                },
                {
                    "sent": "Call function and the SIT here is any structural parameter that we get before we're doing this big cause.",
                    "label": 0
                },
                {
                    "sent": "The current method don't have consistency about whether OG or beta idea or measure should be regularised, so we just keep this inconsistency here because I'm going to compare all these methods using my method later and a little more about L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "So we can treat it as a convex and continuous tailgate.",
                    "label": 0
                },
                {
                    "sent": "Of LSL along, which is which.",
                    "label": 0
                },
                {
                    "sent": "Simply count how many nonzero elements in the theater in the vector Theta.",
                    "label": 0
                },
                {
                    "sent": "But notice that LO is really empty.",
                    "label": 0
                },
                {
                    "sent": "Hard, so transforming this replacing L1 is very important.",
                    "label": 0
                },
                {
                    "sent": "Another treatment is that we can treat their city the L1 as a Laplacian prior distribution.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, however, there are one method is actually not very good for schooling scale free networks.",
                    "label": 1
                },
                {
                    "sent": "Intuitively, as you use L1 regularization, you actually enforce the sparsity uniformly for all the edges those.",
                    "label": 0
                },
                {
                    "sent": "So there's no hope to get some scale free networks so erratically we find that both neighborhood selection and graphical lasso or have a sample size that is polynomial to the maximum degree of the network, but.",
                    "label": 0
                },
                {
                    "sent": "Owning logarithm to the degree of the whole system.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a very very bad useful L1 type of method.",
                    "label": 0
                },
                {
                    "sent": "And experimentally we also find that both our neighborhood selection and the graph glasow doesn't work very well as the maximum degree increases.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to solve this problem, we introduced the power load regularization.",
                    "label": 0
                },
                {
                    "sent": "So again, let's look at the parallel distribution.",
                    "label": 0
                },
                {
                    "sent": "Now I'm wrote it in a more detailed away or die.",
                    "label": 0
                },
                {
                    "sent": "What the eye is, the degree of the ice node.",
                    "label": 1
                },
                {
                    "sent": "So we observe that the DI can be simply wrote as LO one of the ice column of Theta.",
                    "label": 0
                },
                {
                    "sent": "Anne from Anne.",
                    "label": 0
                },
                {
                    "sent": "Using a Bayesian perspective, we can simply add a lock lock prior into our school function and now we have this new regularised score function were where it's a mention over order rose and then take a log and then sum over all the other roles.",
                    "label": 0
                },
                {
                    "sent": "But but this problem?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still very hard cause the appearance of their hours.",
                    "label": 0
                },
                {
                    "sent": "LOLO ROM what I'm going to do is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To relax there also I'm using our one warm and now we get a new regularised score function.",
                    "label": 0
                },
                {
                    "sent": "And another way to look at this new.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh oh, it's called function is that it can actually think as approximate lognormal distribution.",
                    "label": 0
                },
                {
                    "sent": "Lognormal distribution is a distribution whose lock is actually along.",
                    "label": 0
                },
                {
                    "sent": "So if we vote, its its distribution and assume the variance actually goes to infinite.",
                    "label": 0
                },
                {
                    "sent": "So we can actually recover this power law like distribution and this is.",
                    "label": 0
                },
                {
                    "sent": "Ann, usually this lock normal distribution is also used sometimes for modeling the degree of the scale free networks.",
                    "label": 0
                },
                {
                    "sent": "So somehow we can just treat this L1 regularization as as a log prior on our degrees.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did some small modification here.",
                    "label": 0
                },
                {
                    "sent": "We add a small epsilon to represent to to avoid the singularity and we also add some diagonal regularization.",
                    "label": 0
                },
                {
                    "sent": "This is for just for consistent with current methods because graphical lasso usually have some regularization terms.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we have this problem.",
                    "label": 0
                },
                {
                    "sent": "How to solve this optimization problem first over it's it is non differentiable cause the appearance of the L1 regularization also is nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Now because of the log absolute value function, so I plot 1 dimensional case there.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that we can solve this problem by using a sequence of standard L1 regularization problem in which the coefficient of the L1.",
                    "label": 1
                },
                {
                    "sent": "Can be updated using a way that mimic the preferential attachment mechanism of scale free networks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we introduced the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I firstly introduced what's called EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a generalization of EM algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's at each step it approximate the objective function using outbound that is tangent to the current point of your project project.",
                    "label": 1
                },
                {
                    "sent": "If and then you minimize your outbound sequentially.",
                    "label": 0
                },
                {
                    "sent": "By doing this, you can guarantee the monotonic decreasing of the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Objective function, so here is a simulation what happening starting from some in.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issue points it has a low.",
                    "label": 0
                },
                {
                    "sent": "We approximate the function using our bound that is tangent at the current point.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we minimize the upper bound together new points into one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we do this iteratively and we can make sure that it's very easy to see that the city has yellow is actually guaranteed to be no worse than Sita, Sita Solo.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An impractical and in our case is.",
                    "label": 0
                },
                {
                    "sent": "We know that our polo regularization can be actually up bounded by L1 regularization, where the number IG can be rolled in that way, and it's actually if you look at the figure, the number edgy is simply the slope of the of the log curve.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So motivated by that, we can actually roll down a related L1 based optimization with first of all we reinitialize status yellow to be identity matrix and we use it again to represent the estimation at North iteration and then we and then we set set number IG to be a function that depends on the lost the data on the last iteration and then updates data using a standard.",
                    "label": 0
                },
                {
                    "sent": "Allow optimization by doing this, we are guaranteed to monotonic decrease the function and what's in.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing is that if you look at the formula of the number IG, it actually have preferential attachment mechanism, which means that if your degree if some nodes have very high degree distribution, you will get a low value of number IG which which which will decrease the ALS Elo penalty so that the degree of of this node will tends to increase and then you have this positive feedback.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly.",
                    "label": 0
                },
                {
                    "sent": "What happens in Schofield Networks?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some simple properties of our reweighting method fast over it, monotonic increase.",
                    "label": 0
                },
                {
                    "sent": "As I told before, and this message is actually very simple to adaptive to any L1 type of method.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing is that notice that the first iteration of our related method is simply.",
                    "label": 0
                },
                {
                    "sent": "Como L1 regularization with Lambda equal to 2A over epsilon, this can be seen from the formula or or the order.",
                    "label": 1
                },
                {
                    "sent": "Some mention and of the diagonals are Zillow.",
                    "label": 0
                },
                {
                    "sent": "If we initialize data I to identity and this is the reason we we do this initialization.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And suddenly we we will find that later most improvement of the related processes actually in the first few iterations I will talk more in.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment part.",
                    "label": 0
                },
                {
                    "sent": "And finally, we know that there are many similar methods that also used used this related procedure in some other places.",
                    "label": 1
                },
                {
                    "sent": "For example, it's used to optimize the SADF penalty, which is a non convex.",
                    "label": 0
                },
                {
                    "sent": "Non convex regularization.",
                    "label": 0
                },
                {
                    "sent": "Also earlier we have this adaptive lasso method which user predefined.",
                    "label": 1
                },
                {
                    "sent": "Low quality estimation to adjust the weights.",
                    "label": 0
                },
                {
                    "sent": "Although this is only a 2 pass method and also distributed method is also used in the compressive sensing problem where they use it to encourage sparsity and get more accurate results.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our algorithm about before I'm going before I go to the experiment in section, I give a review of other structural regression problems that are based on L1 loss.",
                    "label": 0
                },
                {
                    "sent": "Oh, so one of them.",
                    "label": 0
                },
                {
                    "sent": "So from the for the very variable selection problems we have this group lasso, which encourage the structure inside each subgroup to be consistent.",
                    "label": 1
                },
                {
                    "sent": "And we also have fuse last, which encourage neighborhood.",
                    "label": 0
                },
                {
                    "sent": "Variables are consistent for structured learning.",
                    "label": 0
                },
                {
                    "sent": "Most of them are many of them motivated actually by the group lasso which use this mix.",
                    "label": 0
                },
                {
                    "sent": "Alarm regularization and usually we take the mix long 2B2B2BL2 one which is similar to group LASSO and L Infinity one.",
                    "label": 1
                },
                {
                    "sent": "And specifically there are some work very recent work about the node sparsity.",
                    "label": 0
                },
                {
                    "sent": "Our proposed by Peng and friend friend meant they actually actually tries to find very few nodes in the network that is very important, so this notation probably will relate to our notation of hops an now.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about the experiments, so in all my experiment section I will compare our power, low regularization wins the standard L1 regularization using all these different score functions that I used.",
                    "label": 0
                },
                {
                    "sent": "The first experiment is about is using some simulated skillful network that is generated by the BA model and we generate the partial correlation inverse conference metric using some uniform.",
                    "label": 1
                },
                {
                    "sent": "Using some uniform partial correlation an then we add the regularization coefficient Alpha so that we can get a regularization path.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a curve that we change, choose different Alpha and Prada OC curve with respect to the true to network.",
                    "label": 0
                },
                {
                    "sent": "The dashed lines is the regular graph graphical lasso and the solid line is our method with Paolo Regularization.",
                    "label": 1
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a similar.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same but using the joint sparse regression score function.",
                    "label": 1
                },
                {
                    "sent": "The dashed line is still regular graphical lasso.",
                    "label": 0
                },
                {
                    "sent": "The storylines are method and the magenta line is some method that is based on the joint sparse regression method, but they have some heuristic to actually encourage the scale free network so, but message definitely do much better than the heuristic method.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also this is the score function for neighborhood selection.",
                    "label": 0
                },
                {
                    "sent": "Again, our message is better.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the order of functions.",
                    "label": 0
                },
                {
                    "sent": "And then we have.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we plot the network that we estimated by using different methods, so the left one is the true true network and then the second one is the is the method that using L1 regularization and it looks like we also plot the log log of the of the degree distribution.",
                    "label": 0
                },
                {
                    "sent": "It looks like their standard L1 really don't capture very good about the power law distribution and the heuristic that is used.",
                    "label": 0
                },
                {
                    "sent": "In the joint regression model, does better than the regular regular one method.",
                    "label": 0
                },
                {
                    "sent": "But then our method can actually looks better than their method.",
                    "label": 0
                },
                {
                    "sent": "Also, if you look at the network.",
                    "label": 0
                },
                {
                    "sent": "Our method, which is the rightmost one, actually have more edges that connects to the hops which are labeled by the red colors.",
                    "label": 0
                },
                {
                    "sent": "We also.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually plot the percentage of edges that are connected to the hops and the solid lines are all our methods, and dashed lines are L1 regularised methods.",
                    "label": 1
                },
                {
                    "sent": "We obviously we will see like always our method can get more edges that connects to the hubs.",
                    "label": 0
                },
                {
                    "sent": "We did so to show like how the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formance improve over the iterations we show like so this is the two OC code that I showed before.",
                    "label": 0
                },
                {
                    "sent": "If we, if I do the iteration iteration.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second iteration I can already get this good result and then if I.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To further iterations, this is the third iteration.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forces vision.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Andfifty iteration, so this without actually suggests that, at least in our problem, the most again is in the faster related process, so we actually don't need to do a lot of work, and the this means that our calculation is actually not much worse than the regular L1 method.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did the same thing, but for another network and this network is generated by.",
                    "label": 0
                },
                {
                    "sent": "By using four hops that connects to several other edges, several other nodes, and among these other nodes, we joined them using a random network and the performance is basically the same, very similar as the skillful network.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we did some experiments on some microwave data set.",
                    "label": 0
                },
                {
                    "sent": "The Left network is using standard graphical lasso and the right network is using our method, so definitely.",
                    "label": 0
                },
                {
                    "sent": "Our method makes the network much like much more skillful like.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Terms of future network future work.",
                    "label": 0
                },
                {
                    "sent": "We definitely want to further verify our method in some real biological system.",
                    "label": 1
                },
                {
                    "sent": "Especially probably we can.",
                    "label": 0
                },
                {
                    "sent": "We can look at the hops that we predicted and go to biology to use some experiments to check it.",
                    "label": 1
                },
                {
                    "sent": "Also, we are interested in whether there are some theoretical analysis for this type of method, especially in the case where the degree maximum degree are very high.",
                    "label": 0
                },
                {
                    "sent": "And also we are interesting to incorporate more structure prior an probably using the idea that comes from the urban model model is a is a general framework for modeling complex network that have many structured informations.",
                    "label": 0
                },
                {
                    "sent": "This is my present.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}