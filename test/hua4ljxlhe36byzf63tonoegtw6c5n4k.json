{
    "id": "hua4ljxlhe36byzf63tonoegtw6c5n4k",
    "title": "Learning with Dependencies between Several Response Variables",
    "info": {
        "author": [
            "Volker Tresp, Siemens AG"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Multi-Task Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_tresp_ldb/",
    "segmentation": [
        [
            "So this is the tutorial on learning with dependencies between several response variables from hierarchical Bayesian multitask learning to structural output prediction and relational learning.",
            "I'm focused RESP.",
            "I'm from Siemens in Munich from corporate technology, so that's.",
            "Research and development and.",
            "The core author on this tutorial is KU.",
            "He's currently at NEC Labs America when.",
            "Much of the work was done, which is presented here.",
            "He was also working at Siemens at that time, and he's also here so you can answer all the diff."
        ],
        [
            "The questions.",
            "OK, sure.",
            "So this tutorial is about multiple outputs.",
            "And I want to start with this quote from the book by his teacher.",
            "Only Friedman and data mining.",
            "Multiple outputs do not affect each other's least squares estimates.",
            "And this statement is certainly correct in this.",
            "In the context it was made.",
            "Of linear systems linear multivariate systems.",
            "But unfortunately it has been over generalized, so when I started.",
            "Machine learning it was essentially neural networks and the neural networks we had.",
            "We started off with a lot of applications with multiple outputs.",
            "So for example in.",
            "ZIP code recognition.",
            "There were ten outputs, one for each digit.",
            "And a lot of the initial work was done with neural networks with multiple outputs.",
            "But at some point people thought this is maybe not really important, and learning with one output is much simpler and difficult enough.",
            "Easy to describe and so the emphasis definitely changed and we started doing neural networks with one output.",
            "And also in this book there's almost no reference to multivariate prediction.",
            "There's a little bit on protection methods, but not much, and so it seems that also statistics didn't really consider this in great detail.",
            "So this presentation will be mostly about cases where this statement is not applicable."
        ],
        [
            "And so you go, then you go to the other extreme, and then you look around a little bit.",
            "What has been done in this direction of multiple outputs, and suddenly you discover that there's quite a lot of work.",
            "Are there from different fields?",
            "With different applications in mind so.",
            "If you go through some of the terms or hierarchical Bayes, obviously it's from the Asian community, then the next terms inductive transfer learning multi label prediction, multitask learning is more from the machine learning community.",
            "Then there are random effect models, random parameter vectors, mixed models, mixed effect models, nested models, multi level models, hierarchical linear models, generalized mixed models for these are from the more frequentist statistical side.",
            "Then we have collaborative filtering, which is sort of the application.",
            "Which motivated our work in this direction.",
            "Then the projection methods, Canonical correlation analysis, maximum covariance regression partially squares multivariate regression.",
            "So this is also a subfield, very much application driven.",
            "And then there is a multivariate regression structured output prediction.",
            "Particularly, last thing is quite popular in machine learning.",
            "Currently in the last years.",
            "And we are part of that part of the presentation, at least some of it.",
            "And probably there are many more things that I. Oh yeah, not even aware of.",
            "So this is an attempt to provide a view.",
            "So not necessarily an overview, because I'm not sure that all the important aspects are covered, but hopefully still a very useful view and in general with Asian flavor but not strictly Beijing.",
            "So it's not about.",
            "Complex sampling everywhere.",
            "So the first part will be about hierarchical base and mixed models.",
            "We started with the problem setting and simple solutions.",
            "Then the cooler hierarchical basin mixed models so mixed models is sort of the frequentist equivalent to hierarchical Bayes.",
            "Then a little bit on nonparametric hierarchical Bayes.",
            "Then production methods.",
            "The third part of a multivariate models and structured output and the 4th partner link prediction and relationship addiction.",
            "So this is a lot and of course you cannot.",
            "Everywhere in great depths.",
            "But maybe this is still interesting to get this view on these different aspects and please ask questions because that sort of Inter."
        ],
        [
            "The flow a little bit and.",
            "Makes it more comprehensible.",
            "So this is the classic generic supervised learning task.",
            "So we have a data matrix, the training data and the rows are data points, so that correspond to objects, entities, situations, measurements and so on, and the columns.",
            "Consist of the input.",
            "Representation input vector X one to XN and one single output variable Y who I want to wire.",
            "And typical typical situations why I might be a function of XI with parameter W plus some noise.",
            "So this is a typical setting in regression or classification.",
            "The probability that Yi equals one is some transfer function, sigmoid transfer function applied to the function F of XI.",
            "So this is what people have studied.",
            "Many years and where a lot of work has been done.",
            "So may."
        ],
        [
            "There's a new generic learning problem where the only difference really is that the Y is not one dimensional, but at M dimensional.",
            "So again, Rozar data points columns are now the input vector X and the output vector Y.",
            "And this is also the perspective of the presentation little bit, so we have this situation.",
            "Somebody gave us this data, input several outputs.",
            "What should we do?",
            "Of course, in the particular situation there is often a lot of prior knowledge about the application is pretty clear what one should do, but we want to take take a step back and more think about what could be done in this situation."
        ],
        [
            "So I am starting with a little bit of an overview week before we go in more detail in the overview is a little bit long, maybe so the 1st two things are in hierarchical based mixed models and projection approaches, so we're here we have the data matrix, again blue the inputs green are the outputs.",
            "And in in the first in hierarchical basics models, infection methods each output dimensions for each situation or each task is trained independently.",
            "But not independently given and transformation respectively given the prior distribution that is found using all data.",
            "So you can think of it as having 2 steps.",
            "The first step we are trying to find.",
            "Either this representation is transformation are sort of tune the prior distribution of the parameters.",
            "But when that is done in the next step, we treat each output dimension independently again, so there's some coupling, some exploitation of other output dimensions.",
            "By forming these transformations.",
            "But after that we try, we do independent modeling again, and in hierarchical based statistical strengths between multiple outputs are shared by common parameters in the prior distribution.",
            "So typically we assume a private solution and we tune the parameters in the prior distribution using all the data.",
            "The projection method.",
            "The input is mapped to a lower dimensional space that was found found using all the data.",
            "Yes.",
            "So I have a microphone I think for the camera.",
            "Still difficult to hear.",
            "OK, I tried to speak louder.",
            "The difficult acoustic.",
            "Hello, it's panel.",
            "OK. Yeah, and when is this applicable when we can assume that the functional dependencies for all outputs?",
            "I come from the same simple family of distributions.",
            "So it doesn't have to be a linear model, it can be linear in some basis functions or also in kernels and representations.",
            "But it should be from the same family of things there, so we shouldn't try to do a hierarchical basian thing of the functions have no thing things in common, so the underlying assumptions they should come from sort of the same family of distributions.",
            "So here is 1 example."
        ],
        [
            "From industry data has been collected and models have been trained for implants.",
            "We want to generalize to plant N + 1 where either no or little data is available.",
            "So this is a common situation.",
            "This is a steel rolling mill, so here the steels are compressed so become thinner and for example here it is important to predict the right rolling force and it's impossible to derive these models from first principles.",
            "It has been tried but it wasn't very successful, so it has to be a data driven model.",
            "But there is not a simple problem because the plans they change overtime in some uncontrollable way.",
            "It's not well understood why they change on a daily basis even.",
            "And also if you go to a new plant to news factory, you cannot really take your old model and just apply it to this plant.",
            "But you want would like to inherit the knowledge you have learned in the previous plans.",
            "So this is a typical hierarchical basian situation for the new plan, you don't want to start with zero knowledge, but you can also not assume that the last model can simply be taken and applied here in the new plan.",
            "So this is a real problem."
        ],
        [
            "Oh, here's another example.",
            "Data for length of stay prediction of outcome prediction has been collected for patients in hospitals.",
            "Can we generalize to patients in hospital M + 1?",
            "Now these models are there of course dependent on patient properties, the diagnosis, the age, maybe the procedures which have been taken, but there will be also a variation between different hospitals, so maybe the staff has gotten different training.",
            "A different patient mix different parts of the country, so there might be different reasons why the models are not really exchangeable.",
            "Where we where you need a personalized or special model for each plant, but again, you don't want to like start from zero, you want to exploit all the knowledge you have acquired in the models from from other hospitals."
        ],
        [
            "So here is a little bit of new situation, but let's first look at the data matrix properties of patient one, patient, patient, N. And here length of stay prediction for hospital Wanan for hospital M. And yeah, if you think of it, you might quickly notice something.",
            "Naturally a given patient has typically been only in one hospital.",
            "So technically, for each input, only one output might be available, so it's more this situation.",
            "So we have your patient properties and we only have.",
            "One measurement.",
            "Of all the output dimensions for each patient, 'cause the patient doesn't wander around in different hospitals and tries different things in general.",
            "So in some applications we have to deal with this problem that only little data is available in this matrix, and typically one makes this draw this picture also a little bit differently, but we want to stick with the Matrix an from this point of view we have to assume all the stuff is unknown.",
            "We only have very small number of measurements.",
            "And this is something we are.",
            "Occupation modeling fits perfectly becausw it has no problems dealing with this situation and but other approaches we will learn about today.",
            "I have difficulties in this situation.",
            "Yeah, we will come to that in a second."
        ],
        [
            "10 seconds, but I could point."
        ],
        [
            "Yes.",
            "Also very quick remark.",
            "We will also come to that in 10 seconds.",
            "But the reasons, of course you want to exploit all the data and if you just look at the data in one hospital you don't have so much data available.",
            "So if you have a lot of data, of course it's the right answer, not just ignore all the other data, but if you don't have much data available in the application then you might want to also benefit from the data from the other hospitals.",
            "But we will come to that also nothing."
        ],
        [
            "So it might be very prediction.",
            "Situation is different, so after training we want to maintain the dependencies between the outputs and they're often explicitly models.",
            "And statistical strength.",
            "So parameters exploit all the information on all the variables if they are sensitive to all the variables in all the models and so now.",
            "So then to tune this W you have not only all data point but also all dimensions available and we will study this in more detail.",
            "So here's some examples."
        ],
        [
            "Typically, for a given object, several output variables or labels are measured.",
            "Is it easier to predict em labels than it is to each of them separately?",
            "So here in decision support, for example, for a given patient, many procedures are possible.",
            "So you can think of it as a mapping from patient properties to what procedures should this patient be given?",
            "Like depending on the diagnosis for example?",
            "And and of course, their clusters now.",
            "So the procedures there's some structure there now.",
            "So if you if you're pregnant, you get sort of 10 procedures.",
            "Each pregnant woman gets there, let's say.",
            "So there are definitely clusters in this data and this should be exploited.",
            "Oh, and recommendation system for a given user.",
            "Many items might be of interest.",
            "Well, it's not about what this person exactly like this item, but typically we do some ranking.",
            "So which of the items with the user like best like most so?",
            "But there are many things to be predicted like all the preferences for all the different items?",
            "On the semantic web, for a given text, many annotations are possible.",
            "So the mapping from high dimension, the description of the text to a high dimensional annotation."
        ],
        [
            "So especially cases, structured output prediction where there is a known dependency structure between the outputs, and this simplifies the model.",
            "Because if you want to predict an M dimensional distribution and you don't have any in dependencies or simplification, this is just an impossible problem and in structured output prediction we assume that there are some dependencies, for example, that this is a linear chain that the I&J&J plus one output are coupled in some sense.",
            "Also, sometimes you can exploit parameter sharing which improves the data efficiency.",
            "So if you reduce the parameters because a lot of parameters are the same, this of course helps a lot.",
            "Applicable when the structural dependencies between the outputs are."
        ],
        [
            "No.",
            "So here's an example which we will see a few times during the presentation.",
            "So if you don't get it first time around, maybe later on it will be more clear.",
            "So this is the situation where you have a text sentence.",
            "Let's assume now all the tenants have M words for simplicity.",
            "So these are M words and we want to map them to labels for each word.",
            "And typically we want to label if something is a city or a name or company or profession over football clap something like that.",
            "So and maybe we have 10 classes, so the decision is it's one of these 10 classes or it's none of these 10 classes.",
            "And typically you present that in this application as a linear chain, so the labels are on the top here.",
            "So in this case 8 labels for 8 words XI stands for the ice thing sentence.",
            "And what you're doing here you can define a probability distribution over the wise given the X in this form, which is lock linear model.",
            "But the important thing is to think about this part.",
            "So here are so called feature functions which calculate some sensible features on some X and some Y.",
            "So in this case on the X and the consecutive wise 'cause they form a click in this graph a click is a fully connected.",
            "Graph and we can show that.",
            "Most other probability distributions can be written in this way, so forget about the normalization exponent for.",
            "So what you're seeing here is a superposition of basis functions, and that seems to be intuitive and simple.",
            "So if this is large, should get higher probability with smaller negative should get smaller probability the exponent makes it an or not negative, and it should be normalized, so we will see this a few times during the talk, so you might want to get used to it.",
            "Already a little bit, but you will hopefully understand it at the end of the talk.",
            "One thing to think about it.",
            "It's really not necessarily a prediction in the introduction and say OK Class one Class 0 here you predict probably solution of all the outputs.",
            "So you essentially what you think about it should give you a score for distribution of wires and maybe you're interested in the most likely configuration of the wise.",
            "Then you have to do some one more step to find the most likely configuration, because sometimes it's more intuitive to think of this as a scoring function.",
            "Then the prediction function.",
            "OK, the notation means I set a point, but all dimensions all outputs.",
            "Join some condition.",
            "See.",
            "Oh, this is a click.",
            "So this is some overall clicks, so this is one of them is the next one.",
            "So some overall clicks and then some, all overall features functions defined for the clicks.",
            "Classes at this joint.",
            "I mean no, no, no.",
            "Is that disjoint then simplifies greatly now in general they're not discharged."
        ],
        [
            "Yeah, one thing you can explore is if so, these are the click variables in the click this function.",
            "This is.",
            "Wait this stuff we will learn if these guys don't depend on the click.",
            "So if you apply the same functions to all clicks then this simplifies the problem.",
            "And also if you use the same parameters here then you can do parameter sharing.",
            "So now the index C disappeared and everything so reduces the number of free parameters greatly and.",
            "In some application, is absolutely necessary that this is possible.",
            "And also another effect is that then you can handle also sequences of different length.",
            "Before I said every sentence has M words.",
            "Now we can also deal with.",
            "Center so varying length because we just called."
        ],
        [
            "Hair care about the clicks.",
            "So here's an example.",
            "Another example image restoration.",
            "So in this case X is a whole image, so serialized pixels.",
            "Because we want to have a vector and these are all the pixel labels, let's say and here these are sort of typical clicks.",
            "This is a pixel value.",
            "You want to predict.",
            "This is the measurement, so this should be close.",
            "But this should also depend on neighboring pixels and you formulate it in terms of the corresponding clicks which are.",
            "Only consists of two.",
            "Neighboring pixels here on the neighboring nodes.",
            "And you will notice that this is not."
        ],
        [
            "And not an automated.",
            "This is really a new story.",
            "Of course people have done this in some domains for a long time.",
            "This is another example social network analysis.",
            "Your notes are actors.",
            "Like people who know each other beside each other.",
            "A typical task is the classification of actors based on the attributes of actors.",
            "So maybe want to predict social status or something.",
            "And then you have your income, wealth, whatever.",
            "Neighborhood.",
            "But you also often you you have homophily here, which means that people who are similar to each other.",
            "So if you want to predict something wealth or income or social status.",
            "It's very likely that your friends from the same status and this can be exploited in these type of networks is called Houma Houma failure.",
            "And, uh.",
            "In our so from our point of view, all these guys here are inputs.",
            "The labels of all these actors and all the unknown labels.",
            "Here are the target functions.",
            "So if you have only one social network available for training, which is often the case, then you have to be aware that this is really only one data point.",
            "So this would be all the attributes and this would all be the class labels.",
            "Now, because obviously they're dependent and you cannot just segment them into independent actors or something, they're all coupled.",
            "So one network is one data point and here learning also relies on parameter sharing.",
            "If everybody has its own parameter, then you cannot learn anything.",
            "So yeah, parameter sharing is very important that the dependencies between, let's say the wealth of your neighbors is sort of the same all over the network.",
            "And and you also might have, in varying number of neighbors, so in the linear chain, every guy has two neighbors, or maybe one neighbor.",
            "But here some people might have many neighbors.",
            "Some people might have no no friends.",
            "So you might have many friends, so this makes it also a little bit more complicated, and sometimes you need some form of aggregation to handle."
        ],
        [
            "Oh, here's another example from the presentation from Tosca and the left.",
            "You'll see a sequence of proteins, so this is easy to get how it is, and this is the three dimensional structure which is very difficult to get and sort of a conditional random field type model.",
            "That was the one I showed before would sort of score given a sequence given Candidate 3 dimensional structures.",
            "How well does it fit?",
            "Here is sort of obvious that this is not a prediction, 'cause I don't think you can really predict the three dimensional structure, but you can score it.",
            "You can say or.",
            "This is a good structure.",
            "This is a bad structure.",
            "So after the prediction there's always some sort of optimization going on or candidates candidates."
        ],
        [
            "Oh, this is something I think quite current natural language parsing.",
            "So on the left side you have a sentence on the right side you have the pass tree and again you calculate the score.",
            "How well does this past refit with a sentence?",
            "And then you're trying to evaluate good candidates for past trees."
        ],
        [
            "OK, the last part then will be on link prediction relationship prediction and difference here is that why I J describes the link or relationship between rowenta TI and column entity J.",
            "So both rows and columns now sort of have object properties and different situations.",
            "They might be from the same family, same type of objects, or they might have different types.",
            "So for example.",
            "Users and movies and they also both might have attributes.",
            "And this is sort of an unusual thing, because this is not really a matrix anymore.",
            "Now we sort of.",
            "Just attach these attributes to the column.",
            "Column this column distribution a column column at the display, and this is so that's not really a matrix anymore, so it's a little bit more general, and we will also discuss this in a little bit detail so."
        ],
        [
            "So here's an typical example.",
            "Properties of users, income test, age, whatever.",
            "Here properties of the items like movies, action movies, whatever and you want to predict in a typical application how much, how likely it is that the user would like this object.",
            "So this is a little bit unusual."
        ],
        [
            "And we will discuss this.",
            "So if there's a key message, simple key message prediction accuracy improved in models with several response variables.",
            "If some or all models are sensitive to all outputs, math not very deep.",
            "But then in learning some more parameters, estimates benefit from the multiple outputs.",
            "And so one counterexample is the linear case where you can really decompose the prediction of the multiple outputs in the prediction of every single output.",
            "But in all other situations essentially you have this."
        ],
        [
            "But of course you have to like deal with sensitively.",
            "Otherwise you might also deteriorate.",
            "You might get worse result 1st results, so now we get into the more technical part.",
            "We start with hierarchical Bayes, so the idea is maybe it would say we predict the same thing.",
            "For example, patients length of stay or patients outcome but in different situations are different hospitals.",
            "Different plans maybe?",
            "Also these are different tasks, so that's why sometimes work multiple task prediction."
        ],
        [
            "So we start with problem setting and simple solutions are weak."
        ],
        [
            "To the simple solutions.",
            "So data collected is collected from different situations and it decides or tasks and the goal is to learn objective model.",
            "If Jay so Jay is the task of X1 to M and the question is can data from other situations have to improve to improve the prediction of both one of the members of this set but also for a new new saturation new task.",
            "And for simplicity we will consider models linear in the parameters.",
            "So FJ of X is linear weighted sum of weights in this weights might depend on the task and some future functions fi L of X.",
            "Causes linear case.",
            "The special case, but we cannot have kernels on base functions and then also kernels.",
            "And typically we only have very noisy measurements available from the apps we don't can.",
            "We cannot measure F directly, but there's some noise on the apps and this call then why?"
        ],
        [
            "So the first simple solution is to have one global model, so just ignore that there are different situations, different hospitals, different plans, just put all data in one model, pull all data in one model and train one global model.",
            "So this one simple solutions.",
            "So they are fruits.",
            "So everything is the same, not apples and oranges.",
            "So definitely this data efficient because all the data are used very efficiently.",
            "But the problem of course it ignores the differences in the different situations, so you should obviously always try first the simple solutions if they already work well, you don't have to try the difficult ones.",
            "Then"
        ],
        [
            "Next simple solution is that each model is at each situation.",
            "Each output is modeled individually, so this is trained only on the data for the corresponding output.",
            "The problem is, it's not very efficient in terms of the data you throw away all the information from the other outputs.",
            "So if you have a lot of data for a given situation cause no problem with that.",
            "But if you only have small number of data available then you don't exploit the outputs from the other dimensions.",
            "So only one output dimensional data from output dimension contribute to a parameter estimates."
        ],
        [
            "Yeah, the next I think was also suggestion that if you have different hospitals, why don't you present it in some way?",
            "For example, in this way in this case, it means the patient was in the James Hospital, so we indicate in which hospital or in which situation somebody was and just add that as an additional input.",
            "It's also very data efficient 'cause you only have one model to train and use all the data, but sometimes it's difficult to find a good model of how to integrate this information.",
            "For example, if you have a linear model, obviously the only thing which happens is that you learn a constant offset for each for each situation, which is a little simple, but if you have good insight and how.",
            "The identity of the plant also might influence the prediction.",
            "Then this is also a usable, usable thing, but.",
            "Prediction if I don't know which hospital it comes from, can I still apply the same model, let's say during testing?",
            "I don't know where I'm going.",
            "Oh there yeah, there are solutions.",
            "Yeah, you probably have to estimate something about which what you think based on the data, which hospital it might have come from.",
            "Is it fair if I just ignore this feature during prediction?",
            "Later, when you when you make the prediction, that would probably one reasonable solution."
        ],
        [
            "List the next tutorial.",
            "OK."
        ],
        [
            "So let's then start with a sort of interesting part, at least from my point of view.",
            "OK, so this is the situation.",
            "We have this linear model again.",
            "We assume the parameters are priority come from a normal distribution, zero mean and some isotropic covariance.",
            "So if you're not page and then this is sort of like a weight decay thing and let's assume we have for a new situation.",
            "We have observed these three data points and and all these things here.",
            "I hope this is understandable.",
            "They're supposed to be all these fires all these different basis functions, so they have this local shape like this local Gaussian or something.",
            "And there here densely distributed in the input space.",
            "And here I have drawn some of them, not all of them.",
            "But what happens so in in regions where you have the data, the model probably fits nicely.",
            "The data happen in regions without data.",
            "The prediction is simply 0.",
            "Which makes a lot of sense.",
            "So in regions where I didn't see any data, you should better say zero, then some wild guesses about what it might be.",
            "Food is very sensible and makes a lot of sense.",
            "But"
        ],
        [
            "Now we have more information, so the red, blue and green lines supposed to be models which were trained on similar problems previously, let's say.",
            "And which we want to also exploit.",
            "So if we now observe these three data points over here.",
            "We might say, oh, probably this gonna be between.",
            "The blue and the red.",
            "And so maybe this prediction is more sensible, very different from the prediction we had before.",
            "So in a situation where hierarchical Bayes is applicable, this is going on.",
            "We want to sort of say, OK, we have already observed a lot of different models, and we expect that the new model will be sort of similar to the already observed observed models."
        ],
        [
            "And the key is to to look at the parameters parameter distribution of the previously trained models.",
            "So let's look at two of the basis function, the number 10 and #100.",
            "So this guy and this guy.",
            "And if you look at the parameters for the colored models, we might get this, you know.",
            "So if W 100 is larger W tennis a little smaller, and so on, so you get a distribution from the previous models.",
            "And here a couple more of them.",
            "So the idea of hyperactivation modeling is not to say OK, why should we?",
            "If you have a new model, why should we start again with this stupid assumption?",
            "Zero mean and a topic covariance?",
            "Maybe you should start now.",
            "Use this distribution for the new model.",
            "Saskia so we have a new distribution of the WS.",
            "Given that all the past data again normally distributed but now with the mean vector M and with the covariance Sigma.",
            "So this is the mean vector and the Sigma describes the distribution of these things here.",
            "Each separate.",
            "Yeah, this is a vector, so this thing is always selected.",
            "So so these are two and so and so in this case it would be 200 dimensional singerly.",
            "So it would be difficult to draw.",
            "So there are many dimensions."
        ],
        [
            "So so so new model.",
            "This is a learn prior and I will talk about this concept later on.",
            "W newer, given the data is newer Gaussian and so based on this information we can also predict how a new system without any data would look like.",
            "So what's the expected value of new thing without data given the past data is some of basis functions where it simply substitute the mean for the weights?",
            "And the other information which is important here is what is the covariance between two functional values XC and XC.",
            "So we look at two functional values and we want to see how much they correlate.",
            "So this one is a little larger, so the other one also larger.",
            "Or is the other one lower?",
            "And this can be calculated.",
            "So this is the only one of the few formulas which hopefully you're familiar with.",
            "It's easily to translate this uncertainty into the uncertainty of the function prediction, and for the mean you get this formula and for the covariance you get this formula.",
            "So this is Phi, transposed firefighter, source XI, Sigma.",
            "This Sigma Phi XK.",
            "So this gives you exactly this expression.",
            "The events between these two functional areas and the."
        ],
        [
            "I also tried to try to draw again so this dashed thing now might be the mean function.",
            "Substituting the mean for the weights and this covariance would indicate if I look at two functional values.",
            "And so that the red above the mean, then this is also above the mean.",
            "So they are positively correlated obviously.",
            "So this is what this expression stands for.",
            "The correlation of functional values."
        ],
        [
            "So now we're playing a little mathematical trick.",
            "We can compose this learn Sigma in to VDD, transposed.",
            "Be transposed.",
            "So this is based on a single ability composition.",
            "These are these guys have author noggle orthonormal columns and this diagonal matrix.",
            "I could also written here D but to be consistent with the later stuff I write DD transpose because this diagonal that's trivial.",
            "Just take the square root.",
            "So there's probably no one.",
            "And I can now plug this into this formula.",
            "If you transpose Sigma Phi of XJ and now I can.",
            "So I just plug this into here and then I can break it in this way.",
            "Now I can interpret this equation differently.",
            "I can say here's no more covariance matrix or it's the identity covariance matrix, and these are now my new basis functions.",
            "So I've known are Gaussian parameters with identity covariance matrixes with learned basis functions formed as linear combination of the original basis function.",
            "So if you look at this guy here and you think of it as a new basis function which is standing there, then sigh of K of X or the case basis function is DKK, which is from this guy here and then the sum over VLC from this guy times 5 L of X.",
            "So this just I mean, this is still the same.",
            "Math is just a little bit of a different way of looking at that.",
            "The nice thing is if you stay with these basis functions, you have to work then with this complicated prior distribution, if you use these basis functions, you can now train every model as before with this trivial simple basis function with simple representation with with a normal distribution, zero mean and.",
            "And and at the tropical variants.",
            "Of course, you would have to add the mean again.",
            "So to be clear about this.",
            "So this is."
        ],
        [
            "A graphical thing.",
            "The Sky was going on.",
            "You have the input X you calculate first the original basis functions 5.",
            "Then you transfer form this with a VD transpose thing.",
            "You get the new basis function PSI.",
            "And then you might you learn new weights.",
            "In this usual way, just wait, decay or just isotropic Gaussian distribution and then you get the prediction.",
            "So this is sort of a graphic representation.",
            "What is going on?",
            "And if you drop out, maybe size here with very small dies.",
            "You can also get a dimensionality reduction, but this is not really necessary.",
            "Even if you do this you get improved prediction because by doing this essentially you put a very informative prior on the weights and you get this coupling between the different models.",
            "Why you?",
            "Cause I see.",
            "Your new feature is in your combination of the old feature, so you never bound.",
            "You are you're serious, serious, serious property I guess.",
            "Don't want.",
            "OK, you're right.",
            "These are all linear transformations, and so first you think a lot of linear transformation boil down to like one linear transformation.",
            "So what's the big deal but the.",
            "Right answer is that you put the right distribution on the parameters and if you would you have the.",
            "If you put.",
            "If you look at the original distribution over here, you would put the prior on the parameters which have mean M and and some covariance Sigma.",
            "But if you derive this new basis functions then you can use a prior which is isotropic.",
            "Much simpler to work with.",
            "I mean, do you have some married with this with this new feature?",
            "Yeah, I mean OK. Let's see I mean the merit is."
        ],
        [
            "This is going on direction.",
            "I mean, mathematically, I didn't do anything.",
            "New owner is exactly the."
        ],
        [
            "Same thing, so it's a difference between doing this."
        ],
        [
            "And doing this.",
            "So you think this drive features that you will get a combination of the original features which allow you to easily model this?",
            "I know.",
            "But mathematically, it's exactly doing this.",
            "It's just you can also think of sticky and say OK, this is what I what I think is right.",
            "You have a new product line, prior distribution of of W. Plug them into your original model and do the prediction.",
            "But mathematically it's exactly equivalent to doing this here."
        ],
        [
            "So maybe some theoretical bounds don't change.",
            "I'm not sure, but from like a regular statistical view of Asian view, this is gives you very different results.",
            "Maybe you can discuss more offline.",
            "Yeah, also an interesting comparison.",
            "I mentioned networks the beginning so this looks almost like a neural network.",
            "If you're old enough to know these things are very new.",
            "Very young to know these things so.",
            "So this is 1 hidden layer.",
            "The second layer is the prediction of the multiple outputs.",
            "Of course in their networks has nonlinearities everywhere and so on.",
            "But it's interesting that it's not too far away, so if you just click this problem into another network with the corresponding number of outputs, maybe results are also quite interesting."
        ],
        [
            "Just to also show that there is little bit math behind it because you might be worried you have a small number of data points of weights you have trained and they should define this complicated covariance matrix so you can of course put nice priors on everything and essentially what you want to do is you want to learn this.",
            "Then the mean function and the covariance Sigma and so the technologies are that you put a inverse Wishart distribution on the Sigma.",
            "Given some parameter and sort of something which should be similar to the Sigma.",
            "Put a prior on the mean with some means move, which is typically zero.",
            "And here's some precision.",
            "How much do you believe in this mean then, for each situation you draw a parameter vector out of using this mean and the Sigma, and then you draw all the data.",
            "So these are all the data for situation J with mean W, J5, J Sigma square I.",
            "And then one way to do this to deal with this is you doing M procedure.",
            "In the step you estimate the probability distribution of each weight vector for each for each situation given the data for this situation given the mean and the current estimate of the mean and covariance.",
            "And in the M step you update your mean and your covariance.",
            "So you go up first and back a few times, and then the system converges and you get nicely tuned parameters."
        ],
        [
            "There's an issue if you want to get into this deeper, deeper.",
            "This is 1 definition of the inverse Wishart, so we don't have to look at too closely now, but there are.",
            "There are small differences in the exact definition, and it seems to us that this is the most appropriate definition because it has some nice features in terms of mechanization of the distribution, and this is a reference where this has been discussed."
        ],
        [
            "But this I mean for this presentation is just mathematical detail.",
            "Um?",
            "So the key benefit in her equation modeling for linear systems is a common basis.",
            "Functions are learned that are used for all outputs.",
            "Now we sort of do something a little bit off track, but maybe it's interesting and I'm sure it's more confusing.",
            "Interesting, but I want to discuss it.",
            "Percabeth function based functions derive from a single ability composition.",
            "So the."
        ],
        [
            "It is.",
            "According to our model, the covariance of two functional values is going by side transposed side of the two at the two data points and let's call this guy KIK for the indicator.",
            "But so this should correspond to the covariance in the functional values of the different functions.",
            "And now we assume regression with little noise.",
            "So in this case we can also say oh this K should also be approximately the empirical.",
            "Covariance of the observed wise.",
            "So if we.",
            "If we do this estimate, this should be quite close to this because this is what we're modeling.",
            "We're trying to predict how different functional areas are very, and since now we have several samples from this.",
            "From this function, we can really estimate this also empirically.",
            "And if I look now at this KIK over here it's the inner product of all the data from different situations.",
            "For data point, I inferred data point K, the strainer product.",
            "And if you look at this formula in this formula, you might say OK, why not take PSI K of XI simply BYOK?",
            "It's sort of a little bit stupid thing to do, but why not formally?",
            "Looks like very similar and it would give you.",
            "The covariance kernel, which seems to be quite close to the correct one, and then these weights the weights on the side are sort of the Delta function.",
            "So you put the corresponding output dimension for the corresponding from the corresponding Y entry.",
            "So to learn basic functions are simply given by the output data.",
            "The disadvantage of course are different situations do not benefit from one another, because obviously every every every column is then independent of each other.",
            "But still we can make predictions based on the estimated K. So if you assume that these are the basis functions instead of the size we just arrived, we get this as a covariance kernel estimate and we can do predictions with this one as well.",
            "So it's just an interesting.",
            "Connection to this idea of just doing an empirical estimation of the covariance kernel, why do we do this complicated thing?",
            "We can."
        ],
        [
            "A little bit smarter and maybe do an SVD of YUDV, transposed, and then this expression we've seen before is now this guy, this entity, and it's approximately the same thing.",
            "If we do a reduced rank approximation, or we set a lot of diagonal terms to zero the small ones.",
            "And then this is a little bit more sensible and we get our new basis.",
            "Functions are now DKK, so in this diagonal term terms you I can this you matrix and another way of writing it is this expression over here, and the W is the weights on these guys simply come from the corresponding.",
            "These in this decomposition.",
            "So here it's a little more interesting because the singular values singular vectors are calculated based on all data and statistical strength, shared singular value composition.",
            "We sort of find the best representation in some sense for the data, the best low rank approximation and this might explain a little bit the success of matrix composition methods in collaborative filtering.",
            "For example, the Netflix company competition.",
            "This really high scale competition and corporate filtering.",
            "Alot of the approaches are based on uh.",
            "Decomposition of the matrices and this is one way maybe of looking at this just to think of OK, we define sensible basis functions and reduce noise in the data in this way.",
            "But this is sort of what is sidestep so."
        ],
        [
            "It's not really necessary for the whole flow of the idea.",
            "OK, so the advantages of hierarchical basis that you include, I mean compared, let's say 2 SVD or the empirical version you can include prior knowledge by defining the basis functions by defining the basis functions you give sort of you sort of so show what collation between functions you expect and you're learning typically makes them stiffer, not more flexible.",
            "So if you have a good intuition, what are good basis functions for your?",
            "Application then then you can imply implement this prior knowledge by defining the basis functions, whereas if you do the SVD stuff or the empirical stuff, this is not really so easily possible.",
            "You can also generalize to new input, because after all you learn a mapping from X to Y, and if you have 4 new situation for a new data point, you only know the X.",
            "You can do predictions and as I mentioned before, and if you look at the M equations, that would be also maybe more obvious.",
            "There's no problem with missing outputs, so if you have situations where a lot of the outputs are missing, arrangement base can work with that quite elegantly.",
            "So we talked about learning and inference using AM.",
            "Other inference is off performed via Gibbs sampling or other appropriate methods such as variational learning.",
            "So for example, if you are familiar with latent usually allocation, which is essentially hierarchical Bayesian model, then they have experimented alot of skip sampling and variational learning.",
            "And also another advantage is that the model doesn't have to be linear.",
            "So if you mean any reasonable model is fine, you can do recognition learning with.",
            "And this is a book where you might want to look."
        ],
        [
            "But if you are interested in this more deeply, so three phases in the modeling with no data yet available for a new situation, the model essentially will follow the mean function.",
            "The best thing it can do in the second phase with small data available for new situation model might follow more closely a previous model that fits those data well, so it will sort of out of the past models.",
            "Which one is the best one which looks very close to what I'm doing in the new model and finally with a lot of data it becomes independent of the prior, so if a lot of data you don't have to do with all of this because then you can just take all this large amount of data to train the model individually.",
            "OK, the dimensionality reduction can be achieved if you set some of these terms."
        ],
        [
            "To zero as you discussed.",
            "So from this point of view, we can look at the statement from history again.",
            "So if we know M and Sigma are priority.",
            "So we have some insight into this problem and we have really pretty informative feel about how the mean and the Sigma should be.",
            "And it also corresponds to the empirical observation of parameters we can get later.",
            "Then all output functions are independent.",
            "And then there's no coupling between the different outputs.",
            "Another situation might be if the functions really have no prior common distributions, like if you predict two very difficult different things, where the assumption that they sort of come from the same family of distributions doesn't make sense.",
            "But if the priors learned as I presented, then all measurements influence all predictions by influencing each other via the prior distribution."
        ],
        [
            "Action.",
            "OK, so this was the.",
            "I have one comment about that.",
            "So these models you presented in work when correlated outputs are pretty much same thing right when their length of stay?",
            "Yep, Yep.",
            "Prior or one with import.",
            "Yahoo.",
            "Yes, in principle yes, yes.",
            "So but it's always a challenge how far you can get away from this idea.",
            "So maybe what was it?",
            "Exchange rate and.",
            "Whatever fashionable things suddenly correlate, so maybe there's correlation where you shouldn't expect correlation, but in principle is right.",
            "What you're saying from the from the conceptual point of view, there should be some reason why these things should come from the same family of distributions.",
            "My.",
            "View orders OK. Why don't we just learn the joint distribution of everything and then do conditioning?",
            "Why is that not a viable?",
            "OK. Everything.",
            "Good point, that should be one of the solutions I should have mentioned.",
            "That's true.",
            "I think if you have.",
            "Pretty much complete data then this is possible, but if you.",
            "So if the otherwise the ones you are, you don't need to predict.",
            "You can treat as input, but then they sort of become inputs.",
            "So here I guess the essential part is also the pattern of missing information.",
            "Is is is NOT is is difficult, so to manage, probably that's one answer.",
            "If you're if it's in training, maybe you have complete data, but then when you want to apply it to a new situation, you only know the input.",
            "But then you would have to treat all the other wise as missing data and which is also difficult.",
            "But if you have complete data in training, if your matrix is complete and training then this might be an option.",
            "One can work with.",
            "But maybe a later discussion, though there will be other opinions on this.",
            "OK, so this was a Bayesian treatment.",
            "Now they're just one slide on the frequentist version of this.",
            "So let's look at this model.",
            "Why?",
            "So all the data for output J is M times design matrix five of J.",
            "So this could be specific to J plus CJ.",
            "Times PJ plus epsilon J.",
            "So this is a linear model is called a mixed model.",
            "What one assumes to be known as this file?",
            "So the design matrix and also the Z.",
            "And the interesting cases when both are the same.",
            "OK, but there could be different.",
            "OK then one thing is this fixed effect M. It's unknown, so it has to be estimated and this is what frequentis like to do.",
            "Owner is unknown but can be estimated.",
            "This guy over here is random is focal random effect, which is different for each output.",
            "So that's why it has an index J.",
            "And OK, let's consider the special case.",
            "D equals this fine matrix.",
            "Then it's called regression model with random coefficients and we get the connection to the hierarchical Bayesian model.",
            "If we just put WJ equals M plus BJ.",
            "So M is our old M. The mean of this learned Gaussian distribution and BJ is sort of the deviation from this mean.",
            "So if we do all this then, but these guys are the same here.",
            "We have M + B which is W and looks very much like our hierarchical linear model.",
            "BJ is assumed to be normally distributed with zero mean and covariance Sigma.",
            "Because the mean was already explicitly taken out, and one more aspect here is that you can allow correlated noise.",
            "So the stuff you cannot explain might still contain some correlation.",
            "And in this mixed models they take that into account and this is sometimes what we call collaborative collaborative effect.",
            "'cause if you do collaborative filtering, you can work without any inputs and this sort of this term over here.",
            "So maybe I?",
            "Yeah, I'm not sure if it was clear, but it's you can really translate this week very quickly into the Bayesian framework and the statement maybe to make us mixed models is beige and as a frequentist will ever get 'cause this is what they hate.",
            "Sort of random parameters.",
            "That's what they really like to avoid, but there's something where they sort of accept that they need something like that, and it's a hurricane.",
            "Incentives also, as frequent as a base, and maybe we'll ever get, because when I always said we learned the prior distribution.",
            "And we learn the parameters and to practice something well.",
            "If you're really a true beige, and you'll probably always think about what is he talking about, but this is, I mean intuitively, at least what is going on with you tune the parameters in your so called prior distribution based on the data.",
            "So here's sort of a region where both."
        ],
        [
            "Of them nicely join forces.",
            "In some sense they are also non probabilistic, closely related things.",
            "Recent work here from this group.",
            "If the new maturity and punter 2006, here's a loss function.",
            "He has a two norm.",
            "Constraint on the B is another two norm constraint on the M. Again, W decomposes into M The mean thing for other models and the specific thing the BJ here implicitly there, assuming isotropic covariance.",
            "So essentially what they're learning is the mean and not really the covariance structure over here, so it's a little bit a simpler version if you want, but there's some advantage.",
            "There is now a convex optimization problem, so we don't have to deal with linear Edwards with local optimize any sense."
        ],
        [
            "This is another work from this group.",
            "Again, here the Altoona Morrell Norm and here is called AL 1L2 Norm, which is a little bit reminder of the lesser cost function if you are familiar with that and the effect is that it really removes the sum of the values from the model, so we can really also prune inputs.",
            "So it selects features in the optimization.",
            "And that's also related work from another group."
        ],
        [
            "OK, so this was a problem.",
            "Now we could go back to the probabilistic part again and I want to look briefly at the Gaussian process version of this.",
            "I mean, I tried to convince you before that if we start with this model, linear model parameters are Gaussian distributed.",
            "This sort of technically equivalent to a Gaussian process, so-called Gaussian process, where we define the covariance between two data points.",
            "So this is indicates how much function value is very.",
            "Cool area which is done by this formula which I've shown already and the main function is done by this formula which we have also seen already.",
            "So as in hierarchical base boils down to learning mean and covariance in the Gaussian process.",
            "Hierarchical Bayes we want to learn this mean function.",
            "And the convince Colonel.",
            "And of course we can do it by first learning these guys and then translating it into the kernel space or the functional space.",
            "Or we can also try to learn these guys directly."
        ],
        [
            "So this is the first version we sort of work with the basis function representation of translated into kernel covariance kernel.",
            "So if we work with empirical 1, otherwise the kernel is or the kernel on the training data points, so it's a matrix defined for all pairs of training data points is just the empirical estimate.",
            "This quarter gram matrix.",
            "SVD we work with a single vectors and singular values and we get this expression and in hierarchical base we derive this.",
            "I functions and get this expression for the kernel.",
            "But this OK not much new because it's just."
        ],
        [
            "Sort of, the translation of the previous result into Gaussian process, so maybe it's more interesting to really optimize in the in the really in the mean function and the covariance itself.",
            "So we learn sort of a K, which is a cool, dense kernel and the mean and 1st we do it only at a finite number of points.",
            "So we define typically the training data and maybe some test points, so it's more like transduction."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the tutorial on learning with dependencies between several response variables from hierarchical Bayesian multitask learning to structural output prediction and relational learning.",
                    "label": 1
                },
                {
                    "sent": "I'm focused RESP.",
                    "label": 0
                },
                {
                    "sent": "I'm from Siemens in Munich from corporate technology, so that's.",
                    "label": 0
                },
                {
                    "sent": "Research and development and.",
                    "label": 0
                },
                {
                    "sent": "The core author on this tutorial is KU.",
                    "label": 1
                },
                {
                    "sent": "He's currently at NEC Labs America when.",
                    "label": 0
                },
                {
                    "sent": "Much of the work was done, which is presented here.",
                    "label": 0
                },
                {
                    "sent": "He was also working at Siemens at that time, and he's also here so you can answer all the diff.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The questions.",
                    "label": 0
                },
                {
                    "sent": "OK, sure.",
                    "label": 0
                },
                {
                    "sent": "So this tutorial is about multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "And I want to start with this quote from the book by his teacher.",
                    "label": 0
                },
                {
                    "sent": "Only Friedman and data mining.",
                    "label": 0
                },
                {
                    "sent": "Multiple outputs do not affect each other's least squares estimates.",
                    "label": 1
                },
                {
                    "sent": "And this statement is certainly correct in this.",
                    "label": 0
                },
                {
                    "sent": "In the context it was made.",
                    "label": 0
                },
                {
                    "sent": "Of linear systems linear multivariate systems.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately it has been over generalized, so when I started.",
                    "label": 0
                },
                {
                    "sent": "Machine learning it was essentially neural networks and the neural networks we had.",
                    "label": 0
                },
                {
                    "sent": "We started off with a lot of applications with multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "So for example in.",
                    "label": 0
                },
                {
                    "sent": "ZIP code recognition.",
                    "label": 0
                },
                {
                    "sent": "There were ten outputs, one for each digit.",
                    "label": 0
                },
                {
                    "sent": "And a lot of the initial work was done with neural networks with multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "But at some point people thought this is maybe not really important, and learning with one output is much simpler and difficult enough.",
                    "label": 0
                },
                {
                    "sent": "Easy to describe and so the emphasis definitely changed and we started doing neural networks with one output.",
                    "label": 0
                },
                {
                    "sent": "And also in this book there's almost no reference to multivariate prediction.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit on protection methods, but not much, and so it seems that also statistics didn't really consider this in great detail.",
                    "label": 1
                },
                {
                    "sent": "So this presentation will be mostly about cases where this statement is not applicable.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so you go, then you go to the other extreme, and then you look around a little bit.",
                    "label": 0
                },
                {
                    "sent": "What has been done in this direction of multiple outputs, and suddenly you discover that there's quite a lot of work.",
                    "label": 0
                },
                {
                    "sent": "Are there from different fields?",
                    "label": 0
                },
                {
                    "sent": "With different applications in mind so.",
                    "label": 0
                },
                {
                    "sent": "If you go through some of the terms or hierarchical Bayes, obviously it's from the Asian community, then the next terms inductive transfer learning multi label prediction, multitask learning is more from the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "Then there are random effect models, random parameter vectors, mixed models, mixed effect models, nested models, multi level models, hierarchical linear models, generalized mixed models for these are from the more frequentist statistical side.",
                    "label": 1
                },
                {
                    "sent": "Then we have collaborative filtering, which is sort of the application.",
                    "label": 0
                },
                {
                    "sent": "Which motivated our work in this direction.",
                    "label": 1
                },
                {
                    "sent": "Then the projection methods, Canonical correlation analysis, maximum covariance regression partially squares multivariate regression.",
                    "label": 0
                },
                {
                    "sent": "So this is also a subfield, very much application driven.",
                    "label": 0
                },
                {
                    "sent": "And then there is a multivariate regression structured output prediction.",
                    "label": 0
                },
                {
                    "sent": "Particularly, last thing is quite popular in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Currently in the last years.",
                    "label": 1
                },
                {
                    "sent": "And we are part of that part of the presentation, at least some of it.",
                    "label": 0
                },
                {
                    "sent": "And probably there are many more things that I. Oh yeah, not even aware of.",
                    "label": 0
                },
                {
                    "sent": "So this is an attempt to provide a view.",
                    "label": 0
                },
                {
                    "sent": "So not necessarily an overview, because I'm not sure that all the important aspects are covered, but hopefully still a very useful view and in general with Asian flavor but not strictly Beijing.",
                    "label": 0
                },
                {
                    "sent": "So it's not about.",
                    "label": 0
                },
                {
                    "sent": "Complex sampling everywhere.",
                    "label": 1
                },
                {
                    "sent": "So the first part will be about hierarchical base and mixed models.",
                    "label": 0
                },
                {
                    "sent": "We started with the problem setting and simple solutions.",
                    "label": 1
                },
                {
                    "sent": "Then the cooler hierarchical basin mixed models so mixed models is sort of the frequentist equivalent to hierarchical Bayes.",
                    "label": 0
                },
                {
                    "sent": "Then a little bit on nonparametric hierarchical Bayes.",
                    "label": 0
                },
                {
                    "sent": "Then production methods.",
                    "label": 0
                },
                {
                    "sent": "The third part of a multivariate models and structured output and the 4th partner link prediction and relationship addiction.",
                    "label": 0
                },
                {
                    "sent": "So this is a lot and of course you cannot.",
                    "label": 0
                },
                {
                    "sent": "Everywhere in great depths.",
                    "label": 0
                },
                {
                    "sent": "But maybe this is still interesting to get this view on these different aspects and please ask questions because that sort of Inter.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The flow a little bit and.",
                    "label": 0
                },
                {
                    "sent": "Makes it more comprehensible.",
                    "label": 0
                },
                {
                    "sent": "So this is the classic generic supervised learning task.",
                    "label": 1
                },
                {
                    "sent": "So we have a data matrix, the training data and the rows are data points, so that correspond to objects, entities, situations, measurements and so on, and the columns.",
                    "label": 0
                },
                {
                    "sent": "Consist of the input.",
                    "label": 1
                },
                {
                    "sent": "Representation input vector X one to XN and one single output variable Y who I want to wire.",
                    "label": 0
                },
                {
                    "sent": "And typical typical situations why I might be a function of XI with parameter W plus some noise.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical setting in regression or classification.",
                    "label": 0
                },
                {
                    "sent": "The probability that Yi equals one is some transfer function, sigmoid transfer function applied to the function F of XI.",
                    "label": 0
                },
                {
                    "sent": "So this is what people have studied.",
                    "label": 0
                },
                {
                    "sent": "Many years and where a lot of work has been done.",
                    "label": 0
                },
                {
                    "sent": "So may.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a new generic learning problem where the only difference really is that the Y is not one dimensional, but at M dimensional.",
                    "label": 1
                },
                {
                    "sent": "So again, Rozar data points columns are now the input vector X and the output vector Y.",
                    "label": 1
                },
                {
                    "sent": "And this is also the perspective of the presentation little bit, so we have this situation.",
                    "label": 0
                },
                {
                    "sent": "Somebody gave us this data, input several outputs.",
                    "label": 0
                },
                {
                    "sent": "What should we do?",
                    "label": 0
                },
                {
                    "sent": "Of course, in the particular situation there is often a lot of prior knowledge about the application is pretty clear what one should do, but we want to take take a step back and more think about what could be done in this situation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I am starting with a little bit of an overview week before we go in more detail in the overview is a little bit long, maybe so the 1st two things are in hierarchical based mixed models and projection approaches, so we're here we have the data matrix, again blue the inputs green are the outputs.",
                    "label": 0
                },
                {
                    "sent": "And in in the first in hierarchical basics models, infection methods each output dimensions for each situation or each task is trained independently.",
                    "label": 0
                },
                {
                    "sent": "But not independently given and transformation respectively given the prior distribution that is found using all data.",
                    "label": 1
                },
                {
                    "sent": "So you can think of it as having 2 steps.",
                    "label": 0
                },
                {
                    "sent": "The first step we are trying to find.",
                    "label": 0
                },
                {
                    "sent": "Either this representation is transformation are sort of tune the prior distribution of the parameters.",
                    "label": 0
                },
                {
                    "sent": "But when that is done in the next step, we treat each output dimension independently again, so there's some coupling, some exploitation of other output dimensions.",
                    "label": 0
                },
                {
                    "sent": "By forming these transformations.",
                    "label": 1
                },
                {
                    "sent": "But after that we try, we do independent modeling again, and in hierarchical based statistical strengths between multiple outputs are shared by common parameters in the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So typically we assume a private solution and we tune the parameters in the prior distribution using all the data.",
                    "label": 0
                },
                {
                    "sent": "The projection method.",
                    "label": 0
                },
                {
                    "sent": "The input is mapped to a lower dimensional space that was found found using all the data.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So I have a microphone I think for the camera.",
                    "label": 0
                },
                {
                    "sent": "Still difficult to hear.",
                    "label": 0
                },
                {
                    "sent": "OK, I tried to speak louder.",
                    "label": 0
                },
                {
                    "sent": "The difficult acoustic.",
                    "label": 1
                },
                {
                    "sent": "Hello, it's panel.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, and when is this applicable when we can assume that the functional dependencies for all outputs?",
                    "label": 0
                },
                {
                    "sent": "I come from the same simple family of distributions.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't have to be a linear model, it can be linear in some basis functions or also in kernels and representations.",
                    "label": 0
                },
                {
                    "sent": "But it should be from the same family of things there, so we shouldn't try to do a hierarchical basian thing of the functions have no thing things in common, so the underlying assumptions they should come from sort of the same family of distributions.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 example.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From industry data has been collected and models have been trained for implants.",
                    "label": 1
                },
                {
                    "sent": "We want to generalize to plant N + 1 where either no or little data is available.",
                    "label": 1
                },
                {
                    "sent": "So this is a common situation.",
                    "label": 0
                },
                {
                    "sent": "This is a steel rolling mill, so here the steels are compressed so become thinner and for example here it is important to predict the right rolling force and it's impossible to derive these models from first principles.",
                    "label": 0
                },
                {
                    "sent": "It has been tried but it wasn't very successful, so it has to be a data driven model.",
                    "label": 0
                },
                {
                    "sent": "But there is not a simple problem because the plans they change overtime in some uncontrollable way.",
                    "label": 0
                },
                {
                    "sent": "It's not well understood why they change on a daily basis even.",
                    "label": 0
                },
                {
                    "sent": "And also if you go to a new plant to news factory, you cannot really take your old model and just apply it to this plant.",
                    "label": 0
                },
                {
                    "sent": "But you want would like to inherit the knowledge you have learned in the previous plans.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical hierarchical basian situation for the new plan, you don't want to start with zero knowledge, but you can also not assume that the last model can simply be taken and applied here in the new plan.",
                    "label": 0
                },
                {
                    "sent": "So this is a real problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, here's another example.",
                    "label": 0
                },
                {
                    "sent": "Data for length of stay prediction of outcome prediction has been collected for patients in hospitals.",
                    "label": 1
                },
                {
                    "sent": "Can we generalize to patients in hospital M + 1?",
                    "label": 0
                },
                {
                    "sent": "Now these models are there of course dependent on patient properties, the diagnosis, the age, maybe the procedures which have been taken, but there will be also a variation between different hospitals, so maybe the staff has gotten different training.",
                    "label": 0
                },
                {
                    "sent": "A different patient mix different parts of the country, so there might be different reasons why the models are not really exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Where we where you need a personalized or special model for each plant, but again, you don't want to like start from zero, you want to exploit all the knowledge you have acquired in the models from from other hospitals.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a little bit of new situation, but let's first look at the data matrix properties of patient one, patient, patient, N. And here length of stay prediction for hospital Wanan for hospital M. And yeah, if you think of it, you might quickly notice something.",
                    "label": 1
                },
                {
                    "sent": "Naturally a given patient has typically been only in one hospital.",
                    "label": 1
                },
                {
                    "sent": "So technically, for each input, only one output might be available, so it's more this situation.",
                    "label": 1
                },
                {
                    "sent": "So we have your patient properties and we only have.",
                    "label": 0
                },
                {
                    "sent": "One measurement.",
                    "label": 0
                },
                {
                    "sent": "Of all the output dimensions for each patient, 'cause the patient doesn't wander around in different hospitals and tries different things in general.",
                    "label": 0
                },
                {
                    "sent": "So in some applications we have to deal with this problem that only little data is available in this matrix, and typically one makes this draw this picture also a little bit differently, but we want to stick with the Matrix an from this point of view we have to assume all the stuff is unknown.",
                    "label": 0
                },
                {
                    "sent": "We only have very small number of measurements.",
                    "label": 0
                },
                {
                    "sent": "And this is something we are.",
                    "label": 0
                },
                {
                    "sent": "Occupation modeling fits perfectly becausw it has no problems dealing with this situation and but other approaches we will learn about today.",
                    "label": 0
                },
                {
                    "sent": "I have difficulties in this situation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we will come to that in a second.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "10 seconds, but I could point.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Also very quick remark.",
                    "label": 0
                },
                {
                    "sent": "We will also come to that in 10 seconds.",
                    "label": 0
                },
                {
                    "sent": "But the reasons, of course you want to exploit all the data and if you just look at the data in one hospital you don't have so much data available.",
                    "label": 0
                },
                {
                    "sent": "So if you have a lot of data, of course it's the right answer, not just ignore all the other data, but if you don't have much data available in the application then you might want to also benefit from the data from the other hospitals.",
                    "label": 0
                },
                {
                    "sent": "But we will come to that also nothing.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it might be very prediction.",
                    "label": 0
                },
                {
                    "sent": "Situation is different, so after training we want to maintain the dependencies between the outputs and they're often explicitly models.",
                    "label": 1
                },
                {
                    "sent": "And statistical strength.",
                    "label": 1
                },
                {
                    "sent": "So parameters exploit all the information on all the variables if they are sensitive to all the variables in all the models and so now.",
                    "label": 0
                },
                {
                    "sent": "So then to tune this W you have not only all data point but also all dimensions available and we will study this in more detail.",
                    "label": 0
                },
                {
                    "sent": "So here's some examples.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Typically, for a given object, several output variables or labels are measured.",
                    "label": 1
                },
                {
                    "sent": "Is it easier to predict em labels than it is to each of them separately?",
                    "label": 0
                },
                {
                    "sent": "So here in decision support, for example, for a given patient, many procedures are possible.",
                    "label": 1
                },
                {
                    "sent": "So you can think of it as a mapping from patient properties to what procedures should this patient be given?",
                    "label": 0
                },
                {
                    "sent": "Like depending on the diagnosis for example?",
                    "label": 0
                },
                {
                    "sent": "And and of course, their clusters now.",
                    "label": 0
                },
                {
                    "sent": "So the procedures there's some structure there now.",
                    "label": 0
                },
                {
                    "sent": "So if you if you're pregnant, you get sort of 10 procedures.",
                    "label": 0
                },
                {
                    "sent": "Each pregnant woman gets there, let's say.",
                    "label": 0
                },
                {
                    "sent": "So there are definitely clusters in this data and this should be exploited.",
                    "label": 1
                },
                {
                    "sent": "Oh, and recommendation system for a given user.",
                    "label": 0
                },
                {
                    "sent": "Many items might be of interest.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not about what this person exactly like this item, but typically we do some ranking.",
                    "label": 0
                },
                {
                    "sent": "So which of the items with the user like best like most so?",
                    "label": 0
                },
                {
                    "sent": "But there are many things to be predicted like all the preferences for all the different items?",
                    "label": 0
                },
                {
                    "sent": "On the semantic web, for a given text, many annotations are possible.",
                    "label": 1
                },
                {
                    "sent": "So the mapping from high dimension, the description of the text to a high dimensional annotation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So especially cases, structured output prediction where there is a known dependency structure between the outputs, and this simplifies the model.",
                    "label": 1
                },
                {
                    "sent": "Because if you want to predict an M dimensional distribution and you don't have any in dependencies or simplification, this is just an impossible problem and in structured output prediction we assume that there are some dependencies, for example, that this is a linear chain that the I&J&J plus one output are coupled in some sense.",
                    "label": 1
                },
                {
                    "sent": "Also, sometimes you can exploit parameter sharing which improves the data efficiency.",
                    "label": 0
                },
                {
                    "sent": "So if you reduce the parameters because a lot of parameters are the same, this of course helps a lot.",
                    "label": 0
                },
                {
                    "sent": "Applicable when the structural dependencies between the outputs are.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So here's an example which we will see a few times during the presentation.",
                    "label": 0
                },
                {
                    "sent": "So if you don't get it first time around, maybe later on it will be more clear.",
                    "label": 0
                },
                {
                    "sent": "So this is the situation where you have a text sentence.",
                    "label": 0
                },
                {
                    "sent": "Let's assume now all the tenants have M words for simplicity.",
                    "label": 0
                },
                {
                    "sent": "So these are M words and we want to map them to labels for each word.",
                    "label": 0
                },
                {
                    "sent": "And typically we want to label if something is a city or a name or company or profession over football clap something like that.",
                    "label": 0
                },
                {
                    "sent": "So and maybe we have 10 classes, so the decision is it's one of these 10 classes or it's none of these 10 classes.",
                    "label": 0
                },
                {
                    "sent": "And typically you present that in this application as a linear chain, so the labels are on the top here.",
                    "label": 0
                },
                {
                    "sent": "So in this case 8 labels for 8 words XI stands for the ice thing sentence.",
                    "label": 0
                },
                {
                    "sent": "And what you're doing here you can define a probability distribution over the wise given the X in this form, which is lock linear model.",
                    "label": 0
                },
                {
                    "sent": "But the important thing is to think about this part.",
                    "label": 0
                },
                {
                    "sent": "So here are so called feature functions which calculate some sensible features on some X and some Y.",
                    "label": 0
                },
                {
                    "sent": "So in this case on the X and the consecutive wise 'cause they form a click in this graph a click is a fully connected.",
                    "label": 0
                },
                {
                    "sent": "Graph and we can show that.",
                    "label": 0
                },
                {
                    "sent": "Most other probability distributions can be written in this way, so forget about the normalization exponent for.",
                    "label": 0
                },
                {
                    "sent": "So what you're seeing here is a superposition of basis functions, and that seems to be intuitive and simple.",
                    "label": 0
                },
                {
                    "sent": "So if this is large, should get higher probability with smaller negative should get smaller probability the exponent makes it an or not negative, and it should be normalized, so we will see this a few times during the talk, so you might want to get used to it.",
                    "label": 0
                },
                {
                    "sent": "Already a little bit, but you will hopefully understand it at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "One thing to think about it.",
                    "label": 0
                },
                {
                    "sent": "It's really not necessarily a prediction in the introduction and say OK Class one Class 0 here you predict probably solution of all the outputs.",
                    "label": 0
                },
                {
                    "sent": "So you essentially what you think about it should give you a score for distribution of wires and maybe you're interested in the most likely configuration of the wise.",
                    "label": 0
                },
                {
                    "sent": "Then you have to do some one more step to find the most likely configuration, because sometimes it's more intuitive to think of this as a scoring function.",
                    "label": 0
                },
                {
                    "sent": "Then the prediction function.",
                    "label": 0
                },
                {
                    "sent": "OK, the notation means I set a point, but all dimensions all outputs.",
                    "label": 0
                },
                {
                    "sent": "Join some condition.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Oh, this is a click.",
                    "label": 0
                },
                {
                    "sent": "So this is some overall clicks, so this is one of them is the next one.",
                    "label": 0
                },
                {
                    "sent": "So some overall clicks and then some, all overall features functions defined for the clicks.",
                    "label": 0
                },
                {
                    "sent": "Classes at this joint.",
                    "label": 0
                },
                {
                    "sent": "I mean no, no, no.",
                    "label": 0
                },
                {
                    "sent": "Is that disjoint then simplifies greatly now in general they're not discharged.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, one thing you can explore is if so, these are the click variables in the click this function.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Wait this stuff we will learn if these guys don't depend on the click.",
                    "label": 0
                },
                {
                    "sent": "So if you apply the same functions to all clicks then this simplifies the problem.",
                    "label": 0
                },
                {
                    "sent": "And also if you use the same parameters here then you can do parameter sharing.",
                    "label": 0
                },
                {
                    "sent": "So now the index C disappeared and everything so reduces the number of free parameters greatly and.",
                    "label": 0
                },
                {
                    "sent": "In some application, is absolutely necessary that this is possible.",
                    "label": 0
                },
                {
                    "sent": "And also another effect is that then you can handle also sequences of different length.",
                    "label": 0
                },
                {
                    "sent": "Before I said every sentence has M words.",
                    "label": 0
                },
                {
                    "sent": "Now we can also deal with.",
                    "label": 0
                },
                {
                    "sent": "Center so varying length because we just called.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hair care about the clicks.",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Another example image restoration.",
                    "label": 0
                },
                {
                    "sent": "So in this case X is a whole image, so serialized pixels.",
                    "label": 0
                },
                {
                    "sent": "Because we want to have a vector and these are all the pixel labels, let's say and here these are sort of typical clicks.",
                    "label": 0
                },
                {
                    "sent": "This is a pixel value.",
                    "label": 0
                },
                {
                    "sent": "You want to predict.",
                    "label": 0
                },
                {
                    "sent": "This is the measurement, so this should be close.",
                    "label": 0
                },
                {
                    "sent": "But this should also depend on neighboring pixels and you formulate it in terms of the corresponding clicks which are.",
                    "label": 0
                },
                {
                    "sent": "Only consists of two.",
                    "label": 0
                },
                {
                    "sent": "Neighboring pixels here on the neighboring nodes.",
                    "label": 0
                },
                {
                    "sent": "And you will notice that this is not.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And not an automated.",
                    "label": 0
                },
                {
                    "sent": "This is really a new story.",
                    "label": 0
                },
                {
                    "sent": "Of course people have done this in some domains for a long time.",
                    "label": 0
                },
                {
                    "sent": "This is another example social network analysis.",
                    "label": 1
                },
                {
                    "sent": "Your notes are actors.",
                    "label": 0
                },
                {
                    "sent": "Like people who know each other beside each other.",
                    "label": 0
                },
                {
                    "sent": "A typical task is the classification of actors based on the attributes of actors.",
                    "label": 1
                },
                {
                    "sent": "So maybe want to predict social status or something.",
                    "label": 0
                },
                {
                    "sent": "And then you have your income, wealth, whatever.",
                    "label": 0
                },
                {
                    "sent": "Neighborhood.",
                    "label": 0
                },
                {
                    "sent": "But you also often you you have homophily here, which means that people who are similar to each other.",
                    "label": 0
                },
                {
                    "sent": "So if you want to predict something wealth or income or social status.",
                    "label": 0
                },
                {
                    "sent": "It's very likely that your friends from the same status and this can be exploited in these type of networks is called Houma Houma failure.",
                    "label": 0
                },
                {
                    "sent": "And, uh.",
                    "label": 0
                },
                {
                    "sent": "In our so from our point of view, all these guys here are inputs.",
                    "label": 0
                },
                {
                    "sent": "The labels of all these actors and all the unknown labels.",
                    "label": 0
                },
                {
                    "sent": "Here are the target functions.",
                    "label": 1
                },
                {
                    "sent": "So if you have only one social network available for training, which is often the case, then you have to be aware that this is really only one data point.",
                    "label": 0
                },
                {
                    "sent": "So this would be all the attributes and this would all be the class labels.",
                    "label": 0
                },
                {
                    "sent": "Now, because obviously they're dependent and you cannot just segment them into independent actors or something, they're all coupled.",
                    "label": 0
                },
                {
                    "sent": "So one network is one data point and here learning also relies on parameter sharing.",
                    "label": 1
                },
                {
                    "sent": "If everybody has its own parameter, then you cannot learn anything.",
                    "label": 0
                },
                {
                    "sent": "So yeah, parameter sharing is very important that the dependencies between, let's say the wealth of your neighbors is sort of the same all over the network.",
                    "label": 0
                },
                {
                    "sent": "And and you also might have, in varying number of neighbors, so in the linear chain, every guy has two neighbors, or maybe one neighbor.",
                    "label": 0
                },
                {
                    "sent": "But here some people might have many neighbors.",
                    "label": 0
                },
                {
                    "sent": "Some people might have no no friends.",
                    "label": 0
                },
                {
                    "sent": "So you might have many friends, so this makes it also a little bit more complicated, and sometimes you need some form of aggregation to handle.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, here's another example from the presentation from Tosca and the left.",
                    "label": 0
                },
                {
                    "sent": "You'll see a sequence of proteins, so this is easy to get how it is, and this is the three dimensional structure which is very difficult to get and sort of a conditional random field type model.",
                    "label": 0
                },
                {
                    "sent": "That was the one I showed before would sort of score given a sequence given Candidate 3 dimensional structures.",
                    "label": 0
                },
                {
                    "sent": "How well does it fit?",
                    "label": 0
                },
                {
                    "sent": "Here is sort of obvious that this is not a prediction, 'cause I don't think you can really predict the three dimensional structure, but you can score it.",
                    "label": 0
                },
                {
                    "sent": "You can say or.",
                    "label": 0
                },
                {
                    "sent": "This is a good structure.",
                    "label": 0
                },
                {
                    "sent": "This is a bad structure.",
                    "label": 0
                },
                {
                    "sent": "So after the prediction there's always some sort of optimization going on or candidates candidates.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, this is something I think quite current natural language parsing.",
                    "label": 1
                },
                {
                    "sent": "So on the left side you have a sentence on the right side you have the pass tree and again you calculate the score.",
                    "label": 0
                },
                {
                    "sent": "How well does this past refit with a sentence?",
                    "label": 0
                },
                {
                    "sent": "And then you're trying to evaluate good candidates for past trees.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the last part then will be on link prediction relationship prediction and difference here is that why I J describes the link or relationship between rowenta TI and column entity J.",
                    "label": 1
                },
                {
                    "sent": "So both rows and columns now sort of have object properties and different situations.",
                    "label": 1
                },
                {
                    "sent": "They might be from the same family, same type of objects, or they might have different types.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 1
                },
                {
                    "sent": "Users and movies and they also both might have attributes.",
                    "label": 0
                },
                {
                    "sent": "And this is sort of an unusual thing, because this is not really a matrix anymore.",
                    "label": 0
                },
                {
                    "sent": "Now we sort of.",
                    "label": 0
                },
                {
                    "sent": "Just attach these attributes to the column.",
                    "label": 0
                },
                {
                    "sent": "Column this column distribution a column column at the display, and this is so that's not really a matrix anymore, so it's a little bit more general, and we will also discuss this in a little bit detail so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an typical example.",
                    "label": 0
                },
                {
                    "sent": "Properties of users, income test, age, whatever.",
                    "label": 0
                },
                {
                    "sent": "Here properties of the items like movies, action movies, whatever and you want to predict in a typical application how much, how likely it is that the user would like this object.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit unusual.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we will discuss this.",
                    "label": 0
                },
                {
                    "sent": "So if there's a key message, simple key message prediction accuracy improved in models with several response variables.",
                    "label": 1
                },
                {
                    "sent": "If some or all models are sensitive to all outputs, math not very deep.",
                    "label": 0
                },
                {
                    "sent": "But then in learning some more parameters, estimates benefit from the multiple outputs.",
                    "label": 1
                },
                {
                    "sent": "And so one counterexample is the linear case where you can really decompose the prediction of the multiple outputs in the prediction of every single output.",
                    "label": 0
                },
                {
                    "sent": "But in all other situations essentially you have this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But of course you have to like deal with sensitively.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you might also deteriorate.",
                    "label": 0
                },
                {
                    "sent": "You might get worse result 1st results, so now we get into the more technical part.",
                    "label": 0
                },
                {
                    "sent": "We start with hierarchical Bayes, so the idea is maybe it would say we predict the same thing.",
                    "label": 0
                },
                {
                    "sent": "For example, patients length of stay or patients outcome but in different situations are different hospitals.",
                    "label": 1
                },
                {
                    "sent": "Different plans maybe?",
                    "label": 0
                },
                {
                    "sent": "Also these are different tasks, so that's why sometimes work multiple task prediction.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we start with problem setting and simple solutions are weak.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the simple solutions.",
                    "label": 0
                },
                {
                    "sent": "So data collected is collected from different situations and it decides or tasks and the goal is to learn objective model.",
                    "label": 1
                },
                {
                    "sent": "If Jay so Jay is the task of X1 to M and the question is can data from other situations have to improve to improve the prediction of both one of the members of this set but also for a new new saturation new task.",
                    "label": 1
                },
                {
                    "sent": "And for simplicity we will consider models linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "So FJ of X is linear weighted sum of weights in this weights might depend on the task and some future functions fi L of X.",
                    "label": 1
                },
                {
                    "sent": "Causes linear case.",
                    "label": 0
                },
                {
                    "sent": "The special case, but we cannot have kernels on base functions and then also kernels.",
                    "label": 0
                },
                {
                    "sent": "And typically we only have very noisy measurements available from the apps we don't can.",
                    "label": 0
                },
                {
                    "sent": "We cannot measure F directly, but there's some noise on the apps and this call then why?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first simple solution is to have one global model, so just ignore that there are different situations, different hospitals, different plans, just put all data in one model, pull all data in one model and train one global model.",
                    "label": 1
                },
                {
                    "sent": "So this one simple solutions.",
                    "label": 0
                },
                {
                    "sent": "So they are fruits.",
                    "label": 1
                },
                {
                    "sent": "So everything is the same, not apples and oranges.",
                    "label": 0
                },
                {
                    "sent": "So definitely this data efficient because all the data are used very efficiently.",
                    "label": 0
                },
                {
                    "sent": "But the problem of course it ignores the differences in the different situations, so you should obviously always try first the simple solutions if they already work well, you don't have to try the difficult ones.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next simple solution is that each model is at each situation.",
                    "label": 1
                },
                {
                    "sent": "Each output is modeled individually, so this is trained only on the data for the corresponding output.",
                    "label": 0
                },
                {
                    "sent": "The problem is, it's not very efficient in terms of the data you throw away all the information from the other outputs.",
                    "label": 0
                },
                {
                    "sent": "So if you have a lot of data for a given situation cause no problem with that.",
                    "label": 0
                },
                {
                    "sent": "But if you only have small number of data available then you don't exploit the outputs from the other dimensions.",
                    "label": 0
                },
                {
                    "sent": "So only one output dimensional data from output dimension contribute to a parameter estimates.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, the next I think was also suggestion that if you have different hospitals, why don't you present it in some way?",
                    "label": 0
                },
                {
                    "sent": "For example, in this way in this case, it means the patient was in the James Hospital, so we indicate in which hospital or in which situation somebody was and just add that as an additional input.",
                    "label": 0
                },
                {
                    "sent": "It's also very data efficient 'cause you only have one model to train and use all the data, but sometimes it's difficult to find a good model of how to integrate this information.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a linear model, obviously the only thing which happens is that you learn a constant offset for each for each situation, which is a little simple, but if you have good insight and how.",
                    "label": 0
                },
                {
                    "sent": "The identity of the plant also might influence the prediction.",
                    "label": 0
                },
                {
                    "sent": "Then this is also a usable, usable thing, but.",
                    "label": 0
                },
                {
                    "sent": "Prediction if I don't know which hospital it comes from, can I still apply the same model, let's say during testing?",
                    "label": 0
                },
                {
                    "sent": "I don't know where I'm going.",
                    "label": 0
                },
                {
                    "sent": "Oh there yeah, there are solutions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you probably have to estimate something about which what you think based on the data, which hospital it might have come from.",
                    "label": 0
                },
                {
                    "sent": "Is it fair if I just ignore this feature during prediction?",
                    "label": 0
                },
                {
                    "sent": "Later, when you when you make the prediction, that would probably one reasonable solution.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "List the next tutorial.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's then start with a sort of interesting part, at least from my point of view.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the situation.",
                    "label": 0
                },
                {
                    "sent": "We have this linear model again.",
                    "label": 0
                },
                {
                    "sent": "We assume the parameters are priority come from a normal distribution, zero mean and some isotropic covariance.",
                    "label": 0
                },
                {
                    "sent": "So if you're not page and then this is sort of like a weight decay thing and let's assume we have for a new situation.",
                    "label": 1
                },
                {
                    "sent": "We have observed these three data points and and all these things here.",
                    "label": 1
                },
                {
                    "sent": "I hope this is understandable.",
                    "label": 0
                },
                {
                    "sent": "They're supposed to be all these fires all these different basis functions, so they have this local shape like this local Gaussian or something.",
                    "label": 0
                },
                {
                    "sent": "And there here densely distributed in the input space.",
                    "label": 1
                },
                {
                    "sent": "And here I have drawn some of them, not all of them.",
                    "label": 0
                },
                {
                    "sent": "But what happens so in in regions where you have the data, the model probably fits nicely.",
                    "label": 0
                },
                {
                    "sent": "The data happen in regions without data.",
                    "label": 0
                },
                {
                    "sent": "The prediction is simply 0.",
                    "label": 0
                },
                {
                    "sent": "Which makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "So in regions where I didn't see any data, you should better say zero, then some wild guesses about what it might be.",
                    "label": 0
                },
                {
                    "sent": "Food is very sensible and makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we have more information, so the red, blue and green lines supposed to be models which were trained on similar problems previously, let's say.",
                    "label": 0
                },
                {
                    "sent": "And which we want to also exploit.",
                    "label": 0
                },
                {
                    "sent": "So if we now observe these three data points over here.",
                    "label": 0
                },
                {
                    "sent": "We might say, oh, probably this gonna be between.",
                    "label": 0
                },
                {
                    "sent": "The blue and the red.",
                    "label": 0
                },
                {
                    "sent": "And so maybe this prediction is more sensible, very different from the prediction we had before.",
                    "label": 0
                },
                {
                    "sent": "So in a situation where hierarchical Bayes is applicable, this is going on.",
                    "label": 1
                },
                {
                    "sent": "We want to sort of say, OK, we have already observed a lot of different models, and we expect that the new model will be sort of similar to the already observed observed models.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the key is to to look at the parameters parameter distribution of the previously trained models.",
                    "label": 0
                },
                {
                    "sent": "So let's look at two of the basis function, the number 10 and #100.",
                    "label": 0
                },
                {
                    "sent": "So this guy and this guy.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the parameters for the colored models, we might get this, you know.",
                    "label": 1
                },
                {
                    "sent": "So if W 100 is larger W tennis a little smaller, and so on, so you get a distribution from the previous models.",
                    "label": 0
                },
                {
                    "sent": "And here a couple more of them.",
                    "label": 0
                },
                {
                    "sent": "So the idea of hyperactivation modeling is not to say OK, why should we?",
                    "label": 0
                },
                {
                    "sent": "If you have a new model, why should we start again with this stupid assumption?",
                    "label": 0
                },
                {
                    "sent": "Zero mean and a topic covariance?",
                    "label": 0
                },
                {
                    "sent": "Maybe you should start now.",
                    "label": 0
                },
                {
                    "sent": "Use this distribution for the new model.",
                    "label": 0
                },
                {
                    "sent": "Saskia so we have a new distribution of the WS.",
                    "label": 0
                },
                {
                    "sent": "Given that all the past data again normally distributed but now with the mean vector M and with the covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "So this is the mean vector and the Sigma describes the distribution of these things here.",
                    "label": 0
                },
                {
                    "sent": "Each separate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a vector, so this thing is always selected.",
                    "label": 0
                },
                {
                    "sent": "So so these are two and so and so in this case it would be 200 dimensional singerly.",
                    "label": 0
                },
                {
                    "sent": "So it would be difficult to draw.",
                    "label": 0
                },
                {
                    "sent": "So there are many dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so so new model.",
                    "label": 0
                },
                {
                    "sent": "This is a learn prior and I will talk about this concept later on.",
                    "label": 0
                },
                {
                    "sent": "W newer, given the data is newer Gaussian and so based on this information we can also predict how a new system without any data would look like.",
                    "label": 0
                },
                {
                    "sent": "So what's the expected value of new thing without data given the past data is some of basis functions where it simply substitute the mean for the weights?",
                    "label": 0
                },
                {
                    "sent": "And the other information which is important here is what is the covariance between two functional values XC and XC.",
                    "label": 0
                },
                {
                    "sent": "So we look at two functional values and we want to see how much they correlate.",
                    "label": 0
                },
                {
                    "sent": "So this one is a little larger, so the other one also larger.",
                    "label": 0
                },
                {
                    "sent": "Or is the other one lower?",
                    "label": 0
                },
                {
                    "sent": "And this can be calculated.",
                    "label": 0
                },
                {
                    "sent": "So this is the only one of the few formulas which hopefully you're familiar with.",
                    "label": 0
                },
                {
                    "sent": "It's easily to translate this uncertainty into the uncertainty of the function prediction, and for the mean you get this formula and for the covariance you get this formula.",
                    "label": 0
                },
                {
                    "sent": "So this is Phi, transposed firefighter, source XI, Sigma.",
                    "label": 0
                },
                {
                    "sent": "This Sigma Phi XK.",
                    "label": 0
                },
                {
                    "sent": "So this gives you exactly this expression.",
                    "label": 0
                },
                {
                    "sent": "The events between these two functional areas and the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I also tried to try to draw again so this dashed thing now might be the mean function.",
                    "label": 0
                },
                {
                    "sent": "Substituting the mean for the weights and this covariance would indicate if I look at two functional values.",
                    "label": 0
                },
                {
                    "sent": "And so that the red above the mean, then this is also above the mean.",
                    "label": 0
                },
                {
                    "sent": "So they are positively correlated obviously.",
                    "label": 0
                },
                {
                    "sent": "So this is what this expression stands for.",
                    "label": 0
                },
                {
                    "sent": "The correlation of functional values.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we're playing a little mathematical trick.",
                    "label": 0
                },
                {
                    "sent": "We can compose this learn Sigma in to VDD, transposed.",
                    "label": 0
                },
                {
                    "sent": "Be transposed.",
                    "label": 0
                },
                {
                    "sent": "So this is based on a single ability composition.",
                    "label": 0
                },
                {
                    "sent": "These are these guys have author noggle orthonormal columns and this diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "I could also written here D but to be consistent with the later stuff I write DD transpose because this diagonal that's trivial.",
                    "label": 0
                },
                {
                    "sent": "Just take the square root.",
                    "label": 0
                },
                {
                    "sent": "So there's probably no one.",
                    "label": 0
                },
                {
                    "sent": "And I can now plug this into this formula.",
                    "label": 0
                },
                {
                    "sent": "If you transpose Sigma Phi of XJ and now I can.",
                    "label": 0
                },
                {
                    "sent": "So I just plug this into here and then I can break it in this way.",
                    "label": 0
                },
                {
                    "sent": "Now I can interpret this equation differently.",
                    "label": 0
                },
                {
                    "sent": "I can say here's no more covariance matrix or it's the identity covariance matrix, and these are now my new basis functions.",
                    "label": 1
                },
                {
                    "sent": "So I've known are Gaussian parameters with identity covariance matrixes with learned basis functions formed as linear combination of the original basis function.",
                    "label": 1
                },
                {
                    "sent": "So if you look at this guy here and you think of it as a new basis function which is standing there, then sigh of K of X or the case basis function is DKK, which is from this guy here and then the sum over VLC from this guy times 5 L of X.",
                    "label": 0
                },
                {
                    "sent": "So this just I mean, this is still the same.",
                    "label": 0
                },
                {
                    "sent": "Math is just a little bit of a different way of looking at that.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is if you stay with these basis functions, you have to work then with this complicated prior distribution, if you use these basis functions, you can now train every model as before with this trivial simple basis function with simple representation with with a normal distribution, zero mean and.",
                    "label": 0
                },
                {
                    "sent": "And and at the tropical variants.",
                    "label": 0
                },
                {
                    "sent": "Of course, you would have to add the mean again.",
                    "label": 0
                },
                {
                    "sent": "So to be clear about this.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A graphical thing.",
                    "label": 0
                },
                {
                    "sent": "The Sky was going on.",
                    "label": 0
                },
                {
                    "sent": "You have the input X you calculate first the original basis functions 5.",
                    "label": 0
                },
                {
                    "sent": "Then you transfer form this with a VD transpose thing.",
                    "label": 0
                },
                {
                    "sent": "You get the new basis function PSI.",
                    "label": 0
                },
                {
                    "sent": "And then you might you learn new weights.",
                    "label": 0
                },
                {
                    "sent": "In this usual way, just wait, decay or just isotropic Gaussian distribution and then you get the prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a graphic representation.",
                    "label": 0
                },
                {
                    "sent": "What is going on?",
                    "label": 0
                },
                {
                    "sent": "And if you drop out, maybe size here with very small dies.",
                    "label": 0
                },
                {
                    "sent": "You can also get a dimensionality reduction, but this is not really necessary.",
                    "label": 0
                },
                {
                    "sent": "Even if you do this you get improved prediction because by doing this essentially you put a very informative prior on the weights and you get this coupling between the different models.",
                    "label": 0
                },
                {
                    "sent": "Why you?",
                    "label": 0
                },
                {
                    "sent": "Cause I see.",
                    "label": 0
                },
                {
                    "sent": "Your new feature is in your combination of the old feature, so you never bound.",
                    "label": 0
                },
                {
                    "sent": "You are you're serious, serious, serious property I guess.",
                    "label": 0
                },
                {
                    "sent": "Don't want.",
                    "label": 0
                },
                {
                    "sent": "OK, you're right.",
                    "label": 0
                },
                {
                    "sent": "These are all linear transformations, and so first you think a lot of linear transformation boil down to like one linear transformation.",
                    "label": 0
                },
                {
                    "sent": "So what's the big deal but the.",
                    "label": 0
                },
                {
                    "sent": "Right answer is that you put the right distribution on the parameters and if you would you have the.",
                    "label": 0
                },
                {
                    "sent": "If you put.",
                    "label": 0
                },
                {
                    "sent": "If you look at the original distribution over here, you would put the prior on the parameters which have mean M and and some covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "But if you derive this new basis functions then you can use a prior which is isotropic.",
                    "label": 0
                },
                {
                    "sent": "Much simpler to work with.",
                    "label": 0
                },
                {
                    "sent": "I mean, do you have some married with this with this new feature?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean OK. Let's see I mean the merit is.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is going on direction.",
                    "label": 0
                },
                {
                    "sent": "I mean, mathematically, I didn't do anything.",
                    "label": 0
                },
                {
                    "sent": "New owner is exactly the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing, so it's a difference between doing this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And doing this.",
                    "label": 0
                },
                {
                    "sent": "So you think this drive features that you will get a combination of the original features which allow you to easily model this?",
                    "label": 0
                },
                {
                    "sent": "I know.",
                    "label": 0
                },
                {
                    "sent": "But mathematically, it's exactly doing this.",
                    "label": 0
                },
                {
                    "sent": "It's just you can also think of sticky and say OK, this is what I what I think is right.",
                    "label": 0
                },
                {
                    "sent": "You have a new product line, prior distribution of of W. Plug them into your original model and do the prediction.",
                    "label": 0
                },
                {
                    "sent": "But mathematically it's exactly equivalent to doing this here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe some theoretical bounds don't change.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, but from like a regular statistical view of Asian view, this is gives you very different results.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can discuss more offline.",
                    "label": 0
                },
                {
                    "sent": "Yeah, also an interesting comparison.",
                    "label": 0
                },
                {
                    "sent": "I mentioned networks the beginning so this looks almost like a neural network.",
                    "label": 1
                },
                {
                    "sent": "If you're old enough to know these things are very new.",
                    "label": 0
                },
                {
                    "sent": "Very young to know these things so.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 hidden layer.",
                    "label": 0
                },
                {
                    "sent": "The second layer is the prediction of the multiple outputs.",
                    "label": 1
                },
                {
                    "sent": "Of course in their networks has nonlinearities everywhere and so on.",
                    "label": 0
                },
                {
                    "sent": "But it's interesting that it's not too far away, so if you just click this problem into another network with the corresponding number of outputs, maybe results are also quite interesting.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to also show that there is little bit math behind it because you might be worried you have a small number of data points of weights you have trained and they should define this complicated covariance matrix so you can of course put nice priors on everything and essentially what you want to do is you want to learn this.",
                    "label": 0
                },
                {
                    "sent": "Then the mean function and the covariance Sigma and so the technologies are that you put a inverse Wishart distribution on the Sigma.",
                    "label": 0
                },
                {
                    "sent": "Given some parameter and sort of something which should be similar to the Sigma.",
                    "label": 0
                },
                {
                    "sent": "Put a prior on the mean with some means move, which is typically zero.",
                    "label": 0
                },
                {
                    "sent": "And here's some precision.",
                    "label": 0
                },
                {
                    "sent": "How much do you believe in this mean then, for each situation you draw a parameter vector out of using this mean and the Sigma, and then you draw all the data.",
                    "label": 0
                },
                {
                    "sent": "So these are all the data for situation J with mean W, J5, J Sigma square I.",
                    "label": 0
                },
                {
                    "sent": "And then one way to do this to deal with this is you doing M procedure.",
                    "label": 0
                },
                {
                    "sent": "In the step you estimate the probability distribution of each weight vector for each for each situation given the data for this situation given the mean and the current estimate of the mean and covariance.",
                    "label": 0
                },
                {
                    "sent": "And in the M step you update your mean and your covariance.",
                    "label": 0
                },
                {
                    "sent": "So you go up first and back a few times, and then the system converges and you get nicely tuned parameters.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's an issue if you want to get into this deeper, deeper.",
                    "label": 0
                },
                {
                    "sent": "This is 1 definition of the inverse Wishart, so we don't have to look at too closely now, but there are.",
                    "label": 1
                },
                {
                    "sent": "There are small differences in the exact definition, and it seems to us that this is the most appropriate definition because it has some nice features in terms of mechanization of the distribution, and this is a reference where this has been discussed.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this I mean for this presentation is just mathematical detail.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the key benefit in her equation modeling for linear systems is a common basis.",
                    "label": 1
                },
                {
                    "sent": "Functions are learned that are used for all outputs.",
                    "label": 1
                },
                {
                    "sent": "Now we sort of do something a little bit off track, but maybe it's interesting and I'm sure it's more confusing.",
                    "label": 0
                },
                {
                    "sent": "Interesting, but I want to discuss it.",
                    "label": 0
                },
                {
                    "sent": "Percabeth function based functions derive from a single ability composition.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "According to our model, the covariance of two functional values is going by side transposed side of the two at the two data points and let's call this guy KIK for the indicator.",
                    "label": 0
                },
                {
                    "sent": "But so this should correspond to the covariance in the functional values of the different functions.",
                    "label": 0
                },
                {
                    "sent": "And now we assume regression with little noise.",
                    "label": 1
                },
                {
                    "sent": "So in this case we can also say oh this K should also be approximately the empirical.",
                    "label": 0
                },
                {
                    "sent": "Covariance of the observed wise.",
                    "label": 0
                },
                {
                    "sent": "So if we.",
                    "label": 0
                },
                {
                    "sent": "If we do this estimate, this should be quite close to this because this is what we're modeling.",
                    "label": 0
                },
                {
                    "sent": "We're trying to predict how different functional areas are very, and since now we have several samples from this.",
                    "label": 0
                },
                {
                    "sent": "From this function, we can really estimate this also empirically.",
                    "label": 0
                },
                {
                    "sent": "And if I look now at this KIK over here it's the inner product of all the data from different situations.",
                    "label": 0
                },
                {
                    "sent": "For data point, I inferred data point K, the strainer product.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this formula in this formula, you might say OK, why not take PSI K of XI simply BYOK?",
                    "label": 0
                },
                {
                    "sent": "It's sort of a little bit stupid thing to do, but why not formally?",
                    "label": 0
                },
                {
                    "sent": "Looks like very similar and it would give you.",
                    "label": 0
                },
                {
                    "sent": "The covariance kernel, which seems to be quite close to the correct one, and then these weights the weights on the side are sort of the Delta function.",
                    "label": 0
                },
                {
                    "sent": "So you put the corresponding output dimension for the corresponding from the corresponding Y entry.",
                    "label": 0
                },
                {
                    "sent": "So to learn basic functions are simply given by the output data.",
                    "label": 1
                },
                {
                    "sent": "The disadvantage of course are different situations do not benefit from one another, because obviously every every every column is then independent of each other.",
                    "label": 1
                },
                {
                    "sent": "But still we can make predictions based on the estimated K. So if you assume that these are the basis functions instead of the size we just arrived, we get this as a covariance kernel estimate and we can do predictions with this one as well.",
                    "label": 1
                },
                {
                    "sent": "So it's just an interesting.",
                    "label": 0
                },
                {
                    "sent": "Connection to this idea of just doing an empirical estimation of the covariance kernel, why do we do this complicated thing?",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit smarter and maybe do an SVD of YUDV, transposed, and then this expression we've seen before is now this guy, this entity, and it's approximately the same thing.",
                    "label": 0
                },
                {
                    "sent": "If we do a reduced rank approximation, or we set a lot of diagonal terms to zero the small ones.",
                    "label": 0
                },
                {
                    "sent": "And then this is a little bit more sensible and we get our new basis.",
                    "label": 0
                },
                {
                    "sent": "Functions are now DKK, so in this diagonal term terms you I can this you matrix and another way of writing it is this expression over here, and the W is the weights on these guys simply come from the corresponding.",
                    "label": 0
                },
                {
                    "sent": "These in this decomposition.",
                    "label": 0
                },
                {
                    "sent": "So here it's a little more interesting because the singular values singular vectors are calculated based on all data and statistical strength, shared singular value composition.",
                    "label": 1
                },
                {
                    "sent": "We sort of find the best representation in some sense for the data, the best low rank approximation and this might explain a little bit the success of matrix composition methods in collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "For example, the Netflix company competition.",
                    "label": 0
                },
                {
                    "sent": "This really high scale competition and corporate filtering.",
                    "label": 0
                },
                {
                    "sent": "Alot of the approaches are based on uh.",
                    "label": 0
                },
                {
                    "sent": "Decomposition of the matrices and this is one way maybe of looking at this just to think of OK, we define sensible basis functions and reduce noise in the data in this way.",
                    "label": 0
                },
                {
                    "sent": "But this is sort of what is sidestep so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's not really necessary for the whole flow of the idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so the advantages of hierarchical basis that you include, I mean compared, let's say 2 SVD or the empirical version you can include prior knowledge by defining the basis functions by defining the basis functions you give sort of you sort of so show what collation between functions you expect and you're learning typically makes them stiffer, not more flexible.",
                    "label": 0
                },
                {
                    "sent": "So if you have a good intuition, what are good basis functions for your?",
                    "label": 0
                },
                {
                    "sent": "Application then then you can imply implement this prior knowledge by defining the basis functions, whereas if you do the SVD stuff or the empirical stuff, this is not really so easily possible.",
                    "label": 1
                },
                {
                    "sent": "You can also generalize to new input, because after all you learn a mapping from X to Y, and if you have 4 new situation for a new data point, you only know the X.",
                    "label": 0
                },
                {
                    "sent": "You can do predictions and as I mentioned before, and if you look at the M equations, that would be also maybe more obvious.",
                    "label": 0
                },
                {
                    "sent": "There's no problem with missing outputs, so if you have situations where a lot of the outputs are missing, arrangement base can work with that quite elegantly.",
                    "label": 0
                },
                {
                    "sent": "So we talked about learning and inference using AM.",
                    "label": 0
                },
                {
                    "sent": "Other inference is off performed via Gibbs sampling or other appropriate methods such as variational learning.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you are familiar with latent usually allocation, which is essentially hierarchical Bayesian model, then they have experimented alot of skip sampling and variational learning.",
                    "label": 0
                },
                {
                    "sent": "And also another advantage is that the model doesn't have to be linear.",
                    "label": 0
                },
                {
                    "sent": "So if you mean any reasonable model is fine, you can do recognition learning with.",
                    "label": 0
                },
                {
                    "sent": "And this is a book where you might want to look.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you are interested in this more deeply, so three phases in the modeling with no data yet available for a new situation, the model essentially will follow the mean function.",
                    "label": 1
                },
                {
                    "sent": "The best thing it can do in the second phase with small data available for new situation model might follow more closely a previous model that fits those data well, so it will sort of out of the past models.",
                    "label": 1
                },
                {
                    "sent": "Which one is the best one which looks very close to what I'm doing in the new model and finally with a lot of data it becomes independent of the prior, so if a lot of data you don't have to do with all of this because then you can just take all this large amount of data to train the model individually.",
                    "label": 0
                },
                {
                    "sent": "OK, the dimensionality reduction can be achieved if you set some of these terms.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To zero as you discussed.",
                    "label": 0
                },
                {
                    "sent": "So from this point of view, we can look at the statement from history again.",
                    "label": 0
                },
                {
                    "sent": "So if we know M and Sigma are priority.",
                    "label": 0
                },
                {
                    "sent": "So we have some insight into this problem and we have really pretty informative feel about how the mean and the Sigma should be.",
                    "label": 0
                },
                {
                    "sent": "And it also corresponds to the empirical observation of parameters we can get later.",
                    "label": 0
                },
                {
                    "sent": "Then all output functions are independent.",
                    "label": 1
                },
                {
                    "sent": "And then there's no coupling between the different outputs.",
                    "label": 0
                },
                {
                    "sent": "Another situation might be if the functions really have no prior common distributions, like if you predict two very difficult different things, where the assumption that they sort of come from the same family of distributions doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "But if the priors learned as I presented, then all measurements influence all predictions by influencing each other via the prior distribution.",
                    "label": 1
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was the.",
                    "label": 0
                },
                {
                    "sent": "I have one comment about that.",
                    "label": 0
                },
                {
                    "sent": "So these models you presented in work when correlated outputs are pretty much same thing right when their length of stay?",
                    "label": 0
                },
                {
                    "sent": "Yep, Yep.",
                    "label": 0
                },
                {
                    "sent": "Prior or one with import.",
                    "label": 0
                },
                {
                    "sent": "Yahoo.",
                    "label": 0
                },
                {
                    "sent": "Yes, in principle yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So but it's always a challenge how far you can get away from this idea.",
                    "label": 0
                },
                {
                    "sent": "So maybe what was it?",
                    "label": 0
                },
                {
                    "sent": "Exchange rate and.",
                    "label": 0
                },
                {
                    "sent": "Whatever fashionable things suddenly correlate, so maybe there's correlation where you shouldn't expect correlation, but in principle is right.",
                    "label": 0
                },
                {
                    "sent": "What you're saying from the from the conceptual point of view, there should be some reason why these things should come from the same family of distributions.",
                    "label": 0
                },
                {
                    "sent": "My.",
                    "label": 0
                },
                {
                    "sent": "View orders OK. Why don't we just learn the joint distribution of everything and then do conditioning?",
                    "label": 0
                },
                {
                    "sent": "Why is that not a viable?",
                    "label": 0
                },
                {
                    "sent": "OK. Everything.",
                    "label": 0
                },
                {
                    "sent": "Good point, that should be one of the solutions I should have mentioned.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "I think if you have.",
                    "label": 0
                },
                {
                    "sent": "Pretty much complete data then this is possible, but if you.",
                    "label": 0
                },
                {
                    "sent": "So if the otherwise the ones you are, you don't need to predict.",
                    "label": 0
                },
                {
                    "sent": "You can treat as input, but then they sort of become inputs.",
                    "label": 0
                },
                {
                    "sent": "So here I guess the essential part is also the pattern of missing information.",
                    "label": 0
                },
                {
                    "sent": "Is is is NOT is is difficult, so to manage, probably that's one answer.",
                    "label": 0
                },
                {
                    "sent": "If you're if it's in training, maybe you have complete data, but then when you want to apply it to a new situation, you only know the input.",
                    "label": 0
                },
                {
                    "sent": "But then you would have to treat all the other wise as missing data and which is also difficult.",
                    "label": 0
                },
                {
                    "sent": "But if you have complete data in training, if your matrix is complete and training then this might be an option.",
                    "label": 0
                },
                {
                    "sent": "One can work with.",
                    "label": 0
                },
                {
                    "sent": "But maybe a later discussion, though there will be other opinions on this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was a Bayesian treatment.",
                    "label": 1
                },
                {
                    "sent": "Now they're just one slide on the frequentist version of this.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this model.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "So all the data for output J is M times design matrix five of J.",
                    "label": 0
                },
                {
                    "sent": "So this could be specific to J plus CJ.",
                    "label": 0
                },
                {
                    "sent": "Times PJ plus epsilon J.",
                    "label": 0
                },
                {
                    "sent": "So this is a linear model is called a mixed model.",
                    "label": 0
                },
                {
                    "sent": "What one assumes to be known as this file?",
                    "label": 0
                },
                {
                    "sent": "So the design matrix and also the Z.",
                    "label": 0
                },
                {
                    "sent": "And the interesting cases when both are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, but there could be different.",
                    "label": 0
                },
                {
                    "sent": "OK then one thing is this fixed effect M. It's unknown, so it has to be estimated and this is what frequentis like to do.",
                    "label": 0
                },
                {
                    "sent": "Owner is unknown but can be estimated.",
                    "label": 0
                },
                {
                    "sent": "This guy over here is random is focal random effect, which is different for each output.",
                    "label": 0
                },
                {
                    "sent": "So that's why it has an index J.",
                    "label": 0
                },
                {
                    "sent": "And OK, let's consider the special case.",
                    "label": 0
                },
                {
                    "sent": "D equals this fine matrix.",
                    "label": 0
                },
                {
                    "sent": "Then it's called regression model with random coefficients and we get the connection to the hierarchical Bayesian model.",
                    "label": 1
                },
                {
                    "sent": "If we just put WJ equals M plus BJ.",
                    "label": 0
                },
                {
                    "sent": "So M is our old M. The mean of this learned Gaussian distribution and BJ is sort of the deviation from this mean.",
                    "label": 0
                },
                {
                    "sent": "So if we do all this then, but these guys are the same here.",
                    "label": 0
                },
                {
                    "sent": "We have M + B which is W and looks very much like our hierarchical linear model.",
                    "label": 0
                },
                {
                    "sent": "BJ is assumed to be normally distributed with zero mean and covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "Because the mean was already explicitly taken out, and one more aspect here is that you can allow correlated noise.",
                    "label": 0
                },
                {
                    "sent": "So the stuff you cannot explain might still contain some correlation.",
                    "label": 0
                },
                {
                    "sent": "And in this mixed models they take that into account and this is sometimes what we call collaborative collaborative effect.",
                    "label": 0
                },
                {
                    "sent": "'cause if you do collaborative filtering, you can work without any inputs and this sort of this term over here.",
                    "label": 0
                },
                {
                    "sent": "So maybe I?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure if it was clear, but it's you can really translate this week very quickly into the Bayesian framework and the statement maybe to make us mixed models is beige and as a frequentist will ever get 'cause this is what they hate.",
                    "label": 1
                },
                {
                    "sent": "Sort of random parameters.",
                    "label": 1
                },
                {
                    "sent": "That's what they really like to avoid, but there's something where they sort of accept that they need something like that, and it's a hurricane.",
                    "label": 0
                },
                {
                    "sent": "Incentives also, as frequent as a base, and maybe we'll ever get, because when I always said we learned the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "And we learn the parameters and to practice something well.",
                    "label": 0
                },
                {
                    "sent": "If you're really a true beige, and you'll probably always think about what is he talking about, but this is, I mean intuitively, at least what is going on with you tune the parameters in your so called prior distribution based on the data.",
                    "label": 0
                },
                {
                    "sent": "So here's sort of a region where both.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of them nicely join forces.",
                    "label": 0
                },
                {
                    "sent": "In some sense they are also non probabilistic, closely related things.",
                    "label": 0
                },
                {
                    "sent": "Recent work here from this group.",
                    "label": 0
                },
                {
                    "sent": "If the new maturity and punter 2006, here's a loss function.",
                    "label": 0
                },
                {
                    "sent": "He has a two norm.",
                    "label": 0
                },
                {
                    "sent": "Constraint on the B is another two norm constraint on the M. Again, W decomposes into M The mean thing for other models and the specific thing the BJ here implicitly there, assuming isotropic covariance.",
                    "label": 0
                },
                {
                    "sent": "So essentially what they're learning is the mean and not really the covariance structure over here, so it's a little bit a simpler version if you want, but there's some advantage.",
                    "label": 0
                },
                {
                    "sent": "There is now a convex optimization problem, so we don't have to deal with linear Edwards with local optimize any sense.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another work from this group.",
                    "label": 0
                },
                {
                    "sent": "Again, here the Altoona Morrell Norm and here is called AL 1L2 Norm, which is a little bit reminder of the lesser cost function if you are familiar with that and the effect is that it really removes the sum of the values from the model, so we can really also prune inputs.",
                    "label": 0
                },
                {
                    "sent": "So it selects features in the optimization.",
                    "label": 0
                },
                {
                    "sent": "And that's also related work from another group.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this was a problem.",
                    "label": 0
                },
                {
                    "sent": "Now we could go back to the probabilistic part again and I want to look briefly at the Gaussian process version of this.",
                    "label": 0
                },
                {
                    "sent": "I mean, I tried to convince you before that if we start with this model, linear model parameters are Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "This sort of technically equivalent to a Gaussian process, so-called Gaussian process, where we define the covariance between two data points.",
                    "label": 1
                },
                {
                    "sent": "So this is indicates how much function value is very.",
                    "label": 0
                },
                {
                    "sent": "Cool area which is done by this formula which I've shown already and the main function is done by this formula which we have also seen already.",
                    "label": 1
                },
                {
                    "sent": "So as in hierarchical base boils down to learning mean and covariance in the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical Bayes we want to learn this mean function.",
                    "label": 0
                },
                {
                    "sent": "And the convince Colonel.",
                    "label": 0
                },
                {
                    "sent": "And of course we can do it by first learning these guys and then translating it into the kernel space or the functional space.",
                    "label": 0
                },
                {
                    "sent": "Or we can also try to learn these guys directly.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the first version we sort of work with the basis function representation of translated into kernel covariance kernel.",
                    "label": 0
                },
                {
                    "sent": "So if we work with empirical 1, otherwise the kernel is or the kernel on the training data points, so it's a matrix defined for all pairs of training data points is just the empirical estimate.",
                    "label": 0
                },
                {
                    "sent": "This quarter gram matrix.",
                    "label": 0
                },
                {
                    "sent": "SVD we work with a single vectors and singular values and we get this expression and in hierarchical base we derive this.",
                    "label": 0
                },
                {
                    "sent": "I functions and get this expression for the kernel.",
                    "label": 0
                },
                {
                    "sent": "But this OK not much new because it's just.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of, the translation of the previous result into Gaussian process, so maybe it's more interesting to really optimize in the in the really in the mean function and the covariance itself.",
                    "label": 0
                },
                {
                    "sent": "So we learn sort of a K, which is a cool, dense kernel and the mean and 1st we do it only at a finite number of points.",
                    "label": 1
                },
                {
                    "sent": "So we define typically the training data and maybe some test points, so it's more like transduction.",
                    "label": 1
                }
            ]
        }
    }
}