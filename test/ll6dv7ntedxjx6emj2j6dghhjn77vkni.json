{
    "id": "ll6dv7ntedxjx6emj2j6dghhjn77vkni",
    "title": "Fast Nearest Neighbor Search in Disk-resident Graphs",
    "info": {
        "author": [
            "Purnamrita Sarkar, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/kdd2010_sarkar_fnnsdrg/",
    "segmentation": [
        [
            "My name is Poo Namrita soccer and I'm going to talk about fast nearest neighbor search and disk resident graphs.",
            "This is joint work with Andrew Moore.",
            "So what exactly is near?"
        ],
        [
            "Neighbor searching a graph given a node I I want to find which other nodes are most similar to this node.",
            "I and this shows up in many different interesting practical problems.",
            "For example, take the Facebook graph here suggesting a friend for a given node essentially boils down to finding other nodes which are most similar to that node.",
            "Also in the context of DLP.",
            "If I build a paper Ward author node graph from this DBL Publication database, their keyword specific search boils down to finding.",
            "Finding papers which are most similar to the given set of keywords now for nearest neighbor search we clearly need some sort of notion of nearness or proximity or similarity, and for this talk we're going to focus on random walk based measures of approx."
        ],
        [
            "In particular, personalized page rank and hitting and commute Times Now these measures are intuitive measures of similarity.",
            "Why?",
            "Because if two nodes are connected via lots of shortcuts, they're going to score very high according to these measures and also they have been successfully used in a wide variety of applications in the DBL context the query type was fine.",
            "We came most relevant papers about support vector machines so queries can arbitrary.",
            "This would have been papers about ornaments for example.",
            "And computing these measures at query time with your activity of research.",
            "For this talk, we're going to focus."
        ],
        [
            "And two computational issues associated with these these measures.",
            "The first one is the nuisance of high degree nodes, and often we have algorithms which are local in nature, so we have an algorithm which examines a small neighborhood around the query node and tries to answer questions.",
            "And the moment this neighborhood hits a high degree node, you have to bring in all its neighbors and the size blows up and everything becomes slow.",
            "So that's one of the problems that we will try to address.",
            "The second one is the case where the graph is so large that it does not fit into main memory.",
            "And there are very interesting streaming algorithms, but even for them we have to make a few passes through the entire data set and that might not be desirable at query time.",
            "Now these two different problems might seem completely independent, but we will actually show later in the experiment section.",
            "That interesting solution to the first one actually makes the second solution much faster.",
            "So what we want is external memory framework to represent the graph so that we can one support arbitrary queries and two we can compute many different random walk based measures on it."
        ],
        [
            "OK, so that brings us to the outline of this talk.",
            "First, we're going to introduce these different random log based measures and then we will.",
            "Address the two different issues that I talked about and finally we will give results.",
            "So coming to proximity measures random walk based proximity measures, let's first talk about personal."
        ],
        [
            "Pagerank.",
            "We will also call these PPV.",
            "So the idea is simple.",
            "You start a random walk at no die and now at any step you restart this random walk with probability Alpha from the start mode I and the stationary distribution of this stochastic processes.",
            "The personalized page rank with respect to know die.",
            "So now if there is a node J which has lots of short paths from node I that's going to have a very large personalized Pagerank value.",
            "Essentially that's similar to I.",
            "Another measure is the discounted version of hitting times.",
            "And here again, you start a random walk from Lodi and at any step you either stop if you have hit node J or you stop with probability Alpha and Now the expected time to stop gives you the discounted hitting time from node.",
            "I to know Jan if this is small then two nodes are similar.",
            "OK, now let's come."
        ],
        [
            "The high degree nodes we will first talk about its effect on personalized page rank and then its effect on discounted hitting times so."
        ],
        [
            "As I said before, high degree nodes can often cause problems for computational efficiency.",
            "For example, you have a local algorithm which basically is expanding this small neighborhood around your query node.",
            "You hit a high degree node and your entire neighborhood size just blows up, so the other thing is that even though in real world graphs there aren't too many high degree nodes because of the power law degree distribution, these are very easily reachable because of the small world property.",
            "So the problem persists.",
            "And here is a very."
        ],
        [
            "Simple intuition of what we might do.",
            "Take this note.",
            "This has degree thousand at time step T there is probability must be on it in the next time step each of its neighbors are going to get about 1000 of that probability mass are very very tiny.",
            "Tiny amount of probability.",
            "Our intuition is why not just stop the random walk when it hits the high degree nodes essentially turned the high degree node into a single sink node, excuse me.",
            "So we keep all the incoming edges.",
            "We take all the outgoing edges and turn it into one self loop.",
            "Now how does that affect?"
        ],
        [
            "A personalized page rank.",
            "Let's say I'm computing personalized page rank from node I, and I turn node S into a sink node.",
            "Now, since I'm not letting some probability mass to get to Jay from, I buy by turning us into a sink.",
            "Clearly the PPV from I to J is going to decrease.",
            "The question is by how much we can prove.",
            "Let the contribution through S is the probability of hitting S from I in Alpha discounted walk times.",
            "The personalized page rank from S2, J and the first part is smaller than one because the probability the question is is the second part going to be small.",
            "If S has a huge degree, we can show that for undirected graphs that is essentially true, and in fact we can show in addition that the error at any node J in an undirected graph from the sink node S is going to be upper bounded by.",
            "The degree of J divided by degree of S and that's a good thing because if the sink node has a huge degree compared to node J, then all nodes are going to have small error.",
            "OK, so we also have results showing having relating set of sync nodes, but those are in the paper.",
            "So now let's look at hitting times."
        ],
        [
            "Sensually, but the effect should be roughly the same because again we have random walks and the way we show this is by establishing this really nice relationship between personalized page rank and discounted hitting time.",
            "And again are the main intuition that we will use is personalized page rank from Lodi to node.",
            "J is the hitting probability in Alpha discounted walk of going from I to J times personalized page rank from J2 itself.",
            "Now we again use our relationship between hitting probabilities and hitting times.",
            "And from that we show this relationship.",
            "What this means is hitting times are very similar to personalized page rank except the individual node popularity of J is normalized out and now that I have written hitting times in terms of personalized page rank, I know and I know that the effect of sync notes on personalized page rank is small.",
            "We can also prove that the effect on hitting times is going to be small from turning a high degree node into a sink.",
            "OK, so let's."
        ],
        [
            "Welcome to the second part of the talk.",
            "Discretion."
        ],
        [
            "Graphs the idea is very simple.",
            "We want to put similar nodes close by on disk so that we can quickly retrieve them and essentially we clustered the graph into page sized chunks and Now if the clusters are good quality than a random walk is going to mostly stay inside a cluster, leading to good computational efficiency.",
            "Here is a."
        ],
        [
            "Real example, so this is our cluster taken from the sites here Publication database database.",
            "This is 1/4 ship graph.",
            "All the green nodes are inside the cluster.",
            "All the blue or the dark colored nodes are from neighboring clusters.",
            "So let's zoom in.",
            "If we zoom in we will see that this cluster."
        ],
        [
            "Pretty much just.",
            "This soft robotics people and if we look at the periphery then we will see that there are a lot of machine learning and statistics people from the neighboring clusters.",
            "Now let's take one note from inside the cluster.",
            "We will take Sebastian Throne and we will rank the other nodes using personalized page rank from him.",
            "And here is the rank list.",
            "OK, so.",
            "And we will see that most of these Top Rank list is coming from within the cluster, so they're all robotics people.",
            "Except there are some machine learning in statistics.",
            "People as well from the neighboring clusters, so that kind of follows our intuition random of mostly stays inside a good cluster.",
            "Using that here."
        ],
        [
            "Is a very simple sampling scheme.",
            "We have a clustered graph.",
            "There is this red node from which I want to simulate my random walk.",
            "I load this cluster into memory and I start my random walk and every time the random walk hits a new cluster.",
            "I will basically say here is a page fault because I have to now load in another page or cluster.",
            "And we load in that cluster and we keep continuing.",
            "Now clearly we want to reduce the number of page faults, right and and number of page faults actually depends on how good quality their clusters are.",
            "If there are lots of cross edges, then I'm going to have more and more page faults.",
            "So, and we can also do things like use least recently used buffering scheme to maintain a set of clusters in memory and so on."
        ],
        [
            "So.",
            "We have a sampling scheme that's good because sampling is easy to implement.",
            "Most other most random walk based measures can be estimated using sampling, but the question is can we do any better than sampling to get even fewer number of page faults?",
            "And also how are we getting the clustered graph?",
            "OK, so the first for the first part.",
            "Let's try a deterministic algorithm here is this.",
            "The Gray blob is my entire graph.",
            "I want to compute personalized page rank to the red node Ji Loading JS cluster.",
            "Let's call that NB of J and one possibilities.",
            "I am only going to compute personalized page rank on that cluster, but that will lead to poor approximation unless that cluster is completely disconnected from the rest of the graph.",
            "Because you know, I'm not at all taking into consideration whatever is outside the cluster.",
            "Another effort is where we maintain upper and lower bounds on personalized page rank from nodes inside this cluster to J and now.",
            "Now when I want to."
        ],
        [
            "Expand this cluster around a boundary node and how to find that right boundary in order details that can be found in the paper.",
            "We basically now adding new clusters too, and as a result at any point of time we are maintaining a set of clusters.",
            "As we keep expanding our bounds keep getting tighter.",
            "We get better estimates also for all the nodes outside our current set of clusters, we maintain 1 upper bound to represent their personalized page rank.",
            "Two node JY because.",
            "We can stop when this upper bound falls below a small threshold beta, because then we know that everybody outside has very small personalized page rank and it's safe to stop.",
            "We will show in the experiment section that this has.",
            "This leads to a lot fewer number of page faults than vanilla sampling, and this is not just for computing hitting times.",
            "You can use this same flavor of an algorithm for personalized page rank.",
            "You can use the same flavor to compute hitting time too and OJ here I showed how to compute personalized Pagerank.",
            "OK, so now."
        ],
        [
            "Come to the second question, which is how do we get this clustered graph.",
            "So first we pick a measure.",
            "For clustering.",
            "We pick personalized Pagerank from a set of nodes because this has been shown in the theoretical community that this leads.",
            "This gives good local cuts around chosen seed node and our idea is that compute personalized page rank from a set of anchor nodes.",
            "And now once you have the values, assign every node to its closest anchor and each anchor basically defines one cluster.",
            "So the question is, how do you compute personalized page rank on disk?",
            "There has been other semi external memory algorithms for computing personalized page rank 2A set of nodes on disk and we're going to close the loop by showing how to compute personalized page rank from a set of nodes.",
            "The set a on disk.",
            "And note that our constraint is our nodes or edges do not fit into memory, so we don't have any Rand."
        ],
        [
            "Taxes.",
            "So that gives us our clustering algorithm will call that RW disc, and here is a very brief description of that algorithm.",
            "Essentially, we can compute personalized page rank using just pure power iterations.",
            "The idea there is, at any iteration you compute a probability distribution of a random walk starting from node I, and this distribution can be computed by just doing a matrix vector products.",
            "And how do we do that on disk?",
            "We can do that by doing joint type operation between 2 files.",
            "Both of which are lexicographically sorted now.",
            "The only problem with that is within a few steps I'm going to hit almost all nodes in the graph and my intermediate files are going to be huge in size.",
            "So what we do is we round the small probabilities to zero at any step and we show that this has bounded error and at the same time it brings down the intermediate file size in from quadratic in number of nodes to order number of edges so."
        ],
        [
            "OK, so now let's come to the results section."
        ],
        [
            "Our first part is going to be about the effect of sync nodes.",
            "The second part of the results are about how does deterministic algorithms do versus sampling.",
            "And finally, there is another bit which is about where we show that RW disc actually yields better clusters than matrix, which is an in memory clustering algorithm.",
            "While it requires a lot less memory and we won't have time to go into that bit of results.",
            "Please come by the poster to find out more."
        ],
        [
            "So here are our datasets.",
            "First we have the sites here sub graph.",
            "This is the smallest one.",
            "This is 1/4 ship graph of about 700,000 node taken from the sites here publication database and then we have the BLP.",
            "This is a paper award author graph from the entire DBL corpus has about 12 million edges and finally we have live Journal which is basically an online blogging community and it has about 90 million edges and this was the largest and the most dense.",
            "Graph we had.",
            "So first we will show that."
        ],
        [
            "Out of sync notes on.",
            "Link prediction accuracy and number of page faults or what exactly is link prediction accuracy?",
            "We take any node and now out of its direct neighbors we randomly pick a few of its neighbors.",
            "We hold out the links to those neighbors.",
            "Now we compute personalized Pagerank from this node in this clustered graph by simulating random walks, let's say, and the so the simulation of random walks gives us the number of page faults and once we have the personalized page rank, we rank all the nodes according to that.",
            "And we see if the held out neighbors comes up held out, neighbors come up in the top 10 or 20 rank neighbors.",
            "So that gives us accuracy now for accuracy higher the better for number of page faults lower the better.",
            "We also have in the second column the minimum degree of a sink node.",
            "So if the minimum degree is 1000 that means we turned all nodes with degree above 1000 into a sink.",
            "So the first part of the result is rather intuitive, so we show that by turning high degree nodes.",
            "Into sync nodes, we are decreasing number of page faults by a significant amount, and that's natural because often high degree nodes connect many different clusters together and they lead to a lot of page faults.",
            "Now.",
            "The second part is actually more interesting.",
            "Here we see that the link prediction accuracy actually significantly goes up.",
            "It increases by turning high degree nodes into sinks and our intuition behind this is often in our graphs.",
            "We had this high degree nodes which cause a lot of ambiguity, so since they're connecting all these different parts of the graph together, once a random walk hits that it gets lost in a random part of the graph, and by turning them into a sink we are helping the prediction algorithm.",
            "OK, so now."
        ],
        [
            "It's come to the effect of deterministic algorithm on page faults, and essentially we just show that for all these three different graphs by using a germanistik algorithm, we can actually reduce the number of page faults by quite a significant factor from just using vanilla sampling.",
            "And recall that in the deterministic algorithm you are being very careful about which clusters we want to bring into memory, and that's why it gives us much less page faults because we're being more careful and conservative.",
            "So in conclusion."
        ],
        [
            "We should treat three different things.",
            "First, we showed that turning high degree notes into sinks.",
            "It has bounded effect on personalized page rank and hitting time.",
            "We showed that this is a part of the results.",
            "We did not get time to talk about this actually significantly improves the clustering algorithm significantly improves the running time of it.",
            "It improves the number of page faults in any kind of.",
            "Search algorithm it also improves link prediction accuracy.",
            "We gave two search algorithms on a clustered external memory framework.",
            "The first one was sampling.",
            "This is easy to implement.",
            "You can compute many different measures just by doing sampling.",
            "We also gave a deterministic algorithm which is has nice completeness guarantees.",
            "Essentially, you are guaranteed not to miss potential nearest neighbor.",
            "And also we showed in the results section that it improves the number of page faults significantly.",
            "And finally we gave RW Disk which is a fully external memory algorithm for clustering the graph on disk and that's it, thank you."
        ],
        [
            "So.",
            "Yeah, sure.",
            "So the question is I when I the clustering algorithm that I talked about basically leads to disjoint clusters so one node can only belong to one cluster and there is this very interesting question that what happens when you have multiple overlapping clusters and so.",
            "We didn't do that, but my understanding is that it should because in our method we are also kind of for any node we maintain the set of key clusters that it's close is 2, so that information is there and you can use that information to actually have overlapping clusters I think.",
            "That's.",
            "It.",
            "Yeah, so far from what I can see.",
            "It's generalizable, but we can talk about it more to see the details here.",
            "Yeah, yeah.",
            "Exactly, yeah.",
            "So.",
            "So for them, I think they're there their main.",
            "Goal of that paper is to see if you have good quality clusters from computing.",
            "If you are computing personalized page rank from a seed node, you do a sweep over the notes and you get a cluster.",
            "They're not doing the high degree nodes, maybe because computational efficiency was.",
            "I don't know if they actually used it on graphs and so on, like real graphs for that paper, because that was a theoretical paper, but.",
            "I mean is that your question was it?",
            "Why did why they don't use high degree nodes or?",
            "I see, so I think personally that their method will actually be faster if you do this high degree note thing, because even even there I think the technique is something like you hit a high every time you see a node you bring in all its neighbors and you move the probability mass there and there.",
            "Also, once you have a very high degree nodes, you have to move the probability must all its neighbors and so it can be very dense.",
            "So that probably will be faster if we use this.",
            "So the first part was more like a tool that you can use to make it even faster, yeah?",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Poo Namrita soccer and I'm going to talk about fast nearest neighbor search and disk resident graphs.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Andrew Moore.",
                    "label": 0
                },
                {
                    "sent": "So what exactly is near?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Neighbor searching a graph given a node I I want to find which other nodes are most similar to this node.",
                    "label": 1
                },
                {
                    "sent": "I and this shows up in many different interesting practical problems.",
                    "label": 0
                },
                {
                    "sent": "For example, take the Facebook graph here suggesting a friend for a given node essentially boils down to finding other nodes which are most similar to that node.",
                    "label": 0
                },
                {
                    "sent": "Also in the context of DLP.",
                    "label": 0
                },
                {
                    "sent": "If I build a paper Ward author node graph from this DBL Publication database, their keyword specific search boils down to finding.",
                    "label": 0
                },
                {
                    "sent": "Finding papers which are most similar to the given set of keywords now for nearest neighbor search we clearly need some sort of notion of nearness or proximity or similarity, and for this talk we're going to focus on random walk based measures of approx.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, personalized page rank and hitting and commute Times Now these measures are intuitive measures of similarity.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if two nodes are connected via lots of shortcuts, they're going to score very high according to these measures and also they have been successfully used in a wide variety of applications in the DBL context the query type was fine.",
                    "label": 0
                },
                {
                    "sent": "We came most relevant papers about support vector machines so queries can arbitrary.",
                    "label": 1
                },
                {
                    "sent": "This would have been papers about ornaments for example.",
                    "label": 0
                },
                {
                    "sent": "And computing these measures at query time with your activity of research.",
                    "label": 0
                },
                {
                    "sent": "For this talk, we're going to focus.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And two computational issues associated with these these measures.",
                    "label": 0
                },
                {
                    "sent": "The first one is the nuisance of high degree nodes, and often we have algorithms which are local in nature, so we have an algorithm which examines a small neighborhood around the query node and tries to answer questions.",
                    "label": 1
                },
                {
                    "sent": "And the moment this neighborhood hits a high degree node, you have to bring in all its neighbors and the size blows up and everything becomes slow.",
                    "label": 0
                },
                {
                    "sent": "So that's one of the problems that we will try to address.",
                    "label": 0
                },
                {
                    "sent": "The second one is the case where the graph is so large that it does not fit into main memory.",
                    "label": 0
                },
                {
                    "sent": "And there are very interesting streaming algorithms, but even for them we have to make a few passes through the entire data set and that might not be desirable at query time.",
                    "label": 0
                },
                {
                    "sent": "Now these two different problems might seem completely independent, but we will actually show later in the experiment section.",
                    "label": 0
                },
                {
                    "sent": "That interesting solution to the first one actually makes the second solution much faster.",
                    "label": 0
                },
                {
                    "sent": "So what we want is external memory framework to represent the graph so that we can one support arbitrary queries and two we can compute many different random walk based measures on it.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that brings us to the outline of this talk.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to introduce these different random log based measures and then we will.",
                    "label": 0
                },
                {
                    "sent": "Address the two different issues that I talked about and finally we will give results.",
                    "label": 0
                },
                {
                    "sent": "So coming to proximity measures random walk based proximity measures, let's first talk about personal.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pagerank.",
                    "label": 0
                },
                {
                    "sent": "We will also call these PPV.",
                    "label": 0
                },
                {
                    "sent": "So the idea is simple.",
                    "label": 0
                },
                {
                    "sent": "You start a random walk at no die and now at any step you restart this random walk with probability Alpha from the start mode I and the stationary distribution of this stochastic processes.",
                    "label": 1
                },
                {
                    "sent": "The personalized page rank with respect to know die.",
                    "label": 0
                },
                {
                    "sent": "So now if there is a node J which has lots of short paths from node I that's going to have a very large personalized Pagerank value.",
                    "label": 0
                },
                {
                    "sent": "Essentially that's similar to I.",
                    "label": 0
                },
                {
                    "sent": "Another measure is the discounted version of hitting times.",
                    "label": 0
                },
                {
                    "sent": "And here again, you start a random walk from Lodi and at any step you either stop if you have hit node J or you stop with probability Alpha and Now the expected time to stop gives you the discounted hitting time from node.",
                    "label": 1
                },
                {
                    "sent": "I to know Jan if this is small then two nodes are similar.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's come.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The high degree nodes we will first talk about its effect on personalized page rank and then its effect on discounted hitting times so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said before, high degree nodes can often cause problems for computational efficiency.",
                    "label": 1
                },
                {
                    "sent": "For example, you have a local algorithm which basically is expanding this small neighborhood around your query node.",
                    "label": 0
                },
                {
                    "sent": "You hit a high degree node and your entire neighborhood size just blows up, so the other thing is that even though in real world graphs there aren't too many high degree nodes because of the power law degree distribution, these are very easily reachable because of the small world property.",
                    "label": 1
                },
                {
                    "sent": "So the problem persists.",
                    "label": 0
                },
                {
                    "sent": "And here is a very.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple intuition of what we might do.",
                    "label": 0
                },
                {
                    "sent": "Take this note.",
                    "label": 0
                },
                {
                    "sent": "This has degree thousand at time step T there is probability must be on it in the next time step each of its neighbors are going to get about 1000 of that probability mass are very very tiny.",
                    "label": 0
                },
                {
                    "sent": "Tiny amount of probability.",
                    "label": 0
                },
                {
                    "sent": "Our intuition is why not just stop the random walk when it hits the high degree nodes essentially turned the high degree node into a single sink node, excuse me.",
                    "label": 1
                },
                {
                    "sent": "So we keep all the incoming edges.",
                    "label": 0
                },
                {
                    "sent": "We take all the outgoing edges and turn it into one self loop.",
                    "label": 0
                },
                {
                    "sent": "Now how does that affect?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm computing personalized page rank from node I, and I turn node S into a sink node.",
                    "label": 0
                },
                {
                    "sent": "Now, since I'm not letting some probability mass to get to Jay from, I buy by turning us into a sink.",
                    "label": 0
                },
                {
                    "sent": "Clearly the PPV from I to J is going to decrease.",
                    "label": 0
                },
                {
                    "sent": "The question is by how much we can prove.",
                    "label": 0
                },
                {
                    "sent": "Let the contribution through S is the probability of hitting S from I in Alpha discounted walk times.",
                    "label": 1
                },
                {
                    "sent": "The personalized page rank from S2, J and the first part is smaller than one because the probability the question is is the second part going to be small.",
                    "label": 1
                },
                {
                    "sent": "If S has a huge degree, we can show that for undirected graphs that is essentially true, and in fact we can show in addition that the error at any node J in an undirected graph from the sink node S is going to be upper bounded by.",
                    "label": 0
                },
                {
                    "sent": "The degree of J divided by degree of S and that's a good thing because if the sink node has a huge degree compared to node J, then all nodes are going to have small error.",
                    "label": 0
                },
                {
                    "sent": "OK, so we also have results showing having relating set of sync nodes, but those are in the paper.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at hitting times.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sensually, but the effect should be roughly the same because again we have random walks and the way we show this is by establishing this really nice relationship between personalized page rank and discounted hitting time.",
                    "label": 0
                },
                {
                    "sent": "And again are the main intuition that we will use is personalized page rank from Lodi to node.",
                    "label": 1
                },
                {
                    "sent": "J is the hitting probability in Alpha discounted walk of going from I to J times personalized page rank from J2 itself.",
                    "label": 0
                },
                {
                    "sent": "Now we again use our relationship between hitting probabilities and hitting times.",
                    "label": 0
                },
                {
                    "sent": "And from that we show this relationship.",
                    "label": 1
                },
                {
                    "sent": "What this means is hitting times are very similar to personalized page rank except the individual node popularity of J is normalized out and now that I have written hitting times in terms of personalized page rank, I know and I know that the effect of sync notes on personalized page rank is small.",
                    "label": 1
                },
                {
                    "sent": "We can also prove that the effect on hitting times is going to be small from turning a high degree node into a sink.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Welcome to the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Discretion.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graphs the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "We want to put similar nodes close by on disk so that we can quickly retrieve them and essentially we clustered the graph into page sized chunks and Now if the clusters are good quality than a random walk is going to mostly stay inside a cluster, leading to good computational efficiency.",
                    "label": 1
                },
                {
                    "sent": "Here is a.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Real example, so this is our cluster taken from the sites here Publication database database.",
                    "label": 0
                },
                {
                    "sent": "This is 1/4 ship graph.",
                    "label": 0
                },
                {
                    "sent": "All the green nodes are inside the cluster.",
                    "label": 1
                },
                {
                    "sent": "All the blue or the dark colored nodes are from neighboring clusters.",
                    "label": 0
                },
                {
                    "sent": "So let's zoom in.",
                    "label": 0
                },
                {
                    "sent": "If we zoom in we will see that this cluster.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pretty much just.",
                    "label": 0
                },
                {
                    "sent": "This soft robotics people and if we look at the periphery then we will see that there are a lot of machine learning and statistics people from the neighboring clusters.",
                    "label": 1
                },
                {
                    "sent": "Now let's take one note from inside the cluster.",
                    "label": 1
                },
                {
                    "sent": "We will take Sebastian Throne and we will rank the other nodes using personalized page rank from him.",
                    "label": 0
                },
                {
                    "sent": "And here is the rank list.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And we will see that most of these Top Rank list is coming from within the cluster, so they're all robotics people.",
                    "label": 0
                },
                {
                    "sent": "Except there are some machine learning in statistics.",
                    "label": 0
                },
                {
                    "sent": "People as well from the neighboring clusters, so that kind of follows our intuition random of mostly stays inside a good cluster.",
                    "label": 0
                },
                {
                    "sent": "Using that here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a very simple sampling scheme.",
                    "label": 0
                },
                {
                    "sent": "We have a clustered graph.",
                    "label": 0
                },
                {
                    "sent": "There is this red node from which I want to simulate my random walk.",
                    "label": 0
                },
                {
                    "sent": "I load this cluster into memory and I start my random walk and every time the random walk hits a new cluster.",
                    "label": 1
                },
                {
                    "sent": "I will basically say here is a page fault because I have to now load in another page or cluster.",
                    "label": 0
                },
                {
                    "sent": "And we load in that cluster and we keep continuing.",
                    "label": 0
                },
                {
                    "sent": "Now clearly we want to reduce the number of page faults, right and and number of page faults actually depends on how good quality their clusters are.",
                    "label": 1
                },
                {
                    "sent": "If there are lots of cross edges, then I'm going to have more and more page faults.",
                    "label": 1
                },
                {
                    "sent": "So, and we can also do things like use least recently used buffering scheme to maintain a set of clusters in memory and so on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have a sampling scheme that's good because sampling is easy to implement.",
                    "label": 0
                },
                {
                    "sent": "Most other most random walk based measures can be estimated using sampling, but the question is can we do any better than sampling to get even fewer number of page faults?",
                    "label": 0
                },
                {
                    "sent": "And also how are we getting the clustered graph?",
                    "label": 0
                },
                {
                    "sent": "OK, so the first for the first part.",
                    "label": 0
                },
                {
                    "sent": "Let's try a deterministic algorithm here is this.",
                    "label": 0
                },
                {
                    "sent": "The Gray blob is my entire graph.",
                    "label": 0
                },
                {
                    "sent": "I want to compute personalized page rank to the red node Ji Loading JS cluster.",
                    "label": 0
                },
                {
                    "sent": "Let's call that NB of J and one possibilities.",
                    "label": 0
                },
                {
                    "sent": "I am only going to compute personalized page rank on that cluster, but that will lead to poor approximation unless that cluster is completely disconnected from the rest of the graph.",
                    "label": 1
                },
                {
                    "sent": "Because you know, I'm not at all taking into consideration whatever is outside the cluster.",
                    "label": 0
                },
                {
                    "sent": "Another effort is where we maintain upper and lower bounds on personalized page rank from nodes inside this cluster to J and now.",
                    "label": 0
                },
                {
                    "sent": "Now when I want to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expand this cluster around a boundary node and how to find that right boundary in order details that can be found in the paper.",
                    "label": 0
                },
                {
                    "sent": "We basically now adding new clusters too, and as a result at any point of time we are maintaining a set of clusters.",
                    "label": 1
                },
                {
                    "sent": "As we keep expanding our bounds keep getting tighter.",
                    "label": 1
                },
                {
                    "sent": "We get better estimates also for all the nodes outside our current set of clusters, we maintain 1 upper bound to represent their personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Two node JY because.",
                    "label": 0
                },
                {
                    "sent": "We can stop when this upper bound falls below a small threshold beta, because then we know that everybody outside has very small personalized page rank and it's safe to stop.",
                    "label": 1
                },
                {
                    "sent": "We will show in the experiment section that this has.",
                    "label": 0
                },
                {
                    "sent": "This leads to a lot fewer number of page faults than vanilla sampling, and this is not just for computing hitting times.",
                    "label": 0
                },
                {
                    "sent": "You can use this same flavor of an algorithm for personalized page rank.",
                    "label": 1
                },
                {
                    "sent": "You can use the same flavor to compute hitting time too and OJ here I showed how to compute personalized Pagerank.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come to the second question, which is how do we get this clustered graph.",
                    "label": 0
                },
                {
                    "sent": "So first we pick a measure.",
                    "label": 1
                },
                {
                    "sent": "For clustering.",
                    "label": 0
                },
                {
                    "sent": "We pick personalized Pagerank from a set of nodes because this has been shown in the theoretical community that this leads.",
                    "label": 1
                },
                {
                    "sent": "This gives good local cuts around chosen seed node and our idea is that compute personalized page rank from a set of anchor nodes.",
                    "label": 1
                },
                {
                    "sent": "And now once you have the values, assign every node to its closest anchor and each anchor basically defines one cluster.",
                    "label": 1
                },
                {
                    "sent": "So the question is, how do you compute personalized page rank on disk?",
                    "label": 0
                },
                {
                    "sent": "There has been other semi external memory algorithms for computing personalized page rank 2A set of nodes on disk and we're going to close the loop by showing how to compute personalized page rank from a set of nodes.",
                    "label": 0
                },
                {
                    "sent": "The set a on disk.",
                    "label": 0
                },
                {
                    "sent": "And note that our constraint is our nodes or edges do not fit into memory, so we don't have any Rand.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Taxes.",
                    "label": 0
                },
                {
                    "sent": "So that gives us our clustering algorithm will call that RW disc, and here is a very brief description of that algorithm.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we can compute personalized page rank using just pure power iterations.",
                    "label": 1
                },
                {
                    "sent": "The idea there is, at any iteration you compute a probability distribution of a random walk starting from node I, and this distribution can be computed by just doing a matrix vector products.",
                    "label": 0
                },
                {
                    "sent": "And how do we do that on disk?",
                    "label": 0
                },
                {
                    "sent": "We can do that by doing joint type operation between 2 files.",
                    "label": 1
                },
                {
                    "sent": "Both of which are lexicographically sorted now.",
                    "label": 0
                },
                {
                    "sent": "The only problem with that is within a few steps I'm going to hit almost all nodes in the graph and my intermediate files are going to be huge in size.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we round the small probabilities to zero at any step and we show that this has bounded error and at the same time it brings down the intermediate file size in from quadratic in number of nodes to order number of edges so.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let's come to the results section.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our first part is going to be about the effect of sync nodes.",
                    "label": 0
                },
                {
                    "sent": "The second part of the results are about how does deterministic algorithms do versus sampling.",
                    "label": 0
                },
                {
                    "sent": "And finally, there is another bit which is about where we show that RW disc actually yields better clusters than matrix, which is an in memory clustering algorithm.",
                    "label": 1
                },
                {
                    "sent": "While it requires a lot less memory and we won't have time to go into that bit of results.",
                    "label": 0
                },
                {
                    "sent": "Please come by the poster to find out more.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are our datasets.",
                    "label": 0
                },
                {
                    "sent": "First we have the sites here sub graph.",
                    "label": 0
                },
                {
                    "sent": "This is the smallest one.",
                    "label": 0
                },
                {
                    "sent": "This is 1/4 ship graph of about 700,000 node taken from the sites here publication database and then we have the BLP.",
                    "label": 0
                },
                {
                    "sent": "This is a paper award author graph from the entire DBL corpus has about 12 million edges and finally we have live Journal which is basically an online blogging community and it has about 90 million edges and this was the largest and the most dense.",
                    "label": 0
                },
                {
                    "sent": "Graph we had.",
                    "label": 0
                },
                {
                    "sent": "So first we will show that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out of sync notes on.",
                    "label": 0
                },
                {
                    "sent": "Link prediction accuracy and number of page faults or what exactly is link prediction accuracy?",
                    "label": 0
                },
                {
                    "sent": "We take any node and now out of its direct neighbors we randomly pick a few of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "We hold out the links to those neighbors.",
                    "label": 0
                },
                {
                    "sent": "Now we compute personalized Pagerank from this node in this clustered graph by simulating random walks, let's say, and the so the simulation of random walks gives us the number of page faults and once we have the personalized page rank, we rank all the nodes according to that.",
                    "label": 0
                },
                {
                    "sent": "And we see if the held out neighbors comes up held out, neighbors come up in the top 10 or 20 rank neighbors.",
                    "label": 0
                },
                {
                    "sent": "So that gives us accuracy now for accuracy higher the better for number of page faults lower the better.",
                    "label": 0
                },
                {
                    "sent": "We also have in the second column the minimum degree of a sink node.",
                    "label": 1
                },
                {
                    "sent": "So if the minimum degree is 1000 that means we turned all nodes with degree above 1000 into a sink.",
                    "label": 0
                },
                {
                    "sent": "So the first part of the result is rather intuitive, so we show that by turning high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "Into sync nodes, we are decreasing number of page faults by a significant amount, and that's natural because often high degree nodes connect many different clusters together and they lead to a lot of page faults.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The second part is actually more interesting.",
                    "label": 0
                },
                {
                    "sent": "Here we see that the link prediction accuracy actually significantly goes up.",
                    "label": 0
                },
                {
                    "sent": "It increases by turning high degree nodes into sinks and our intuition behind this is often in our graphs.",
                    "label": 0
                },
                {
                    "sent": "We had this high degree nodes which cause a lot of ambiguity, so since they're connecting all these different parts of the graph together, once a random walk hits that it gets lost in a random part of the graph, and by turning them into a sink we are helping the prediction algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's come to the effect of deterministic algorithm on page faults, and essentially we just show that for all these three different graphs by using a germanistik algorithm, we can actually reduce the number of page faults by quite a significant factor from just using vanilla sampling.",
                    "label": 0
                },
                {
                    "sent": "And recall that in the deterministic algorithm you are being very careful about which clusters we want to bring into memory, and that's why it gives us much less page faults because we're being more careful and conservative.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We should treat three different things.",
                    "label": 0
                },
                {
                    "sent": "First, we showed that turning high degree notes into sinks.",
                    "label": 1
                },
                {
                    "sent": "It has bounded effect on personalized page rank and hitting time.",
                    "label": 1
                },
                {
                    "sent": "We showed that this is a part of the results.",
                    "label": 1
                },
                {
                    "sent": "We did not get time to talk about this actually significantly improves the clustering algorithm significantly improves the running time of it.",
                    "label": 0
                },
                {
                    "sent": "It improves the number of page faults in any kind of.",
                    "label": 1
                },
                {
                    "sent": "Search algorithm it also improves link prediction accuracy.",
                    "label": 1
                },
                {
                    "sent": "We gave two search algorithms on a clustered external memory framework.",
                    "label": 1
                },
                {
                    "sent": "The first one was sampling.",
                    "label": 0
                },
                {
                    "sent": "This is easy to implement.",
                    "label": 0
                },
                {
                    "sent": "You can compute many different measures just by doing sampling.",
                    "label": 0
                },
                {
                    "sent": "We also gave a deterministic algorithm which is has nice completeness guarantees.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you are guaranteed not to miss potential nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "And also we showed in the results section that it improves the number of page faults significantly.",
                    "label": 0
                },
                {
                    "sent": "And finally we gave RW Disk which is a fully external memory algorithm for clustering the graph on disk and that's it, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "So the question is I when I the clustering algorithm that I talked about basically leads to disjoint clusters so one node can only belong to one cluster and there is this very interesting question that what happens when you have multiple overlapping clusters and so.",
                    "label": 0
                },
                {
                    "sent": "We didn't do that, but my understanding is that it should because in our method we are also kind of for any node we maintain the set of key clusters that it's close is 2, so that information is there and you can use that information to actually have overlapping clusters I think.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so far from what I can see.",
                    "label": 0
                },
                {
                    "sent": "It's generalizable, but we can talk about it more to see the details here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Exactly, yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So for them, I think they're there their main.",
                    "label": 0
                },
                {
                    "sent": "Goal of that paper is to see if you have good quality clusters from computing.",
                    "label": 0
                },
                {
                    "sent": "If you are computing personalized page rank from a seed node, you do a sweep over the notes and you get a cluster.",
                    "label": 0
                },
                {
                    "sent": "They're not doing the high degree nodes, maybe because computational efficiency was.",
                    "label": 0
                },
                {
                    "sent": "I don't know if they actually used it on graphs and so on, like real graphs for that paper, because that was a theoretical paper, but.",
                    "label": 0
                },
                {
                    "sent": "I mean is that your question was it?",
                    "label": 0
                },
                {
                    "sent": "Why did why they don't use high degree nodes or?",
                    "label": 0
                },
                {
                    "sent": "I see, so I think personally that their method will actually be faster if you do this high degree note thing, because even even there I think the technique is something like you hit a high every time you see a node you bring in all its neighbors and you move the probability mass there and there.",
                    "label": 0
                },
                {
                    "sent": "Also, once you have a very high degree nodes, you have to move the probability must all its neighbors and so it can be very dense.",
                    "label": 0
                },
                {
                    "sent": "So that probably will be faster if we use this.",
                    "label": 0
                },
                {
                    "sent": "So the first part was more like a tool that you can use to make it even faster, yeah?",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}