{
    "id": "csftezynolasgoaxe3zeowr2klbg2vyn",
    "title": "A polynomial algorithm for the inference of context free languages",
    "info": {
        "author": [
            "Alexander Clark, Royal Holloway, University of London"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/icgi08_clark_pai/",
    "segmentation": [
        [
            "OK, so the second talk with this session given by Alexander Clark on the polynomial algorithm for the inference of the tree graph languages.",
            "Yeah, so this is joint work with Grammy and memory and some of this work was done while I was visiting lift in Marseille, so I'd like to thank Francois underneath."
        ],
        [
            "Tasting me.",
            "So summary, this is a talk about distribution of learning algorithm.",
            "Which I explain what I mean by that later.",
            "It is positive, unstructured data and membership queries, so it's a fairly traditional protocol.",
            "It sufficient in the sense that it uses polynomial update time.",
            "Ann is correct for a large class of context free languages that includes all regular languages.",
            "And we get this result by essentially switching to a more general formalism by using a context sensitive formalism.",
            "And the result is the algorithm is essentially trivial, so there's a certain amount of technical detail.",
            "In fact, angle detail in the paper.",
            "In fact, the paper is almost exclusively technical detail.",
            "What I'm going to do in this talk is to give a very high level overview of the main ideas and principles of the proof.",
            "Try to explain why this how we get this result and also it works in practice, so it works on toy examples and I'm doing some large scale."
        ],
        [
            "Parents amount.",
            "So why well, my research goal is a fairly classic grammatical inference, while I'm interested in studying natural language in particular, the interest in first language acquisition.",
            "So really, what I view what I'm doing is being essentially almost theoretical syntax.",
            "Positive data from membership queries are not that close to what the situation is.",
            "A child learner finds herself in him or herself in, but we can consider it as being a placeholder for a more realistic probabilistic learning paradigm.",
            "If we're going to do this, then we need to have quite expressive models.",
            "Natural languages are very, very complex.",
            "They have gender phenomena, case.",
            "They have a quite complex, and they're also mildly context sensitive.",
            "So to make progress in this, we need to accept the fact that we're going to need to use.",
            "Quite rich models."
        ],
        [
            "Now, in normal GI, whatever that means.",
            "Typically we given a class of representations and we studied the learnability of these representations under various assumptions.",
            "So I think this is the this is."
        ],
        [
            "Wrong way round really because in first language acquisition we don't know what the representations are, but we do know that they are learnable.",
            "So linguists sometimes talk as though they know the representations are, but they don't.",
            "OK, but we do know that that'll be cause the infant child can learn.",
            "So the research strategy I've been falling for the last few years is really to look for representations that are intrinsically learnable.",
            "By that I mean.",
            "I think the real problems are computational and not information theoretic, so the real problem is not the absence of negative data or VC dimensions or things like this.",
            "The real problems are the computational complexity of finding an appropriate hypothesis.",
            "And to do this, I think we need to use representations without hidden structure.",
            "So I mean by this that we must.",
            "Representational primitives must be observable.",
            "In fact, we must be able to define any representational primitive of representation.",
            "Any symbol nonterminal state has to be defined, able in purely language theoretic terms.",
            "So rather than having really function from our representations into languages, we have a function from our languages into our representations.",
            "And we can do this in natural language because we can pick our representation."
        ],
        [
            "So.",
            "Typically we proceed by looking the relationship between various substrings.",
            "So when we're learning regular languages were interested generating the prefix or suffix prefix suffix relation, so we're interested in the relationship between two strings you envy where they're related.",
            "The prefix use rated the suffix the if their concatenation is in the language, and using this we then look at this relation.",
            "We look at the.",
            "Either the right congruence classes or residual languages which are just defined in this way and the tradition.",
            "Normally the inference of regular languages using deterministic finite state automotor.",
            "Proceeds by essentially defining the states of our Canonical DFA to be precisely these to be precisely these residual languages, and that's one of the great success stories of grammatical inference.",
            "So I think we can learn a lot from this success story by recognizing the states these supposedly hidden hard to find states are in fact observable, and they can be defined in language theoretic terms.",
            "Now when we move on."
        ],
        [
            "From regular languages to context, free languages name just define what context is.",
            "Context is just a pair of strings, LR and we can combine a context with a substring here just by wrapping the context around the subject to get this.",
            "So when we look at context free languages, what we're interested in really is relationship between a context on a substring.",
            "So we define this relation here as being the context LR is related to you.",
            "If when we wrap EU with the LR.",
            "The resulting thing is on the language now closely to the to the residual languages.",
            "Alright conferences?",
            "Here we have the the set of context of a string here, which is sometimes in linguistics is called the distribution of the substring, so a classic term from from 1940s and 1950s or structural linguistics.",
            "So this is just the set of all context that particular string has an important thing to realize is that we can observe this right if we just have a lot of data.",
            "We can just.",
            "We will just observe certain numbers, so we observe not all of it, but some subset of this policy of you."
        ],
        [
            "So again, what do I mean by distributional learning?",
            "Well, in normal GI, when we have given a string, we want to predict if a particular string in the language OK, which we think is being function from the set of all strings into."
        ],
        [
            "One in this regional I'm going to consider slightly richer model here.",
            "We want to model this function C want to model for any string U we want to predict the set of contexts of that string included.",
            "This is more general becausw the here the land and being mean.",
            "The empty string.",
            "Here if Lambda Lambda is in the context set of you and that's just the same as you being anything.",
            "So this is just slightly more general way of looking at things."
        ],
        [
            "Now there are two problems with modeling.",
            "See if you the first is obviously that the number of strings you that we're considering, which is Signal Star is infinite.",
            "So the way we want to deal with that is we're going to take some finite set of primitive elements which are going to be strings K such that we know about.",
            "See of you for these finite element.",
            "So if U as in K then we know what see of you is like.",
            "OK, so we have these finite set of parameter elements and then the second thing we need is some way of computing.",
            "See of UV from its part C of you and sievi.",
            "OK, so if we have these primitive elements and we have some way of combining these primitive elements then this will give allows to define it for the whole set."
        ],
        [
            "And the second problem is that see of you may be infinite.",
            "Invite will often be infinite if L is infinite.",
            "So we need some finite representation of C."
        ],
        [
            "So one solution.",
            "Which is the work idea with Grammy and also that Michael your snack was talking about is that is really based as I view it on this insight here that if see of you is equal to see of you primed and see a visit to your V prime and then see if UV is equal to see you prime V prime.",
            "That's pretty obvious and using this we define these syntactic congruence which is just two things being congruent if their contexts are equal and we write the equivalence class for this finite representation.",
            "Vitamins here are just the congruence classes of the observed strings.",
            "And then we can essentially write down a representation using these simple combinatoric rules.",
            "OK, so these basic rules give us at context free grammar in Chomsky normal form.",
            "OK, so if you can tell where the Seaview is equal to see of the, then learning is essentially trivial.",
            "OK, so it just drops out."
        ],
        [
            "But there are problems with this.",
            "The major problem really is that we're modeling.",
            "See of you is just being some finite unstructured set of congruence classes.",
            "And in real language and natural language is really No 2 words exactly like if 2 words were exactly like you wouldn't need to have two words basically.",
            "And there are lots and lots of conference classes, so that means that the two things it means that learning is very slow because you have to require a very large number of these congruence classes and also means it's hard to get it right because you have things that are very similar, so you're likely to make mistakes.",
            "The important thing to realize is that rather than representing sort of finite unstructured set of Congress classes, we need to represent in some way the algebraic structure of this set."
        ],
        [
            "See if you and.",
            "The thing to realize is the congruence classes are sets of context.",
            "Ultimately there sets, so the appropriate algebraic structure to look at is a lattice, and the first person I think you realize this was a Frenchman called Sestiere in 1960.",
            "I think this is the only paper he wrote, so it was a good run anyway.",
            "100% of his papers were good, but you can't save many people, so give me example here.",
            "If we have just using a regular context free grammar here so you have some non terminal end which derives N&V and we have some.",
            "Other non term, the right M and also derives V. Then the distribution of V is going to be the union of the distributions of M&M.",
            "OK, so here we see that in fact we see this operation.",
            "This sort of algebraic operation among C that is essential lattice operation."
        ],
        [
            "So here's an example of lattice for, you know one of the our old friends at the NBA team.",
            "Beat the end.",
            "So this is lattice is just meant to represent a fragment of an infinite lattice.",
            "What we have here is at the top.",
            "Here we have the set of all strings, and here we have the set of no strings.",
            "Now what is interesting here?",
            "Here we have the set of all strings are in the language.",
            "And what's interesting here is that if we look at.",
            "These two things survey and see if ABC of NCAABR or similar right?",
            "They have alot of things in common, but actually this the context of a include all the context of AAB and they strictly included cause this context, a Lambda ABB is clear context of a, but it's not a context of of a be OK.",
            "So here we have some.",
            "Even in this really really trivial language we have an example of where actually modeling this lattice structure is helpful.",
            "An ATM BCN is not a substitutable language.",
            "OK so if we want to allow this work was driven just by trying to say well we want to extend.",
            "Method of learning such useful languages to learn this really, really trivial thing.",
            "So in order to learn this properly, we want to model this lattice.",
            "We need to accept the fact we need to represent in some way the fact that these two things here are not completely different, though related."
        ],
        [
            "And if we look at natural language here, here is just an example of some English pronouns here, so.",
            "What we see is we see the various relations between these things.",
            "Here we have you below Wepay and up here we have more abstract categories here.",
            "So this for example.",
            "Here is a set of all pronouns that can occur in subject position, so we naturally get these sorts of hierarchies when we look at natural."
        ],
        [
            "Which phenomenon, in fact, it turns out you get many, many comments cause you do get extra extension congruence classes when you deal with natural."
        ],
        [
            "So how do we need to represent the lattice of distributions?",
            "Well, the simple thing to do is just to take a finite set of context F angusta model CFW intersecting with F. So this gives us 2 to the power of F congruence classes.",
            "So if we have F, here we have arisen expressive formalism that we can represent an exponential number or common causes and.",
            "The match with the underlying data structure isn't perfect, but ultimately that doesn't really matter."
        ],
        [
            "So what's the representation?",
            "Well, what we want to do is if we can learn this lattice structure, we can also define a representation that directly uses this lattice structure.",
            "OK, So what we want to do here is the base of this is.",
            "Is this similar to what we were talking about before?",
            "Is that if see of you if you include the context of you prime the company, include the context of the primes, then the context of UV includes the context of viewpoint viewpoint.",
            "So here we have this sort of recursive decomposition that.",
            "Respects the lattice structure because we have these this partial order that we get from lattice and and."
        ],
        [
            "Other way of writing this?",
            "Is just this where we sent you a converting before any is into unions.",
            "So this is just this.",
            "This agent says exactly the same.",
            "It says the context W no matter how we split W into U&V and over any U primes that match it is going to contain Seaview Prime V Prime.",
            "So this is just exactly the same way of saying this."
        ],
        [
            "Less clear.",
            "Here's a way that's probably more clear right, which is that basically when we take.",
            "If we if we have some string UV and we want to find out what it's context is like, we can split into UV.",
            "You can spend to you envy.",
            "We can look, try and find some you primed and some V prime there.",
            "Above it we stick EU prime V prime together to get you primed, primed and then UV will be below that.",
            "OK so this gives us a recursive way of computing EUV in terms of you kind of."
        ],
        [
            "Ground.",
            "Now slightly more complicated examples this.",
            "Let's say we have UVW or.",
            "Clearly we can split this in two different ways.",
            "We can either spend to UV&W, in which case we know this is going to be below XY, or you can split into U&VW, in which case we know it's going to be able to pick you.",
            "OK, so this actually takes us.",
            "Out of the context free formalism right?",
            "In order to represent this fact, the fact that we have constraints that go across different derivations, we need to go outside the context free form as it took me very long time to."
        ],
        [
            "The last letter.",
            "So the formula that we use is contextual binary feature grammars.",
            "In the paper we we, we present binary feature grammars and then talk about contextual binary feature grammar.",
            "Here I'm squishing them together for ease of exposition.",
            "Broadly speaking, we have a 2 pole here where we have F is all set of features, which is a set of contexts.",
            "FS is the sentence feature which is just this trivial feature.",
            "Lambda Lambda, which is the one we want to.",
            "We have to be able to predict to be able to look at the language membership.",
            "We have two sorts of productions we have next production risks to a where a is a letter.",
            "And X is a set of these features of certain contexts, and we have the normal sort of productions and what these normal productions here X goes to ways that mean really is that if you has features Y.",
            "And V has features that then the concatenation of you and he is going to have the features in X, right?",
            "So really the arrow here goes the wrong way, right?",
            "the Irish is really saying we can take a wireless head together and we can combine them and when we combine them we're going to get next.",
            "So what we're going to do is we're going to define a recursive map for all of our strings here in from single star into sets of sets of contexts, and we want this map here to approximate the true context distribution."
        ],
        [
            "So we just do this in the the the way you expect.",
            "We start off with the empty thing.",
            "We introduced the lexical things and then we just take the Union over all applicable rules."
        ],
        [
            "And this turns out to be.",
            "You can do this efficiently using dynamic programming.",
            "It's just it's basically just the same as a CKY parsing algorithm, which you can do in cubic time."
        ],
        [
            "So BF jeez, that's ignored, dropping the contextual thing.",
            "Just considering BF, Jesus being where the features can be anything or a very powerful formalism.",
            "They include all context free grammars and they can represent non context free languages.",
            "They can also compactly represent languages that require exponentially large context free grammars like permutation languages and their equivalent to some classes subclass of range concatenation grammars, which is a fairly powerful formalism introduced by.",
            "Abulia"
        ],
        [
            "A few years ago now, assuming we have a finite set of strings K, these are little primitives and we have a set of features which are Contacts and a membership Oracle to answer all the questions.",
            "Then essentially you can write down a grammar.",
            "What we do is we just if if we have a UV and UV being K and see of you and see the combined to Form C of UV.",
            "Then we just add a production like this saying see of you and see if the combined to form C of UV an simile for every letter A.",
            "In Sigma we add a lexical rule.",
            "It just says.",
            "Basically A has the features that it has, right?",
            "It's a sort of vacuous rule."
        ],
        [
            "Now, so this means ready that I'm given an Oracle for the language and a choice McCain if we can write down to grandma G of kno left so ger hypothesis then is a function of K&F.",
            "And so natural question is really is it easy to find the right care net?",
            "Or maybe we've just swapped one problem for another."
        ],
        [
            "In fact, it turns out to be easy for these two reasons.",
            "First of all.",
            "It turns out that as K increases, the language monotonically increases.",
            "This is a fairly obvious, I think, that as you increase K, the set of productions increases and as a set of production increases, a set of features you predict will increase, so that's values.",
            "That's fairly straightforward.",
            "So if K plus contains K, then the language defined by GK plus LF contains this."
        ],
        [
            "Maybe it's not quite so obvious.",
            "Is that as F increases, the language monotonically decreases.",
            "OK, so if we add features.",
            "So if F plus includes F, then the language defined by this actually decreases OK. Now the reason for this is that the features really define the conditions under which we predict a feature on the head there, like if then rules.",
            "So if you had why envy has features there, then we predict that UV has some features.",
            "So if we increase the number of features were actually making this conditions stronger, which means we're going to reduce the set of features we predict for longer strings.",
            "OK, so the features in a way represent conditions on or in the grammar."
        ],
        [
            "So what does this mean?",
            "Well, this is a simple example here.",
            "This is a Dyke language here, So what we're doing here is we're showing along the X axis.",
            "Here we have.",
            "Strings, which we're just taking in the order of frequency their car in.",
            "In a random sample and up here we have features that again just the context that we observe in order of their current.",
            "Now we can see here is that as we as we increase F as we go up here as we go in this direction, the error which is the overgeneralize error over generalization error decreases.",
            "So here red is.",
            "This is an error measure, red is bad, white is very very good.",
            "So wherever we are here, if we go up we're going error is reducing.",
            "OK."
        ],
        [
            "And Conversely, if we look at the under generalization error, OK, as we go right here as we increase K again, the under generalization error decreases.",
            "OK, so as we're going from right, we reduce one sort of error as we go up, we reduce the other.",
            "So the obvious algorithm here is, broadly speaking, to go diagonally."
        ],
        [
            "Which is what we do.",
            "So the algorithm is and the algorithm we put in the paper is really just.",
            "The simplest possible argument is correct, so and is polynomial deficient, so we're not really trying to make.",
            "We're not trying to claim that economy is very scalable, it's it's naive.",
            "It's inefficient, but it's nonetheless polynomial incorrect.",
            "So the basic idea is that if the language under generates, then you add some more examples to your kernel, and if it over generates then you add some more features.",
            "You had some context, so if you're under generate, you go right.",
            "If it over generates, you go up."
        ],
        [
            "So more formally, documents, we take a sequence of strings.",
            "Our data set is just all the strings you observed so far.",
            "Then we have the set of all context that we've observed in our data, and the set of all substrings that we've observed in the data, and we combine them every which way.",
            "And this gives us a test set which is very similar to something that Peter Adriaens used in one of his papers.",
            "I think you got the 1st order explosion, which is probably better name than mine, but.",
            "So here we have when we go through all these examples in the test set.",
            "So if W is in the language but it's not in the current hypothesis, then we're under generalizing.",
            "So we just add some strings to K and we add a bunch of we had all of our context F. If we're overgeneralizing.",
            "Then we just add the can't."
        ],
        [
            "So this means by the two monotonic monotonicity Lammers, right?",
            "We're going to be going in the right direction, but are we actually going to converge, or you're going to get to the right place?",
            "So there are two complementary properties here.",
            "One is the finite context property, which means we're never going to over generalize, and the other is the finite kernel property, which means there were never going to generalize this.",
            "This one is not so important.",
            "The important one here is this finite context property, which I'll talk about."
        ],
        [
            "So here's a diagram just to give a big picture here.",
            "So broadly speaking here, we have a K along the bottom and F along the thing.",
            "What we'll see is that essentially when we have when we see enough of a sample here, this is a note here.",
            "Once we get the right of this line, we're only we're never going to make any under generation errors.",
            "OK, so the left of here we may make under generation errors, we get to the right of this line is going to be.",
            "It's not going to generate enough, and so we're going to have a line here, which is goes up, which is a line tells us how many features we need.",
            "So when we are below this line, we may make over generalization.",
            "Once we get above, this line is going to be correct, so we're going to do is we want to get into this wedge in the top right hand corner."
        ],
        [
            "So the finite context property is this.",
            "Thing is that this is the basic property which defines what is learnable learnable within this framework.",
            "And this says we have a string you in a language has a finite context property.",
            "If there's a finite set of contexts.",
            "So a finite subset of its distribution, such that if we have any string that contains that has those that finite set of contexts, then it has all of the other contexts as well.",
            "OK, So what we're saying is if you get, you can define a finite set and whenever you get that if you have this finite set of examples then you get everything.",
            "And this is kind of the inductively.",
            "All all algorithms have to have this somewhere so it can be pretty well hidden.",
            "Basically, this is the way we go from a finite set of evidence F of you about the sort of context to an infinite set."
        ],
        [
            "And if you compare the substitutable languages, substitute languages are one where any string.",
            "In see of you is enough for this inductive leap, so substitutability just says if you pick any stringency of you, then if C of V contains that string, then Cav is equal to see of you.",
            "And if you compare Adrian's idea of context, separability is saying basically there is one string in here or saying that there is a finite set."
        ],
        [
            "So.",
            "More generally, you just say a set of features is fiducial for K. If anyone came for any V subset, CSV contains.",
            "These that set of features then survey contains uses just the way we essentially rollout this.",
            "This definition for two finite set.",
            "So the key point here is really that F needs to be a function of K. As K increases, we need more features and that's that diagonal line that we saw earlier and the key lemma in the proof really that joins everything together says that if F is fiducial then the BFG predicts only correct features.",
            "I this FG of W. Contains this and that uses several different things, but essentially."
        ],
        [
            "It's trivial.",
            "So the scope of the finite context principle.",
            "Well, all regular languages have the this finite context principle, since the number of syntactic congruence classes finite, all substitutable languages have the finite context principle.",
            "For some non context free languages, have the finite context property.",
            "So here for example the mixed language has put as a finance context property but there are context free languages that do not have their SCP.",
            "So for example, this is the simplest example I could come up with.",
            "A language doesn't have the.",
            "Doesn't have this property and it's it's.",
            "It's an entree.",
            "I mean it.",
            "It took me awhile.",
            "Took me awhile to come up with it.",
            "It's this sort of phenomenon where you have essentially an infinite infinite sequence of strings.",
            "Influence and limit point.",
            "It's not something that actually seems to occur in natural languages at all, so I conjecture that natural languages may have the finite context property."
        ],
        [
            "The finite kernel of property just basically says it is equivalent to saying it has a finite representation.",
            "So what we define a language is being a finite set is a kernel for languages for any set of features F it has.",
            "It satisfies this property, so when we have a finite kernel, then eventually as we add kernels, eventually we're going to get a large enough hypothesis or regular languages have finite kernels.",
            "There are context free languages that don't have a finite kernel.",
            "In the way we in the way we define this, but we expect in future work to be able to weaken this requirement to include or context free languages.",
            "So there's a more rapid way of learning these things that remove this account, but the FBP is."
        ],
        [
            "The really important limitation.",
            "So the main result is that the algorithm described identifies and limit the class of languages which satisfy the FCP and the FKP.",
            "It uses polynomial update time.",
            "Uses positive data plus polynomial calls.",
            "A polynomial number of calls to the membership Oracle.",
            "It includes all regular languages, disjoint Panda languages like language, mostly standard examples that we've looked at."
        ],
        [
            "So future work.",
            "The thing that's most important here is is to use rapid generalization.",
            "As we said that the algorithm.",
            "Essentially learns the congruence classes one by one, so it can't generalize to learning more abstract rules.",
            "There's a way of doing that, essentially where you, when you're predicting a feature there rather than taking particular sets of features, you intersect them again, so you look in the.",
            "The sort of semi lattice defined by the examples you've seen to produce more abstract rules.",
            "This, I think, will give us a more more truly polynomial result and also takes us out of context out of the context.",
            "Free class into context sensitive languages.",
            "Tried to get a pack result.",
            "Features that are set in context and look at natural language.",
            "Natural language experiments."
        ],
        [
            "So conclusions.",
            "So we've distributional learning, which normally looks at looking individual congruence classes, can be generalized to model the lattice of context substring occurrences.",
            "So this requires.",
            "I mean, it forces a switch to a context sensitive representation that's directly based on the lattice, and one of the one of the consequences, which is you slightly abandoned.",
            "The traditional ideas of constituent structure.",
            "It gives rise to efficient algorithms for learning context free and potentially context sensitive languages that have this fairly weak property, which is the finite context property, and this is a linguistically I think, very interesting for.",
            "For several reasons.",
            "First of all, natural languages, as far as I can tell, do seem to have this finite context property.",
            "Secondly, match languages are mildly context sensitive, and finally they do seem to have the sort of particular sort of richly structured nature that these lattices produce.",
            "That's awful.",
            "I I guess.",
            "Um?",
            "Well, I said Paul name update time is what I've formally demonstrated here.",
            "I also think of it really is being a practical issue, just not just theoretical worst case, but actually that you can use these techniques too.",
            "Albums that can run on large amounts of data, so I'm interested in theories that guide to practice, not theory for itself, and so so efficient means.",
            "Efficient upside down, but also efficient.",
            "Ultimately in the amount of data that you need to use."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the second talk with this session given by Alexander Clark on the polynomial algorithm for the inference of the tree graph languages.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is joint work with Grammy and memory and some of this work was done while I was visiting lift in Marseille, so I'd like to thank Francois underneath.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tasting me.",
                    "label": 0
                },
                {
                    "sent": "So summary, this is a talk about distribution of learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which I explain what I mean by that later.",
                    "label": 0
                },
                {
                    "sent": "It is positive, unstructured data and membership queries, so it's a fairly traditional protocol.",
                    "label": 1
                },
                {
                    "sent": "It sufficient in the sense that it uses polynomial update time.",
                    "label": 0
                },
                {
                    "sent": "Ann is correct for a large class of context free languages that includes all regular languages.",
                    "label": 1
                },
                {
                    "sent": "And we get this result by essentially switching to a more general formalism by using a context sensitive formalism.",
                    "label": 0
                },
                {
                    "sent": "And the result is the algorithm is essentially trivial, so there's a certain amount of technical detail.",
                    "label": 0
                },
                {
                    "sent": "In fact, angle detail in the paper.",
                    "label": 0
                },
                {
                    "sent": "In fact, the paper is almost exclusively technical detail.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do in this talk is to give a very high level overview of the main ideas and principles of the proof.",
                    "label": 0
                },
                {
                    "sent": "Try to explain why this how we get this result and also it works in practice, so it works on toy examples and I'm doing some large scale.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parents amount.",
                    "label": 0
                },
                {
                    "sent": "So why well, my research goal is a fairly classic grammatical inference, while I'm interested in studying natural language in particular, the interest in first language acquisition.",
                    "label": 0
                },
                {
                    "sent": "So really, what I view what I'm doing is being essentially almost theoretical syntax.",
                    "label": 0
                },
                {
                    "sent": "Positive data from membership queries are not that close to what the situation is.",
                    "label": 0
                },
                {
                    "sent": "A child learner finds herself in him or herself in, but we can consider it as being a placeholder for a more realistic probabilistic learning paradigm.",
                    "label": 1
                },
                {
                    "sent": "If we're going to do this, then we need to have quite expressive models.",
                    "label": 1
                },
                {
                    "sent": "Natural languages are very, very complex.",
                    "label": 0
                },
                {
                    "sent": "They have gender phenomena, case.",
                    "label": 1
                },
                {
                    "sent": "They have a quite complex, and they're also mildly context sensitive.",
                    "label": 0
                },
                {
                    "sent": "So to make progress in this, we need to accept the fact that we're going to need to use.",
                    "label": 0
                },
                {
                    "sent": "Quite rich models.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in normal GI, whatever that means.",
                    "label": 1
                },
                {
                    "sent": "Typically we given a class of representations and we studied the learnability of these representations under various assumptions.",
                    "label": 1
                },
                {
                    "sent": "So I think this is the this is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wrong way round really because in first language acquisition we don't know what the representations are, but we do know that they are learnable.",
                    "label": 1
                },
                {
                    "sent": "So linguists sometimes talk as though they know the representations are, but they don't.",
                    "label": 0
                },
                {
                    "sent": "OK, but we do know that that'll be cause the infant child can learn.",
                    "label": 1
                },
                {
                    "sent": "So the research strategy I've been falling for the last few years is really to look for representations that are intrinsically learnable.",
                    "label": 0
                },
                {
                    "sent": "By that I mean.",
                    "label": 0
                },
                {
                    "sent": "I think the real problems are computational and not information theoretic, so the real problem is not the absence of negative data or VC dimensions or things like this.",
                    "label": 1
                },
                {
                    "sent": "The real problems are the computational complexity of finding an appropriate hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And to do this, I think we need to use representations without hidden structure.",
                    "label": 1
                },
                {
                    "sent": "So I mean by this that we must.",
                    "label": 0
                },
                {
                    "sent": "Representational primitives must be observable.",
                    "label": 0
                },
                {
                    "sent": "In fact, we must be able to define any representational primitive of representation.",
                    "label": 0
                },
                {
                    "sent": "Any symbol nonterminal state has to be defined, able in purely language theoretic terms.",
                    "label": 0
                },
                {
                    "sent": "So rather than having really function from our representations into languages, we have a function from our languages into our representations.",
                    "label": 0
                },
                {
                    "sent": "And we can do this in natural language because we can pick our representation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Typically we proceed by looking the relationship between various substrings.",
                    "label": 0
                },
                {
                    "sent": "So when we're learning regular languages were interested generating the prefix or suffix prefix suffix relation, so we're interested in the relationship between two strings you envy where they're related.",
                    "label": 1
                },
                {
                    "sent": "The prefix use rated the suffix the if their concatenation is in the language, and using this we then look at this relation.",
                    "label": 0
                },
                {
                    "sent": "We look at the.",
                    "label": 0
                },
                {
                    "sent": "Either the right congruence classes or residual languages which are just defined in this way and the tradition.",
                    "label": 0
                },
                {
                    "sent": "Normally the inference of regular languages using deterministic finite state automotor.",
                    "label": 1
                },
                {
                    "sent": "Proceeds by essentially defining the states of our Canonical DFA to be precisely these to be precisely these residual languages, and that's one of the great success stories of grammatical inference.",
                    "label": 0
                },
                {
                    "sent": "So I think we can learn a lot from this success story by recognizing the states these supposedly hidden hard to find states are in fact observable, and they can be defined in language theoretic terms.",
                    "label": 0
                },
                {
                    "sent": "Now when we move on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From regular languages to context, free languages name just define what context is.",
                    "label": 1
                },
                {
                    "sent": "Context is just a pair of strings, LR and we can combine a context with a substring here just by wrapping the context around the subject to get this.",
                    "label": 1
                },
                {
                    "sent": "So when we look at context free languages, what we're interested in really is relationship between a context on a substring.",
                    "label": 0
                },
                {
                    "sent": "So we define this relation here as being the context LR is related to you.",
                    "label": 0
                },
                {
                    "sent": "If when we wrap EU with the LR.",
                    "label": 0
                },
                {
                    "sent": "The resulting thing is on the language now closely to the to the residual languages.",
                    "label": 0
                },
                {
                    "sent": "Alright conferences?",
                    "label": 0
                },
                {
                    "sent": "Here we have the the set of context of a string here, which is sometimes in linguistics is called the distribution of the substring, so a classic term from from 1940s and 1950s or structural linguistics.",
                    "label": 0
                },
                {
                    "sent": "So this is just the set of all context that particular string has an important thing to realize is that we can observe this right if we just have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "We can just.",
                    "label": 0
                },
                {
                    "sent": "We will just observe certain numbers, so we observe not all of it, but some subset of this policy of you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, what do I mean by distributional learning?",
                    "label": 0
                },
                {
                    "sent": "Well, in normal GI, when we have given a string, we want to predict if a particular string in the language OK, which we think is being function from the set of all strings into.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One in this regional I'm going to consider slightly richer model here.",
                    "label": 0
                },
                {
                    "sent": "We want to model this function C want to model for any string U we want to predict the set of contexts of that string included.",
                    "label": 1
                },
                {
                    "sent": "This is more general becausw the here the land and being mean.",
                    "label": 0
                },
                {
                    "sent": "The empty string.",
                    "label": 0
                },
                {
                    "sent": "Here if Lambda Lambda is in the context set of you and that's just the same as you being anything.",
                    "label": 0
                },
                {
                    "sent": "So this is just slightly more general way of looking at things.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are two problems with modeling.",
                    "label": 1
                },
                {
                    "sent": "See if you the first is obviously that the number of strings you that we're considering, which is Signal Star is infinite.",
                    "label": 0
                },
                {
                    "sent": "So the way we want to deal with that is we're going to take some finite set of primitive elements which are going to be strings K such that we know about.",
                    "label": 1
                },
                {
                    "sent": "See of you for these finite element.",
                    "label": 0
                },
                {
                    "sent": "So if U as in K then we know what see of you is like.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have these finite set of parameter elements and then the second thing we need is some way of computing.",
                    "label": 0
                },
                {
                    "sent": "See of UV from its part C of you and sievi.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we have these primitive elements and we have some way of combining these primitive elements then this will give allows to define it for the whole set.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second problem is that see of you may be infinite.",
                    "label": 0
                },
                {
                    "sent": "Invite will often be infinite if L is infinite.",
                    "label": 1
                },
                {
                    "sent": "So we need some finite representation of C.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one solution.",
                    "label": 0
                },
                {
                    "sent": "Which is the work idea with Grammy and also that Michael your snack was talking about is that is really based as I view it on this insight here that if see of you is equal to see of you primed and see a visit to your V prime and then see if UV is equal to see you prime V prime.",
                    "label": 0
                },
                {
                    "sent": "That's pretty obvious and using this we define these syntactic congruence which is just two things being congruent if their contexts are equal and we write the equivalence class for this finite representation.",
                    "label": 0
                },
                {
                    "sent": "Vitamins here are just the congruence classes of the observed strings.",
                    "label": 1
                },
                {
                    "sent": "And then we can essentially write down a representation using these simple combinatoric rules.",
                    "label": 0
                },
                {
                    "sent": "OK, so these basic rules give us at context free grammar in Chomsky normal form.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you can tell where the Seaview is equal to see of the, then learning is essentially trivial.",
                    "label": 1
                },
                {
                    "sent": "OK, so it just drops out.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there are problems with this.",
                    "label": 0
                },
                {
                    "sent": "The major problem really is that we're modeling.",
                    "label": 0
                },
                {
                    "sent": "See of you is just being some finite unstructured set of congruence classes.",
                    "label": 1
                },
                {
                    "sent": "And in real language and natural language is really No 2 words exactly like if 2 words were exactly like you wouldn't need to have two words basically.",
                    "label": 0
                },
                {
                    "sent": "And there are lots and lots of conference classes, so that means that the two things it means that learning is very slow because you have to require a very large number of these congruence classes and also means it's hard to get it right because you have things that are very similar, so you're likely to make mistakes.",
                    "label": 1
                },
                {
                    "sent": "The important thing to realize is that rather than representing sort of finite unstructured set of Congress classes, we need to represent in some way the algebraic structure of this set.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See if you and.",
                    "label": 0
                },
                {
                    "sent": "The thing to realize is the congruence classes are sets of context.",
                    "label": 1
                },
                {
                    "sent": "Ultimately there sets, so the appropriate algebraic structure to look at is a lattice, and the first person I think you realize this was a Frenchman called Sestiere in 1960.",
                    "label": 0
                },
                {
                    "sent": "I think this is the only paper he wrote, so it was a good run anyway.",
                    "label": 0
                },
                {
                    "sent": "100% of his papers were good, but you can't save many people, so give me example here.",
                    "label": 1
                },
                {
                    "sent": "If we have just using a regular context free grammar here so you have some non terminal end which derives N&V and we have some.",
                    "label": 0
                },
                {
                    "sent": "Other non term, the right M and also derives V. Then the distribution of V is going to be the union of the distributions of M&M.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we see that in fact we see this operation.",
                    "label": 0
                },
                {
                    "sent": "This sort of algebraic operation among C that is essential lattice operation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of lattice for, you know one of the our old friends at the NBA team.",
                    "label": 0
                },
                {
                    "sent": "Beat the end.",
                    "label": 0
                },
                {
                    "sent": "So this is lattice is just meant to represent a fragment of an infinite lattice.",
                    "label": 0
                },
                {
                    "sent": "What we have here is at the top.",
                    "label": 0
                },
                {
                    "sent": "Here we have the set of all strings, and here we have the set of no strings.",
                    "label": 0
                },
                {
                    "sent": "Now what is interesting here?",
                    "label": 0
                },
                {
                    "sent": "Here we have the set of all strings are in the language.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting here is that if we look at.",
                    "label": 0
                },
                {
                    "sent": "These two things survey and see if ABC of NCAABR or similar right?",
                    "label": 0
                },
                {
                    "sent": "They have alot of things in common, but actually this the context of a include all the context of AAB and they strictly included cause this context, a Lambda ABB is clear context of a, but it's not a context of of a be OK.",
                    "label": 0
                },
                {
                    "sent": "So here we have some.",
                    "label": 0
                },
                {
                    "sent": "Even in this really really trivial language we have an example of where actually modeling this lattice structure is helpful.",
                    "label": 0
                },
                {
                    "sent": "An ATM BCN is not a substitutable language.",
                    "label": 0
                },
                {
                    "sent": "OK so if we want to allow this work was driven just by trying to say well we want to extend.",
                    "label": 0
                },
                {
                    "sent": "Method of learning such useful languages to learn this really, really trivial thing.",
                    "label": 0
                },
                {
                    "sent": "So in order to learn this properly, we want to model this lattice.",
                    "label": 0
                },
                {
                    "sent": "We need to accept the fact we need to represent in some way the fact that these two things here are not completely different, though related.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we look at natural language here, here is just an example of some English pronouns here, so.",
                    "label": 0
                },
                {
                    "sent": "What we see is we see the various relations between these things.",
                    "label": 0
                },
                {
                    "sent": "Here we have you below Wepay and up here we have more abstract categories here.",
                    "label": 0
                },
                {
                    "sent": "So this for example.",
                    "label": 0
                },
                {
                    "sent": "Here is a set of all pronouns that can occur in subject position, so we naturally get these sorts of hierarchies when we look at natural.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which phenomenon, in fact, it turns out you get many, many comments cause you do get extra extension congruence classes when you deal with natural.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we need to represent the lattice of distributions?",
                    "label": 1
                },
                {
                    "sent": "Well, the simple thing to do is just to take a finite set of context F angusta model CFW intersecting with F. So this gives us 2 to the power of F congruence classes.",
                    "label": 0
                },
                {
                    "sent": "So if we have F, here we have arisen expressive formalism that we can represent an exponential number or common causes and.",
                    "label": 0
                },
                {
                    "sent": "The match with the underlying data structure isn't perfect, but ultimately that doesn't really matter.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the representation?",
                    "label": 0
                },
                {
                    "sent": "Well, what we want to do is if we can learn this lattice structure, we can also define a representation that directly uses this lattice structure.",
                    "label": 1
                },
                {
                    "sent": "OK, So what we want to do here is the base of this is.",
                    "label": 0
                },
                {
                    "sent": "Is this similar to what we were talking about before?",
                    "label": 0
                },
                {
                    "sent": "Is that if see of you if you include the context of you prime the company, include the context of the primes, then the context of UV includes the context of viewpoint viewpoint.",
                    "label": 0
                },
                {
                    "sent": "So here we have this sort of recursive decomposition that.",
                    "label": 0
                },
                {
                    "sent": "Respects the lattice structure because we have these this partial order that we get from lattice and and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other way of writing this?",
                    "label": 0
                },
                {
                    "sent": "Is just this where we sent you a converting before any is into unions.",
                    "label": 0
                },
                {
                    "sent": "So this is just this.",
                    "label": 0
                },
                {
                    "sent": "This agent says exactly the same.",
                    "label": 0
                },
                {
                    "sent": "It says the context W no matter how we split W into U&V and over any U primes that match it is going to contain Seaview Prime V Prime.",
                    "label": 0
                },
                {
                    "sent": "So this is just exactly the same way of saying this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Less clear.",
                    "label": 0
                },
                {
                    "sent": "Here's a way that's probably more clear right, which is that basically when we take.",
                    "label": 0
                },
                {
                    "sent": "If we if we have some string UV and we want to find out what it's context is like, we can split into UV.",
                    "label": 0
                },
                {
                    "sent": "You can spend to you envy.",
                    "label": 0
                },
                {
                    "sent": "We can look, try and find some you primed and some V prime there.",
                    "label": 0
                },
                {
                    "sent": "Above it we stick EU prime V prime together to get you primed, primed and then UV will be below that.",
                    "label": 0
                },
                {
                    "sent": "OK so this gives us a recursive way of computing EUV in terms of you kind of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ground.",
                    "label": 0
                },
                {
                    "sent": "Now slightly more complicated examples this.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have UVW or.",
                    "label": 0
                },
                {
                    "sent": "Clearly we can split this in two different ways.",
                    "label": 0
                },
                {
                    "sent": "We can either spend to UV&W, in which case we know this is going to be below XY, or you can split into U&VW, in which case we know it's going to be able to pick you.",
                    "label": 0
                },
                {
                    "sent": "OK, so this actually takes us.",
                    "label": 0
                },
                {
                    "sent": "Out of the context free formalism right?",
                    "label": 0
                },
                {
                    "sent": "In order to represent this fact, the fact that we have constraints that go across different derivations, we need to go outside the context free form as it took me very long time to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last letter.",
                    "label": 0
                },
                {
                    "sent": "So the formula that we use is contextual binary feature grammars.",
                    "label": 0
                },
                {
                    "sent": "In the paper we we, we present binary feature grammars and then talk about contextual binary feature grammar.",
                    "label": 0
                },
                {
                    "sent": "Here I'm squishing them together for ease of exposition.",
                    "label": 0
                },
                {
                    "sent": "Broadly speaking, we have a 2 pole here where we have F is all set of features, which is a set of contexts.",
                    "label": 0
                },
                {
                    "sent": "FS is the sentence feature which is just this trivial feature.",
                    "label": 0
                },
                {
                    "sent": "Lambda Lambda, which is the one we want to.",
                    "label": 0
                },
                {
                    "sent": "We have to be able to predict to be able to look at the language membership.",
                    "label": 0
                },
                {
                    "sent": "We have two sorts of productions we have next production risks to a where a is a letter.",
                    "label": 0
                },
                {
                    "sent": "And X is a set of these features of certain contexts, and we have the normal sort of productions and what these normal productions here X goes to ways that mean really is that if you has features Y.",
                    "label": 0
                },
                {
                    "sent": "And V has features that then the concatenation of you and he is going to have the features in X, right?",
                    "label": 1
                },
                {
                    "sent": "So really the arrow here goes the wrong way, right?",
                    "label": 0
                },
                {
                    "sent": "the Irish is really saying we can take a wireless head together and we can combine them and when we combine them we're going to get next.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to define a recursive map for all of our strings here in from single star into sets of sets of contexts, and we want this map here to approximate the true context distribution.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we just do this in the the the way you expect.",
                    "label": 0
                },
                {
                    "sent": "We start off with the empty thing.",
                    "label": 0
                },
                {
                    "sent": "We introduced the lexical things and then we just take the Union over all applicable rules.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this turns out to be.",
                    "label": 0
                },
                {
                    "sent": "You can do this efficiently using dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "It's just it's basically just the same as a CKY parsing algorithm, which you can do in cubic time.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So BF jeez, that's ignored, dropping the contextual thing.",
                    "label": 0
                },
                {
                    "sent": "Just considering BF, Jesus being where the features can be anything or a very powerful formalism.",
                    "label": 0
                },
                {
                    "sent": "They include all context free grammars and they can represent non context free languages.",
                    "label": 1
                },
                {
                    "sent": "They can also compactly represent languages that require exponentially large context free grammars like permutation languages and their equivalent to some classes subclass of range concatenation grammars, which is a fairly powerful formalism introduced by.",
                    "label": 1
                },
                {
                    "sent": "Abulia",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A few years ago now, assuming we have a finite set of strings K, these are little primitives and we have a set of features which are Contacts and a membership Oracle to answer all the questions.",
                    "label": 1
                },
                {
                    "sent": "Then essentially you can write down a grammar.",
                    "label": 0
                },
                {
                    "sent": "What we do is we just if if we have a UV and UV being K and see of you and see the combined to Form C of UV.",
                    "label": 0
                },
                {
                    "sent": "Then we just add a production like this saying see of you and see if the combined to form C of UV an simile for every letter A.",
                    "label": 0
                },
                {
                    "sent": "In Sigma we add a lexical rule.",
                    "label": 0
                },
                {
                    "sent": "It just says.",
                    "label": 0
                },
                {
                    "sent": "Basically A has the features that it has, right?",
                    "label": 0
                },
                {
                    "sent": "It's a sort of vacuous rule.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, so this means ready that I'm given an Oracle for the language and a choice McCain if we can write down to grandma G of kno left so ger hypothesis then is a function of K&F.",
                    "label": 1
                },
                {
                    "sent": "And so natural question is really is it easy to find the right care net?",
                    "label": 0
                },
                {
                    "sent": "Or maybe we've just swapped one problem for another.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, it turns out to be easy for these two reasons.",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "It turns out that as K increases, the language monotonically increases.",
                    "label": 1
                },
                {
                    "sent": "This is a fairly obvious, I think, that as you increase K, the set of productions increases and as a set of production increases, a set of features you predict will increase, so that's values.",
                    "label": 0
                },
                {
                    "sent": "That's fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "So if K plus contains K, then the language defined by GK plus LF contains this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe it's not quite so obvious.",
                    "label": 0
                },
                {
                    "sent": "Is that as F increases, the language monotonically decreases.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we add features.",
                    "label": 0
                },
                {
                    "sent": "So if F plus includes F, then the language defined by this actually decreases OK. Now the reason for this is that the features really define the conditions under which we predict a feature on the head there, like if then rules.",
                    "label": 1
                },
                {
                    "sent": "So if you had why envy has features there, then we predict that UV has some features.",
                    "label": 0
                },
                {
                    "sent": "So if we increase the number of features were actually making this conditions stronger, which means we're going to reduce the set of features we predict for longer strings.",
                    "label": 0
                },
                {
                    "sent": "OK, so the features in a way represent conditions on or in the grammar.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "Well, this is a simple example here.",
                    "label": 0
                },
                {
                    "sent": "This is a Dyke language here, So what we're doing here is we're showing along the X axis.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "Strings, which we're just taking in the order of frequency their car in.",
                    "label": 0
                },
                {
                    "sent": "In a random sample and up here we have features that again just the context that we observe in order of their current.",
                    "label": 0
                },
                {
                    "sent": "Now we can see here is that as we as we increase F as we go up here as we go in this direction, the error which is the overgeneralize error over generalization error decreases.",
                    "label": 0
                },
                {
                    "sent": "So here red is.",
                    "label": 0
                },
                {
                    "sent": "This is an error measure, red is bad, white is very very good.",
                    "label": 0
                },
                {
                    "sent": "So wherever we are here, if we go up we're going error is reducing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Conversely, if we look at the under generalization error, OK, as we go right here as we increase K again, the under generalization error decreases.",
                    "label": 0
                },
                {
                    "sent": "OK, so as we're going from right, we reduce one sort of error as we go up, we reduce the other.",
                    "label": 0
                },
                {
                    "sent": "So the obvious algorithm here is, broadly speaking, to go diagonally.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is what we do.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is and the algorithm we put in the paper is really just.",
                    "label": 1
                },
                {
                    "sent": "The simplest possible argument is correct, so and is polynomial deficient, so we're not really trying to make.",
                    "label": 0
                },
                {
                    "sent": "We're not trying to claim that economy is very scalable, it's it's naive.",
                    "label": 0
                },
                {
                    "sent": "It's inefficient, but it's nonetheless polynomial incorrect.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that if the language under generates, then you add some more examples to your kernel, and if it over generates then you add some more features.",
                    "label": 1
                },
                {
                    "sent": "You had some context, so if you're under generate, you go right.",
                    "label": 1
                },
                {
                    "sent": "If it over generates, you go up.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more formally, documents, we take a sequence of strings.",
                    "label": 1
                },
                {
                    "sent": "Our data set is just all the strings you observed so far.",
                    "label": 0
                },
                {
                    "sent": "Then we have the set of all context that we've observed in our data, and the set of all substrings that we've observed in the data, and we combine them every which way.",
                    "label": 0
                },
                {
                    "sent": "And this gives us a test set which is very similar to something that Peter Adriaens used in one of his papers.",
                    "label": 0
                },
                {
                    "sent": "I think you got the 1st order explosion, which is probably better name than mine, but.",
                    "label": 0
                },
                {
                    "sent": "So here we have when we go through all these examples in the test set.",
                    "label": 1
                },
                {
                    "sent": "So if W is in the language but it's not in the current hypothesis, then we're under generalizing.",
                    "label": 1
                },
                {
                    "sent": "So we just add some strings to K and we add a bunch of we had all of our context F. If we're overgeneralizing.",
                    "label": 0
                },
                {
                    "sent": "Then we just add the can't.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this means by the two monotonic monotonicity Lammers, right?",
                    "label": 1
                },
                {
                    "sent": "We're going to be going in the right direction, but are we actually going to converge, or you're going to get to the right place?",
                    "label": 0
                },
                {
                    "sent": "So there are two complementary properties here.",
                    "label": 0
                },
                {
                    "sent": "One is the finite context property, which means we're never going to over generalize, and the other is the finite kernel property, which means there were never going to generalize this.",
                    "label": 1
                },
                {
                    "sent": "This one is not so important.",
                    "label": 0
                },
                {
                    "sent": "The important one here is this finite context property, which I'll talk about.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a diagram just to give a big picture here.",
                    "label": 0
                },
                {
                    "sent": "So broadly speaking here, we have a K along the bottom and F along the thing.",
                    "label": 0
                },
                {
                    "sent": "What we'll see is that essentially when we have when we see enough of a sample here, this is a note here.",
                    "label": 0
                },
                {
                    "sent": "Once we get the right of this line, we're only we're never going to make any under generation errors.",
                    "label": 0
                },
                {
                    "sent": "OK, so the left of here we may make under generation errors, we get to the right of this line is going to be.",
                    "label": 0
                },
                {
                    "sent": "It's not going to generate enough, and so we're going to have a line here, which is goes up, which is a line tells us how many features we need.",
                    "label": 0
                },
                {
                    "sent": "So when we are below this line, we may make over generalization.",
                    "label": 0
                },
                {
                    "sent": "Once we get above, this line is going to be correct, so we're going to do is we want to get into this wedge in the top right hand corner.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the finite context property is this.",
                    "label": 1
                },
                {
                    "sent": "Thing is that this is the basic property which defines what is learnable learnable within this framework.",
                    "label": 1
                },
                {
                    "sent": "And this says we have a string you in a language has a finite context property.",
                    "label": 1
                },
                {
                    "sent": "If there's a finite set of contexts.",
                    "label": 0
                },
                {
                    "sent": "So a finite subset of its distribution, such that if we have any string that contains that has those that finite set of contexts, then it has all of the other contexts as well.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're saying is if you get, you can define a finite set and whenever you get that if you have this finite set of examples then you get everything.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of the inductively.",
                    "label": 0
                },
                {
                    "sent": "All all algorithms have to have this somewhere so it can be pretty well hidden.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is the way we go from a finite set of evidence F of you about the sort of context to an infinite set.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you compare the substitutable languages, substitute languages are one where any string.",
                    "label": 1
                },
                {
                    "sent": "In see of you is enough for this inductive leap, so substitutability just says if you pick any stringency of you, then if C of V contains that string, then Cav is equal to see of you.",
                    "label": 0
                },
                {
                    "sent": "And if you compare Adrian's idea of context, separability is saying basically there is one string in here or saying that there is a finite set.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "More generally, you just say a set of features is fiducial for K. If anyone came for any V subset, CSV contains.",
                    "label": 0
                },
                {
                    "sent": "These that set of features then survey contains uses just the way we essentially rollout this.",
                    "label": 0
                },
                {
                    "sent": "This definition for two finite set.",
                    "label": 0
                },
                {
                    "sent": "So the key point here is really that F needs to be a function of K. As K increases, we need more features and that's that diagonal line that we saw earlier and the key lemma in the proof really that joins everything together says that if F is fiducial then the BFG predicts only correct features.",
                    "label": 1
                },
                {
                    "sent": "I this FG of W. Contains this and that uses several different things, but essentially.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's trivial.",
                    "label": 0
                },
                {
                    "sent": "So the scope of the finite context principle.",
                    "label": 1
                },
                {
                    "sent": "Well, all regular languages have the this finite context principle, since the number of syntactic congruence classes finite, all substitutable languages have the finite context principle.",
                    "label": 1
                },
                {
                    "sent": "For some non context free languages, have the finite context property.",
                    "label": 0
                },
                {
                    "sent": "So here for example the mixed language has put as a finance context property but there are context free languages that do not have their SCP.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is the simplest example I could come up with.",
                    "label": 0
                },
                {
                    "sent": "A language doesn't have the.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have this property and it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's an entree.",
                    "label": 0
                },
                {
                    "sent": "I mean it.",
                    "label": 0
                },
                {
                    "sent": "It took me awhile.",
                    "label": 0
                },
                {
                    "sent": "Took me awhile to come up with it.",
                    "label": 0
                },
                {
                    "sent": "It's this sort of phenomenon where you have essentially an infinite infinite sequence of strings.",
                    "label": 0
                },
                {
                    "sent": "Influence and limit point.",
                    "label": 0
                },
                {
                    "sent": "It's not something that actually seems to occur in natural languages at all, so I conjecture that natural languages may have the finite context property.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The finite kernel of property just basically says it is equivalent to saying it has a finite representation.",
                    "label": 0
                },
                {
                    "sent": "So what we define a language is being a finite set is a kernel for languages for any set of features F it has.",
                    "label": 1
                },
                {
                    "sent": "It satisfies this property, so when we have a finite kernel, then eventually as we add kernels, eventually we're going to get a large enough hypothesis or regular languages have finite kernels.",
                    "label": 0
                },
                {
                    "sent": "There are context free languages that don't have a finite kernel.",
                    "label": 1
                },
                {
                    "sent": "In the way we in the way we define this, but we expect in future work to be able to weaken this requirement to include or context free languages.",
                    "label": 0
                },
                {
                    "sent": "So there's a more rapid way of learning these things that remove this account, but the FBP is.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The really important limitation.",
                    "label": 0
                },
                {
                    "sent": "So the main result is that the algorithm described identifies and limit the class of languages which satisfy the FCP and the FKP.",
                    "label": 1
                },
                {
                    "sent": "It uses polynomial update time.",
                    "label": 1
                },
                {
                    "sent": "Uses positive data plus polynomial calls.",
                    "label": 1
                },
                {
                    "sent": "A polynomial number of calls to the membership Oracle.",
                    "label": 0
                },
                {
                    "sent": "It includes all regular languages, disjoint Panda languages like language, mostly standard examples that we've looked at.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So future work.",
                    "label": 0
                },
                {
                    "sent": "The thing that's most important here is is to use rapid generalization.",
                    "label": 0
                },
                {
                    "sent": "As we said that the algorithm.",
                    "label": 1
                },
                {
                    "sent": "Essentially learns the congruence classes one by one, so it can't generalize to learning more abstract rules.",
                    "label": 0
                },
                {
                    "sent": "There's a way of doing that, essentially where you, when you're predicting a feature there rather than taking particular sets of features, you intersect them again, so you look in the.",
                    "label": 0
                },
                {
                    "sent": "The sort of semi lattice defined by the examples you've seen to produce more abstract rules.",
                    "label": 1
                },
                {
                    "sent": "This, I think, will give us a more more truly polynomial result and also takes us out of context out of the context.",
                    "label": 0
                },
                {
                    "sent": "Free class into context sensitive languages.",
                    "label": 1
                },
                {
                    "sent": "Tried to get a pack result.",
                    "label": 0
                },
                {
                    "sent": "Features that are set in context and look at natural language.",
                    "label": 1
                },
                {
                    "sent": "Natural language experiments.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusions.",
                    "label": 0
                },
                {
                    "sent": "So we've distributional learning, which normally looks at looking individual congruence classes, can be generalized to model the lattice of context substring occurrences.",
                    "label": 1
                },
                {
                    "sent": "So this requires.",
                    "label": 0
                },
                {
                    "sent": "I mean, it forces a switch to a context sensitive representation that's directly based on the lattice, and one of the one of the consequences, which is you slightly abandoned.",
                    "label": 1
                },
                {
                    "sent": "The traditional ideas of constituent structure.",
                    "label": 0
                },
                {
                    "sent": "It gives rise to efficient algorithms for learning context free and potentially context sensitive languages that have this fairly weak property, which is the finite context property, and this is a linguistically I think, very interesting for.",
                    "label": 1
                },
                {
                    "sent": "For several reasons.",
                    "label": 1
                },
                {
                    "sent": "First of all, natural languages, as far as I can tell, do seem to have this finite context property.",
                    "label": 0
                },
                {
                    "sent": "Secondly, match languages are mildly context sensitive, and finally they do seem to have the sort of particular sort of richly structured nature that these lattices produce.",
                    "label": 0
                },
                {
                    "sent": "That's awful.",
                    "label": 0
                },
                {
                    "sent": "I I guess.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, I said Paul name update time is what I've formally demonstrated here.",
                    "label": 0
                },
                {
                    "sent": "I also think of it really is being a practical issue, just not just theoretical worst case, but actually that you can use these techniques too.",
                    "label": 0
                },
                {
                    "sent": "Albums that can run on large amounts of data, so I'm interested in theories that guide to practice, not theory for itself, and so so efficient means.",
                    "label": 0
                },
                {
                    "sent": "Efficient upside down, but also efficient.",
                    "label": 0
                },
                {
                    "sent": "Ultimately in the amount of data that you need to use.",
                    "label": 0
                }
            ]
        }
    }
}