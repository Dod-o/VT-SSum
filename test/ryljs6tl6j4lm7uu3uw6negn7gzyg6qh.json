{
    "id": "ryljs6tl6j4lm7uu3uw6negn7gzyg6qh",
    "title": "Sample-Based Planning for Continuous Action Markov Decision Processes",
    "info": {
        "author": [
            "Chris Mansley, Department of Computer Science, Rutgers, The State University of New Jersey"
        ],
        "published": "July 21, 2011",
        "recorded": "June 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence->Planning and Scheduling"
        ]
    },
    "url": "http://videolectures.net/icaps2011_mansley_markov/",
    "segmentation": [
        [
            "Who I'm Chris Mansally and I've done this work.",
            "Enjoy joint work with Ari Weinstein and Michael Wittmann like Michael Littman at Rutgers University.",
            "And so I just want to give some background as to where we how we got here, and then I'll sneak up on our solution.",
            "Hopefully it'll be obvious by the time I get there.",
            "Typically in our lab, what we do is reinforcement learning and actually more specifically, what we focus on is learning transition and reward functions within environments that have very interesting structure and we like to do this in with the little interaction with the environment as possible.",
            "So we focus on sample complexity.",
            "So you might ask what we're doing at a planning conference if we're focused on sample complexity in reinforcement learning so."
        ],
        [
            "So unfortunately what happens is that when we Start learning more and more interesting transition and reward functions, we get to the point where the going from that to solving the Bellman equation and getting to the final output our policy is actually becomes the bottleneck.",
            "We've been able to in our lab.",
            "Learn very, very interesting structures an now going from those interesting structures to the final policy.",
            "The best thing we've learned how to do is flatten the structures into a big big MDP and then compute the Bellman equation and go from there.",
            "And clearly this is not the way to go if you Start learning more and more interesting structures, so there's been a lot."
        ],
        [
            "Lot of work in the one side of the problem in continuous state spaces or very large state spaces, and the typical problem there is the summation that becomes unfortunate when you start solving the Bellman equation from there.",
            "But there's lots of work on standard machine learning approaches for function approximation to get good estimates for approximately doing the dynamic programming."
        ],
        [
            "However, this is another part of the element equation.",
            "This Max operator that there's not a lot of work that's been done in attacking this from an enforcement learning point of view or on the planning side.",
            "Solving the bellman equation with this Max operator.",
            "I'm kind of ignoring the control theoretic and linear based policies.",
            "I sort of mean in general there's not been a lot of work there, so this talk is how can we attack that problem in one way?",
            "Going from really interesting structures in the reward and transition learning those structures and being able to plan with those structures.",
            "If they have maybe continuous action spaces."
        ],
        [
            "So when you solve the, when you saw the Belmont Pictures, many classic ways of doing that, you can convert it to an LP and or you can do some value iteration or policy iteration.",
            "But unfortunately all these methods kind of scale with the state in action spaces.",
            "Basically you need to enumerate all of your States and actions.",
            "So that's the classic approach, and in 99 Michael Kerns and Andrew being an chime in or came up with this great algorithm called Smart Sampling.",
            "That gives you some guarantees on not being able to not needing to enumerate all of the states in your MVP and still be able to get epsilon optimal value functions out and be able to plan optimally like that, and the number of samples that you have to generate from these transition and reward functions is sort of independent of the size of the state space, so if you have a very very big state space.",
            "And you have these nice compact representations of your transition or reward functions.",
            "The number of samples you need doesn't really depend the number of samples you need from those functions doesn't really depend on the size of that state space, so that's great, but unfortunately it still requires a ton of samples to be able to do this on unreasonably amount, and that's to cover all of the possibilities in the probabilistic model, you have to be able to know that you can't have these very unfortunate things happened when you sample, so this work is great, but we need to be able to do things.",
            "More efficiently in terms of the number of."
        ],
        [
            "Wells from these transition rewards and the idea is can we use ideas from exploration, exploitation problems to better the director search?",
            "And if you've been at icaps at all between the MTS planning, I mean the CTS workshop on Sunday, Allen Friends talk on UCT on Tuesday and yesterday one of the best paper or things on UCT as well.",
            "We know that we can use this exploration exploitation problem to do this directing of our."
        ],
        [
            "Search, so I'm going to go through some of this, so hopefully this will be review for all of us because of the talks I mentioned, but I'm going to go through it anyway because some of the math on the bottom is going to say the same, and that's kind of important.",
            "This algorithm, UCB for the simplest explanation and simplest idea of exploration exploration in the bandit case where there's only one state and you have K actions and you want to pick the best action.",
            "To maximize your reward, and UCB is great because it's basically known to be the optimal thing to do in this situation.",
            "How to balance this?",
            "Exploring things, exploring arms that you don't know an getting reward from arms that you do know?",
            "And it's really there's two terms.",
            "The exploration exploitation term which keeps track of them.",
            "You keep track of the return that you get from the arms and in the exploration term, which basically is the number of samples you've seen at each arm versus the number of samples you've seen.",
            "Total for all of the arms.",
            "So this is great, because it does optimally.",
            "Unfortunately, it's in discrete number of arms.",
            "As you increase the number of arms in the regret bound, the number of arms matters, but the assumption is that the number of arms you have versus how long of time frame.",
            "Is the number of arms is very small, so we typically don't worry about that.",
            "But if you start moving to continuous domains when you start, if you want to discretize it naively or something that starts to matter because you have a lot of arm."
        ],
        [
            "So we know that using UCB we can kind of extend this to the sequential decision making task by building it into a tree and having a UCB node at each node in the tree.",
            "And it's very similar to the sparse sampling approach bar sampling kind of builds breath first search of the MDP, whereas the UCT kind of build this depth first search of the MDP where you're making a decision making this trade off.",
            "The exploration exploitation tradeoff as you go down the tree."
        ],
        [
            "Again, I'm going through this a little fast because I assume we've all seen the UCT talks.",
            "So this is my little diagram about UCT.",
            "So the first round that you go through you basically do a random roll out, or you have a policy to do the rollout.",
            "And in the second round because you took the blue action the first time, your exploration parameter basically tells you you need to take the yellow action the next time, so it takes the yellow action.",
            "You kind of do a random roll out again, but now the last the next time.",
            "Let's say that S one is the better state that you saw better returns from that when you take now you take the blue action because that was so you saw both actions.",
            "So you know that you know basically the same amount of information about both.",
            "So now you take the blue action and then in S1 you take the blue action again because you haven't seen that before, so that's you."
        ],
        [
            "GT again.",
            "So now, like I said, UCB algorithm is restricted to discrete action spaces.",
            "An UCT is restricted to discrete action in discrete state space is because you need to get that return from going through the same state multiple times to be able to gain the benefits from it.",
            "So you might think, well, are there other bandit algorithms that we could put in there to kind of release this constraint on our algorithm?",
            "So there's this wonderful work by Sebastien Bubeck and Remy Moonos that he actually talked about on Sunday in the workshop.",
            "And it's called hierarchical optimistic optimization, and who algorithm is basically generalizes the UCB algorithm to well behaved continuous bandit problems and the well behaved.",
            "There's some technical conditions if you're really interested, but it's pretty normal in physical systems.",
            "Basically, it's not too restrictive, so the idea is really simple.",
            "You have some end dimensional action space that you want to be able to find the maximum action or arm.",
            "Basically an you basically break up the space similar to a KD tree.",
            "So you're partitioning into volumes.",
            "And in each volume you're going to track the number of samples in that volume that you've selected, and then the return from that whole volume.",
            "So at the top of the tree you're building a tree at the top of the tree.",
            "You're keeping track of the whole volume and how much return you got in the whole volume and how many pulls you have in the whole volume at the next level down.",
            "You basically have nodes for like half the volume and the other half the volume and the poles in those in the return.",
            "In those and you work your way down looking for the maximum node."
        ],
        [
            "So I have some graphics that I think will help in a second.",
            "Well, what's interesting about the algorithm to do this is that the first part of the algorithm exactly the same as the UCB algorithm, basically have your ex exploitation term, your exploration term, and then there's this extra term on the end that."
        ],
        [
            "Remy used diameter in the talk on Sunday in that that's just a term for the spatial size of each of these volumes in the continuous action space.",
            "So the idea is basically large volumes and few samples you don't know a lot about that you want to explore that volume of the space and small volumes.",
            "An lots of samples are known, so you don't need to explore that part of the space anymore."
        ],
        [
            "So if you're a picture person, so these are both the same, this is 10,000 samples and this is 1000 samples, so the top is the mean.",
            "It's a stochastic function, but that's the mean of the function, so you can see that it builds history, so the top tree is basically selecting a sample in the middle and then all of the points all the way down on the samples that it selects as it builds history and you can see it's very nice.",
            "It adds a lot more fine discretization in areas that are high reward and a lot more course discretization.",
            "In areas of low reward, and as you add more samples, it just makes that refinement more better.",
            "So the idea is is that towards the peak there you're going to zero in very quickly, so the action that you pull near that peak is going to quickly reduce an error between the maximum action and the action that you're going to select."
        ],
        [
            "Ann and I also have this comparison, so it's who is just a different.",
            "Projection of that.",
            "But the UCB algorithm you have to pick a discretization for your actions, because it only works with a fixed number of actions, and so this is just a plot of the mean that it gets back from the stochastic function and then over there on the right is The Who function, The Who algorithm and the discretization that that would get in that same environment.",
            "So again, you see that you see that it puts a lot more discretization on the areas that you care about and puts a lot more samples in the areas that you care about and puts a lot less samples in areas that you don't care about."
        ],
        [
            "So at this point, I hope it's kind of you're thinking already.",
            "Well, I've got this great continuous action bandit algorithm.",
            "We can do the same thing that we did with UCT and put that Banner algorithm in the tree and then do the UCT algorithm with just The Who in the tree instead of the UCB algorithm.",
            "So I'll really miss called Hoot, obviously.",
            "And it's exactly the same as the UCT algorithm, except you just pull out the UCB algorithm and you put in The Who algorithm.",
            "And so that's our, that's our."
        ],
        [
            "Within our contribution and now I have some empirical results to kind of motivate why we think this is a good thing to do.",
            "So this is a double integrator domain.",
            "If you don't know if it's classic control theory.",
            "Physics stuff is basically a point on a line.",
            "No, no friction and you want to get to the origin.",
            "It starts off the origin you want to get to the origin and you get to accelerate it in either direction.",
            "So you want to accelerate, probably up to maximum and then slow it down and hit that origin perfectly and hold it at the origin.",
            "And then there's some noise in your action when you try to accelerate.",
            "There's some Gaussian noise.",
            "Let's say around the action.",
            "So the first one is samples for planning step.",
            "So basically in UCT you basically can throw away all your samples every time you do a planning step.",
            "So we did in powers of two on the bottom there and then the total reward you get back from the entire environmental scenario run.",
            "So what's interesting about this is that early on who does very poorly because it doesn't have enough samples to make a determination where the discretization is happening.",
            "It's basically exploring this space, but it very quickly determines that there's a good discretization that it can find and then it starts exploiting that discretization.",
            "You can see at the top.",
            "And if you can't see the confidence intervals, basically there are smaller than the dot eventually, so it finds a better policy than all of the cities that we tried, and I would just like to note that we gave the most friendly discretization we knew of to the UC tease.",
            "So you could think of poor discretizations for the action space for the UCT, and it would fail horribly, like for example in this domain it really needs a 0 action to not do anything so I can sit on that zero point.",
            "If you don't have a 0 action, it has to dinner and sit there, and.",
            "Go back and forth and do much worse.",
            "So this is like the best thing that you can do for the UCT discretization.",
            "I don't care about that one right now, so and then we extend it to number of action dimensions.",
            "So this was a 1 dimensional thing and we artificially extended it to end dimensions.",
            "And what's really nice is with the cities, they drop off a lot faster because the blow up of the action space is exponential, but since the hood algorithm is dealing directly with that continuous space, it can eliminate huge swaths of the action space very quickly, so it doesn't even need to look there.",
            "So it's been dealing a lot better with that exponential blowup of action from the other ones."
        ],
        [
            "And then we also there's this domain called the bicycle domain.",
            "Here on a bicycle, lots of physics and you can control the turn and control your lean and the same thing happens in the samples per plant planning stuff we do.",
            "Basically this interpolation between the worst kind of discrimination and the best kind of description and note for the UCT 5 again, that's a known good discretization of the action space.",
            "We took somebody a lot of work to get that discretization to work out correctly.",
            "So the UCT 5, the top one.",
            "There is kind of artificially gaining some benefits, and Interestingly enough for three, five, and seven.",
            "It does pretty well there, but as you add more action dimensions as you as you add more discretizations in the in the in the action space, it falls off because it just can't sample enough of the action space."
        ],
        [
            "So so future work.",
            "My Co author is working on an idea to basically optimize the end step sequence of actions using who directly not going through the UCT tree, and that's working out very well for him.",
            "An Remy has also been working on that, that similar along the similar lines, and we also had a note in the paper that we have some ideas about how to extend this to continuous state spaces by kind of taking the jutri and kind of waiting it and smearing it across the state space so places that are nearby can kind of leverage information that you got in.",
            "Nearby locations, but it's really not tested and we've had some complaints from some reviewers before, so we didn't really want to make sure."
        ],
        [
            "Put it out there so.",
            "In summary, choosing action Action discretization is nontrivial, and I've shown some good action dispositions for UCT, but you could choose bad ones and it would not workout.",
            "So if you have a distance metric for your action space and your value function is locally smooth.",
            "Use hoots, not vanilla UCT.",
            "For these type of domains."
        ],
        [
            "So thank you.",
            "Hi nice work so this hood algorithm actually do two things right.",
            "It it first computer discretization and also solve the problem using that discretization.",
            "So I wonder what will happen to you City.",
            "If you give the discretization that Hood found, so yeah, we ask that question, but it's a little more difficult than that, because since you have a WHO node at every point in the UCT tree, you're going to have it.",
            "You may have a different discretization at every node in the tree, so it's not that you can just take the discretization at the top node, import that over we.",
            "We thought about that, but we haven't done that because probably time.",
            "Pascal.",
            "Costco.",
            "So when you do, you see being the tree the update for the exploration term is very easy like that.",
            "That's a very fast.",
            "Again, you say something about how that works with who.",
            "If that really slows things down or not, yeah, so the computation time we do take a hit in UCB is basically linear.",
            "And it's great.",
            "But in who?",
            "Since you may, you should have to rebuild the tree every time you add a sample to it.",
            "It can be N squared in the number of samples in the nodes in the tree there is an extension that Remy has that if you know the height, if you know the number of samples that you're going to put in each node in the tree, it's going to be inloggen.",
            "But yes, it's not.",
            "It's no longer linear to update the structure, the UCB structure.",
            "It's now going to be sort of N log N. So I was wondering how well the algorithm scales with the number of dimensions.",
            "'cause generally speaking, when people do discretization even KD3 style, it doesn't scale well with the number of dimensions.",
            "So I mean it all depends on how you mean scale.",
            "Some people think that 4 dimensions is large and some people think that 100 dimensions is large, so up into the 10s it seems to be fine, like so 1415 dimensions.",
            "I would say it scales better than a discretization for you.",
            "See T. Basically if you have to discretize it yourself, it's going to be bad because you either choose a uniform discretization and then you don't have enough samples to sample everything.",
            "So under the under our restrictions of we want to use the least number of samples to do something pretty good.",
            "This seems to scale very well, but I agree that any algorithms based on KD tree types things are going to fall down at some point and.",
            "We do a lot of work with them in the lab for other reasons as well, and we and we know that eventually it's going to become an issue, so Cousin Alternative would be to use a Gaussian process and I think I've seen somewhere people combine Gaussian processes with UCT, but I just can't recall where and I was wondering if there is indeed such work or if you're aware of anything along those lines 'cause that would not be subject to the same issues for dimensionality which is be subject to, I guess the number of data points that you have.",
            "That's interesting because yeah, but it's going to be.",
            "It's going to be N cubed in the data points, right?",
            "So that's the drawback.",
            "So it's nonparametric so it won't scale with the amount of data.",
            "But it will scale with the number of dimensions.",
            "So maybe you're just pushing it from one place to another then.",
            "That would be interesting."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who I'm Chris Mansally and I've done this work.",
                    "label": 0
                },
                {
                    "sent": "Enjoy joint work with Ari Weinstein and Michael Wittmann like Michael Littman at Rutgers University.",
                    "label": 1
                },
                {
                    "sent": "And so I just want to give some background as to where we how we got here, and then I'll sneak up on our solution.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it'll be obvious by the time I get there.",
                    "label": 0
                },
                {
                    "sent": "Typically in our lab, what we do is reinforcement learning and actually more specifically, what we focus on is learning transition and reward functions within environments that have very interesting structure and we like to do this in with the little interaction with the environment as possible.",
                    "label": 0
                },
                {
                    "sent": "So we focus on sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So you might ask what we're doing at a planning conference if we're focused on sample complexity in reinforcement learning so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So unfortunately what happens is that when we Start learning more and more interesting transition and reward functions, we get to the point where the going from that to solving the Bellman equation and getting to the final output our policy is actually becomes the bottleneck.",
                    "label": 0
                },
                {
                    "sent": "We've been able to in our lab.",
                    "label": 0
                },
                {
                    "sent": "Learn very, very interesting structures an now going from those interesting structures to the final policy.",
                    "label": 0
                },
                {
                    "sent": "The best thing we've learned how to do is flatten the structures into a big big MDP and then compute the Bellman equation and go from there.",
                    "label": 1
                },
                {
                    "sent": "And clearly this is not the way to go if you Start learning more and more interesting structures, so there's been a lot.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lot of work in the one side of the problem in continuous state spaces or very large state spaces, and the typical problem there is the summation that becomes unfortunate when you start solving the Bellman equation from there.",
                    "label": 0
                },
                {
                    "sent": "But there's lots of work on standard machine learning approaches for function approximation to get good estimates for approximately doing the dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, this is another part of the element equation.",
                    "label": 0
                },
                {
                    "sent": "This Max operator that there's not a lot of work that's been done in attacking this from an enforcement learning point of view or on the planning side.",
                    "label": 0
                },
                {
                    "sent": "Solving the bellman equation with this Max operator.",
                    "label": 1
                },
                {
                    "sent": "I'm kind of ignoring the control theoretic and linear based policies.",
                    "label": 0
                },
                {
                    "sent": "I sort of mean in general there's not been a lot of work there, so this talk is how can we attack that problem in one way?",
                    "label": 0
                },
                {
                    "sent": "Going from really interesting structures in the reward and transition learning those structures and being able to plan with those structures.",
                    "label": 0
                },
                {
                    "sent": "If they have maybe continuous action spaces.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when you solve the, when you saw the Belmont Pictures, many classic ways of doing that, you can convert it to an LP and or you can do some value iteration or policy iteration.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately all these methods kind of scale with the state in action spaces.",
                    "label": 0
                },
                {
                    "sent": "Basically you need to enumerate all of your States and actions.",
                    "label": 0
                },
                {
                    "sent": "So that's the classic approach, and in 99 Michael Kerns and Andrew being an chime in or came up with this great algorithm called Smart Sampling.",
                    "label": 0
                },
                {
                    "sent": "That gives you some guarantees on not being able to not needing to enumerate all of the states in your MVP and still be able to get epsilon optimal value functions out and be able to plan optimally like that, and the number of samples that you have to generate from these transition and reward functions is sort of independent of the size of the state space, so if you have a very very big state space.",
                    "label": 1
                },
                {
                    "sent": "And you have these nice compact representations of your transition or reward functions.",
                    "label": 0
                },
                {
                    "sent": "The number of samples you need doesn't really depend the number of samples you need from those functions doesn't really depend on the size of that state space, so that's great, but unfortunately it still requires a ton of samples to be able to do this on unreasonably amount, and that's to cover all of the possibilities in the probabilistic model, you have to be able to know that you can't have these very unfortunate things happened when you sample, so this work is great, but we need to be able to do things.",
                    "label": 0
                },
                {
                    "sent": "More efficiently in terms of the number of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wells from these transition rewards and the idea is can we use ideas from exploration, exploitation problems to better the director search?",
                    "label": 1
                },
                {
                    "sent": "And if you've been at icaps at all between the MTS planning, I mean the CTS workshop on Sunday, Allen Friends talk on UCT on Tuesday and yesterday one of the best paper or things on UCT as well.",
                    "label": 0
                },
                {
                    "sent": "We know that we can use this exploration exploitation problem to do this directing of our.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Search, so I'm going to go through some of this, so hopefully this will be review for all of us because of the talks I mentioned, but I'm going to go through it anyway because some of the math on the bottom is going to say the same, and that's kind of important.",
                    "label": 0
                },
                {
                    "sent": "This algorithm, UCB for the simplest explanation and simplest idea of exploration exploration in the bandit case where there's only one state and you have K actions and you want to pick the best action.",
                    "label": 1
                },
                {
                    "sent": "To maximize your reward, and UCB is great because it's basically known to be the optimal thing to do in this situation.",
                    "label": 0
                },
                {
                    "sent": "How to balance this?",
                    "label": 0
                },
                {
                    "sent": "Exploring things, exploring arms that you don't know an getting reward from arms that you do know?",
                    "label": 0
                },
                {
                    "sent": "And it's really there's two terms.",
                    "label": 0
                },
                {
                    "sent": "The exploration exploitation term which keeps track of them.",
                    "label": 0
                },
                {
                    "sent": "You keep track of the return that you get from the arms and in the exploration term, which basically is the number of samples you've seen at each arm versus the number of samples you've seen.",
                    "label": 0
                },
                {
                    "sent": "Total for all of the arms.",
                    "label": 0
                },
                {
                    "sent": "So this is great, because it does optimally.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's in discrete number of arms.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of arms in the regret bound, the number of arms matters, but the assumption is that the number of arms you have versus how long of time frame.",
                    "label": 1
                },
                {
                    "sent": "Is the number of arms is very small, so we typically don't worry about that.",
                    "label": 0
                },
                {
                    "sent": "But if you start moving to continuous domains when you start, if you want to discretize it naively or something that starts to matter because you have a lot of arm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we know that using UCB we can kind of extend this to the sequential decision making task by building it into a tree and having a UCB node at each node in the tree.",
                    "label": 1
                },
                {
                    "sent": "And it's very similar to the sparse sampling approach bar sampling kind of builds breath first search of the MDP, whereas the UCT kind of build this depth first search of the MDP where you're making a decision making this trade off.",
                    "label": 0
                },
                {
                    "sent": "The exploration exploitation tradeoff as you go down the tree.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, I'm going through this a little fast because I assume we've all seen the UCT talks.",
                    "label": 0
                },
                {
                    "sent": "So this is my little diagram about UCT.",
                    "label": 0
                },
                {
                    "sent": "So the first round that you go through you basically do a random roll out, or you have a policy to do the rollout.",
                    "label": 0
                },
                {
                    "sent": "And in the second round because you took the blue action the first time, your exploration parameter basically tells you you need to take the yellow action the next time, so it takes the yellow action.",
                    "label": 0
                },
                {
                    "sent": "You kind of do a random roll out again, but now the last the next time.",
                    "label": 0
                },
                {
                    "sent": "Let's say that S one is the better state that you saw better returns from that when you take now you take the blue action because that was so you saw both actions.",
                    "label": 0
                },
                {
                    "sent": "So you know that you know basically the same amount of information about both.",
                    "label": 0
                },
                {
                    "sent": "So now you take the blue action and then in S1 you take the blue action again because you haven't seen that before, so that's you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "GT again.",
                    "label": 0
                },
                {
                    "sent": "So now, like I said, UCB algorithm is restricted to discrete action spaces.",
                    "label": 1
                },
                {
                    "sent": "An UCT is restricted to discrete action in discrete state space is because you need to get that return from going through the same state multiple times to be able to gain the benefits from it.",
                    "label": 0
                },
                {
                    "sent": "So you might think, well, are there other bandit algorithms that we could put in there to kind of release this constraint on our algorithm?",
                    "label": 0
                },
                {
                    "sent": "So there's this wonderful work by Sebastien Bubeck and Remy Moonos that he actually talked about on Sunday in the workshop.",
                    "label": 0
                },
                {
                    "sent": "And it's called hierarchical optimistic optimization, and who algorithm is basically generalizes the UCB algorithm to well behaved continuous bandit problems and the well behaved.",
                    "label": 1
                },
                {
                    "sent": "There's some technical conditions if you're really interested, but it's pretty normal in physical systems.",
                    "label": 1
                },
                {
                    "sent": "Basically, it's not too restrictive, so the idea is really simple.",
                    "label": 0
                },
                {
                    "sent": "You have some end dimensional action space that you want to be able to find the maximum action or arm.",
                    "label": 0
                },
                {
                    "sent": "Basically an you basically break up the space similar to a KD tree.",
                    "label": 0
                },
                {
                    "sent": "So you're partitioning into volumes.",
                    "label": 0
                },
                {
                    "sent": "And in each volume you're going to track the number of samples in that volume that you've selected, and then the return from that whole volume.",
                    "label": 0
                },
                {
                    "sent": "So at the top of the tree you're building a tree at the top of the tree.",
                    "label": 0
                },
                {
                    "sent": "You're keeping track of the whole volume and how much return you got in the whole volume and how many pulls you have in the whole volume at the next level down.",
                    "label": 0
                },
                {
                    "sent": "You basically have nodes for like half the volume and the other half the volume and the poles in those in the return.",
                    "label": 0
                },
                {
                    "sent": "In those and you work your way down looking for the maximum node.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have some graphics that I think will help in a second.",
                    "label": 0
                },
                {
                    "sent": "Well, what's interesting about the algorithm to do this is that the first part of the algorithm exactly the same as the UCB algorithm, basically have your ex exploitation term, your exploration term, and then there's this extra term on the end that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Remy used diameter in the talk on Sunday in that that's just a term for the spatial size of each of these volumes in the continuous action space.",
                    "label": 0
                },
                {
                    "sent": "So the idea is basically large volumes and few samples you don't know a lot about that you want to explore that volume of the space and small volumes.",
                    "label": 1
                },
                {
                    "sent": "An lots of samples are known, so you don't need to explore that part of the space anymore.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you're a picture person, so these are both the same, this is 10,000 samples and this is 1000 samples, so the top is the mean.",
                    "label": 0
                },
                {
                    "sent": "It's a stochastic function, but that's the mean of the function, so you can see that it builds history, so the top tree is basically selecting a sample in the middle and then all of the points all the way down on the samples that it selects as it builds history and you can see it's very nice.",
                    "label": 0
                },
                {
                    "sent": "It adds a lot more fine discretization in areas that are high reward and a lot more course discretization.",
                    "label": 0
                },
                {
                    "sent": "In areas of low reward, and as you add more samples, it just makes that refinement more better.",
                    "label": 0
                },
                {
                    "sent": "So the idea is is that towards the peak there you're going to zero in very quickly, so the action that you pull near that peak is going to quickly reduce an error between the maximum action and the action that you're going to select.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann and I also have this comparison, so it's who is just a different.",
                    "label": 0
                },
                {
                    "sent": "Projection of that.",
                    "label": 0
                },
                {
                    "sent": "But the UCB algorithm you have to pick a discretization for your actions, because it only works with a fixed number of actions, and so this is just a plot of the mean that it gets back from the stochastic function and then over there on the right is The Who function, The Who algorithm and the discretization that that would get in that same environment.",
                    "label": 0
                },
                {
                    "sent": "So again, you see that you see that it puts a lot more discretization on the areas that you care about and puts a lot more samples in the areas that you care about and puts a lot less samples in areas that you don't care about.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at this point, I hope it's kind of you're thinking already.",
                    "label": 0
                },
                {
                    "sent": "Well, I've got this great continuous action bandit algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can do the same thing that we did with UCT and put that Banner algorithm in the tree and then do the UCT algorithm with just The Who in the tree instead of the UCB algorithm.",
                    "label": 1
                },
                {
                    "sent": "So I'll really miss called Hoot, obviously.",
                    "label": 0
                },
                {
                    "sent": "And it's exactly the same as the UCT algorithm, except you just pull out the UCB algorithm and you put in The Who algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so that's our, that's our.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Within our contribution and now I have some empirical results to kind of motivate why we think this is a good thing to do.",
                    "label": 0
                },
                {
                    "sent": "So this is a double integrator domain.",
                    "label": 1
                },
                {
                    "sent": "If you don't know if it's classic control theory.",
                    "label": 0
                },
                {
                    "sent": "Physics stuff is basically a point on a line.",
                    "label": 0
                },
                {
                    "sent": "No, no friction and you want to get to the origin.",
                    "label": 0
                },
                {
                    "sent": "It starts off the origin you want to get to the origin and you get to accelerate it in either direction.",
                    "label": 0
                },
                {
                    "sent": "So you want to accelerate, probably up to maximum and then slow it down and hit that origin perfectly and hold it at the origin.",
                    "label": 0
                },
                {
                    "sent": "And then there's some noise in your action when you try to accelerate.",
                    "label": 0
                },
                {
                    "sent": "There's some Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Let's say around the action.",
                    "label": 1
                },
                {
                    "sent": "So the first one is samples for planning step.",
                    "label": 0
                },
                {
                    "sent": "So basically in UCT you basically can throw away all your samples every time you do a planning step.",
                    "label": 0
                },
                {
                    "sent": "So we did in powers of two on the bottom there and then the total reward you get back from the entire environmental scenario run.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting about this is that early on who does very poorly because it doesn't have enough samples to make a determination where the discretization is happening.",
                    "label": 0
                },
                {
                    "sent": "It's basically exploring this space, but it very quickly determines that there's a good discretization that it can find and then it starts exploiting that discretization.",
                    "label": 0
                },
                {
                    "sent": "You can see at the top.",
                    "label": 0
                },
                {
                    "sent": "And if you can't see the confidence intervals, basically there are smaller than the dot eventually, so it finds a better policy than all of the cities that we tried, and I would just like to note that we gave the most friendly discretization we knew of to the UC tease.",
                    "label": 0
                },
                {
                    "sent": "So you could think of poor discretizations for the action space for the UCT, and it would fail horribly, like for example in this domain it really needs a 0 action to not do anything so I can sit on that zero point.",
                    "label": 0
                },
                {
                    "sent": "If you don't have a 0 action, it has to dinner and sit there, and.",
                    "label": 0
                },
                {
                    "sent": "Go back and forth and do much worse.",
                    "label": 0
                },
                {
                    "sent": "So this is like the best thing that you can do for the UCT discretization.",
                    "label": 0
                },
                {
                    "sent": "I don't care about that one right now, so and then we extend it to number of action dimensions.",
                    "label": 1
                },
                {
                    "sent": "So this was a 1 dimensional thing and we artificially extended it to end dimensions.",
                    "label": 0
                },
                {
                    "sent": "And what's really nice is with the cities, they drop off a lot faster because the blow up of the action space is exponential, but since the hood algorithm is dealing directly with that continuous space, it can eliminate huge swaths of the action space very quickly, so it doesn't even need to look there.",
                    "label": 0
                },
                {
                    "sent": "So it's been dealing a lot better with that exponential blowup of action from the other ones.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we also there's this domain called the bicycle domain.",
                    "label": 0
                },
                {
                    "sent": "Here on a bicycle, lots of physics and you can control the turn and control your lean and the same thing happens in the samples per plant planning stuff we do.",
                    "label": 0
                },
                {
                    "sent": "Basically this interpolation between the worst kind of discrimination and the best kind of description and note for the UCT 5 again, that's a known good discretization of the action space.",
                    "label": 0
                },
                {
                    "sent": "We took somebody a lot of work to get that discretization to work out correctly.",
                    "label": 0
                },
                {
                    "sent": "So the UCT 5, the top one.",
                    "label": 0
                },
                {
                    "sent": "There is kind of artificially gaining some benefits, and Interestingly enough for three, five, and seven.",
                    "label": 0
                },
                {
                    "sent": "It does pretty well there, but as you add more action dimensions as you as you add more discretizations in the in the in the action space, it falls off because it just can't sample enough of the action space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so future work.",
                    "label": 0
                },
                {
                    "sent": "My Co author is working on an idea to basically optimize the end step sequence of actions using who directly not going through the UCT tree, and that's working out very well for him.",
                    "label": 1
                },
                {
                    "sent": "An Remy has also been working on that, that similar along the similar lines, and we also had a note in the paper that we have some ideas about how to extend this to continuous state spaces by kind of taking the jutri and kind of waiting it and smearing it across the state space so places that are nearby can kind of leverage information that you got in.",
                    "label": 0
                },
                {
                    "sent": "Nearby locations, but it's really not tested and we've had some complaints from some reviewers before, so we didn't really want to make sure.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put it out there so.",
                    "label": 0
                },
                {
                    "sent": "In summary, choosing action Action discretization is nontrivial, and I've shown some good action dispositions for UCT, but you could choose bad ones and it would not workout.",
                    "label": 0
                },
                {
                    "sent": "So if you have a distance metric for your action space and your value function is locally smooth.",
                    "label": 1
                },
                {
                    "sent": "Use hoots, not vanilla UCT.",
                    "label": 0
                },
                {
                    "sent": "For these type of domains.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi nice work so this hood algorithm actually do two things right.",
                    "label": 0
                },
                {
                    "sent": "It it first computer discretization and also solve the problem using that discretization.",
                    "label": 0
                },
                {
                    "sent": "So I wonder what will happen to you City.",
                    "label": 0
                },
                {
                    "sent": "If you give the discretization that Hood found, so yeah, we ask that question, but it's a little more difficult than that, because since you have a WHO node at every point in the UCT tree, you're going to have it.",
                    "label": 0
                },
                {
                    "sent": "You may have a different discretization at every node in the tree, so it's not that you can just take the discretization at the top node, import that over we.",
                    "label": 0
                },
                {
                    "sent": "We thought about that, but we haven't done that because probably time.",
                    "label": 0
                },
                {
                    "sent": "Pascal.",
                    "label": 0
                },
                {
                    "sent": "Costco.",
                    "label": 0
                },
                {
                    "sent": "So when you do, you see being the tree the update for the exploration term is very easy like that.",
                    "label": 0
                },
                {
                    "sent": "That's a very fast.",
                    "label": 0
                },
                {
                    "sent": "Again, you say something about how that works with who.",
                    "label": 0
                },
                {
                    "sent": "If that really slows things down or not, yeah, so the computation time we do take a hit in UCB is basically linear.",
                    "label": 0
                },
                {
                    "sent": "And it's great.",
                    "label": 0
                },
                {
                    "sent": "But in who?",
                    "label": 0
                },
                {
                    "sent": "Since you may, you should have to rebuild the tree every time you add a sample to it.",
                    "label": 0
                },
                {
                    "sent": "It can be N squared in the number of samples in the nodes in the tree there is an extension that Remy has that if you know the height, if you know the number of samples that you're going to put in each node in the tree, it's going to be inloggen.",
                    "label": 0
                },
                {
                    "sent": "But yes, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's no longer linear to update the structure, the UCB structure.",
                    "label": 0
                },
                {
                    "sent": "It's now going to be sort of N log N. So I was wondering how well the algorithm scales with the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "'cause generally speaking, when people do discretization even KD3 style, it doesn't scale well with the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "So I mean it all depends on how you mean scale.",
                    "label": 0
                },
                {
                    "sent": "Some people think that 4 dimensions is large and some people think that 100 dimensions is large, so up into the 10s it seems to be fine, like so 1415 dimensions.",
                    "label": 0
                },
                {
                    "sent": "I would say it scales better than a discretization for you.",
                    "label": 0
                },
                {
                    "sent": "See T. Basically if you have to discretize it yourself, it's going to be bad because you either choose a uniform discretization and then you don't have enough samples to sample everything.",
                    "label": 0
                },
                {
                    "sent": "So under the under our restrictions of we want to use the least number of samples to do something pretty good.",
                    "label": 0
                },
                {
                    "sent": "This seems to scale very well, but I agree that any algorithms based on KD tree types things are going to fall down at some point and.",
                    "label": 0
                },
                {
                    "sent": "We do a lot of work with them in the lab for other reasons as well, and we and we know that eventually it's going to become an issue, so Cousin Alternative would be to use a Gaussian process and I think I've seen somewhere people combine Gaussian processes with UCT, but I just can't recall where and I was wondering if there is indeed such work or if you're aware of anything along those lines 'cause that would not be subject to the same issues for dimensionality which is be subject to, I guess the number of data points that you have.",
                    "label": 0
                },
                {
                    "sent": "That's interesting because yeah, but it's going to be.",
                    "label": 0
                },
                {
                    "sent": "It's going to be N cubed in the data points, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the drawback.",
                    "label": 0
                },
                {
                    "sent": "So it's nonparametric so it won't scale with the amount of data.",
                    "label": 0
                },
                {
                    "sent": "But it will scale with the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "So maybe you're just pushing it from one place to another then.",
                    "label": 0
                },
                {
                    "sent": "That would be interesting.",
                    "label": 0
                }
            ]
        }
    }
}