{
    "id": "mmexjjqvvthegtaoef7lkazjl7h7ur3v",
    "title": "Media fragmentation and annotation technologies",
    "info": {
        "author": [
            "Vasileios Mezaris, CERTH - Centre for Research and Technology Hellas"
        ],
        "published": "Feb. 10, 2014",
        "recorded": "January 2014",
        "category": [
            "Top->Computer Science->Software and Tools",
            "Top->Computer Science->Semantic Web->Annotation",
            "Top->Computer Science->Semantic Web->Applications"
        ]
    },
    "url": "http://videolectures.net/wmpa2014_mezaris_media_fragmentation/",
    "segmentation": [
        [
            "I'm with the Information Technologies Institute of the Center Forest and technology allow us, which is located in Thessaloniki, Greece and I will talk today about media fragmentation and annotation technologies.",
            "So the start point of everything, how we fragment the media and how we start annotating it."
        ],
        [
            "The overview of the presentation is as follows.",
            "Will have first an introduction and overall motivation of this this work.",
            "Then we will discuss in more detail some technologies for media fragment creation, annotation and these technologies include the temporal segmentation of video to sorts.",
            "The further temporal segmentation of it to scenes.",
            "Visual concept detection, which is 1 first step to annotating the videos.",
            "Event detection which is enriching.",
            "These concept annotation with also event labels.",
            "And object re detection will also see some demos of technologies for this kind of processing and for each of the presented technologies will go through somewhat more precise problem statements.",
            "So what exactly do we want to do when we say, for instance, temporal segmentation to sorts will have a brief overview of the literature in this particular area, then a closer look to one or two media mixer promoted approaches for doing this, some indicative experiments and results to get an idea of.",
            "How good and how mature its technology is, and then we will have a few conclusions and some references for additional reading."
        ],
        [
            "First, motivation.",
            "What do we have?",
            "We have media items which can be long videos, for instance, or sets of images or long text.",
            "And what do we want to do?",
            "We want to enable fine grained access to the media and particularly discussing video.",
            "We want to enable fine grained access to the video.",
            "So what do we need to do?",
            "We need to break down its media item to meaningful fragments and then of course we want to annotate it's fragmente so that we can make it searchable as some of the previous.",
            "Speakers in the introductory presentations showed we want for instance, to detect dogs that there is a dog there, despite the fact that this is not already part of the videos file name or some tags that have been added to it.",
            "So this fragmentation ordination there are two approaches, two general ways to go for this.",
            "One is to do it manually and the other is do it with automatic techniques.",
            "When we do it manually we gain of course inaccuracy because we can do it very very accurately.",
            "If we want to.",
            "But the downside is that speed, cost and effort required for doing this are sometimes prohibitive, particularly when we talk about large volumes of video.",
            "So doing manual fragmentation, for instance, annotation is something that is probably feasible when we talk about high value content.",
            "Which is not of so much so high volume.",
            "For instance Hollywood movie.",
            "There are millions and millions of euros spent for producing the movie, so doing the extra work of fragmenting it into a certain scenes and adding annotations is something that is visible.",
            "On the other hand, when we talk about user generated content, so the videos that we saw in the introductory presentation that are uploaded to YouTube every minute or the videos that are included in one of the leading.",
            "Archives then this manual annotation is of course prohibitive and we have to go for automatic techniques where we lose in accuracy.",
            "Sometimes we will see about this will talk about this in the sequel, but for so we gain in speed and the effort that is required for doing so.",
            "So this makes it feasible.",
            "Media mixer, as also Lyndon sold in his store, promotes technologies for the automatic fragmentation annotation of video content, and.",
            "As I already showed, we will discuss certain scene detection for for temporal fragmentation of video.",
            "The annotation of it with visual concepts and events, and some instance level annotation.",
            "Also object detection, their detection of known objects."
        ],
        [
            "Starting with Fragmente creation and sorts, we first have to define what a sort is.",
            "This is a sequence of consecutive frames taken without interruption by a single camera.",
            "So sought, segmentation means finding the boundaries of these changes between between salts.",
            "And this is the foundation of most high level video analysis approaches.",
            "So when we do when we want to do annotation, for instance, we first have to segment the video into sorts, because different sorts may so many many different things, so it doesn't make sense to have a list of concepts that appear in a very long video.",
            "It makes sense to break it down first into fragments into sorts and then identify the few concepts the few objects or events that relate to this particular fragment.",
            "So, so change is manifested by shifting the visual content.",
            "We don't need to look at other modalities, it's just the visual one."
        ],
        [
            "So this is for instance an example of a sort change we have here a few frames that are obviously taken without interruption.",
            "And then there's an interruption.",
            "The visual content suddenly changes and this is a sort change.",
            "This is an abrupt so changed by the."
        ],
        [
            "Way and we can have also gradual changes where again the visual content changes but it changes over a period of time or over a sequence of frames.",
            "And this is for instance that dissolve where we can see that the two completely different visual contents they blend at some point and one changes to the other.",
            "And we can have other types of graphs."
        ],
        [
            "Transitions as well, such as a wipe so.",
            "This again gradually changes to a completely different visual content, but in a different way."
        ],
        [
            "And also fade instant fade outs where the first sort fades into a white or black frame and then a new one gradually comes in."
        ],
        [
            "So what are the challenges here?",
            "The challenge is to detect these changes, not just the abrupt, but also the gradual ones, but without being misled, misled by other artifacts that appear in the video.",
            "So without being misled by illumination changes which can be caused for instance by camera flash slides without being misled by fast camera movement or any object movement, any local local motion."
        ],
        [
            "And here we have a few examples again.",
            "So here we have a flashlight.",
            "You can see that it's obviously a set of frames taken without interruption by a single camera, but depending on the technique used for evaluating the differences between shots between frames, if you take a histogram of this frame and a histogram of this one, then you will see a significant difference.",
            "So you have to choose the right way of comparing frames to make sure that you are not.",
            "Affected by this kind of artifacts and."
        ],
        [
            "Similarly, for camera movement where we can see here that the content of the sort changes in time because of the camera changing view."
        ],
        [
            "And also because of local motion.",
            "So we have a couple of people there and a car occludes them temporarily and this change could be mistaken as a sort change.",
            "So we have to make sure that our methods don't have this problem."
        ],
        [
            "The related work to sort segmentation can be generally organized according to the data that you work with.",
            "So whether you work with uncompressed data or with compressed video directly, the features that you use, which also depends of course on what kind of data you have to work with.",
            "If you work with the roll video or not, and we have the techniques algorithmically being organized mainly into threshold based techniques and learning based techniques.",
            "So starting with compressed video segmentation methods, this reduce the computational complexity of the processing by avoiding the full decompression of the video so they play with features such as the DC coefficients of the city, coded video or the motion vectors in MPEG video.",
            "Also, they use macroblock information of specific frames, so if the blocks are intra coded or skipped or Inter coded, this can indicate the temporal continuity of the frames and therefore can be used as a hint for finding times the location in time where there's a short change.",
            "Generally compressed video methods are very very fast but.",
            "For a little bit lower accuracy as compared to uncompressed video methods."
        ],
        [
            "So in uncompressed video there is much much more work in terms of segmentation.",
            "There are many techniques that range from pairwise pixel comparisons and the comparison of global histograms to other techniques that even use local visual features or bag of words or even even do actually matching of local features along frames to find differences.",
            "These features and the differences of them along time you can evaluate either by using thresholds, which is the simplest approach, but thresholds are often hard to tune.",
            "Or you can use the machine learning techniques such as support vector machines, for instance, where you learn from a training set and then you can evaluate the differences of these features in in an unprocessed video.",
            "In general, as we will see later on, also we can get high detection accuracy and relatively low computational load even when working with uncompressed data.",
            "So this is the preferred approach in most applications."
        ],
        [
            "So here are a few more details about the media Mixer proposed approach.",
            "This is a learning based approach, so we use an SVM to evaluate the differences of features.",
            "The features that we extract for from every keyframe are three global global image features which are called color coherence.",
            "The Macbeth color histogram and aluminum center of gravity.",
            "So we use these to four feature vectors for every frame and then we compare the differences of feature vectors.",
            "Four pairs of frames, either consecutive or non consecutive.",
            "And then this distance vector is an input to an SVM classifier which identifies which decides on whether we have a short boundary or not.",
            "At every given."
        ],
        [
            "Time.",
            "An extension of this can also be.",
            "Used to alleviate the impact of flashes in the video so we can do a simple post processing of of the SVM outputs when we have flashes in a news video.",
            "These typically come in in a sequence in a very short time.",
            "You have several flashes, so even a simple post processing of the out of the SVM classifier can, so the parts of the video where we have.",
            "A set of flashlight so we can detect this pattern in the output of the SVM classifier.",
            "But of course there are many other ways we can deal with this problem."
        ],
        [
            "Some experiments."
        ],
        [
            "So typically we evaluate the sort segmentation using precision.",
            "Recall and the F score Precision is the fraction is defined as the fraction of detected sorts.",
            "That's correspond to actual source of the video.",
            "So how many of the detections were correct?",
            "And recall is the fraction of actual source of the videos that have been successfully detected, so it's how many of the actual source we have there were detected and how many were missed.",
            "Higher values are of course better in this case for both measures, so the optimal would be to have 100%.",
            "Here we can see some figures of the media mixer promoted approach and we can see that we have precision recall around 90%, which is on a par with the state of the art.",
            "Approaches in this area.",
            "And our media mixer proposed media mixer promoted method runs in about real time, so it's a little little bit slower than real time.",
            "Real time is identified as the videos actual duration.",
            "So if we have a one hour video this is it takes about one hour or a little over one hour to do this on a single core.",
            "We can.",
            "See a short demo of this.",
            "So this is a video where we have done.",
            "We have run the sub segmentation and whenever you sort these detected we just print and you sort label as a subtitle.",
            "To this to this video.",
            "So here we are.",
            "In Short, 2 S one was the starting and here we change to another sort so.",
            "An abrupt transition in this case was detected successfully.",
            "Then we can see our method is not sensitive to camera motion and local object motion.",
            "So here you can see that the visual content changes significantly but still it is a single sort.",
            "It is a set of frames taken without interruption and we can correctly detect this.",
            "Then we had another change here.",
            "So the idea is that we need to detect.",
            "The changes the short changes but still be able to.",
            "Not detect aronis so changes when we have this kind of motion and this is important for applications.",
            "By the way, you can see that there is a significant similarity.",
            "In the content of these sorts.",
            "And this will lead us to the next part of the presentation, which has to do with temporal segmentation to scenes.",
            "So let's go back to the presentation."
        ],
        [
            "Some conclusions about the shot detection before we proceed to see segmentation so the overall accuracy of South detection methods is high.",
            "It's about 90%.",
            "This is sufficient for any application.",
            "Any subsequent processing of the video.",
            "The detection of gradual transitions and the handling of intense local and global motion is still a bit more challenging, so abrupt transitions we can very easily detect.",
            "But when we have.",
            "Long gradual transitions.",
            "We can be sometimes misled and the same holds for motion.",
            "Intense motion can sometimes lead to the detection of erroneous changes.",
            "And real time or near real time processing is visible, but of course much faster processing may be needed in some applications.",
            "So when it comes for instance to the user generated content uploaded to YouTube.",
            "We saw that it is a lot of content per minute.",
            "So what one would probably choose in this kind of application would be a compressed domain method, which can be like 100% fast 100 times faster than real time.",
            "It may lose a bit in accuracy, but would be able to handle this kind of volume of content more effectively."
        ],
        [
            "Some additional reading on short detection."
        ],
        [
            "And we move on to the detection of since the creation of longer temporal fragments.",
            "So what is a scene as seen as a higher level temporal video segment which is elementary in terms of the semantic content covering either a single event or several related events taking place in parallel.",
            "This means that the scene is a longer is typically a longer chunk of video than assault.",
            "It is not about just detecting visual visual changes in this case.",
            "It is about detecting changes in the topic of the the the meaning that is conveyed and this makes it also more more tricky.",
            "So this is of course again important for subsequent applications such as summarization, indexing and video browsing.",
            "It is important in many cases when it comes to video hyperlinking where you want to use scenes in many cases as the as the elementary.",
            "Unit of video.",
            "Because the scene is meaningful on its own, whereas assault is sometimes too short to be meaningful on its own.",
            "And what is important when it comes to processing the video is that the same change is not manifested by just changing the visual content, so it is not as easy to detect them as it was with swords."
        ],
        [
            "So here for instance, in this example again, here we have obviously assault change because the visual content of the video changes.",
            "But what do we know about the scene?",
            "Is it the same change or not?",
            "Is it about the same topic or is this some event in Vancouver and there's some demonstration in Greece we don't know by just looking at the frames and This is why scene segmentation is more challenging."
        ],
        [
            "The basic assumptions that are typically made when it comes to seem segmentation is that first of all assault cannot belong to more than one scenes, so we assume that if we have shot segmentation 1st and then we try to group the sorts then we can successfully detect since we don't miss any scene boundaries.",
            "This is an assumption that holds in most cases, not always so we can have examples with news videos.",
            "With this assumption, doesn't hold, there's a single.",
            "Anchorperson, for instance talking about many different events in a single shot, but generally speaking, this is an assumption that is typically made.",
            "So, since segmentation based on this assumption."
        ],
        [
            "Is most often performed by just detecting the salt changes so we have some sorts automatically identified and then."
        ],
        [
            "Group these shots into one or more scenes."
        ],
        [
            "In terms of related work, this can be generally organized according to the data that we work with, so we have unique model versus multimodal approaches.",
            "The dependence or not on specific domain knowledge, and the domain of choice.",
            "So do we want to do scene segmentation for generic video or are we working specifically with news videos or documentaries?",
            "This makes a lot of difference in the approach we use and also in the algorithms that we can use.",
            "You model methods starting with the type of information that we use in modern methods.",
            "As the name suggests, use just one type of information, typically visual cues, so they come as an extension of visual sort segmentation, whereas multiple ones combined visual cues with audio, speech transcripts, and other kinds of information.",
            "As we will see in the sequel.",
            "Domain versus domain domain specific versus domain independent approaches.",
            "There's no need to explain here when it comes to having domain specific approaches.",
            "What is important is to.",
            "Knowledge about the specific domains to exploit, for instance, knowledge about the structure of news and this helps us to do a scene segmentation more effectively.",
            "And there are many algorithmic approaches to solving this problem.",
            "Some are graph based.",
            "For instance in transition graph, others are more clustering based.",
            "For instance using hierarchical clustering.",
            "There are statistical methods and many others."
        ],
        [
            "Here's some media mixer promoted approach.",
            "This is based on the same transition graph which was proposed by Young Yoann you in 1998.",
            "The idea here is that we have the different sorts we have identified them.",
            "We can represent them with one or more keyframes.",
            "We have, of course, the temporal ordering of the sorts, because this is also known and then based on visual similarity we do a grouping of the sorts into clusters.",
            "Also taking into account.",
            "Their temporal proximity, so we have sorts.",
            "1:15 and 1:20, which are obviously similar in visual content, being part of 1 cluster, then again so it's 100 and 16118 for the same reason, and another couple of sorts.",
            "Then 121 is left alone.",
            "And this leads us to the creation of this kind of graph, where the nodes of the graph are the clusters that we have identified based on the visual similarity and the edges of the graph indicate the temporal succession of the nodes based on the temporal succession of the swords that are included in these nodes and then scene segmentation is comes to detecting cut edges which are edges which if.",
            "Removed if a single one of them is if a single edge is removed then this graph ends up being two disconnected graphs.",
            "So identifying cut edges essentially gives us the scene boundaries.",
            "In this case, this is a method that was originally proposed doing this clustering by just looking at the visual similarity of sorts.",
            "So just low level feature similarity evaluation."
        ],
        [
            "What media Mixer promotes is an extension of this approach that is built so as to exploit multiple types of audio visual information.",
            "So the idea is that first we had a fast transition graph approximation that we introduced and this is many times faster than the original one and actually allows us to repeat the same transition graph transition and the construction and the detection of cut edges.",
            "Many times in the order of thousands.",
            "And then the approaches to do to create sets of sin transition graphs using different kinds of information.",
            "So visual low level visual features as the original approach, but also using the output of visual concept detectors using audio features and using audio event detectors.",
            "The output of audio event detectors.",
            "These are four different modalities that we can exploit and then we create multiple skin transition graphs using any of these types of.",
            "Features and we do a probabilistic merging of their results."
        ],
        [
            "So the evaluation of scene segmentation approach is typically done using the coverage and overflow measures, which are not so much different from precision and recall, but also take into account the temporal, overlapping or non overlapping of the segments that are produced.",
            "So what we can see here is that we have.",
            "I should mention that the objective the desire is to have a coverage of 100%.",
            "This is the optimal value and an overflow of 0.",
            "We can see that we get something like 75% of coverage and 20 or 25% overflow, which is reasonably good.",
            "We have an F score in the 70s range.",
            "The algorithm we have is very fast, it's just one.",
            "1.5% of real time if you exclude the processes of feature extraction and this is a result of working with salts.",
            "So we try to group shots and this means that we have very few few items within the video, few sorts and we can easily do this process."
        ],
        [
            "What is more interesting is this diagram, which shows the contribution of the different types of information to this process.",
            "So what we have here is the F score on.",
            "The vertical axis higher values are better and here we have the number of visual concepts that we use for the analysis.",
            "The baseline is this straight line which is around 75% F score.",
            "It's a straight line because this is using just the visual low level features, which means that it is not a function of the number of concepts that we use.",
            "Then this curve here is.",
            "How we do with using just the output of visual concept detectors when the this changes.",
            "Of course, when the number of concept detector varies, we can see that generally speaking or in all cases we are below the baseline, But what's interesting is this dotted line with the axis.",
            "This is what we get when we combine the two so we can see that combining the low level visual features and the output of constant detectors can give us a nice approximately 5% improvement.",
            "And the other two lines on top of the other two curves are when we add to this set of features also the.",
            "Audio the low level audio information.",
            "And then the solid line on the solid curve on the top is when we also use the output of audio event detectors.",
            "So when we use all four types of visual audio visual information that we have, so this, this shows how the different types of information can contribute to more accurate detection of the scenes.",
            "And of course it also shows the important of audio, audio information and audio events there.",
            "So you can have as an example.",
            "You can have two shots that are very very different in terms of their visual content, but if the audio if background is similar and you can have for instance Rd traffic.",
            "Sounds or classroom related sounds.",
            "Classroom related audio events then these are cues that the two sorts, despite being visually very different, belong actually to the same scene.",
            "They describe the same the same event maybe from completely different visual angles, but still the same."
        ],
        [
            "A few conclusions about the scene detection.",
            "So since segmentation is less accurate than the short segmentation we saw in the previous case, our results were in the order of 90%, but still the results are good enough for improving access to meaningful fragments in various applications.",
            "And we've seen this with retrieval where retrieving scenes instead of retrieving salts helps you get more more useful information.",
            "We've also seen it with video hyperlinking applications.",
            "Where linking to scenes rather than linking to single shots helps the user to get more meaningful.",
            "Video segments as as result of the linking process so it facilitates access more meaningful access to the video.",
            "Using more than just low level visual features helps a lot as we saw in the previous diagram.",
            "And what this be taking to be taken seriously is the choice between domain specific versus domain independent approaches so.",
            "What we've seen in all our experiments is that when we have a domain specific problem, then using a domain specific approach as well that exploits knowledge about the structure of the video in this particular domain is very, very helpful."
        ],
        [
            "Some additional reading on scene detect."
        ],
        [
            "And now that we have seen how to create meaningful temporal fragments of the video, let's move on to discuss how we annotate them, starting with visual concept detection, what is the goal here?",
            "Is having some visual content.",
            "To come to a set of concepts that describe this content and some degree of confidence that we are right in this selection.",
            "So not just a binary decision that we have here, hand and Sky and UFO, but also some degree of confidence, showing how confident the algorithm is that this concept actually appears in the video.",
            "This is useful as you will see for also for subsequent analysis."
        ],
        [
            "The related work.",
            "This is a diagram showing more or less how most of the modern approaches to constant detection work, so starting with the visual information, which can be the.",
            "Video sort itself, or some keyframes of that sort.",
            "We have interest point detection description because most modern techniques work with local visual features.",
            "Then we have the representation of these features using a bag of words or feature vectors, or a number of other techniques.",
            "And then we have one or more machine learning based classifiers per concept that try to find a mapping between.",
            "These low level feature representations and the concepts that we're looking for.",
            "So in order to build a competitive system for concept detection, one needs to use of course the.",
            "Correct, kind of features which are typically rotation and scale invariant descriptors sometimes also color invariant, sometimes using actually color based features so that you get also the color information in this process.",
            "State of the art representations of them such as feature vectors typically need to fuse multiple descriptors and concept detectors, so it's not a matter of choosing just one of these methods for description, it's a matter of combining multiple complementary descriptors and representation methods.",
            "Exploiting concept correlations is important, so as human beings we know that sun and Sky, for instance, often appeared together.",
            "This is something that we also need to take into account.",
            "In building an algorithm for detecting the concepts and also to exploit temporal information in videos, because there are many studies which show that there is some form of temporal continuity also when it comes to the concepts that appear in the video."
        ],
        [
            "In terms of feature extraction, there are way too many things here to cover in this.",
            "In this limited time, but in terms of feature extraction, there global and local visual features most of the time we use local ones such as.",
            "Sift, surf and color variants of them.",
            "There are many motion features, some of them are extensions of the local static features.",
            "For instance steep or most sift, and there are many others, including using a text and audio.",
            "Although these are for limited use compared to visual features.",
            "For encoding these features into a meaningful descriptor, there are techniques such as pyramidal decomposition that you use for including some kind of.",
            "Structure information into your representation.",
            "There is a bag of words, feature vectors which are most recent extensions.",
            "There are different ways to deal with the Fusion of multiple descriptors, so early Fusion versus late Fusion.",
            "In terms of machine learning, there are also many options here.",
            "Support vector machines are the most usual choice, although there are other possibilities such as using multi label learning approaches.",
            "And there are different ways of taking into account the correlation of constants."
        ],
        [
            "We'll see here a couple of media mixer promoted approaches.",
            "These are based on the typical concept detection pipeline that we saw in the previous slides.",
            "The first one is.",
            "Do you have two layers stacking architecture which means that we have a first layer where we build multiple independent concept detectors based on support vector machines.",
            "This is the typical approach, but then we introduce a second layer of concept detection where we take as input the output of the first layer support vector machines.",
            "We construct low dimensional model vectors so we aggregate the results of constant detection for a number of different concepts.",
            "And then we use multi label learning to capture the correlations between these concepts and refine the output of the VM's of the first layer.",
            "For the first layer, one additional media mixer promoted approach that we have is to use video tama graphs as well in addition to keyframes, and we'll see what this means."
        ],
        [
            "So video tamograph.",
            "So these are two dimensional slices of 3D video volume with one dimension in time and one dimension in space.",
            "This is a 2D cross section of the video as well as we can see on these examples, but it is different from the static keyframes because keyframes have both dimensions in space.",
            "Here we maintain one dimension in space and one in time, so it's a different cross section.",
            "And these we used in the same way that the typical keyframes are used, so we process them with the whole pipeline of local local interest point detection description with SIFT or other variants of it and so forth.",
            "So this is 1 one technique that we used for introducing into the process some kind of temporal information information about the temporal evolution of the video signal.",
            "But we do this without resorting to expensive special temporal features such as steep, which would require processing an awful lot of frames pursuit an would.",
            "Increase the computational complexity of the concept detection approach."
        ],
        [
            "So we can see some results here where we have the dama graphs.",
            "This is across all all concepts.",
            "I think this is the tama graphs alone.",
            "The keyframes alone we can see that the key frames which are more meaningful.",
            "Samples of a video sort get to perform better, but when you combine the two, when you combine the keyframes with the tamograph so you can get a significant boost in the performance of concept detection so you can detect the concepts more accurately, and this is particularly true when it comes to detecting motion related concepts.",
            "So when it comes to very static concept then the additional motion information that we introduce.",
            "Doesn't play.",
            "That bigger role.",
            "But when it comes to detecting motion related concepts, it can improve the accuracy by 50% or more."
        ],
        [
            "And here a bit a few more details about the stacking approach that we used to take into account the concept correlations.",
            "So as I mentioned already, we.",
            "Use the individual concept detectors that we built based on support vector machine classifiers.",
            "Then we construct a model vector by concatenating the responses of these concept detectors.",
            "And we use this train and MLK and then model.",
            "Which explicitly uses the label correlations in the neighborhood of each of the tested instances.",
            "So."
        ],
        [
            "This.",
            "Slide maybe explains a bit.",
            "This is what we do.",
            "So at the second layer we have a single classifier that takes into account all the concept detection results of the first layer and produces a set of.",
            "Concept detection results again, which are more consistent in terms of the correlations of the concepts whereas.",
            "The typical approach is to just use a set of independent classifiers, so train its concept classifier separately, completely ignoring the presence or absence of any other concept.",
            "The same videos."
        ],
        [
            "Again, some experimental results.",
            "The system one is just the independent concept detectors, so this is the typical approach of the literature and System 2 is when we add the second layer stacking approach that we have and we can see that we have.",
            "We can have significant improvements.",
            "Both when evaluating concept detection in a retrieval scenario where we have the mean average precision.",
            "Looking at single concept based retrieval and also when evaluating how well a single video has been annotating has been annotated.",
            "So when looking at the ordering of the labels that have been assigned to every individual video, these are two different kinds of evaluations that we can do for concept detection."
        ],
        [
            "Some conclusions here looking also at the results of previous years because concept detection is a very active area of video analysis concept detections has progressed a lot in the past few years.",
            "The results are still far from perfect, but they are already useful in some applications, so for retrieval, for instance, they can be useful despite being imperfect, and you can think of you can.",
            "Understand this by looking at the Google results of text retrieval, so the Google results are still not perfect when you look for something it doesn't.",
            "Google doesn't magically understand what you mean and produce.",
            "You present to you, just relevant links.",
            "Still, when you get a page of 10 results and four, five of them are what you're looking for, you're happy, so the retrieval results don't have to be perfect to be.",
            "Useful to someone, and the same goes here.",
            "The concept detection results are far from perfect, but when looking at using them for concept based retrieval they can already be useful in real applications, so most information is of course important, but extracting the traditional motion descriptors is computationally expensive and This is why most approaches work with keyframes and we've also introduced the Tama graphs which.",
            "Take into account some kind of motion information, but still they are like keyframes when it comes to processing them.",
            "Linear support vector machines are very popular as a machine learning method for developing the individual concept classifiers and then using a second layer of multi label learning for exploring the concept correlations is also an approach that can introduce significant gains.",
            "Still, I think though that if you look at the literature you will see that the computationally efficient detection of concepts in video when considering hundreds or thousands of concepts is still a significant silence.",
            "So we're always looking for ways to improve the computational requirements of concept detection.",
            "And we can see a demo here.",
            "Which is the.",
            "A website.",
            "You could also log in yourselves.",
            "What we have here is a demo of video shot segmentation concept detection, so we have some videos which have been processed without segmentation concept detection.",
            "And.",
            "You can access the videos at the fragment level, so at the short level.",
            "This is.",
            "Playing with some delay because of the network, but you will see here.",
            "Playing.",
            "Just a single sort of.",
            "Of a lecture video.",
            "This is done by exploiting the media fragment specifications that you will most probably hear about in the next presentation.",
            "So we are playing just a single sort which has been automatically detected, but we don't have a video file with just this sort.",
            "In the in the background we have single video for the whole hour, long single file for the whole hour long video.",
            "But we can play the source and what we can see here are also some results of automatic concept detection, so we can see.",
            "Some of the concepts that have been detected as being relevant to this sort in descending order of the degree of confidence that the algorithm has in these constants being present in the salt, and we can see these degrees also here.",
            "So this is this is an example and we can.",
            "See by just clicking on this on any of these bars.",
            "If the network responds, then we see.",
            "On in this window.",
            "The set of video video sorts in our collection that actually relate to this this concept in descending order again of the degree of confidence.",
            "And if we look at all the shots we have in their collection.",
            "Then we see that despite this concept detection being imperfect, we can actually have on the top of the sorted list sorts that are in most cases related to this concept.",
            "So they sold the lecture, and when we go down this list.",
            "If the page loads, we will see that we have with the degrees of confidence close to 0.",
            "All the thoughts in our collection that do not relate to this concept.",
            "You will have to.",
            "Trust me about this because the page doesn't seem to load, but the point is that when it comes to using concept detection for retrieval.",
            "Although it's not perfect.",
            "It makes sense to use it even with the current state of technology.",
            "We can also see here an example of how the video has been segmented into sorts.",
            "So here we see single video.",
            "And one key frame pursuit for this video and the ordering, of course, is the ordering of the sorts in the video the temporal order.",
            "So this is a sort based representation of this video.",
            "Again, you can see how different sorts have been correctly identified, because here we have obviously Electra sort and then we have a sort of the slides and then Electro sword again and so forth and using also the concept detection results we can separate different kinds of.",
            "Visual content that appears in this video, so get just the slides, for instance."
        ],
        [
            "So some additional reading on concept."
        ],
        [
            "Detection and then we move onto event detection.",
            "What is the objective here is the objective is to also detect events, so the kind of concept level annotation that we have with the previous set of techniques extended by also including some event event labels to this set of labels and the idea here is that so far we know how this image detect concepts like hand Sky see both whatever what we want now is also to detect a higher level event like we're facing in some place."
        ],
        [
            "These are a few examples of events of meaningful events that one would like to detect.",
            "This come from the trackbed multimedia event detection task which works on this problem and these are three indicative events that have been used for evaluating the systems.",
            "Among many other events, of course, in the past couple of years."
        ],
        [
            "How do we perform event detection?",
            "There are two main ways to do this.",
            "One is to treat it the same way we treat the concept detection problem.",
            "So start with a low level features.",
            "On the one hand, we have the event labels.",
            "On the other hand, and we try to train a machine learning approach that directly finds a mapping between those.",
            "These low level features and the event labels that we have.",
            "This is the first one.",
            "The second approach is to use a model vector based approach which is using the concept detection results as a stepping stone for performing event detection.",
            "So in this case we have a set of concept detectors and there are a lot of works on deciding how to choose concepts for this task and how to train the concept detectors as we saw in the previous slides.",
            "So from the low level features you go to detecting concepts with the usual machine learning techniques that we've seen, and then you get a set of responses from the concept detectors and this is used again as a feature for having another.",
            "Another set of machine.",
            "Another step of machine learning that tries to map these concept detector responses to the event labels.",
            "And of course we have hybrid approaches that combine.",
            "Combine the both of them.",
            "In general, model vector based approaches are very interesting for two reasons.",
            "The one is that they've been shown to perform a little bit better than when working directly with the low level features.",
            "The other is that when you work with this kind of approach, you can also do event recounting so you can go back when you decide on an event label.",
            "You can also go back to the features, which in this case is the concepts that led you to this event detection.",
            "And refine the concept detection results that you have to find even more meaningful concepts that are consistent with the event that you have detected."
        ],
        [
            "And this is.",
            "The basis of the media mixer promoted approach for event detection, so we have, of course, temporal video segmentation at first 2 sorts.",
            "This is baseline for all the processing.",
            "Then we have low level feature extraction which is based on again local features and bug of words.",
            "We apply a set of trained visual concept detectors which typically are SVM based detectors and we have constants in the order of hundreds.",
            "So a few 100 concepts.",
            "These detectors may be seemingly relevant to the South events, but then we use this vector of responses of these concept detectors as a feature.",
            "As a video representation that is of this kind.",
            "So for every concept we have one score and this is the input to another step of machine learning."
        ],
        [
            "Machine learning in this case.",
            "Be again another support vector machine classifier, but also since we have many noisy concept detectors and many relevant ones, those events that we're looking for, it makes sense to 1st do some dimensionality reduction, so we discard.",
            "The unnecessary dimensions of this feature vector and then possibly do possibly apply, and SVM classifier or even do some simpler nearest neighbor matching to decide on the event.",
            "And we have developed several approaches in this direction.",
            "Some new discriminate analysis algorithms that exploit the inherent sub subclass.",
            "Nature of the data and this is important for being able to discard those concept detection results that do not make sense for this particular event detection task and use just the rest to perform the actual event detection."
        ],
        [
            "So I will not tie you with detail."
        ],
        [
            "Of this what I can show you is the differences between using the traditional learning approach of having linear SVM classifiers as the second layer of learning.",
            "So this one is having the model vectors and then applying linear SVM on them to detect events.",
            "And this is when you use an intelligent dimensionality reduction approach combined with.",
            "Linear SVM so you can see that we can have significant gains in performance which shows that.",
            "The type of machine learning that you choose for for this problem, and also for any other problem, is very important.",
            "It's it's not just the features that you use that make a significant difference."
        ],
        [
            "Some general conclusions and hints on event detection.",
            "So the performance of the event detection system increases by discarding irrelevant or noisy concept detections.",
            "This is what our mixer, subclass, discriminant analysis or any other discriminant analysis technique we do.",
            "Effectively combining multiple classifiers is important.",
            "So we use multiple classifiers per concept, not just one.",
            "And exploiting the subclass structure of the event data is again important because this is what we have with events.",
            "They are typically constructed by subevents and this is a structure that we want to exploit.",
            "The low level features of course, are also important, and several other studies have shown that when it comes to event detection, it is the motion features that are the most important out of all the different types of low level features that we can use followed in decreasing.",
            "Order of importance by static visual features such as SIFT and the like.",
            "And then for some events, the audio features do provide complementary information for other events.",
            "They do not really contribute to improve detection.",
            "There is a problem with using of course motion features in large scale video and attention problems.",
            "It's the cost of extracting these features.",
            "We also solve this briefly when it comes when it came to concept detection.",
            "So static visual features they may be not as effective, but they're much much faster to compute than typical motion features in video.",
            "And then combining low level features and model vectors is always an important approach.",
            "Something to remember in event detection because it always gives you some performance gains."
        ],
        [
            "Some additional reading."
        ],
        [
            "And we move on to the last section of the presentation, which has to do with the creation of spatio temporal fragments and and also the annotation of them by means of object re detection.",
            "What do we mean here?",
            "We mean to find instances of a specific object within a single video or within a collection of videos.",
            "So we have an object of interest.",
            "We know that it is this particular bicycle or this particular painting.",
            "We have an annotation for this.",
            "This is.",
            "My bicycle or.",
            "You know, Lyndon's bicycle or whatever, and we want to detect in the video.",
            "And localize spatially and temporally, not just any bike, any any instance of that concept, but this particular instance, this particular object.",
            "And this leads us to the creation of spatial temporal fragments.",
            "And if we have an annotation for the object that we start with, if we know that it's my bike, then this annotation can of course be propagated to these spatial temporal fragments."
        ],
        [
            "The related work in this area.",
            "This is a typically an object matching problem, so there there's lots of work here with primarily was surprised.",
            "Again, local features such as Sift.",
            "This is, I think, the original application of safety as well or is close to the original application of safety.",
            "And then we also it's typical also to do some geometry based evaluation of the matching between local descriptors so that you make sure that you're talking about the same object and not just similar features but geometrically ordered in a completely different way.",
            "So there are various extensions to this baseline, which for instance include the combined use of key points and motion information.",
            "So add some motion there.",
            "Using the bag of words representation for pruning, the set of possible matches, and then do the more expensive.",
            "Local feature matching and geometric verification with just a subset of the original frames or using graph matching approach."
        ],
        [
            "Roaches are several options, so the media mixer promoted approach is based on starting from the baseline of using local features and doing a matching, but we try to improve both the detection accuracy and reduce the processing time and this we do by exploiting the power of modern graphics processing units for doing parts of the processing, exploiting the structure of the video for the sampling of frames, and this is.",
            "One yet another point where we exploit the results of automatic segmentation of the video to sorts in order to make much faster the object detection and we enhance robustness to scale variations of."
        ],
        [
            "Short object.",
            "So this is how we exploit the structure.",
            "Of the video, the automatic segmentation, so we have the video.",
            "It is automatically fragmented into a number of sorts and then for every sort we extract just a few keyframes and we start by doing the matching.",
            "Looking for the for the object of interest in these selected keyframes.",
            "If there is no detection there, then we don't bother to try to process all the other frames of that sort, but instead we just move to the next sort and the next representative keyframes the next sort.",
            "And if and only if there is a detection there, we move on.",
            "To process the rest of the frames of the shot.",
            "Since typically an object of interest doesn't appear on an entire video, but in some specific parts of that video, this approach is very effective for reducing the computational complexity of object detection.",
            "And if the network works, I will show an example of doing this online."
        ],
        [
            "So another problem we have, we have significant scale variations of the object in the video, so a maybe the object that we're looking for.",
            "But B&C, so how this object may appear in the video?",
            "Maybe it's particularly zoomed in and only a small part of this object is visible.",
            "Or in other cases we have a more distant view and this whole object is just.",
            "Very small part of the frame.",
            "So we try to address this by preprocessing the original image so when someone the user says that this is the object I'm looking for, we automatically generate a zoomed in instance by taking the center part of that frame and also assumed out instance and then we use all these three images for doing the matching and matching with any one of them.",
            "Means for us at detection of the object that we're looking for.",
            "This helps us.",
            "Detect more effectively.",
            "Zoomed in or zoomed out instances of the object."
        ],
        [
            "In the video.",
            "These are a few examples of objects that user may be looking for and this again relate to one of the shows that Lindon mentioned in his introductory talk, the.",
            "Antique roadshow so."
        ],
        [
            "We can see some results here where we can see that we can have very very high precision and recall.",
            "I should mention here that these figures depend alot on the choice of objects that we have.",
            "So typically when you work with.",
            "Kind of 2D objects that are complex enough, for instance paintings.",
            "Then you have a nice set of information to work with and we can have very accurately detection there when it comes to either simpler objects or when it comes to objects that can have many different views.",
            "For instance, a car where the front view of the car and the side view in the rear view are completely different.",
            "Then it is much more challenging to accurately detect these instances.",
            "And in the case of such objects, we would get significantly lower precision and recall.",
            "In many cases, the algorithm is quite fast, so about 10% or even less of the videos duration for performing their detection, and this process includes the whole processing of the video, so also the.",
            "Extraction of the local features and the matching.",
            "Of course with the exception of such segmentation.",
            "This is yet another example of what we're looking for, and this is what I described as a 2D object.",
            "It's a painting, and this is how it has been automatically re detected the green rectangles in this case indicate correct detection, so we were able to detect the object when we had zoomed in instances of it with just parts of the original object being visible when we had some partial occlusion with the hand here.",
            "Or when we had more distant and also distant with occlusion views of the same painting.",
            "Here's another example, but this is a much more challenging object, so here if we were given just this image.",
            "It would be much more difficult to detect the object when it is rotated and is shown from a different view.",
            "What we do here is we exploit multiple instances of the original object and we use all of them for doing the matching and this helps us again in correctly detecting re detecting this object in the video despite the viewpoint variations."
        ],
        [
            "So some confusions accurately.",
            "Detection of objects is possible.",
            "As I already commented on the choice of objects plays a significant role.",
            "It is possible to do it significantly faster than real time when exploiting the video structure.",
            "And of course the GPU, but I didn't go into details of what processes we speed it up using the GPU.",
            "And there are several possible uses for of this kind of technology.",
            "One is instance level annotation as I mentioned, if we have.",
            "An image of my bike and we have the annotation that it's my bike.",
            "Then we can propagate this label throughout the video by doing re detection of these objects.",
            "Is finding linking related videos or fragments of them?",
            "So if we have a collection of videos and we can re detect a specific object in five of them, then we know that these videos are related and how are they related?",
            "They at least have the same object, the same specific object appearing in them and this is an indication that the user.",
            "For instance, interested in one video may be interested in the others as well, and we can support with this with these results.",
            "Other analysis task as well so seen detection.",
            "For instance when we have a set of sorts in a video and we know that some of these sorts do contain the same object, then this is yet another cue that these sorts belong to the same scene similarly to the audio events or the concept detection results that we exploited for same detection.",
            "And let's see a demo again.",
            "So this is.",
            "Another video.",
            "Which plays.",
            "We can stop it.",
            "We can select the object.",
            "That we are looking for, so let's make it this object this painting.",
            "And we say we detect this.",
            "So processing processing processing.",
            "And in a few seconds.",
            "We start seeing the results of every detection.",
            "This algorithm in this case runs online.",
            "So as we speak, it performs the.",
            "Feature extraction of the video and then the matching with the object that we just marked.",
            "And here we have a time bar with the video.",
            "Red means that the video this part of the video hasn't been processed yet, so it will be.",
            "Changing colors soon.",
            "Blue indicates the sorts where we have detected that the object is present and Gray is the set of sorts where we have detected that this particular object doesn't appear and we can move on and say for instance play play out this sort.",
            "If the network works.",
            "In the meantime, you see that the video continues to be processed online and.",
            "The object is re detected.",
            "So this is an example of how it works, and I invite you to also play with these online demonstrators.",
            "Both this one and the previous one with the concept detection results.",
            "They are online and you can.",
            "You can work with them.",
            "You can.",
            "Play with them.",
            "OK."
        ],
        [
            "Some additional reading on object read text."
        ],
        [
            "And for those interesting and some concluding remarks on the whole.",
            "Talk so we discuss different classes of techniques for media fragment creation, annotation there of course several others there.",
            "There's object recognition, there's face detection, tracking, clustering and recognition.",
            "There's quality assessment, so visual quality assessment is another.",
            "Analysis process that may be important in some applications to identify high quality fragments from low quality fragments in terms of the artifacts that appear in these parts of the video, there's sentiment and emotion detection is yet another technique for enriching the annotations of the video, not just concepts, not just events, but also the sentiments or emotions.",
            "An important thing to keep in mind is that not all of these possible analysis techniques.",
            "Are suitable for every possible problem.",
            "So when we have a problem in hand and we need to process a set of videos, we need to think carefully of what kind of techniques, what kind of classes of techniques it makes sense to apply and also when it comes to specific class of techniques like for instance concept detection, what kind of specific technique out of all the possibilities for concept detection we should apply to this particular content?",
            "And of course, understanding the problem and the volume of data that we have.",
            "The value of the data and the variability of the data is key to selecting appropriate methods when we have, for instance, the domain specific problem, we know that the videos will just be news videos.",
            "Then there are lots of concepts that it doesn't make sense to try to detect because they will never appear in these videos when it comes to processing unconstrained user generated content content, on the other hand, we may need a much richer set of concepts.",
            "And in most, in some or most cases, the automatic analysis results remain far from perfect.",
            "But in many cases they're still useful in the right domain or for solving the right problem.",
            "For instance concept based video retrieval."
        ],
        [
            "And with these concluding thoughts, thank you for listening to this talk and please.",
            "Ask any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm with the Information Technologies Institute of the Center Forest and technology allow us, which is located in Thessaloniki, Greece and I will talk today about media fragmentation and annotation technologies.",
                    "label": 0
                },
                {
                    "sent": "So the start point of everything, how we fragment the media and how we start annotating it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The overview of the presentation is as follows.",
                    "label": 0
                },
                {
                    "sent": "Will have first an introduction and overall motivation of this this work.",
                    "label": 0
                },
                {
                    "sent": "Then we will discuss in more detail some technologies for media fragment creation, annotation and these technologies include the temporal segmentation of video to sorts.",
                    "label": 1
                },
                {
                    "sent": "The further temporal segmentation of it to scenes.",
                    "label": 1
                },
                {
                    "sent": "Visual concept detection, which is 1 first step to annotating the videos.",
                    "label": 0
                },
                {
                    "sent": "Event detection which is enriching.",
                    "label": 0
                },
                {
                    "sent": "These concept annotation with also event labels.",
                    "label": 0
                },
                {
                    "sent": "And object re detection will also see some demos of technologies for this kind of processing and for each of the presented technologies will go through somewhat more precise problem statements.",
                    "label": 0
                },
                {
                    "sent": "So what exactly do we want to do when we say, for instance, temporal segmentation to sorts will have a brief overview of the literature in this particular area, then a closer look to one or two media mixer promoted approaches for doing this, some indicative experiments and results to get an idea of.",
                    "label": 1
                },
                {
                    "sent": "How good and how mature its technology is, and then we will have a few conclusions and some references for additional reading.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, motivation.",
                    "label": 0
                },
                {
                    "sent": "What do we have?",
                    "label": 0
                },
                {
                    "sent": "We have media items which can be long videos, for instance, or sets of images or long text.",
                    "label": 1
                },
                {
                    "sent": "And what do we want to do?",
                    "label": 0
                },
                {
                    "sent": "We want to enable fine grained access to the media and particularly discussing video.",
                    "label": 1
                },
                {
                    "sent": "We want to enable fine grained access to the video.",
                    "label": 0
                },
                {
                    "sent": "So what do we need to do?",
                    "label": 0
                },
                {
                    "sent": "We need to break down its media item to meaningful fragments and then of course we want to annotate it's fragmente so that we can make it searchable as some of the previous.",
                    "label": 1
                },
                {
                    "sent": "Speakers in the introductory presentations showed we want for instance, to detect dogs that there is a dog there, despite the fact that this is not already part of the videos file name or some tags that have been added to it.",
                    "label": 0
                },
                {
                    "sent": "So this fragmentation ordination there are two approaches, two general ways to go for this.",
                    "label": 0
                },
                {
                    "sent": "One is to do it manually and the other is do it with automatic techniques.",
                    "label": 0
                },
                {
                    "sent": "When we do it manually we gain of course inaccuracy because we can do it very very accurately.",
                    "label": 0
                },
                {
                    "sent": "If we want to.",
                    "label": 0
                },
                {
                    "sent": "But the downside is that speed, cost and effort required for doing this are sometimes prohibitive, particularly when we talk about large volumes of video.",
                    "label": 0
                },
                {
                    "sent": "So doing manual fragmentation, for instance, annotation is something that is probably feasible when we talk about high value content.",
                    "label": 0
                },
                {
                    "sent": "Which is not of so much so high volume.",
                    "label": 0
                },
                {
                    "sent": "For instance Hollywood movie.",
                    "label": 0
                },
                {
                    "sent": "There are millions and millions of euros spent for producing the movie, so doing the extra work of fragmenting it into a certain scenes and adding annotations is something that is visible.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when we talk about user generated content, so the videos that we saw in the introductory presentation that are uploaded to YouTube every minute or the videos that are included in one of the leading.",
                    "label": 0
                },
                {
                    "sent": "Archives then this manual annotation is of course prohibitive and we have to go for automatic techniques where we lose in accuracy.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we will see about this will talk about this in the sequel, but for so we gain in speed and the effort that is required for doing so.",
                    "label": 0
                },
                {
                    "sent": "So this makes it feasible.",
                    "label": 1
                },
                {
                    "sent": "Media mixer, as also Lyndon sold in his store, promotes technologies for the automatic fragmentation annotation of video content, and.",
                    "label": 0
                },
                {
                    "sent": "As I already showed, we will discuss certain scene detection for for temporal fragmentation of video.",
                    "label": 0
                },
                {
                    "sent": "The annotation of it with visual concepts and events, and some instance level annotation.",
                    "label": 0
                },
                {
                    "sent": "Also object detection, their detection of known objects.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Starting with Fragmente creation and sorts, we first have to define what a sort is.",
                    "label": 0
                },
                {
                    "sent": "This is a sequence of consecutive frames taken without interruption by a single camera.",
                    "label": 1
                },
                {
                    "sent": "So sought, segmentation means finding the boundaries of these changes between between salts.",
                    "label": 1
                },
                {
                    "sent": "And this is the foundation of most high level video analysis approaches.",
                    "label": 0
                },
                {
                    "sent": "So when we do when we want to do annotation, for instance, we first have to segment the video into sorts, because different sorts may so many many different things, so it doesn't make sense to have a list of concepts that appear in a very long video.",
                    "label": 0
                },
                {
                    "sent": "It makes sense to break it down first into fragments into sorts and then identify the few concepts the few objects or events that relate to this particular fragment.",
                    "label": 1
                },
                {
                    "sent": "So, so change is manifested by shifting the visual content.",
                    "label": 0
                },
                {
                    "sent": "We don't need to look at other modalities, it's just the visual one.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is for instance an example of a sort change we have here a few frames that are obviously taken without interruption.",
                    "label": 1
                },
                {
                    "sent": "And then there's an interruption.",
                    "label": 0
                },
                {
                    "sent": "The visual content suddenly changes and this is a sort change.",
                    "label": 1
                },
                {
                    "sent": "This is an abrupt so changed by the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Way and we can have also gradual changes where again the visual content changes but it changes over a period of time or over a sequence of frames.",
                    "label": 1
                },
                {
                    "sent": "And this is for instance that dissolve where we can see that the two completely different visual contents they blend at some point and one changes to the other.",
                    "label": 1
                },
                {
                    "sent": "And we can have other types of graphs.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transitions as well, such as a wipe so.",
                    "label": 0
                },
                {
                    "sent": "This again gradually changes to a completely different visual content, but in a different way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also fade instant fade outs where the first sort fades into a white or black frame and then a new one gradually comes in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the challenges here?",
                    "label": 0
                },
                {
                    "sent": "The challenge is to detect these changes, not just the abrupt, but also the gradual ones, but without being misled, misled by other artifacts that appear in the video.",
                    "label": 0
                },
                {
                    "sent": "So without being misled by illumination changes which can be caused for instance by camera flash slides without being misled by fast camera movement or any object movement, any local local motion.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we have a few examples again.",
                    "label": 0
                },
                {
                    "sent": "So here we have a flashlight.",
                    "label": 0
                },
                {
                    "sent": "You can see that it's obviously a set of frames taken without interruption by a single camera, but depending on the technique used for evaluating the differences between shots between frames, if you take a histogram of this frame and a histogram of this one, then you will see a significant difference.",
                    "label": 0
                },
                {
                    "sent": "So you have to choose the right way of comparing frames to make sure that you are not.",
                    "label": 0
                },
                {
                    "sent": "Affected by this kind of artifacts and.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, for camera movement where we can see here that the content of the sort changes in time because of the camera changing view.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also because of local motion.",
                    "label": 0
                },
                {
                    "sent": "So we have a couple of people there and a car occludes them temporarily and this change could be mistaken as a sort change.",
                    "label": 0
                },
                {
                    "sent": "So we have to make sure that our methods don't have this problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The related work to sort segmentation can be generally organized according to the data that you work with.",
                    "label": 1
                },
                {
                    "sent": "So whether you work with uncompressed data or with compressed video directly, the features that you use, which also depends of course on what kind of data you have to work with.",
                    "label": 0
                },
                {
                    "sent": "If you work with the roll video or not, and we have the techniques algorithmically being organized mainly into threshold based techniques and learning based techniques.",
                    "label": 0
                },
                {
                    "sent": "So starting with compressed video segmentation methods, this reduce the computational complexity of the processing by avoiding the full decompression of the video so they play with features such as the DC coefficients of the city, coded video or the motion vectors in MPEG video.",
                    "label": 1
                },
                {
                    "sent": "Also, they use macroblock information of specific frames, so if the blocks are intra coded or skipped or Inter coded, this can indicate the temporal continuity of the frames and therefore can be used as a hint for finding times the location in time where there's a short change.",
                    "label": 0
                },
                {
                    "sent": "Generally compressed video methods are very very fast but.",
                    "label": 0
                },
                {
                    "sent": "For a little bit lower accuracy as compared to uncompressed video methods.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in uncompressed video there is much much more work in terms of segmentation.",
                    "label": 0
                },
                {
                    "sent": "There are many techniques that range from pairwise pixel comparisons and the comparison of global histograms to other techniques that even use local visual features or bag of words or even even do actually matching of local features along frames to find differences.",
                    "label": 1
                },
                {
                    "sent": "These features and the differences of them along time you can evaluate either by using thresholds, which is the simplest approach, but thresholds are often hard to tune.",
                    "label": 1
                },
                {
                    "sent": "Or you can use the machine learning techniques such as support vector machines, for instance, where you learn from a training set and then you can evaluate the differences of these features in in an unprocessed video.",
                    "label": 0
                },
                {
                    "sent": "In general, as we will see later on, also we can get high detection accuracy and relatively low computational load even when working with uncompressed data.",
                    "label": 1
                },
                {
                    "sent": "So this is the preferred approach in most applications.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are a few more details about the media Mixer proposed approach.",
                    "label": 0
                },
                {
                    "sent": "This is a learning based approach, so we use an SVM to evaluate the differences of features.",
                    "label": 0
                },
                {
                    "sent": "The features that we extract for from every keyframe are three global global image features which are called color coherence.",
                    "label": 0
                },
                {
                    "sent": "The Macbeth color histogram and aluminum center of gravity.",
                    "label": 1
                },
                {
                    "sent": "So we use these to four feature vectors for every frame and then we compare the differences of feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Four pairs of frames, either consecutive or non consecutive.",
                    "label": 0
                },
                {
                    "sent": "And then this distance vector is an input to an SVM classifier which identifies which decides on whether we have a short boundary or not.",
                    "label": 0
                },
                {
                    "sent": "At every given.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time.",
                    "label": 0
                },
                {
                    "sent": "An extension of this can also be.",
                    "label": 0
                },
                {
                    "sent": "Used to alleviate the impact of flashes in the video so we can do a simple post processing of of the SVM outputs when we have flashes in a news video.",
                    "label": 0
                },
                {
                    "sent": "These typically come in in a sequence in a very short time.",
                    "label": 0
                },
                {
                    "sent": "You have several flashes, so even a simple post processing of the out of the SVM classifier can, so the parts of the video where we have.",
                    "label": 0
                },
                {
                    "sent": "A set of flashlight so we can detect this pattern in the output of the SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "But of course there are many other ways we can deal with this problem.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So typically we evaluate the sort segmentation using precision.",
                    "label": 0
                },
                {
                    "sent": "Recall and the F score Precision is the fraction is defined as the fraction of detected sorts.",
                    "label": 1
                },
                {
                    "sent": "That's correspond to actual source of the video.",
                    "label": 1
                },
                {
                    "sent": "So how many of the detections were correct?",
                    "label": 0
                },
                {
                    "sent": "And recall is the fraction of actual source of the videos that have been successfully detected, so it's how many of the actual source we have there were detected and how many were missed.",
                    "label": 1
                },
                {
                    "sent": "Higher values are of course better in this case for both measures, so the optimal would be to have 100%.",
                    "label": 0
                },
                {
                    "sent": "Here we can see some figures of the media mixer promoted approach and we can see that we have precision recall around 90%, which is on a par with the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Approaches in this area.",
                    "label": 0
                },
                {
                    "sent": "And our media mixer proposed media mixer promoted method runs in about real time, so it's a little little bit slower than real time.",
                    "label": 1
                },
                {
                    "sent": "Real time is identified as the videos actual duration.",
                    "label": 0
                },
                {
                    "sent": "So if we have a one hour video this is it takes about one hour or a little over one hour to do this on a single core.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "See a short demo of this.",
                    "label": 0
                },
                {
                    "sent": "So this is a video where we have done.",
                    "label": 0
                },
                {
                    "sent": "We have run the sub segmentation and whenever you sort these detected we just print and you sort label as a subtitle.",
                    "label": 0
                },
                {
                    "sent": "To this to this video.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "In Short, 2 S one was the starting and here we change to another sort so.",
                    "label": 0
                },
                {
                    "sent": "An abrupt transition in this case was detected successfully.",
                    "label": 0
                },
                {
                    "sent": "Then we can see our method is not sensitive to camera motion and local object motion.",
                    "label": 0
                },
                {
                    "sent": "So here you can see that the visual content changes significantly but still it is a single sort.",
                    "label": 0
                },
                {
                    "sent": "It is a set of frames taken without interruption and we can correctly detect this.",
                    "label": 0
                },
                {
                    "sent": "Then we had another change here.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we need to detect.",
                    "label": 0
                },
                {
                    "sent": "The changes the short changes but still be able to.",
                    "label": 0
                },
                {
                    "sent": "Not detect aronis so changes when we have this kind of motion and this is important for applications.",
                    "label": 0
                },
                {
                    "sent": "By the way, you can see that there is a significant similarity.",
                    "label": 0
                },
                {
                    "sent": "In the content of these sorts.",
                    "label": 0
                },
                {
                    "sent": "And this will lead us to the next part of the presentation, which has to do with temporal segmentation to scenes.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the presentation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some conclusions about the shot detection before we proceed to see segmentation so the overall accuracy of South detection methods is high.",
                    "label": 1
                },
                {
                    "sent": "It's about 90%.",
                    "label": 1
                },
                {
                    "sent": "This is sufficient for any application.",
                    "label": 0
                },
                {
                    "sent": "Any subsequent processing of the video.",
                    "label": 0
                },
                {
                    "sent": "The detection of gradual transitions and the handling of intense local and global motion is still a bit more challenging, so abrupt transitions we can very easily detect.",
                    "label": 1
                },
                {
                    "sent": "But when we have.",
                    "label": 0
                },
                {
                    "sent": "Long gradual transitions.",
                    "label": 0
                },
                {
                    "sent": "We can be sometimes misled and the same holds for motion.",
                    "label": 0
                },
                {
                    "sent": "Intense motion can sometimes lead to the detection of erroneous changes.",
                    "label": 1
                },
                {
                    "sent": "And real time or near real time processing is visible, but of course much faster processing may be needed in some applications.",
                    "label": 0
                },
                {
                    "sent": "So when it comes for instance to the user generated content uploaded to YouTube.",
                    "label": 0
                },
                {
                    "sent": "We saw that it is a lot of content per minute.",
                    "label": 0
                },
                {
                    "sent": "So what one would probably choose in this kind of application would be a compressed domain method, which can be like 100% fast 100 times faster than real time.",
                    "label": 0
                },
                {
                    "sent": "It may lose a bit in accuracy, but would be able to handle this kind of volume of content more effectively.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some additional reading on short detection.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we move on to the detection of since the creation of longer temporal fragments.",
                    "label": 0
                },
                {
                    "sent": "So what is a scene as seen as a higher level temporal video segment which is elementary in terms of the semantic content covering either a single event or several related events taking place in parallel.",
                    "label": 1
                },
                {
                    "sent": "This means that the scene is a longer is typically a longer chunk of video than assault.",
                    "label": 0
                },
                {
                    "sent": "It is not about just detecting visual visual changes in this case.",
                    "label": 0
                },
                {
                    "sent": "It is about detecting changes in the topic of the the the meaning that is conveyed and this makes it also more more tricky.",
                    "label": 0
                },
                {
                    "sent": "So this is of course again important for subsequent applications such as summarization, indexing and video browsing.",
                    "label": 0
                },
                {
                    "sent": "It is important in many cases when it comes to video hyperlinking where you want to use scenes in many cases as the as the elementary.",
                    "label": 0
                },
                {
                    "sent": "Unit of video.",
                    "label": 0
                },
                {
                    "sent": "Because the scene is meaningful on its own, whereas assault is sometimes too short to be meaningful on its own.",
                    "label": 0
                },
                {
                    "sent": "And what is important when it comes to processing the video is that the same change is not manifested by just changing the visual content, so it is not as easy to detect them as it was with swords.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here for instance, in this example again, here we have obviously assault change because the visual content of the video changes.",
                    "label": 0
                },
                {
                    "sent": "But what do we know about the scene?",
                    "label": 0
                },
                {
                    "sent": "Is it the same change or not?",
                    "label": 0
                },
                {
                    "sent": "Is it about the same topic or is this some event in Vancouver and there's some demonstration in Greece we don't know by just looking at the frames and This is why scene segmentation is more challenging.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic assumptions that are typically made when it comes to seem segmentation is that first of all assault cannot belong to more than one scenes, so we assume that if we have shot segmentation 1st and then we try to group the sorts then we can successfully detect since we don't miss any scene boundaries.",
                    "label": 1
                },
                {
                    "sent": "This is an assumption that holds in most cases, not always so we can have examples with news videos.",
                    "label": 0
                },
                {
                    "sent": "With this assumption, doesn't hold, there's a single.",
                    "label": 0
                },
                {
                    "sent": "Anchorperson, for instance talking about many different events in a single shot, but generally speaking, this is an assumption that is typically made.",
                    "label": 0
                },
                {
                    "sent": "So, since segmentation based on this assumption.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is most often performed by just detecting the salt changes so we have some sorts automatically identified and then.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Group these shots into one or more scenes.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of related work, this can be generally organized according to the data that we work with, so we have unique model versus multimodal approaches.",
                    "label": 1
                },
                {
                    "sent": "The dependence or not on specific domain knowledge, and the domain of choice.",
                    "label": 1
                },
                {
                    "sent": "So do we want to do scene segmentation for generic video or are we working specifically with news videos or documentaries?",
                    "label": 0
                },
                {
                    "sent": "This makes a lot of difference in the approach we use and also in the algorithms that we can use.",
                    "label": 0
                },
                {
                    "sent": "You model methods starting with the type of information that we use in modern methods.",
                    "label": 0
                },
                {
                    "sent": "As the name suggests, use just one type of information, typically visual cues, so they come as an extension of visual sort segmentation, whereas multiple ones combined visual cues with audio, speech transcripts, and other kinds of information.",
                    "label": 1
                },
                {
                    "sent": "As we will see in the sequel.",
                    "label": 0
                },
                {
                    "sent": "Domain versus domain domain specific versus domain independent approaches.",
                    "label": 0
                },
                {
                    "sent": "There's no need to explain here when it comes to having domain specific approaches.",
                    "label": 0
                },
                {
                    "sent": "What is important is to.",
                    "label": 0
                },
                {
                    "sent": "Knowledge about the specific domains to exploit, for instance, knowledge about the structure of news and this helps us to do a scene segmentation more effectively.",
                    "label": 1
                },
                {
                    "sent": "And there are many algorithmic approaches to solving this problem.",
                    "label": 0
                },
                {
                    "sent": "Some are graph based.",
                    "label": 1
                },
                {
                    "sent": "For instance in transition graph, others are more clustering based.",
                    "label": 0
                },
                {
                    "sent": "For instance using hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "There are statistical methods and many others.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's some media mixer promoted approach.",
                    "label": 0
                },
                {
                    "sent": "This is based on the same transition graph which was proposed by Young Yoann you in 1998.",
                    "label": 1
                },
                {
                    "sent": "The idea here is that we have the different sorts we have identified them.",
                    "label": 0
                },
                {
                    "sent": "We can represent them with one or more keyframes.",
                    "label": 0
                },
                {
                    "sent": "We have, of course, the temporal ordering of the sorts, because this is also known and then based on visual similarity we do a grouping of the sorts into clusters.",
                    "label": 0
                },
                {
                    "sent": "Also taking into account.",
                    "label": 0
                },
                {
                    "sent": "Their temporal proximity, so we have sorts.",
                    "label": 0
                },
                {
                    "sent": "1:15 and 1:20, which are obviously similar in visual content, being part of 1 cluster, then again so it's 100 and 16118 for the same reason, and another couple of sorts.",
                    "label": 0
                },
                {
                    "sent": "Then 121 is left alone.",
                    "label": 0
                },
                {
                    "sent": "And this leads us to the creation of this kind of graph, where the nodes of the graph are the clusters that we have identified based on the visual similarity and the edges of the graph indicate the temporal succession of the nodes based on the temporal succession of the swords that are included in these nodes and then scene segmentation is comes to detecting cut edges which are edges which if.",
                    "label": 0
                },
                {
                    "sent": "Removed if a single one of them is if a single edge is removed then this graph ends up being two disconnected graphs.",
                    "label": 1
                },
                {
                    "sent": "So identifying cut edges essentially gives us the scene boundaries.",
                    "label": 0
                },
                {
                    "sent": "In this case, this is a method that was originally proposed doing this clustering by just looking at the visual similarity of sorts.",
                    "label": 0
                },
                {
                    "sent": "So just low level feature similarity evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What media Mixer promotes is an extension of this approach that is built so as to exploit multiple types of audio visual information.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that first we had a fast transition graph approximation that we introduced and this is many times faster than the original one and actually allows us to repeat the same transition graph transition and the construction and the detection of cut edges.",
                    "label": 0
                },
                {
                    "sent": "Many times in the order of thousands.",
                    "label": 0
                },
                {
                    "sent": "And then the approaches to do to create sets of sin transition graphs using different kinds of information.",
                    "label": 1
                },
                {
                    "sent": "So visual low level visual features as the original approach, but also using the output of visual concept detectors using audio features and using audio event detectors.",
                    "label": 0
                },
                {
                    "sent": "The output of audio event detectors.",
                    "label": 0
                },
                {
                    "sent": "These are four different modalities that we can exploit and then we create multiple skin transition graphs using any of these types of.",
                    "label": 0
                },
                {
                    "sent": "Features and we do a probabilistic merging of their results.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the evaluation of scene segmentation approach is typically done using the coverage and overflow measures, which are not so much different from precision and recall, but also take into account the temporal, overlapping or non overlapping of the segments that are produced.",
                    "label": 0
                },
                {
                    "sent": "So what we can see here is that we have.",
                    "label": 0
                },
                {
                    "sent": "I should mention that the objective the desire is to have a coverage of 100%.",
                    "label": 0
                },
                {
                    "sent": "This is the optimal value and an overflow of 0.",
                    "label": 0
                },
                {
                    "sent": "We can see that we get something like 75% of coverage and 20 or 25% overflow, which is reasonably good.",
                    "label": 0
                },
                {
                    "sent": "We have an F score in the 70s range.",
                    "label": 0
                },
                {
                    "sent": "The algorithm we have is very fast, it's just one.",
                    "label": 0
                },
                {
                    "sent": "1.5% of real time if you exclude the processes of feature extraction and this is a result of working with salts.",
                    "label": 0
                },
                {
                    "sent": "So we try to group shots and this means that we have very few few items within the video, few sorts and we can easily do this process.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is more interesting is this diagram, which shows the contribution of the different types of information to this process.",
                    "label": 1
                },
                {
                    "sent": "So what we have here is the F score on.",
                    "label": 0
                },
                {
                    "sent": "The vertical axis higher values are better and here we have the number of visual concepts that we use for the analysis.",
                    "label": 0
                },
                {
                    "sent": "The baseline is this straight line which is around 75% F score.",
                    "label": 0
                },
                {
                    "sent": "It's a straight line because this is using just the visual low level features, which means that it is not a function of the number of concepts that we use.",
                    "label": 0
                },
                {
                    "sent": "Then this curve here is.",
                    "label": 0
                },
                {
                    "sent": "How we do with using just the output of visual concept detectors when the this changes.",
                    "label": 0
                },
                {
                    "sent": "Of course, when the number of concept detector varies, we can see that generally speaking or in all cases we are below the baseline, But what's interesting is this dotted line with the axis.",
                    "label": 0
                },
                {
                    "sent": "This is what we get when we combine the two so we can see that combining the low level visual features and the output of constant detectors can give us a nice approximately 5% improvement.",
                    "label": 0
                },
                {
                    "sent": "And the other two lines on top of the other two curves are when we add to this set of features also the.",
                    "label": 0
                },
                {
                    "sent": "Audio the low level audio information.",
                    "label": 0
                },
                {
                    "sent": "And then the solid line on the solid curve on the top is when we also use the output of audio event detectors.",
                    "label": 0
                },
                {
                    "sent": "So when we use all four types of visual audio visual information that we have, so this, this shows how the different types of information can contribute to more accurate detection of the scenes.",
                    "label": 0
                },
                {
                    "sent": "And of course it also shows the important of audio, audio information and audio events there.",
                    "label": 0
                },
                {
                    "sent": "So you can have as an example.",
                    "label": 0
                },
                {
                    "sent": "You can have two shots that are very very different in terms of their visual content, but if the audio if background is similar and you can have for instance Rd traffic.",
                    "label": 0
                },
                {
                    "sent": "Sounds or classroom related sounds.",
                    "label": 0
                },
                {
                    "sent": "Classroom related audio events then these are cues that the two sorts, despite being visually very different, belong actually to the same scene.",
                    "label": 0
                },
                {
                    "sent": "They describe the same the same event maybe from completely different visual angles, but still the same.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A few conclusions about the scene detection.",
                    "label": 0
                },
                {
                    "sent": "So since segmentation is less accurate than the short segmentation we saw in the previous case, our results were in the order of 90%, but still the results are good enough for improving access to meaningful fragments in various applications.",
                    "label": 1
                },
                {
                    "sent": "And we've seen this with retrieval where retrieving scenes instead of retrieving salts helps you get more more useful information.",
                    "label": 0
                },
                {
                    "sent": "We've also seen it with video hyperlinking applications.",
                    "label": 0
                },
                {
                    "sent": "Where linking to scenes rather than linking to single shots helps the user to get more meaningful.",
                    "label": 0
                },
                {
                    "sent": "Video segments as as result of the linking process so it facilitates access more meaningful access to the video.",
                    "label": 1
                },
                {
                    "sent": "Using more than just low level visual features helps a lot as we saw in the previous diagram.",
                    "label": 0
                },
                {
                    "sent": "And what this be taking to be taken seriously is the choice between domain specific versus domain independent approaches so.",
                    "label": 0
                },
                {
                    "sent": "What we've seen in all our experiments is that when we have a domain specific problem, then using a domain specific approach as well that exploits knowledge about the structure of the video in this particular domain is very, very helpful.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some additional reading on scene detect.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now that we have seen how to create meaningful temporal fragments of the video, let's move on to discuss how we annotate them, starting with visual concept detection, what is the goal here?",
                    "label": 0
                },
                {
                    "sent": "Is having some visual content.",
                    "label": 0
                },
                {
                    "sent": "To come to a set of concepts that describe this content and some degree of confidence that we are right in this selection.",
                    "label": 0
                },
                {
                    "sent": "So not just a binary decision that we have here, hand and Sky and UFO, but also some degree of confidence, showing how confident the algorithm is that this concept actually appears in the video.",
                    "label": 0
                },
                {
                    "sent": "This is useful as you will see for also for subsequent analysis.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The related work.",
                    "label": 0
                },
                {
                    "sent": "This is a diagram showing more or less how most of the modern approaches to constant detection work, so starting with the visual information, which can be the.",
                    "label": 0
                },
                {
                    "sent": "Video sort itself, or some keyframes of that sort.",
                    "label": 0
                },
                {
                    "sent": "We have interest point detection description because most modern techniques work with local visual features.",
                    "label": 0
                },
                {
                    "sent": "Then we have the representation of these features using a bag of words or feature vectors, or a number of other techniques.",
                    "label": 1
                },
                {
                    "sent": "And then we have one or more machine learning based classifiers per concept that try to find a mapping between.",
                    "label": 0
                },
                {
                    "sent": "These low level feature representations and the concepts that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "So in order to build a competitive system for concept detection, one needs to use of course the.",
                    "label": 1
                },
                {
                    "sent": "Correct, kind of features which are typically rotation and scale invariant descriptors sometimes also color invariant, sometimes using actually color based features so that you get also the color information in this process.",
                    "label": 1
                },
                {
                    "sent": "State of the art representations of them such as feature vectors typically need to fuse multiple descriptors and concept detectors, so it's not a matter of choosing just one of these methods for description, it's a matter of combining multiple complementary descriptors and representation methods.",
                    "label": 0
                },
                {
                    "sent": "Exploiting concept correlations is important, so as human beings we know that sun and Sky, for instance, often appeared together.",
                    "label": 0
                },
                {
                    "sent": "This is something that we also need to take into account.",
                    "label": 0
                },
                {
                    "sent": "In building an algorithm for detecting the concepts and also to exploit temporal information in videos, because there are many studies which show that there is some form of temporal continuity also when it comes to the concepts that appear in the video.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of feature extraction, there are way too many things here to cover in this.",
                    "label": 0
                },
                {
                    "sent": "In this limited time, but in terms of feature extraction, there global and local visual features most of the time we use local ones such as.",
                    "label": 0
                },
                {
                    "sent": "Sift, surf and color variants of them.",
                    "label": 1
                },
                {
                    "sent": "There are many motion features, some of them are extensions of the local static features.",
                    "label": 0
                },
                {
                    "sent": "For instance steep or most sift, and there are many others, including using a text and audio.",
                    "label": 0
                },
                {
                    "sent": "Although these are for limited use compared to visual features.",
                    "label": 1
                },
                {
                    "sent": "For encoding these features into a meaningful descriptor, there are techniques such as pyramidal decomposition that you use for including some kind of.",
                    "label": 0
                },
                {
                    "sent": "Structure information into your representation.",
                    "label": 0
                },
                {
                    "sent": "There is a bag of words, feature vectors which are most recent extensions.",
                    "label": 0
                },
                {
                    "sent": "There are different ways to deal with the Fusion of multiple descriptors, so early Fusion versus late Fusion.",
                    "label": 0
                },
                {
                    "sent": "In terms of machine learning, there are also many options here.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines are the most usual choice, although there are other possibilities such as using multi label learning approaches.",
                    "label": 1
                },
                {
                    "sent": "And there are different ways of taking into account the correlation of constants.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We'll see here a couple of media mixer promoted approaches.",
                    "label": 0
                },
                {
                    "sent": "These are based on the typical concept detection pipeline that we saw in the previous slides.",
                    "label": 0
                },
                {
                    "sent": "The first one is.",
                    "label": 0
                },
                {
                    "sent": "Do you have two layers stacking architecture which means that we have a first layer where we build multiple independent concept detectors based on support vector machines.",
                    "label": 1
                },
                {
                    "sent": "This is the typical approach, but then we introduce a second layer of concept detection where we take as input the output of the first layer support vector machines.",
                    "label": 0
                },
                {
                    "sent": "We construct low dimensional model vectors so we aggregate the results of constant detection for a number of different concepts.",
                    "label": 1
                },
                {
                    "sent": "And then we use multi label learning to capture the correlations between these concepts and refine the output of the VM's of the first layer.",
                    "label": 0
                },
                {
                    "sent": "For the first layer, one additional media mixer promoted approach that we have is to use video tama graphs as well in addition to keyframes, and we'll see what this means.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So video tamograph.",
                    "label": 0
                },
                {
                    "sent": "So these are two dimensional slices of 3D video volume with one dimension in time and one dimension in space.",
                    "label": 1
                },
                {
                    "sent": "This is a 2D cross section of the video as well as we can see on these examples, but it is different from the static keyframes because keyframes have both dimensions in space.",
                    "label": 0
                },
                {
                    "sent": "Here we maintain one dimension in space and one in time, so it's a different cross section.",
                    "label": 1
                },
                {
                    "sent": "And these we used in the same way that the typical keyframes are used, so we process them with the whole pipeline of local local interest point detection description with SIFT or other variants of it and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 one technique that we used for introducing into the process some kind of temporal information information about the temporal evolution of the video signal.",
                    "label": 1
                },
                {
                    "sent": "But we do this without resorting to expensive special temporal features such as steep, which would require processing an awful lot of frames pursuit an would.",
                    "label": 0
                },
                {
                    "sent": "Increase the computational complexity of the concept detection approach.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can see some results here where we have the dama graphs.",
                    "label": 0
                },
                {
                    "sent": "This is across all all concepts.",
                    "label": 0
                },
                {
                    "sent": "I think this is the tama graphs alone.",
                    "label": 0
                },
                {
                    "sent": "The keyframes alone we can see that the key frames which are more meaningful.",
                    "label": 0
                },
                {
                    "sent": "Samples of a video sort get to perform better, but when you combine the two, when you combine the keyframes with the tamograph so you can get a significant boost in the performance of concept detection so you can detect the concepts more accurately, and this is particularly true when it comes to detecting motion related concepts.",
                    "label": 0
                },
                {
                    "sent": "So when it comes to very static concept then the additional motion information that we introduce.",
                    "label": 0
                },
                {
                    "sent": "Doesn't play.",
                    "label": 0
                },
                {
                    "sent": "That bigger role.",
                    "label": 0
                },
                {
                    "sent": "But when it comes to detecting motion related concepts, it can improve the accuracy by 50% or more.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here a bit a few more details about the stacking approach that we used to take into account the concept correlations.",
                    "label": 1
                },
                {
                    "sent": "So as I mentioned already, we.",
                    "label": 0
                },
                {
                    "sent": "Use the individual concept detectors that we built based on support vector machine classifiers.",
                    "label": 0
                },
                {
                    "sent": "Then we construct a model vector by concatenating the responses of these concept detectors.",
                    "label": 1
                },
                {
                    "sent": "And we use this train and MLK and then model.",
                    "label": 1
                },
                {
                    "sent": "Which explicitly uses the label correlations in the neighborhood of each of the tested instances.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Slide maybe explains a bit.",
                    "label": 0
                },
                {
                    "sent": "This is what we do.",
                    "label": 0
                },
                {
                    "sent": "So at the second layer we have a single classifier that takes into account all the concept detection results of the first layer and produces a set of.",
                    "label": 0
                },
                {
                    "sent": "Concept detection results again, which are more consistent in terms of the correlations of the concepts whereas.",
                    "label": 0
                },
                {
                    "sent": "The typical approach is to just use a set of independent classifiers, so train its concept classifier separately, completely ignoring the presence or absence of any other concept.",
                    "label": 0
                },
                {
                    "sent": "The same videos.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, some experimental results.",
                    "label": 0
                },
                {
                    "sent": "The system one is just the independent concept detectors, so this is the typical approach of the literature and System 2 is when we add the second layer stacking approach that we have and we can see that we have.",
                    "label": 0
                },
                {
                    "sent": "We can have significant improvements.",
                    "label": 0
                },
                {
                    "sent": "Both when evaluating concept detection in a retrieval scenario where we have the mean average precision.",
                    "label": 0
                },
                {
                    "sent": "Looking at single concept based retrieval and also when evaluating how well a single video has been annotating has been annotated.",
                    "label": 0
                },
                {
                    "sent": "So when looking at the ordering of the labels that have been assigned to every individual video, these are two different kinds of evaluations that we can do for concept detection.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some conclusions here looking also at the results of previous years because concept detection is a very active area of video analysis concept detections has progressed a lot in the past few years.",
                    "label": 1
                },
                {
                    "sent": "The results are still far from perfect, but they are already useful in some applications, so for retrieval, for instance, they can be useful despite being imperfect, and you can think of you can.",
                    "label": 0
                },
                {
                    "sent": "Understand this by looking at the Google results of text retrieval, so the Google results are still not perfect when you look for something it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Google doesn't magically understand what you mean and produce.",
                    "label": 0
                },
                {
                    "sent": "You present to you, just relevant links.",
                    "label": 0
                },
                {
                    "sent": "Still, when you get a page of 10 results and four, five of them are what you're looking for, you're happy, so the retrieval results don't have to be perfect to be.",
                    "label": 0
                },
                {
                    "sent": "Useful to someone, and the same goes here.",
                    "label": 0
                },
                {
                    "sent": "The concept detection results are far from perfect, but when looking at using them for concept based retrieval they can already be useful in real applications, so most information is of course important, but extracting the traditional motion descriptors is computationally expensive and This is why most approaches work with keyframes and we've also introduced the Tama graphs which.",
                    "label": 1
                },
                {
                    "sent": "Take into account some kind of motion information, but still they are like keyframes when it comes to processing them.",
                    "label": 0
                },
                {
                    "sent": "Linear support vector machines are very popular as a machine learning method for developing the individual concept classifiers and then using a second layer of multi label learning for exploring the concept correlations is also an approach that can introduce significant gains.",
                    "label": 1
                },
                {
                    "sent": "Still, I think though that if you look at the literature you will see that the computationally efficient detection of concepts in video when considering hundreds or thousands of concepts is still a significant silence.",
                    "label": 0
                },
                {
                    "sent": "So we're always looking for ways to improve the computational requirements of concept detection.",
                    "label": 0
                },
                {
                    "sent": "And we can see a demo here.",
                    "label": 0
                },
                {
                    "sent": "Which is the.",
                    "label": 0
                },
                {
                    "sent": "A website.",
                    "label": 0
                },
                {
                    "sent": "You could also log in yourselves.",
                    "label": 0
                },
                {
                    "sent": "What we have here is a demo of video shot segmentation concept detection, so we have some videos which have been processed without segmentation concept detection.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can access the videos at the fragment level, so at the short level.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Playing with some delay because of the network, but you will see here.",
                    "label": 0
                },
                {
                    "sent": "Playing.",
                    "label": 0
                },
                {
                    "sent": "Just a single sort of.",
                    "label": 0
                },
                {
                    "sent": "Of a lecture video.",
                    "label": 0
                },
                {
                    "sent": "This is done by exploiting the media fragment specifications that you will most probably hear about in the next presentation.",
                    "label": 0
                },
                {
                    "sent": "So we are playing just a single sort which has been automatically detected, but we don't have a video file with just this sort.",
                    "label": 0
                },
                {
                    "sent": "In the in the background we have single video for the whole hour, long single file for the whole hour long video.",
                    "label": 0
                },
                {
                    "sent": "But we can play the source and what we can see here are also some results of automatic concept detection, so we can see.",
                    "label": 0
                },
                {
                    "sent": "Some of the concepts that have been detected as being relevant to this sort in descending order of the degree of confidence that the algorithm has in these constants being present in the salt, and we can see these degrees also here.",
                    "label": 0
                },
                {
                    "sent": "So this is this is an example and we can.",
                    "label": 0
                },
                {
                    "sent": "See by just clicking on this on any of these bars.",
                    "label": 0
                },
                {
                    "sent": "If the network responds, then we see.",
                    "label": 0
                },
                {
                    "sent": "On in this window.",
                    "label": 0
                },
                {
                    "sent": "The set of video video sorts in our collection that actually relate to this this concept in descending order again of the degree of confidence.",
                    "label": 0
                },
                {
                    "sent": "And if we look at all the shots we have in their collection.",
                    "label": 0
                },
                {
                    "sent": "Then we see that despite this concept detection being imperfect, we can actually have on the top of the sorted list sorts that are in most cases related to this concept.",
                    "label": 0
                },
                {
                    "sent": "So they sold the lecture, and when we go down this list.",
                    "label": 0
                },
                {
                    "sent": "If the page loads, we will see that we have with the degrees of confidence close to 0.",
                    "label": 0
                },
                {
                    "sent": "All the thoughts in our collection that do not relate to this concept.",
                    "label": 0
                },
                {
                    "sent": "You will have to.",
                    "label": 0
                },
                {
                    "sent": "Trust me about this because the page doesn't seem to load, but the point is that when it comes to using concept detection for retrieval.",
                    "label": 0
                },
                {
                    "sent": "Although it's not perfect.",
                    "label": 0
                },
                {
                    "sent": "It makes sense to use it even with the current state of technology.",
                    "label": 0
                },
                {
                    "sent": "We can also see here an example of how the video has been segmented into sorts.",
                    "label": 0
                },
                {
                    "sent": "So here we see single video.",
                    "label": 0
                },
                {
                    "sent": "And one key frame pursuit for this video and the ordering, of course, is the ordering of the sorts in the video the temporal order.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort based representation of this video.",
                    "label": 0
                },
                {
                    "sent": "Again, you can see how different sorts have been correctly identified, because here we have obviously Electra sort and then we have a sort of the slides and then Electro sword again and so forth and using also the concept detection results we can separate different kinds of.",
                    "label": 0
                },
                {
                    "sent": "Visual content that appears in this video, so get just the slides, for instance.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some additional reading on concept.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detection and then we move onto event detection.",
                    "label": 0
                },
                {
                    "sent": "What is the objective here is the objective is to also detect events, so the kind of concept level annotation that we have with the previous set of techniques extended by also including some event event labels to this set of labels and the idea here is that so far we know how this image detect concepts like hand Sky see both whatever what we want now is also to detect a higher level event like we're facing in some place.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are a few examples of events of meaningful events that one would like to detect.",
                    "label": 0
                },
                {
                    "sent": "This come from the trackbed multimedia event detection task which works on this problem and these are three indicative events that have been used for evaluating the systems.",
                    "label": 0
                },
                {
                    "sent": "Among many other events, of course, in the past couple of years.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we perform event detection?",
                    "label": 0
                },
                {
                    "sent": "There are two main ways to do this.",
                    "label": 0
                },
                {
                    "sent": "One is to treat it the same way we treat the concept detection problem.",
                    "label": 0
                },
                {
                    "sent": "So start with a low level features.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, we have the event labels.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, and we try to train a machine learning approach that directly finds a mapping between those.",
                    "label": 0
                },
                {
                    "sent": "These low level features and the event labels that we have.",
                    "label": 1
                },
                {
                    "sent": "This is the first one.",
                    "label": 0
                },
                {
                    "sent": "The second approach is to use a model vector based approach which is using the concept detection results as a stepping stone for performing event detection.",
                    "label": 1
                },
                {
                    "sent": "So in this case we have a set of concept detectors and there are a lot of works on deciding how to choose concepts for this task and how to train the concept detectors as we saw in the previous slides.",
                    "label": 0
                },
                {
                    "sent": "So from the low level features you go to detecting concepts with the usual machine learning techniques that we've seen, and then you get a set of responses from the concept detectors and this is used again as a feature for having another.",
                    "label": 0
                },
                {
                    "sent": "Another set of machine.",
                    "label": 0
                },
                {
                    "sent": "Another step of machine learning that tries to map these concept detector responses to the event labels.",
                    "label": 1
                },
                {
                    "sent": "And of course we have hybrid approaches that combine.",
                    "label": 0
                },
                {
                    "sent": "Combine the both of them.",
                    "label": 0
                },
                {
                    "sent": "In general, model vector based approaches are very interesting for two reasons.",
                    "label": 0
                },
                {
                    "sent": "The one is that they've been shown to perform a little bit better than when working directly with the low level features.",
                    "label": 0
                },
                {
                    "sent": "The other is that when you work with this kind of approach, you can also do event recounting so you can go back when you decide on an event label.",
                    "label": 0
                },
                {
                    "sent": "You can also go back to the features, which in this case is the concepts that led you to this event detection.",
                    "label": 0
                },
                {
                    "sent": "And refine the concept detection results that you have to find even more meaningful concepts that are consistent with the event that you have detected.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "The basis of the media mixer promoted approach for event detection, so we have, of course, temporal video segmentation at first 2 sorts.",
                    "label": 1
                },
                {
                    "sent": "This is baseline for all the processing.",
                    "label": 1
                },
                {
                    "sent": "Then we have low level feature extraction which is based on again local features and bug of words.",
                    "label": 0
                },
                {
                    "sent": "We apply a set of trained visual concept detectors which typically are SVM based detectors and we have constants in the order of hundreds.",
                    "label": 1
                },
                {
                    "sent": "So a few 100 concepts.",
                    "label": 1
                },
                {
                    "sent": "These detectors may be seemingly relevant to the South events, but then we use this vector of responses of these concept detectors as a feature.",
                    "label": 0
                },
                {
                    "sent": "As a video representation that is of this kind.",
                    "label": 0
                },
                {
                    "sent": "So for every concept we have one score and this is the input to another step of machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Machine learning in this case.",
                    "label": 0
                },
                {
                    "sent": "Be again another support vector machine classifier, but also since we have many noisy concept detectors and many relevant ones, those events that we're looking for, it makes sense to 1st do some dimensionality reduction, so we discard.",
                    "label": 0
                },
                {
                    "sent": "The unnecessary dimensions of this feature vector and then possibly do possibly apply, and SVM classifier or even do some simpler nearest neighbor matching to decide on the event.",
                    "label": 0
                },
                {
                    "sent": "And we have developed several approaches in this direction.",
                    "label": 0
                },
                {
                    "sent": "Some new discriminate analysis algorithms that exploit the inherent sub subclass.",
                    "label": 0
                },
                {
                    "sent": "Nature of the data and this is important for being able to discard those concept detection results that do not make sense for this particular event detection task and use just the rest to perform the actual event detection.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will not tie you with detail.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this what I can show you is the differences between using the traditional learning approach of having linear SVM classifiers as the second layer of learning.",
                    "label": 0
                },
                {
                    "sent": "So this one is having the model vectors and then applying linear SVM on them to detect events.",
                    "label": 0
                },
                {
                    "sent": "And this is when you use an intelligent dimensionality reduction approach combined with.",
                    "label": 0
                },
                {
                    "sent": "Linear SVM so you can see that we can have significant gains in performance which shows that.",
                    "label": 0
                },
                {
                    "sent": "The type of machine learning that you choose for for this problem, and also for any other problem, is very important.",
                    "label": 0
                },
                {
                    "sent": "It's it's not just the features that you use that make a significant difference.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some general conclusions and hints on event detection.",
                    "label": 0
                },
                {
                    "sent": "So the performance of the event detection system increases by discarding irrelevant or noisy concept detections.",
                    "label": 1
                },
                {
                    "sent": "This is what our mixer, subclass, discriminant analysis or any other discriminant analysis technique we do.",
                    "label": 1
                },
                {
                    "sent": "Effectively combining multiple classifiers is important.",
                    "label": 0
                },
                {
                    "sent": "So we use multiple classifiers per concept, not just one.",
                    "label": 1
                },
                {
                    "sent": "And exploiting the subclass structure of the event data is again important because this is what we have with events.",
                    "label": 0
                },
                {
                    "sent": "They are typically constructed by subevents and this is a structure that we want to exploit.",
                    "label": 0
                },
                {
                    "sent": "The low level features of course, are also important, and several other studies have shown that when it comes to event detection, it is the motion features that are the most important out of all the different types of low level features that we can use followed in decreasing.",
                    "label": 1
                },
                {
                    "sent": "Order of importance by static visual features such as SIFT and the like.",
                    "label": 0
                },
                {
                    "sent": "And then for some events, the audio features do provide complementary information for other events.",
                    "label": 0
                },
                {
                    "sent": "They do not really contribute to improve detection.",
                    "label": 0
                },
                {
                    "sent": "There is a problem with using of course motion features in large scale video and attention problems.",
                    "label": 0
                },
                {
                    "sent": "It's the cost of extracting these features.",
                    "label": 0
                },
                {
                    "sent": "We also solve this briefly when it comes when it came to concept detection.",
                    "label": 0
                },
                {
                    "sent": "So static visual features they may be not as effective, but they're much much faster to compute than typical motion features in video.",
                    "label": 0
                },
                {
                    "sent": "And then combining low level features and model vectors is always an important approach.",
                    "label": 0
                },
                {
                    "sent": "Something to remember in event detection because it always gives you some performance gains.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some additional reading.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we move on to the last section of the presentation, which has to do with the creation of spatio temporal fragments and and also the annotation of them by means of object re detection.",
                    "label": 0
                },
                {
                    "sent": "What do we mean here?",
                    "label": 0
                },
                {
                    "sent": "We mean to find instances of a specific object within a single video or within a collection of videos.",
                    "label": 1
                },
                {
                    "sent": "So we have an object of interest.",
                    "label": 1
                },
                {
                    "sent": "We know that it is this particular bicycle or this particular painting.",
                    "label": 0
                },
                {
                    "sent": "We have an annotation for this.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "My bicycle or.",
                    "label": 0
                },
                {
                    "sent": "You know, Lyndon's bicycle or whatever, and we want to detect in the video.",
                    "label": 0
                },
                {
                    "sent": "And localize spatially and temporally, not just any bike, any any instance of that concept, but this particular instance, this particular object.",
                    "label": 0
                },
                {
                    "sent": "And this leads us to the creation of spatial temporal fragments.",
                    "label": 0
                },
                {
                    "sent": "And if we have an annotation for the object that we start with, if we know that it's my bike, then this annotation can of course be propagated to these spatial temporal fragments.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The related work in this area.",
                    "label": 0
                },
                {
                    "sent": "This is a typically an object matching problem, so there there's lots of work here with primarily was surprised.",
                    "label": 0
                },
                {
                    "sent": "Again, local features such as Sift.",
                    "label": 0
                },
                {
                    "sent": "This is, I think, the original application of safety as well or is close to the original application of safety.",
                    "label": 0
                },
                {
                    "sent": "And then we also it's typical also to do some geometry based evaluation of the matching between local descriptors so that you make sure that you're talking about the same object and not just similar features but geometrically ordered in a completely different way.",
                    "label": 0
                },
                {
                    "sent": "So there are various extensions to this baseline, which for instance include the combined use of key points and motion information.",
                    "label": 1
                },
                {
                    "sent": "So add some motion there.",
                    "label": 0
                },
                {
                    "sent": "Using the bag of words representation for pruning, the set of possible matches, and then do the more expensive.",
                    "label": 1
                },
                {
                    "sent": "Local feature matching and geometric verification with just a subset of the original frames or using graph matching approach.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roaches are several options, so the media mixer promoted approach is based on starting from the baseline of using local features and doing a matching, but we try to improve both the detection accuracy and reduce the processing time and this we do by exploiting the power of modern graphics processing units for doing parts of the processing, exploiting the structure of the video for the sampling of frames, and this is.",
                    "label": 0
                },
                {
                    "sent": "One yet another point where we exploit the results of automatic segmentation of the video to sorts in order to make much faster the object detection and we enhance robustness to scale variations of.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Short object.",
                    "label": 0
                },
                {
                    "sent": "So this is how we exploit the structure.",
                    "label": 0
                },
                {
                    "sent": "Of the video, the automatic segmentation, so we have the video.",
                    "label": 0
                },
                {
                    "sent": "It is automatically fragmented into a number of sorts and then for every sort we extract just a few keyframes and we start by doing the matching.",
                    "label": 0
                },
                {
                    "sent": "Looking for the for the object of interest in these selected keyframes.",
                    "label": 1
                },
                {
                    "sent": "If there is no detection there, then we don't bother to try to process all the other frames of that sort, but instead we just move to the next sort and the next representative keyframes the next sort.",
                    "label": 1
                },
                {
                    "sent": "And if and only if there is a detection there, we move on.",
                    "label": 0
                },
                {
                    "sent": "To process the rest of the frames of the shot.",
                    "label": 0
                },
                {
                    "sent": "Since typically an object of interest doesn't appear on an entire video, but in some specific parts of that video, this approach is very effective for reducing the computational complexity of object detection.",
                    "label": 0
                },
                {
                    "sent": "And if the network works, I will show an example of doing this online.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another problem we have, we have significant scale variations of the object in the video, so a maybe the object that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "But B&C, so how this object may appear in the video?",
                    "label": 1
                },
                {
                    "sent": "Maybe it's particularly zoomed in and only a small part of this object is visible.",
                    "label": 0
                },
                {
                    "sent": "Or in other cases we have a more distant view and this whole object is just.",
                    "label": 0
                },
                {
                    "sent": "Very small part of the frame.",
                    "label": 1
                },
                {
                    "sent": "So we try to address this by preprocessing the original image so when someone the user says that this is the object I'm looking for, we automatically generate a zoomed in instance by taking the center part of that frame and also assumed out instance and then we use all these three images for doing the matching and matching with any one of them.",
                    "label": 1
                },
                {
                    "sent": "Means for us at detection of the object that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "This helps us.",
                    "label": 0
                },
                {
                    "sent": "Detect more effectively.",
                    "label": 0
                },
                {
                    "sent": "Zoomed in or zoomed out instances of the object.",
                    "label": 1
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the video.",
                    "label": 0
                },
                {
                    "sent": "These are a few examples of objects that user may be looking for and this again relate to one of the shows that Lindon mentioned in his introductory talk, the.",
                    "label": 0
                },
                {
                    "sent": "Antique roadshow so.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can see some results here where we can see that we can have very very high precision and recall.",
                    "label": 0
                },
                {
                    "sent": "I should mention here that these figures depend alot on the choice of objects that we have.",
                    "label": 0
                },
                {
                    "sent": "So typically when you work with.",
                    "label": 0
                },
                {
                    "sent": "Kind of 2D objects that are complex enough, for instance paintings.",
                    "label": 0
                },
                {
                    "sent": "Then you have a nice set of information to work with and we can have very accurately detection there when it comes to either simpler objects or when it comes to objects that can have many different views.",
                    "label": 0
                },
                {
                    "sent": "For instance, a car where the front view of the car and the side view in the rear view are completely different.",
                    "label": 0
                },
                {
                    "sent": "Then it is much more challenging to accurately detect these instances.",
                    "label": 0
                },
                {
                    "sent": "And in the case of such objects, we would get significantly lower precision and recall.",
                    "label": 0
                },
                {
                    "sent": "In many cases, the algorithm is quite fast, so about 10% or even less of the videos duration for performing their detection, and this process includes the whole processing of the video, so also the.",
                    "label": 1
                },
                {
                    "sent": "Extraction of the local features and the matching.",
                    "label": 0
                },
                {
                    "sent": "Of course with the exception of such segmentation.",
                    "label": 1
                },
                {
                    "sent": "This is yet another example of what we're looking for, and this is what I described as a 2D object.",
                    "label": 0
                },
                {
                    "sent": "It's a painting, and this is how it has been automatically re detected the green rectangles in this case indicate correct detection, so we were able to detect the object when we had zoomed in instances of it with just parts of the original object being visible when we had some partial occlusion with the hand here.",
                    "label": 0
                },
                {
                    "sent": "Or when we had more distant and also distant with occlusion views of the same painting.",
                    "label": 0
                },
                {
                    "sent": "Here's another example, but this is a much more challenging object, so here if we were given just this image.",
                    "label": 0
                },
                {
                    "sent": "It would be much more difficult to detect the object when it is rotated and is shown from a different view.",
                    "label": 0
                },
                {
                    "sent": "What we do here is we exploit multiple instances of the original object and we use all of them for doing the matching and this helps us again in correctly detecting re detecting this object in the video despite the viewpoint variations.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some confusions accurately.",
                    "label": 0
                },
                {
                    "sent": "Detection of objects is possible.",
                    "label": 1
                },
                {
                    "sent": "As I already commented on the choice of objects plays a significant role.",
                    "label": 1
                },
                {
                    "sent": "It is possible to do it significantly faster than real time when exploiting the video structure.",
                    "label": 0
                },
                {
                    "sent": "And of course the GPU, but I didn't go into details of what processes we speed it up using the GPU.",
                    "label": 1
                },
                {
                    "sent": "And there are several possible uses for of this kind of technology.",
                    "label": 0
                },
                {
                    "sent": "One is instance level annotation as I mentioned, if we have.",
                    "label": 0
                },
                {
                    "sent": "An image of my bike and we have the annotation that it's my bike.",
                    "label": 0
                },
                {
                    "sent": "Then we can propagate this label throughout the video by doing re detection of these objects.",
                    "label": 0
                },
                {
                    "sent": "Is finding linking related videos or fragments of them?",
                    "label": 1
                },
                {
                    "sent": "So if we have a collection of videos and we can re detect a specific object in five of them, then we know that these videos are related and how are they related?",
                    "label": 0
                },
                {
                    "sent": "They at least have the same object, the same specific object appearing in them and this is an indication that the user.",
                    "label": 0
                },
                {
                    "sent": "For instance, interested in one video may be interested in the others as well, and we can support with this with these results.",
                    "label": 0
                },
                {
                    "sent": "Other analysis task as well so seen detection.",
                    "label": 0
                },
                {
                    "sent": "For instance when we have a set of sorts in a video and we know that some of these sorts do contain the same object, then this is yet another cue that these sorts belong to the same scene similarly to the audio events or the concept detection results that we exploited for same detection.",
                    "label": 0
                },
                {
                    "sent": "And let's see a demo again.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Another video.",
                    "label": 0
                },
                {
                    "sent": "Which plays.",
                    "label": 0
                },
                {
                    "sent": "We can stop it.",
                    "label": 0
                },
                {
                    "sent": "We can select the object.",
                    "label": 0
                },
                {
                    "sent": "That we are looking for, so let's make it this object this painting.",
                    "label": 0
                },
                {
                    "sent": "And we say we detect this.",
                    "label": 0
                },
                {
                    "sent": "So processing processing processing.",
                    "label": 0
                },
                {
                    "sent": "And in a few seconds.",
                    "label": 0
                },
                {
                    "sent": "We start seeing the results of every detection.",
                    "label": 0
                },
                {
                    "sent": "This algorithm in this case runs online.",
                    "label": 0
                },
                {
                    "sent": "So as we speak, it performs the.",
                    "label": 0
                },
                {
                    "sent": "Feature extraction of the video and then the matching with the object that we just marked.",
                    "label": 0
                },
                {
                    "sent": "And here we have a time bar with the video.",
                    "label": 0
                },
                {
                    "sent": "Red means that the video this part of the video hasn't been processed yet, so it will be.",
                    "label": 0
                },
                {
                    "sent": "Changing colors soon.",
                    "label": 0
                },
                {
                    "sent": "Blue indicates the sorts where we have detected that the object is present and Gray is the set of sorts where we have detected that this particular object doesn't appear and we can move on and say for instance play play out this sort.",
                    "label": 0
                },
                {
                    "sent": "If the network works.",
                    "label": 0
                },
                {
                    "sent": "In the meantime, you see that the video continues to be processed online and.",
                    "label": 0
                },
                {
                    "sent": "The object is re detected.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of how it works, and I invite you to also play with these online demonstrators.",
                    "label": 0
                },
                {
                    "sent": "Both this one and the previous one with the concept detection results.",
                    "label": 0
                },
                {
                    "sent": "They are online and you can.",
                    "label": 0
                },
                {
                    "sent": "You can work with them.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Play with them.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some additional reading on object read text.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for those interesting and some concluding remarks on the whole.",
                    "label": 0
                },
                {
                    "sent": "Talk so we discuss different classes of techniques for media fragment creation, annotation there of course several others there.",
                    "label": 1
                },
                {
                    "sent": "There's object recognition, there's face detection, tracking, clustering and recognition.",
                    "label": 0
                },
                {
                    "sent": "There's quality assessment, so visual quality assessment is another.",
                    "label": 0
                },
                {
                    "sent": "Analysis process that may be important in some applications to identify high quality fragments from low quality fragments in terms of the artifacts that appear in these parts of the video, there's sentiment and emotion detection is yet another technique for enriching the annotations of the video, not just concepts, not just events, but also the sentiments or emotions.",
                    "label": 0
                },
                {
                    "sent": "An important thing to keep in mind is that not all of these possible analysis techniques.",
                    "label": 1
                },
                {
                    "sent": "Are suitable for every possible problem.",
                    "label": 0
                },
                {
                    "sent": "So when we have a problem in hand and we need to process a set of videos, we need to think carefully of what kind of techniques, what kind of classes of techniques it makes sense to apply and also when it comes to specific class of techniques like for instance concept detection, what kind of specific technique out of all the possibilities for concept detection we should apply to this particular content?",
                    "label": 1
                },
                {
                    "sent": "And of course, understanding the problem and the volume of data that we have.",
                    "label": 0
                },
                {
                    "sent": "The value of the data and the variability of the data is key to selecting appropriate methods when we have, for instance, the domain specific problem, we know that the videos will just be news videos.",
                    "label": 0
                },
                {
                    "sent": "Then there are lots of concepts that it doesn't make sense to try to detect because they will never appear in these videos when it comes to processing unconstrained user generated content content, on the other hand, we may need a much richer set of concepts.",
                    "label": 0
                },
                {
                    "sent": "And in most, in some or most cases, the automatic analysis results remain far from perfect.",
                    "label": 1
                },
                {
                    "sent": "But in many cases they're still useful in the right domain or for solving the right problem.",
                    "label": 1
                },
                {
                    "sent": "For instance concept based video retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with these concluding thoughts, thank you for listening to this talk and please.",
                    "label": 0
                },
                {
                    "sent": "Ask any questions.",
                    "label": 0
                }
            ]
        }
    }
}