{
    "id": "bbplnkoarcfmay7qaluil2yr4khboqpc",
    "title": "TRM - Learning Dependencies between Text and Structure with Topical Relational Models",
    "info": {
        "author": [
            "Rudi Studer, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2013_studer_relational_models/",
    "segmentation": [
        [
            "I'm going to present TRM topic relational models that aim at coming up with topic models that are able to somehow capture the dependency between textual information.",
            "The one hand and structural information on the other hand.",
            "That is true and work together with several people.",
            "Really.",
            "Peter was formally in my group, is nervous, IBM Research in Dublin tanked.",
            "Ron was also in my group is now at State University in San Jose, in California and Young Tower who is still in my crew.",
            "Pat Catan can't swim."
        ],
        [
            "So what is the structure of my talk?",
            "I will start with some introductory remarks.",
            "While all these topics might be relevant.",
            "And then the main part of my presentation will be covered by the second and third part of my presentation, where will describe the topic of relational model TRM.",
            "What it is about and what are the results that are generated in the end by that approach and I will present some parts of the relation we have done.",
            "Other parts you can then read in the paper that conclude.",
            "OK, let's say."
        ],
        [
            "But with the introduction, that might be rather brief, and for that audience here that we have a lot of RDF data around that combines structural aspects like having entities being related to other entities being classified as belonging to some kind of class, but also being described in addition with attributes that provide textual information.",
            "So we have these semi structured data around when you consider classical topic models.",
            "They are typically defined as distribution over words, so they just take the text part into account, but do not really consider the understructure part there.",
            "Also nowadays some topic models around that also consider structural aspects like relationships in social networks or citation networks, but are typically restricted to be handling only a few types of relations and not that many types you typically find in RDF data.",
            "And then you will find that area of statistical relational learning where you have all kinds of approaches around and later on we will use these kind of approaches as a baseline where we compare our approach to.",
            "So that is the context.",
            "We have a row."
        ],
        [
            "So you all know these RDF graphs where you have different entities.",
            "Trust take one.",
            "Here the entities C1 which is of type company which manual which has some kind of products like Avon which is of type auto mobile and has the name ODT.",
            "So we refer to some German car manufacturers.",
            "The name of the company is also did you hear that extra information providing some information about the Audi company?",
            "So we have structural information and textual information.",
            "What we all know of."
        ],
        [
            "What are the kind of topics we want to come up in the end and here you see and rough overview.",
            "I will come back to that later on in some more detail you see that we have four topics, T1 through T4 and is usually are characterized by some words like company, employer, industry for topic T one.",
            "But you also see we want in the end to have some class description related to the topic so that we know that classes like organization or company are relevant.",
            "And also we have relations like key person of a company or owning a company that also pop up as part of the topic characterizations.",
            "So that is what we aim for, how to learn topics that cover birds that cover classes in that cover, relations in the end."
        ],
        [
            "Coming back to our example about calming effect ring domain, we see we have two cards to address the issue that we have broad head rocinas data, we have a lot of different type of classes and relationships that have to be covered and we also have to address the sparseness issue becausw.",
            "Given the lot of different relationships and companies and selected number of topics, we have to somehow deal with its Parsons issue."
        ],
        [
            "So what are the challenges we want to address in that context?",
            "How to deal with these RDF structures and how to address it?",
            "Regina T, and sparseness so that are the challenges we want to address."
        ],
        [
            "When we look into the state of the art, you find all kind of different models that are somehow relevant for what we are doing here.",
            "So we see the more classical topic models that are based on word Co occurrence and can for example be used for clustering modulable documents.",
            "I already mentioned approaches that are related to homogeneous networks where you handle for example of social networks based on the friendship relationships.",
            "Like that is done for example in another approach.",
            "Here we also have some approaches that addressing the structural issues, how to correlate structure on the one hand, with topics, on the other hand, but for example, that TMP approach is not really addressing that sparsity issue.",
            "And then we have these approaches in the statistical relational learning like Markov logic networks and other approaches, and we will use them later on as.",
            "Baseline for our evaluation."
        ],
        [
            "OK, so that was the introductory part of why is that setting relevant and what is the context and what are we aiming for in the end?",
            "So let's now come to the second part topical relational models.",
            "What are they about and how they are constructed and what is the output of that kind of approach in the end?"
        ],
        [
            "So what is the characteristic we have around where we want to learn a model that is able to capture the correlations between the classes, relations and text that are inherent in these RDF data?",
            "We also want to come up with topics that provide a low dimensional representation of text in these RDF context.",
            "So that exploit with some extent the structural information we have there so that you can come up with the models that somehow capture the relationship between the words like employer or merger.",
            "And to see but which classes they are highly correlated in our example, maybe Mr. Glass company?",
            "Further aspect is how can we further define the relationship between the classes?",
            "On the one hand and the topics on the other hand.",
            "So we use weights in order to capture that and the basic idea is that when you consider different topics where words like for example employee or merger high have higher probability, we also want to have high weights for these words in the vector that is describing the.",
            "Glass company and that is also true for other classes in our model and in the end we use patient network for doing all that learning in that context."
        ],
        [
            "So what is now specific for the approach we are developing or having have developed?",
            "We start from the RDF data that come with some specific structures, so we have these resources like C1 or A1 in our example for C1.",
            "We know that these of type company it has a name attribute like Audi and has some text associated describing that company in more detail.",
            "And we use that structure in order to generate patient network in the end.",
            "So what you see here is that we use these ingredients of the RDF model to define the variables that are then the components in the patient network.",
            "So we come up with random valuable for company.",
            "That is the class information.",
            "Here we come up with random valuable for product and parent that other tool relations we find here not so we come up with the variables for the textual information for the words you find like for example outta here or German here.",
            "So that are the.",
            "Random valuables that are generated based on the RDF on the RDF structure.",
            "What are the second ingredients of that approach?",
            "We define three classes of hidden variables that are then used to correlate these components with each other.",
            "One is the so-called topic indicator vector that indicates what are the topics specific entities associated with.",
            "So from all the topics that end up in our final model, that entity will only be associated with some subset of these of these topics, and that is handled by that.",
            "Topic indicator vector B.",
            "Based on that, when you then know what other topics are specific entity is associated with.",
            "The second variable describes the proportions between these different topics so that you know what are the different topics and how they relate to each other.",
            "When you have a given resource like C1 and finally we also have the more classical topic word assignment that addresses the textual information you'll find there.",
            "So when you look into this structure, we have more or less two ingredients.",
            "One is the more relational part that comes from the RDF structures and one is the more classical topical part that is more known from these topic models in the end."
        ],
        [
            "OK, let's come back to our example.",
            "You have already seen that very briefly here.",
            "In our trivial example, we have four topics T1 through T4, and you see for all topics we have listed here, the specific words that have a high probability to show up in that specific topic.",
            "So that is the standard way you cover the textual information.",
            "So for topic T1 we have words like company, employer, industry.",
            "That have a higher probability to show up in the context of topic model T1, whereas for topic tool we have words like CEO or Manager or engineer.",
            "So that's the Classical World Co occurrence with related to the topic models.",
            "But then we have these two additional ingredients, so we have the information that is related to the class information we have in these RDF data around.",
            "So you see, for example for topic T1 we have classes like organization or company or airline.",
            "And you also see that cardiac glass companies popping up in.",
            "Topic T1, so it's a popping up in T2 and T3 but not in T4 and you see the different weights so company is most relevant for topic T1 because here we have the highest weight compared to the other topics.",
            "Which means when you consider the entities that are of type of company, they are most often related with textual information where words like company, employer, industry are used.",
            "So that is the relationship between.",
            "The word information in the glass information, and there's a third and create, and we also have the relationship information, so we have these directed relationships, like owning company relating topic T1 with topic T tool again by using these weights indicating how relevant that relationship is for the different topics.",
            "Again we can characterize that in a way that entities.",
            "The the the topic T one that show that holding company relationship are more often related to birds like company employing industry when compared to entities that are related to topic T2 or T3.",
            "We also take here the direction of the relationship into account, so it's relevant whether the relationship is going from T122 or vice versa.",
            "So in that way we try to capture.",
            "On the one hand, word that excellent information, and on the other hand we want to capture the role class is an relationships play in that context.",
            "So that is the model that is generated as an output."
        ],
        [
            "Now let's have a brief look at the evaluation."
        ],
        [
            "We use two datasets to evaluate that approach.",
            "One was taken from the PLP or the conference and paper information, and we use, for example, the abstract as representative for textual data.",
            "In that context, second domain was taken from DB Pedia, where we selected a subset of the movie domain describing some movie information and they are when you look into the details what is taken as attribute values is then treated as textual data in that context.",
            "We consider two tasks.",
            "One is link prediction, the other one is object clustering.",
            "I will only address the link prediction object clustering you may find in the in the paper."
        ],
        [
            "So when we talk about link prediction, what was the task of why we wanted to predict the auto relationship between papers and orders for one domain an for DB pedia?",
            "We wanted to predict the starving relation between movies and actors so that are the tooling predictions and we compare that with well known approaches from the literature."
        ],
        [
            "So you see, here of the first evaluation for the DB LP data set, we use different splitting strategies for training data and test data, so that are the three different settings you'll find here, and we always consider precision recall and accuracy, and you see the yellow column here is representing our TM approach that it behaves very well compared to the other approaches.",
            "The Spectra precision, recall and accuracy, so that was the order link produce."
        ],
        [
            "When we moved to DBPR relating the addressing the movie domain, we can see that our approach is behaving rather well, but we also see that in that domain the MLN approach is doing very well with respect to recall, but we know then go into details.",
            "You see that you get a high recall but somehow related to the issue that you also get a lot of false positives.",
            "So that is somehow related to each other you can."
        ],
        [
            "See that into some more detail when you look into the evaluation of how the different approaches really behave with respect to true negatives.",
            "So to predict that link does not exist between a paper and an order, for example, or the actor is not really starring in a specific movie, and then you see that.",
            "MLN approach that provided rather high recall, but including a lot of false positives, behaves rather weakly.",
            "In that context, becausw for predicting the true negative rate.",
            "Merlin is not really well working again, our approach is rather good working compared with these other approaches, so that is 1 evaluation.",
            "The paper you find a second evaluation at addressing the object clustering.",
            "Evaluation"
        ],
        [
            "Issue.",
            "OK."
        ],
        [
            "Let's conclude we think that these topical relational models provide good approach for handling these text, which are the F data and are well able to capture the relationship between the textual information, the structural information.",
            "You have seen a 2 application scenarios link prediction object clustering.",
            "We also work on other application scenarios for like for example when you're think of hybrid sparkle queries, you post with these RDF data.",
            "One issue is for optimizing the query processing to estimate the result size of intermediate results.",
            "In that context, you can use that approach as well, or when you think of keyword based search on RDF data.",
            "For really coming up with the results, we have to somehow estimate what are the most probable connection between entities given the specific keywords for the query.",
            "So that are also application areas where that model might be used and in the future we might work on things like for example hierarchical topic models.",
            "OK, that is what I wanted to present.",
            "Thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to present TRM topic relational models that aim at coming up with topic models that are able to somehow capture the dependency between textual information.",
                    "label": 0
                },
                {
                    "sent": "The one hand and structural information on the other hand.",
                    "label": 0
                },
                {
                    "sent": "That is true and work together with several people.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Peter was formally in my group, is nervous, IBM Research in Dublin tanked.",
                    "label": 0
                },
                {
                    "sent": "Ron was also in my group is now at State University in San Jose, in California and Young Tower who is still in my crew.",
                    "label": 0
                },
                {
                    "sent": "Pat Catan can't swim.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the structure of my talk?",
                    "label": 0
                },
                {
                    "sent": "I will start with some introductory remarks.",
                    "label": 0
                },
                {
                    "sent": "While all these topics might be relevant.",
                    "label": 0
                },
                {
                    "sent": "And then the main part of my presentation will be covered by the second and third part of my presentation, where will describe the topic of relational model TRM.",
                    "label": 1
                },
                {
                    "sent": "What it is about and what are the results that are generated in the end by that approach and I will present some parts of the relation we have done.",
                    "label": 0
                },
                {
                    "sent": "Other parts you can then read in the paper that conclude.",
                    "label": 0
                },
                {
                    "sent": "OK, let's say.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But with the introduction, that might be rather brief, and for that audience here that we have a lot of RDF data around that combines structural aspects like having entities being related to other entities being classified as belonging to some kind of class, but also being described in addition with attributes that provide textual information.",
                    "label": 1
                },
                {
                    "sent": "So we have these semi structured data around when you consider classical topic models.",
                    "label": 1
                },
                {
                    "sent": "They are typically defined as distribution over words, so they just take the text part into account, but do not really consider the understructure part there.",
                    "label": 1
                },
                {
                    "sent": "Also nowadays some topic models around that also consider structural aspects like relationships in social networks or citation networks, but are typically restricted to be handling only a few types of relations and not that many types you typically find in RDF data.",
                    "label": 0
                },
                {
                    "sent": "And then you will find that area of statistical relational learning where you have all kinds of approaches around and later on we will use these kind of approaches as a baseline where we compare our approach to.",
                    "label": 0
                },
                {
                    "sent": "So that is the context.",
                    "label": 0
                },
                {
                    "sent": "We have a row.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you all know these RDF graphs where you have different entities.",
                    "label": 0
                },
                {
                    "sent": "Trust take one.",
                    "label": 0
                },
                {
                    "sent": "Here the entities C1 which is of type company which manual which has some kind of products like Avon which is of type auto mobile and has the name ODT.",
                    "label": 0
                },
                {
                    "sent": "So we refer to some German car manufacturers.",
                    "label": 0
                },
                {
                    "sent": "The name of the company is also did you hear that extra information providing some information about the Audi company?",
                    "label": 1
                },
                {
                    "sent": "So we have structural information and textual information.",
                    "label": 0
                },
                {
                    "sent": "What we all know of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What are the kind of topics we want to come up in the end and here you see and rough overview.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that later on in some more detail you see that we have four topics, T1 through T4 and is usually are characterized by some words like company, employer, industry for topic T one.",
                    "label": 0
                },
                {
                    "sent": "But you also see we want in the end to have some class description related to the topic so that we know that classes like organization or company are relevant.",
                    "label": 0
                },
                {
                    "sent": "And also we have relations like key person of a company or owning a company that also pop up as part of the topic characterizations.",
                    "label": 0
                },
                {
                    "sent": "So that is what we aim for, how to learn topics that cover birds that cover classes in that cover, relations in the end.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming back to our example about calming effect ring domain, we see we have two cards to address the issue that we have broad head rocinas data, we have a lot of different type of classes and relationships that have to be covered and we also have to address the sparseness issue becausw.",
                    "label": 0
                },
                {
                    "sent": "Given the lot of different relationships and companies and selected number of topics, we have to somehow deal with its Parsons issue.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are the challenges we want to address in that context?",
                    "label": 0
                },
                {
                    "sent": "How to deal with these RDF structures and how to address it?",
                    "label": 0
                },
                {
                    "sent": "Regina T, and sparseness so that are the challenges we want to address.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we look into the state of the art, you find all kind of different models that are somehow relevant for what we are doing here.",
                    "label": 0
                },
                {
                    "sent": "So we see the more classical topic models that are based on word Co occurrence and can for example be used for clustering modulable documents.",
                    "label": 1
                },
                {
                    "sent": "I already mentioned approaches that are related to homogeneous networks where you handle for example of social networks based on the friendship relationships.",
                    "label": 1
                },
                {
                    "sent": "Like that is done for example in another approach.",
                    "label": 0
                },
                {
                    "sent": "Here we also have some approaches that addressing the structural issues, how to correlate structure on the one hand, with topics, on the other hand, but for example, that TMP approach is not really addressing that sparsity issue.",
                    "label": 1
                },
                {
                    "sent": "And then we have these approaches in the statistical relational learning like Markov logic networks and other approaches, and we will use them later on as.",
                    "label": 0
                },
                {
                    "sent": "Baseline for our evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was the introductory part of why is that setting relevant and what is the context and what are we aiming for in the end?",
                    "label": 0
                },
                {
                    "sent": "So let's now come to the second part topical relational models.",
                    "label": 1
                },
                {
                    "sent": "What are they about and how they are constructed and what is the output of that kind of approach in the end?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the characteristic we have around where we want to learn a model that is able to capture the correlations between the classes, relations and text that are inherent in these RDF data?",
                    "label": 1
                },
                {
                    "sent": "We also want to come up with topics that provide a low dimensional representation of text in these RDF context.",
                    "label": 0
                },
                {
                    "sent": "So that exploit with some extent the structural information we have there so that you can come up with the models that somehow capture the relationship between the words like employer or merger.",
                    "label": 0
                },
                {
                    "sent": "And to see but which classes they are highly correlated in our example, maybe Mr. Glass company?",
                    "label": 1
                },
                {
                    "sent": "Further aspect is how can we further define the relationship between the classes?",
                    "label": 0
                },
                {
                    "sent": "On the one hand and the topics on the other hand.",
                    "label": 1
                },
                {
                    "sent": "So we use weights in order to capture that and the basic idea is that when you consider different topics where words like for example employee or merger high have higher probability, we also want to have high weights for these words in the vector that is describing the.",
                    "label": 0
                },
                {
                    "sent": "Glass company and that is also true for other classes in our model and in the end we use patient network for doing all that learning in that context.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is now specific for the approach we are developing or having have developed?",
                    "label": 0
                },
                {
                    "sent": "We start from the RDF data that come with some specific structures, so we have these resources like C1 or A1 in our example for C1.",
                    "label": 0
                },
                {
                    "sent": "We know that these of type company it has a name attribute like Audi and has some text associated describing that company in more detail.",
                    "label": 0
                },
                {
                    "sent": "And we use that structure in order to generate patient network in the end.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is that we use these ingredients of the RDF model to define the variables that are then the components in the patient network.",
                    "label": 0
                },
                {
                    "sent": "So we come up with random valuable for company.",
                    "label": 0
                },
                {
                    "sent": "That is the class information.",
                    "label": 0
                },
                {
                    "sent": "Here we come up with random valuable for product and parent that other tool relations we find here not so we come up with the variables for the textual information for the words you find like for example outta here or German here.",
                    "label": 0
                },
                {
                    "sent": "So that are the.",
                    "label": 0
                },
                {
                    "sent": "Random valuables that are generated based on the RDF on the RDF structure.",
                    "label": 0
                },
                {
                    "sent": "What are the second ingredients of that approach?",
                    "label": 0
                },
                {
                    "sent": "We define three classes of hidden variables that are then used to correlate these components with each other.",
                    "label": 0
                },
                {
                    "sent": "One is the so-called topic indicator vector that indicates what are the topics specific entities associated with.",
                    "label": 0
                },
                {
                    "sent": "So from all the topics that end up in our final model, that entity will only be associated with some subset of these of these topics, and that is handled by that.",
                    "label": 0
                },
                {
                    "sent": "Topic indicator vector B.",
                    "label": 0
                },
                {
                    "sent": "Based on that, when you then know what other topics are specific entity is associated with.",
                    "label": 0
                },
                {
                    "sent": "The second variable describes the proportions between these different topics so that you know what are the different topics and how they relate to each other.",
                    "label": 0
                },
                {
                    "sent": "When you have a given resource like C1 and finally we also have the more classical topic word assignment that addresses the textual information you'll find there.",
                    "label": 0
                },
                {
                    "sent": "So when you look into this structure, we have more or less two ingredients.",
                    "label": 0
                },
                {
                    "sent": "One is the more relational part that comes from the RDF structures and one is the more classical topical part that is more known from these topic models in the end.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's come back to our example.",
                    "label": 0
                },
                {
                    "sent": "You have already seen that very briefly here.",
                    "label": 0
                },
                {
                    "sent": "In our trivial example, we have four topics T1 through T4, and you see for all topics we have listed here, the specific words that have a high probability to show up in that specific topic.",
                    "label": 0
                },
                {
                    "sent": "So that is the standard way you cover the textual information.",
                    "label": 0
                },
                {
                    "sent": "So for topic T1 we have words like company, employer, industry.",
                    "label": 0
                },
                {
                    "sent": "That have a higher probability to show up in the context of topic model T1, whereas for topic tool we have words like CEO or Manager or engineer.",
                    "label": 0
                },
                {
                    "sent": "So that's the Classical World Co occurrence with related to the topic models.",
                    "label": 0
                },
                {
                    "sent": "But then we have these two additional ingredients, so we have the information that is related to the class information we have in these RDF data around.",
                    "label": 0
                },
                {
                    "sent": "So you see, for example for topic T1 we have classes like organization or company or airline.",
                    "label": 0
                },
                {
                    "sent": "And you also see that cardiac glass companies popping up in.",
                    "label": 0
                },
                {
                    "sent": "Topic T1, so it's a popping up in T2 and T3 but not in T4 and you see the different weights so company is most relevant for topic T1 because here we have the highest weight compared to the other topics.",
                    "label": 0
                },
                {
                    "sent": "Which means when you consider the entities that are of type of company, they are most often related with textual information where words like company, employer, industry are used.",
                    "label": 0
                },
                {
                    "sent": "So that is the relationship between.",
                    "label": 0
                },
                {
                    "sent": "The word information in the glass information, and there's a third and create, and we also have the relationship information, so we have these directed relationships, like owning company relating topic T1 with topic T tool again by using these weights indicating how relevant that relationship is for the different topics.",
                    "label": 0
                },
                {
                    "sent": "Again we can characterize that in a way that entities.",
                    "label": 0
                },
                {
                    "sent": "The the the topic T one that show that holding company relationship are more often related to birds like company employing industry when compared to entities that are related to topic T2 or T3.",
                    "label": 0
                },
                {
                    "sent": "We also take here the direction of the relationship into account, so it's relevant whether the relationship is going from T122 or vice versa.",
                    "label": 1
                },
                {
                    "sent": "So in that way we try to capture.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, word that excellent information, and on the other hand we want to capture the role class is an relationships play in that context.",
                    "label": 0
                },
                {
                    "sent": "So that is the model that is generated as an output.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's have a brief look at the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use two datasets to evaluate that approach.",
                    "label": 0
                },
                {
                    "sent": "One was taken from the PLP or the conference and paper information, and we use, for example, the abstract as representative for textual data.",
                    "label": 1
                },
                {
                    "sent": "In that context, second domain was taken from DB Pedia, where we selected a subset of the movie domain describing some movie information and they are when you look into the details what is taken as attribute values is then treated as textual data in that context.",
                    "label": 1
                },
                {
                    "sent": "We consider two tasks.",
                    "label": 0
                },
                {
                    "sent": "One is link prediction, the other one is object clustering.",
                    "label": 1
                },
                {
                    "sent": "I will only address the link prediction object clustering you may find in the in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we talk about link prediction, what was the task of why we wanted to predict the auto relationship between papers and orders for one domain an for DB pedia?",
                    "label": 0
                },
                {
                    "sent": "We wanted to predict the starving relation between movies and actors so that are the tooling predictions and we compare that with well known approaches from the literature.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you see, here of the first evaluation for the DB LP data set, we use different splitting strategies for training data and test data, so that are the three different settings you'll find here, and we always consider precision recall and accuracy, and you see the yellow column here is representing our TM approach that it behaves very well compared to the other approaches.",
                    "label": 0
                },
                {
                    "sent": "The Spectra precision, recall and accuracy, so that was the order link produce.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we moved to DBPR relating the addressing the movie domain, we can see that our approach is behaving rather well, but we also see that in that domain the MLN approach is doing very well with respect to recall, but we know then go into details.",
                    "label": 0
                },
                {
                    "sent": "You see that you get a high recall but somehow related to the issue that you also get a lot of false positives.",
                    "label": 0
                },
                {
                    "sent": "So that is somehow related to each other you can.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See that into some more detail when you look into the evaluation of how the different approaches really behave with respect to true negatives.",
                    "label": 0
                },
                {
                    "sent": "So to predict that link does not exist between a paper and an order, for example, or the actor is not really starring in a specific movie, and then you see that.",
                    "label": 0
                },
                {
                    "sent": "MLN approach that provided rather high recall, but including a lot of false positives, behaves rather weakly.",
                    "label": 0
                },
                {
                    "sent": "In that context, becausw for predicting the true negative rate.",
                    "label": 1
                },
                {
                    "sent": "Merlin is not really well working again, our approach is rather good working compared with these other approaches, so that is 1 evaluation.",
                    "label": 0
                },
                {
                    "sent": "The paper you find a second evaluation at addressing the object clustering.",
                    "label": 0
                },
                {
                    "sent": "Evaluation",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issue.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's conclude we think that these topical relational models provide good approach for handling these text, which are the F data and are well able to capture the relationship between the textual information, the structural information.",
                    "label": 0
                },
                {
                    "sent": "You have seen a 2 application scenarios link prediction object clustering.",
                    "label": 1
                },
                {
                    "sent": "We also work on other application scenarios for like for example when you're think of hybrid sparkle queries, you post with these RDF data.",
                    "label": 1
                },
                {
                    "sent": "One issue is for optimizing the query processing to estimate the result size of intermediate results.",
                    "label": 0
                },
                {
                    "sent": "In that context, you can use that approach as well, or when you think of keyword based search on RDF data.",
                    "label": 0
                },
                {
                    "sent": "For really coming up with the results, we have to somehow estimate what are the most probable connection between entities given the specific keywords for the query.",
                    "label": 1
                },
                {
                    "sent": "So that are also application areas where that model might be used and in the future we might work on things like for example hierarchical topic models.",
                    "label": 0
                },
                {
                    "sent": "OK, that is what I wanted to present.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}