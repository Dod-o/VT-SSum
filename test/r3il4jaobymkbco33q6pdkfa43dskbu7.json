{
    "id": "r3il4jaobymkbco33q6pdkfa43dskbu7",
    "title": "Measures of Statistical Dependence",
    "info": {
        "author": [
            "Arthur Gretton, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss06au_gretton_msd/",
    "segmentation": [
        [
            "So you have a cigarette, and so I work at the Max Planck to like some extent on applications of machine learning to neuroscience.",
            "So for instance, some of the problems we have is such as class.",
            "Robert tells you.",
            "I mean when you get brain signals, there are many different components due to many different sources, some of which may be interference and some of which are important.",
            "You want to separate the important ones.",
            "Another problem we encounter is that if you record.",
            "From electrodes you get many neurons.",
            "They all have different spikes at different times, and you might want to know like which neurons are making what spikes, and what makes them spike.",
            "So these are other applications that you can use machine learning for.",
            "These will motivate a bit.",
            "Some of the methods I'll talk about today, so I won't speak about the details of the neuroscience."
        ],
        [
            "Of things OK, so the talk is going to have three components.",
            "The first one is my view of independent component analysis, so I know that class Robert has talked to a bit about this already.",
            "I'll be giving a view from a slightly different direction.",
            "So that's going to then motivate the next part of the talk, which is kernel independence measures.",
            "This is one of the things I work on.",
            "So this is basically the sort of philosophy or underlying thing is that given some number of pairs of random variables, can you determine if the distributions that generated this random variables are independent or dependent?",
            "So one obvious application of this is ICA, but general independence test has much wider implications and just source separation, so I'll talk a bit about that.",
            "The last thing I'll go over is this thing called the two sample problem, which is that you get 2 samples and you want to know if they are from the same distribution or not.",
            "And yeah, this is a problem which is very interesting if you've got, for instance the neural spiking that I talked about.",
            "And if you want to work out if it's the same neuron which is generating the spikes that you measured on different days, or if it's a different one."
        ],
        [
            "Yep.",
            "Is that better and everyone read it OK good?",
            "OK, so just some notational conventions.",
            "I write random variables in a sensory front, and I will overload the notation for distributions and densities, so a distribution and the density will be written as a capital San serif P. And also I'm using covariance matrices, which I'm sure you know what they are."
        ],
        [
            "Tie.",
            "Right then, so OK. First I'll talk about ICA in a sort of."
        ],
        [
            "No sense.",
            "So this is just a page of notation, so you know what I say is you have independent sources.",
            "You mix them linearly with the matrix, which I'm going to call A and then you make some observations.",
            "And then what you're trying to do is to estimate the unmixing matrix.",
            "Now the important sort of way to think of ICA is that you're really only caring about the matrix, and so you don't want to try and workout what your sources are.",
            "All you want to say is that they are independent and you want to estimate the matrix.",
            "You don't want to estimate anything else about your sources.",
            "So what I'm going to do first is to talk about a very simple case where you actually know what your sources should look like.",
            "And then you want to just solve for the matrix, and I'm going to sort of use that as a springboard to talk about more."
        ],
        [
            "ICA methods?",
            "So this is when you have a sample because of course you don't have the true densities in real life, so that's my notation for this."
        ],
        [
            "And examples classes talk to you about OK, so here is toy example of ICA just to sort of get everyone familiar with the diagrams that I will be using.",
            "So the first source is uniform.",
            "It has a distribution which I've drawn a sample from.",
            "It's flat, the second one is bimodal it."
        ],
        [
            "As to peaks.",
            "And when you multiply this by a matrix, this is the kind of transform that you're making, so you can have some share.",
            "You can have some rotation you can sort of stretch it in the X&Y axis.",
            "And you're trying to work out what the matrix is that made this transformation."
        ],
        [
            "So, given that you actually know what the source densities look like, there are actually some respects in which you can't recover the matrix that combine them.",
            "And I'm going to just talk about some of these examples now.",
            "So in the first example, you know that both have a probability density functions are Gaussian.",
            "And you're mixing them this time with the matrix that rotates them.",
            "And even though you know what the sources look like, you cannot recover this rotation."
        ],
        [
            "Matrix.",
            "So here's basically an illustration of that.",
            "You have your input sources on the left, which I'm always going to draw in red, and then you're rotating them and you can see that you've just got a sphere and you're rotating this randomly, but there's no way, even though you know your sources, that you can recover the matrix that makes them."
        ],
        [
            "So here's another example here.",
            "You have two sources that are the same.",
            "And you are again rotating them, so by 45 degrees you can recover the well.",
            "You know that the sources are not independent and so you can make some effort to recover them by rotating them backwards.",
            "But when these sources are rotating by 90 degrees, when you swap the source order, of course you can't know that you've done that because the sources are the same.",
            "So again, even though you know your sources, you can't recover them."
        ],
        [
            "OK, the final example is if your sources have a symmetric distribution around the origin and then you just flip them so your mixing matrix is a flip.",
            "In that case, again, you're not able to recover this matrix, so you know your sources, but this is an uncertainty in your recovering of them.",
            "So that's just a summary of these things.",
            "The other thing I mean you can't separate signals to the everywhere constant, because this is a meaningless."
        ],
        [
            "Thing to do?",
            "The Gaussian case I talked about.",
            "Another case is that if this is sort of a point that gets made later, if you only care if your signals are independent, you can't recover their amplitude because you don't know in a sense how big the signals were to start with, as compared with how big they are made by the matrix, and so if you're saying only that you want independent sources, you can't recover how much the sources made each.",
            "Sorry, the mixing matrix made individual sources bigger or smaller."
        ],
        [
            "OK, so the first step of ICA is decorrelation, so I know clouds."
        ],
        [
            "Commit about this.",
            "So I won't sort of talk too much, but yeah, the idea is that you multiply the sources by a matrix.",
            "Which is going to make them."
        ],
        [
            "Have zero correlation.",
            "So to show what this means, here's an example.",
            "You've mixed the sources with a completely random matrix.",
            "That's the central picture.",
            "And then on the right hand side you can see the source is decorrelated, although it's a bit dim.",
            "So basically what this means is that having the correlated the source is all you need to do is to rotate them to recover them.",
            "So the decorrelation is recovering all of the mixing matrix except for rotation."
        ],
        [
            "Now just a quick drawback of doing the correlation and rotation separately, is that in fact you don't get as good an estimate of the unmixing matrix as you would get where you to solve for it, like in its entirity.",
            "So solving for the whole matrix is much harder, but splitting it up in this way is a way of saying that you think that getting the decorrelation right is much more important than getting the remaining part of the matrix right, and so if you sort of go through and do the analysis, you can find that the precision with which you can recover the mixing matrix is less.",
            "If you do it in two steps and if you do it in one.",
            "Generally this doesn't matter, but it can matter.",
            "For instance, for sources which have unbounded variants and things like that."
        ],
        [
            "OK, so what's left is to recover the rotation.",
            "And so here I've sort of enlarged the pictures that you saw earlier."
        ],
        [
            "So rotation matrices look like this.",
            "So in two dimensions, the rotation matrix is parameterized by only one parameter, which is this angle of rotation.",
            "For higher dimensions, you can generalize this so you can have rotations about the XY and Z axis, which I've written there, and so on."
        ],
        [
            "So let's again look at the case where we know what our source density looks like.",
            "So what I say.",
            "What I've drawn up in the top RAM, you have an estimate B of your unmixing matrix.",
            "And you have P hat of S, which is your source density.",
            "So that's what you believe your source density to be.",
            "And The thing is that here you your source density model is correct, but your unmixing matrix you don't know you're trying to estimate it.",
            "Now the estimated density of your observation matrix given your source model and given your estimate B is given by this central expression, there P hat of X.",
            "Which is basically yeah you can.",
            "You can find the expression for that as a function of B and it's a function of the source densities.",
            "Now to recover B you can maximize the expected log likelihood, which is the expectation of the log of your estimated density function.",
            "And when, yeah, when you sort of have to estimate this with a sample, you get the bottom expression there where you can see the expectation of the log is now computed empirically on the basis of the sample that you observed.",
            "But bear in mind that the source Den."
        ],
        [
            "City is correct here.",
            "So here the sources are both mixtures of Gaussians.",
            "What you do in this case is that you rotate them by 45 degrees and you get the central picture there.",
            "And on the right hand side, I've written the log.",
            "Likelihood is a function of the angle of rotation, so the red bar in the center is the correct answer, and then the log likelihood is is the blue curve and you can see that the log likelihood is maximized when your unmixing matrix has the correct angle."
        ],
        [
            "Say yeah, this is.",
            "Now where this is wrong and where this also creates problems in real ICA is if your model is incorrect.",
            "So imagine your input sources to Laplace densities, so that's sort of very heavy tailed densities, which I've drawn on the left hand side and you rotate them by 45 degrees, but you use the same model that I used before, so these mixtures of Gaussians.",
            "Now you can see that the best rotation that aligns these samples with the model is in fact one that doesn't change it at all from 45 degrees, because that's the point at which the density of the mixtures best overlaps the model.",
            "So using this incorrect model for this source combination is going to give you the worst possible answer, which is that it's going to give you the most independent sources rather than the least.",
            "So this is a risk you do when you're using a model which isn't correct."
        ],
        [
            "OK, so now I'm going to talk about ICA as it's done in practice and this is using contrast function."
        ],
        [
            "So, just to recap on something I talked about earlier, you remember that you can't recover the sources for certain operations because this is sort of inherent ambiguity.",
            "So if you swap the source ordering, for instance, you don't necessarily recover them.",
            "If you swap the sign, you don't necessarily recover, and if you re scale likewise.",
            "So basically what ICA should do is not to try to recover this source matrix exactly, but only up to these ambiguities."
        ],
        [
            "And So what I what people define then, is something called a contrast function, which is 0 if and only if the recovered sources are mutually independent.",
            "So this means that it doesn't care about these ambiguities in the mixing matrix, it only cares about getting independent sources, and the sources recovered will still be independent despite these ambiguities.",
            "So this is sort of ideal of a contrast function, but in practice what people really do is that they have something which they want to be the smallest when the random variables as independent as possible, so it's not necessarily going to be 0 and the random variables are not necessarily going to be independent, but people would like them to be, so it's a sort of in practice.",
            "Yeah, it's not this nice idea of being 0 independence.",
            "And the other thing is that maximum likelihood with the correct density model we're going to see is going to give you the best possible contrast that you can get."
        ],
        [
            "OK, so I'm going to talk now about how maximum likelihood with the correct densities gives you a contrast function.",
            "So you can decompose the maximum likelihood into a sum of two terms, a constant term and a KL divergent between the density of the unmixed sources.",
            "So this is this P of BX up there, and the model density of your sources.",
            "Since this is the P hat of S. So the KL divergent is something which is 0 if and only if the two densities are the same.",
            "So this is something which is going to be 0 when you're unmixing by multiplying the observations by your estimated matrix.",
            "B is equal to the original sources.",
            "So obviously when your density model is correct, that's exactly the definition of a contrast function, so."
        ],
        [
            "Now another one which is very nice thing in theory is the mutual information.",
            "So this is going to be equal to 0 if and only if the joint density has the same as the product of the marginal densities.",
            "So this is the same as saying it's zero if and only if the random variables are independent.",
            "Now people don't use this in ICA because it's basically hard to estimate empirically.",
            "So in 2004 someone actually did a paper on this where they used a very clever method to estimate mutual information.",
            "But before that it had kind of been."
        ],
        [
            "Ignored.",
            "So you can actually simplify this expression in the following way.",
            "You have a sum of three terms, the sum of the entropies of the unmixed source is the entropy of the observations, which is constant with respect to the unmixing matrix, so you can ignore it analog determinant term, which you can ignore as well because it's zero when you're when you're having a rotation matrix.",
            "So it's equal to 1 rather.",
            "So this means that as far as optimizing over the unmixing matrix is concerned, you only need to care about the sum of the entropies.",
            "And entropies are easier to estimate the mutual information, but actually I mean not.",
            "It's still quite tricky.",
            "So rather than using entropies, what people do instead is to use the expectation of some nonlinear function of the observations so this nonlinear function in an ideal world would be the log of the density, but since density estimation is a tough problem, you use some other nonlinear function."
        ],
        [
            "So this is an example of some nonlinear functions that people use, so there's infomax, which is sort of heuristic.",
            "And fast ICA as well.",
            "I mean generally what these functions try to do is to sort of describe in some cartoon sort of term the properties that you care about.",
            "So if for instance you cared about non gaussianity, you could expand the densities about the Gaussian using the gram shalya expansion and then you could use a low order approximation to this and then you could say that this captures the properties that you care about and that all of these other sort of properties are less."
        ],
        [
            "Interesting.",
            "So we're going to see shortly how this works in practice.",
            "So first of all I need to define something called kurtosis, which is a statistic which describes certain properties of random variables.",
            "So basically it's sort of describing in a way how spread out the random variables are.",
            "So Super Gaussian source has very heavy tails and so it's sort of very spread out and a sub Gaussian source is something that's more compact and more concentrated about the origin.",
            "And some contrast functions are described explicitly in terms of the kurtosis.",
            "So this bottom one there the Jade type contrast is defined in this respect.",
            "And others implicitly need the kurtosis to be big or to."
        ],
        [
            "A small so I've just given an example now of two sources and then mixed versions for Super Gaussian for sub Gaussian.",
            "So the uniform source density is a sub Gaussian example and the Super Gaussian one is much more heavy tailed, so you can see it's got a lot of sort of outlier."
        ],
        [
            "OK, now here is just applying these contrast functions that I defined to sources which are both Super Gaussian and sub Gaussian.",
            "And you can remember that these contrast function should be smallest when you get the correct answer.",
            "So here you can see in fact that for Jade, when the sources are Super Gaussian, the contrast is biggest.",
            "When you get the correct answer.",
            "And Conversely, the infomax an fast ICA contrast the biggest at sub Gaussian sources.",
            "So this example is quite simple.",
            "You could say, well, you can just make a test if it's super Gaussian or sub Gaussian.",
            "If it's yeah, if it's super Gaussian, you just flip Jade around.",
            "If it's sub Gaussian, you just flip IMAX and fast I see around.",
            "But I mean this is this is sort of a quite simple example and in practice you can get a much more subtle difference between the true characteristics of the sources."
        ],
        [
            "What you're looking for?",
            "So this means that if you're using a fixed nonlinear function and you're trying to take the expectation of this function, and say that this is a bit like the entropy, then you will be able to find sources that trick that.",
            "And this kurtosis example is 1 example.",
            "But there are others as well.",
            "So if you're using an algorithm on the Internet, you have to be careful with it.",
            "And yeah, Klaus also talked about some tests of how good a particular nonlinear function is in estimating.",
            "Like yeah, the independent sources."
        ],
        [
            "OK, so if your sources are not IID, then there's another trick you can do which is on this slide.",
            "So basically, if your sources are like music sources for instance, then they depend on their values in the past, so sound wave or something is not going to be independent at each time instant, but the value of the signal is going to depend very heavily on the values it had at previous times.",
            "So this means that.",
            "When you're separating sources that have this property, you can do much better than to just make them independent at any given time instance, which is that you can actually say that the source one source at any particular time should be independent of all the other sources at every possible time in the past.",
            "So if you have a criterion which measures independence, you can measure it not like between the sources, not only at the present, but also between one source and it's past values.",
            "You basically have, like many, many more constraints than you would have and you should be able to get a better solution like that.",
            "So I'm going to give an example here where we just use correlation instead of independent.",
            "So it's just a sort of proxy measure for independence.",
            "And you can also define the correlation here.",
            "Between a signal and it's past values.",
            "So this is the correlation of like a vector of sources at time T and the vector of these sources.",
            "Yep.",
            "T rump.",
            "Yes, sorry, that's a chief.",
            "Another tell.",
            "By the way, yeah, so that means that if you have station resources, it's not going to change this covariance matrix when you measure it at different times, but only the delay.",
            "With respect to which you measure, it is important."
        ],
        [
            "OK, so this now can give you a nice optimization problem.",
            "So this top line here says that you should have no correlation with the signal and itself at any at any given time, and so basically you're multiplying.",
            "Your observations by the truth and mixing matrix and then you're getting something that should look like a diagonal, so no signal should be correlated with any other.",
            "But you can also do this with this time delayed correlation, so you multiply it with the same true and mixing matrix, and you should also get something which is diagonal.",
            "So a signal should only be correlated with itself at past times and not with the other signals.",
            "So this then gives you an optimization problem with respect to one matrix, which you can just solve.",
            "So you do a bit of algebra and you get this expression, which is a closed form solution for the unmixing matrix.",
            "Now, if you have multiple time delays, then there's a procedure called joint diagonalization and that allows you again to recover your sources in this case, and so you can sort of jointly solve for all of the time delayed."
        ],
        [
            "Equations.",
            "OK, so this was."
        ],
        [
            "The quick introduction to ICA and now I'm going to talk about kernel independence measures.",
            "So these are basically kernel functions of your variables, which is zero only at independent.",
            "So in this sense there are contrast function as is used in ICA, so they're more than decorrelation.",
            "They take into account high order dependence as ICA, as mutual information does.",
            "Um?",
            "Now, a great advantage of these kernel independence measures, which we're going to see is that they make sensible assumptions about smoothness.",
            "So there's an underlying density function which is smooth and the kernel functions will make certain assumptions about how smooth these densities are.",
            "Tell you what this means in practice, but it basically means that they're very much more robust than than some other methods.",
            "Um, so applications are independent component analysis.",
            "Someone has used it for feature selection, so finding innocence features that are important for making predictions.",
            "And I've used it also for finding dependence detection in the visual cortex.",
            "So when you take an MRI scan of the visual cortex, you have a certain amount of activity going on and you want to know if activity in different parts is independent or dependent.",
            "So if there's some sort of Co processing going on."
        ],
        [
            "OK, so the particular kernel independence test I'll talk about is called the constraint covariance or cocoa.",
            "And it has the three properties which I've described here.",
            "It's an independence measure for universal kernels.",
            "Now I'll say what it means to say kernel is universal shortly.",
            "And I will also talk about how to make a statistical test of independence using the constraint covariance.",
            "So this is a different application to ICA.",
            "You just want to test if variables are independent or dependent.",
            "I'll talk about where dependence is hard to detect, and this will also be where the smoothness assumption comes in, and I'll also talk about how to choose the kernel because you have, like if you're using a Gaussian kernel, you have a kernel size and you have to choose that.",
            "Anne.",
            "And yeah, I mean the great advantage of this test and this is like oh, good independent tests have.",
            "This property is set as your sample size increases, the chance that your test is wrong is going to drop very fast.",
            "And I'll also talk about I've used this method in ICA and I'll say where it's good."
        ],
        [
            "I see as well.",
            "OK, so let's just look at what it means to detect dependence.",
            "So first I'll talk only about second order dependence.",
            "So you get these pairs of points which have drawn here, so you get X1 and Y1 together.",
            "They're in different spaces.",
            "These spaces can be high dimensional and you get X2 and Y2 and so on.",
            "And you want to know is there any dependence between the distribution of X and the distribution of why?",
            "So are X&Y related in?"
        ],
        [
            "Anyway.",
            "So one way to do this, which is a second order method, is that you basically take the interview, choose two directions, One Direction in the X space and one in the white space.",
            "And you take the inner product of every point in the X space with its direction and every point every point in the white space with its direction and so then you get 2 vectors which are as long as your sample size.",
            "And then you basically take the covariance between these vectors.",
            "So you take the inner products.",
            "Now if this is 0 for every pair of directions that you can choose, then there's no second order dependence between the X and the way, so there's no correlation between the X and the way.",
            "Yeah, in the."
        ],
        [
            "Case.",
            "So how do you generalize this to general measures of dependence as opposed to just second order?",
            "So what you can do then is to map your points into some infinite dimensional space.",
            "So this is the arcade chess.",
            "So this mapping I just remind you it has like it's infinitely long and it's got basically nonlinear functions of your input.",
            "So the higher the index, the more rough like the less smooth these nonlinear functions are, and it's also got these scaling factors in front of these nonlinear functions, and so as the index of the scaling function gets higher, these also becomes smaller.",
            "So you basically emphasize less and less the rougher and rougher functions.",
            "And so now you can do exactly as I described in the previous slide, so you can choose two directions in these infinite dimension feature spaces.",
            "You can take the inner product of each of the points with these infinite dimensional features, and you can obtain the covariance between these inner products and this is going to be 0 only at independence as modular.",
            "Some property of these.",
            "Um?",
            "Which is the universality.",
            "So just a thing to pay attention to.",
            "First, I've written here the same nonlinear function for the X and the Y, but they don't have to be the same.",
            "There can be different.",
            "The other thing is that I've divided here by the arcade just norms of these projection vectors.",
            "So because we're in an arc AHS season, our functions and these are KHS norms are roughness penalties, So what you're saying here is that you want to find basically functions of your random variables which are not too rough, which have high covariance, and This is why you need a roughness penalty.",
            "If you allowed your functions to be arbitrarily rough, then you could make the covariance between them.",
            "Arbitrarily big, so you have to have some sort of constraint on these functions, and so that's why you have this."
        ],
        [
            "OK, so now if you want to compute this in practice, you need to use the kernel trick.",
            "So this basically takes this infinite dimensional problem and makes it reasonable intractable.",
            "So this is a lot like what you see in kernel PCA.",
            "Here, I've abused notation a bit.",
            "The problem on the previous slide is equivalent to this problem, where these are the infinite dimensional projections and this is an infinite dimensional covariance matrix between these enormous vectors.",
            "Now when you have a finite sample, you have this empirical estimate of this infinite dimensional covariance."
        ],
        [
            "You also can write your directions that you project on as linear combinations of the map, sample points, and the reason for this is that any component orthogonal to them would just vanish when you take the inner products on this slide.",
            "In that top equation.",
            "Now that you have.",
            "OK, so you then OK.",
            "I'm defining my kernel function now as being the inner product between the sample points."
        ],
        [
            "And then doing this allows you to write the equation from a couple of slides back entirely in terms of these matrices of inner products.",
            "So this means that now you are able to solve it.",
            "I mean, this is this standard approach."
        ],
        [
            "OK, so the constrained covariance is 0 if and only if the random variables are independent.",
            "There are actually other methods that have this property as well which are quite similar to this, so I should just mention them.",
            "So here I've taken a covariance.",
            "You could also take a correlation.",
            "There are also some other generalizations which are bounds on the mutual information, which I won't go into here, but they are also related to either the covariance or the correlation between these infinite dimensional vectors."
        ],
        [
            "OK, so now I mentioned universality several times.",
            "So why is it that the kernel needs to be universal?",
            "So the best way to say this is to describe what happens when it isn't.",
            "So here is a kernel that isn't universal.",
            "It's explained kernel, and here I've taken the spectrum of this kernel and you can see this spectrum has some points at which it's 0.",
            "So what this means is that if you define.",
            "Um?",
            "OK, so you remember a few slides back.",
            "We had these infinitely infinite dimensional vectors.",
            "In the feature space.",
            "So basically, if your kernel has some zeros, it means that it's effectively crossing out certain points in these infinite dimensional vectors, so it's the same as saying that it's going to penalize frequencies at this over functions at these frequently frequencies infinitely much.",
            "So if your density.",
            "Has components at these frequencies.",
            "Then you're going to have a lot of trouble.",
            "So if you're penalizing these frequencies infinitely much, then these functions here are trying in some sense to adapt to a density which has very strong components at a point where there infinitely penalized.",
            "So they're basically not going to be able to detect the the dependence encoded at these frequencies."
        ],
        [
            "OK, so now I'll talk about him doing statistical tests of independence.",
            "Using this constraint covariance.",
            "So for this I'll just give a quick overview of statistical testing terminology.",
            "So you have a space of probability distributions which you partition into two subsets, which is this calligraphic P nought and its complement?",
            "And you have some probability measure which is taken from somewhere in this space.",
            "Now what you want to test is the hypothesis of whether your PZ is in calligraphic peanut or it isn't.",
            "So the null hypothesis is we define it as being that PZ is in peanut and the alternative hypothesis is that it isn't.",
            "Now you don't have access to PZ.",
            "In practice, what you have is a sample from it.",
            "And what you want to do then is to say that if the sample is in some particular region, you will reject H. Note, you will say that PZ is not in P0.",
            "And if it's outside this rejection region in some acceptance region, then you accept that the null hypothesis is true.",
            "So I mean, this is quite abstract, so in practice, what you need to do is you have to compute some function of the sample, which is called a test statistic.",
            "So usually this Maps to the real line, but it doesn't have to.",
            "And then you have basically intervals on the real line which this test statistic falls into, and some of these intervals of the rejection region and the remainder of the acceptance region.",
            "So just to give a concrete example, if you compute a sample mean.",
            "Then you might want to say is this sample mean positive or not?",
            "So it's sorry is the underlying mean that generated this sample mean positive or not?",
            "So to test this, what you would do is you would say, well, is the sample mean not too far from zero in the positive direction, and if it's if it's sort of like, you know if it's close to 0 or negative then you would say Yep, I believe that the true mean is not bigger than 0.",
            "And if it's very, very positive you say, well, then it's very unlikely that the true mean is less than."
        ],
        [
            "Here.",
            "OK, so now we want to say.",
            "I mean you can make any number of tests and you want to say how good a particular test is.",
            "So there are two kinds of errors that a test can make, so one is a Type 1 error, which is you reject the null hypothesis even though it's true.",
            "And then there's the Type 2 error which you accept the null hypothesis, even though it's false.",
            "So it's very.",
            "Messy to remember which is which, but nonetheless.",
            "So from this you can describe the power of the test.",
            "Which is the probability under the true distribution that the sample is in the rejection region.",
            "So what this means is that you want the power to be close to zero when the null hypothesis is true, because that means that you want it to be very unlikely when the null hypothesis is true that you saw your sample goes in the rejection region and you reject the null hypothesis, and likewise it should be close to one when you're in the complement of peanuts.",
            "So when you should reject the null hypothesis, the probability of rejecting it for a particular sample should be high.",
            "Now the level of the test is something that here, as a user have to specify.",
            "So what this is is an upper bound on the probability of making a Type 1 error.",
            "So what you're going to do is, you're going to say, well, my test should have this upper bound, and then the quality of the test is then innocence determined by how bad your Type 2 error can be.",
            "Dependent on this upper bound.",
            "So I mean, this is, I mean the upper bound is something you specify, but it doesn't let you cheat.",
            "So if you're saying if the mean if you're testing that the mean is less than or equal to 0, you could say, Oh well, I'll say it's always true, and then my Alpha is going to be 0.",
            "But of course, then your Type 2 error is going to be dreadful.",
            "It's going to be 100, so yeah."
        ],
        [
            "OK, so now I'm going to talk about specifically testing independence.",
            "And one difficulty in testing independence is in fact that no test, no matter how good, is going to do it perfectly.",
            "For a finite sample size.",
            "And I'm just going to give a quick example of this.",
            "So here you've got your total set of probability distributions over a particular vector, and you partition this into two sets.",
            "So one set is the set of all possible independent random variables and the other set is all possible dependent ones.",
            "Now what you can say now is my test statistic is a very simple one.",
            "It's just some test.",
            "We don't care what it looks like, but it's going to return one when you're dependent and zero when you're independent.",
            "So very, very simple to make a test with this statistic.",
            "It's just is it one or is it 0?",
            "And we don't care about how we got this answer.",
            "We just say, like magically, we got the best possible tests that can give us this answer.",
            "And what's more, you say that you have this upper bound?",
            "This is your design which is Alpha on the Type 1 error.",
            "Now you can prove that for a finite sample size there is.",
            "There exists some very very tricky distribution.",
            "Which gives you a Type 2 error greater than one minus Alpha.",
            "So if you say I want my test to have a probability of type 1 error of 5%, then for any finite sample there's going to be a really tricky distribution that will give you a 95% type 2 error, no matter how good your test is.",
            "Um?",
            "So the way I can tell you the way you can construct this, this very bad example is very simple, so you have your independent distribution.",
            "And then you have independent one, which is a mixture of this independent guy and then some dependant one with very very small probability.",
            "So what this means is that if you take some number of samples from the dependent one, it's very unlikely that you'll ever see the independent distribution.",
            "You need a huge number of samples before you see it, and so that means that for, like for any sort of large sample size, you can make the mixture weight of the dependent component so small that you're just basically never going to see it for the number of samples that you drew, and so your Type 2 error is going to be as large as you want it to."
        ],
        [
            "OK, so this is a very like that.",
            "Example was a very simple example of a hard to detect dependence.",
            "So now I'm going to talk about a bit of a tougher example, which is where like it's going to describe why you need to make good smoothness assumptions.",
            "If you're going to do a test.",
            "So the joint density function I'm going to use for this example is a kind of weird one.",
            "So it's a constant term plus a scaled product of these nonlinear functions.",
            "So these are the same nonlinear functions that we saw earlier for the kernels.",
            "So and then for the infinite dimensional OK, just so it has the same property as this coefficient gets bigger, these functions get rougher and rougher, so you can imagine a density that's getting rougher and rougher as your coefficients as the L term gets larger.",
            "Now The thing is that as you increase the L value, what this is going to do is to make the constraint covariance arbitrarily small.",
            "Now, the reason for this is these norms in the denominator.",
            "So as this gets rougher these functions are going to try and match the density which is generating the random variables.",
            "But if the density is very rough then these functions will be also very nonsmooth.",
            "This denominator is going to become very large, and this quantity is going to become very small.",
            "And the bigger the L said, the rougher the density.",
            "The smaller the constrained covariance will be."
        ],
        [
            "Um?",
            "Now just to give you an example, I mean here I've used is my nonlinear function.",
            "This product of sinusoids.",
            "And up here you have a case where this L is very small.",
            "You have a smooth density and you can see here just from this 500 sample that the X&Y are dependent.",
            "Because yeah, if X has values around here, then yeah.",
            "I mean you can sort of see that the density is changing, whereas in this case when you have a very rough density from 500 samples, you can't even see that it's not just a uniform density and a uniform density over X&Y is an independent density.",
            "So in fact, what you want is that for your test statistic not to tell you when it only sees 500 samples.",
            "Yes, I'm really sure that it was this density, so your test statistic has to be smaller when the density is rougher.",
            "Otherwise it's going to mislead you.",
            "It's going to tell you that it found some extremely difficult dependence, which you would need millions of samples to detect reliably, and so This is why having this function norm penalization gives you a test which is robust against making this."
        ],
        [
            "Mistake.",
            "And so this is how it works in practice.",
            "I've got rougher and rougher densities.",
            "This blue line here is the constraint covariance, and this red line is the constraint covariance for this sample size when the density is uniform and you can see that for these last two.",
            "The constraint covariance is statistically indistinguishable from the value you would get with the uniform density.",
            "So what this means is that you don't have enough samples at that point to detect that the random variables are dependent."
        ],
        [
            "OK, so now I've described the constraint events.",
            "I'll just talk about how you're going to get a test of dependence from this statistic so you know that it's zero at independence, and you know that it's going to be positive when it's not independent.",
            "So what you want to do is to do a test that the constraint covariance is going to be 0 or not zero or positive basically.",
            "So what you can prove, and I won't talk about how this is done, is that the empirical constraint covariance is going to be converging quite fast to the population one.",
            "So with high probability as you get more samples, the empirical estimate of the constraint covariance is going to be close to the population one, and the rate at which it gets closer is this term.",
            "Here, one on Route 10.",
            "So what you want to do is to then say is the constraint covariance bigger?",
            "Then this term.",
            "This term has two components of interest, so one is the Alpha.",
            "This is the probability of the random variables being independent, but you believe that their dependent and so this you have to control.",
            "You have to tell it that.",
            "And the other thing is 1 on Route N. So this is saying that as you get more samples this threshold is getting smaller and so your test is just going to say is the empirical constraint covariance bigger than this threshold?",
            "If it is, then you say they're dependent.",
            "If it isn't, you say they are independent and this will give you a test of size Alpha.",
            "So that's yeah, just reminding you what Alpha means, yeah?",
            "And this is this kind of has an interesting sort of philosophical implication, because what it's saying also is that as you get more samples for whatever else you choose, your Type 2 error is going to drop to zero at a fast rate at rate one on route end.",
            "So this is kind of weird because you can also show like independently that any regression method even like one that gets an arbitrarily good error for infinite data, is going to can learn this regression very, very slowly, so you can find you can invent a very tricky regression problem which is going to make even the best possible algorithm learn very, very slowly.",
            "But what this result is saying is that it can detect that random variables are dependent very fast.",
            "So your your Type 2 error is dropping very fast.",
            "So what this means is that even though you can't necessarily do regression between the random variables, as you get more data, you can nonetheless detect quite quickly as you get more data that there is a dependence, but."
        ],
        [
            "In them.",
            "So that's quite weird.",
            "OK, so.",
            "On a sort of more practical note, choosing the kernel size.",
            "So I've defined this function, which is going to in fact change as the kernel size changes.",
            "And you want to know effectively like what kernel size to choose.",
            "So I'll just describe now how to do this so.",
            "As a reminder, here's the arcade.",
            "Just normal function, and so here you've got these terms here, which are the Fourier coefficients of the kernel.",
            "And So what this means is that for rougher functions, so when these components at large I a big you're going to penalize them more because these inverses of the Fourier transform of the kernel are going to grow.",
            "So what this means in a sort of concrete sense is given by this drawing here.",
            "So I've drawn two kernels here.",
            "One is a very wide kernel and one is a very narrow kernel.",
            "OK, so you just changing the kernel size.",
            "Now the Fourier transform of these have the exact opposite property.",
            "The narrow kernel has a very broad Fourier transform because it's sharp, so it has high frequency components and the thick kernel has a very narrow Fourier transform because it only needs components at low frequencies.",
            "So what this means is that when you use an arrow kernel, you're going to penalize rough functions less because the inverse of this is going to grow more slowly than you will if you use a very broad Colonel.",
            "So a very broad kernel gives you a very large penalty for, uh, functions.",
            "So this might lead you to think, well, OK, if I can make.",
            "Um?",
            "If I can make rough functions not penalized at all.",
            "Then I could make my constrained covariance arbitrarily big.",
            "Because you could just make the kernel smaller and smaller.",
            "You can make the function norms in the denominator less and less penalized, and yeah."
        ],
        [
            "But in fact this is not true.",
            "And the reason is because the constraint covariance is computed on the basis of a sample.",
            "So what this means is that as you drop the kernel size, your constraint covariance is increasing, but only up to a point.",
            "And at this point even the samples from a space like your examples, so your samples from any one variable and no longer even similar to themselves, and so then it's going to drop again.",
            "So basically what this means is that your Alpha your upper bound on the Type 1 error is something that you set that to minimize the Type 2 error.",
            "For this Alpha you should choose your kernel size that maximizes the constraint covariance.",
            "So.",
            "If you maximize the constraint covariance, then basically that means that if for the if you optimize your kernel size to make cocoa as big as possible, and even then it doesn't cross the threshold for dependence, then you're absolutely sure that no dependence exists between them.",
            "So it's in a sense somewhere sensitive tests that you can get.",
            "Um?"
        ],
        [
            "In that respect, yeah.",
            "OK, so the other application of this would be independent component analysis, so you've got a contrast function.",
            "And yeah, it makes sense to do ICA with it, so this one.",
            "It doesn't have the tricky properties that you get.",
            "Yeah, it doesn't have the tricky properties you have with the fixed contrast functions.",
            "And the reason being that.",
            "Yeah, you basically, these functions are adapting to the density and so you can't sort of create a density which is in a sense contradicting the nonlinearity that you've chosen to unmixed signals.",
            "So in practice I'll just a couple of notes on performance, the kernel methods, and I think a lot of the modern methods, like the ones using like true estimates of mutual information.",
            "The one is there's another one using true estimates of entropy, so these at the moment are still too slow for large scale problems, so if you've got more than around 16 sources then you're in trouble.",
            "The kernel methods specifically have better resistance to outliers than any other method that I've tested, so this is an advantage.",
            "And the other thing is that source code ptosis doesn't affect performance.",
            "So you saw that a lot of the classical methods are.",
            "Nonlinearity is a very sensitive to kurtosis and this isn't true of the kernel methods and a lot of modern methods as well."
        ],
        [
            "So I'll just show you what I mean.",
            "So here this is an experiment that I did.",
            "Hey, this is a performance of performance measure of how accurately you mix C samples, so it said divergents measure between the true mixing matrix in your estimate of it.",
            "And here I've adjusted the kurtosis of the sources.",
            "So I've got here 2 classical methods when is fast ICA and when is Jade and you can see that both of them in the vicinity of zero kurtosis very badly.",
            "So these fixed nonlinear functions.",
            "When you're getting close to 0 kurtosis are going to give you a lot of trouble.",
            "And here this bottom line here is all of these modern methods, so some of them are kernel methods, some of them are other methods which adapt to the sources and you can see that they basically don't care at all about the kurtosis.",
            "So this is basically the advantage of using.",
            "More sophisticated method that depends on the source densities rather than just fixing your nonlinearity and hoping that the source densities conform."
        ],
        [
            "To that.",
            "OK, this experiment now is outlier resistance, so don't worry bout this graph.",
            "This graph is basically the measure of divergents.",
            "Again, this is the number of outliers.",
            "So what you're doing is that you mix your sources and then you add some random outliers to this mixture.",
            "And here is fast ICA which is terrible.",
            "Here are some other modern methods that use estimates of the mutual information or use estimates of the entropy, and these are the kernel methods and the kernel methods.",
            "You can see much less sensitive than other methods that we can compare with."
        ],
        [
            "OK, so I'm going to talk now about the two sample problem."
        ],
        [
            "So this is a completely different angle to testing independence.",
            "Although yeah, there are some similarities to some of the techniques we are going to use when we compared to kernel ICA.",
            "So the two sample problem is basically you are given two samples and you want to test if they come from the same distribution or a different distribution.",
            "So the example I gave earlier was the spike example you want to test if the spikes you measured on different days come from the same neuron or from a different one.",
            "So you want to know if you can aggregate the data that you collected over these two days or not.",
            "Another case is speaker identification.",
            "So you have basically somebody who talks a bit.",
            "You get samples from this speech and then you have some sort of database of where he's still previously and you want to know if it's the same person or not.",
            "Another example is comparing paintings.",
            "You take patches from images and then you sort of use that as a way of modeling particular painter style, and then you have another painting.",
            "You take patches from that you want to know if it's the same paint or not."
        ],
        [
            "So the test statistic we're going to use for this problem is something called the maximum mean discrepancy.",
            "Again, we need our arcade just to be universal, and that's for reasons kind of similar to why we needed it for independence testing.",
            "And this is our definition.",
            "So you basically you take the supremum of all functions with unit norm.",
            "Of the difference of the expectations of these functions.",
            "So what you're trying to do is, you're trying to find a function which in a sense encodes the difference between the density that generated the X and the density that generated the way.",
            "And if whatever function you choose, you can't get this to be different then the density that generated these two is going to be the same.",
            "So this is the sort of idea."
        ],
        [
            "Behind it.",
            "OK, so now I'm going to talk about how to compute this with kernels, so there's a subtlety here, like when I talked about covariance.",
            "In fact, you have to be a bit careful when you're defining these infinite dimensional covariance matrices, so there it's a bit messy, but here I'll just describe what's going on in a bit more detail.",
            "So what you want to know is how do you define here?",
            "You have a mapping of some function, some random variable to an arc ahs and you want to say what does it mean to define the mean of this infinite dimensional mapping?",
            "Like what is the meaning of this mapping in the arc HS?",
            "So what it is?",
            "I mean, you need to know that because you want to say well, what do I do when I take an inner product of a function in the dark Ages with them in?",
            "And what we can do is to define the mean basically as being the inner product, the expectation of the inner product of this mapping with.",
            "Some function and so basically it's defined the mean element in the arc.",
            "HS is defined such that when you take an inner product with another element in the RKHS, you get the expectation of the function of that element.",
            "So, but nonetheless it's some infinite dimensional vector.",
            "It just happens to be the one that has this property.",
            "So the other trick I'm going to use this one.",
            "If you take the norm of a vector in the RKHS, that's the same as taking the biggest function in the unit ball and taking the inner product of that function with the vector."
        ],
        [
            "OK, so now we can define this maximum mean discrepancy in terms of kernels using the two things on that last slide.",
            "So first of all you can define this difference in the expectations of X / Y as the inner product of an arc HS function with these mean elements that we've defined.",
            "OK, that's by the definition of the mean elements.",
            "OK, now this is equal to the norm.",
            "By this trick we used to define the norm.",
            "You have a square here because that's from there.",
            "The norm is the inner product of something with itself when it's squared.",
            "And now you've got inner products, so you can use the kernel trick, and that's what we've done down here.",
            "Now the subtlety here is that you've got here at X&X Dash.",
            "So what you have to do in fact is make an independent copy of the X so that you can do this, because otherwise you would get some coupling between the things if they were the same.",
            "So this is just something you need to bear in mind."
        ],
        [
            "And where it causes complications is when you're making an empirical estimate.",
            "And So what happens if you get IID samples from PX&PY?",
            "To sort of get an empirical version of this independent copy trick what you need to do is take the sum only over those random variables with different indices only over the sample points.",
            "Sorry with different indices.",
            "So what you're doing is you've got your sample X which has endpoints and you take the sum only over the dissimilar ones, and then there's M * N -- 1 of those."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you have a cigarette, and so I work at the Max Planck to like some extent on applications of machine learning to neuroscience.",
                    "label": 0
                },
                {
                    "sent": "So for instance, some of the problems we have is such as class.",
                    "label": 0
                },
                {
                    "sent": "Robert tells you.",
                    "label": 0
                },
                {
                    "sent": "I mean when you get brain signals, there are many different components due to many different sources, some of which may be interference and some of which are important.",
                    "label": 0
                },
                {
                    "sent": "You want to separate the important ones.",
                    "label": 0
                },
                {
                    "sent": "Another problem we encounter is that if you record.",
                    "label": 0
                },
                {
                    "sent": "From electrodes you get many neurons.",
                    "label": 0
                },
                {
                    "sent": "They all have different spikes at different times, and you might want to know like which neurons are making what spikes, and what makes them spike.",
                    "label": 0
                },
                {
                    "sent": "So these are other applications that you can use machine learning for.",
                    "label": 0
                },
                {
                    "sent": "These will motivate a bit.",
                    "label": 0
                },
                {
                    "sent": "Some of the methods I'll talk about today, so I won't speak about the details of the neuroscience.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of things OK, so the talk is going to have three components.",
                    "label": 0
                },
                {
                    "sent": "The first one is my view of independent component analysis, so I know that class Robert has talked to a bit about this already.",
                    "label": 1
                },
                {
                    "sent": "I'll be giving a view from a slightly different direction.",
                    "label": 0
                },
                {
                    "sent": "So that's going to then motivate the next part of the talk, which is kernel independence measures.",
                    "label": 0
                },
                {
                    "sent": "This is one of the things I work on.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the sort of philosophy or underlying thing is that given some number of pairs of random variables, can you determine if the distributions that generated this random variables are independent or dependent?",
                    "label": 0
                },
                {
                    "sent": "So one obvious application of this is ICA, but general independence test has much wider implications and just source separation, so I'll talk a bit about that.",
                    "label": 0
                },
                {
                    "sent": "The last thing I'll go over is this thing called the two sample problem, which is that you get 2 samples and you want to know if they are from the same distribution or not.",
                    "label": 1
                },
                {
                    "sent": "And yeah, this is a problem which is very interesting if you've got, for instance the neural spiking that I talked about.",
                    "label": 0
                },
                {
                    "sent": "And if you want to work out if it's the same neuron which is generating the spikes that you measured on different days, or if it's a different one.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Is that better and everyone read it OK good?",
                    "label": 0
                },
                {
                    "sent": "OK, so just some notational conventions.",
                    "label": 0
                },
                {
                    "sent": "I write random variables in a sensory front, and I will overload the notation for distributions and densities, so a distribution and the density will be written as a capital San serif P. And also I'm using covariance matrices, which I'm sure you know what they are.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tie.",
                    "label": 0
                },
                {
                    "sent": "Right then, so OK. First I'll talk about ICA in a sort of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No sense.",
                    "label": 0
                },
                {
                    "sent": "So this is just a page of notation, so you know what I say is you have independent sources.",
                    "label": 0
                },
                {
                    "sent": "You mix them linearly with the matrix, which I'm going to call A and then you make some observations.",
                    "label": 0
                },
                {
                    "sent": "And then what you're trying to do is to estimate the unmixing matrix.",
                    "label": 0
                },
                {
                    "sent": "Now the important sort of way to think of ICA is that you're really only caring about the matrix, and so you don't want to try and workout what your sources are.",
                    "label": 0
                },
                {
                    "sent": "All you want to say is that they are independent and you want to estimate the matrix.",
                    "label": 0
                },
                {
                    "sent": "You don't want to estimate anything else about your sources.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do first is to talk about a very simple case where you actually know what your sources should look like.",
                    "label": 0
                },
                {
                    "sent": "And then you want to just solve for the matrix, and I'm going to sort of use that as a springboard to talk about more.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "ICA methods?",
                    "label": 0
                },
                {
                    "sent": "So this is when you have a sample because of course you don't have the true densities in real life, so that's my notation for this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And examples classes talk to you about OK, so here is toy example of ICA just to sort of get everyone familiar with the diagrams that I will be using.",
                    "label": 0
                },
                {
                    "sent": "So the first source is uniform.",
                    "label": 1
                },
                {
                    "sent": "It has a distribution which I've drawn a sample from.",
                    "label": 1
                },
                {
                    "sent": "It's flat, the second one is bimodal it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As to peaks.",
                    "label": 0
                },
                {
                    "sent": "And when you multiply this by a matrix, this is the kind of transform that you're making, so you can have some share.",
                    "label": 0
                },
                {
                    "sent": "You can have some rotation you can sort of stretch it in the X&Y axis.",
                    "label": 0
                },
                {
                    "sent": "And you're trying to work out what the matrix is that made this transformation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given that you actually know what the source densities look like, there are actually some respects in which you can't recover the matrix that combine them.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to just talk about some of these examples now.",
                    "label": 0
                },
                {
                    "sent": "So in the first example, you know that both have a probability density functions are Gaussian.",
                    "label": 1
                },
                {
                    "sent": "And you're mixing them this time with the matrix that rotates them.",
                    "label": 0
                },
                {
                    "sent": "And even though you know what the sources look like, you cannot recover this rotation.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "So here's basically an illustration of that.",
                    "label": 0
                },
                {
                    "sent": "You have your input sources on the left, which I'm always going to draw in red, and then you're rotating them and you can see that you've just got a sphere and you're rotating this randomly, but there's no way, even though you know your sources, that you can recover the matrix that makes them.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's another example here.",
                    "label": 0
                },
                {
                    "sent": "You have two sources that are the same.",
                    "label": 0
                },
                {
                    "sent": "And you are again rotating them, so by 45 degrees you can recover the well.",
                    "label": 0
                },
                {
                    "sent": "You know that the sources are not independent and so you can make some effort to recover them by rotating them backwards.",
                    "label": 0
                },
                {
                    "sent": "But when these sources are rotating by 90 degrees, when you swap the source order, of course you can't know that you've done that because the sources are the same.",
                    "label": 0
                },
                {
                    "sent": "So again, even though you know your sources, you can't recover them.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the final example is if your sources have a symmetric distribution around the origin and then you just flip them so your mixing matrix is a flip.",
                    "label": 0
                },
                {
                    "sent": "In that case, again, you're not able to recover this matrix, so you know your sources, but this is an uncertainty in your recovering of them.",
                    "label": 0
                },
                {
                    "sent": "So that's just a summary of these things.",
                    "label": 0
                },
                {
                    "sent": "The other thing I mean you can't separate signals to the everywhere constant, because this is a meaningless.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing to do?",
                    "label": 0
                },
                {
                    "sent": "The Gaussian case I talked about.",
                    "label": 0
                },
                {
                    "sent": "Another case is that if this is sort of a point that gets made later, if you only care if your signals are independent, you can't recover their amplitude because you don't know in a sense how big the signals were to start with, as compared with how big they are made by the matrix, and so if you're saying only that you want independent sources, you can't recover how much the sources made each.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the mixing matrix made individual sources bigger or smaller.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first step of ICA is decorrelation, so I know clouds.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Commit about this.",
                    "label": 0
                },
                {
                    "sent": "So I won't sort of talk too much, but yeah, the idea is that you multiply the sources by a matrix.",
                    "label": 0
                },
                {
                    "sent": "Which is going to make them.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have zero correlation.",
                    "label": 0
                },
                {
                    "sent": "So to show what this means, here's an example.",
                    "label": 0
                },
                {
                    "sent": "You've mixed the sources with a completely random matrix.",
                    "label": 0
                },
                {
                    "sent": "That's the central picture.",
                    "label": 0
                },
                {
                    "sent": "And then on the right hand side you can see the source is decorrelated, although it's a bit dim.",
                    "label": 0
                },
                {
                    "sent": "So basically what this means is that having the correlated the source is all you need to do is to rotate them to recover them.",
                    "label": 0
                },
                {
                    "sent": "So the decorrelation is recovering all of the mixing matrix except for rotation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now just a quick drawback of doing the correlation and rotation separately, is that in fact you don't get as good an estimate of the unmixing matrix as you would get where you to solve for it, like in its entirity.",
                    "label": 1
                },
                {
                    "sent": "So solving for the whole matrix is much harder, but splitting it up in this way is a way of saying that you think that getting the decorrelation right is much more important than getting the remaining part of the matrix right, and so if you sort of go through and do the analysis, you can find that the precision with which you can recover the mixing matrix is less.",
                    "label": 1
                },
                {
                    "sent": "If you do it in two steps and if you do it in one.",
                    "label": 0
                },
                {
                    "sent": "Generally this doesn't matter, but it can matter.",
                    "label": 0
                },
                {
                    "sent": "For instance, for sources which have unbounded variants and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's left is to recover the rotation.",
                    "label": 0
                },
                {
                    "sent": "And so here I've sort of enlarged the pictures that you saw earlier.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So rotation matrices look like this.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions, the rotation matrix is parameterized by only one parameter, which is this angle of rotation.",
                    "label": 0
                },
                {
                    "sent": "For higher dimensions, you can generalize this so you can have rotations about the XY and Z axis, which I've written there, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's again look at the case where we know what our source density looks like.",
                    "label": 0
                },
                {
                    "sent": "So what I say.",
                    "label": 0
                },
                {
                    "sent": "What I've drawn up in the top RAM, you have an estimate B of your unmixing matrix.",
                    "label": 1
                },
                {
                    "sent": "And you have P hat of S, which is your source density.",
                    "label": 0
                },
                {
                    "sent": "So that's what you believe your source density to be.",
                    "label": 0
                },
                {
                    "sent": "And The thing is that here you your source density model is correct, but your unmixing matrix you don't know you're trying to estimate it.",
                    "label": 0
                },
                {
                    "sent": "Now the estimated density of your observation matrix given your source model and given your estimate B is given by this central expression, there P hat of X.",
                    "label": 0
                },
                {
                    "sent": "Which is basically yeah you can.",
                    "label": 0
                },
                {
                    "sent": "You can find the expression for that as a function of B and it's a function of the source densities.",
                    "label": 0
                },
                {
                    "sent": "Now to recover B you can maximize the expected log likelihood, which is the expectation of the log of your estimated density function.",
                    "label": 1
                },
                {
                    "sent": "And when, yeah, when you sort of have to estimate this with a sample, you get the bottom expression there where you can see the expectation of the log is now computed empirically on the basis of the sample that you observed.",
                    "label": 0
                },
                {
                    "sent": "But bear in mind that the source Den.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "City is correct here.",
                    "label": 0
                },
                {
                    "sent": "So here the sources are both mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "What you do in this case is that you rotate them by 45 degrees and you get the central picture there.",
                    "label": 0
                },
                {
                    "sent": "And on the right hand side, I've written the log.",
                    "label": 0
                },
                {
                    "sent": "Likelihood is a function of the angle of rotation, so the red bar in the center is the correct answer, and then the log likelihood is is the blue curve and you can see that the log likelihood is maximized when your unmixing matrix has the correct angle.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "Now where this is wrong and where this also creates problems in real ICA is if your model is incorrect.",
                    "label": 0
                },
                {
                    "sent": "So imagine your input sources to Laplace densities, so that's sort of very heavy tailed densities, which I've drawn on the left hand side and you rotate them by 45 degrees, but you use the same model that I used before, so these mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Now you can see that the best rotation that aligns these samples with the model is in fact one that doesn't change it at all from 45 degrees, because that's the point at which the density of the mixtures best overlaps the model.",
                    "label": 0
                },
                {
                    "sent": "So using this incorrect model for this source combination is going to give you the worst possible answer, which is that it's going to give you the most independent sources rather than the least.",
                    "label": 0
                },
                {
                    "sent": "So this is a risk you do when you're using a model which isn't correct.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to talk about ICA as it's done in practice and this is using contrast function.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just to recap on something I talked about earlier, you remember that you can't recover the sources for certain operations because this is sort of inherent ambiguity.",
                    "label": 0
                },
                {
                    "sent": "So if you swap the source ordering, for instance, you don't necessarily recover them.",
                    "label": 0
                },
                {
                    "sent": "If you swap the sign, you don't necessarily recover, and if you re scale likewise.",
                    "label": 0
                },
                {
                    "sent": "So basically what ICA should do is not to try to recover this source matrix exactly, but only up to these ambiguities.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what I what people define then, is something called a contrast function, which is 0 if and only if the recovered sources are mutually independent.",
                    "label": 1
                },
                {
                    "sent": "So this means that it doesn't care about these ambiguities in the mixing matrix, it only cares about getting independent sources, and the sources recovered will still be independent despite these ambiguities.",
                    "label": 1
                },
                {
                    "sent": "So this is sort of ideal of a contrast function, but in practice what people really do is that they have something which they want to be the smallest when the random variables as independent as possible, so it's not necessarily going to be 0 and the random variables are not necessarily going to be independent, but people would like them to be, so it's a sort of in practice.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not this nice idea of being 0 independence.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that maximum likelihood with the correct density model we're going to see is going to give you the best possible contrast that you can get.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk now about how maximum likelihood with the correct densities gives you a contrast function.",
                    "label": 1
                },
                {
                    "sent": "So you can decompose the maximum likelihood into a sum of two terms, a constant term and a KL divergent between the density of the unmixed sources.",
                    "label": 0
                },
                {
                    "sent": "So this is this P of BX up there, and the model density of your sources.",
                    "label": 0
                },
                {
                    "sent": "Since this is the P hat of S. So the KL divergent is something which is 0 if and only if the two densities are the same.",
                    "label": 1
                },
                {
                    "sent": "So this is something which is going to be 0 when you're unmixing by multiplying the observations by your estimated matrix.",
                    "label": 0
                },
                {
                    "sent": "B is equal to the original sources.",
                    "label": 0
                },
                {
                    "sent": "So obviously when your density model is correct, that's exactly the definition of a contrast function, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now another one which is very nice thing in theory is the mutual information.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be equal to 0 if and only if the joint density has the same as the product of the marginal densities.",
                    "label": 1
                },
                {
                    "sent": "So this is the same as saying it's zero if and only if the random variables are independent.",
                    "label": 0
                },
                {
                    "sent": "Now people don't use this in ICA because it's basically hard to estimate empirically.",
                    "label": 0
                },
                {
                    "sent": "So in 2004 someone actually did a paper on this where they used a very clever method to estimate mutual information.",
                    "label": 0
                },
                {
                    "sent": "But before that it had kind of been.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ignored.",
                    "label": 0
                },
                {
                    "sent": "So you can actually simplify this expression in the following way.",
                    "label": 0
                },
                {
                    "sent": "You have a sum of three terms, the sum of the entropies of the unmixed source is the entropy of the observations, which is constant with respect to the unmixing matrix, so you can ignore it analog determinant term, which you can ignore as well because it's zero when you're when you're having a rotation matrix.",
                    "label": 0
                },
                {
                    "sent": "So it's equal to 1 rather.",
                    "label": 0
                },
                {
                    "sent": "So this means that as far as optimizing over the unmixing matrix is concerned, you only need to care about the sum of the entropies.",
                    "label": 0
                },
                {
                    "sent": "And entropies are easier to estimate the mutual information, but actually I mean not.",
                    "label": 1
                },
                {
                    "sent": "It's still quite tricky.",
                    "label": 0
                },
                {
                    "sent": "So rather than using entropies, what people do instead is to use the expectation of some nonlinear function of the observations so this nonlinear function in an ideal world would be the log of the density, but since density estimation is a tough problem, you use some other nonlinear function.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an example of some nonlinear functions that people use, so there's infomax, which is sort of heuristic.",
                    "label": 0
                },
                {
                    "sent": "And fast ICA as well.",
                    "label": 0
                },
                {
                    "sent": "I mean generally what these functions try to do is to sort of describe in some cartoon sort of term the properties that you care about.",
                    "label": 0
                },
                {
                    "sent": "So if for instance you cared about non gaussianity, you could expand the densities about the Gaussian using the gram shalya expansion and then you could use a low order approximation to this and then you could say that this captures the properties that you care about and that all of these other sort of properties are less.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "So we're going to see shortly how this works in practice.",
                    "label": 0
                },
                {
                    "sent": "So first of all I need to define something called kurtosis, which is a statistic which describes certain properties of random variables.",
                    "label": 0
                },
                {
                    "sent": "So basically it's sort of describing in a way how spread out the random variables are.",
                    "label": 0
                },
                {
                    "sent": "So Super Gaussian source has very heavy tails and so it's sort of very spread out and a sub Gaussian source is something that's more compact and more concentrated about the origin.",
                    "label": 0
                },
                {
                    "sent": "And some contrast functions are described explicitly in terms of the kurtosis.",
                    "label": 0
                },
                {
                    "sent": "So this bottom one there the Jade type contrast is defined in this respect.",
                    "label": 0
                },
                {
                    "sent": "And others implicitly need the kurtosis to be big or to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A small so I've just given an example now of two sources and then mixed versions for Super Gaussian for sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So the uniform source density is a sub Gaussian example and the Super Gaussian one is much more heavy tailed, so you can see it's got a lot of sort of outlier.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now here is just applying these contrast functions that I defined to sources which are both Super Gaussian and sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And you can remember that these contrast function should be smallest when you get the correct answer.",
                    "label": 0
                },
                {
                    "sent": "So here you can see in fact that for Jade, when the sources are Super Gaussian, the contrast is biggest.",
                    "label": 0
                },
                {
                    "sent": "When you get the correct answer.",
                    "label": 0
                },
                {
                    "sent": "And Conversely, the infomax an fast ICA contrast the biggest at sub Gaussian sources.",
                    "label": 0
                },
                {
                    "sent": "So this example is quite simple.",
                    "label": 0
                },
                {
                    "sent": "You could say, well, you can just make a test if it's super Gaussian or sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If it's yeah, if it's super Gaussian, you just flip Jade around.",
                    "label": 0
                },
                {
                    "sent": "If it's sub Gaussian, you just flip IMAX and fast I see around.",
                    "label": 0
                },
                {
                    "sent": "But I mean this is this is sort of a quite simple example and in practice you can get a much more subtle difference between the true characteristics of the sources.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you're looking for?",
                    "label": 0
                },
                {
                    "sent": "So this means that if you're using a fixed nonlinear function and you're trying to take the expectation of this function, and say that this is a bit like the entropy, then you will be able to find sources that trick that.",
                    "label": 0
                },
                {
                    "sent": "And this kurtosis example is 1 example.",
                    "label": 0
                },
                {
                    "sent": "But there are others as well.",
                    "label": 0
                },
                {
                    "sent": "So if you're using an algorithm on the Internet, you have to be careful with it.",
                    "label": 1
                },
                {
                    "sent": "And yeah, Klaus also talked about some tests of how good a particular nonlinear function is in estimating.",
                    "label": 0
                },
                {
                    "sent": "Like yeah, the independent sources.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if your sources are not IID, then there's another trick you can do which is on this slide.",
                    "label": 0
                },
                {
                    "sent": "So basically, if your sources are like music sources for instance, then they depend on their values in the past, so sound wave or something is not going to be independent at each time instant, but the value of the signal is going to depend very heavily on the values it had at previous times.",
                    "label": 0
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "When you're separating sources that have this property, you can do much better than to just make them independent at any given time instance, which is that you can actually say that the source one source at any particular time should be independent of all the other sources at every possible time in the past.",
                    "label": 0
                },
                {
                    "sent": "So if you have a criterion which measures independence, you can measure it not like between the sources, not only at the present, but also between one source and it's past values.",
                    "label": 0
                },
                {
                    "sent": "You basically have, like many, many more constraints than you would have and you should be able to get a better solution like that.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give an example here where we just use correlation instead of independent.",
                    "label": 0
                },
                {
                    "sent": "So it's just a sort of proxy measure for independence.",
                    "label": 0
                },
                {
                    "sent": "And you can also define the correlation here.",
                    "label": 0
                },
                {
                    "sent": "Between a signal and it's past values.",
                    "label": 0
                },
                {
                    "sent": "So this is the correlation of like a vector of sources at time T and the vector of these sources.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "T rump.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry, that's a chief.",
                    "label": 0
                },
                {
                    "sent": "Another tell.",
                    "label": 0
                },
                {
                    "sent": "By the way, yeah, so that means that if you have station resources, it's not going to change this covariance matrix when you measure it at different times, but only the delay.",
                    "label": 0
                },
                {
                    "sent": "With respect to which you measure, it is important.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this now can give you a nice optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So this top line here says that you should have no correlation with the signal and itself at any at any given time, and so basically you're multiplying.",
                    "label": 0
                },
                {
                    "sent": "Your observations by the truth and mixing matrix and then you're getting something that should look like a diagonal, so no signal should be correlated with any other.",
                    "label": 0
                },
                {
                    "sent": "But you can also do this with this time delayed correlation, so you multiply it with the same true and mixing matrix, and you should also get something which is diagonal.",
                    "label": 1
                },
                {
                    "sent": "So a signal should only be correlated with itself at past times and not with the other signals.",
                    "label": 0
                },
                {
                    "sent": "So this then gives you an optimization problem with respect to one matrix, which you can just solve.",
                    "label": 0
                },
                {
                    "sent": "So you do a bit of algebra and you get this expression, which is a closed form solution for the unmixing matrix.",
                    "label": 1
                },
                {
                    "sent": "Now, if you have multiple time delays, then there's a procedure called joint diagonalization and that allows you again to recover your sources in this case, and so you can sort of jointly solve for all of the time delayed.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The quick introduction to ICA and now I'm going to talk about kernel independence measures.",
                    "label": 0
                },
                {
                    "sent": "So these are basically kernel functions of your variables, which is zero only at independent.",
                    "label": 1
                },
                {
                    "sent": "So in this sense there are contrast function as is used in ICA, so they're more than decorrelation.",
                    "label": 0
                },
                {
                    "sent": "They take into account high order dependence as ICA, as mutual information does.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Now, a great advantage of these kernel independence measures, which we're going to see is that they make sensible assumptions about smoothness.",
                    "label": 0
                },
                {
                    "sent": "So there's an underlying density function which is smooth and the kernel functions will make certain assumptions about how smooth these densities are.",
                    "label": 1
                },
                {
                    "sent": "Tell you what this means in practice, but it basically means that they're very much more robust than than some other methods.",
                    "label": 1
                },
                {
                    "sent": "Um, so applications are independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "Someone has used it for feature selection, so finding innocence features that are important for making predictions.",
                    "label": 0
                },
                {
                    "sent": "And I've used it also for finding dependence detection in the visual cortex.",
                    "label": 0
                },
                {
                    "sent": "So when you take an MRI scan of the visual cortex, you have a certain amount of activity going on and you want to know if activity in different parts is independent or dependent.",
                    "label": 0
                },
                {
                    "sent": "So if there's some sort of Co processing going on.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the particular kernel independence test I'll talk about is called the constraint covariance or cocoa.",
                    "label": 0
                },
                {
                    "sent": "And it has the three properties which I've described here.",
                    "label": 0
                },
                {
                    "sent": "It's an independence measure for universal kernels.",
                    "label": 1
                },
                {
                    "sent": "Now I'll say what it means to say kernel is universal shortly.",
                    "label": 0
                },
                {
                    "sent": "And I will also talk about how to make a statistical test of independence using the constraint covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is a different application to ICA.",
                    "label": 0
                },
                {
                    "sent": "You just want to test if variables are independent or dependent.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about where dependence is hard to detect, and this will also be where the smoothness assumption comes in, and I'll also talk about how to choose the kernel because you have, like if you're using a Gaussian kernel, you have a kernel size and you have to choose that.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I mean the great advantage of this test and this is like oh, good independent tests have.",
                    "label": 1
                },
                {
                    "sent": "This property is set as your sample size increases, the chance that your test is wrong is going to drop very fast.",
                    "label": 0
                },
                {
                    "sent": "And I'll also talk about I've used this method in ICA and I'll say where it's good.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I see as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just look at what it means to detect dependence.",
                    "label": 0
                },
                {
                    "sent": "So first I'll talk only about second order dependence.",
                    "label": 0
                },
                {
                    "sent": "So you get these pairs of points which have drawn here, so you get X1 and Y1 together.",
                    "label": 1
                },
                {
                    "sent": "They're in different spaces.",
                    "label": 0
                },
                {
                    "sent": "These spaces can be high dimensional and you get X2 and Y2 and so on.",
                    "label": 1
                },
                {
                    "sent": "And you want to know is there any dependence between the distribution of X and the distribution of why?",
                    "label": 0
                },
                {
                    "sent": "So are X&Y related in?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "So one way to do this, which is a second order method, is that you basically take the interview, choose two directions, One Direction in the X space and one in the white space.",
                    "label": 1
                },
                {
                    "sent": "And you take the inner product of every point in the X space with its direction and every point every point in the white space with its direction and so then you get 2 vectors which are as long as your sample size.",
                    "label": 0
                },
                {
                    "sent": "And then you basically take the covariance between these vectors.",
                    "label": 0
                },
                {
                    "sent": "So you take the inner products.",
                    "label": 0
                },
                {
                    "sent": "Now if this is 0 for every pair of directions that you can choose, then there's no second order dependence between the X and the way, so there's no correlation between the X and the way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "So how do you generalize this to general measures of dependence as opposed to just second order?",
                    "label": 0
                },
                {
                    "sent": "So what you can do then is to map your points into some infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is the arcade chess.",
                    "label": 0
                },
                {
                    "sent": "So this mapping I just remind you it has like it's infinitely long and it's got basically nonlinear functions of your input.",
                    "label": 0
                },
                {
                    "sent": "So the higher the index, the more rough like the less smooth these nonlinear functions are, and it's also got these scaling factors in front of these nonlinear functions, and so as the index of the scaling function gets higher, these also becomes smaller.",
                    "label": 0
                },
                {
                    "sent": "So you basically emphasize less and less the rougher and rougher functions.",
                    "label": 0
                },
                {
                    "sent": "And so now you can do exactly as I described in the previous slide, so you can choose two directions in these infinite dimension feature spaces.",
                    "label": 0
                },
                {
                    "sent": "You can take the inner product of each of the points with these infinite dimensional features, and you can obtain the covariance between these inner products and this is going to be 0 only at independence as modular.",
                    "label": 0
                },
                {
                    "sent": "Some property of these.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which is the universality.",
                    "label": 0
                },
                {
                    "sent": "So just a thing to pay attention to.",
                    "label": 0
                },
                {
                    "sent": "First, I've written here the same nonlinear function for the X and the Y, but they don't have to be the same.",
                    "label": 0
                },
                {
                    "sent": "There can be different.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that I've divided here by the arcade just norms of these projection vectors.",
                    "label": 0
                },
                {
                    "sent": "So because we're in an arc AHS season, our functions and these are KHS norms are roughness penalties, So what you're saying here is that you want to find basically functions of your random variables which are not too rough, which have high covariance, and This is why you need a roughness penalty.",
                    "label": 0
                },
                {
                    "sent": "If you allowed your functions to be arbitrarily rough, then you could make the covariance between them.",
                    "label": 0
                },
                {
                    "sent": "Arbitrarily big, so you have to have some sort of constraint on these functions, and so that's why you have this.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now if you want to compute this in practice, you need to use the kernel trick.",
                    "label": 1
                },
                {
                    "sent": "So this basically takes this infinite dimensional problem and makes it reasonable intractable.",
                    "label": 0
                },
                {
                    "sent": "So this is a lot like what you see in kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "Here, I've abused notation a bit.",
                    "label": 0
                },
                {
                    "sent": "The problem on the previous slide is equivalent to this problem, where these are the infinite dimensional projections and this is an infinite dimensional covariance matrix between these enormous vectors.",
                    "label": 0
                },
                {
                    "sent": "Now when you have a finite sample, you have this empirical estimate of this infinite dimensional covariance.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You also can write your directions that you project on as linear combinations of the map, sample points, and the reason for this is that any component orthogonal to them would just vanish when you take the inner products on this slide.",
                    "label": 0
                },
                {
                    "sent": "In that top equation.",
                    "label": 0
                },
                {
                    "sent": "Now that you have.",
                    "label": 0
                },
                {
                    "sent": "OK, so you then OK.",
                    "label": 0
                },
                {
                    "sent": "I'm defining my kernel function now as being the inner product between the sample points.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then doing this allows you to write the equation from a couple of slides back entirely in terms of these matrices of inner products.",
                    "label": 0
                },
                {
                    "sent": "So this means that now you are able to solve it.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is this standard approach.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the constrained covariance is 0 if and only if the random variables are independent.",
                    "label": 0
                },
                {
                    "sent": "There are actually other methods that have this property as well which are quite similar to this, so I should just mention them.",
                    "label": 0
                },
                {
                    "sent": "So here I've taken a covariance.",
                    "label": 0
                },
                {
                    "sent": "You could also take a correlation.",
                    "label": 0
                },
                {
                    "sent": "There are also some other generalizations which are bounds on the mutual information, which I won't go into here, but they are also related to either the covariance or the correlation between these infinite dimensional vectors.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I mentioned universality several times.",
                    "label": 0
                },
                {
                    "sent": "So why is it that the kernel needs to be universal?",
                    "label": 0
                },
                {
                    "sent": "So the best way to say this is to describe what happens when it isn't.",
                    "label": 1
                },
                {
                    "sent": "So here is a kernel that isn't universal.",
                    "label": 1
                },
                {
                    "sent": "It's explained kernel, and here I've taken the spectrum of this kernel and you can see this spectrum has some points at which it's 0.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that if you define.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so you remember a few slides back.",
                    "label": 0
                },
                {
                    "sent": "We had these infinitely infinite dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "In the feature space.",
                    "label": 0
                },
                {
                    "sent": "So basically, if your kernel has some zeros, it means that it's effectively crossing out certain points in these infinite dimensional vectors, so it's the same as saying that it's going to penalize frequencies at this over functions at these frequently frequencies infinitely much.",
                    "label": 0
                },
                {
                    "sent": "So if your density.",
                    "label": 0
                },
                {
                    "sent": "Has components at these frequencies.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to have a lot of trouble.",
                    "label": 0
                },
                {
                    "sent": "So if you're penalizing these frequencies infinitely much, then these functions here are trying in some sense to adapt to a density which has very strong components at a point where there infinitely penalized.",
                    "label": 0
                },
                {
                    "sent": "So they're basically not going to be able to detect the the dependence encoded at these frequencies.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I'll talk about him doing statistical tests of independence.",
                    "label": 1
                },
                {
                    "sent": "Using this constraint covariance.",
                    "label": 0
                },
                {
                    "sent": "So for this I'll just give a quick overview of statistical testing terminology.",
                    "label": 0
                },
                {
                    "sent": "So you have a space of probability distributions which you partition into two subsets, which is this calligraphic P nought and its complement?",
                    "label": 1
                },
                {
                    "sent": "And you have some probability measure which is taken from somewhere in this space.",
                    "label": 0
                },
                {
                    "sent": "Now what you want to test is the hypothesis of whether your PZ is in calligraphic peanut or it isn't.",
                    "label": 0
                },
                {
                    "sent": "So the null hypothesis is we define it as being that PZ is in peanut and the alternative hypothesis is that it isn't.",
                    "label": 1
                },
                {
                    "sent": "Now you don't have access to PZ.",
                    "label": 0
                },
                {
                    "sent": "In practice, what you have is a sample from it.",
                    "label": 0
                },
                {
                    "sent": "And what you want to do then is to say that if the sample is in some particular region, you will reject H. Note, you will say that PZ is not in P0.",
                    "label": 0
                },
                {
                    "sent": "And if it's outside this rejection region in some acceptance region, then you accept that the null hypothesis is true.",
                    "label": 1
                },
                {
                    "sent": "So I mean, this is quite abstract, so in practice, what you need to do is you have to compute some function of the sample, which is called a test statistic.",
                    "label": 0
                },
                {
                    "sent": "So usually this Maps to the real line, but it doesn't have to.",
                    "label": 0
                },
                {
                    "sent": "And then you have basically intervals on the real line which this test statistic falls into, and some of these intervals of the rejection region and the remainder of the acceptance region.",
                    "label": 1
                },
                {
                    "sent": "So just to give a concrete example, if you compute a sample mean.",
                    "label": 0
                },
                {
                    "sent": "Then you might want to say is this sample mean positive or not?",
                    "label": 0
                },
                {
                    "sent": "So it's sorry is the underlying mean that generated this sample mean positive or not?",
                    "label": 0
                },
                {
                    "sent": "So to test this, what you would do is you would say, well, is the sample mean not too far from zero in the positive direction, and if it's if it's sort of like, you know if it's close to 0 or negative then you would say Yep, I believe that the true mean is not bigger than 0.",
                    "label": 0
                },
                {
                    "sent": "And if it's very, very positive you say, well, then it's very unlikely that the true mean is less than.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we want to say.",
                    "label": 0
                },
                {
                    "sent": "I mean you can make any number of tests and you want to say how good a particular test is.",
                    "label": 1
                },
                {
                    "sent": "So there are two kinds of errors that a test can make, so one is a Type 1 error, which is you reject the null hypothesis even though it's true.",
                    "label": 1
                },
                {
                    "sent": "And then there's the Type 2 error which you accept the null hypothesis, even though it's false.",
                    "label": 0
                },
                {
                    "sent": "So it's very.",
                    "label": 0
                },
                {
                    "sent": "Messy to remember which is which, but nonetheless.",
                    "label": 1
                },
                {
                    "sent": "So from this you can describe the power of the test.",
                    "label": 1
                },
                {
                    "sent": "Which is the probability under the true distribution that the sample is in the rejection region.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that you want the power to be close to zero when the null hypothesis is true, because that means that you want it to be very unlikely when the null hypothesis is true that you saw your sample goes in the rejection region and you reject the null hypothesis, and likewise it should be close to one when you're in the complement of peanuts.",
                    "label": 0
                },
                {
                    "sent": "So when you should reject the null hypothesis, the probability of rejecting it for a particular sample should be high.",
                    "label": 0
                },
                {
                    "sent": "Now the level of the test is something that here, as a user have to specify.",
                    "label": 1
                },
                {
                    "sent": "So what this is is an upper bound on the probability of making a Type 1 error.",
                    "label": 0
                },
                {
                    "sent": "So what you're going to do is, you're going to say, well, my test should have this upper bound, and then the quality of the test is then innocence determined by how bad your Type 2 error can be.",
                    "label": 0
                },
                {
                    "sent": "Dependent on this upper bound.",
                    "label": 0
                },
                {
                    "sent": "So I mean, this is, I mean the upper bound is something you specify, but it doesn't let you cheat.",
                    "label": 0
                },
                {
                    "sent": "So if you're saying if the mean if you're testing that the mean is less than or equal to 0, you could say, Oh well, I'll say it's always true, and then my Alpha is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "But of course, then your Type 2 error is going to be dreadful.",
                    "label": 0
                },
                {
                    "sent": "It's going to be 100, so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to talk about specifically testing independence.",
                    "label": 0
                },
                {
                    "sent": "And one difficulty in testing independence is in fact that no test, no matter how good, is going to do it perfectly.",
                    "label": 0
                },
                {
                    "sent": "For a finite sample size.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to give a quick example of this.",
                    "label": 0
                },
                {
                    "sent": "So here you've got your total set of probability distributions over a particular vector, and you partition this into two sets.",
                    "label": 0
                },
                {
                    "sent": "So one set is the set of all possible independent random variables and the other set is all possible dependent ones.",
                    "label": 1
                },
                {
                    "sent": "Now what you can say now is my test statistic is a very simple one.",
                    "label": 0
                },
                {
                    "sent": "It's just some test.",
                    "label": 0
                },
                {
                    "sent": "We don't care what it looks like, but it's going to return one when you're dependent and zero when you're independent.",
                    "label": 0
                },
                {
                    "sent": "So very, very simple to make a test with this statistic.",
                    "label": 0
                },
                {
                    "sent": "It's just is it one or is it 0?",
                    "label": 0
                },
                {
                    "sent": "And we don't care about how we got this answer.",
                    "label": 0
                },
                {
                    "sent": "We just say, like magically, we got the best possible tests that can give us this answer.",
                    "label": 0
                },
                {
                    "sent": "And what's more, you say that you have this upper bound?",
                    "label": 0
                },
                {
                    "sent": "This is your design which is Alpha on the Type 1 error.",
                    "label": 0
                },
                {
                    "sent": "Now you can prove that for a finite sample size there is.",
                    "label": 1
                },
                {
                    "sent": "There exists some very very tricky distribution.",
                    "label": 0
                },
                {
                    "sent": "Which gives you a Type 2 error greater than one minus Alpha.",
                    "label": 0
                },
                {
                    "sent": "So if you say I want my test to have a probability of type 1 error of 5%, then for any finite sample there's going to be a really tricky distribution that will give you a 95% type 2 error, no matter how good your test is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the way I can tell you the way you can construct this, this very bad example is very simple, so you have your independent distribution.",
                    "label": 0
                },
                {
                    "sent": "And then you have independent one, which is a mixture of this independent guy and then some dependant one with very very small probability.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that if you take some number of samples from the dependent one, it's very unlikely that you'll ever see the independent distribution.",
                    "label": 0
                },
                {
                    "sent": "You need a huge number of samples before you see it, and so that means that for, like for any sort of large sample size, you can make the mixture weight of the dependent component so small that you're just basically never going to see it for the number of samples that you drew, and so your Type 2 error is going to be as large as you want it to.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a very like that.",
                    "label": 0
                },
                {
                    "sent": "Example was a very simple example of a hard to detect dependence.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to talk about a bit of a tougher example, which is where like it's going to describe why you need to make good smoothness assumptions.",
                    "label": 0
                },
                {
                    "sent": "If you're going to do a test.",
                    "label": 0
                },
                {
                    "sent": "So the joint density function I'm going to use for this example is a kind of weird one.",
                    "label": 0
                },
                {
                    "sent": "So it's a constant term plus a scaled product of these nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "So these are the same nonlinear functions that we saw earlier for the kernels.",
                    "label": 0
                },
                {
                    "sent": "So and then for the infinite dimensional OK, just so it has the same property as this coefficient gets bigger, these functions get rougher and rougher, so you can imagine a density that's getting rougher and rougher as your coefficients as the L term gets larger.",
                    "label": 0
                },
                {
                    "sent": "Now The thing is that as you increase the L value, what this is going to do is to make the constraint covariance arbitrarily small.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason for this is these norms in the denominator.",
                    "label": 1
                },
                {
                    "sent": "So as this gets rougher these functions are going to try and match the density which is generating the random variables.",
                    "label": 0
                },
                {
                    "sent": "But if the density is very rough then these functions will be also very nonsmooth.",
                    "label": 0
                },
                {
                    "sent": "This denominator is going to become very large, and this quantity is going to become very small.",
                    "label": 0
                },
                {
                    "sent": "And the bigger the L said, the rougher the density.",
                    "label": 0
                },
                {
                    "sent": "The smaller the constrained covariance will be.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now just to give you an example, I mean here I've used is my nonlinear function.",
                    "label": 0
                },
                {
                    "sent": "This product of sinusoids.",
                    "label": 0
                },
                {
                    "sent": "And up here you have a case where this L is very small.",
                    "label": 0
                },
                {
                    "sent": "You have a smooth density and you can see here just from this 500 sample that the X&Y are dependent.",
                    "label": 1
                },
                {
                    "sent": "Because yeah, if X has values around here, then yeah.",
                    "label": 1
                },
                {
                    "sent": "I mean you can sort of see that the density is changing, whereas in this case when you have a very rough density from 500 samples, you can't even see that it's not just a uniform density and a uniform density over X&Y is an independent density.",
                    "label": 1
                },
                {
                    "sent": "So in fact, what you want is that for your test statistic not to tell you when it only sees 500 samples.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm really sure that it was this density, so your test statistic has to be smaller when the density is rougher.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's going to mislead you.",
                    "label": 0
                },
                {
                    "sent": "It's going to tell you that it found some extremely difficult dependence, which you would need millions of samples to detect reliably, and so This is why having this function norm penalization gives you a test which is robust against making this.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mistake.",
                    "label": 0
                },
                {
                    "sent": "And so this is how it works in practice.",
                    "label": 0
                },
                {
                    "sent": "I've got rougher and rougher densities.",
                    "label": 0
                },
                {
                    "sent": "This blue line here is the constraint covariance, and this red line is the constraint covariance for this sample size when the density is uniform and you can see that for these last two.",
                    "label": 0
                },
                {
                    "sent": "The constraint covariance is statistically indistinguishable from the value you would get with the uniform density.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that you don't have enough samples at that point to detect that the random variables are dependent.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I've described the constraint events.",
                    "label": 0
                },
                {
                    "sent": "I'll just talk about how you're going to get a test of dependence from this statistic so you know that it's zero at independence, and you know that it's going to be positive when it's not independent.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is to do a test that the constraint covariance is going to be 0 or not zero or positive basically.",
                    "label": 0
                },
                {
                    "sent": "So what you can prove, and I won't talk about how this is done, is that the empirical constraint covariance is going to be converging quite fast to the population one.",
                    "label": 1
                },
                {
                    "sent": "So with high probability as you get more samples, the empirical estimate of the constraint covariance is going to be close to the population one, and the rate at which it gets closer is this term.",
                    "label": 0
                },
                {
                    "sent": "Here, one on Route 10.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is to then say is the constraint covariance bigger?",
                    "label": 0
                },
                {
                    "sent": "Then this term.",
                    "label": 0
                },
                {
                    "sent": "This term has two components of interest, so one is the Alpha.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of the random variables being independent, but you believe that their dependent and so this you have to control.",
                    "label": 1
                },
                {
                    "sent": "You have to tell it that.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is 1 on Route N. So this is saying that as you get more samples this threshold is getting smaller and so your test is just going to say is the empirical constraint covariance bigger than this threshold?",
                    "label": 0
                },
                {
                    "sent": "If it is, then you say they're dependent.",
                    "label": 0
                },
                {
                    "sent": "If it isn't, you say they are independent and this will give you a test of size Alpha.",
                    "label": 1
                },
                {
                    "sent": "So that's yeah, just reminding you what Alpha means, yeah?",
                    "label": 0
                },
                {
                    "sent": "And this is this kind of has an interesting sort of philosophical implication, because what it's saying also is that as you get more samples for whatever else you choose, your Type 2 error is going to drop to zero at a fast rate at rate one on route end.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of weird because you can also show like independently that any regression method even like one that gets an arbitrarily good error for infinite data, is going to can learn this regression very, very slowly, so you can find you can invent a very tricky regression problem which is going to make even the best possible algorithm learn very, very slowly.",
                    "label": 0
                },
                {
                    "sent": "But what this result is saying is that it can detect that random variables are dependent very fast.",
                    "label": 0
                },
                {
                    "sent": "So your your Type 2 error is dropping very fast.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that even though you can't necessarily do regression between the random variables, as you get more data, you can nonetheless detect quite quickly as you get more data that there is a dependence, but.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In them.",
                    "label": 0
                },
                {
                    "sent": "So that's quite weird.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "On a sort of more practical note, choosing the kernel size.",
                    "label": 0
                },
                {
                    "sent": "So I've defined this function, which is going to in fact change as the kernel size changes.",
                    "label": 0
                },
                {
                    "sent": "And you want to know effectively like what kernel size to choose.",
                    "label": 0
                },
                {
                    "sent": "So I'll just describe now how to do this so.",
                    "label": 0
                },
                {
                    "sent": "As a reminder, here's the arcade.",
                    "label": 0
                },
                {
                    "sent": "Just normal function, and so here you've got these terms here, which are the Fourier coefficients of the kernel.",
                    "label": 0
                },
                {
                    "sent": "And So what this means is that for rougher functions, so when these components at large I a big you're going to penalize them more because these inverses of the Fourier transform of the kernel are going to grow.",
                    "label": 0
                },
                {
                    "sent": "So what this means in a sort of concrete sense is given by this drawing here.",
                    "label": 0
                },
                {
                    "sent": "So I've drawn two kernels here.",
                    "label": 0
                },
                {
                    "sent": "One is a very wide kernel and one is a very narrow kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just changing the kernel size.",
                    "label": 0
                },
                {
                    "sent": "Now the Fourier transform of these have the exact opposite property.",
                    "label": 0
                },
                {
                    "sent": "The narrow kernel has a very broad Fourier transform because it's sharp, so it has high frequency components and the thick kernel has a very narrow Fourier transform because it only needs components at low frequencies.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that when you use an arrow kernel, you're going to penalize rough functions less because the inverse of this is going to grow more slowly than you will if you use a very broad Colonel.",
                    "label": 0
                },
                {
                    "sent": "So a very broad kernel gives you a very large penalty for, uh, functions.",
                    "label": 0
                },
                {
                    "sent": "So this might lead you to think, well, OK, if I can make.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If I can make rough functions not penalized at all.",
                    "label": 0
                },
                {
                    "sent": "Then I could make my constrained covariance arbitrarily big.",
                    "label": 0
                },
                {
                    "sent": "Because you could just make the kernel smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "You can make the function norms in the denominator less and less penalized, and yeah.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in fact this is not true.",
                    "label": 0
                },
                {
                    "sent": "And the reason is because the constraint covariance is computed on the basis of a sample.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that as you drop the kernel size, your constraint covariance is increasing, but only up to a point.",
                    "label": 1
                },
                {
                    "sent": "And at this point even the samples from a space like your examples, so your samples from any one variable and no longer even similar to themselves, and so then it's going to drop again.",
                    "label": 0
                },
                {
                    "sent": "So basically what this means is that your Alpha your upper bound on the Type 1 error is something that you set that to minimize the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "For this Alpha you should choose your kernel size that maximizes the constraint covariance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you maximize the constraint covariance, then basically that means that if for the if you optimize your kernel size to make cocoa as big as possible, and even then it doesn't cross the threshold for dependence, then you're absolutely sure that no dependence exists between them.",
                    "label": 0
                },
                {
                    "sent": "So it's in a sense somewhere sensitive tests that you can get.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that respect, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so the other application of this would be independent component analysis, so you've got a contrast function.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it makes sense to do ICA with it, so this one.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have the tricky properties that you get.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it doesn't have the tricky properties you have with the fixed contrast functions.",
                    "label": 0
                },
                {
                    "sent": "And the reason being that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you basically, these functions are adapting to the density and so you can't sort of create a density which is in a sense contradicting the nonlinearity that you've chosen to unmixed signals.",
                    "label": 0
                },
                {
                    "sent": "So in practice I'll just a couple of notes on performance, the kernel methods, and I think a lot of the modern methods, like the ones using like true estimates of mutual information.",
                    "label": 0
                },
                {
                    "sent": "The one is there's another one using true estimates of entropy, so these at the moment are still too slow for large scale problems, so if you've got more than around 16 sources then you're in trouble.",
                    "label": 1
                },
                {
                    "sent": "The kernel methods specifically have better resistance to outliers than any other method that I've tested, so this is an advantage.",
                    "label": 1
                },
                {
                    "sent": "And the other thing is that source code ptosis doesn't affect performance.",
                    "label": 0
                },
                {
                    "sent": "So you saw that a lot of the classical methods are.",
                    "label": 0
                },
                {
                    "sent": "Nonlinearity is a very sensitive to kurtosis and this isn't true of the kernel methods and a lot of modern methods as well.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just show you what I mean.",
                    "label": 0
                },
                {
                    "sent": "So here this is an experiment that I did.",
                    "label": 0
                },
                {
                    "sent": "Hey, this is a performance of performance measure of how accurately you mix C samples, so it said divergents measure between the true mixing matrix in your estimate of it.",
                    "label": 1
                },
                {
                    "sent": "And here I've adjusted the kurtosis of the sources.",
                    "label": 1
                },
                {
                    "sent": "So I've got here 2 classical methods when is fast ICA and when is Jade and you can see that both of them in the vicinity of zero kurtosis very badly.",
                    "label": 0
                },
                {
                    "sent": "So these fixed nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "When you're getting close to 0 kurtosis are going to give you a lot of trouble.",
                    "label": 0
                },
                {
                    "sent": "And here this bottom line here is all of these modern methods, so some of them are kernel methods, some of them are other methods which adapt to the sources and you can see that they basically don't care at all about the kurtosis.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the advantage of using.",
                    "label": 0
                },
                {
                    "sent": "More sophisticated method that depends on the source densities rather than just fixing your nonlinearity and hoping that the source densities conform.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To that.",
                    "label": 0
                },
                {
                    "sent": "OK, this experiment now is outlier resistance, so don't worry bout this graph.",
                    "label": 1
                },
                {
                    "sent": "This graph is basically the measure of divergents.",
                    "label": 0
                },
                {
                    "sent": "Again, this is the number of outliers.",
                    "label": 1
                },
                {
                    "sent": "So what you're doing is that you mix your sources and then you add some random outliers to this mixture.",
                    "label": 0
                },
                {
                    "sent": "And here is fast ICA which is terrible.",
                    "label": 0
                },
                {
                    "sent": "Here are some other modern methods that use estimates of the mutual information or use estimates of the entropy, and these are the kernel methods and the kernel methods.",
                    "label": 0
                },
                {
                    "sent": "You can see much less sensitive than other methods that we can compare with.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk now about the two sample problem.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a completely different angle to testing independence.",
                    "label": 0
                },
                {
                    "sent": "Although yeah, there are some similarities to some of the techniques we are going to use when we compared to kernel ICA.",
                    "label": 0
                },
                {
                    "sent": "So the two sample problem is basically you are given two samples and you want to test if they come from the same distribution or a different distribution.",
                    "label": 0
                },
                {
                    "sent": "So the example I gave earlier was the spike example you want to test if the spikes you measured on different days come from the same neuron or from a different one.",
                    "label": 1
                },
                {
                    "sent": "So you want to know if you can aggregate the data that you collected over these two days or not.",
                    "label": 1
                },
                {
                    "sent": "Another case is speaker identification.",
                    "label": 0
                },
                {
                    "sent": "So you have basically somebody who talks a bit.",
                    "label": 0
                },
                {
                    "sent": "You get samples from this speech and then you have some sort of database of where he's still previously and you want to know if it's the same person or not.",
                    "label": 0
                },
                {
                    "sent": "Another example is comparing paintings.",
                    "label": 0
                },
                {
                    "sent": "You take patches from images and then you sort of use that as a way of modeling particular painter style, and then you have another painting.",
                    "label": 0
                },
                {
                    "sent": "You take patches from that you want to know if it's the same paint or not.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the test statistic we're going to use for this problem is something called the maximum mean discrepancy.",
                    "label": 0
                },
                {
                    "sent": "Again, we need our arcade just to be universal, and that's for reasons kind of similar to why we needed it for independence testing.",
                    "label": 0
                },
                {
                    "sent": "And this is our definition.",
                    "label": 0
                },
                {
                    "sent": "So you basically you take the supremum of all functions with unit norm.",
                    "label": 0
                },
                {
                    "sent": "Of the difference of the expectations of these functions.",
                    "label": 0
                },
                {
                    "sent": "So what you're trying to do is, you're trying to find a function which in a sense encodes the difference between the density that generated the X and the density that generated the way.",
                    "label": 0
                },
                {
                    "sent": "And if whatever function you choose, you can't get this to be different then the density that generated these two is going to be the same.",
                    "label": 0
                },
                {
                    "sent": "So this is the sort of idea.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Behind it.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to talk about how to compute this with kernels, so there's a subtlety here, like when I talked about covariance.",
                    "label": 0
                },
                {
                    "sent": "In fact, you have to be a bit careful when you're defining these infinite dimensional covariance matrices, so there it's a bit messy, but here I'll just describe what's going on in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "So what you want to know is how do you define here?",
                    "label": 0
                },
                {
                    "sent": "You have a mapping of some function, some random variable to an arc ahs and you want to say what does it mean to define the mean of this infinite dimensional mapping?",
                    "label": 0
                },
                {
                    "sent": "Like what is the meaning of this mapping in the arc HS?",
                    "label": 0
                },
                {
                    "sent": "So what it is?",
                    "label": 0
                },
                {
                    "sent": "I mean, you need to know that because you want to say well, what do I do when I take an inner product of a function in the dark Ages with them in?",
                    "label": 0
                },
                {
                    "sent": "And what we can do is to define the mean basically as being the inner product, the expectation of the inner product of this mapping with.",
                    "label": 0
                },
                {
                    "sent": "Some function and so basically it's defined the mean element in the arc.",
                    "label": 0
                },
                {
                    "sent": "HS is defined such that when you take an inner product with another element in the RKHS, you get the expectation of the function of that element.",
                    "label": 0
                },
                {
                    "sent": "So, but nonetheless it's some infinite dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "It just happens to be the one that has this property.",
                    "label": 0
                },
                {
                    "sent": "So the other trick I'm going to use this one.",
                    "label": 0
                },
                {
                    "sent": "If you take the norm of a vector in the RKHS, that's the same as taking the biggest function in the unit ball and taking the inner product of that function with the vector.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we can define this maximum mean discrepancy in terms of kernels using the two things on that last slide.",
                    "label": 1
                },
                {
                    "sent": "So first of all you can define this difference in the expectations of X / Y as the inner product of an arc HS function with these mean elements that we've defined.",
                    "label": 0
                },
                {
                    "sent": "OK, that's by the definition of the mean elements.",
                    "label": 0
                },
                {
                    "sent": "OK, now this is equal to the norm.",
                    "label": 0
                },
                {
                    "sent": "By this trick we used to define the norm.",
                    "label": 0
                },
                {
                    "sent": "You have a square here because that's from there.",
                    "label": 0
                },
                {
                    "sent": "The norm is the inner product of something with itself when it's squared.",
                    "label": 0
                },
                {
                    "sent": "And now you've got inner products, so you can use the kernel trick, and that's what we've done down here.",
                    "label": 0
                },
                {
                    "sent": "Now the subtlety here is that you've got here at X&X Dash.",
                    "label": 0
                },
                {
                    "sent": "So what you have to do in fact is make an independent copy of the X so that you can do this, because otherwise you would get some coupling between the things if they were the same.",
                    "label": 0
                },
                {
                    "sent": "So this is just something you need to bear in mind.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And where it causes complications is when you're making an empirical estimate.",
                    "label": 0
                },
                {
                    "sent": "And So what happens if you get IID samples from PX&PY?",
                    "label": 0
                },
                {
                    "sent": "To sort of get an empirical version of this independent copy trick what you need to do is take the sum only over those random variables with different indices only over the sample points.",
                    "label": 0
                },
                {
                    "sent": "Sorry with different indices.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing is you've got your sample X which has endpoints and you take the sum only over the dissimilar ones, and then there's M * N -- 1 of those.",
                    "label": 0
                }
            ]
        }
    }
}