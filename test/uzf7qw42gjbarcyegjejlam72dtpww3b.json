{
    "id": "uzf7qw42gjbarcyegjejlam72dtpww3b",
    "title": "ORCHID - Reduction-Ratio-Optimal Computation of Geo-Spatial Distances for Link Discovery",
    "info": {
        "author": [
            "Axel-Cyrille Ngonga Ngomo, University of Leipzig"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Business->Management->Knowledge Management"
        ]
    },
    "url": "http://videolectures.net/iswc2013_ngonga_ngomo_orchid/",
    "segmentation": [
        [
            "My name is Axel.",
            "I'm from the University of Leipzig in Germany and I'm going to talk about orchid, which is an approach for the reduction ratio optimal computation of geospatial distances falling discovery.",
            "What I'm not going to do is I'm not going to cover any proofs and I'm basically going to try to stick to the core of their approach so that I can stick to the time given.",
            "So what is?"
        ],
        [
            "This whole thing about it's obviously about Link discovery and the question that we can ask ourselves is why do we need that?",
            "Actually it is very obviously the 4th link data principle.",
            "We need links between different knowledge Bases 2 for feel quite a few different tasks including for example cross ontology question answering.",
            "The idea being here that you have two knowledge bases and you basically want to be able to answer questions where you need parts of the data that included the 1st and in the second knowledge base or you need to be able to see which resources are related to each other.",
            "Storage spaces, that's one of the domains where you need links.",
            "You obviously also need links when you data integration of iterative queries under like and if we look at the current state of the cloud that is to cloud as described in the loudcloud state Web PY page, you will see that you have approximately 30 billion triples, but only 0.5 billion links.",
            "That is less than 2% on those leaks are mostly same last links, so there's obviously quite a need to generate links across knowledge bases on the link.",
            "Dataweb"
        ],
        [
            "Formerly what people?",
            "I mean the formal definition of link discovery policy is that we assume that we are given two sets of source resources, setty of target resources, on a certain relation R, that we want to compute, which is not necessarily the LCMS relation.",
            "It could also be something like near that is 2.",
            "Geospatial resources are nearby, each other close to each other according to a certain distance threshold, and what we're asked to find is to set M of pair St from.",
            "The product of source and target that are such that the relation R holds between S&T now in most cases this is quite difficult to compute and we usually approximate this set by using this set and prime.",
            "Primaries then defined as the parents St that are such that similarity of SES larger or equal to a certain threshold or the distance between S&T is less or equal to a certain threshold.",
            "No, if you assume this paradigm that quite a few problems that come about when trying to use it, the first problem being the time complexity.",
            "Obviously it's a quadratic problem, and if you assume that we want to link for example cities from DB pedia, which cities from Geo name and we need only one millisecond to compute the similarity between the two cities will need 69 days.",
            "And if you were to try to link the picture with the whole offering geodata it would take us decades on the assumption.",
            "Basically this quadratic problem.",
            "So that's the first problem that we need to solve.",
            "The link discovery."
        ],
        [
            "The second problem that we need to solve is obviously complexity or specifications themselves.",
            "So in most cases when the problems are not trivial, we need quite large function similarity functions.",
            "That is that we also calling specifications to be able to find correct links, that is, to be able to achieve a high F measure, and that is quite tricky to promising addressed by using machine learning, unsupervised machine learning and the like, But that's not what we're going to talk about today today."
        ],
        [
            "We're going to focus on linking geospatial entities, and with some of the problems that are related to that, obviously.",
            "The description of geospatial entities differs very much from the other entities, because here we use polylines polygons under like that is complex structures and shapes.",
            "This gives you.",
            "You can't see my pointer.",
            "Basically picture the picture there gives you an example of such a Polygon that is 1981 points that are used to describe Norway, and if you wanted to know the geospatial resources that are nearby Norway, you would actually have to compare these Polygon with other polygons.",
            "The problem we have is that we are dealing with this with this space where the distance is the author drumming distance.",
            "That is basically the distance across the X and the Y axis actually depend on each other.",
            "So we kind of distance function.",
            "Should we actually use to compare these programs?",
            "And how do we deal with the runtime complexity problem that comes about when trying to compare polygons?"
        ],
        [
            "So what we're going to talk about is 1 approach.",
            "To do that, we're going to use the host of distance, and I'm basically going to present scalable approaches to verify the condition House of distance between two sets of points is less or equal to a certain threshold Tetteh."
        ],
        [
            "The second problem that we have to deal with is that we actually usually have have quite a few of these polygons.",
            "That is, we have a large number of geospatial entities, and if you were to use a naive approach to solve this problem, we will have a quadratic number of House of distance computations.",
            "Question is, how do we deal with that?"
        ],
        [
            "And I'm going to talk about a reduction ratio optimal approach and combining with the first solution to basically achieve good runtimes on this particular problem."
        ],
        [
            "Now I talked about the House of Distance.",
            "How is it defined?",
            "We assume that each source resource consists of a set of points as one to SN, and that each target resource also consists of a set of points T1 to T and then the host of distance is defined as the maximum.",
            "Overall, SSI of the minimal distance from SSI to TJ and I'll just give you a short example to explain that if you imagine.",
            "Sorry that this pointer doesn't work, but the matrix on your left hand side.",
            "Yes, your left hand side that basically gives you the distance between the different points of the polygons.",
            "That is, the distance from S one to two.",
            "One for example would be 2 wider distance from SC2T2 would be 5.",
            "Now what you need to do to compute the House of Distance would be to compute the distance between S1 and all the all the TIS or TJ's that is two 134 and then take the minimum of that which would be 2.",
            "You do the same thing for S2B2 and four S 3 W one and then over all these.",
            "More values it then build a maximum which will give you a host of distance of two, which is obviously a quadratic problem.",
            "The question that we need to ask ourselves is how?"
        ],
        [
            "We make it faster.",
            "That is, how can we make sure that we are less than quadratic?",
            "In the best case and that we actually make use of the threshold Theta and the first intuition that we can use here is the fact that if we find I point SI that is such that the minimal distance from SSI to all the TJ's of the target resource is larger than a threshold, then we can actually turn into computation because we know that the House of distance will be larger than the threshold that we've said.",
            "But if we use the same example as before, and we assume that our threshold is one, you will see that the minimal distance from S1 to all the other points, that is from T1T2T3 and T4 is 2, which is larger than one.",
            "Then obviously we can stop computing because we know that the whole soft distance with would be larger than that."
        ],
        [
            "OK, that's the first simple intuition.",
            "We can actually go further and try to reuse information that we create during the computation, and this is how we can do it.",
            "We use the idea of bounding circles.",
            "Idea here is that we want to compute a circle that encloses the resource that we are dealing with completely.",
            "If we have such a circle, we actually know that the House of distance between the first Polygon that is the S Polygon on the.",
            "The left hand side and the second Polygon on the right hand side will be larger or equal to the distance between the center of the circles minus the radii.",
            "The sum of the radii of the circles.",
            "And."
        ],
        [
            "Based on that, we can actually approximate the host of distance between two polygons before we compute it.",
            "So the idea is that if we compute the approximation and we find that it is larger than our threshold, then actually we don't need to compute the real distance.",
            "How do we do that?",
            "The idea here is simply to compute the smallest circle that encloses the Polygon completely and that can be done as follows or we need to do is to compute the distance between all the points in the Polygon.",
            "Then we take the two points that are furthest away from each other.",
            "The center of our circle would then be the point that's exactly in the middle of the two points.",
            "That is, the points X plus and X minus.",
            "Here, while the radius will be half of the distance between the two points that are further from each other, and we can then based on that, because you now have the centers and radii, we can actually approximate the distance between the two polygons and the paper.",
            "We actually prove that that's the smallest circle and that there is no smaller circle that encloses the Polygon completely.",
            "And this just gives you a picture of how that looks like.",
            "So basically on the left hand side you have the Polygon under circle that encloses it and on the right hand side you have this same thing for the target Polygon and them U of SC which is in between would then be the approximation of the House of distance.",
            "We know that all those distance will be larger or equal to that particular menu.",
            "OK."
        ],
        [
            "Now we can actually go further.",
            "I said pointed out before, we have to compute the distance between all the S eyes and all that I so we can actually use the triangle inequality too.",
            "Improve the quality of our approximations.",
            "What we do is that we first to the bounding circle text and if the polygons actually passed that test, what we do next is trying to approximate the distance between the size and TJ's.",
            "We do so by reusing by start we start by computing the distance between South and 21 and then two approximation approximately distance from South one 2T2T3 and so on.",
            "Simply by using the inequality that we have up there and if.",
            "The test is not passed.",
            "That is, if all the distances are larger than the threshold that we said, we can actually terminate the computation.",
            "I would repeat that on all the size."
        ],
        [
            "OK. That's just a rough overview of how to compute how substances quickly.",
            "Now the second step that we need to undertake is how do we then combine that with the Automic space?",
            "So how do we chop down the space in such a way that we only compute distances between polygons that might have the distance that is less or equal to the House of distance threshold that we've set?",
            "I'm just going to give you a short overview of how we do that.",
            "It is very similar to the H R3 algorithm which was presented last year DI SWC."
        ],
        [
            "Only difference here is that the distances across the different axis actually depend on each other.",
            "That is, if two points have the same longitude, then you can actually compute the distance between these two points by using only the latitude that is.",
            "The distance between this point.",
            "So basically the point with coordinates 00 and the points with coordinates zero 10.",
            "Yeah no, the other way.",
            "So basically if two points have the same the same longitude and then you just need to use the Latitude if two points at the same latitude though, you need to not reach longitude.",
            "There are so the distance between 2.00 point zero 90 would not be the same distance as to point the distance between the point that is at zero 60.",
            "That basically here and 6090 those actually, the distance between the two points would be half of the distance of the points at the equator.",
            "Be cause of this course emulation here."
        ],
        [
            "That basically means that when we do the grid computation, we actually need to include the cosine of the latitude at which we are and what we come up with such shapes that basically approximate the square within which all the points.",
            "All the points that are that have a distance less or equal to the similarity or the distance threshold that we've said would actually be all the technical details are given in the paper of three minutes to go."
        ],
        [
            "So just give you an overview of the experiments.",
            "We used three different datasets.",
            "We use the nuts data set that contains of contents, very large polygons and but not that many of them want holes in 500 approximately.",
            "We used to be PG38 with 700,000 points that were not Polygon sourceware, single points, and we use the link to data data set.",
            "The version of 2011 with three million ways in them and the hardware description is given there."
        ],
        [
            "We're able to show is that if we just use this sample of the data.",
            "So basically we just use the host of computation time reduction for the computation that pointed out before we can actually reduce the runtime already by two orders of magnitude.",
            "And that was the case on the link to data set."
        ],
        [
            "When we don't have polygons, that is, we only have points that was actually a very important result for us.",
            "We, the runtime doesn't get poorer.",
            "So basically the fact that we use all these approximations or not does not kill our runtime."
        ],
        [
            "And if we use orchestra is we combine the discretization approach with the House of Computational approach, where we're actually able to show is that we reduce the runtime by two further orders of magnitude."
        ],
        [
            "One interesting result was as well that OPS tool needs or algorithm needs quite a bit of memory because you have to store all these indexes so when we deal with really large data set it is only by using approximations that we can actually fit the whole memory and the whole index is in 10 gigs of memory and the runtimes are all given."
        ],
        [
            "The paper we also carried out some scalability experiments to figure out how we scale and the interesting thing here is that we scale linearly with the number of mappings that we generate, which basically means that our approach actually scales in the order of the square root of time of size of S times size of T, which is quite a good result from our point."
        ],
        [
            "If you.",
            "OK, so this basically sums up the presentation presented approaches for the time efficient computation of household distances and how you can combine them with discretization approaches within a space with Automic.",
            "Distance is an empirical evaluation such as that the complexity of the whole approaches within of square root of South times T and in future work we do want to combine or kit with order distances between polygons and provide a parallel implementation of the whole thing in GPU as well as a load balancer.",
            "Approach to compute these distances efficiently."
        ],
        [
            "That was rushed through the content of the paper and if you have any questions, I'll be happy to answer them and give you more detail during the break.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Axel.",
                    "label": 0
                },
                {
                    "sent": "I'm from the University of Leipzig in Germany and I'm going to talk about orchid, which is an approach for the reduction ratio optimal computation of geospatial distances falling discovery.",
                    "label": 1
                },
                {
                    "sent": "What I'm not going to do is I'm not going to cover any proofs and I'm basically going to try to stick to the core of their approach so that I can stick to the time given.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This whole thing about it's obviously about Link discovery and the question that we can ask ourselves is why do we need that?",
                    "label": 0
                },
                {
                    "sent": "Actually it is very obviously the 4th link data principle.",
                    "label": 0
                },
                {
                    "sent": "We need links between different knowledge Bases 2 for feel quite a few different tasks including for example cross ontology question answering.",
                    "label": 0
                },
                {
                    "sent": "The idea being here that you have two knowledge bases and you basically want to be able to answer questions where you need parts of the data that included the 1st and in the second knowledge base or you need to be able to see which resources are related to each other.",
                    "label": 0
                },
                {
                    "sent": "Storage spaces, that's one of the domains where you need links.",
                    "label": 0
                },
                {
                    "sent": "You obviously also need links when you data integration of iterative queries under like and if we look at the current state of the cloud that is to cloud as described in the loudcloud state Web PY page, you will see that you have approximately 30 billion triples, but only 0.5 billion links.",
                    "label": 1
                },
                {
                    "sent": "That is less than 2% on those leaks are mostly same last links, so there's obviously quite a need to generate links across knowledge bases on the link.",
                    "label": 0
                },
                {
                    "sent": "Dataweb",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formerly what people?",
                    "label": 0
                },
                {
                    "sent": "I mean the formal definition of link discovery policy is that we assume that we are given two sets of source resources, setty of target resources, on a certain relation R, that we want to compute, which is not necessarily the LCMS relation.",
                    "label": 0
                },
                {
                    "sent": "It could also be something like near that is 2.",
                    "label": 0
                },
                {
                    "sent": "Geospatial resources are nearby, each other close to each other according to a certain distance threshold, and what we're asked to find is to set M of pair St from.",
                    "label": 0
                },
                {
                    "sent": "The product of source and target that are such that the relation R holds between S&T now in most cases this is quite difficult to compute and we usually approximate this set by using this set and prime.",
                    "label": 0
                },
                {
                    "sent": "Primaries then defined as the parents St that are such that similarity of SES larger or equal to a certain threshold or the distance between S&T is less or equal to a certain threshold.",
                    "label": 0
                },
                {
                    "sent": "No, if you assume this paradigm that quite a few problems that come about when trying to use it, the first problem being the time complexity.",
                    "label": 0
                },
                {
                    "sent": "Obviously it's a quadratic problem, and if you assume that we want to link for example cities from DB pedia, which cities from Geo name and we need only one millisecond to compute the similarity between the two cities will need 69 days.",
                    "label": 0
                },
                {
                    "sent": "And if you were to try to link the picture with the whole offering geodata it would take us decades on the assumption.",
                    "label": 0
                },
                {
                    "sent": "Basically this quadratic problem.",
                    "label": 0
                },
                {
                    "sent": "So that's the first problem that we need to solve.",
                    "label": 0
                },
                {
                    "sent": "The link discovery.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second problem that we need to solve is obviously complexity or specifications themselves.",
                    "label": 0
                },
                {
                    "sent": "So in most cases when the problems are not trivial, we need quite large function similarity functions.",
                    "label": 0
                },
                {
                    "sent": "That is that we also calling specifications to be able to find correct links, that is, to be able to achieve a high F measure, and that is quite tricky to promising addressed by using machine learning, unsupervised machine learning and the like, But that's not what we're going to talk about today today.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to focus on linking geospatial entities, and with some of the problems that are related to that, obviously.",
                    "label": 1
                },
                {
                    "sent": "The description of geospatial entities differs very much from the other entities, because here we use polylines polygons under like that is complex structures and shapes.",
                    "label": 0
                },
                {
                    "sent": "This gives you.",
                    "label": 0
                },
                {
                    "sent": "You can't see my pointer.",
                    "label": 0
                },
                {
                    "sent": "Basically picture the picture there gives you an example of such a Polygon that is 1981 points that are used to describe Norway, and if you wanted to know the geospatial resources that are nearby Norway, you would actually have to compare these Polygon with other polygons.",
                    "label": 0
                },
                {
                    "sent": "The problem we have is that we are dealing with this with this space where the distance is the author drumming distance.",
                    "label": 0
                },
                {
                    "sent": "That is basically the distance across the X and the Y axis actually depend on each other.",
                    "label": 1
                },
                {
                    "sent": "So we kind of distance function.",
                    "label": 1
                },
                {
                    "sent": "Should we actually use to compare these programs?",
                    "label": 0
                },
                {
                    "sent": "And how do we deal with the runtime complexity problem that comes about when trying to compare polygons?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to talk about is 1 approach.",
                    "label": 0
                },
                {
                    "sent": "To do that, we're going to use the host of distance, and I'm basically going to present scalable approaches to verify the condition House of distance between two sets of points is less or equal to a certain threshold Tetteh.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second problem that we have to deal with is that we actually usually have have quite a few of these polygons.",
                    "label": 0
                },
                {
                    "sent": "That is, we have a large number of geospatial entities, and if you were to use a naive approach to solve this problem, we will have a quadratic number of House of distance computations.",
                    "label": 1
                },
                {
                    "sent": "Question is, how do we deal with that?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to talk about a reduction ratio optimal approach and combining with the first solution to basically achieve good runtimes on this particular problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I talked about the House of Distance.",
                    "label": 0
                },
                {
                    "sent": "How is it defined?",
                    "label": 0
                },
                {
                    "sent": "We assume that each source resource consists of a set of points as one to SN, and that each target resource also consists of a set of points T1 to T and then the host of distance is defined as the maximum.",
                    "label": 0
                },
                {
                    "sent": "Overall, SSI of the minimal distance from SSI to TJ and I'll just give you a short example to explain that if you imagine.",
                    "label": 0
                },
                {
                    "sent": "Sorry that this pointer doesn't work, but the matrix on your left hand side.",
                    "label": 0
                },
                {
                    "sent": "Yes, your left hand side that basically gives you the distance between the different points of the polygons.",
                    "label": 0
                },
                {
                    "sent": "That is, the distance from S one to two.",
                    "label": 0
                },
                {
                    "sent": "One for example would be 2 wider distance from SC2T2 would be 5.",
                    "label": 0
                },
                {
                    "sent": "Now what you need to do to compute the House of Distance would be to compute the distance between S1 and all the all the TIS or TJ's that is two 134 and then take the minimum of that which would be 2.",
                    "label": 0
                },
                {
                    "sent": "You do the same thing for S2B2 and four S 3 W one and then over all these.",
                    "label": 0
                },
                {
                    "sent": "More values it then build a maximum which will give you a host of distance of two, which is obviously a quadratic problem.",
                    "label": 0
                },
                {
                    "sent": "The question that we need to ask ourselves is how?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We make it faster.",
                    "label": 0
                },
                {
                    "sent": "That is, how can we make sure that we are less than quadratic?",
                    "label": 0
                },
                {
                    "sent": "In the best case and that we actually make use of the threshold Theta and the first intuition that we can use here is the fact that if we find I point SI that is such that the minimal distance from SSI to all the TJ's of the target resource is larger than a threshold, then we can actually turn into computation because we know that the House of distance will be larger than the threshold that we've said.",
                    "label": 0
                },
                {
                    "sent": "But if we use the same example as before, and we assume that our threshold is one, you will see that the minimal distance from S1 to all the other points, that is from T1T2T3 and T4 is 2, which is larger than one.",
                    "label": 0
                },
                {
                    "sent": "Then obviously we can stop computing because we know that the whole soft distance with would be larger than that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's the first simple intuition.",
                    "label": 0
                },
                {
                    "sent": "We can actually go further and try to reuse information that we create during the computation, and this is how we can do it.",
                    "label": 0
                },
                {
                    "sent": "We use the idea of bounding circles.",
                    "label": 0
                },
                {
                    "sent": "Idea here is that we want to compute a circle that encloses the resource that we are dealing with completely.",
                    "label": 0
                },
                {
                    "sent": "If we have such a circle, we actually know that the House of distance between the first Polygon that is the S Polygon on the.",
                    "label": 0
                },
                {
                    "sent": "The left hand side and the second Polygon on the right hand side will be larger or equal to the distance between the center of the circles minus the radii.",
                    "label": 0
                },
                {
                    "sent": "The sum of the radii of the circles.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on that, we can actually approximate the host of distance between two polygons before we compute it.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if we compute the approximation and we find that it is larger than our threshold, then actually we don't need to compute the real distance.",
                    "label": 0
                },
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "The idea here is simply to compute the smallest circle that encloses the Polygon completely and that can be done as follows or we need to do is to compute the distance between all the points in the Polygon.",
                    "label": 0
                },
                {
                    "sent": "Then we take the two points that are furthest away from each other.",
                    "label": 0
                },
                {
                    "sent": "The center of our circle would then be the point that's exactly in the middle of the two points.",
                    "label": 0
                },
                {
                    "sent": "That is, the points X plus and X minus.",
                    "label": 0
                },
                {
                    "sent": "Here, while the radius will be half of the distance between the two points that are further from each other, and we can then based on that, because you now have the centers and radii, we can actually approximate the distance between the two polygons and the paper.",
                    "label": 0
                },
                {
                    "sent": "We actually prove that that's the smallest circle and that there is no smaller circle that encloses the Polygon completely.",
                    "label": 0
                },
                {
                    "sent": "And this just gives you a picture of how that looks like.",
                    "label": 0
                },
                {
                    "sent": "So basically on the left hand side you have the Polygon under circle that encloses it and on the right hand side you have this same thing for the target Polygon and them U of SC which is in between would then be the approximation of the House of distance.",
                    "label": 0
                },
                {
                    "sent": "We know that all those distance will be larger or equal to that particular menu.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can actually go further.",
                    "label": 0
                },
                {
                    "sent": "I said pointed out before, we have to compute the distance between all the S eyes and all that I so we can actually use the triangle inequality too.",
                    "label": 1
                },
                {
                    "sent": "Improve the quality of our approximations.",
                    "label": 0
                },
                {
                    "sent": "What we do is that we first to the bounding circle text and if the polygons actually passed that test, what we do next is trying to approximate the distance between the size and TJ's.",
                    "label": 0
                },
                {
                    "sent": "We do so by reusing by start we start by computing the distance between South and 21 and then two approximation approximately distance from South one 2T2T3 and so on.",
                    "label": 0
                },
                {
                    "sent": "Simply by using the inequality that we have up there and if.",
                    "label": 0
                },
                {
                    "sent": "The test is not passed.",
                    "label": 0
                },
                {
                    "sent": "That is, if all the distances are larger than the threshold that we said, we can actually terminate the computation.",
                    "label": 0
                },
                {
                    "sent": "I would repeat that on all the size.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. That's just a rough overview of how to compute how substances quickly.",
                    "label": 0
                },
                {
                    "sent": "Now the second step that we need to undertake is how do we then combine that with the Automic space?",
                    "label": 0
                },
                {
                    "sent": "So how do we chop down the space in such a way that we only compute distances between polygons that might have the distance that is less or equal to the House of distance threshold that we've set?",
                    "label": 0
                },
                {
                    "sent": "I'm just going to give you a short overview of how we do that.",
                    "label": 0
                },
                {
                    "sent": "It is very similar to the H R3 algorithm which was presented last year DI SWC.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only difference here is that the distances across the different axis actually depend on each other.",
                    "label": 0
                },
                {
                    "sent": "That is, if two points have the same longitude, then you can actually compute the distance between these two points by using only the latitude that is.",
                    "label": 0
                },
                {
                    "sent": "The distance between this point.",
                    "label": 0
                },
                {
                    "sent": "So basically the point with coordinates 00 and the points with coordinates zero 10.",
                    "label": 0
                },
                {
                    "sent": "Yeah no, the other way.",
                    "label": 0
                },
                {
                    "sent": "So basically if two points have the same the same longitude and then you just need to use the Latitude if two points at the same latitude though, you need to not reach longitude.",
                    "label": 0
                },
                {
                    "sent": "There are so the distance between 2.00 point zero 90 would not be the same distance as to point the distance between the point that is at zero 60.",
                    "label": 0
                },
                {
                    "sent": "That basically here and 6090 those actually, the distance between the two points would be half of the distance of the points at the equator.",
                    "label": 0
                },
                {
                    "sent": "Be cause of this course emulation here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That basically means that when we do the grid computation, we actually need to include the cosine of the latitude at which we are and what we come up with such shapes that basically approximate the square within which all the points.",
                    "label": 0
                },
                {
                    "sent": "All the points that are that have a distance less or equal to the similarity or the distance threshold that we've said would actually be all the technical details are given in the paper of three minutes to go.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just give you an overview of the experiments.",
                    "label": 0
                },
                {
                    "sent": "We used three different datasets.",
                    "label": 0
                },
                {
                    "sent": "We use the nuts data set that contains of contents, very large polygons and but not that many of them want holes in 500 approximately.",
                    "label": 0
                },
                {
                    "sent": "We used to be PG38 with 700,000 points that were not Polygon sourceware, single points, and we use the link to data data set.",
                    "label": 0
                },
                {
                    "sent": "The version of 2011 with three million ways in them and the hardware description is given there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're able to show is that if we just use this sample of the data.",
                    "label": 0
                },
                {
                    "sent": "So basically we just use the host of computation time reduction for the computation that pointed out before we can actually reduce the runtime already by two orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "And that was the case on the link to data set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we don't have polygons, that is, we only have points that was actually a very important result for us.",
                    "label": 0
                },
                {
                    "sent": "We, the runtime doesn't get poorer.",
                    "label": 0
                },
                {
                    "sent": "So basically the fact that we use all these approximations or not does not kill our runtime.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we use orchestra is we combine the discretization approach with the House of Computational approach, where we're actually able to show is that we reduce the runtime by two further orders of magnitude.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One interesting result was as well that OPS tool needs or algorithm needs quite a bit of memory because you have to store all these indexes so when we deal with really large data set it is only by using approximations that we can actually fit the whole memory and the whole index is in 10 gigs of memory and the runtimes are all given.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The paper we also carried out some scalability experiments to figure out how we scale and the interesting thing here is that we scale linearly with the number of mappings that we generate, which basically means that our approach actually scales in the order of the square root of time of size of S times size of T, which is quite a good result from our point.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "OK, so this basically sums up the presentation presented approaches for the time efficient computation of household distances and how you can combine them with discretization approaches within a space with Automic.",
                    "label": 1
                },
                {
                    "sent": "Distance is an empirical evaluation such as that the complexity of the whole approaches within of square root of South times T and in future work we do want to combine or kit with order distances between polygons and provide a parallel implementation of the whole thing in GPU as well as a load balancer.",
                    "label": 0
                },
                {
                    "sent": "Approach to compute these distances efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was rushed through the content of the paper and if you have any questions, I'll be happy to answer them and give you more detail during the break.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}