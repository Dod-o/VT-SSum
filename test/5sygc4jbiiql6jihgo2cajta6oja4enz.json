{
    "id": "5sygc4jbiiql6jihgo2cajta6oja4enz",
    "title": "Learning multi-scale temporal dynamics with recurrent neural networks",
    "info": {
        "author": [
            "Graham Taylor, School of Engineering, University of Guelph"
        ],
        "published": "March 7, 2016",
        "recorded": "December 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Physics->Statistical Physics"
        ]
    },
    "url": "http://videolectures.net/netadis2015_taylor_neural_networks/",
    "segmentation": [
        [
            "So I'd like to talk to you today about some advancements in recurrent neural networks, again from a more computer science or engineering perspective.",
            "So the first part of my talk will be discussing some different network architectures, and then I'll focus on an application towards the end of the talk.",
            "First of all, I'd like to."
        ],
        [
            "I think that the collaborators on this project it's truly an international collaboration and also spans across industry and academics.",
            "The majority of the heavy lifting has been performed by Natalia never over.",
            "Who's out here, PhD student that's Co supervised by myself and Christian Wolff at in so Lyon, France.",
            "One of my own master students, Griffin Lacey, is also been involved in the project as well as some collaborators from MIT and Google.",
            "So yes, Sir."
        ],
        [
            "Our Rudy was is one of the organizers of this workshop, and unfortunately he he wasn't able to attend.",
            "But he asked me to join and bring deep learning perspective to this workshop, and many of you have been at the conference, have known that deep learning is becoming increasingly popular and hyped up.",
            "And this is in part because of its industrial successes on a number of different applications.",
            "So I just wanted to highlight some of the areas that it has been successful in application wise.",
            "So the big three being vision, speech and natural language processing.",
            "So it's actually had successes in areas that are quite different than a lot of the topics that have been discussed at this workshop, many of them on the biological and most recently the chemical side.",
            "What's common to all of these application domains is the type of data.",
            "That's being exploited here, and that's very unstructured and high dimensional data.",
            "So one thing about common to all of these methods is the data type.",
            "But another thing is that the types of techniques that have become used quite."
        ],
        [
            "Frequently, and there's actually been a bit of an arrowing over recent years.",
            "I would say since the term deep learning has been coined, sort of in the mid 2000s, there's been really this, narrowing towards to what I would call Hammers of deep learning.",
            "One being this something called the convolutional neural network, which many of you heard about many times already at this conference and on the other side, recurrent neural networks, which I'll be talking about today.",
            "Now there's a particular form of recurrent neural network on the right which is called the long short term memory will elaborate on that in a little bit, but the point I wanted to make here is that both of these types of systems that have been employed a fair bit recently are extremely heavily engineered and done in a way that's very specific to a particular domain.",
            "So confidence on the left have been engineered very much towards vision type systems, even though they've been applied to speech and also text as well.",
            "On the other side, recurrent neural networks, particularly architectures like LCMS, have been very specifically architected to maintain long-term temporal dependencies.",
            "So really, the question I'd like to put forward today is, can we move towards simpler but equally powerful architectures that are not so domain?",
            "Engineered but really sort of mimic the general purpose computational capabilities of the brain."
        ],
        [
            "OK, so let's talk about recurrent neural networks a little bit more in detail.",
            "This will be a review for many of you that know about these systems, but some people in the audience may not be familiar with them, so I thought I'd just just walk through them briefly."
        ],
        [
            "So first of all, a recurrent neural network is essentially a multilayer perceptron or standard feedforward network that's just been replicated in time at every time step, there are some inputs which will always call X in the rest of this talk, there we have a linear transformation that's applied to them.",
            "We also include some input."
        ],
        [
            "From the past.",
            "And we sum up these inputs coming."
        ],
        [
            "From the input in the past, instantiations of the hiddens push these through some sort of nonlinear operation which we're going to call PSI and compute the new hidden state.",
            "Then we take that hidden state.",
            "We apply another linear."
        ],
        [
            "Information to it perhaps apply another non linearity fly at the output and that generates so one step of output.",
            "So this essentially except for this little bit on the left coming from the past is essentially just a feedforward regular vanilla feedforward network."
        ],
        [
            "So extending this to time essentially on both ends, we again replicating this architecture with shared parameters gives us this recurrent neural network system, and this is essentially what's been very popular in a number of different applications.",
            "So one key point here about the recurrent neural network is that if you view it as a very deep neural network in time, when you enroll it through time, it's in fact could be depending on the sequence.",
            "Is your modeling 100 or 200 layers deep, and that adds to the difficulty of training these sorts of models."
        ],
        [
            "The update rule is actually very very simple, so from a mathematical perspective, you essentially need to apply to operations at each time step.",
            "So this is the update to the hidden units that I mentioned before, which take into account the previous activations of the hidden's and then the incoming input, and then the output again being computed from the hidden state."
        ],
        [
            "OK, so.",
            "I the recurrent neural networks obviously hold a lot of promise.",
            "They can model arbitrarily complex sequences.",
            "They can model very long term dependencies in theory.",
            "And the other nice aspect of these models is that you can compute exact gradient information by using the backpropagation through time algorithm.",
            "There's other there's other algorithms that can be applied to.",
            "There's some very interesting algorithms actually been presented at this conference for actually avoiding a backwards pass in these in these systems.",
            "So essentially there's exact algorithms to compute these gradients.",
            "The."
        ],
        [
            "Other nice aspects of these models of their generality, so I borrowed this picture from Andre Karpathy, who has a very nice article describing what he calls the unreasonable effectiveness of recurrent neural networks.",
            "And this is just essentially a picture of the different types of architectures they generalize, so of course there's a recurrent neural network generalizes this sort of feedforward architecture, but one can do sort of 1 to many, many to one, many to many.",
            "And many to many mapping, so this is the one on the right is something we sort of classically think of for recurrent neural networks, but this sort of architecture, where the network accepts, accepts an input, produces a representation, and then takes that sort of in capsulation or or vector form of that representation that's been learned, and then generates some sort of dynamic output has been very popular recently, so this is actually appeared in areas like.",
            "Last year at NIPS being presented for machine translation, so taking it in certain English sentence produce a French sentence, I saw a post earlier today on taking some written language commands and doing some sort of path planning for a robot.",
            "There's also been a lot of work on taking sort of image information as input and then generating text based captions that describe those images, so it's very.",
            "Yes, sorry, time is always flowing from left to right in these architectures.",
            "And that will be a conventional stay with for the rest of this."
        ],
        [
            "OK, so you know these architectures are extremely general.",
            "They're powerful, they're complex, but the reality is that training them via the standard gradient descent procedures will often fail on simple sort of toy tasks typically formulated tends to test their ability to maintain memory.",
            "Remember very long events from very long time ago, so this is going to be attributed to what's called the vanishing or exploding gradients problem, which was.",
            "As extensively explored in the 1990s, nobody really converged on a reasonable way of solving this problem, and there has been a resurgence again, sort of towards the late 2000s on techniques that are both sort of architecturally motivated and also motivated from the perspective of optimization."
        ],
        [
            "So let's let's just discuss some of these architectural changes that have been proposed in recent years.",
            "So the first one, actually the gating family, goes back to the late 90s when people were exploring these types of architectures very actively.",
            "And this was put."
        ],
        [
            "Word in a model called Long short term memory from the researchers in Europe.",
            "Now this architecture has been refined and simplified and as a result it sort of in more uptake and usage.",
            "Lately, it's also been adapted into simpler models like gated recurrent units, so I'm going to just quickly talk about this architecture because it's been so dominant at this conference and other venues as well."
        ],
        [
            "So the first of all the key concept in an LS TM is the idea of gating, which I mentioned before and essentially the idea of gating is that you have these.",
            "You can think of the most liked apps that you can turn on and off, and so this would be seen in these little these units which essentially have little X through them, and so the at the heart of an L stem cell is this little pink guy here.",
            "And this is a memory unit.",
            "And this memory unit can continue to sort of feedback to itself, and this is what allows it to maintain a very long term history of what's happened in the past.",
            "And this again, is some sort of arbitrary representation of what's happened.",
            "So it can either sort of remember what's going on in the past, or it can update itself given the information from previous time steps, and also the input that's being presented now, the information that flows into this unit again is, you can think of this again like a top that can be turned on and off.",
            "And this is a function of some other parameters and as well as the inputs and the previous time steps.",
            "So we sort of dynamically dynamically allow information to flow into this unit.",
            "We allow information to flow out and be read off, and we also have what's called the Forget gate here, which will essentially destroy information that's being held in this memory cell.",
            "And so again, this has been architected to maintain very long term dependencies.",
            "You'll see compared to the.",
            "The equations that are associated with a standard recurrent neural network.",
            "There are a fair bit more complicated and numerous.",
            "This is 1 variant of an R and LS team, which is essentially the simplest form of them.",
            "It seems that almost every paper in LCMS presents a slightly different formulation.",
            "There have actually been a couple of studies this past year that are posted on archive.",
            "One of them called LCMS, a search space Odyssey, an essentially these these works have iterated through millions of different LCM architecture to really discuss what sort of what kinds of architectural tricks are really most important, and it turns out that all of these variants of LCMS work equally the same.",
            "The forget gates and the output gates are are most important and you can go and read more about it.",
            "I mean this if you're new to LCMS and you want they are complicated architecture, you want to learn more.",
            "I recommend this blog post by Chrysoula.",
            "It's all about understanding LCMS.",
            "OK, so back to other architectural salute."
        ],
        [
            "Things that have been proposed for the long term memory problem.",
            "There's been authors that have investigated depth.",
            "OK, so I mentioned before that in RNN is sort of inherently deep because you can enroll it in time.",
            "But you can also place structural depth between the input and the hidden layer as well as from hidden to hidden and from hidden to output.",
            "So you can place additional layers in there, and this actually improves performance and this has been explored by Residents Canyon colleagues.",
            "One can also change the typical type of non linearity.",
            "Often it's been a 10 H unit.",
            "And more recently, along with feedforward neural networks, people have started to explore linear rectified units.",
            "This has been attempted in LS teams and requires a certain a special type of initialization to get it to work well, but there's some work earlier this year that explains how to use rectifiers in the context of.",
            "LCMS.",
            "Now there's another class of models, and that's what I'm going to."
        ],
        [
            "Focus on today which have different groups of hidden units that operate at different timescales.",
            "This was first explored long time ago back in sort of the first wave of recurrent neural networks by Arab NGOs Group and more recently explored by the Sea Group in Switzerland and I'm going to talk a little bit more about optimization but then come back to this idea of units operating at different clocks.",
            "OK."
        ],
        [
            "So first of all, in terms of the optimization class of improvements to our NNS, what really got this second wave of RNN exploration to start with some work by James Martens, anilius its cover at the University of Toronto, who explored 2nd order methods for optimization.",
            "And they proposed the Hessian free technique, which was adopted and used pretty widely for a couple of years or so, but that is sort of fallen by the wayside in favor of simpler techniques there.",
            "Of course, the idea of using random reservoirs, and so we're only learning the connections in particular parts of the network.",
            "So, for example, using random dynamics and then changing the hidden to output weights as opposed to changing all the weights in the network and then in terms of the exploding gradients problem.",
            "Almost All authors are using some sort of form of gradient clipping or normalization, and if you do this type of technique, you don't actually have to rely so much on more sophisticated 2nd order optimization methods.",
            "Finally, there have been a number of authors who, starting with Alias Discover, have explored different initialization strategies.",
            "Being very careful about how you initialize the weights in these sorts of networks and actually as borrowed some ideas from reservoir like Echo State Networks and so forth in terms of inspiring the way that these networks could be initialized.",
            "OK, so that's just sort of an overview of what's happened in the last few years with respect to recurrent neural networks and the classes of modifications that's been made to the basic RNN architecture that have.",
            "Made it so successful on industrial strength problems.",
            "Now."
        ],
        [
            "In terms of what I'd like to talk about, today is 1 modification that's similar in nature to the STM, but produces a network that's competitive in terms of performance, and this is the idea that's called the Clockwork RNN by Jan Cook, Nikken and colleagues.",
            "I think he's giving a talk in another workshop right now as we speak.",
            "The idea here is to partition the hidden units into what are called modules, and these modules are all operating at different Clock cycles.",
            "This actually is a type of sparse connectivity in the recurrent neural network because not all hidden units are connected to other hidden units, and I'll show you a picture of that in a second, so this actually reduces the number of parameters compared to the vanilla RNN."
        ],
        [
            "So let's just look at a picture of this clockwork RNN.",
            "So the first thing to look at is essentially the module.",
            "So I'm just going to draw a picture of sort of 1 unit per module, but typically in practice you'll have several units operating at these different Clock cycles, so you have essentially very fast units here drawn in pink, and these fast units update just like the standard RNN every time step.",
            "You have sort of medium type of units which would maybe update here every second time step an you also have slower units which update only these ones updating every four times steps.",
            "We sort of define these.",
            "We have a sort of a Clock parameter and in this in this setup right here the period of each of these modules is 2 to the K. OK, so we have one at 1182 and when it operating at 4 the other change I mentioned between this sort of system in a standard.",
            "Recurrent neural network, or that there's limited connectivity?",
            "So essentially the fast units are connected to each other and receive input from all of the other modules.",
            "But the any type of unit only receives input from slower units.",
            "OK, so it's not full connectivity, and that's the reason why the number of parameters is reduced in this module."
        ],
        [
            "Now to do an update.",
            "Actually, you know compared to the STM we write the update rules.",
            "It doesn't look that much different than the standard RNN.",
            "In fact, the update to terms of producing the output is the exact same.",
            "What changes is how you actually update your hidden units, and So what you've done now is you've divided your hidden units into blocks.",
            "I said these blocks.",
            "These reference the different clocks and so you update only if you are.",
            "If you essentially take your time step.",
            "You might it with the particular Clock you're operating it, and that gives 0.",
            "Otherwise you just keep your state maintained from the previous steps.",
            "OK, so it really has kind of a flavor of LS teams and in the way that information can be sort of kept over long periods of time.",
            "It's just that the LTM has essentially more complexity and a little bit more control of the sort of the propagation of this information, and then forgetting of this information.",
            "Yes.",
            "In training or so, usually when I say complexity, I mean the number of parameters and the network, but also sort of architecturally how it's been formed.",
            "Correct, yes.",
            "So again by reducing the number of parameters and simplifying in terms of the number of parameters, this network is less susceptible to overfitting."
        ],
        [
            "OK, so just to give we gave these update rules in terms of the math just to give a visual depiction.",
            "This sort of diagram was used by the original authors of how this network operates, so this gives an example at time step six, OK, we've selected N = 2 as I mentioned before, which means that the clocks are operating at sort of one time step of a period of 1, two and four and eight and so forth.",
            "And So what happens at time Step 6 is that only the modules that have.",
            "K = 1 and our end of the K = 1 end of the K = 2 get updated.",
            "OK, So what you see in the structure of this you matrix, which is the matrix that connects hidden units to hidden units, is that these these fast units are basically connected to all of the other modules and they take updates from everybody and these units operating time Step 2 are only receiving input from the modules that are slower than them.",
            "OK, and again we have grey here.",
            "To denote in terms of the output units that are not being updated at this time, Step 6.",
            "And then we also have Gray in the parameters, meaning that at this time step 6 these parameters are not being updated because they're not affecting the output.",
            "We also have some especially structure in the weights as well, so again the the weights that connect to the input also are being sort of organized by rows corresponding to modules, and affectively only the weights corresponding to module one and module 2.",
            "Are being applied to the input to compute the updates for these groups of modules.",
            "OK, so it's actually much cheaper computationally to perform an update in this sort of model due to the sparsity.",
            "Then it is to apply an update and."
        ],
        [
            "Current neural network.",
            "OK, so some of the problems associated with these clockwork RNN's ascentia Lee, you have these very slow units which are inactive over very long periods of time.",
            "Therefore as a result there by nature kind of undertrained.",
            "OK, so we saw before that we have these these blocks of the weight matrices that are not being updated at certain time steps, so the very slow units don't get as much training information as the faster units and due to this reason.",
            "Essentially, given a fixed amount of training steps, these slow units barely contributed to the prediction made by the network.",
            "The other main problem experienced by these clockwork RN ends are that their shift variant.",
            "So essentially because the Clock rate is tide to the actual time step, if you just shift the input a little bit, that's going to cause the hidden activities and therefore the outputs to be quite different."
        ],
        [
            "So we have a picture of this where we've essentially taken a data set of inertial measurement.",
            "So these are people using a phone.",
            "I'll talk more about the application a little bit later, but we have some inertial measurements from a phone.",
            "Someone performing different activities.",
            "So on top it's sort of reading and walking on the bottom, it's writing and sitting and we take the original sequence of these activities.",
            "We apply this to an RNN that's been trained so the original vanilla RNN, and we read out the trace.",
            "Of hidden units for different sequences, so the different colors here represent sequences that have been just shifted by one time step, and we measure the hidden unit traces on these shifted sequences.",
            "And then we just shift them back accordingly and align them temporarily.",
            "So we see that for the vanilla RNN, counting for the shift that happened, we sort of shift all the hidden unit traces back to the beginning.",
            "They're all aligned essentially, but for the clockwork RNN, even after we do this shift, we have wildly different.",
            "Traces on this particular hidden unit, so you see this shift variance yes.",
            "The reduction in the Paris peace.",
            "Also that Indian.",
            "Something in terms of learning identity.",
            "It's both OK, so computationally you reduce number of parameters.",
            "The updates are faster, you can control overfitting, but you also have sort of explicitly incorporated sort of multiscale temporal dynamics into the network, and that can be advantageous from a modeling perspective.",
            "Helpful, thanks.",
            "That's right, yes.",
            "So you could also do that sort of on a neuron by neuron basis as well as opposed to grouping them into these modules."
        ],
        [
            "OK, so in terms of addressing these properties we've proposed is a variant of the Clockwork RNN, which we call the dense clockwork RNN, and the simple idea here is to just actually replicate these units at the time steps where they were not included in the seat.",
            "In the clockwork RNN.",
            "So the idea is to share parameters between these different units, but essentially have units that update at particular clocks or or focus on receiving updates.",
            "According to a certain Clock cycle, but actually continuously be active.",
            "So the idea here is, again, there's no change in the fastest group of units, the same as the clockwork RNN, but essentially the next group of units that update given two time steps ago, they will continue to update every time step, but they'll always look back to frames, essentially to receive updates similar to these fastest units.",
            "Up here will always look back 4 frames, perform an update.",
            "But they're essentially copied, and so they will indeed perform updates at every step.",
            "OK, so the actual the connectivity becomes a little bit more complicated in terms of drawing it out here, but the idea of only receiving updates from the modules with slower Clock still exists right here.",
            "So again, because of the parameter sharing, we're actually not increasing the number of parameters relative to the standard clockwork RNN, but we gain a shift invariant model."
        ],
        [
            "Again, you can just recall this picture I showed before in terms of performing the updates with the Clockwork RNN."
        ],
        [
            "It changes a little bit when we look at the dense Clock works and so the reason why we write this out here is just sort of formulate mathematically a definition of the updates.",
            "This is done in terms of formulating a essentially a structured matrix which represents the connectivity.",
            "Again, the idea that only units receive updates from slower modules receive updates from slower modules.",
            "We also have this H matrix.",
            "So before we had just the vector of hidden activities, now what we've had to do to formulate this?",
            "Is maintain a history of hidden activations?",
            "OK, so we formed this matrix of histories and then we compute the matrix product of these and then we essentially this operator here is essentially operated that takes the diagonal of the result and formulates a vector from that.",
            "Now there's a lot of.",
            "There's there's, you know when we take the diagonal, we're obviously throwing away a lot of computation that we just did so in practice we wouldn't actually do that.",
            "This is just sort of a way of writing this out mathematically, so in practice you would only actually compute.",
            "The updates that you actually actually need.",
            "OK, so the downside.",
            "The last thing I want to say about this.",
            "The downside is that you now have to maintain a short history depending on the number of different modules you have of what happened, so you have a little bit of an extra memory gain, but in terms of computationally in terms of the number of parameters as well, there's really no difference with the standard clockwork module."
        ],
        [
            "OK, so we we test this on the same example you saw before with the regular Clock works on these two different traces corresponding to different types of inputs.",
            "You'll see the Clock works being shift variant whereas the dense Clock works.",
            "Now we see same with the villanelle RNN, their shift invariant?",
            "Let's sort the property that we want to see.",
            "We'll compare them a little bit later on actual application.",
            "OK, so I think we have just a few couple minutes left, right?",
            "OK, so just very quickly."
        ],
        [
            "In terms of the application that we.",
            "Proposed for these these modules and develop them in the context of this.",
            "This is actually done as I mentioned before with Google.",
            "The idea of the fact that entering a pin or passcode or some sort of password on your mobile device is really annoying.",
            "It takes time.",
            "It's also not completely secure, so the idea is can we do what's called continuous authentication?",
            "So can your phone be constantly aware of who has it.",
            "So if you're using your phone, you type a certain way, you're in certain locations.",
            "It has a camera.",
            "I can see your face.",
            "It knows that you're using it and it unlocks.",
            "If someone else grabs your phone, it will lock."
        ],
        [
            "Basically we carried out actually the largest cell phone based biometric study with 1500 volunteers who all received the identical phone, which is a Nexus 5 that had 13 sensors and they perform several months of sort of natural usage with these phones and we collected all of the sensor data they signed up for this.",
            "They agreed to share all of their sensor data with the project, and so the various sensors include camera, GPS, everything that we've been typing so we can look at keystroke patterns.",
            "Where they've been, as well as the accelerometer and the gyroscope.",
            "So the way this project works is different teams worked on different modalities and essentially our team focused only on the accelerometer and gyroscope.",
            "Now these modalities on their own are actually not that useful for authenticating users.",
            "They're actually very weak biometric.",
            "But when you combine them with these other modalities, you actually get something that is much stronger than pins or passcodes.",
            "OK, so we don't have a ton of time."
        ],
        [
            "So I'm not going to go into the details of the project Ascentia Lee.",
            "I mentioned this idea of convolutional neural networks, recurrent neural networks and so forth.",
            "These were used as feature extractors for a generative model that actually did user authentication.",
            "So the key thing here was that we couldn't actually train the convolutional."
        ],
        [
            "Networks or recurrent neural networks to do discrimination on the phone itself because we're limited, sort of computationally and we couldn't do it on the cloud either because we can't send people's private data over to the cloud to enroll them in the program.",
            "So we essentially did a technique."
        ],
        [
            "Where we fit a generative model to a large user base, we push that to the phone and then on the phone we would essentially just do a slight adaptation of the generative model from what we call the universal background model of everybody's data towards that client.",
            "And so the main result here."
        ],
        [
            "Going to just skip over a little bit is to compare different ways of doing feature extraction, so either with using convolutional networks that are trained on fairly long sequences.",
            "So on the order of 10 seconds per sequence to RNN type models which are trained on shorter sequences.",
            "So sequences of roughly one second."
        ],
        [
            "The results are that essentially, you know, on a purely class appear classification classification task.",
            "Looking at just the various means of a feature extraction, these dense Clock works actually perform the best, outperforming the convolutional networks and the other.",
            "The other point here is that these these RNS, which are actually able to sort of explicitly model temporal contexts, tend to do better than the comnets which implicitly model it via the convolutional operation.",
            "But don't explicitly model it."
        ],
        [
            "Apart from just looking at the future extraction alone and how it performs on the classification tasks, this is on the full biometric setup where we had basically held out users that had that we've never seen before.",
            "We do this again as I described this very simple adaptation of this universal background model, given maybe a week or so of them interacting with the device, and then we measured the ability of the model to authenticate them, and So what you're seeing here is what's called equal error rate, so an equal error rate of.",
            "20% would mean that sort of 20% of 20% of the time a user would be a false user would be authenticated as the correct user and also 80% of the time you would be using your phone and would actually recognize it.",
            "It's you.",
            "So it goes both ways.",
            "That's why they call it equal error rate.",
            "And then we also have.",
            "This is measured on the validation set and the common way in this domain to measure error is sort of called this.",
            "This half total error rate, which is essentially a blend between the false acceptance rate equal between false acceptance rate and false rejection rate, and again we see that the these dense Clock words are outperforming the other convolutional methods as well as the standard recurrent neural networks.",
            "OK, so I think I'm going to conclude."
        ],
        [
            "Here and just essentially say that these convolutional neural networks in recurrent neural networks have been obviously very popular in various applications, but they're heavily engineered models and it's nice to be able to turn to other other areas like the ones we've been talking about today towards systems that can be equally powerful, powerful but less engineered to particular domains.",
            "So we presented a new variant of the clockwork RN ends which are shift invariant and also have favorable computational properties.",
            "Anne works essentially exploring these sorts of models on different types of modalities, beyond accelerated accelerometers and gyroscopes.",
            "And then we're also investigating other types of these neural models and other variants of these dense Clock works on other applications like gesture recognition, activity recognition.",
            "So thank you very much."
        ],
        [
            "Thanks for your attention and will take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd like to talk to you today about some advancements in recurrent neural networks, again from a more computer science or engineering perspective.",
                    "label": 1
                },
                {
                    "sent": "So the first part of my talk will be discussing some different network architectures, and then I'll focus on an application towards the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'd like to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that the collaborators on this project it's truly an international collaboration and also spans across industry and academics.",
                    "label": 0
                },
                {
                    "sent": "The majority of the heavy lifting has been performed by Natalia never over.",
                    "label": 0
                },
                {
                    "sent": "Who's out here, PhD student that's Co supervised by myself and Christian Wolff at in so Lyon, France.",
                    "label": 0
                },
                {
                    "sent": "One of my own master students, Griffin Lacey, is also been involved in the project as well as some collaborators from MIT and Google.",
                    "label": 0
                },
                {
                    "sent": "So yes, Sir.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our Rudy was is one of the organizers of this workshop, and unfortunately he he wasn't able to attend.",
                    "label": 0
                },
                {
                    "sent": "But he asked me to join and bring deep learning perspective to this workshop, and many of you have been at the conference, have known that deep learning is becoming increasingly popular and hyped up.",
                    "label": 0
                },
                {
                    "sent": "And this is in part because of its industrial successes on a number of different applications.",
                    "label": 0
                },
                {
                    "sent": "So I just wanted to highlight some of the areas that it has been successful in application wise.",
                    "label": 0
                },
                {
                    "sent": "So the big three being vision, speech and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "So it's actually had successes in areas that are quite different than a lot of the topics that have been discussed at this workshop, many of them on the biological and most recently the chemical side.",
                    "label": 0
                },
                {
                    "sent": "What's common to all of these application domains is the type of data.",
                    "label": 0
                },
                {
                    "sent": "That's being exploited here, and that's very unstructured and high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So one thing about common to all of these methods is the data type.",
                    "label": 0
                },
                {
                    "sent": "But another thing is that the types of techniques that have become used quite.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Frequently, and there's actually been a bit of an arrowing over recent years.",
                    "label": 0
                },
                {
                    "sent": "I would say since the term deep learning has been coined, sort of in the mid 2000s, there's been really this, narrowing towards to what I would call Hammers of deep learning.",
                    "label": 0
                },
                {
                    "sent": "One being this something called the convolutional neural network, which many of you heard about many times already at this conference and on the other side, recurrent neural networks, which I'll be talking about today.",
                    "label": 1
                },
                {
                    "sent": "Now there's a particular form of recurrent neural network on the right which is called the long short term memory will elaborate on that in a little bit, but the point I wanted to make here is that both of these types of systems that have been employed a fair bit recently are extremely heavily engineered and done in a way that's very specific to a particular domain.",
                    "label": 0
                },
                {
                    "sent": "So confidence on the left have been engineered very much towards vision type systems, even though they've been applied to speech and also text as well.",
                    "label": 0
                },
                {
                    "sent": "On the other side, recurrent neural networks, particularly architectures like LCMS, have been very specifically architected to maintain long-term temporal dependencies.",
                    "label": 0
                },
                {
                    "sent": "So really, the question I'd like to put forward today is, can we move towards simpler but equally powerful architectures that are not so domain?",
                    "label": 0
                },
                {
                    "sent": "Engineered but really sort of mimic the general purpose computational capabilities of the brain.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's talk about recurrent neural networks a little bit more in detail.",
                    "label": 0
                },
                {
                    "sent": "This will be a review for many of you that know about these systems, but some people in the audience may not be familiar with them, so I thought I'd just just walk through them briefly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, a recurrent neural network is essentially a multilayer perceptron or standard feedforward network that's just been replicated in time at every time step, there are some inputs which will always call X in the rest of this talk, there we have a linear transformation that's applied to them.",
                    "label": 0
                },
                {
                    "sent": "We also include some input.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the past.",
                    "label": 0
                },
                {
                    "sent": "And we sum up these inputs coming.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the input in the past, instantiations of the hiddens push these through some sort of nonlinear operation which we're going to call PSI and compute the new hidden state.",
                    "label": 0
                },
                {
                    "sent": "Then we take that hidden state.",
                    "label": 0
                },
                {
                    "sent": "We apply another linear.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information to it perhaps apply another non linearity fly at the output and that generates so one step of output.",
                    "label": 0
                },
                {
                    "sent": "So this essentially except for this little bit on the left coming from the past is essentially just a feedforward regular vanilla feedforward network.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So extending this to time essentially on both ends, we again replicating this architecture with shared parameters gives us this recurrent neural network system, and this is essentially what's been very popular in a number of different applications.",
                    "label": 0
                },
                {
                    "sent": "So one key point here about the recurrent neural network is that if you view it as a very deep neural network in time, when you enroll it through time, it's in fact could be depending on the sequence.",
                    "label": 0
                },
                {
                    "sent": "Is your modeling 100 or 200 layers deep, and that adds to the difficulty of training these sorts of models.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The update rule is actually very very simple, so from a mathematical perspective, you essentially need to apply to operations at each time step.",
                    "label": 0
                },
                {
                    "sent": "So this is the update to the hidden units that I mentioned before, which take into account the previous activations of the hidden's and then the incoming input, and then the output again being computed from the hidden state.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I the recurrent neural networks obviously hold a lot of promise.",
                    "label": 0
                },
                {
                    "sent": "They can model arbitrarily complex sequences.",
                    "label": 0
                },
                {
                    "sent": "They can model very long term dependencies in theory.",
                    "label": 1
                },
                {
                    "sent": "And the other nice aspect of these models is that you can compute exact gradient information by using the backpropagation through time algorithm.",
                    "label": 1
                },
                {
                    "sent": "There's other there's other algorithms that can be applied to.",
                    "label": 0
                },
                {
                    "sent": "There's some very interesting algorithms actually been presented at this conference for actually avoiding a backwards pass in these in these systems.",
                    "label": 0
                },
                {
                    "sent": "So essentially there's exact algorithms to compute these gradients.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other nice aspects of these models of their generality, so I borrowed this picture from Andre Karpathy, who has a very nice article describing what he calls the unreasonable effectiveness of recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "And this is just essentially a picture of the different types of architectures they generalize, so of course there's a recurrent neural network generalizes this sort of feedforward architecture, but one can do sort of 1 to many, many to one, many to many.",
                    "label": 0
                },
                {
                    "sent": "And many to many mapping, so this is the one on the right is something we sort of classically think of for recurrent neural networks, but this sort of architecture, where the network accepts, accepts an input, produces a representation, and then takes that sort of in capsulation or or vector form of that representation that's been learned, and then generates some sort of dynamic output has been very popular recently, so this is actually appeared in areas like.",
                    "label": 0
                },
                {
                    "sent": "Last year at NIPS being presented for machine translation, so taking it in certain English sentence produce a French sentence, I saw a post earlier today on taking some written language commands and doing some sort of path planning for a robot.",
                    "label": 0
                },
                {
                    "sent": "There's also been a lot of work on taking sort of image information as input and then generating text based captions that describe those images, so it's very.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry, time is always flowing from left to right in these architectures.",
                    "label": 0
                },
                {
                    "sent": "And that will be a conventional stay with for the rest of this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you know these architectures are extremely general.",
                    "label": 0
                },
                {
                    "sent": "They're powerful, they're complex, but the reality is that training them via the standard gradient descent procedures will often fail on simple sort of toy tasks typically formulated tends to test their ability to maintain memory.",
                    "label": 1
                },
                {
                    "sent": "Remember very long events from very long time ago, so this is going to be attributed to what's called the vanishing or exploding gradients problem, which was.",
                    "label": 1
                },
                {
                    "sent": "As extensively explored in the 1990s, nobody really converged on a reasonable way of solving this problem, and there has been a resurgence again, sort of towards the late 2000s on techniques that are both sort of architecturally motivated and also motivated from the perspective of optimization.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's let's just discuss some of these architectural changes that have been proposed in recent years.",
                    "label": 0
                },
                {
                    "sent": "So the first one, actually the gating family, goes back to the late 90s when people were exploring these types of architectures very actively.",
                    "label": 0
                },
                {
                    "sent": "And this was put.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Word in a model called Long short term memory from the researchers in Europe.",
                    "label": 0
                },
                {
                    "sent": "Now this architecture has been refined and simplified and as a result it sort of in more uptake and usage.",
                    "label": 0
                },
                {
                    "sent": "Lately, it's also been adapted into simpler models like gated recurrent units, so I'm going to just quickly talk about this architecture because it's been so dominant at this conference and other venues as well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first of all the key concept in an LS TM is the idea of gating, which I mentioned before and essentially the idea of gating is that you have these.",
                    "label": 0
                },
                {
                    "sent": "You can think of the most liked apps that you can turn on and off, and so this would be seen in these little these units which essentially have little X through them, and so the at the heart of an L stem cell is this little pink guy here.",
                    "label": 0
                },
                {
                    "sent": "And this is a memory unit.",
                    "label": 0
                },
                {
                    "sent": "And this memory unit can continue to sort of feedback to itself, and this is what allows it to maintain a very long term history of what's happened in the past.",
                    "label": 0
                },
                {
                    "sent": "And this again, is some sort of arbitrary representation of what's happened.",
                    "label": 0
                },
                {
                    "sent": "So it can either sort of remember what's going on in the past, or it can update itself given the information from previous time steps, and also the input that's being presented now, the information that flows into this unit again is, you can think of this again like a top that can be turned on and off.",
                    "label": 0
                },
                {
                    "sent": "And this is a function of some other parameters and as well as the inputs and the previous time steps.",
                    "label": 0
                },
                {
                    "sent": "So we sort of dynamically dynamically allow information to flow into this unit.",
                    "label": 0
                },
                {
                    "sent": "We allow information to flow out and be read off, and we also have what's called the Forget gate here, which will essentially destroy information that's being held in this memory cell.",
                    "label": 0
                },
                {
                    "sent": "And so again, this has been architected to maintain very long term dependencies.",
                    "label": 0
                },
                {
                    "sent": "You'll see compared to the.",
                    "label": 0
                },
                {
                    "sent": "The equations that are associated with a standard recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "There are a fair bit more complicated and numerous.",
                    "label": 0
                },
                {
                    "sent": "This is 1 variant of an R and LS team, which is essentially the simplest form of them.",
                    "label": 0
                },
                {
                    "sent": "It seems that almost every paper in LCMS presents a slightly different formulation.",
                    "label": 0
                },
                {
                    "sent": "There have actually been a couple of studies this past year that are posted on archive.",
                    "label": 0
                },
                {
                    "sent": "One of them called LCMS, a search space Odyssey, an essentially these these works have iterated through millions of different LCM architecture to really discuss what sort of what kinds of architectural tricks are really most important, and it turns out that all of these variants of LCMS work equally the same.",
                    "label": 0
                },
                {
                    "sent": "The forget gates and the output gates are are most important and you can go and read more about it.",
                    "label": 0
                },
                {
                    "sent": "I mean this if you're new to LCMS and you want they are complicated architecture, you want to learn more.",
                    "label": 0
                },
                {
                    "sent": "I recommend this blog post by Chrysoula.",
                    "label": 0
                },
                {
                    "sent": "It's all about understanding LCMS.",
                    "label": 0
                },
                {
                    "sent": "OK, so back to other architectural salute.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things that have been proposed for the long term memory problem.",
                    "label": 0
                },
                {
                    "sent": "There's been authors that have investigated depth.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mentioned before that in RNN is sort of inherently deep because you can enroll it in time.",
                    "label": 0
                },
                {
                    "sent": "But you can also place structural depth between the input and the hidden layer as well as from hidden to hidden and from hidden to output.",
                    "label": 0
                },
                {
                    "sent": "So you can place additional layers in there, and this actually improves performance and this has been explored by Residents Canyon colleagues.",
                    "label": 0
                },
                {
                    "sent": "One can also change the typical type of non linearity.",
                    "label": 0
                },
                {
                    "sent": "Often it's been a 10 H unit.",
                    "label": 0
                },
                {
                    "sent": "And more recently, along with feedforward neural networks, people have started to explore linear rectified units.",
                    "label": 0
                },
                {
                    "sent": "This has been attempted in LS teams and requires a certain a special type of initialization to get it to work well, but there's some work earlier this year that explains how to use rectifiers in the context of.",
                    "label": 0
                },
                {
                    "sent": "LCMS.",
                    "label": 0
                },
                {
                    "sent": "Now there's another class of models, and that's what I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Focus on today which have different groups of hidden units that operate at different timescales.",
                    "label": 1
                },
                {
                    "sent": "This was first explored long time ago back in sort of the first wave of recurrent neural networks by Arab NGOs Group and more recently explored by the Sea Group in Switzerland and I'm going to talk a little bit more about optimization but then come back to this idea of units operating at different clocks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, in terms of the optimization class of improvements to our NNS, what really got this second wave of RNN exploration to start with some work by James Martens, anilius its cover at the University of Toronto, who explored 2nd order methods for optimization.",
                    "label": 0
                },
                {
                    "sent": "And they proposed the Hessian free technique, which was adopted and used pretty widely for a couple of years or so, but that is sort of fallen by the wayside in favor of simpler techniques there.",
                    "label": 0
                },
                {
                    "sent": "Of course, the idea of using random reservoirs, and so we're only learning the connections in particular parts of the network.",
                    "label": 0
                },
                {
                    "sent": "So, for example, using random dynamics and then changing the hidden to output weights as opposed to changing all the weights in the network and then in terms of the exploding gradients problem.",
                    "label": 0
                },
                {
                    "sent": "Almost All authors are using some sort of form of gradient clipping or normalization, and if you do this type of technique, you don't actually have to rely so much on more sophisticated 2nd order optimization methods.",
                    "label": 0
                },
                {
                    "sent": "Finally, there have been a number of authors who, starting with Alias Discover, have explored different initialization strategies.",
                    "label": 0
                },
                {
                    "sent": "Being very careful about how you initialize the weights in these sorts of networks and actually as borrowed some ideas from reservoir like Echo State Networks and so forth in terms of inspiring the way that these networks could be initialized.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just sort of an overview of what's happened in the last few years with respect to recurrent neural networks and the classes of modifications that's been made to the basic RNN architecture that have.",
                    "label": 0
                },
                {
                    "sent": "Made it so successful on industrial strength problems.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of what I'd like to talk about, today is 1 modification that's similar in nature to the STM, but produces a network that's competitive in terms of performance, and this is the idea that's called the Clockwork RNN by Jan Cook, Nikken and colleagues.",
                    "label": 0
                },
                {
                    "sent": "I think he's giving a talk in another workshop right now as we speak.",
                    "label": 0
                },
                {
                    "sent": "The idea here is to partition the hidden units into what are called modules, and these modules are all operating at different Clock cycles.",
                    "label": 0
                },
                {
                    "sent": "This actually is a type of sparse connectivity in the recurrent neural network because not all hidden units are connected to other hidden units, and I'll show you a picture of that in a second, so this actually reduces the number of parameters compared to the vanilla RNN.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just look at a picture of this clockwork RNN.",
                    "label": 0
                },
                {
                    "sent": "So the first thing to look at is essentially the module.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to draw a picture of sort of 1 unit per module, but typically in practice you'll have several units operating at these different Clock cycles, so you have essentially very fast units here drawn in pink, and these fast units update just like the standard RNN every time step.",
                    "label": 0
                },
                {
                    "sent": "You have sort of medium type of units which would maybe update here every second time step an you also have slower units which update only these ones updating every four times steps.",
                    "label": 0
                },
                {
                    "sent": "We sort of define these.",
                    "label": 0
                },
                {
                    "sent": "We have a sort of a Clock parameter and in this in this setup right here the period of each of these modules is 2 to the K. OK, so we have one at 1182 and when it operating at 4 the other change I mentioned between this sort of system in a standard.",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural network, or that there's limited connectivity?",
                    "label": 0
                },
                {
                    "sent": "So essentially the fast units are connected to each other and receive input from all of the other modules.",
                    "label": 0
                },
                {
                    "sent": "But the any type of unit only receives input from slower units.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not full connectivity, and that's the reason why the number of parameters is reduced in this module.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to do an update.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know compared to the STM we write the update rules.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look that much different than the standard RNN.",
                    "label": 0
                },
                {
                    "sent": "In fact, the update to terms of producing the output is the exact same.",
                    "label": 0
                },
                {
                    "sent": "What changes is how you actually update your hidden units, and So what you've done now is you've divided your hidden units into blocks.",
                    "label": 0
                },
                {
                    "sent": "I said these blocks.",
                    "label": 0
                },
                {
                    "sent": "These reference the different clocks and so you update only if you are.",
                    "label": 0
                },
                {
                    "sent": "If you essentially take your time step.",
                    "label": 0
                },
                {
                    "sent": "You might it with the particular Clock you're operating it, and that gives 0.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you just keep your state maintained from the previous steps.",
                    "label": 0
                },
                {
                    "sent": "OK, so it really has kind of a flavor of LS teams and in the way that information can be sort of kept over long periods of time.",
                    "label": 0
                },
                {
                    "sent": "It's just that the LTM has essentially more complexity and a little bit more control of the sort of the propagation of this information, and then forgetting of this information.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In training or so, usually when I say complexity, I mean the number of parameters and the network, but also sort of architecturally how it's been formed.",
                    "label": 0
                },
                {
                    "sent": "Correct, yes.",
                    "label": 0
                },
                {
                    "sent": "So again by reducing the number of parameters and simplifying in terms of the number of parameters, this network is less susceptible to overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to give we gave these update rules in terms of the math just to give a visual depiction.",
                    "label": 0
                },
                {
                    "sent": "This sort of diagram was used by the original authors of how this network operates, so this gives an example at time step six, OK, we've selected N = 2 as I mentioned before, which means that the clocks are operating at sort of one time step of a period of 1, two and four and eight and so forth.",
                    "label": 0
                },
                {
                    "sent": "And So what happens at time Step 6 is that only the modules that have.",
                    "label": 0
                },
                {
                    "sent": "K = 1 and our end of the K = 1 end of the K = 2 get updated.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you see in the structure of this you matrix, which is the matrix that connects hidden units to hidden units, is that these these fast units are basically connected to all of the other modules and they take updates from everybody and these units operating time Step 2 are only receiving input from the modules that are slower than them.",
                    "label": 0
                },
                {
                    "sent": "OK, and again we have grey here.",
                    "label": 0
                },
                {
                    "sent": "To denote in terms of the output units that are not being updated at this time, Step 6.",
                    "label": 0
                },
                {
                    "sent": "And then we also have Gray in the parameters, meaning that at this time step 6 these parameters are not being updated because they're not affecting the output.",
                    "label": 0
                },
                {
                    "sent": "We also have some especially structure in the weights as well, so again the the weights that connect to the input also are being sort of organized by rows corresponding to modules, and affectively only the weights corresponding to module one and module 2.",
                    "label": 0
                },
                {
                    "sent": "Are being applied to the input to compute the updates for these groups of modules.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's actually much cheaper computationally to perform an update in this sort of model due to the sparsity.",
                    "label": 0
                },
                {
                    "sent": "Then it is to apply an update and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Current neural network.",
                    "label": 0
                },
                {
                    "sent": "OK, so some of the problems associated with these clockwork RNN's ascentia Lee, you have these very slow units which are inactive over very long periods of time.",
                    "label": 1
                },
                {
                    "sent": "Therefore as a result there by nature kind of undertrained.",
                    "label": 0
                },
                {
                    "sent": "OK, so we saw before that we have these these blocks of the weight matrices that are not being updated at certain time steps, so the very slow units don't get as much training information as the faster units and due to this reason.",
                    "label": 0
                },
                {
                    "sent": "Essentially, given a fixed amount of training steps, these slow units barely contributed to the prediction made by the network.",
                    "label": 0
                },
                {
                    "sent": "The other main problem experienced by these clockwork RN ends are that their shift variant.",
                    "label": 0
                },
                {
                    "sent": "So essentially because the Clock rate is tide to the actual time step, if you just shift the input a little bit, that's going to cause the hidden activities and therefore the outputs to be quite different.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have a picture of this where we've essentially taken a data set of inertial measurement.",
                    "label": 0
                },
                {
                    "sent": "So these are people using a phone.",
                    "label": 0
                },
                {
                    "sent": "I'll talk more about the application a little bit later, but we have some inertial measurements from a phone.",
                    "label": 0
                },
                {
                    "sent": "Someone performing different activities.",
                    "label": 0
                },
                {
                    "sent": "So on top it's sort of reading and walking on the bottom, it's writing and sitting and we take the original sequence of these activities.",
                    "label": 0
                },
                {
                    "sent": "We apply this to an RNN that's been trained so the original vanilla RNN, and we read out the trace.",
                    "label": 1
                },
                {
                    "sent": "Of hidden units for different sequences, so the different colors here represent sequences that have been just shifted by one time step, and we measure the hidden unit traces on these shifted sequences.",
                    "label": 0
                },
                {
                    "sent": "And then we just shift them back accordingly and align them temporarily.",
                    "label": 0
                },
                {
                    "sent": "So we see that for the vanilla RNN, counting for the shift that happened, we sort of shift all the hidden unit traces back to the beginning.",
                    "label": 0
                },
                {
                    "sent": "They're all aligned essentially, but for the clockwork RNN, even after we do this shift, we have wildly different.",
                    "label": 0
                },
                {
                    "sent": "Traces on this particular hidden unit, so you see this shift variance yes.",
                    "label": 0
                },
                {
                    "sent": "The reduction in the Paris peace.",
                    "label": 0
                },
                {
                    "sent": "Also that Indian.",
                    "label": 0
                },
                {
                    "sent": "Something in terms of learning identity.",
                    "label": 0
                },
                {
                    "sent": "It's both OK, so computationally you reduce number of parameters.",
                    "label": 0
                },
                {
                    "sent": "The updates are faster, you can control overfitting, but you also have sort of explicitly incorporated sort of multiscale temporal dynamics into the network, and that can be advantageous from a modeling perspective.",
                    "label": 1
                },
                {
                    "sent": "Helpful, thanks.",
                    "label": 0
                },
                {
                    "sent": "That's right, yes.",
                    "label": 0
                },
                {
                    "sent": "So you could also do that sort of on a neuron by neuron basis as well as opposed to grouping them into these modules.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in terms of addressing these properties we've proposed is a variant of the Clockwork RNN, which we call the dense clockwork RNN, and the simple idea here is to just actually replicate these units at the time steps where they were not included in the seat.",
                    "label": 0
                },
                {
                    "sent": "In the clockwork RNN.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to share parameters between these different units, but essentially have units that update at particular clocks or or focus on receiving updates.",
                    "label": 0
                },
                {
                    "sent": "According to a certain Clock cycle, but actually continuously be active.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is, again, there's no change in the fastest group of units, the same as the clockwork RNN, but essentially the next group of units that update given two time steps ago, they will continue to update every time step, but they'll always look back to frames, essentially to receive updates similar to these fastest units.",
                    "label": 0
                },
                {
                    "sent": "Up here will always look back 4 frames, perform an update.",
                    "label": 0
                },
                {
                    "sent": "But they're essentially copied, and so they will indeed perform updates at every step.",
                    "label": 0
                },
                {
                    "sent": "OK, so the actual the connectivity becomes a little bit more complicated in terms of drawing it out here, but the idea of only receiving updates from the modules with slower Clock still exists right here.",
                    "label": 0
                },
                {
                    "sent": "So again, because of the parameter sharing, we're actually not increasing the number of parameters relative to the standard clockwork RNN, but we gain a shift invariant model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, you can just recall this picture I showed before in terms of performing the updates with the Clockwork RNN.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It changes a little bit when we look at the dense Clock works and so the reason why we write this out here is just sort of formulate mathematically a definition of the updates.",
                    "label": 0
                },
                {
                    "sent": "This is done in terms of formulating a essentially a structured matrix which represents the connectivity.",
                    "label": 0
                },
                {
                    "sent": "Again, the idea that only units receive updates from slower modules receive updates from slower modules.",
                    "label": 0
                },
                {
                    "sent": "We also have this H matrix.",
                    "label": 0
                },
                {
                    "sent": "So before we had just the vector of hidden activities, now what we've had to do to formulate this?",
                    "label": 0
                },
                {
                    "sent": "Is maintain a history of hidden activations?",
                    "label": 0
                },
                {
                    "sent": "OK, so we formed this matrix of histories and then we compute the matrix product of these and then we essentially this operator here is essentially operated that takes the diagonal of the result and formulates a vector from that.",
                    "label": 0
                },
                {
                    "sent": "Now there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "There's there's, you know when we take the diagonal, we're obviously throwing away a lot of computation that we just did so in practice we wouldn't actually do that.",
                    "label": 0
                },
                {
                    "sent": "This is just sort of a way of writing this out mathematically, so in practice you would only actually compute.",
                    "label": 0
                },
                {
                    "sent": "The updates that you actually actually need.",
                    "label": 0
                },
                {
                    "sent": "OK, so the downside.",
                    "label": 0
                },
                {
                    "sent": "The last thing I want to say about this.",
                    "label": 0
                },
                {
                    "sent": "The downside is that you now have to maintain a short history depending on the number of different modules you have of what happened, so you have a little bit of an extra memory gain, but in terms of computationally in terms of the number of parameters as well, there's really no difference with the standard clockwork module.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we we test this on the same example you saw before with the regular Clock works on these two different traces corresponding to different types of inputs.",
                    "label": 0
                },
                {
                    "sent": "You'll see the Clock works being shift variant whereas the dense Clock works.",
                    "label": 0
                },
                {
                    "sent": "Now we see same with the villanelle RNN, their shift invariant?",
                    "label": 0
                },
                {
                    "sent": "Let's sort the property that we want to see.",
                    "label": 0
                },
                {
                    "sent": "We'll compare them a little bit later on actual application.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think we have just a few couple minutes left, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so just very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of the application that we.",
                    "label": 0
                },
                {
                    "sent": "Proposed for these these modules and develop them in the context of this.",
                    "label": 0
                },
                {
                    "sent": "This is actually done as I mentioned before with Google.",
                    "label": 0
                },
                {
                    "sent": "The idea of the fact that entering a pin or passcode or some sort of password on your mobile device is really annoying.",
                    "label": 0
                },
                {
                    "sent": "It takes time.",
                    "label": 0
                },
                {
                    "sent": "It's also not completely secure, so the idea is can we do what's called continuous authentication?",
                    "label": 0
                },
                {
                    "sent": "So can your phone be constantly aware of who has it.",
                    "label": 0
                },
                {
                    "sent": "So if you're using your phone, you type a certain way, you're in certain locations.",
                    "label": 0
                },
                {
                    "sent": "It has a camera.",
                    "label": 0
                },
                {
                    "sent": "I can see your face.",
                    "label": 0
                },
                {
                    "sent": "It knows that you're using it and it unlocks.",
                    "label": 0
                },
                {
                    "sent": "If someone else grabs your phone, it will lock.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically we carried out actually the largest cell phone based biometric study with 1500 volunteers who all received the identical phone, which is a Nexus 5 that had 13 sensors and they perform several months of sort of natural usage with these phones and we collected all of the sensor data they signed up for this.",
                    "label": 1
                },
                {
                    "sent": "They agreed to share all of their sensor data with the project, and so the various sensors include camera, GPS, everything that we've been typing so we can look at keystroke patterns.",
                    "label": 0
                },
                {
                    "sent": "Where they've been, as well as the accelerometer and the gyroscope.",
                    "label": 0
                },
                {
                    "sent": "So the way this project works is different teams worked on different modalities and essentially our team focused only on the accelerometer and gyroscope.",
                    "label": 0
                },
                {
                    "sent": "Now these modalities on their own are actually not that useful for authenticating users.",
                    "label": 0
                },
                {
                    "sent": "They're actually very weak biometric.",
                    "label": 0
                },
                {
                    "sent": "But when you combine them with these other modalities, you actually get something that is much stronger than pins or passcodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so we don't have a ton of time.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not going to go into the details of the project Ascentia Lee.",
                    "label": 0
                },
                {
                    "sent": "I mentioned this idea of convolutional neural networks, recurrent neural networks and so forth.",
                    "label": 0
                },
                {
                    "sent": "These were used as feature extractors for a generative model that actually did user authentication.",
                    "label": 0
                },
                {
                    "sent": "So the key thing here was that we couldn't actually train the convolutional.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Networks or recurrent neural networks to do discrimination on the phone itself because we're limited, sort of computationally and we couldn't do it on the cloud either because we can't send people's private data over to the cloud to enroll them in the program.",
                    "label": 0
                },
                {
                    "sent": "So we essentially did a technique.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we fit a generative model to a large user base, we push that to the phone and then on the phone we would essentially just do a slight adaptation of the generative model from what we call the universal background model of everybody's data towards that client.",
                    "label": 0
                },
                {
                    "sent": "And so the main result here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to just skip over a little bit is to compare different ways of doing feature extraction, so either with using convolutional networks that are trained on fairly long sequences.",
                    "label": 0
                },
                {
                    "sent": "So on the order of 10 seconds per sequence to RNN type models which are trained on shorter sequences.",
                    "label": 0
                },
                {
                    "sent": "So sequences of roughly one second.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results are that essentially, you know, on a purely class appear classification classification task.",
                    "label": 0
                },
                {
                    "sent": "Looking at just the various means of a feature extraction, these dense Clock works actually perform the best, outperforming the convolutional networks and the other.",
                    "label": 0
                },
                {
                    "sent": "The other point here is that these these RNS, which are actually able to sort of explicitly model temporal contexts, tend to do better than the comnets which implicitly model it via the convolutional operation.",
                    "label": 0
                },
                {
                    "sent": "But don't explicitly model it.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apart from just looking at the future extraction alone and how it performs on the classification tasks, this is on the full biometric setup where we had basically held out users that had that we've never seen before.",
                    "label": 0
                },
                {
                    "sent": "We do this again as I described this very simple adaptation of this universal background model, given maybe a week or so of them interacting with the device, and then we measured the ability of the model to authenticate them, and So what you're seeing here is what's called equal error rate, so an equal error rate of.",
                    "label": 0
                },
                {
                    "sent": "20% would mean that sort of 20% of 20% of the time a user would be a false user would be authenticated as the correct user and also 80% of the time you would be using your phone and would actually recognize it.",
                    "label": 0
                },
                {
                    "sent": "It's you.",
                    "label": 0
                },
                {
                    "sent": "So it goes both ways.",
                    "label": 0
                },
                {
                    "sent": "That's why they call it equal error rate.",
                    "label": 0
                },
                {
                    "sent": "And then we also have.",
                    "label": 0
                },
                {
                    "sent": "This is measured on the validation set and the common way in this domain to measure error is sort of called this.",
                    "label": 0
                },
                {
                    "sent": "This half total error rate, which is essentially a blend between the false acceptance rate equal between false acceptance rate and false rejection rate, and again we see that the these dense Clock words are outperforming the other convolutional methods as well as the standard recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I'm going to conclude.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here and just essentially say that these convolutional neural networks in recurrent neural networks have been obviously very popular in various applications, but they're heavily engineered models and it's nice to be able to turn to other other areas like the ones we've been talking about today towards systems that can be equally powerful, powerful but less engineered to particular domains.",
                    "label": 0
                },
                {
                    "sent": "So we presented a new variant of the clockwork RN ends which are shift invariant and also have favorable computational properties.",
                    "label": 0
                },
                {
                    "sent": "Anne works essentially exploring these sorts of models on different types of modalities, beyond accelerated accelerometers and gyroscopes.",
                    "label": 0
                },
                {
                    "sent": "And then we're also investigating other types of these neural models and other variants of these dense Clock works on other applications like gesture recognition, activity recognition.",
                    "label": 1
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for your attention and will take questions.",
                    "label": 0
                }
            ]
        }
    }
}