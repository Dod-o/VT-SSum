{
    "id": "yxvdicqzy4uw6bjsdodltgcjmohxw7ed",
    "title": "Domain-Independent Abstract Generation for Focused Meeting Summarization",
    "info": {
        "author": [
            "Lu Wang, Department of Computer Science, Cornell University"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_wang_summarization/",
    "segmentation": [
        [
            "So now I'm going to talk about another work with Professor Clark Claire Cardie, domain independent abstract generation for focused meeting summarization."
        ],
        [
            "Meetings play are very important role in our daily life for collaboration and information sharing."
        ],
        [
            "However, it will be very tedious to read the whole meeting transcript to get the key out of it.",
            "So we need some automatic meeting summarization technique to quickly access the the essential content and for the managers they also can use it to track the project the project progress."
        ],
        [
            "Where specifically interested in this set of problem called focused meeting summarization which is to generate some Rays only for some specific important out food of the meeting.",
            "Like decisions made action items arise.",
            "Problem discussed, progress made."
        ],
        [
            "Here's an example on the left hand side is a clip from the am I meeting corpus and we also show on the right hand side is a decision summary constructed by human annotator.",
            "You can see from this example that any extract from this clip will be likely to contain noisy information redundancy.",
            "But if you look at human constructed summary, the human annotator tend to gather Geist information 1st and rewrite it in a non conversational abstract style."
        ],
        [
            "Existing meeting summarization systems are mostly extractive or a bunch of some supervised learning approaches have been proposed other for utterance level or first level summarization.",
            "However, worry at all showed that users definitely prefer the abstract style summaries over the extracts."
        ],
        [
            "So our goal in this work is to, given a set of a cluster of dialogue acts, we would like to produce a short, non conversational abstract style summary."
        ],
        [
            "Our contributions include we.",
            "We first propose this fully automatic domain independent abstract generation framework for focused meeting summarization and then we rely on task specific templates to get our abstract generation process and we will present a novel template instruction algorithm with multiple sequence alignment.",
            "Finally, we use an over generated Anna rank strategy.",
            "For surface realization."
        ],
        [
            "Our work is related to those work beyond extracting meeting summarization.",
            "Murray at all present an abstractions system.",
            "They first map those utterances to the corresponding summary type, an extractor ones with most entities.",
            "But this system is still extractive in nature, and sentence compression has also been used to Java redundant words by Leo and Leo in 2009."
        ],
        [
            "Our work is also inspired by content concept to text generation.",
            "The generation process is is your only decompose into content selection.",
            "Anna surface related surface realization and in the training phase it usually takes us input.",
            "A set of structure, database records and Prolog.",
            "Textual description for example and list at all.",
            "They generate text based on the decisions made to select the records fields and proper templates.",
            "However the templates they learn here are domain specific, which means it cannot be used.",
            "In a new domain."
        ],
        [
            "Here's the framework of our work.",
            "We firstly use relation extraction technique to extract a bunch of summary or see relation instances.",
            "An in surface realization we fill those relation instances into automatically learn templates.",
            "Then we use the statistical rancor to select one best abstract for each relation instance.",
            "The post processing will remove the redundancy and generate a short final summary.",
            "Now I'm going to talk about account selection first."
        ],
        [
            "We first define the relation instance as indicator argument pair, where indicator evokes a relation, an argument is the target.",
            "I highlight two relation instances in the example pair.",
            "Anne.",
            "We also show a human return summary.",
            "And you can see that the human annotator reuse the relation instances or part of them to construct this abstract summary."
        ],
        [
            "So we we require our relation instance candidates to be having the indicator of none or verb argument has to be a noun phrase, prepositional phrase, or objective or phrase.",
            "We further constrain the based on the dependency relation.",
            "For example, if the indicator is among the argument has to be the modifier or complement of the indicator."
        ],
        [
            "Then we're training a binary classifier based on support vector machine to predict whether this relation instances summary worthy or not.",
            "Simple features included TF IDF scores.",
            "We also consider the discourse features such as whether it is relation, instance is uttered bioman, speaker or is shared by a fire."
        ],
        [
            "Before we talk about the surface realization process, I would like to present our novel algorithm to induce the summary of.",
            "To induce the summary templates."
        ],
        [
            "The core idea of this summary templates extraction is based on multiple sequence alignment.",
            "I might say is commonly used in bioinformatics, but currently it has been useful paraphrase learning."
        ],
        [
            "Here's an example given for sequences as one to ask for.",
            "We are our goal is to find best alignment between this this force and this for sequence is what we need is a scorer.",
            "It takes us input two items.",
            "If these two items are the same, it gets a score one if the item is matched to insert it slot, it gets a score 0 if there's a mismatching issue, be penalized."
        ],
        [
            "Before running this MSA terrorism, I would like to cluster those sentences or the abstract sentences in the training data or according to their lexical or structural similarity.",
            "Be aware that we don't want those sentences to be clustered becausw.",
            "They're describing the same domain specific content.",
            "For example, natural language processing or machine learning, so we will first replace all the periods of dates, numbers, and proper names with generic semantic labels.",
            "Then we further replace sequences of words that appear in both abstract and supporting dialogue X.",
            "By a label indicating the first step.",
            "After this generic label replacement process, we apply hierarchical clustering from Bud Leanne later than three to cluster those sentences based on a word engram overlap."
        ],
        [
            "Here's an example for the generic label replacement.",
            "Given a set of abstracts shown above.",
            "We identified the."
        ],
        [
            "Classes of words show both in the abstract and dialogue acts.",
            "And then we replace the."
        ],
        [
            "Sequence waste the corresponding phrase label."
        ],
        [
            "Now we can run our MSA.",
            "However, computing optimal Msas and be complete, so we implemented with an approximate algorithm which is we iteratively aligns to sequence and using the resulting alarming as the new sequence."
        ],
        [
            "After Reading Embassy we can get a lattice shown here.",
            "Now we can define the backbone nodes as the tokens shared by more than 50% of the sentences.",
            "Those backbone were those backbone nodes."
        ],
        [
            "Will be used for template induction.",
            "Specifically, I returned the lexical form of raised label from the bank bundle.",
            "Which are shaded here an we abstract for the non backbone nodes.",
            "We will abstract them out by replacing them ways there.",
            "Phrase type label and the result will be shown.",
            "Here."
        ],
        [
            "Now with those automatically learned templates, we can use it in our Surface realization pipeline."
        ],
        [
            "The code I dear of our surface realization component relies on this over generate an rank strategy.",
            "Which has been used for census planning and question generation."
        ],
        [
            "In the temple feeling, we first represent a template with their parsing tree like this.",
            "And then we will fill those lots with our relation instances get from the content selection component."
        ],
        [
            "Before that we would like to define three types of constant level transformation of the relation instances."
        ],
        [
            "For example, we have this template and here comes a relation instance with the indicator as want and argument as an LCD display with spinning well."
        ],
        [
            "First operation is called full constant mapping.",
            "We just directly mapped indicator an argument to the noun phrase to the verb phrase and noun phrase."
        ],
        [
            "A second operation is called Subconsultant Mapping, where we can remove part of the constant node.",
            "In this case we just remove a prepositional phrase modifier from our argument."
        ],
        [
            "Removal is simply just remove the whole incident.",
            "In this case we remove our indicator.",
            "This is very likely less likely to happen in the object generation process."
        ],
        [
            "After Temple fully we will get a list of possible abstracts for each relation instance."
        ],
        [
            "Then we train us to discover anchor based on support vector regression to get one best abstract for each relation instance.",
            "But features we try to use to measure if this generated object has a verb or whether it starts with verb.",
            "We also use language model features where we train our language model based on the Greek word and we measure the probability of this generous abstract based on this language model."
        ],
        [
            "The last step will be redundancy handling, where we just simply use a greedy algorithm from linnan bells to remove the to minimize the redundancy of the summary and output it as the final final output."
        ],
        [
            "We evaluate our algorithms on two disparate corpora.",
            "One is called am I meeting corpus.",
            "It has 139 scenario driven meetings and it also contains abstracts for decisiones, action items and problems.",
            "We also evaluated on taxi meeting corpus actually meeting cops is kind of different because it contains naturally occurring meetings which is will be much more difficult to summarize.",
            "It contains objects for decissions progress and problems."
        ],
        [
            "So firstly we would like an isolated content selection part from the whole system.",
            "We will have to evaluate this.",
            "See if our system can get this information from the given cluster.",
            "We use root, which is a standard summarization metric and compared with two unsupervised baselines, one is the longest utterance from the cluster and the other one is the centroid utterance.",
            "We also compared with state of the art, supervised learning based approach.",
            "It is both an utterance level and the total token level summarization."
        ],
        [
            "So here's a result for the content selection component for action item summarization.",
            "It runs on.",
            "They are my corpus.",
            "You can see the yellow line in the top is Oracle, which consists of the words from the witch show, both in the abstract and supporting dialogue X.",
            "And then it comes our system, which is the green green line.",
            "So we can see that by changing the number of meetings in our training data, our system can uniformly outperform other compared system here."
        ],
        [
            "And this result can also be observed on other tasks.",
            "For example, this is content selection for decision summary on the AM I meeting corpus."
        ],
        [
            "We also tested on the decision summary on the axis of us, and we observed the same thing."
        ],
        [
            "Now we can evaluate our full system.",
            "We use blue, which is designed for machine translation task, but it has also be used for evaluating all language generation systems.",
            "We also ask for different human judges to evaluate the fluency, semantic correctness and overall quality of our system summary."
        ],
        [
            "Here's the result on a full system evaluation.",
            "Again, the top line here is the article and the next Green Blue line is our system.",
            "We can see from this figure that our systems performance can get increased by increasing the number of meetings in the training data and with enough number of meetings in the training sample, our system can significantly outperform other compare system."
        ],
        [
            "And this has also been observed for decision summary on EMI problems.",
            "Ano"
        ],
        [
            "Salon X equals."
        ],
        [
            "Here are some other interesting result we would like to show here, where we treat the system on one corpus and test it on the other domain for example."
        ],
        [
            "The first column we train, our exit corpus and test it on a on my decision summarization.",
            "We compared our system with SVM based approach which is trained on in domain data and you can see.",
            "Our system other training on indoor my data, all of domain data can both out outperform our compared system.",
            "And specifically, our system trained on out of domain data can produce comparable results with the system trained on in domain data.",
            "Then"
        ],
        [
            "We ask the human judges to evaluate the fluency semantic correctness of our system compared to the SVM based system, and we can see that human justice judges rate our system higher in the fluency and also our system can produce much shorter summaries than the compared system.",
            "There is also select our system as the best system in more than 62% of scenarios."
        ],
        [
            "Here's a sample summary we have seen this example throughout this presentation and on the right hand side we show our system generated summaries in blue.",
            "And.",
            "So from here you can see that our system generates some Rays can not only get the gist information from the original meeting clip and also remove the redundancy.",
            "And they're fluent as well."
        ],
        [
            "To conclude, in this talk we present a domain independent abstract generation framework for focused meeting summarization and we use the experimental results show that our system can uniformly outperform this data state of the art systems and our system also exhibit an ability to train on auto domain data to generate abstracts for GNU target domain."
        ],
        [
            "Thank you.",
            "I hope I didn't miss it at the input of the MSA, multiple sequence alignment is the sentence structures right?",
            "So the input of MSA to sentence structures.",
            "So after you align those sequences, how do you define which where is the beginning elements of the pattern?",
            "Where?",
            "How do you define the end of the pattern?",
            "So we're going to talk a piece of the.",
            "Leidos sequences so.",
            "We use the original sentence as the sequence.",
            "Each word will be a sequence and after we replace those tokens with generic label, we apply this.",
            "I'm at the algorithm and So what do you mean by start of it?",
            "I think the startup just say.",
            "If taking this like Backbone knows as example the start up, it will be just that this group.",
            "We're not sure whether to just OK. Alright, thank you very much.",
            "Yeah, related question, which is to this.",
            "I thought you were doing single document summarization of a single meeting's input.",
            "Is that correct?",
            "The input actually is the summary related there, like already is the what is so.",
            "The input will be just.",
            "Is now the full meeting.",
            "It's just the summary related dialogue acts, which means we already know that a decision will be made out of this clip.",
            "Well, I guess what I'm wondering is where are you getting the multiple sentences which have the similar pieces?",
            "You're talking about the MSA process, right?",
            "Yeah, OK, the input to the MSA.",
            "I apply a clustering process before that.",
            "So in this step the we replace it with those generic labels.",
            "Yeah, stand that.",
            "But how are you getting if you're doing single document summarization, you have four sentences with a repeated very similar text.",
            "I don't see on the example that you showed that you get that.",
            "Normally you would get that if your input was coming from multiple documents.",
            "So would you mind going ahead?",
            "I'm sorry to you're talking about how can we gather similar."
        ],
        [
            "This here yes.",
            "Oh, so here's the.",
            "So here's the special thing for am I meeting composed an extra meeting covers for?",
            "Am I the same scenario will be taken place in from different groups of people.",
            "Say we have ten groups of people.",
            "They will contact the same projects.",
            "They have the same goal, so the summaries will be similar to each other but but we don't like training on like a portion of it and the other for a certain type of like meetings.",
            "We will put them all Internet or the test data.",
            "OK, yeah.",
            "So these corpora originally are recordings of these simulated meetings.",
            "Did you and you run on the manual transcripts you experiments?",
            "Did you ever run experiments on the original ASR output to see how much you lose?",
            "We didn't do that, but yeah, we can try it and see how it works because it's just like initially like experiments on abstractive summarization.",
            "So we don't want to do that at this moment.",
            "Yeah, thank you.",
            "Are there any other questions?",
            "Barber I notice one of your features was main speaker.",
            "Is that annotated on the data, and if not, how did you find who the main speaker is?",
            "We set a threshold like this person say uttered 30% of the utterances in the meeting.",
            "We will say, OK, this is the most bigger yes, not manually annotated.",
            "Thank you.",
            "I have a question on multiple sequence alignment.",
            "Sometimes doesn't work well if you have a a baby in one input and then BBA.",
            "In another input.",
            "Example yesterday with it X versus wooded X yesterday.",
            "Have you had problems with this kind of?",
            "The sentence is, oh, we we didn't observe it in the data, but that's a good question.",
            "But we can.",
            "We can extend it by using like more sophisticated like similarity metric, right?",
            "Because.",
            "Kindly just like word ngram.",
            "But we can extend it by using semantic similarity.",
            "OK, thank you any other questions?",
            "Yeah, last question.",
            "So just for clarification, you mentioned answering Cathy's question that you already given the utterances which are relevant to the summary.",
            "Do I remember correctly that this is actually manually annotated so you don't need to run a classifier for this?",
            "Or did you in your experiments, run a classifier to 1st detect these utterances?",
            "So we have two experimental setup.",
            "I didn't go into details here.",
            "Firstly, will assume we know like all those utterances related to a certain say summary.",
            "This is called true clustering of it, and the other expander setup is we don't assume that we will use our previous work to cluster those together and got those dialogue Class dialogue act clusters and generate some right out of it.",
            "Yes, so in your results here, did I miss that you show both of these conditions or?",
            "I didn't, but the similar trends will hold for the system generated clusters.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about another work with Professor Clark Claire Cardie, domain independent abstract generation for focused meeting summarization.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meetings play are very important role in our daily life for collaboration and information sharing.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, it will be very tedious to read the whole meeting transcript to get the key out of it.",
                    "label": 0
                },
                {
                    "sent": "So we need some automatic meeting summarization technique to quickly access the the essential content and for the managers they also can use it to track the project the project progress.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where specifically interested in this set of problem called focused meeting summarization which is to generate some Rays only for some specific important out food of the meeting.",
                    "label": 1
                },
                {
                    "sent": "Like decisions made action items arise.",
                    "label": 0
                },
                {
                    "sent": "Problem discussed, progress made.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an example on the left hand side is a clip from the am I meeting corpus and we also show on the right hand side is a decision summary constructed by human annotator.",
                    "label": 1
                },
                {
                    "sent": "You can see from this example that any extract from this clip will be likely to contain noisy information redundancy.",
                    "label": 1
                },
                {
                    "sent": "But if you look at human constructed summary, the human annotator tend to gather Geist information 1st and rewrite it in a non conversational abstract style.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Existing meeting summarization systems are mostly extractive or a bunch of some supervised learning approaches have been proposed other for utterance level or first level summarization.",
                    "label": 0
                },
                {
                    "sent": "However, worry at all showed that users definitely prefer the abstract style summaries over the extracts.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal in this work is to, given a set of a cluster of dialogue acts, we would like to produce a short, non conversational abstract style summary.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our contributions include we.",
                    "label": 0
                },
                {
                    "sent": "We first propose this fully automatic domain independent abstract generation framework for focused meeting summarization and then we rely on task specific templates to get our abstract generation process and we will present a novel template instruction algorithm with multiple sequence alignment.",
                    "label": 1
                },
                {
                    "sent": "Finally, we use an over generated Anna rank strategy.",
                    "label": 0
                },
                {
                    "sent": "For surface realization.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our work is related to those work beyond extracting meeting summarization.",
                    "label": 1
                },
                {
                    "sent": "Murray at all present an abstractions system.",
                    "label": 0
                },
                {
                    "sent": "They first map those utterances to the corresponding summary type, an extractor ones with most entities.",
                    "label": 1
                },
                {
                    "sent": "But this system is still extractive in nature, and sentence compression has also been used to Java redundant words by Leo and Leo in 2009.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our work is also inspired by content concept to text generation.",
                    "label": 0
                },
                {
                    "sent": "The generation process is is your only decompose into content selection.",
                    "label": 1
                },
                {
                    "sent": "Anna surface related surface realization and in the training phase it usually takes us input.",
                    "label": 0
                },
                {
                    "sent": "A set of structure, database records and Prolog.",
                    "label": 0
                },
                {
                    "sent": "Textual description for example and list at all.",
                    "label": 0
                },
                {
                    "sent": "They generate text based on the decisions made to select the records fields and proper templates.",
                    "label": 1
                },
                {
                    "sent": "However the templates they learn here are domain specific, which means it cannot be used.",
                    "label": 0
                },
                {
                    "sent": "In a new domain.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the framework of our work.",
                    "label": 0
                },
                {
                    "sent": "We firstly use relation extraction technique to extract a bunch of summary or see relation instances.",
                    "label": 1
                },
                {
                    "sent": "An in surface realization we fill those relation instances into automatically learn templates.",
                    "label": 1
                },
                {
                    "sent": "Then we use the statistical rancor to select one best abstract for each relation instance.",
                    "label": 1
                },
                {
                    "sent": "The post processing will remove the redundancy and generate a short final summary.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk about account selection first.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We first define the relation instance as indicator argument pair, where indicator evokes a relation, an argument is the target.",
                    "label": 1
                },
                {
                    "sent": "I highlight two relation instances in the example pair.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We also show a human return summary.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the human annotator reuse the relation instances or part of them to construct this abstract summary.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we we require our relation instance candidates to be having the indicator of none or verb argument has to be a noun phrase, prepositional phrase, or objective or phrase.",
                    "label": 1
                },
                {
                    "sent": "We further constrain the based on the dependency relation.",
                    "label": 0
                },
                {
                    "sent": "For example, if the indicator is among the argument has to be the modifier or complement of the indicator.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we're training a binary classifier based on support vector machine to predict whether this relation instances summary worthy or not.",
                    "label": 1
                },
                {
                    "sent": "Simple features included TF IDF scores.",
                    "label": 1
                },
                {
                    "sent": "We also consider the discourse features such as whether it is relation, instance is uttered bioman, speaker or is shared by a fire.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we talk about the surface realization process, I would like to present our novel algorithm to induce the summary of.",
                    "label": 0
                },
                {
                    "sent": "To induce the summary templates.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The core idea of this summary templates extraction is based on multiple sequence alignment.",
                    "label": 0
                },
                {
                    "sent": "I might say is commonly used in bioinformatics, but currently it has been useful paraphrase learning.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example given for sequences as one to ask for.",
                    "label": 0
                },
                {
                    "sent": "We are our goal is to find best alignment between this this force and this for sequence is what we need is a scorer.",
                    "label": 0
                },
                {
                    "sent": "It takes us input two items.",
                    "label": 0
                },
                {
                    "sent": "If these two items are the same, it gets a score one if the item is matched to insert it slot, it gets a score 0 if there's a mismatching issue, be penalized.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before running this MSA terrorism, I would like to cluster those sentences or the abstract sentences in the training data or according to their lexical or structural similarity.",
                    "label": 1
                },
                {
                    "sent": "Be aware that we don't want those sentences to be clustered becausw.",
                    "label": 0
                },
                {
                    "sent": "They're describing the same domain specific content.",
                    "label": 1
                },
                {
                    "sent": "For example, natural language processing or machine learning, so we will first replace all the periods of dates, numbers, and proper names with generic semantic labels.",
                    "label": 1
                },
                {
                    "sent": "Then we further replace sequences of words that appear in both abstract and supporting dialogue X.",
                    "label": 0
                },
                {
                    "sent": "By a label indicating the first step.",
                    "label": 0
                },
                {
                    "sent": "After this generic label replacement process, we apply hierarchical clustering from Bud Leanne later than three to cluster those sentences based on a word engram overlap.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example for the generic label replacement.",
                    "label": 0
                },
                {
                    "sent": "Given a set of abstracts shown above.",
                    "label": 0
                },
                {
                    "sent": "We identified the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classes of words show both in the abstract and dialogue acts.",
                    "label": 0
                },
                {
                    "sent": "And then we replace the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sequence waste the corresponding phrase label.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can run our MSA.",
                    "label": 0
                },
                {
                    "sent": "However, computing optimal Msas and be complete, so we implemented with an approximate algorithm which is we iteratively aligns to sequence and using the resulting alarming as the new sequence.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After Reading Embassy we can get a lattice shown here.",
                    "label": 0
                },
                {
                    "sent": "Now we can define the backbone nodes as the tokens shared by more than 50% of the sentences.",
                    "label": 1
                },
                {
                    "sent": "Those backbone were those backbone nodes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be used for template induction.",
                    "label": 0
                },
                {
                    "sent": "Specifically, I returned the lexical form of raised label from the bank bundle.",
                    "label": 0
                },
                {
                    "sent": "Which are shaded here an we abstract for the non backbone nodes.",
                    "label": 0
                },
                {
                    "sent": "We will abstract them out by replacing them ways there.",
                    "label": 0
                },
                {
                    "sent": "Phrase type label and the result will be shown.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now with those automatically learned templates, we can use it in our Surface realization pipeline.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The code I dear of our surface realization component relies on this over generate an rank strategy.",
                    "label": 0
                },
                {
                    "sent": "Which has been used for census planning and question generation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the temple feeling, we first represent a template with their parsing tree like this.",
                    "label": 0
                },
                {
                    "sent": "And then we will fill those lots with our relation instances get from the content selection component.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before that we would like to define three types of constant level transformation of the relation instances.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, we have this template and here comes a relation instance with the indicator as want and argument as an LCD display with spinning well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First operation is called full constant mapping.",
                    "label": 0
                },
                {
                    "sent": "We just directly mapped indicator an argument to the noun phrase to the verb phrase and noun phrase.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A second operation is called Subconsultant Mapping, where we can remove part of the constant node.",
                    "label": 0
                },
                {
                    "sent": "In this case we just remove a prepositional phrase modifier from our argument.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Removal is simply just remove the whole incident.",
                    "label": 0
                },
                {
                    "sent": "In this case we remove our indicator.",
                    "label": 0
                },
                {
                    "sent": "This is very likely less likely to happen in the object generation process.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After Temple fully we will get a list of possible abstracts for each relation instance.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we train us to discover anchor based on support vector regression to get one best abstract for each relation instance.",
                    "label": 1
                },
                {
                    "sent": "But features we try to use to measure if this generated object has a verb or whether it starts with verb.",
                    "label": 0
                },
                {
                    "sent": "We also use language model features where we train our language model based on the Greek word and we measure the probability of this generous abstract based on this language model.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last step will be redundancy handling, where we just simply use a greedy algorithm from linnan bells to remove the to minimize the redundancy of the summary and output it as the final final output.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluate our algorithms on two disparate corpora.",
                    "label": 1
                },
                {
                    "sent": "One is called am I meeting corpus.",
                    "label": 0
                },
                {
                    "sent": "It has 139 scenario driven meetings and it also contains abstracts for decisiones, action items and problems.",
                    "label": 1
                },
                {
                    "sent": "We also evaluated on taxi meeting corpus actually meeting cops is kind of different because it contains naturally occurring meetings which is will be much more difficult to summarize.",
                    "label": 0
                },
                {
                    "sent": "It contains objects for decissions progress and problems.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So firstly we would like an isolated content selection part from the whole system.",
                    "label": 1
                },
                {
                    "sent": "We will have to evaluate this.",
                    "label": 0
                },
                {
                    "sent": "See if our system can get this information from the given cluster.",
                    "label": 0
                },
                {
                    "sent": "We use root, which is a standard summarization metric and compared with two unsupervised baselines, one is the longest utterance from the cluster and the other one is the centroid utterance.",
                    "label": 0
                },
                {
                    "sent": "We also compared with state of the art, supervised learning based approach.",
                    "label": 1
                },
                {
                    "sent": "It is both an utterance level and the total token level summarization.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a result for the content selection component for action item summarization.",
                    "label": 0
                },
                {
                    "sent": "It runs on.",
                    "label": 0
                },
                {
                    "sent": "They are my corpus.",
                    "label": 0
                },
                {
                    "sent": "You can see the yellow line in the top is Oracle, which consists of the words from the witch show, both in the abstract and supporting dialogue X.",
                    "label": 0
                },
                {
                    "sent": "And then it comes our system, which is the green green line.",
                    "label": 0
                },
                {
                    "sent": "So we can see that by changing the number of meetings in our training data, our system can uniformly outperform other compared system here.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this result can also be observed on other tasks.",
                    "label": 0
                },
                {
                    "sent": "For example, this is content selection for decision summary on the AM I meeting corpus.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also tested on the decision summary on the axis of us, and we observed the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can evaluate our full system.",
                    "label": 0
                },
                {
                    "sent": "We use blue, which is designed for machine translation task, but it has also be used for evaluating all language generation systems.",
                    "label": 1
                },
                {
                    "sent": "We also ask for different human judges to evaluate the fluency, semantic correctness and overall quality of our system summary.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the result on a full system evaluation.",
                    "label": 0
                },
                {
                    "sent": "Again, the top line here is the article and the next Green Blue line is our system.",
                    "label": 0
                },
                {
                    "sent": "We can see from this figure that our systems performance can get increased by increasing the number of meetings in the training data and with enough number of meetings in the training sample, our system can significantly outperform other compare system.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has also been observed for decision summary on EMI problems.",
                    "label": 0
                },
                {
                    "sent": "Ano",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Salon X equals.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some other interesting result we would like to show here, where we treat the system on one corpus and test it on the other domain for example.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first column we train, our exit corpus and test it on a on my decision summarization.",
                    "label": 0
                },
                {
                    "sent": "We compared our system with SVM based approach which is trained on in domain data and you can see.",
                    "label": 0
                },
                {
                    "sent": "Our system other training on indoor my data, all of domain data can both out outperform our compared system.",
                    "label": 0
                },
                {
                    "sent": "And specifically, our system trained on out of domain data can produce comparable results with the system trained on in domain data.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We ask the human judges to evaluate the fluency semantic correctness of our system compared to the SVM based system, and we can see that human justice judges rate our system higher in the fluency and also our system can produce much shorter summaries than the compared system.",
                    "label": 0
                },
                {
                    "sent": "There is also select our system as the best system in more than 62% of scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a sample summary we have seen this example throughout this presentation and on the right hand side we show our system generated summaries in blue.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So from here you can see that our system generates some Rays can not only get the gist information from the original meeting clip and also remove the redundancy.",
                    "label": 0
                },
                {
                    "sent": "And they're fluent as well.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude, in this talk we present a domain independent abstract generation framework for focused meeting summarization and we use the experimental results show that our system can uniformly outperform this data state of the art systems and our system also exhibit an ability to train on auto domain data to generate abstracts for GNU target domain.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I hope I didn't miss it at the input of the MSA, multiple sequence alignment is the sentence structures right?",
                    "label": 0
                },
                {
                    "sent": "So the input of MSA to sentence structures.",
                    "label": 0
                },
                {
                    "sent": "So after you align those sequences, how do you define which where is the beginning elements of the pattern?",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "How do you define the end of the pattern?",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk a piece of the.",
                    "label": 0
                },
                {
                    "sent": "Leidos sequences so.",
                    "label": 0
                },
                {
                    "sent": "We use the original sentence as the sequence.",
                    "label": 0
                },
                {
                    "sent": "Each word will be a sequence and after we replace those tokens with generic label, we apply this.",
                    "label": 0
                },
                {
                    "sent": "I'm at the algorithm and So what do you mean by start of it?",
                    "label": 0
                },
                {
                    "sent": "I think the startup just say.",
                    "label": 0
                },
                {
                    "sent": "If taking this like Backbone knows as example the start up, it will be just that this group.",
                    "label": 0
                },
                {
                    "sent": "We're not sure whether to just OK. Alright, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yeah, related question, which is to this.",
                    "label": 0
                },
                {
                    "sent": "I thought you were doing single document summarization of a single meeting's input.",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "The input actually is the summary related there, like already is the what is so.",
                    "label": 0
                },
                {
                    "sent": "The input will be just.",
                    "label": 0
                },
                {
                    "sent": "Is now the full meeting.",
                    "label": 0
                },
                {
                    "sent": "It's just the summary related dialogue acts, which means we already know that a decision will be made out of this clip.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess what I'm wondering is where are you getting the multiple sentences which have the similar pieces?",
                    "label": 0
                },
                {
                    "sent": "You're talking about the MSA process, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, the input to the MSA.",
                    "label": 0
                },
                {
                    "sent": "I apply a clustering process before that.",
                    "label": 0
                },
                {
                    "sent": "So in this step the we replace it with those generic labels.",
                    "label": 0
                },
                {
                    "sent": "Yeah, stand that.",
                    "label": 0
                },
                {
                    "sent": "But how are you getting if you're doing single document summarization, you have four sentences with a repeated very similar text.",
                    "label": 0
                },
                {
                    "sent": "I don't see on the example that you showed that you get that.",
                    "label": 0
                },
                {
                    "sent": "Normally you would get that if your input was coming from multiple documents.",
                    "label": 0
                },
                {
                    "sent": "So would you mind going ahead?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry to you're talking about how can we gather similar.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This here yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, so here's the.",
                    "label": 0
                },
                {
                    "sent": "So here's the special thing for am I meeting composed an extra meeting covers for?",
                    "label": 0
                },
                {
                    "sent": "Am I the same scenario will be taken place in from different groups of people.",
                    "label": 0
                },
                {
                    "sent": "Say we have ten groups of people.",
                    "label": 0
                },
                {
                    "sent": "They will contact the same projects.",
                    "label": 0
                },
                {
                    "sent": "They have the same goal, so the summaries will be similar to each other but but we don't like training on like a portion of it and the other for a certain type of like meetings.",
                    "label": 0
                },
                {
                    "sent": "We will put them all Internet or the test data.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "So these corpora originally are recordings of these simulated meetings.",
                    "label": 0
                },
                {
                    "sent": "Did you and you run on the manual transcripts you experiments?",
                    "label": 0
                },
                {
                    "sent": "Did you ever run experiments on the original ASR output to see how much you lose?",
                    "label": 0
                },
                {
                    "sent": "We didn't do that, but yeah, we can try it and see how it works because it's just like initially like experiments on abstractive summarization.",
                    "label": 0
                },
                {
                    "sent": "So we don't want to do that at this moment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "Barber I notice one of your features was main speaker.",
                    "label": 0
                },
                {
                    "sent": "Is that annotated on the data, and if not, how did you find who the main speaker is?",
                    "label": 0
                },
                {
                    "sent": "We set a threshold like this person say uttered 30% of the utterances in the meeting.",
                    "label": 0
                },
                {
                    "sent": "We will say, OK, this is the most bigger yes, not manually annotated.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I have a question on multiple sequence alignment.",
                    "label": 0
                },
                {
                    "sent": "Sometimes doesn't work well if you have a a baby in one input and then BBA.",
                    "label": 0
                },
                {
                    "sent": "In another input.",
                    "label": 0
                },
                {
                    "sent": "Example yesterday with it X versus wooded X yesterday.",
                    "label": 0
                },
                {
                    "sent": "Have you had problems with this kind of?",
                    "label": 0
                },
                {
                    "sent": "The sentence is, oh, we we didn't observe it in the data, but that's a good question.",
                    "label": 0
                },
                {
                    "sent": "But we can.",
                    "label": 0
                },
                {
                    "sent": "We can extend it by using like more sophisticated like similarity metric, right?",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Kindly just like word ngram.",
                    "label": 0
                },
                {
                    "sent": "But we can extend it by using semantic similarity.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah, last question.",
                    "label": 0
                },
                {
                    "sent": "So just for clarification, you mentioned answering Cathy's question that you already given the utterances which are relevant to the summary.",
                    "label": 0
                },
                {
                    "sent": "Do I remember correctly that this is actually manually annotated so you don't need to run a classifier for this?",
                    "label": 0
                },
                {
                    "sent": "Or did you in your experiments, run a classifier to 1st detect these utterances?",
                    "label": 0
                },
                {
                    "sent": "So we have two experimental setup.",
                    "label": 0
                },
                {
                    "sent": "I didn't go into details here.",
                    "label": 0
                },
                {
                    "sent": "Firstly, will assume we know like all those utterances related to a certain say summary.",
                    "label": 0
                },
                {
                    "sent": "This is called true clustering of it, and the other expander setup is we don't assume that we will use our previous work to cluster those together and got those dialogue Class dialogue act clusters and generate some right out of it.",
                    "label": 0
                },
                {
                    "sent": "Yes, so in your results here, did I miss that you show both of these conditions or?",
                    "label": 0
                },
                {
                    "sent": "I didn't, but the similar trends will hold for the system generated clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}