{
    "id": "5jxfin4p7w6jrcvawerosvknvrvtedaq",
    "title": "Diskriminative Sequence Labeling by Z-score Optimization",
    "info": {
        "author": [
            "Elisa Ricci, University of Perugia"
        ],
        "published": "Jan. 29, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/ecml07_ricci_dsl/",
    "segmentation": [
        [
            "My name is Alyssa Ritchey and I am a PhD student at University of Perugia.",
            "This is a joint work with Felda Baranello cristianini that we developed while."
        ],
        [
            "I was at University of Bristol and this talk I will briefly introduce sequence labeling.",
            "Will present in particular discriminative approaches for sequence labeling learning.",
            "I will introduce a new algorithm that we developed and it is based on this score maximization.",
            "Finally we will present some experimental results and discuss some computational issues behind our methods and then I will conclude with some ID."
        ],
        [
            "Yet for further works, sequence labeling basically is the task of assigning labels.",
            "Why to a sequence X?",
            "For example, sequence labeling arising must have many application, for example in part of speech tagging, X can be the sequence of words in a sentence and then then why it can be part of speech tags.",
            "Usually either Markov model are used for labeling, but there are too many drop.",
            "There are too many.",
            "To draw back in using traditional hidden Markov model, the first drawback is that conditional independence assumption sometimes are too strict to model the sequence labeling problem.",
            "Another problem is that the training is realized in a generative fashion and with maximum likelihood approach."
        ],
        [
            "To overcome these difficulties recently, some models called discriminative models has been introduced.",
            "They basically specify the conditional probability instead of the joint probability, but more importantly, they don't require an strict independence assumption.",
            "So when labeling an observation, arbitrary past and future observation can be taken into account.",
            "Moreover, arbitrary feature of the survey shun can be considered.",
            "In this way it's more easy to represent the long range dependencies between observer."
        ],
        [
            "Mission.",
            "And the usual framework that is adopted in discriminative approaches for sequence labeling is to model the problem as a multi label supervised classification task.",
            "So basically they output IRA sequences and in the task is given a training set of input output values of sequences.",
            "We want to learn a function H from the input space to the output space such that the optimal output label sequences can be reconstructed from the given input sequences.",
            "An official test sample of pair of sequences, usually to do to perform this task are a linear score function that model the interaction between input and output sequences is introduced, and it is a function of vector of parimeter tita.",
            "And then once the parimeter are determined, the prediction can be made by computing the argmax of this call."
        ],
        [
            "Function.",
            "And in this talk, for simplicity we I just consider a model of chain conditional random field.",
            "We hmm feature.",
            "It means that I just consider in that graph two kind of feature feature that model the interaction between an observation at step K and associated the label at step K, and that we can call a mission feature and then some transition feature that.",
            "More than the interaction between the between the two adjacent labels in this framework in this core is simply given by the logarithm of the conditional probability.",
            "Then Parimeter Tita contains the vector that contains the logarithm of transition and emission probabilities and feature vector containing the associated statistics."
        ],
        [
            "Sufficient statistics?",
            "So basically the task we are interested in is given a training set T we want to find the parimeter set Theta such that the given area Newton's test sample.",
            "We can reconstruct the label sequences given the observed sequences and the most straightforward approach in discriminately among the discriminative approaches is to define 01 loss and to minimize an upper bound on the empirical risk.",
            "In the training set an in this way, the number of incorrect label sequences is minimized, and this is basically the approach that is using most of the algorithm like conditional random fields, Max merging algorithms or in the mark or perceptron."
        ],
        [
            "Average perceptron?",
            "What we propose it and you motivation to solve this problem and we define a new optimization criterion that we think is more appropriate for long sequences and non separable cases.",
            "We want to minimize the an upper bound on a ranking loss.",
            "In practice we want to minimize the number of pages that score larger than they given correct pace.",
            "To do that, we define our objective function to be the disk or.",
            "This car is a statistic measure that basically in our case.",
            "Defined in this way, we consider the distribution of all possible scores, and we want to separate and this car is the difference between the score of the optimal payer.",
            "Is there a minus the mean of the distribution divided by the standard deviation?",
            "So basically maximizing this object, we want the score of the optimal pair is maximally separated from the bulk of all incorrect one.",
            "So we want to maximize the distance between the score of the optimal.",
            "And the mean of the distribution measured in terms of."
        ],
        [
            "Other deviation to solve this problem we have to put the disk or are in a form that we can optimize it, optimize it and in particular we define.",
            "We see that this card can be expressed as a function of the para meters.",
            "In particular, it is very easy to see that the mean of the distribution is a linear function of the parameter and the variance can be expressed as a function of the parimeter as well.",
            "So basically the finding the vector B that contains the difference between the feature associated to the optimal pair and mean vector, and we can, we can see that the discovery is a function of the parimeter data, so we are ready to optimizing it if we know how to compute the matrix B and the vector B and the covariance matrix.",
            "See this is not an easy."
        ],
        [
            "Ask because basically the number of possible label sequences given an observation sequence is used.",
            "In particular is exponentially in the language of the sequences, but we have devised an algorithm to compute each term of the mean vector and covariance matrix by dynamic programming algorithms.",
            "And here I'll show an example of themselves or algorithm for computing the mean of this statistic associated to transition and emission probability.",
            "An offer on observation queue at state P. This algorithm is similar to the forward algorithm in HMM.",
            "The only difference is the recursive formula that make consent to obtain the mean of the emission probability at each step of the dynamic programming tables.",
            "Basically, I just show the algorithm for emission probabilities of observation.",
            "Sleepy, but to compute all the other element of the matrix C and then vector me move, we simply change the recursive formulas and some."
        ],
        [
            "Initial condition.",
            "They basically I don't have time to go in detail in the algorithm at the basic idea behind this algorithm.",
            "Is that mean values can be computed considering that the expected value of a sum of variable step K is given by the expected values or the sum of the previous step plus the expected value of current step.",
            "A similar philosophy can be adopted for 2nd order moments and variances can be computed by just centering the 2nd order."
        ],
        [
            "Moments.",
            "Until now I have just considered the discord for one pair of.",
            "For one payroll input, output pairs of sequences, but we're interested in defining the discord for the interior training set.",
            "This can be done quite easily if we assume that the contributor of each pair in the training set is independent from the other space, and we can see easily that the mean of the training set is basically given by the sum of the mean of each pair, and this happen as well for the variance.",
            "And so basically we can put the discord of the training set exactly in the same form of the discord for just one pair, having defining the vector V star and sister in the metric system.",
            "And so we get the problem that we want to solve.",
            "It is to maximize the discord of the of the training set an.",
            "To solve this problem is quite easy since we can observe that this problem is invariant to scaling and the.",
            "Through the monotone intensity of the square root, we can put the problem in the order form on the right side.",
            "So we have a quadratic programming problem with the linear constraint.",
            "Or better, we can simply solve find the method the vector or optimal vector by inverting the symmetrix multiplying.",
            "Parmie and to solve to invest these metrics, even if it's quite large size, we can adopt fast.",
            "Can you gather my gradient method?"
        ],
        [
            "I am going to show some results associated to this method an in the first experiments we just consider some artificial data.",
            "We consider hmm feature in conditional random field an where then the size of the observation alphabet is 4 and the size of the Eden alphabet history.",
            "So we have 21 para meters.",
            "We consider random sequence of length.",
            "Then we want to do their mind the optimal parameter vector, Theta.",
            "But since the sequence are generated randomly this these optimal vector parameter may not exist, so we plot the training error or in terms of learning of labeling accuracy when the training set size varies.",
            "So basically we plot the learning curve an.",
            "Here we can see that this car it is the blue curve outperform conditional random field, maximum margin algorithm and.",
            "Either Mark Perceptron, voted Perceptron Ann and it is the blue line.",
            "But we need another experiment we considered with the same data.",
            "Also slightly different version of our algorithm that consider the Hamming distance between levels and we compare it with a maximum margin algorithm that also considered the Hamming distance.",
            "In this case we see that the algorithm compatible performance but.",
            "If we analyze the training time, we see that our approach is much more faster when the training set size increase."
        ],
        [
            "You know there's Perimental.",
            "We consider real data.",
            "We're going to see the task of named entity recognition.",
            "That is basically the task to locate name entities in text.",
            "An entity of interest can be location, person, organization, and in particular we consider the Spanish new wires artikkel corpus for the special session of Natural Language competition in two."
        ],
        [
            "1002 we consider, in particular 300 sequences with an average length of 30 words.",
            "We have to assign 9 labels and that are basically non name, beginning and continuation of person, organization, location, initial Launius names.",
            "We consider two set of feature HMM feature in set S1 and Insert S 2 S 1 feature plus hmm features of the previous next towards this demonstrated that our.",
            "Method apply not just to change conditional random field with hidden Markov feature but also to arbitrary features that can also be spelling features.",
            "Even if I didn't report results here.",
            "So basically if I consider you can just dip your disk or approach.",
            "I can see that outperform Max margin algorithm either Marco Perceptron, conditional random fields and again if the version of the coming with me.",
            "Loss is considered.",
            "The performers are compatible with the Max margin algorithm with Hamming loss."
        ],
        [
            "And then one problem that we have, you know we had in developing our algorithm is that for large feature space and the computational cost becomes large.",
            "In fact the main bottleneck is the computation of the metrics.",
            "See because its side then it's a quadratic in the number of the parameters.",
            "But for example we we solved this problem in two ways.",
            "The first solution is to exploit the structure and the sparseness of these metrics and here and we saw that if we consider just HMM features, the number of different values in the symmetrix is linear in the size of the observation alphabet are.",
            "Here I have reported an experiment for all of the Symmetrix 4.",
            "This data observation alphabet size equal to four and then.",
            "And on the observation alphabet sides equal to four and the state alphabet sides equal to three and we see that there's a lot of redundancy in the symmetrix."
        ],
        [
            "And second another solution that we device and what we don't do not investigate so much is the user sampling strategy for computing the mean vector Moran, the covariance matrix C. And basically I show some results on an artificial data set.",
            "We consider random sequences again and we have that the state alphabet size is fixed to two, while the observation alphabet sides.",
            "Increases we compare the performance into in term of labeling error of the approach with the discord and approach with the discord with metrics computed by sampling on just 100 parts, and we see that performance in term of error compatible.",
            "While the approximate technique is much more faster if we consider the training time that are."
        ],
        [
            "Parentheses.",
            "So to conclude, I have presented the new method for discriminative sequence labeling.",
            "We see that the accuracy is compatible with the state of the art algorithm.",
            "Like Max modern approaches, we see that this method is easy to implement, uh, since it relies on dynamic programming for metrics computation and simple optimization problem.",
            "This method is pretty fast for large training sides and a reasonable number of features.",
            "Since I mean and variances can be computed.",
            "The computation of mean and variances can be parallelized and can you get the gradient technique can be used for inverting the symmetrix.",
            "Here I have presented an application of this method to sequence labeling.",
            "But we also analyze the problem or sequence parts learning and sequence alignment and future work could be to test the scalability of this method to large data sets using approximate technique or to develop a dual version of the algorithm with kernels.",
            "Thank you for all your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Alyssa Ritchey and I am a PhD student at University of Perugia.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with Felda Baranello cristianini that we developed while.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was at University of Bristol and this talk I will briefly introduce sequence labeling.",
                    "label": 0
                },
                {
                    "sent": "Will present in particular discriminative approaches for sequence labeling learning.",
                    "label": 1
                },
                {
                    "sent": "I will introduce a new algorithm that we developed and it is based on this score maximization.",
                    "label": 0
                },
                {
                    "sent": "Finally we will present some experimental results and discuss some computational issues behind our methods and then I will conclude with some ID.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yet for further works, sequence labeling basically is the task of assigning labels.",
                    "label": 1
                },
                {
                    "sent": "Why to a sequence X?",
                    "label": 1
                },
                {
                    "sent": "For example, sequence labeling arising must have many application, for example in part of speech tagging, X can be the sequence of words in a sentence and then then why it can be part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "Usually either Markov model are used for labeling, but there are too many drop.",
                    "label": 0
                },
                {
                    "sent": "There are too many.",
                    "label": 0
                },
                {
                    "sent": "To draw back in using traditional hidden Markov model, the first drawback is that conditional independence assumption sometimes are too strict to model the sequence labeling problem.",
                    "label": 0
                },
                {
                    "sent": "Another problem is that the training is realized in a generative fashion and with maximum likelihood approach.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To overcome these difficulties recently, some models called discriminative models has been introduced.",
                    "label": 0
                },
                {
                    "sent": "They basically specify the conditional probability instead of the joint probability, but more importantly, they don't require an strict independence assumption.",
                    "label": 1
                },
                {
                    "sent": "So when labeling an observation, arbitrary past and future observation can be taken into account.",
                    "label": 1
                },
                {
                    "sent": "Moreover, arbitrary feature of the survey shun can be considered.",
                    "label": 0
                },
                {
                    "sent": "In this way it's more easy to represent the long range dependencies between observer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "And the usual framework that is adopted in discriminative approaches for sequence labeling is to model the problem as a multi label supervised classification task.",
                    "label": 1
                },
                {
                    "sent": "So basically they output IRA sequences and in the task is given a training set of input output values of sequences.",
                    "label": 1
                },
                {
                    "sent": "We want to learn a function H from the input space to the output space such that the optimal output label sequences can be reconstructed from the given input sequences.",
                    "label": 0
                },
                {
                    "sent": "An official test sample of pair of sequences, usually to do to perform this task are a linear score function that model the interaction between input and output sequences is introduced, and it is a function of vector of parimeter tita.",
                    "label": 0
                },
                {
                    "sent": "And then once the parimeter are determined, the prediction can be made by computing the argmax of this call.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "And in this talk, for simplicity we I just consider a model of chain conditional random field.",
                    "label": 0
                },
                {
                    "sent": "We hmm feature.",
                    "label": 0
                },
                {
                    "sent": "It means that I just consider in that graph two kind of feature feature that model the interaction between an observation at step K and associated the label at step K, and that we can call a mission feature and then some transition feature that.",
                    "label": 0
                },
                {
                    "sent": "More than the interaction between the between the two adjacent labels in this framework in this core is simply given by the logarithm of the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Then Parimeter Tita contains the vector that contains the logarithm of transition and emission probabilities and feature vector containing the associated statistics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sufficient statistics?",
                    "label": 0
                },
                {
                    "sent": "So basically the task we are interested in is given a training set T we want to find the parimeter set Theta such that the given area Newton's test sample.",
                    "label": 0
                },
                {
                    "sent": "We can reconstruct the label sequences given the observed sequences and the most straightforward approach in discriminately among the discriminative approaches is to define 01 loss and to minimize an upper bound on the empirical risk.",
                    "label": 1
                },
                {
                    "sent": "In the training set an in this way, the number of incorrect label sequences is minimized, and this is basically the approach that is using most of the algorithm like conditional random fields, Max merging algorithms or in the mark or perceptron.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Average perceptron?",
                    "label": 0
                },
                {
                    "sent": "What we propose it and you motivation to solve this problem and we define a new optimization criterion that we think is more appropriate for long sequences and non separable cases.",
                    "label": 1
                },
                {
                    "sent": "We want to minimize the an upper bound on a ranking loss.",
                    "label": 1
                },
                {
                    "sent": "In practice we want to minimize the number of pages that score larger than they given correct pace.",
                    "label": 0
                },
                {
                    "sent": "To do that, we define our objective function to be the disk or.",
                    "label": 0
                },
                {
                    "sent": "This car is a statistic measure that basically in our case.",
                    "label": 0
                },
                {
                    "sent": "Defined in this way, we consider the distribution of all possible scores, and we want to separate and this car is the difference between the score of the optimal payer.",
                    "label": 0
                },
                {
                    "sent": "Is there a minus the mean of the distribution divided by the standard deviation?",
                    "label": 0
                },
                {
                    "sent": "So basically maximizing this object, we want the score of the optimal pair is maximally separated from the bulk of all incorrect one.",
                    "label": 0
                },
                {
                    "sent": "So we want to maximize the distance between the score of the optimal.",
                    "label": 0
                },
                {
                    "sent": "And the mean of the distribution measured in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other deviation to solve this problem we have to put the disk or are in a form that we can optimize it, optimize it and in particular we define.",
                    "label": 0
                },
                {
                    "sent": "We see that this card can be expressed as a function of the para meters.",
                    "label": 0
                },
                {
                    "sent": "In particular, it is very easy to see that the mean of the distribution is a linear function of the parameter and the variance can be expressed as a function of the parimeter as well.",
                    "label": 1
                },
                {
                    "sent": "So basically the finding the vector B that contains the difference between the feature associated to the optimal pair and mean vector, and we can, we can see that the discovery is a function of the parimeter data, so we are ready to optimizing it if we know how to compute the matrix B and the vector B and the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "See this is not an easy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ask because basically the number of possible label sequences given an observation sequence is used.",
                    "label": 1
                },
                {
                    "sent": "In particular is exponentially in the language of the sequences, but we have devised an algorithm to compute each term of the mean vector and covariance matrix by dynamic programming algorithms.",
                    "label": 1
                },
                {
                    "sent": "And here I'll show an example of themselves or algorithm for computing the mean of this statistic associated to transition and emission probability.",
                    "label": 1
                },
                {
                    "sent": "An offer on observation queue at state P. This algorithm is similar to the forward algorithm in HMM.",
                    "label": 0
                },
                {
                    "sent": "The only difference is the recursive formula that make consent to obtain the mean of the emission probability at each step of the dynamic programming tables.",
                    "label": 0
                },
                {
                    "sent": "Basically, I just show the algorithm for emission probabilities of observation.",
                    "label": 0
                },
                {
                    "sent": "Sleepy, but to compute all the other element of the matrix C and then vector me move, we simply change the recursive formulas and some.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Initial condition.",
                    "label": 0
                },
                {
                    "sent": "They basically I don't have time to go in detail in the algorithm at the basic idea behind this algorithm.",
                    "label": 1
                },
                {
                    "sent": "Is that mean values can be computed considering that the expected value of a sum of variable step K is given by the expected values or the sum of the previous step plus the expected value of current step.",
                    "label": 1
                },
                {
                    "sent": "A similar philosophy can be adopted for 2nd order moments and variances can be computed by just centering the 2nd order.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moments.",
                    "label": 0
                },
                {
                    "sent": "Until now I have just considered the discord for one pair of.",
                    "label": 0
                },
                {
                    "sent": "For one payroll input, output pairs of sequences, but we're interested in defining the discord for the interior training set.",
                    "label": 0
                },
                {
                    "sent": "This can be done quite easily if we assume that the contributor of each pair in the training set is independent from the other space, and we can see easily that the mean of the training set is basically given by the sum of the mean of each pair, and this happen as well for the variance.",
                    "label": 0
                },
                {
                    "sent": "And so basically we can put the discord of the training set exactly in the same form of the discord for just one pair, having defining the vector V star and sister in the metric system.",
                    "label": 0
                },
                {
                    "sent": "And so we get the problem that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "It is to maximize the discord of the of the training set an.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem is quite easy since we can observe that this problem is invariant to scaling and the.",
                    "label": 0
                },
                {
                    "sent": "Through the monotone intensity of the square root, we can put the problem in the order form on the right side.",
                    "label": 0
                },
                {
                    "sent": "So we have a quadratic programming problem with the linear constraint.",
                    "label": 0
                },
                {
                    "sent": "Or better, we can simply solve find the method the vector or optimal vector by inverting the symmetrix multiplying.",
                    "label": 0
                },
                {
                    "sent": "Parmie and to solve to invest these metrics, even if it's quite large size, we can adopt fast.",
                    "label": 0
                },
                {
                    "sent": "Can you gather my gradient method?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am going to show some results associated to this method an in the first experiments we just consider some artificial data.",
                    "label": 0
                },
                {
                    "sent": "We consider hmm feature in conditional random field an where then the size of the observation alphabet is 4 and the size of the Eden alphabet history.",
                    "label": 0
                },
                {
                    "sent": "So we have 21 para meters.",
                    "label": 0
                },
                {
                    "sent": "We consider random sequence of length.",
                    "label": 0
                },
                {
                    "sent": "Then we want to do their mind the optimal parameter vector, Theta.",
                    "label": 0
                },
                {
                    "sent": "But since the sequence are generated randomly this these optimal vector parameter may not exist, so we plot the training error or in terms of learning of labeling accuracy when the training set size varies.",
                    "label": 0
                },
                {
                    "sent": "So basically we plot the learning curve an.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that this car it is the blue curve outperform conditional random field, maximum margin algorithm and.",
                    "label": 0
                },
                {
                    "sent": "Either Mark Perceptron, voted Perceptron Ann and it is the blue line.",
                    "label": 0
                },
                {
                    "sent": "But we need another experiment we considered with the same data.",
                    "label": 0
                },
                {
                    "sent": "Also slightly different version of our algorithm that consider the Hamming distance between levels and we compare it with a maximum margin algorithm that also considered the Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "In this case we see that the algorithm compatible performance but.",
                    "label": 0
                },
                {
                    "sent": "If we analyze the training time, we see that our approach is much more faster when the training set size increase.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know there's Perimental.",
                    "label": 0
                },
                {
                    "sent": "We consider real data.",
                    "label": 0
                },
                {
                    "sent": "We're going to see the task of named entity recognition.",
                    "label": 1
                },
                {
                    "sent": "That is basically the task to locate name entities in text.",
                    "label": 0
                },
                {
                    "sent": "An entity of interest can be location, person, organization, and in particular we consider the Spanish new wires artikkel corpus for the special session of Natural Language competition in two.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1002 we consider, in particular 300 sequences with an average length of 30 words.",
                    "label": 1
                },
                {
                    "sent": "We have to assign 9 labels and that are basically non name, beginning and continuation of person, organization, location, initial Launius names.",
                    "label": 0
                },
                {
                    "sent": "We consider two set of feature HMM feature in set S1 and Insert S 2 S 1 feature plus hmm features of the previous next towards this demonstrated that our.",
                    "label": 0
                },
                {
                    "sent": "Method apply not just to change conditional random field with hidden Markov feature but also to arbitrary features that can also be spelling features.",
                    "label": 0
                },
                {
                    "sent": "Even if I didn't report results here.",
                    "label": 0
                },
                {
                    "sent": "So basically if I consider you can just dip your disk or approach.",
                    "label": 0
                },
                {
                    "sent": "I can see that outperform Max margin algorithm either Marco Perceptron, conditional random fields and again if the version of the coming with me.",
                    "label": 0
                },
                {
                    "sent": "Loss is considered.",
                    "label": 0
                },
                {
                    "sent": "The performers are compatible with the Max margin algorithm with Hamming loss.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then one problem that we have, you know we had in developing our algorithm is that for large feature space and the computational cost becomes large.",
                    "label": 0
                },
                {
                    "sent": "In fact the main bottleneck is the computation of the metrics.",
                    "label": 0
                },
                {
                    "sent": "See because its side then it's a quadratic in the number of the parameters.",
                    "label": 0
                },
                {
                    "sent": "But for example we we solved this problem in two ways.",
                    "label": 0
                },
                {
                    "sent": "The first solution is to exploit the structure and the sparseness of these metrics and here and we saw that if we consider just HMM features, the number of different values in the symmetrix is linear in the size of the observation alphabet are.",
                    "label": 1
                },
                {
                    "sent": "Here I have reported an experiment for all of the Symmetrix 4.",
                    "label": 0
                },
                {
                    "sent": "This data observation alphabet size equal to four and then.",
                    "label": 0
                },
                {
                    "sent": "And on the observation alphabet sides equal to four and the state alphabet sides equal to three and we see that there's a lot of redundancy in the symmetrix.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And second another solution that we device and what we don't do not investigate so much is the user sampling strategy for computing the mean vector Moran, the covariance matrix C. And basically I show some results on an artificial data set.",
                    "label": 0
                },
                {
                    "sent": "We consider random sequences again and we have that the state alphabet size is fixed to two, while the observation alphabet sides.",
                    "label": 0
                },
                {
                    "sent": "Increases we compare the performance into in term of labeling error of the approach with the discord and approach with the discord with metrics computed by sampling on just 100 parts, and we see that performance in term of error compatible.",
                    "label": 0
                },
                {
                    "sent": "While the approximate technique is much more faster if we consider the training time that are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parentheses.",
                    "label": 0
                },
                {
                    "sent": "So to conclude, I have presented the new method for discriminative sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "We see that the accuracy is compatible with the state of the art algorithm.",
                    "label": 1
                },
                {
                    "sent": "Like Max modern approaches, we see that this method is easy to implement, uh, since it relies on dynamic programming for metrics computation and simple optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This method is pretty fast for large training sides and a reasonable number of features.",
                    "label": 1
                },
                {
                    "sent": "Since I mean and variances can be computed.",
                    "label": 0
                },
                {
                    "sent": "The computation of mean and variances can be parallelized and can you get the gradient technique can be used for inverting the symmetrix.",
                    "label": 0
                },
                {
                    "sent": "Here I have presented an application of this method to sequence labeling.",
                    "label": 0
                },
                {
                    "sent": "But we also analyze the problem or sequence parts learning and sequence alignment and future work could be to test the scalability of this method to large data sets using approximate technique or to develop a dual version of the algorithm with kernels.",
                    "label": 1
                },
                {
                    "sent": "Thank you for all your attention.",
                    "label": 0
                }
            ]
        }
    }
}