{
    "id": "o57tdpbxt43lkuhxmalyvc7loc3uvbj5",
    "title": "Probability and Mathematical Needs",
    "info": {
        "author": [
            "Sandrine Anthoine, Aix-Marseille Universit\u00e9"
        ],
        "published": "Aug. 5, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/bootcamp2010_anthoine_pmn/",
    "segmentation": [
        [
            "Morning.",
            "So, um, it seems that I should say a few words about myself before we begin, so I'm a researcher in the math lab, and I do research in image processing in applied math.",
            "So the goal of today is to give you some.",
            "Basic necessary mathematics that you may be using this week or not.",
            "Everything is going to be starting from the real beginning, so you probably know everything I'm going to say today, so I apologize if you know everything.",
            "If you don't, you can ask questions and.",
            "Well, let's begin.",
            "So we'll talk about several."
        ],
        [
            "Things.",
            "The the direction is divided into 3 parts.",
            "The first part is going to talk about linear algebra and vector spaces and this stuff.",
            "In the second part, will talk about probabilities, and in the third partner I'll talk a little bit about optimization.",
            "Anne Anne will start with linear algebra, so I'm assuming that your preferred vectors phase is mine and is auto after is pretty.",
            "He's pretty nice to deal with, right?",
            "In R2 you take 2 vectors.",
            "You envy and you're able.",
            "You are able to send them by just adding V on top of you here.",
            "I'm getting a vector there.",
            "You able to do other stuff like multiplying you by a scalar so you can multiply you by two and you get a vector that is twice as long as you.",
            "And then you can do a bunch of stuff.",
            "Actually you can define things that are lies.",
            "And we can define what's the projection of W on this line.",
            "And that would be this vector there.",
            "Anne.",
            "What else?",
            "Um?",
            "So it's pretty easy to work with our two an."
        ],
        [
            "I guess.",
            "The first generalization of our trees are an with an any any integer and our three, for example, is your usual space.",
            "What you have in our three is that you can sum of vectors.",
            "You can multiply them by a scalar.",
            "And you can also.",
            "Actually you can write any vector in RN as a sum of N vectors.",
            "So I wrote them rum E1E N which are the basic vector basis there.",
            "An you can figure out that any element in our end is a sum of of those vectors.",
            "I'll give you two other example of vector spaces.",
            "The segment one here is the set of solution of a second order differential equation that is homogeneous.",
            "So if you take functions that go from auto or that are twice differentiable and you take the set of these functions that verify this equation, then you can show that it a vector space that it's stable by addition that if F is in the vector space, then it suppose it is also in the vector space.",
            "And that you can multiply them by a scalar.",
            "And likewise for RN.",
            "You can write any function that is in SQI will sum of two.",
            "Two special functions, the cosine function and assign functions.",
            "Anne, this decomposition there is actually unique.",
            "They."
        ],
        [
            "There are spaces for which you cannot actually decompose any.",
            "The elements on a finite set of vectors and those would be called infinite dimensional vector spaces and an example of that is L2 of our so L2 of R is the space of square integrable functions.",
            "It's stable by addition and by multiplication by a real number.",
            "And there's something which."
        ],
        [
            "I haven't.",
            "That I have not written for the other spaces, you can actually define a dot product and a norm.",
            "The DOT product of two functions in in L2 is the integral of their product and it's equivalent to your usual dot product in our two here, which would be if you is the vector U1U2 and V is V1V2 then your dot product.",
            "Is the sum U1V1 plus U2V2?",
            "So that product in there is two years and norm which is the square root of the integral of the square of F. And it's gonna be useful because you you define with with this dot product and Norm, you're going to be able to define things that are orthogonal.",
            "An decomposing vectors on sets of orthogonal thing is going to make your life easier.",
            "And the next thing you have in in two of our which is important is this closeness.",
            "So if you take a sequence of function F and that converges to F. For which the norm of the difference converges to 0, then F the limit function is easier 2.",
            "It's not always true, right sets are not always closed if you take for example.",
            "If your set is a.",
            "Sorry, the interval between zero and one but not containing you or one.",
            "Then you can find.",
            "If you take XN.",
            "The sequence which is 1 / N for each N, then XN goes to 0.",
            "But zero is not in your in your set S, so which means that you may have limits of things that are all in S, that for which the limit is not in S, and that's when S is not closed.",
            "OK, well, two of our of this disability that if you have a convergent sequence, you're sure that the limit is in L2 steel, and that's important.",
            "OK, so I'll go to her."
        ],
        [
            "For more different definition of vector spaces, which is just summarizing what we've said so far.",
            "If you have a set S, it's called a vector space.",
            "If the addition is a stable operation.",
            "If the addition is Additionally commutative and distributive and associative.",
            "SMS Seven element 40 addition.",
            "An every element of Asmus must be invertible for the addition, so basically you have an opposite minus.",
            "An operation which is a minus.",
            "In addition, if you want to read vector space, you need to have S, which is stable by multiplication by any real number.",
            "Or any scalar I wrote?",
            "Any scalar Lambda?",
            "And the multiplication by a scalar must have the properties that it does.",
            "It is associative and distributive over the addition.",
            "So all of this is basically just saying you have an addition.",
            "If you add two vectors, it's still a vector.",
            "You have the opposite of a vector that exists, and the multiplication of a vector by a scalar yields a vector as well.",
            "Um, so useful thing when you want to see what's going on on Elsa vector spaces to divide it into smaller spaces, and so subspaces are basically subsets of S which have the same properties, so they are stable by addition.",
            "They're stable by multiplication by a scalar, OK?"
        ],
        [
            "No, there are subspaces that have.",
            "That help you decompose your vector space easily and those would be called supplementary subspaces.",
            "So if I take out two again.",
            "Let's see my subspace, as is the one that is defined by my my first vector vector U one here.",
            "Then if you take any other vector that is not collinear to you, and so that that is not in the same direction as you want, for example this one, there V. We use subspace, which is the the set of all the the.",
            "The vectors that are proportional to V, those two subspaces there, so I'll call this this line F1 and F2 are actually supplementary subspaces, which means that for any.",
            "Find some that you having or two.",
            "Then you may write it as a sum of.",
            "Uh.",
            "So if I do the projections there.",
            "So your vector accesses can be written as.",
            "X One times the first vector U 1 + X two.",
            "Times the second vector V. And actually, if 2 two subspaces are set to be supplementary, if such a decomposition always exists, OK?",
            "So up there actually, in the notations out there it would.",
            "This thing would be X of F1 and this thing would be X of F2.",
            "Now this subspace here F1 is just generated by U-1 an.",
            "Similarly if you take a family of vectors in S, you can generate a vector subspace from this in these vectors.",
            "To take X1 and 2X NB to BN vectors of South, then the span of X1 XN is the sets of finite linear combinations that you can make from X1 to XN.",
            "So that's the thing up there.",
            "Ann, you have a.",
            "So the notion of weather.",
            "You can.",
            "Anne.",
            "Well, let's see the.",
            "The family of the exercise is set to be linearly independent if decomposition of NEY in the subspace span of X1 XN as a sum of.",
            "Dexi is unique.",
            "And Conversely, what did I say if the yeah OK. Alright, so basically.",
            "It's easy to expand vector.",
            "Why that is in the span of X1, XN as a sum of the Lambda IXI.",
            "Now this decomposition maybe maybe unique or not, and it's unique if and only if your vectors.",
            "Are linearly independent?",
            "So maybe I write again an example in our in our two.",
            "Now we've just seen that if you take you.",
            "And we need to be, not.",
            "Not in the same direction.",
            "Then you can write any X.",
            "In S, as is as a sum.",
            "Here.",
            "Well, actually if you add another vector, a third vector maybe?",
            "Let's see this one here W. And it's still true that for any vector here in R2, you can write it as a sum of.",
            "Our first component in you plus a second component in V + 1/3 component in W. OK, but now this this this scalars.",
            "There are not unique anymore and the reason they're not unique is because you can find if you add up.",
            "So let's say.",
            "If you add up you.",
            "And a smaller proportion of V. And W maybe twice time times W, then you get 0 vector.",
            "So each time because you have this linear combination.",
            "You press, let's say this thing is.",
            "Maybe 1/3?",
            "1/3 of V. 2 W is equal to the nerve vector.",
            "Then for any X in R2.",
            "Then um.",
            "If you take those skaters skaters here, I'm sorry those curls here Xux vianex W an.",
            "If you add 2112 X yuan 1/3 two XV and two 2X W then you get you'll get exactly the same vector again in our tool.",
            "Is that clear?",
            "OK.",
            "Forget it, yes there OK?",
            "So basically because you have a combination with non zero coefficient that makes these things you then this this writing there is not unique for linearly independent vectors.",
            "There's no such combination an that makes that the decompositions are unique and that also gives you the dimension of the vector space that is generated by X1 to XN and this dimension is exactly.",
            "The The Cardinal of the largest family of vectors that is linearly independent in the subspace.",
            "OK so already is of dimension D or two is of dimension 2.",
            "Obviously our set of solutions to a second order or marginalized differential equations.",
            "The one we've seen just before is of dimension 2 as well.",
            "And as I said, L2 of our.",
            "It is actually of infinite dimension.",
            "OK, some subspaces that may be useful, or hyper planes and their define as subspaces of of your origin vector space, which have a supplementary of dimension exactly one.",
            "OK, so in Rd Annie subspace of dimension D -- 1 is a hyperplane.",
            "OK, the reason why we define diaper prints through their supplementaries is if you have a.",
            "A set with infinite dimension at the beginning.",
            "Then the operating still has an infinite dimension, so you can't really say the hyperplane as dimension minus one.",
            "The dimension of the original space, right?",
            "That's what you should think about."
        ],
        [
            "So now basically a basis of a of a subspace is generative and linearly independent family.",
            "In this subspace they all have exactly the same Cardinal.",
            "And the yield unique decomposition for any X in this subspace.",
            "So in Rd we've seen that the basis vectors there are.",
            "Those vectors there are do you the basis in in L2 of the zero 2\u03c0.",
            "So the set of functions that are square into integrable in this interval 002 Pi.",
            "Then you can show that the set of function cosine and T & empty for all possible integers.",
            "Ann is a basis as well and so that will tell you that if you have a square integrable function on zero 2\u03c0, then you can extract decompose it as such a sum where the M&BM are unique coefficients OK. And so that enables you to see a function as a sum of OK signs here.",
            "Of cosines and sines for which you know that from the properties right those those are simple functions which you can see really easily.",
            "OK, so what's next?",
            "And in our two, for example, we talked about projection just before and we also did.",
            "We talk about Dot products, not yet.",
            "So actually the next question is if you have such a decomposition, how are you going to find the M and BMS right?",
            "How are you going to find the coefficients?",
            "In front of your business vectors.",
            "And so an easy way to do that is is."
        ],
        [
            "And you are Victor.",
            "Space as on top of having the addition and multiplication by the scare, also has a dot product.",
            "So the DOT product in Audi is the usual dot product we've seen before and it's linked to the Euclidean norm in already.",
            "And you can see the dot product as a measure of how.",
            "Of what is the angle between X and two 2 vectors X&Y?",
            "OK, so if you have a vector space with the dot product then any subspace of it as a unique orthogonal supplementary.",
            "Which means that the two sets F an it's orthogonal there.",
            "Where, which means that the discipline entry has the special properties that if X is in F, ANAN why is in the supplementary?",
            "Then you you automatically have that the dot product is equal to 0, or that two elements of F and F1 or orthogonal.",
            "OK, so the dot product being 0.",
            "You should think about this as the two vectors being orthogonal OK and so.",
            "Now that you."
        ],
        [
            "If you have a unique supplementary, then what you can do is to easily decompose your vectors.",
            "Annual have relations between the norm of the different parts of your vectors.",
            "So that's what's written here.",
            "If you have two supplementary spaces, then the norm of X is squared is equal to the norm of it spot on the subspace F plus it's spot on the orthogonal space, which is only saying that, for example.",
            "The normal of this vector is the square of this length plus this ranks right.",
            "This length is.",
            "This would be the the projection of a on your subspace F. This would be the projection of a on your subspace effort toggle, and so the norm of this vector squared with this vector squared is the norm of this one.",
            "So that's your Pythagorean theorem, right?",
            "OK so I I just reminding you what a norm is up there and I'm not going to tell you again what it is you.",
            "So again, X&Y also also if and only if their dot product is zero and.",
            "So the last definition I wanted to give is what is a Hilbert space?",
            "So a Hilbert space is a vector space where you have a dot product and Additionally the space is closed for the norm that is yielded by the DOT product.",
            "So again at two of our research space, right?",
            "It's a vector space, the dot product of two function is the integration of the product of the two functions.",
            "And this year is the norm, which is the norm, which is the integration of F squared.",
            "The square root of the integral of F squared.",
            "An if you again the fact that the space is closed means that if you have a sequence of vector in each that converges, then you're sure that the limit is in each as well."
        ],
        [
            "So once you have the authoritie, you may look for also going to basis.",
            "An obviously your usual basis in RG is an Arsenal in the North, so normal basis.",
            "The basis is also normally if the dot product between two and two vector basis is is either one.",
            "If it's the same element or zero if it's two different elements and what happens is.",
            "For example, in L2 of zero 2\u03c0, then we've seen that this is a basis, and it's actually an ortho normal.",
            "It's absolutely sorry.",
            "After Google basis, I'm not really sure I know the coefficients there, but in any case it's really nice with an orthonormal basis.",
            "Is that if you want to find the coefficient of the decomposition, it's really easy.",
            "You just have to take the dot products so if you and we form an orthonormal basis of R2 here.",
            "So do you would be envy would be 2 vector that orthogonal and that have length one exactly so for example.",
            "You could have.",
            "Let me rephrase that.",
            "You have like links one and it's orthogonal there as well.",
            "Then the coefficients of X on this basis, or simply the dot products Xu.",
            "And exit.",
            "OK, so.",
            "And you also have this nice property coming from the fact that.",
            "The norm of X square is the sum.",
            "Of of its two projections.",
            "Then you can write this in terms of the coefficients actuan, XG and so.",
            "This year is that the norm of X square is just the sum of the square root of the coefficients.",
            "Is there right there?",
            "Yeah, that's why it's written up there in the point.",
            "Number two, OK?"
        ],
        [
            "So now if you have a space with the DOT product and you consider an hyperplane, then it's easy to see what's.",
            "For example, the equation of the hyperplane or the distance from a point to the disciple plane or.",
            "The projection of any point to decipher on this hyperplane an.",
            "The reason is that hyperplanes have simplement areas of of dimension exactly 1.",
            "So not too if you if I want to draw a significant picture for this one, that better drew something in all three which might not be really nice because I don't grow really well.",
            "But anyways you have to think about it.",
            "The reason I'm doing that in all three is that because in R2.",
            "1 dimensional spaces or lines that go to go to zero.",
            "And there also hyperplanes, so you won't see the difference between the hyperplane and it's supplementary.",
            "It's easier in R3.",
            "OK, so in our three I'm going to consider the hyperplanes which is this hyperplane there.",
            "Which is, uh, the pain with the.",
            "Let's see XY.",
            "The plane studied.",
            "This is defined by Y is equal to zero OK?",
            "So if you take any vector.",
            "X here.",
            "Nina in R3.",
            "Then you made a composite really easily as a sum of something which is.",
            "Just perpendicular to this plane.",
            "But it's something that's in the DX ZZ plane, right?",
            "And those two vectors there are signal.",
            "Ann, what's really easy to see is that this projection here of X2 on on to your hyperplane is exactly what it's exactly.",
            "So the projection.",
            "Of X2 on to your iPad screen XD is exactly X minus this projection on on the orthogonal which is of dimension one.",
            "So it's projection only on the the white code in it.",
            "So if you have a hyperplane computing the projection on the of a vector on it is really easy because you just have to compute the projection on its on its supplementary.",
            "Which is which is here the.",
            "The Y axis and you have only one schedule product to compute, right?",
            "Well, if I was to give you think about it, if you were in if you if I were to give you only two vector Destin that spanned the hyperplane here that spend this plane you have two dot products to compute there to find this projection.",
            "OK, so if you're in RN then a hyperplane is of dimension N -- 1.",
            "You may find a basis vector that's that is of Lexan minus one.",
            "But then you have.",
            "Minus one that products to compute to compute the projection of X on the hyperplane.",
            "It's easier to go to the supplementary which is of dimension only one and compute only one that product to compute the projection right?",
            "On top of that, you can see that because you have this nice property that the the square of the norm is too.",
            "Do some up then the distance.",
            "Of X to my eye popping each is is squared.",
            "Plus, the dot product of X on the Y coordinate squared.",
            "Is exactly the norm of X ^2.",
            "Now if I want to find this distance, I'll just take the norm of X and I minus the square of the coefficient on the orthogonal subspace, OK. And the last.",
            "The last thing that is written on this transparency, which is actually the first thing, is that because when H is in the neiper pain, then using its supplementary you may find an equation of of eight, which is really easy.",
            "Just said that any vectors in H is a vector that is orthogonal.",
            "To you when you is the vector that spans the supplimentary of GI pain, right?",
            "It's actually more exact."
        ],
        [
            "Be more difficult to say than than to see, right if you if you are in there are two an your hyperplane is.",
            "The line that is orthogonal to the vector U.",
            "Then you have this equation of.",
            "Therefore the hyperplane the distance.",
            "Yeah, actually that is.",
            "That was really wrong, right?",
            "Maybe you said Yes button, but it was really wrong.",
            "OK, so I'll come back to that anyways.",
            "The distance of each to your hyperplane is exactly given by the absolute value of the dot product of X with X with U and the projection.",
            "Is this one so?",
            "Here.",
            "Actually, this would be the norm of the projection.",
            "Of X.",
            "On your hyperplane there and the distance of X to your upper I printed is only this this distance there right?",
            "So here in my in my drawing there.",
            "This is the projection.",
            "Sorry.",
            "That's the projection of X.",
            "On my Oprah pain.",
            "And that would be the distance, an sorry.",
            "This makes here is the distance of X to my hyperplane, which is exactly the length of this dot product of the projection on the Y axis, right OK?",
            "Now if you have.",
            "Now we'll go from from subspaces to linear set of equations and matrices, so let's say."
        ],
        [
            "Assume that you have several hyperplanes and you want to find the index section of this desire place.",
            "Then you'll end up with, let's say you have MI per plans.",
            "Then you have em equations and so X is in the intersection.",
            "If those M equations are true at the same time.",
            "So it gives you a sense of EM linear equations.",
            "With the unknowns which are U DX1X2XD OK?",
            "An this set of linear equation may be written as a matrix vector product.",
            "Which is right here?",
            "OK. Now if you forget about."
        ],
        [
            "I prepare and then you just have really sets of linear equations.",
            "You may have a second term here which yields a non zero vector there OK."
        ],
        [
            "So the main matrices matrix in R. With MM Rosen D column D columns is an array of numbers of real numbers.",
            "It's actually an array made made of.",
            "I'm real vectors, or equivalently, the column vectors.",
            "And you can can see a matrix vector product.",
            "As in two ways you can see you time X as.",
            "A vector in Rd in RM, sorry.",
            "Yeah you times.",
            "So if X is in Rd and you is an M by the matrix then the vectors that you obtained by multiplying U by X is on is a vector in RM OK and you can see it as the combination of the column vectors of U.",
            "So that's the first writing that you have up there.",
            "It's a combination of the the columns of you with the coefficient being the.",
            "The coefficients that you had in X an equivalently, you could see it directly there.",
            "It's a vector in RM and each of the entries actually a dot product between X, your vector.",
            "He ran the the rule vectors of you which I wrote.",
            "You want you want to Umm with the the indices at the bottom.",
            "OK, so.",
            "Actually, you may also see a matrix as your as a representation of an operator that goes from our D2 RM OK. And the first writing tells you basically that the output of the operator applied to X is a vector that is in the span of the column vectors of you OK."
        ],
        [
            "So I write matrixes with timelines and D columns as in in in in a more compact way as you see on the right side there and what you can see is that basically the set of matrices in with emeralds and the current is a vector space OK.",
            "The addition is obtained by summing every entries of the two matrices and the product by.",
            "The product by a scalar is obtained.",
            "I haven't, I didn't write it there.",
            "The product by a real real number is multiplying each entry of the matrix by the Screener number.",
            "You can also define a matrix product.",
            "For which the elements are returned there, so it's the basic extension of the matrix vector product.",
            "You had several vectors.",
            "You can see B as a set of federal column vector, and each column of the product AB column I of the product AB is a times the ice vector of be OK. And you can also transpose matrices, so if you flip the dimensions of the matrix and you flip all the elements inside, then you get the transpose of the matrix."
        ],
        [
            "Um?",
            "When the matrices are square, so there is the same number of rows and columns, then you may find matrices that are invertible for the product.",
            "Some some matrices are easier to understand, for example diagonal matrices.",
            "So if you multiply a vector by a diagonal matrix, then you get the entry.",
            "The entries of the the.",
            "The product is just.",
            "I read that.",
            "So this is my diagonal matrix.",
            "X is A is a vector in Rd and so the.",
            "The the product DX is the vector.",
            "With coordinates from the 1X1 on the 2X2 of two Lambda D. XD.",
            "Other matrices that are easier, easy to deal with are upper or lower triangular matrices.",
            "Do those two kinds of matrices.",
            "Also, symmetric matrix matrices are are of importance.",
            "An unitary matrix matrices.",
            "Which are matrices for which the transpose times the matrix itself?",
            "Is the identity matrix where the identity is a diagonal matrix is only once under diagonal."
        ],
        [
            "So the big question is, can you invert?",
            "When can you invert a matrix?",
            "And so you can really easily see that if it is diagonal or if D is diagonal, then inverting this system.",
            "The X is equal to Y.",
            "So I give you ynd an.",
            "Your goal is to find X.",
            "It's really easy as long it's it's easy and it's possible if and only if all the coefficients from that you are non zero.",
            "OK now you can see for lower or upper triangular system the same condition apply.",
            "You can invert the matrix in the family if the product of the diagonal elements is nonzero.",
            "And so it's let's take a lower triangular matrix.",
            "You can invert it easily, provided that the diagonal elements are non 0.",
            "So the system up there is equivalent.",
            "Is this linear system here?",
            "If you want to invert this matrix, you start from substitute for finding X one by dividing by the diagonal defer diagonal element.",
            "And."
        ],
        [
            "And once you have X1 then you can."
        ],
        [
            "Maybe next one in here and solve for X2 so that that would be the next step here."
        ],
        [
            "And once you have X one X2, then you can plug them in to find X3 an you see you see that you can go from this first to this last equation, replacing at every step all the previous coordinates X one to escape.",
            "And then.",
            "As you have in front of you and you.",
            "Unknown, XC.",
            "You have a diagonal coefficient which is non zero.",
            "Then you're always going to be able to solve for escape OK.",
            "So if you have diagonal or or or or trigger on triangular matrices, you just need to know if the product of the diagonal."
        ],
        [
            "I meant is non 0.",
            "An the generalization to that is, is the determinant.",
            "So let's see for General Matrix A in R2.",
            "With two lines into columns is ABCD.",
            "Then you can easily check that a is invertible if and only if AD minus BC is non zero and that the inverse is the one that I gave you up there.",
            "Uh, an in general?",
            "If you want to know if it if a matrix or dividing matrix is invertible, you have to find its determinant which is written this way.",
            "The determinant may be, for example defined recursively by saying that the determinant of a matrix.",
            "Is a sum of determinant of demy de minus of.",
            "Is this some of the smaller determinants?",
            "So that's what's written up there.",
            "Let's say we take J is equal to 1.",
            "Then this formula says the determinant is the sum of AI.",
            "One times this coefficients their AI one then.",
            "Is do those the coefficients of the first column of the Matrix and the coefficient that you you put here is actually the determinant of the matrix that is obtained from a without the first column an with without the rule number I OK.",
            "So this thing here is a determinant of a D -- 1 by D -- 1 matrix that that is obtained from a by substracting 2 lines and columns.",
            "But yeah.",
            "OK, so definitely there's a formula from the invert for the inverse of a matrix using the determinants, but you will never use it.",
            "Why?",
            "Why is that?",
            "Because you'd have to find all this this this matrix.",
            "This matrix of determines there an.",
            "It's not really practical so.",
            "The next thing you wanna know is."
        ],
        [
            "Is.",
            "Maybe other ways to find an hour, how to invert a matrix?",
            "OK, to do so.",
            "Then we look inside the matrix and we try to decompose it into an easier matrix.",
            "Or maybe what I should say is that when you look for eigenvalues.",
            "And then you would be looking for.",
            "Uh.",
            "An equivalent diagonal matrix to your matrix A.",
            "So in terms of an up, if you think of a in terms of an operator, then a you're dividing matrix is an operator that goes from Audi to Ord.",
            "And you may find a nice basis where this operator is actually diagonal.",
            "If you do so, then it's really easy to invert the matrix on this basis, right?",
            "Well, it's not always possible, so the operator is.",
            "It doesn't necessarily have.",
            "They are going to form on some basis, but you may want to to get used to get close to that.",
            "So the eigenvalues of a matrix or.",
            "The skaters Lambda such that there there is a vector and non zero vector such that that a Lambda Evie is equal to Lambda V. The eigenvalues are going to be the values that you put on the the diagonal of the equivalent operator that is diagonal OK and the the visa.",
            "So the eigenvector are going to be the vector that you put in the in the new basis on which the operator is diagonal.",
            "Equivalently, you say you can see that Lambda is an eigenvalue if and only if the determinant of the matrix A minus Lambda D identity is 0.",
            "Which means that a minus somebody end entity is not invertible.",
            "OK, Ann.",
            "Either way.",
            "Yeah, you say that visa.",
            "She is the eigenvector associated to the eigenvalue.",
            "If even run DV OK. Diagonal matrices of four diagonal matrices.",
            "Obviously the eigenvalues are the diagonal element, and it's not true for triangular matrices, right?",
            "Um, Zero is an eigenvalue if and only if it is not invertible, and we think that is diagonalizable if there exists a basis of eigenvector which can be written as if you put your, your eigenvectors are columns as columns of a matrix P. Then the products P * D * P -- 1 is actually exactly equals 2, eight or.",
            "If you put if you have this P transferred there, you can see that AP is a critic is exactly P * D. And so if you think of a column vector of of of P here 8 times this column vector is exactly the same column vector of this matrix there and as D is diagonal.",
            "This means that eight times this vector is excited is is exactly scared that is the diagonal term times the same vector, so.",
            "This this rating there is nothing more than saying that the is the is the matrix with column vectors are the atigun vectors of a OK."
        ],
        [
            "No, there's a special.",
            "There's a special thing that happens when.",
            "Your matrix is symmetric.",
            "Is that if your matrix symmetry, then you're guaranteed to find another normal basis of eigenvectors.",
            "So not only are you guaranteed to that the matrix is that they're going to realizable, but also that you can find an ortho normal basis, then that is a basis of eigenvectors, so this can be written as a is P * D * P transpose, where P is a unitary matrix.",
            "In addition, we say that a symmetric Smith symmetric matrix is semidefinite positive if the dot product between X&Y axes always positive for any vector.",
            "OK. Then the eigenvalues of the Matrix A are positive and."
        ],
        [
            "That would be true.",
            "For sorry for any diagonal matrix for which the diagonal elements are positive, non negative and for any matrix that can be written as B, transpose B for another matrix OK."
        ],
        [
            "And the matrix is set to be definite positive.",
            "This care product is zero only if X is itself is 0.",
            "Then it means that.",
            "At it this implies that AR as."
        ],
        [
            "Only strictly positive I can values again the diagonal matrix with positive entries that are not serious then is definite positive an if.",
            "If it can be written as a product B, transpose B and is invertible, then it's definite positive.",
            "Those matrices are important first of all, because you can diagonalize.",
            "Diagonalize them on.",
            "Just a normal basis, and if in addition there definite positive, then you can verify that this Form X scare the dot product if you use the usual dot product San X&X then it's defining and you dot product which I I wrote to see with us with an index A and this form.",
            "Here is a new Note dot product.",
            "It gives you a new norm on H. And so you can think of everything as your you can think of your vector space.",
            "Each with with this new number.",
            "OK."
        ],
        [
            "Now, if you don't have a symmetric symmetric symmetric matrix.",
            "It may not be diagonalizable.",
            "It may also be that your matrix is not even square, but using the.",
            "Using for example, so let's.",
            "Let's say B is an M by the matrix and using for example B transpose B or B * B transpose, which we've seen are symmetric semidefinite positive matrices.",
            "Then you can find an interesting decomposition of B which is the singular value decomposition.",
            "So here at the top you can see that I will that be transpose B.",
            "Is there gonna risible on the North?",
            "A normal basis in Audi?",
            "And that's B. Transpose B is V times Delta 1 * V transpose.",
            "You can also write the same thing for B * B transpose in RM Now.",
            "You can.",
            "2221 can actually show then the two diagonal matrices that one and D2 have exactly the same nonzero elements.",
            "OK, there there are two matrices which are which are diagonal but with different dimensions.",
            "But they have exactly the same 10 elements, and you can verify that B is equal is exactly equal to U * V * V transpose.",
            "Where where D is the diagonal matrix made of these non zero elements that you've found in D1 or D2.",
            "U is the author normal basis of RM that diagonalizes B * B transpose and V is the the the author normal basis in Rd that diagonalizes B transpose B. OK. Now, once you have that, what what can you what?",
            "Obviously the SVD decomposition of B transpose.",
            "This is the.",
            "Is similar an what you should think you should think about the SVD decomposition as a way of explaining B in terms of a diagonal matrix an as an operator be sense vector in RD2 vectors in RN?",
            "And if you change from the Canonical basis to the basis defined by V GND and from the Canonical basis to the base is defined by U in RM, then you then you operator that is defined by B is exactly exactly diagonal, which means that.",
            "Um, if I right.",
            "So if be my operator that.",
            "To a vector in our D sends it to.",
            "BX URM.",
            "So this decomposition are in the Canonical basis of Audion RN.",
            "OK, now if you write.",
            "Switch.",
            "So V is made of.",
            "D vectors in Rd, which are which former North so normal basis of Rd OK.",
            "If X.",
            "So you can write X as in decomposition.",
            "It's decomposition.",
            "On this basis I write this as X~ sorry.",
            "So it is one X2 daddy Times V. VI OK.",
            "Sorry.",
            "And then you also have a basis you.",
            "You won.",
            "2.",
            "Umm?",
            "In aram.",
            "Then my operator applied to X, can be easily written as the sum.",
            "Of the.",
            "I'm.",
            "Of the Lambda I * X~ I times um.",
            "You I. OK. And that's because you I. Hi operator applied to UI is exactly Lambda.",
            "I'm Lambda I times VI.",
            "So writing this is saying again that be my operator is diagonal from this basis to this basis.",
            "And so you can see that B is much easily computable from this basis to this one.",
            "Then from the Canonical basis OK.",
            "So now that you."
        ],
        [
            "Have all this decomposition.",
            "What did I write?",
            "OK, yeah, in terms of vocabulary you can say that.",
            "This this writing is the singular value decomposition of B and that the value Lambda 12, Lambda key or its singular values.",
            "OK now, um.",
            "There are other ways."
        ],
        [
            "Simplify matrices.",
            "Anne.",
            "I I wrote a few of them here the Lu factorization, the Cholesky decomposition or the QR decomposition an their important because they help you invert matrices easily, so.",
            "You can see that for the Edu factorisation.",
            "For example, if you can write the metrics A as a product as a product S L * U where early is a lower triangular matrix and you is an upper triangular matrix, then solving for the inverse of is going to be easy.",
            "And why is that?",
            "Because if you want to solve a is equal to BB should be a.",
            "A smaller with a small letter there B is a vector in RM in Rd, sorry.",
            "Then X = B may be solved in two steps.",
            "The first one is to solve for.",
            "I think it's wrong, OK?",
            "Help you.",
            "Yeah, the first the first step is to solve for LLZ is equal to be where you look for the and you have be your your target vector.",
            "An alley which is a lower triangular matrix, so it's easy to invert and the second step is to solve for UX is equal to Z where you this time is a is an upper triangular matrix and it's easy to solve as well.",
            "So the condition for it to be.",
            "To have a an unique factorization is that its diagonal is dominant, which is the condition that is on the upper right side there.",
            "Um?",
            "For symmetric semidefinite positive matrices there.",
            "There's another decomposition which is called assure as he decomposition, which basically says that if you have a symmetric positive matrix, then it may be seen as as the square of another matrix.",
            "Now the last decomposition here is the QR decomposition.",
            "If you have a matrix with emeralds and the column, you may.",
            "You may decompose it as the product of a unitary matrix.",
            "In RMN an upper triangular matrix or so.",
            "Um, here, assuming that it would be invertible, so M is equal to D. Then you have a system with here triangular system which is easy to solve.",
            "And here a change of basis because two is unitary.",
            "I, I think that's what I wanted to say about vector spaces and linear algebra."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morning.",
                    "label": 0
                },
                {
                    "sent": "So, um, it seems that I should say a few words about myself before we begin, so I'm a researcher in the math lab, and I do research in image processing in applied math.",
                    "label": 0
                },
                {
                    "sent": "So the goal of today is to give you some.",
                    "label": 0
                },
                {
                    "sent": "Basic necessary mathematics that you may be using this week or not.",
                    "label": 0
                },
                {
                    "sent": "Everything is going to be starting from the real beginning, so you probably know everything I'm going to say today, so I apologize if you know everything.",
                    "label": 0
                },
                {
                    "sent": "If you don't, you can ask questions and.",
                    "label": 0
                },
                {
                    "sent": "Well, let's begin.",
                    "label": 0
                },
                {
                    "sent": "So we'll talk about several.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "The the direction is divided into 3 parts.",
                    "label": 0
                },
                {
                    "sent": "The first part is going to talk about linear algebra and vector spaces and this stuff.",
                    "label": 1
                },
                {
                    "sent": "In the second part, will talk about probabilities, and in the third partner I'll talk a little bit about optimization.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne will start with linear algebra, so I'm assuming that your preferred vectors phase is mine and is auto after is pretty.",
                    "label": 0
                },
                {
                    "sent": "He's pretty nice to deal with, right?",
                    "label": 0
                },
                {
                    "sent": "In R2 you take 2 vectors.",
                    "label": 0
                },
                {
                    "sent": "You envy and you're able.",
                    "label": 0
                },
                {
                    "sent": "You are able to send them by just adding V on top of you here.",
                    "label": 0
                },
                {
                    "sent": "I'm getting a vector there.",
                    "label": 0
                },
                {
                    "sent": "You able to do other stuff like multiplying you by a scalar so you can multiply you by two and you get a vector that is twice as long as you.",
                    "label": 0
                },
                {
                    "sent": "And then you can do a bunch of stuff.",
                    "label": 0
                },
                {
                    "sent": "Actually you can define things that are lies.",
                    "label": 0
                },
                {
                    "sent": "And we can define what's the projection of W on this line.",
                    "label": 0
                },
                {
                    "sent": "And that would be this vector there.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "What else?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So it's pretty easy to work with our two an.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "The first generalization of our trees are an with an any any integer and our three, for example, is your usual space.",
                    "label": 0
                },
                {
                    "sent": "What you have in our three is that you can sum of vectors.",
                    "label": 0
                },
                {
                    "sent": "You can multiply them by a scalar.",
                    "label": 0
                },
                {
                    "sent": "And you can also.",
                    "label": 0
                },
                {
                    "sent": "Actually you can write any vector in RN as a sum of N vectors.",
                    "label": 0
                },
                {
                    "sent": "So I wrote them rum E1E N which are the basic vector basis there.",
                    "label": 0
                },
                {
                    "sent": "An you can figure out that any element in our end is a sum of of those vectors.",
                    "label": 0
                },
                {
                    "sent": "I'll give you two other example of vector spaces.",
                    "label": 0
                },
                {
                    "sent": "The segment one here is the set of solution of a second order differential equation that is homogeneous.",
                    "label": 0
                },
                {
                    "sent": "So if you take functions that go from auto or that are twice differentiable and you take the set of these functions that verify this equation, then you can show that it a vector space that it's stable by addition that if F is in the vector space, then it suppose it is also in the vector space.",
                    "label": 0
                },
                {
                    "sent": "And that you can multiply them by a scalar.",
                    "label": 0
                },
                {
                    "sent": "And likewise for RN.",
                    "label": 0
                },
                {
                    "sent": "You can write any function that is in SQI will sum of two.",
                    "label": 0
                },
                {
                    "sent": "Two special functions, the cosine function and assign functions.",
                    "label": 0
                },
                {
                    "sent": "Anne, this decomposition there is actually unique.",
                    "label": 0
                },
                {
                    "sent": "They.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are spaces for which you cannot actually decompose any.",
                    "label": 0
                },
                {
                    "sent": "The elements on a finite set of vectors and those would be called infinite dimensional vector spaces and an example of that is L2 of our so L2 of R is the space of square integrable functions.",
                    "label": 0
                },
                {
                    "sent": "It's stable by addition and by multiplication by a real number.",
                    "label": 0
                },
                {
                    "sent": "And there's something which.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I haven't.",
                    "label": 0
                },
                {
                    "sent": "That I have not written for the other spaces, you can actually define a dot product and a norm.",
                    "label": 0
                },
                {
                    "sent": "The DOT product of two functions in in L2 is the integral of their product and it's equivalent to your usual dot product in our two here, which would be if you is the vector U1U2 and V is V1V2 then your dot product.",
                    "label": 0
                },
                {
                    "sent": "Is the sum U1V1 plus U2V2?",
                    "label": 0
                },
                {
                    "sent": "So that product in there is two years and norm which is the square root of the integral of the square of F. And it's gonna be useful because you you define with with this dot product and Norm, you're going to be able to define things that are orthogonal.",
                    "label": 0
                },
                {
                    "sent": "An decomposing vectors on sets of orthogonal thing is going to make your life easier.",
                    "label": 0
                },
                {
                    "sent": "And the next thing you have in in two of our which is important is this closeness.",
                    "label": 0
                },
                {
                    "sent": "So if you take a sequence of function F and that converges to F. For which the norm of the difference converges to 0, then F the limit function is easier 2.",
                    "label": 0
                },
                {
                    "sent": "It's not always true, right sets are not always closed if you take for example.",
                    "label": 0
                },
                {
                    "sent": "If your set is a.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the interval between zero and one but not containing you or one.",
                    "label": 0
                },
                {
                    "sent": "Then you can find.",
                    "label": 0
                },
                {
                    "sent": "If you take XN.",
                    "label": 0
                },
                {
                    "sent": "The sequence which is 1 / N for each N, then XN goes to 0.",
                    "label": 0
                },
                {
                    "sent": "But zero is not in your in your set S, so which means that you may have limits of things that are all in S, that for which the limit is not in S, and that's when S is not closed.",
                    "label": 0
                },
                {
                    "sent": "OK, well, two of our of this disability that if you have a convergent sequence, you're sure that the limit is in L2 steel, and that's important.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll go to her.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For more different definition of vector spaces, which is just summarizing what we've said so far.",
                    "label": 0
                },
                {
                    "sent": "If you have a set S, it's called a vector space.",
                    "label": 1
                },
                {
                    "sent": "If the addition is a stable operation.",
                    "label": 1
                },
                {
                    "sent": "If the addition is Additionally commutative and distributive and associative.",
                    "label": 0
                },
                {
                    "sent": "SMS Seven element 40 addition.",
                    "label": 1
                },
                {
                    "sent": "An every element of Asmus must be invertible for the addition, so basically you have an opposite minus.",
                    "label": 0
                },
                {
                    "sent": "An operation which is a minus.",
                    "label": 0
                },
                {
                    "sent": "In addition, if you want to read vector space, you need to have S, which is stable by multiplication by any real number.",
                    "label": 0
                },
                {
                    "sent": "Or any scalar I wrote?",
                    "label": 1
                },
                {
                    "sent": "Any scalar Lambda?",
                    "label": 0
                },
                {
                    "sent": "And the multiplication by a scalar must have the properties that it does.",
                    "label": 0
                },
                {
                    "sent": "It is associative and distributive over the addition.",
                    "label": 0
                },
                {
                    "sent": "So all of this is basically just saying you have an addition.",
                    "label": 0
                },
                {
                    "sent": "If you add two vectors, it's still a vector.",
                    "label": 0
                },
                {
                    "sent": "You have the opposite of a vector that exists, and the multiplication of a vector by a scalar yields a vector as well.",
                    "label": 0
                },
                {
                    "sent": "Um, so useful thing when you want to see what's going on on Elsa vector spaces to divide it into smaller spaces, and so subspaces are basically subsets of S which have the same properties, so they are stable by addition.",
                    "label": 0
                },
                {
                    "sent": "They're stable by multiplication by a scalar, OK?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, there are subspaces that have.",
                    "label": 0
                },
                {
                    "sent": "That help you decompose your vector space easily and those would be called supplementary subspaces.",
                    "label": 0
                },
                {
                    "sent": "So if I take out two again.",
                    "label": 0
                },
                {
                    "sent": "Let's see my subspace, as is the one that is defined by my my first vector vector U one here.",
                    "label": 0
                },
                {
                    "sent": "Then if you take any other vector that is not collinear to you, and so that that is not in the same direction as you want, for example this one, there V. We use subspace, which is the the set of all the the.",
                    "label": 0
                },
                {
                    "sent": "The vectors that are proportional to V, those two subspaces there, so I'll call this this line F1 and F2 are actually supplementary subspaces, which means that for any.",
                    "label": 0
                },
                {
                    "sent": "Find some that you having or two.",
                    "label": 0
                },
                {
                    "sent": "Then you may write it as a sum of.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So if I do the projections there.",
                    "label": 0
                },
                {
                    "sent": "So your vector accesses can be written as.",
                    "label": 0
                },
                {
                    "sent": "X One times the first vector U 1 + X two.",
                    "label": 0
                },
                {
                    "sent": "Times the second vector V. And actually, if 2 two subspaces are set to be supplementary, if such a decomposition always exists, OK?",
                    "label": 0
                },
                {
                    "sent": "So up there actually, in the notations out there it would.",
                    "label": 0
                },
                {
                    "sent": "This thing would be X of F1 and this thing would be X of F2.",
                    "label": 0
                },
                {
                    "sent": "Now this subspace here F1 is just generated by U-1 an.",
                    "label": 0
                },
                {
                    "sent": "Similarly if you take a family of vectors in S, you can generate a vector subspace from this in these vectors.",
                    "label": 1
                },
                {
                    "sent": "To take X1 and 2X NB to BN vectors of South, then the span of X1 XN is the sets of finite linear combinations that you can make from X1 to XN.",
                    "label": 0
                },
                {
                    "sent": "So that's the thing up there.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have a.",
                    "label": 0
                },
                {
                    "sent": "So the notion of weather.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, let's see the.",
                    "label": 0
                },
                {
                    "sent": "The family of the exercise is set to be linearly independent if decomposition of NEY in the subspace span of X1 XN as a sum of.",
                    "label": 0
                },
                {
                    "sent": "Dexi is unique.",
                    "label": 0
                },
                {
                    "sent": "And Conversely, what did I say if the yeah OK. Alright, so basically.",
                    "label": 0
                },
                {
                    "sent": "It's easy to expand vector.",
                    "label": 0
                },
                {
                    "sent": "Why that is in the span of X1, XN as a sum of the Lambda IXI.",
                    "label": 0
                },
                {
                    "sent": "Now this decomposition maybe maybe unique or not, and it's unique if and only if your vectors.",
                    "label": 0
                },
                {
                    "sent": "Are linearly independent?",
                    "label": 0
                },
                {
                    "sent": "So maybe I write again an example in our in our two.",
                    "label": 0
                },
                {
                    "sent": "Now we've just seen that if you take you.",
                    "label": 0
                },
                {
                    "sent": "And we need to be, not.",
                    "label": 0
                },
                {
                    "sent": "Not in the same direction.",
                    "label": 0
                },
                {
                    "sent": "Then you can write any X.",
                    "label": 0
                },
                {
                    "sent": "In S, as is as a sum.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Well, actually if you add another vector, a third vector maybe?",
                    "label": 0
                },
                {
                    "sent": "Let's see this one here W. And it's still true that for any vector here in R2, you can write it as a sum of.",
                    "label": 0
                },
                {
                    "sent": "Our first component in you plus a second component in V + 1/3 component in W. OK, but now this this this scalars.",
                    "label": 0
                },
                {
                    "sent": "There are not unique anymore and the reason they're not unique is because you can find if you add up.",
                    "label": 0
                },
                {
                    "sent": "So let's say.",
                    "label": 0
                },
                {
                    "sent": "If you add up you.",
                    "label": 0
                },
                {
                    "sent": "And a smaller proportion of V. And W maybe twice time times W, then you get 0 vector.",
                    "label": 0
                },
                {
                    "sent": "So each time because you have this linear combination.",
                    "label": 0
                },
                {
                    "sent": "You press, let's say this thing is.",
                    "label": 0
                },
                {
                    "sent": "Maybe 1/3?",
                    "label": 0
                },
                {
                    "sent": "1/3 of V. 2 W is equal to the nerve vector.",
                    "label": 1
                },
                {
                    "sent": "Then for any X in R2.",
                    "label": 0
                },
                {
                    "sent": "Then um.",
                    "label": 0
                },
                {
                    "sent": "If you take those skaters skaters here, I'm sorry those curls here Xux vianex W an.",
                    "label": 0
                },
                {
                    "sent": "If you add 2112 X yuan 1/3 two XV and two 2X W then you get you'll get exactly the same vector again in our tool.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Forget it, yes there OK?",
                    "label": 0
                },
                {
                    "sent": "So basically because you have a combination with non zero coefficient that makes these things you then this this writing there is not unique for linearly independent vectors.",
                    "label": 0
                },
                {
                    "sent": "There's no such combination an that makes that the decompositions are unique and that also gives you the dimension of the vector space that is generated by X1 to XN and this dimension is exactly.",
                    "label": 0
                },
                {
                    "sent": "The The Cardinal of the largest family of vectors that is linearly independent in the subspace.",
                    "label": 1
                },
                {
                    "sent": "OK so already is of dimension D or two is of dimension 2.",
                    "label": 0
                },
                {
                    "sent": "Obviously our set of solutions to a second order or marginalized differential equations.",
                    "label": 0
                },
                {
                    "sent": "The one we've seen just before is of dimension 2 as well.",
                    "label": 0
                },
                {
                    "sent": "And as I said, L2 of our.",
                    "label": 0
                },
                {
                    "sent": "It is actually of infinite dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, some subspaces that may be useful, or hyper planes and their define as subspaces of of your origin vector space, which have a supplementary of dimension exactly one.",
                    "label": 1
                },
                {
                    "sent": "OK, so in Rd Annie subspace of dimension D -- 1 is a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, the reason why we define diaper prints through their supplementaries is if you have a.",
                    "label": 0
                },
                {
                    "sent": "A set with infinite dimension at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Then the operating still has an infinite dimension, so you can't really say the hyperplane as dimension minus one.",
                    "label": 0
                },
                {
                    "sent": "The dimension of the original space, right?",
                    "label": 0
                },
                {
                    "sent": "That's what you should think about.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now basically a basis of a of a subspace is generative and linearly independent family.",
                    "label": 1
                },
                {
                    "sent": "In this subspace they all have exactly the same Cardinal.",
                    "label": 0
                },
                {
                    "sent": "And the yield unique decomposition for any X in this subspace.",
                    "label": 0
                },
                {
                    "sent": "So in Rd we've seen that the basis vectors there are.",
                    "label": 0
                },
                {
                    "sent": "Those vectors there are do you the basis in in L2 of the zero 2\u03c0.",
                    "label": 0
                },
                {
                    "sent": "So the set of functions that are square into integrable in this interval 002 Pi.",
                    "label": 0
                },
                {
                    "sent": "Then you can show that the set of function cosine and T & empty for all possible integers.",
                    "label": 0
                },
                {
                    "sent": "Ann is a basis as well and so that will tell you that if you have a square integrable function on zero 2\u03c0, then you can extract decompose it as such a sum where the M&BM are unique coefficients OK. And so that enables you to see a function as a sum of OK signs here.",
                    "label": 0
                },
                {
                    "sent": "Of cosines and sines for which you know that from the properties right those those are simple functions which you can see really easily.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's next?",
                    "label": 0
                },
                {
                    "sent": "And in our two, for example, we talked about projection just before and we also did.",
                    "label": 0
                },
                {
                    "sent": "We talk about Dot products, not yet.",
                    "label": 0
                },
                {
                    "sent": "So actually the next question is if you have such a decomposition, how are you going to find the M and BMS right?",
                    "label": 0
                },
                {
                    "sent": "How are you going to find the coefficients?",
                    "label": 0
                },
                {
                    "sent": "In front of your business vectors.",
                    "label": 0
                },
                {
                    "sent": "And so an easy way to do that is is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you are Victor.",
                    "label": 0
                },
                {
                    "sent": "Space as on top of having the addition and multiplication by the scare, also has a dot product.",
                    "label": 0
                },
                {
                    "sent": "So the DOT product in Audi is the usual dot product we've seen before and it's linked to the Euclidean norm in already.",
                    "label": 1
                },
                {
                    "sent": "And you can see the dot product as a measure of how.",
                    "label": 0
                },
                {
                    "sent": "Of what is the angle between X and two 2 vectors X&Y?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have a vector space with the dot product then any subspace of it as a unique orthogonal supplementary.",
                    "label": 1
                },
                {
                    "sent": "Which means that the two sets F an it's orthogonal there.",
                    "label": 0
                },
                {
                    "sent": "Where, which means that the discipline entry has the special properties that if X is in F, ANAN why is in the supplementary?",
                    "label": 0
                },
                {
                    "sent": "Then you you automatically have that the dot product is equal to 0, or that two elements of F and F1 or orthogonal.",
                    "label": 0
                },
                {
                    "sent": "OK, so the dot product being 0.",
                    "label": 0
                },
                {
                    "sent": "You should think about this as the two vectors being orthogonal OK and so.",
                    "label": 0
                },
                {
                    "sent": "Now that you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you have a unique supplementary, then what you can do is to easily decompose your vectors.",
                    "label": 0
                },
                {
                    "sent": "Annual have relations between the norm of the different parts of your vectors.",
                    "label": 0
                },
                {
                    "sent": "So that's what's written here.",
                    "label": 0
                },
                {
                    "sent": "If you have two supplementary spaces, then the norm of X is squared is equal to the norm of it spot on the subspace F plus it's spot on the orthogonal space, which is only saying that, for example.",
                    "label": 0
                },
                {
                    "sent": "The normal of this vector is the square of this length plus this ranks right.",
                    "label": 0
                },
                {
                    "sent": "This length is.",
                    "label": 0
                },
                {
                    "sent": "This would be the the projection of a on your subspace F. This would be the projection of a on your subspace effort toggle, and so the norm of this vector squared with this vector squared is the norm of this one.",
                    "label": 0
                },
                {
                    "sent": "So that's your Pythagorean theorem, right?",
                    "label": 0
                },
                {
                    "sent": "OK so I I just reminding you what a norm is up there and I'm not going to tell you again what it is you.",
                    "label": 0
                },
                {
                    "sent": "So again, X&Y also also if and only if their dot product is zero and.",
                    "label": 0
                },
                {
                    "sent": "So the last definition I wanted to give is what is a Hilbert space?",
                    "label": 0
                },
                {
                    "sent": "So a Hilbert space is a vector space where you have a dot product and Additionally the space is closed for the norm that is yielded by the DOT product.",
                    "label": 1
                },
                {
                    "sent": "So again at two of our research space, right?",
                    "label": 0
                },
                {
                    "sent": "It's a vector space, the dot product of two function is the integration of the product of the two functions.",
                    "label": 0
                },
                {
                    "sent": "And this year is the norm, which is the norm, which is the integration of F squared.",
                    "label": 0
                },
                {
                    "sent": "The square root of the integral of F squared.",
                    "label": 0
                },
                {
                    "sent": "An if you again the fact that the space is closed means that if you have a sequence of vector in each that converges, then you're sure that the limit is in each as well.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once you have the authoritie, you may look for also going to basis.",
                    "label": 0
                },
                {
                    "sent": "An obviously your usual basis in RG is an Arsenal in the North, so normal basis.",
                    "label": 1
                },
                {
                    "sent": "The basis is also normally if the dot product between two and two vector basis is is either one.",
                    "label": 0
                },
                {
                    "sent": "If it's the same element or zero if it's two different elements and what happens is.",
                    "label": 0
                },
                {
                    "sent": "For example, in L2 of zero 2\u03c0, then we've seen that this is a basis, and it's actually an ortho normal.",
                    "label": 1
                },
                {
                    "sent": "It's absolutely sorry.",
                    "label": 1
                },
                {
                    "sent": "After Google basis, I'm not really sure I know the coefficients there, but in any case it's really nice with an orthonormal basis.",
                    "label": 0
                },
                {
                    "sent": "Is that if you want to find the coefficient of the decomposition, it's really easy.",
                    "label": 1
                },
                {
                    "sent": "You just have to take the dot products so if you and we form an orthonormal basis of R2 here.",
                    "label": 0
                },
                {
                    "sent": "So do you would be envy would be 2 vector that orthogonal and that have length one exactly so for example.",
                    "label": 0
                },
                {
                    "sent": "You could have.",
                    "label": 0
                },
                {
                    "sent": "Let me rephrase that.",
                    "label": 0
                },
                {
                    "sent": "You have like links one and it's orthogonal there as well.",
                    "label": 0
                },
                {
                    "sent": "Then the coefficients of X on this basis, or simply the dot products Xu.",
                    "label": 0
                },
                {
                    "sent": "And exit.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And you also have this nice property coming from the fact that.",
                    "label": 0
                },
                {
                    "sent": "The norm of X square is the sum.",
                    "label": 0
                },
                {
                    "sent": "Of of its two projections.",
                    "label": 0
                },
                {
                    "sent": "Then you can write this in terms of the coefficients actuan, XG and so.",
                    "label": 0
                },
                {
                    "sent": "This year is that the norm of X square is just the sum of the square root of the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Is there right there?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's why it's written up there in the point.",
                    "label": 0
                },
                {
                    "sent": "Number two, OK?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if you have a space with the DOT product and you consider an hyperplane, then it's easy to see what's.",
                    "label": 0
                },
                {
                    "sent": "For example, the equation of the hyperplane or the distance from a point to the disciple plane or.",
                    "label": 1
                },
                {
                    "sent": "The projection of any point to decipher on this hyperplane an.",
                    "label": 0
                },
                {
                    "sent": "The reason is that hyperplanes have simplement areas of of dimension exactly 1.",
                    "label": 0
                },
                {
                    "sent": "So not too if you if I want to draw a significant picture for this one, that better drew something in all three which might not be really nice because I don't grow really well.",
                    "label": 0
                },
                {
                    "sent": "But anyways you have to think about it.",
                    "label": 0
                },
                {
                    "sent": "The reason I'm doing that in all three is that because in R2.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional spaces or lines that go to go to zero.",
                    "label": 0
                },
                {
                    "sent": "And there also hyperplanes, so you won't see the difference between the hyperplane and it's supplementary.",
                    "label": 0
                },
                {
                    "sent": "It's easier in R3.",
                    "label": 0
                },
                {
                    "sent": "OK, so in our three I'm going to consider the hyperplanes which is this hyperplane there.",
                    "label": 0
                },
                {
                    "sent": "Which is, uh, the pain with the.",
                    "label": 0
                },
                {
                    "sent": "Let's see XY.",
                    "label": 0
                },
                {
                    "sent": "The plane studied.",
                    "label": 0
                },
                {
                    "sent": "This is defined by Y is equal to zero OK?",
                    "label": 0
                },
                {
                    "sent": "So if you take any vector.",
                    "label": 0
                },
                {
                    "sent": "X here.",
                    "label": 0
                },
                {
                    "sent": "Nina in R3.",
                    "label": 0
                },
                {
                    "sent": "Then you made a composite really easily as a sum of something which is.",
                    "label": 0
                },
                {
                    "sent": "Just perpendicular to this plane.",
                    "label": 0
                },
                {
                    "sent": "But it's something that's in the DX ZZ plane, right?",
                    "label": 0
                },
                {
                    "sent": "And those two vectors there are signal.",
                    "label": 0
                },
                {
                    "sent": "Ann, what's really easy to see is that this projection here of X2 on on to your hyperplane is exactly what it's exactly.",
                    "label": 0
                },
                {
                    "sent": "So the projection.",
                    "label": 0
                },
                {
                    "sent": "Of X2 on to your iPad screen XD is exactly X minus this projection on on the orthogonal which is of dimension one.",
                    "label": 0
                },
                {
                    "sent": "So it's projection only on the the white code in it.",
                    "label": 0
                },
                {
                    "sent": "So if you have a hyperplane computing the projection on the of a vector on it is really easy because you just have to compute the projection on its on its supplementary.",
                    "label": 0
                },
                {
                    "sent": "Which is which is here the.",
                    "label": 0
                },
                {
                    "sent": "The Y axis and you have only one schedule product to compute, right?",
                    "label": 0
                },
                {
                    "sent": "Well, if I was to give you think about it, if you were in if you if I were to give you only two vector Destin that spanned the hyperplane here that spend this plane you have two dot products to compute there to find this projection.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're in RN then a hyperplane is of dimension N -- 1.",
                    "label": 0
                },
                {
                    "sent": "You may find a basis vector that's that is of Lexan minus one.",
                    "label": 0
                },
                {
                    "sent": "But then you have.",
                    "label": 0
                },
                {
                    "sent": "Minus one that products to compute to compute the projection of X on the hyperplane.",
                    "label": 1
                },
                {
                    "sent": "It's easier to go to the supplementary which is of dimension only one and compute only one that product to compute the projection right?",
                    "label": 0
                },
                {
                    "sent": "On top of that, you can see that because you have this nice property that the the square of the norm is too.",
                    "label": 1
                },
                {
                    "sent": "Do some up then the distance.",
                    "label": 0
                },
                {
                    "sent": "Of X to my eye popping each is is squared.",
                    "label": 0
                },
                {
                    "sent": "Plus, the dot product of X on the Y coordinate squared.",
                    "label": 0
                },
                {
                    "sent": "Is exactly the norm of X ^2.",
                    "label": 0
                },
                {
                    "sent": "Now if I want to find this distance, I'll just take the norm of X and I minus the square of the coefficient on the orthogonal subspace, OK. And the last.",
                    "label": 0
                },
                {
                    "sent": "The last thing that is written on this transparency, which is actually the first thing, is that because when H is in the neiper pain, then using its supplementary you may find an equation of of eight, which is really easy.",
                    "label": 1
                },
                {
                    "sent": "Just said that any vectors in H is a vector that is orthogonal.",
                    "label": 0
                },
                {
                    "sent": "To you when you is the vector that spans the supplimentary of GI pain, right?",
                    "label": 0
                },
                {
                    "sent": "It's actually more exact.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be more difficult to say than than to see, right if you if you are in there are two an your hyperplane is.",
                    "label": 0
                },
                {
                    "sent": "The line that is orthogonal to the vector U.",
                    "label": 0
                },
                {
                    "sent": "Then you have this equation of.",
                    "label": 0
                },
                {
                    "sent": "Therefore the hyperplane the distance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually that is.",
                    "label": 0
                },
                {
                    "sent": "That was really wrong, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe you said Yes button, but it was really wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll come back to that anyways.",
                    "label": 0
                },
                {
                    "sent": "The distance of each to your hyperplane is exactly given by the absolute value of the dot product of X with X with U and the projection.",
                    "label": 0
                },
                {
                    "sent": "Is this one so?",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Actually, this would be the norm of the projection.",
                    "label": 0
                },
                {
                    "sent": "Of X.",
                    "label": 0
                },
                {
                    "sent": "On your hyperplane there and the distance of X to your upper I printed is only this this distance there right?",
                    "label": 0
                },
                {
                    "sent": "So here in my in my drawing there.",
                    "label": 0
                },
                {
                    "sent": "This is the projection.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "That's the projection of X.",
                    "label": 0
                },
                {
                    "sent": "On my Oprah pain.",
                    "label": 0
                },
                {
                    "sent": "And that would be the distance, an sorry.",
                    "label": 0
                },
                {
                    "sent": "This makes here is the distance of X to my hyperplane, which is exactly the length of this dot product of the projection on the Y axis, right OK?",
                    "label": 1
                },
                {
                    "sent": "Now if you have.",
                    "label": 0
                },
                {
                    "sent": "Now we'll go from from subspaces to linear set of equations and matrices, so let's say.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assume that you have several hyperplanes and you want to find the index section of this desire place.",
                    "label": 0
                },
                {
                    "sent": "Then you'll end up with, let's say you have MI per plans.",
                    "label": 0
                },
                {
                    "sent": "Then you have em equations and so X is in the intersection.",
                    "label": 0
                },
                {
                    "sent": "If those M equations are true at the same time.",
                    "label": 0
                },
                {
                    "sent": "So it gives you a sense of EM linear equations.",
                    "label": 0
                },
                {
                    "sent": "With the unknowns which are U DX1X2XD OK?",
                    "label": 0
                },
                {
                    "sent": "An this set of linear equation may be written as a matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "Which is right here?",
                    "label": 0
                },
                {
                    "sent": "OK. Now if you forget about.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I prepare and then you just have really sets of linear equations.",
                    "label": 0
                },
                {
                    "sent": "You may have a second term here which yields a non zero vector there OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main matrices matrix in R. With MM Rosen D column D columns is an array of numbers of real numbers.",
                    "label": 1
                },
                {
                    "sent": "It's actually an array made made of.",
                    "label": 1
                },
                {
                    "sent": "I'm real vectors, or equivalently, the column vectors.",
                    "label": 0
                },
                {
                    "sent": "And you can can see a matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "As in two ways you can see you time X as.",
                    "label": 0
                },
                {
                    "sent": "A vector in Rd in RM, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah you times.",
                    "label": 0
                },
                {
                    "sent": "So if X is in Rd and you is an M by the matrix then the vectors that you obtained by multiplying U by X is on is a vector in RM OK and you can see it as the combination of the column vectors of U.",
                    "label": 0
                },
                {
                    "sent": "So that's the first writing that you have up there.",
                    "label": 0
                },
                {
                    "sent": "It's a combination of the the columns of you with the coefficient being the.",
                    "label": 0
                },
                {
                    "sent": "The coefficients that you had in X an equivalently, you could see it directly there.",
                    "label": 0
                },
                {
                    "sent": "It's a vector in RM and each of the entries actually a dot product between X, your vector.",
                    "label": 0
                },
                {
                    "sent": "He ran the the rule vectors of you which I wrote.",
                    "label": 0
                },
                {
                    "sent": "You want you want to Umm with the the indices at the bottom.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Actually, you may also see a matrix as your as a representation of an operator that goes from our D2 RM OK. And the first writing tells you basically that the output of the operator applied to X is a vector that is in the span of the column vectors of you OK.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I write matrixes with timelines and D columns as in in in in a more compact way as you see on the right side there and what you can see is that basically the set of matrices in with emeralds and the current is a vector space OK.",
                    "label": 1
                },
                {
                    "sent": "The addition is obtained by summing every entries of the two matrices and the product by.",
                    "label": 0
                },
                {
                    "sent": "The product by a scalar is obtained.",
                    "label": 0
                },
                {
                    "sent": "I haven't, I didn't write it there.",
                    "label": 1
                },
                {
                    "sent": "The product by a real real number is multiplying each entry of the matrix by the Screener number.",
                    "label": 1
                },
                {
                    "sent": "You can also define a matrix product.",
                    "label": 0
                },
                {
                    "sent": "For which the elements are returned there, so it's the basic extension of the matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "You had several vectors.",
                    "label": 0
                },
                {
                    "sent": "You can see B as a set of federal column vector, and each column of the product AB column I of the product AB is a times the ice vector of be OK. And you can also transpose matrices, so if you flip the dimensions of the matrix and you flip all the elements inside, then you get the transpose of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "When the matrices are square, so there is the same number of rows and columns, then you may find matrices that are invertible for the product.",
                    "label": 1
                },
                {
                    "sent": "Some some matrices are easier to understand, for example diagonal matrices.",
                    "label": 1
                },
                {
                    "sent": "So if you multiply a vector by a diagonal matrix, then you get the entry.",
                    "label": 0
                },
                {
                    "sent": "The entries of the the.",
                    "label": 0
                },
                {
                    "sent": "The product is just.",
                    "label": 0
                },
                {
                    "sent": "I read that.",
                    "label": 0
                },
                {
                    "sent": "So this is my diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "X is A is a vector in Rd and so the.",
                    "label": 0
                },
                {
                    "sent": "The the product DX is the vector.",
                    "label": 0
                },
                {
                    "sent": "With coordinates from the 1X1 on the 2X2 of two Lambda D. XD.",
                    "label": 0
                },
                {
                    "sent": "Other matrices that are easier, easy to deal with are upper or lower triangular matrices.",
                    "label": 1
                },
                {
                    "sent": "Do those two kinds of matrices.",
                    "label": 0
                },
                {
                    "sent": "Also, symmetric matrix matrices are are of importance.",
                    "label": 1
                },
                {
                    "sent": "An unitary matrix matrices.",
                    "label": 0
                },
                {
                    "sent": "Which are matrices for which the transpose times the matrix itself?",
                    "label": 0
                },
                {
                    "sent": "Is the identity matrix where the identity is a diagonal matrix is only once under diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the big question is, can you invert?",
                    "label": 0
                },
                {
                    "sent": "When can you invert a matrix?",
                    "label": 1
                },
                {
                    "sent": "And so you can really easily see that if it is diagonal or if D is diagonal, then inverting this system.",
                    "label": 0
                },
                {
                    "sent": "The X is equal to Y.",
                    "label": 0
                },
                {
                    "sent": "So I give you ynd an.",
                    "label": 0
                },
                {
                    "sent": "Your goal is to find X.",
                    "label": 0
                },
                {
                    "sent": "It's really easy as long it's it's easy and it's possible if and only if all the coefficients from that you are non zero.",
                    "label": 0
                },
                {
                    "sent": "OK now you can see for lower or upper triangular system the same condition apply.",
                    "label": 1
                },
                {
                    "sent": "You can invert the matrix in the family if the product of the diagonal elements is nonzero.",
                    "label": 0
                },
                {
                    "sent": "And so it's let's take a lower triangular matrix.",
                    "label": 0
                },
                {
                    "sent": "You can invert it easily, provided that the diagonal elements are non 0.",
                    "label": 0
                },
                {
                    "sent": "So the system up there is equivalent.",
                    "label": 0
                },
                {
                    "sent": "Is this linear system here?",
                    "label": 0
                },
                {
                    "sent": "If you want to invert this matrix, you start from substitute for finding X one by dividing by the diagonal defer diagonal element.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once you have X1 then you can.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe next one in here and solve for X2 so that that would be the next step here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once you have X one X2, then you can plug them in to find X3 an you see you see that you can go from this first to this last equation, replacing at every step all the previous coordinates X one to escape.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "As you have in front of you and you.",
                    "label": 0
                },
                {
                    "sent": "Unknown, XC.",
                    "label": 0
                },
                {
                    "sent": "You have a diagonal coefficient which is non zero.",
                    "label": 0
                },
                {
                    "sent": "Then you're always going to be able to solve for escape OK.",
                    "label": 0
                },
                {
                    "sent": "So if you have diagonal or or or or trigger on triangular matrices, you just need to know if the product of the diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I meant is non 0.",
                    "label": 0
                },
                {
                    "sent": "An the generalization to that is, is the determinant.",
                    "label": 0
                },
                {
                    "sent": "So let's see for General Matrix A in R2.",
                    "label": 0
                },
                {
                    "sent": "With two lines into columns is ABCD.",
                    "label": 0
                },
                {
                    "sent": "Then you can easily check that a is invertible if and only if AD minus BC is non zero and that the inverse is the one that I gave you up there.",
                    "label": 1
                },
                {
                    "sent": "Uh, an in general?",
                    "label": 0
                },
                {
                    "sent": "If you want to know if it if a matrix or dividing matrix is invertible, you have to find its determinant which is written this way.",
                    "label": 1
                },
                {
                    "sent": "The determinant may be, for example defined recursively by saying that the determinant of a matrix.",
                    "label": 0
                },
                {
                    "sent": "Is a sum of determinant of demy de minus of.",
                    "label": 0
                },
                {
                    "sent": "Is this some of the smaller determinants?",
                    "label": 0
                },
                {
                    "sent": "So that's what's written up there.",
                    "label": 0
                },
                {
                    "sent": "Let's say we take J is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Then this formula says the determinant is the sum of AI.",
                    "label": 1
                },
                {
                    "sent": "One times this coefficients their AI one then.",
                    "label": 0
                },
                {
                    "sent": "Is do those the coefficients of the first column of the Matrix and the coefficient that you you put here is actually the determinant of the matrix that is obtained from a without the first column an with without the rule number I OK.",
                    "label": 0
                },
                {
                    "sent": "So this thing here is a determinant of a D -- 1 by D -- 1 matrix that that is obtained from a by substracting 2 lines and columns.",
                    "label": 0
                },
                {
                    "sent": "But yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so definitely there's a formula from the invert for the inverse of a matrix using the determinants, but you will never use it.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Because you'd have to find all this this this matrix.",
                    "label": 0
                },
                {
                    "sent": "This matrix of determines there an.",
                    "label": 0
                },
                {
                    "sent": "It's not really practical so.",
                    "label": 0
                },
                {
                    "sent": "The next thing you wanna know is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Maybe other ways to find an hour, how to invert a matrix?",
                    "label": 0
                },
                {
                    "sent": "OK, to do so.",
                    "label": 0
                },
                {
                    "sent": "Then we look inside the matrix and we try to decompose it into an easier matrix.",
                    "label": 0
                },
                {
                    "sent": "Or maybe what I should say is that when you look for eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And then you would be looking for.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "An equivalent diagonal matrix to your matrix A.",
                    "label": 0
                },
                {
                    "sent": "So in terms of an up, if you think of a in terms of an operator, then a you're dividing matrix is an operator that goes from Audi to Ord.",
                    "label": 0
                },
                {
                    "sent": "And you may find a nice basis where this operator is actually diagonal.",
                    "label": 0
                },
                {
                    "sent": "If you do so, then it's really easy to invert the matrix on this basis, right?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not always possible, so the operator is.",
                    "label": 0
                },
                {
                    "sent": "It doesn't necessarily have.",
                    "label": 0
                },
                {
                    "sent": "They are going to form on some basis, but you may want to to get used to get close to that.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvalues of a matrix or.",
                    "label": 1
                },
                {
                    "sent": "The skaters Lambda such that there there is a vector and non zero vector such that that a Lambda Evie is equal to Lambda V. The eigenvalues are going to be the values that you put on the the diagonal of the equivalent operator that is diagonal OK and the the visa.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvector are going to be the vector that you put in the in the new basis on which the operator is diagonal.",
                    "label": 0
                },
                {
                    "sent": "Equivalently, you say you can see that Lambda is an eigenvalue if and only if the determinant of the matrix A minus Lambda D identity is 0.",
                    "label": 0
                },
                {
                    "sent": "Which means that a minus somebody end entity is not invertible.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                },
                {
                    "sent": "Either way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you say that visa.",
                    "label": 0
                },
                {
                    "sent": "She is the eigenvector associated to the eigenvalue.",
                    "label": 1
                },
                {
                    "sent": "If even run DV OK. Diagonal matrices of four diagonal matrices.",
                    "label": 0
                },
                {
                    "sent": "Obviously the eigenvalues are the diagonal element, and it's not true for triangular matrices, right?",
                    "label": 1
                },
                {
                    "sent": "Um, Zero is an eigenvalue if and only if it is not invertible, and we think that is diagonalizable if there exists a basis of eigenvector which can be written as if you put your, your eigenvectors are columns as columns of a matrix P. Then the products P * D * P -- 1 is actually exactly equals 2, eight or.",
                    "label": 1
                },
                {
                    "sent": "If you put if you have this P transferred there, you can see that AP is a critic is exactly P * D. And so if you think of a column vector of of of P here 8 times this column vector is exactly the same column vector of this matrix there and as D is diagonal.",
                    "label": 0
                },
                {
                    "sent": "This means that eight times this vector is excited is is exactly scared that is the diagonal term times the same vector, so.",
                    "label": 0
                },
                {
                    "sent": "This this rating there is nothing more than saying that the is the is the matrix with column vectors are the atigun vectors of a OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, there's a special.",
                    "label": 0
                },
                {
                    "sent": "There's a special thing that happens when.",
                    "label": 0
                },
                {
                    "sent": "Your matrix is symmetric.",
                    "label": 0
                },
                {
                    "sent": "Is that if your matrix symmetry, then you're guaranteed to find another normal basis of eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So not only are you guaranteed to that the matrix is that they're going to realizable, but also that you can find an ortho normal basis, then that is a basis of eigenvectors, so this can be written as a is P * D * P transpose, where P is a unitary matrix.",
                    "label": 0
                },
                {
                    "sent": "In addition, we say that a symmetric Smith symmetric matrix is semidefinite positive if the dot product between X&Y axes always positive for any vector.",
                    "label": 1
                },
                {
                    "sent": "OK. Then the eigenvalues of the Matrix A are positive and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That would be true.",
                    "label": 0
                },
                {
                    "sent": "For sorry for any diagonal matrix for which the diagonal elements are positive, non negative and for any matrix that can be written as B, transpose B for another matrix OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the matrix is set to be definite positive.",
                    "label": 1
                },
                {
                    "sent": "This care product is zero only if X is itself is 0.",
                    "label": 0
                },
                {
                    "sent": "Then it means that.",
                    "label": 0
                },
                {
                    "sent": "At it this implies that AR as.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only strictly positive I can values again the diagonal matrix with positive entries that are not serious then is definite positive an if.",
                    "label": 1
                },
                {
                    "sent": "If it can be written as a product B, transpose B and is invertible, then it's definite positive.",
                    "label": 1
                },
                {
                    "sent": "Those matrices are important first of all, because you can diagonalize.",
                    "label": 0
                },
                {
                    "sent": "Diagonalize them on.",
                    "label": 0
                },
                {
                    "sent": "Just a normal basis, and if in addition there definite positive, then you can verify that this Form X scare the dot product if you use the usual dot product San X&X then it's defining and you dot product which I I wrote to see with us with an index A and this form.",
                    "label": 0
                },
                {
                    "sent": "Here is a new Note dot product.",
                    "label": 0
                },
                {
                    "sent": "It gives you a new norm on H. And so you can think of everything as your you can think of your vector space.",
                    "label": 1
                },
                {
                    "sent": "Each with with this new number.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if you don't have a symmetric symmetric symmetric matrix.",
                    "label": 0
                },
                {
                    "sent": "It may not be diagonalizable.",
                    "label": 0
                },
                {
                    "sent": "It may also be that your matrix is not even square, but using the.",
                    "label": 0
                },
                {
                    "sent": "Using for example, so let's.",
                    "label": 0
                },
                {
                    "sent": "Let's say B is an M by the matrix and using for example B transpose B or B * B transpose, which we've seen are symmetric semidefinite positive matrices.",
                    "label": 1
                },
                {
                    "sent": "Then you can find an interesting decomposition of B which is the singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So here at the top you can see that I will that be transpose B.",
                    "label": 0
                },
                {
                    "sent": "Is there gonna risible on the North?",
                    "label": 0
                },
                {
                    "sent": "A normal basis in Audi?",
                    "label": 0
                },
                {
                    "sent": "And that's B. Transpose B is V times Delta 1 * V transpose.",
                    "label": 0
                },
                {
                    "sent": "You can also write the same thing for B * B transpose in RM Now.",
                    "label": 1
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "2221 can actually show then the two diagonal matrices that one and D2 have exactly the same nonzero elements.",
                    "label": 0
                },
                {
                    "sent": "OK, there there are two matrices which are which are diagonal but with different dimensions.",
                    "label": 0
                },
                {
                    "sent": "But they have exactly the same 10 elements, and you can verify that B is equal is exactly equal to U * V * V transpose.",
                    "label": 0
                },
                {
                    "sent": "Where where D is the diagonal matrix made of these non zero elements that you've found in D1 or D2.",
                    "label": 0
                },
                {
                    "sent": "U is the author normal basis of RM that diagonalizes B * B transpose and V is the the the author normal basis in Rd that diagonalizes B transpose B. OK. Now, once you have that, what what can you what?",
                    "label": 0
                },
                {
                    "sent": "Obviously the SVD decomposition of B transpose.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Is similar an what you should think you should think about the SVD decomposition as a way of explaining B in terms of a diagonal matrix an as an operator be sense vector in RD2 vectors in RN?",
                    "label": 0
                },
                {
                    "sent": "And if you change from the Canonical basis to the basis defined by V GND and from the Canonical basis to the base is defined by U in RM, then you then you operator that is defined by B is exactly exactly diagonal, which means that.",
                    "label": 0
                },
                {
                    "sent": "Um, if I right.",
                    "label": 0
                },
                {
                    "sent": "So if be my operator that.",
                    "label": 0
                },
                {
                    "sent": "To a vector in our D sends it to.",
                    "label": 0
                },
                {
                    "sent": "BX URM.",
                    "label": 0
                },
                {
                    "sent": "So this decomposition are in the Canonical basis of Audion RN.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you write.",
                    "label": 0
                },
                {
                    "sent": "Switch.",
                    "label": 0
                },
                {
                    "sent": "So V is made of.",
                    "label": 0
                },
                {
                    "sent": "D vectors in Rd, which are which former North so normal basis of Rd OK.",
                    "label": 0
                },
                {
                    "sent": "If X.",
                    "label": 0
                },
                {
                    "sent": "So you can write X as in decomposition.",
                    "label": 0
                },
                {
                    "sent": "It's decomposition.",
                    "label": 0
                },
                {
                    "sent": "On this basis I write this as X~ sorry.",
                    "label": 0
                },
                {
                    "sent": "So it is one X2 daddy Times V. VI OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And then you also have a basis you.",
                    "label": 0
                },
                {
                    "sent": "You won.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Umm?",
                    "label": 0
                },
                {
                    "sent": "In aram.",
                    "label": 0
                },
                {
                    "sent": "Then my operator applied to X, can be easily written as the sum.",
                    "label": 0
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Of the Lambda I * X~ I times um.",
                    "label": 0
                },
                {
                    "sent": "You I. OK. And that's because you I. Hi operator applied to UI is exactly Lambda.",
                    "label": 0
                },
                {
                    "sent": "I'm Lambda I times VI.",
                    "label": 0
                },
                {
                    "sent": "So writing this is saying again that be my operator is diagonal from this basis to this basis.",
                    "label": 0
                },
                {
                    "sent": "And so you can see that B is much easily computable from this basis to this one.",
                    "label": 0
                },
                {
                    "sent": "Then from the Canonical basis OK.",
                    "label": 0
                },
                {
                    "sent": "So now that you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have all this decomposition.",
                    "label": 0
                },
                {
                    "sent": "What did I write?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, in terms of vocabulary you can say that.",
                    "label": 0
                },
                {
                    "sent": "This this writing is the singular value decomposition of B and that the value Lambda 12, Lambda key or its singular values.",
                    "label": 1
                },
                {
                    "sent": "OK now, um.",
                    "label": 0
                },
                {
                    "sent": "There are other ways.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simplify matrices.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I I wrote a few of them here the Lu factorization, the Cholesky decomposition or the QR decomposition an their important because they help you invert matrices easily, so.",
                    "label": 0
                },
                {
                    "sent": "You can see that for the Edu factorisation.",
                    "label": 0
                },
                {
                    "sent": "For example, if you can write the metrics A as a product as a product S L * U where early is a lower triangular matrix and you is an upper triangular matrix, then solving for the inverse of is going to be easy.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Because if you want to solve a is equal to BB should be a.",
                    "label": 0
                },
                {
                    "sent": "A smaller with a small letter there B is a vector in RM in Rd, sorry.",
                    "label": 0
                },
                {
                    "sent": "Then X = B may be solved in two steps.",
                    "label": 1
                },
                {
                    "sent": "The first one is to solve for.",
                    "label": 0
                },
                {
                    "sent": "I think it's wrong, OK?",
                    "label": 0
                },
                {
                    "sent": "Help you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the first the first step is to solve for LLZ is equal to be where you look for the and you have be your your target vector.",
                    "label": 1
                },
                {
                    "sent": "An alley which is a lower triangular matrix, so it's easy to invert and the second step is to solve for UX is equal to Z where you this time is a is an upper triangular matrix and it's easy to solve as well.",
                    "label": 0
                },
                {
                    "sent": "So the condition for it to be.",
                    "label": 0
                },
                {
                    "sent": "To have a an unique factorization is that its diagonal is dominant, which is the condition that is on the upper right side there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "For symmetric semidefinite positive matrices there.",
                    "label": 1
                },
                {
                    "sent": "There's another decomposition which is called assure as he decomposition, which basically says that if you have a symmetric positive matrix, then it may be seen as as the square of another matrix.",
                    "label": 0
                },
                {
                    "sent": "Now the last decomposition here is the QR decomposition.",
                    "label": 1
                },
                {
                    "sent": "If you have a matrix with emeralds and the column, you may.",
                    "label": 0
                },
                {
                    "sent": "You may decompose it as the product of a unitary matrix.",
                    "label": 0
                },
                {
                    "sent": "In RMN an upper triangular matrix or so.",
                    "label": 0
                },
                {
                    "sent": "Um, here, assuming that it would be invertible, so M is equal to D. Then you have a system with here triangular system which is easy to solve.",
                    "label": 0
                },
                {
                    "sent": "And here a change of basis because two is unitary.",
                    "label": 0
                },
                {
                    "sent": "I, I think that's what I wanted to say about vector spaces and linear algebra.",
                    "label": 0
                }
            ]
        }
    }
}