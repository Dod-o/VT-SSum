{
    "id": "g3qwqvgu52l25ux6abnmjf7oqoxev2nu",
    "title": "Reinforcement Learning",
    "info": {
        "author": [
            "Michael Littman, Department of Computer Science, Rutgers, The State University of New Jersey"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_littman_rl/",
    "segmentation": [
        [
            "Hi everybody, I'm Michael littman.",
            "I would wear the badge, but they say that it interferes with the microphones.",
            "I've run out of places to hang things off of myself, got pocket pocket, other pocket.",
            "So I first met Zubeen before he went to grad school.",
            "I think he was doing a summer internship where I was a researcher in New Jersey in the US, and it's just so wonderful to see him, you know.",
            "As the big shot everywhere, he's such a great guy so I really I'm just really flattered that that he invited me to be here and I'll do my best to convey some of the sorts of things that I work on to all of you.",
            "Alright, so good.",
            "So I was my title was reinforcement learning and then when I looked at what I actually was going to talk about, I decided I should probably modify it a little bit less.",
            "You think I've covered all of reinforcement learning, so I'm going to focus on model based reinforcement learning.",
            "But I'll say a little bit about reinforcement learning more generally, so you at least can put that into some kind of context."
        ],
        [
            "So here's the basic pitch, so I'm going to have two days to talk to you guys.",
            "First day I'm going to focus on some background information introduction specifically about reinforcement learning and some classical methods like you learning things that we know about these methods, and then I'll focus on this sort of stuff that I've been really interested in lately, which is model based reinforcement learning and how it relates to that.",
            "I'll talk about the pack MVP framework and quick learning now looking at the schedule of what's been going on before I came.",
            "I expected to see lots of stuff like I studied when I was studying machine learning back in grad school, but it seems to be completely different Now, so I don't even know if it's the same stuff anyway.",
            "So did you guys look at pack at all?",
            "Have we talked about have you talked about probably approximately OK, good, alright?",
            "So I'm going to take that idea and apply it in the reinforcement learning setting.",
            "Explain what that means and tomorrow I'll talk about more current trends, other kinds of more scattered topics that connect in various ways but are a little bit less coherent and foundational, but are kind of the things that people are really working on now.",
            "And if you've got great ideas there.",
            "There's a community who would very much like to hear from you."
        ],
        [
            "So that's that's the basic pitch.",
            "So let me start off just, you know, since we're at Microsoft and and they make computer software, we're going to play a little game.",
            "Alright, so see if I can get this up and running.",
            "This screen is awesome by the way.",
            "I've never seen such a good projector look at that look how sharp man makes my makes my demo look good.",
            "Alright, so alright so this is the game and the rules.",
            "So here's the thing.",
            "So I don't know how we're going to play it all as like how many of us are there 120 or whatever?",
            "That may be a little tricky so we're going to try it anyway.",
            "See how that goes.",
            "So here's the rules of the game.",
            "There's you're trying to end the game you're trying to win the game.",
            "That's the rule so far, and you've got your actions.",
            "The things that you can do are.",
            "Up, down, left, right A&B.",
            "OK.",
            "Begin.",
            "Hey we got it up.",
            "A.",
            "B.",
            "Up I heard up down here up again left.",
            "Down.",
            "Down a.",
            "Wait wait wait, wait, OK, so here's the thing.",
            "So I've done this demo many many, many times and at that moment everybody every time I've ever done this they make a noise, but you guys didn't make any noise, so you're supposed to go or something like that, but maybe you didn't notice.",
            "Or maybe you're just so blase about the world that this like, Oh yeah, well, you know.",
            "Course the red circle was going to get small van.",
            "Alright, so this is clearly a tough crowd so I will.",
            "I will do OK.",
            "So the last thing that I hit was a the last thing that happened is there was a red circle and it got smaller.",
            "And it sort of seemed vaguely.",
            "I don't know if this is exactly true, but I heard I heard of what sounded like a female voice saying B and then a whole bunch of male voices saying hey, but it wasn't.",
            "It wasn't just it wasn't just like let's do a now.",
            "It's like no, no no.",
            "I really want it to be OK Alright, so here we are we hit a last.",
            "I hear it up and I'm going to hit.",
            "I'm gonna hit up.",
            "Right, right, right.",
            "Here there's disagreement.",
            "Let's just let's let's follow this thread.",
            "Go ahead good.",
            "Right?",
            "Up be good OK good thought.",
            "Left you were supposed to make a noise again, but that's OK.",
            "I'll let I'll let it go.",
            "Write a.",
            "Down down down down left.",
            "Right, so somebody who just said know what tell me what you're thinking?",
            "What would you like?",
            "Take it to the right, alright?",
            "So let's lol just will do be I heard that.",
            "A up up left left.",
            "Left up up.",
            "You win alright.",
            "Well done, well done.",
            "Well that went that went about as well as could be expected for a group of 100 people playing a video game.",
            "You don't get to do that every day.",
            "Alright, let's how do I get my screen back though?",
            "It says keynote.",
            "Interesting oono"
        ],
        [
            "Yep, I just didn't see it.",
            "There we go.",
            "OK good so OK.",
            "So now you know reinforcement learning.",
            "This was really great.",
            "No wait.",
            "So let me let me let me say a little bit more.",
            "So that was an example of of we were basically faced with a reinforcement learning task.",
            "We were given the goal at least the knowledge of when we achieved the goal or not.",
            "We were given a set of actions we were given.",
            "State were given some information about what's going on right now we had to choose actions to lead us through and get to a goal.",
            "That's not the only kind of game we can play in reinforcement learning.",
            "This is a.",
            "Very simple example of a robotic system that we built in our lab, but in general reinforcement learning we have an agent interacting with some environment.",
            "It was you and the screen a minute ago perceptions or state actions, things that you can do, rewards and this sequence repeats.",
            "They get a reward, but then you get you.",
            "You have to live in the bed that you just made for yourself, right?",
            "So whatever state you actually brought yourself too, that's when you.",
            "That's where you get to make your next decision, so this whole thing just keeps looping over and over again and your task is to choose actions to maximize rewards.",
            "Now in the game that we just played, the reward was really simple is basically.",
            "No cost, no cost until you win, and then you win and we get to go on with the lecture.",
            "What makes us learning and not, say planning.",
            "There's a whole field in artificial intelligence that studies planning is that there's something missing in the background knowledge or something about the rules of the game that we don't necessarily know.",
            "So in this concrete example with the robot, it's actually learning which way to turn.",
            "It's only got 2 actions, which we could think of his left and right, but it just thinks of, as you know, 615 and 937.",
            "It's trying to minimize time so it's keeping track of the Clock as it's going, and we're trying to do is see this bar that doesn't know that, just like you didn't know what the goal was at the end of your game, but but it knows it when it sees it, so we have a little piece of code in there that detects whenever that unnatural pink color comes up that it has achieved its goal and it gets a plus one for that likes those plus ones and what it's using to guide itself is.",
            "It's got camera input, which it takes to be in this case, just a histogram of colors, so each direction that it looks out of its.",
            "Nose camera thiebeau.",
            "It sees out and it counts up how many of each different kind of color it sees from a set of something like 15 that we just listed out, and that's how it represents the world.",
            "And then it takes an action and it sees how that changes.",
            "An based on this experience of connecting.",
            "Here's what I saw.",
            "Here's what I did.",
            "Here's what I saw next.",
            "did I get a reward or not?",
            "It can start to get better and better at its task.",
            "And what this video is supposed to show is the Aibo actually getting better at figuring out based on what it sees, which way it should turn to face the pink ball.",
            "In the beginning, it doesn't know.",
            "It doesn't even know that the actions are inverses of each other, right?",
            "It just knows that.",
            "It does, it does the first action and then it does the second action and things look sort of vaguely like they did a moment ago, and it's going to have to put those together and eventually start acting well in the environment.",
            "So this I think this example is really nice because it's.",
            "It's real in the sense that it's actually working.",
            "The robot here is dealing with real data from the real world.",
            "It's not necessarily what you call a practical problem now that we've solved this, it's not clear how to sell it or monetize it, but the point is that we can connect up these ideas with data actually coming."
        ],
        [
            "From a real data source.",
            "Alright, so mainly if you want what do you need to do?",
            "If you're trying to solve one of these reinforcement learning problems so couple of things are really important and I think you can see them in both the examples that we've looked at so far.",
            "One is that you have to generalize your experience in the little video game thing that we played.",
            "There weren't very many states that we visited more than once, and in fact the total number of states that were reachable is on the order of about 500.",
            "So there was.",
            "That would have been really tedious, even maybe even more tedious, to actually have to go through and actually experience all those states.",
            "But that's not what you did.",
            "You started to have a higher level picture of how the States related to each other.",
            "And how we might get to some new and interesting States and you know, similar things happen in the robot.",
            "Example, you can think of this as being the learning problem, taking data from the world and coming up figuring out with the regularity's are so you can generalize from that.",
            "That's one part of the problem.",
            "In reinforcement learning.",
            "We have this other side of the problem as well, which is about making sequential decisions.",
            "So dealing properly with delayed gratification.",
            "So you may can you take an action and it may not pay off right away.",
            "So in the video game example, at one point we had as a group subgoal.",
            "We need to get to the Red Square.",
            "Why the Red Square by the way?",
            "'cause the ball was red.",
            "Yes Sir, you didn't know this, but if we had played the game again, that ball might have been a different color.",
            "But that's cool that you figured that out.",
            "Even without seeing the second example.",
            "So, but we had, so we had an idea of where we wanted to go, and no single action was going to accomplish that, so we had to chain them together to get to where we wanted to go.",
            "And you know, the planning community has thought about these sorts of things.",
            "There's lots of problems in the world that have to have this kind of structure where you actually have to generate a sequence to achieve your end.",
            "And what glues these two things together is the problem of exploration and exploitation because the learner is actually embedded in the problem itself and it's trying to win the game or get high reward and it's trying to figure out how to win the game and get high reward.",
            "Each action that it takes is sort of playing both of those roles.",
            "And sometimes there's some.",
            "There's some tension or tradeoffs between them where you want to take an action 'cause you're going to learn something from it, and you want to take an action because you're going to gain something from it, doesn't directly.",
            "So this problem in some sense, if reinforcement learning gets to claim anything again.",
            "For itself, the learning people think about learning, planning people, think about planning reinforcement, learning people can think about how these two things are going to do together in a way that's going to."
        ],
        [
            "The whole system work.",
            "The the standard formal mathematical model that people in the field uses that of the Markov decision process.",
            "It's very tempting to equate the two and CEO reinforcement learning that MVP's, but I don't think people really should think about it that way.",
            "You really want to think of MVP's as being a very convenient mathematical model that captures a lot of what we care bout in the reinforcement learning setting and therefore coming up with better algorithms and better ways of dealing with these things is a good thing.",
            "So probably for the rest of my two lectures, I will equate the two, but I just I should at least make it clear that you really shouldn't think about it that way.",
            "Alright, so this model dates back to the 50s.",
            "Richard Bellman and model sequential environments.",
            "The way to think about this, and we'll have a couple examples of this as we go.",
            "We've got some kind of Markov system with N states from each state.",
            "You can choose any of K different actions.",
            "And just to make things well defined, we have some notion of a discount factor that says we're going to be getting rewards as we move through the state space.",
            "Future rewards are worth a little bit less than current rewards and their discounted in this case, geometrically at what's going to happen is that each step each time step T the agent is going to be informed of what the state of the system is SMT and it gets to choose an action which will call a sub T, and there's going to be then as a result of that, some payoff are some tea whose expected value is our of SPM.",
            "Hating these chairs so much.",
            "Are of SMTHNG City is going to be the expected reward, and then there's going to be a transition to an exit.",
            "Notice that the expected reward only depends on the state and the action.",
            "That's the sense in which it's mark off the other sensor which is Markov, is the probability the next date is determined by this function.",
            "Capital T which says.",
            "If you're in some state SMT and you choose some action, a sub T, the probability of reaching some next state S prime is this expression, and so really you can think about what's going on here is, it's an extended conversation between the agent who's making the decisions and deciding what to do and the environment which is the transition and reward functions, and it goes like this right state goes this way.",
            "Action goes this way.",
            "Reward goes this way and then we get.",
            "Then we play again.",
            "That round just goes on and on and on.",
            "Like that alright so?",
            "Given that set up and given these quantities as we've we've defined them, we can.",
            "We can define what it means to behave optimally in this environment.",
            "In this class of environments, and the way we do that is, we say, well we can define QSAQ is the the Q function, or sometimes people call it quality function, but I think it's just a convenient letter that says what is the future expected reward for an agent that acts optimally, maximize its reward starting from state S, assuming the very first thing it does is action A.",
            "Which might not be the best thing to do, but let's just say we're going to take one action A and then from then on we're going to smartly and take, take, take the right actions.",
            "We could say what this is going to be, whatever the immediate reward is for taking that action in that state, plus the rest, and then what the rest is is where we're going to end up in some new state, in particular state S prime is going to be reached with probability, SAS prime and from that state we're going to be choosing actions optimally, so will take whichever action has the highest value in that resulting state.",
            "So Max overall actions.",
            "Of the Q values of the state that we end up in, take the expected value of that.",
            "So we're going to sum it up overall next states to average to average all the next dates together by their probability, and that's that's our future.",
            "And then we discount that because that's one step into the future.",
            "So this gets us our geometric discounting.",
            "This gets us our expected value, and somewhat inconveniently, there's a queue on both sides of this equation.",
            "I get to be between it for a moment and so that we have a system of simultaneous system of equations which is almost linear.",
            "You know if we could just blot out the Max, this would be a system of simultaneous system of linear equations, which would be very convenient.",
            "Well, the Max makes it a bit more inconvenient, but nonetheless this has a unique solution and there's a bunch of algorithms that can be used to solve that.",
            "If you know the transition function and the reward function.",
            "OK, those algorithms tend to run in time like polynomial in the total number of states in action, so NNK.",
            "Which is sometimes inconvenient because and like for example in the little.",
            "Video game example that we did before.",
            "If the grid is big enough and there's enough different objects going on, the number of states can be astronomically large.",
            "So being polynomial in that is, it's not that much to be proud of, but nevertheless, it's nice that when the state space is small that you can actually define what the optimal solution is, and it's all very convenient.",
            "Any any questions about that?",
            "How we doing so far?",
            "Yeah.",
            "Expected fewer what about another policy which gives you slightly or expected reward but waited over parties?",
            "Ah, right, so this is completely focused on this sort of risk neutral case.",
            "We're just going to take expected value across the board, yes, so there has been some work on looking at things, ways of redefining this that are more risk sensitive in various ways.",
            "Get things, get uglier and it's not been as popular, but certainly if you're thinking about, for example, financial applications, it's really critical to factor in some of that variance.",
            "Yeah, I won't say much more about that, but that's it's a valid point.",
            "I can point you at some literature if you if you like.",
            "OK, alright so but anyway, given this this function that we could solve for if you know this value if you have these values Now we know what we need to do to behave optimally.",
            "We just choose at time T when we're in state St, we look at all the actions we can take from there, and whichever has the highest Q value.",
            "That's the one we choose at time T. So as long as we keep acting that way, then we maximize our cumulative expected discounted reward.",
            "Now what makes us not just the computational problem or straight up computational problem is that we're going to assume that the reward function and the transition function are to some extent unknown, so it can't be that we can just take some specification closer eyes to the real world, grind away on a supercomputer for a bit, and then act optimally.",
            "We're going to have to interact with the environment to find out what really happens when we take actions, and so that's in some sense some amount of experimentation, some amount of learning is going to be needed."
        ],
        [
            "Alright, so here's that robot finding the ball problem again.",
            "In two forms, this is sort of how it would look from above.",
            "This my best attempt at drawing and I bow from above, so this is the eyeball is at different orientations at different times.",
            "Some of the orientations that can see the pink ball, some of them it can't.",
            "And.",
            "Here's how you might view that as a Markov decision process.",
            "Each of those orientations now becomes a state, and the actions are transitions between these states.",
            "The arrows could be that in some cases some actions actually have some probability of keeping you in the same state.",
            "Sometimes they move you a little bit one way or the other in the chain.",
            "Maybe it skips in some places, but some of the states actually have reward associated with them.",
            "Some of them have sort of mild time penalties for just making those transitions from the perspective of how we're going to be thinking about these for the rest of.",
            "Today we can sort of imagine that this is.",
            "This is the representation that we get.",
            "We get States and actions and transitions.",
            "We get like a graph structure like this and we're going to work on that in various ways."
        ],
        [
            "Alright, so OK.",
            "So now we have a basic notion of what it means to be in an environment.",
            "What it is that we're trying to do to maximize behavior environment.",
            "How do we do this as a learning problem if we don't know the transition and reward function?",
            "How do we go about actually acting to do that?",
            "It turns out there's other families of approaches in reinforcement learning.",
            "We're going to be focused on.",
            "I guess I'm going to be focused on model based reinforcement learning, but it's really helpful and we're going to mention I'm going to mention all these other ones as well, so let me sketch it out for you.",
            "So the one class of methods.",
            "Are what could be called policy search methods and policy search methods are trying to learn the mapping from states to actions.",
            "When you see this, what should you do to maximize reward and we represent that mapping as a policy \u03c0.",
            "That's a good thing, because that really is what we're trying to learn.",
            "Ultimately is how to behave how to actually act in the world to maximize reward.",
            "So that seems like a very convenient thing to be trying to learn.",
            "The problem with it is that we don't actually get direct experience of this form.",
            "There's nothing in the world that says, hey, you were just in that state.",
            "I know you took Action 7 but you really should have taken action one.",
            "So when you set up your learning set of learning examples, make sure that you tell yourself to go for action.",
            "1 yes.",
            "Is it obvious that we should always take the same action in the same state?",
            "So because of discounting so I don't know if it should be obvious or not?",
            "I always shy away from using that word 'cause you always offend people.",
            "So yes, of course it's obvious.",
            "OK, moving on no no, no.",
            "So because of the geometric discounting and the fact that we're maximizing expected value and the fact that we have the Markov property, it is the case that when you return to if in the running of a policy for some reason, you return to the same state, you should do the same thing again during learning.",
            "Of course that's not the case, because during learning you may want to try something and discover that's not really what I want to do there, so your policy could be changing during learning, But yeah, usually what we take the target of learning to be is the mapping that maximizes expected discounted reward.",
            "And there is such a stationary mapping that always map states to a given state to a given action.",
            "Alright, OK so policy search so alright, so the problem with this is we don't actually get training examples directly that tell us what action we should have taken given the state that we were just in, yes.",
            "Time is that we said.",
            "Ah, so the equation that I had before is you could call it Infinite Horizon or indeterminate horizon.",
            "It's essentially we're summing an infinite sequence of rewards, but it's decaying, so the effect of future is.",
            "Sort of truncated asymptotically.",
            "Same, actually.",
            "I say yes, if there is a So what happens in the model as I described it and there's variance of this?",
            "But is that the horizon is always a fixed distance away?",
            "Right, so if it was the case that as you take a step, you're actually getting closer and closer to the end, then it could be the case that you want to start acting differently in the same state, because it really isn't the same state you're in that state, but closer to death, as it were.",
            "Yeah, you may want to act differently as you get closer to death.",
            "OK, on that on that note, so because we don't have a direct training, examples of this form, researchers have looked at other ways that we might be able to get better data and train those functions more accurately.",
            "So one type, another set of methods are value function based methods.",
            "These are sort of the most popular.",
            "If you turn to a random reinforcement learning paper published it at a machine learning conference, it probably is mainly about this kind of learning.",
            "So what's happening here is what we're going to try to learn.",
            "Is the Q function.",
            "We're going to learn to map States and actions to future expected rewards.",
            "And if we had that, we could use it to act optimally, right?",
            "If you actually had the right Q function.",
            "If you could learn that, you could use it to act on, because you could just say OK, I'm in some state, which action would gives me the highest predicted reward?",
            "That's the action that I'll choose, so we can kind of map one of these the Q function into the pie.",
            "By searching for the action that maximizes value so that so we haven't really lost much.",
            "There's a little bit of work in getting recovering the policy, but we do get much more direct kind of feedback because at some point we're in some state we try some action and then were acting in the world and we can keep track of how much reward we got and then just send it back and say, OK, well now I have sort of training information that I could use to train that example backward in time for me, and so most of the methods here actually focus on exactly how do you propagate that information backwards so you can set up training examples.",
            "You know, moderately successful.",
            "When I talk about model based reinforcement learning, the problem is is different yet again here, what we're saying is, let's learn, let's focus our learning on the mapping from States and actions to next dates and rewards.",
            "This is a learning problem in the sense that or more traditional kind of learning problem, because we're imagining that.",
            "That these functions actually capture this information, and each time we act in the world, we get a training example like a real legitimate training example.",
            "We don't have back propagated in time or very far in time anyway, because we tried that state, we tried that action.",
            "I don't know if it was a good thing or a bad thing, but I know where I ended up.",
            "I can see that I'm in some new state S prime and then I got some immediate reward R. So from a learning standpoint, I think this is a much more direct and much easier to work with set of methods from a computational standpoint.",
            "It's tricky though, right?",
            "So we could learn these, but now how do you actually?",
            "Act like I know how the world works.",
            "What should I do now?",
            "You have to do something like solving those Bellman equations for Q, which is fairly computationally intensive to get Q and then we have Q we can use that to get the policy.",
            "So we have a bunch of tradeoffs here.",
            "Here on this side we have much more direct use.",
            "We're actually learning the thing we really want, but the learning is much more indirect because we don't get exact training examples from the environment on this side.",
            "The learning itself is very direct.",
            "Every time we take a step in the environment, it's another training example.",
            "But to use it now requires a lot of grinding away in computation.",
            "So.",
            "Depending on the kind of problem that you're working on where you fit on this, you know what the appropriate method to use it varies.",
            "So in a lot of the robot stuff that we've done, computation is much faster than the robot getting from point A to point B.",
            "The robot is very slow in the world.",
            "Oh, I said like why is the world purple?",
            "'cause I'm in the purple box, there we go so.",
            "Apparently purple erases my context.",
            "Right, so in these robot examples data is really expensive.",
            "It takes a very long time for the robot to actually make it across the room, but compared to that computation is relatively cheap, so a lot of our stuff that we've done in my lab has focused on model based approaches on this side, though I heard a really great talk recently, I think it was a Microsoft guy actually too.",
            "And what he was working on was low level architectural decisions in the circuit of a computer.",
            "The memory circuit like deciding which memory accesses to do at which time, when you've got a multithreaded architecture.",
            "Let's say something like that and what was in his case, he had absolutely no computation time at all, every every learning every piece of learning had to happen at Clock speed, right?",
            "So you have no time.",
            "But on the other hand, in three seconds you get 2 million training examples or something like that, right so?",
            "Tons and tons of data.",
            "Data is basically free computations, expensive.",
            "Not surprisingly, he ended up using a actually wasn't even policy search method using a value function based method, so he was able to do this Max fast enough and he was able to back propagate information and train fast enough and we asked him questions like, well, you know what happens if you when you retrain it.",
            "He's like I don't retrain it, it just constantly retraining itself because the data is just flowing in at such an incredible rate.",
            "So I don't want to leave anybody with depression that I'm here to bash.",
            "Any methods on this side?",
            "But I'm going to focus on methods on that side anyway."
        ],
        [
            "'cause they're interesting.",
            "Alright, so I'm going to tell you about Q learning, which is kind of the classic prototypical method in that middle column, and the value phase function value function based method because it, for two reasons.",
            "One is it's almost hard not to go out and try this.",
            "'cause this is, the algorithm is already on.",
            "It's like 2 lines of the slide.",
            "And it really does some remarkable things.",
            "It's just it's just very slow to learn.",
            "So how does this work?",
            "What's happening is we've got an agent.",
            "It's at some state.",
            "It chooses some action it finds itself in some next state, and with some rewards.",
            "So we have that SRS piece of data.",
            "And now the question is, what do we do?",
            "Well, what the Q learning algorithm does is it keeps a table or keep some kind of function that Maps state and it's the Q function map States and actions to future reward and it's going to update that function in the face of this new data as follows.",
            "So I have this sort of cartoon version of Q because there's the real Q that I defined before.",
            "That's the true if you actually knew what the environment was, that's the real Q function.",
            "This is some kind of approximation of it.",
            "Some cartoon of it, and so we're going to update that by taking the old value and adding a little bit of learning rate.",
            "Amount of it to what we add to it is the immediate reward plus the discounted approximation of what the value of the next state is state plus one minus what the old value was.",
            "So this is the difference in.",
            "You could look at it as a temporal difference as how does my prediction now differ from what it would be one step later?",
            "So right, so remember that the value as the essay value there S at time.",
            "TSA value should be equal to the expected value of the reward plus the discounted value of the future states.",
            "So what we're doing here is just trying to bring those two things back in line with each other.",
            "OK, so, but that's the whole thing, right?",
            "That's the whole algorithm.",
            "You just keep doing that over and over and over and over again.",
            "If it's the case that we visit all states in actions infinitely often and we decay the learning rates according to the standard rules of decaying learning rates, then this cartoon value version of Q actually converges.",
            "2 The real Q function.",
            "So.",
            "It solves the whole MDP.",
            "The only way that we know to solve MPs in polynomial time is using linear programming, so it's a fairly sophisticated computational procedure, but really you can get the exact same answer in the limit by just doing these little updates here, so that's kind of neat.",
            "So so one thing, one thing that one affected that had is because it is so easy to implement.",
            "There was A and I guess there's sort of continues to be a lot of people who just implement this and try it in various ways, and so there's just lots of papers that some of them are not as well thought out as others where where they're like.",
            "Oh, so we applied Q learning to problem X.",
            "Which is good and we got a lot of experience from that, but it's not always the best tool."
        ],
        [
            "The job.",
            "So let me contrast this with us with a more complicated thing, which is the model based learner.",
            "So just give it the same set up here.",
            "So now we're going to learn a cartoon version of the reward function, R transition function T, and the Q function Q, and we're going to get the same kind of experience coming in SA next Sr. No next day, sorry.",
            "SARC next S and we're going to do is update the reward to be like the average of the rewards that we've seen for that state action pair.",
            "We're going to update the transition function to be to represent the probability distribution.",
            "As we we have observed it and then.",
            "So we're just keeping kind of an empirical model of the transitions and rewards.",
            "Then we'll take the Q function to be.",
            "The Q function that we get by solving the model that comes from our estimated model.",
            "So instead of actually solving for the Q function with the real model, which we don't know, we just take our best guess based on our estimates that we've done so far and solve it based on that.",
            "OK, so not surprisingly.",
            "Again, if we visit all states actions infinitely often and we decay are learning rates, then this cartoon value cartoon version of the Q function goes to the real Q function.",
            "I cited me there, but really this is.",
            "I think this is kind of obvious, I just I just happened to have written it down in my thesis so I didn't know what else to say, but.",
            "At least I read that right, so I do know that it's citable, alright so.",
            "This seems similar, right?",
            "So we have Q learning which was really, really simple and converges in the limit to the Q function and we've got this big mess which converges in the limit to the real Q function, so you know it seems like the QQ value of Q learning version of this is at least simpler.",
            "But convergence in the limit to the optimal Q function is not the only goal that you might want.",
            "The field was focused on this for a number of years, I think because convert converging to the real Q function is better than not converging to the real Q function.",
            "And there was certainly a lot of methods that were out and about before Chris Watkins thesis that didn't right.",
            "There's like well, and then we'll kind of increase this value and will decrease this other value.",
            "And yeah, it gets stuck sometimes, but not often.",
            "Works AI is often guilty of what I call best case analysis.",
            "There's like in computer science.",
            "Worst case analysis will say, well, this is you know N cubed in the worst case it will give us the answer.",
            "Best case analysis says, well, we're going to run this and I can show you an example where it works.",
            "And there's value in that, but you just have to realize that it's a complementary kind of value to the worst case analysis.",
            "In the worst case, it doesn't."
        ],
        [
            "Mark alright, so that's one kind of goal is convergence in the limit to the optimal Q function.",
            "Here's another goal.",
            "We're going to call pack MDP reinforcement learning.",
            "So what do we mean by this will pack as you told me, you may have looked at already probably approximately correct is an idea that was developed in the context of learning classifiers, but the probably in the approximately in the correct part are very general concepts, and so we can apply them in other ways.",
            "So here's how you might apply these ideas in the reinforcement learning setting.",
            "A number of people have done this.",
            "We're going to be given just like impact.",
            "We're going to be given epsilon in a Delta.",
            "Let's say we know that we're going to learn in some MVP.",
            "It really is an MDP, so we're going to assume the Markov property and so forth.",
            "We know that there's K actions and states, we just don't know what they do, and we've got a discount factor.",
            "We're going to say a strategy of learning a learning algorithm is going to be acting in the world.",
            "It's going to reach a state, and it's going to take some action, and it's going to repeat that over and over and over again.",
            "We're going to say it makes a mistake each time step T, where the action that it took at time step T in the state that it was actually in time zippity.",
            "The true Q value of that action in that state is worse than the best that you could have done in that state, minus R epsilon.",
            "So this is the approximately correct part of the pack.",
            "This is saying that an action is approximately correct if the Q value is close to the optimal Q value.",
            "Alright, so so that's an if you don't do that then that's a mistake and we're going to let Em be abound on the total number of mistakes, and that bound has to hold with high probability 1 minus Delta.",
            "So this is the probably part of the pack.",
            "Phrase what we'd like is, can we come up with some kind of way of acting in the world?",
            "Some kind of learning algorithm, such that there's an M that holds for that learning algorithm that's polynomial in the number of action states, one over epsilon, one over Delta.",
            "And let's say the effective horizon 1 /, 1 minus gamma.",
            "OK, so if you really want, if you really care if gamma is close to one, that means you really care bout far distant rewards.",
            "Yeah, so maybe that's going to make your learning take longer 'cause you care about things that are far away.",
            "If the discount factor is closer to 0.",
            "This is a smaller number and you can actually would like to be able to learn faster in that case, if you don't care too much about the future.",
            "Alright, so the only way that you're going to be able to achieve this kind of bound is by striking some kind of balance between exploration and exploitation.",
            "An algorithm that takes its best guess of what it thinks the world is and always acts according to that could end up making tons and tons of mistakes because it hasn't learned what the correct model is yet.",
            "On the other hand, an algorithm that painstakingly goes out and checks every estimates every parameter to the NTH degree, you can't estimate any of these exactly, but you can get them.",
            "Very very very very accurate is wasting a lot of time making a lot of mistakes, doing experiments, that time that it could have been spent exploiting gaining near optimal reward.",
            "So you really do have to strike a balance between these two, so that's kind of challenging."
        ],
        [
            "So the first thing to point out is that Q learning is not pack MDP.",
            "Now that's not entirely fair to say.",
            "I said it anyway, but it's not entirely fair to say on the part of the reason for that is Q learning.",
            "There is no Q learning Q learning is actually a family of algorithms and you get very different kinds of behavior depending on how you initialize the Q values.",
            "Initialize the Q function.",
            "How you do exploration.",
            "That is to say, remember that what the theorem said is that you try all states, all actions in all states infinitely often, but at any moment in time you have to choose some action based on your current Q values and how you do that has an impact on how the algorithm works.",
            "And how you decay the learning rates turns out to matter too.",
            "So different different versions of Q learning decay in different ways actually have different mathematical properties.",
            "But that being said, here's what we know.",
            "Many of the versions, every version that we've been able to study, is not pack MDP, and there's a couple where the jury is still out, but there's no versions that we've been able to show actually achieve that pack MDP guarantee.",
            "And most of the counter examples for why it doesn't work boil down to the combination lock, so here's this.",
            "I'll show you the combination lock, 'cause then you could have seen it.",
            "So we have a chain of states.",
            "There's one action that pushes you this way on the chain and there's the other action that no matter where you are, resets you back to the beginning.",
            "Except for the very very last state in the chain, which is a self loop and you can get lots of reward from their high reward.",
            "Otherwise the reset action has a very tiny little reward and the go forward action has no roll cake.",
            "As you can see how this maybe is a little bit tricky in the beginning if you don't know that you're trying to go out there and get this high reward at the end, you're just kind of bumbling around exploring and what you're seeing.",
            "A lot of the states that act more or less the same, and if you're choosing actions 'cause you don't know which action is better than the other one.",
            "So if you're choosing actions randomly, you're going to be spending basically your entire life in this state.",
            "Going to wander out a little bit and then you'll randomly try the blue action and it takes you all the way back to the beginning.",
            "Bumbling around randomly like this that I'm the expected time that it's going to take to get all the way to the end of this chain and actually experienced this high reward for the first time is going to be exponential in the size of the chain link.",
            "So unless it's actually pushing itself, a learning algorithm is pushing itself out on the chain to see if there's something else out there then it's not going to be able to get that high reward, and it's not necessary.",
            "Well, depending on exactly what the discount rate is and how big that 20 is, and those probabilities and so forth.",
            "It will not be getting near optimal or it will be making mistakes pretty much forever.",
            "Alright, so.",
            "Yeah, so that's the bad kind of example.",
            "So you really do need and if you want to get this guarantee of near optimal reward then you really have to push out and explore more than what Q learning is able to do."
        ],
        [
            "Without some direction.",
            "So what about a model based approach?",
            "Well, turns out model based approaches can be pack MVP that they don't necessarily have to be 'cause.",
            "Again, you can think of this as being a family of algorithms as well.",
            "So let's go back to our combination lock example.",
            "So let's imagine that we've explored for awhile, and here's what our model based learner has learned so far that it knows about these six states.",
            "But it's never tried that black action from this last state.",
            "It's just never tried it, right?",
            "'cause it's learning as it goes, so it has no idea what's out there.",
            "So now what happens?",
            "Well, depending on what it assumes.",
            "Might be out there.",
            "The model based method is going to behave very differently, so let's look at a little 2 by 2.",
            "It can either assume that what's out there has relatively high reward, like more than what it's getting in the rest of the chain, or that it has relatively low reward.",
            "But the truth is, it could either be low or high.",
            "And those are disconnected from each other.",
            "So if it's the case that really what's out there is low reward and the algorithm assumes that what's out there is low reward, then it's not going to even try to go out there and find it.",
            "And that actually is optimal in this case.",
            "On the other hand, if the truth is that it's really low out there, there's nothing good out there, but it assumes there is something good out there.",
            "What's going to happen?",
            "It's going to say, hey, the optimal policy, as far as I know, is to go out there and get that reward so it goes out there.",
            "It gets to the question mark, and then it's sadly disappointed 'cause there's really no good reward out there.",
            "'cause the truth is.",
            "In this case the reward was low.",
            "So what happened is it went visited the question Mark.",
            "It explored if you will it learn something new about it, and then.",
            "Like many of us, it learned that it wasn't all that it's cracked up to be, and so, but now that's good, because in its model it now represents that information and it won't make that mistake again.",
            "On the other hand, let's say that the truth of the matter is that there's high reward out there.",
            "Well, if there's high reward out there, and it assumes that the reward is high, then that's great.",
            "It's going to go out there and find that reward, and that was the right thing to do.",
            "But, and here's where the asymmetry comes in.",
            "If the truth is that there's high reward out there, but the algorithm assumes there's not, it's not going to go out there and get it.",
            "It's never going to be getting near optimal reward.",
            "It's going to be sub optimal, and it's going to stay sub optimal because it's not learning, it's not getting the experience it needs to find out that it's wrong.",
            "So you can see there's sort of irritating asymmetry that comes up if it assumes stuff that it doesn't know has low reward, then we don't get a pack MVP guarantee.",
            "On the other hand, we can, if we assume that things that are out there that we don't know about are relatively good.",
            "And in fact it can be packed MVP as long as the each time it learns something that actually get something out of that, learning right?",
            "If it's the case that that.",
            "It's truly low.",
            "It thinks it's high.",
            "It goes out there, discovers that it really is low, and then forgets that right.",
            "Then sometime later it's going to be Oh yeah, and it's going to go out there again and it's gotta actually you have to put bounds on how quickly it learns from its mistakes.",
            "But if you have that then you can actually get these nice guarantees that with quickly with high probability it's."
        ],
        [
            "Maybe near optimal.",
            "And the basic idea that drives the story that I was just telling a lot of the algorithms that actually have been shown to have these guarantees is the notion of optimism under uncertainty.",
            "If you don't know any better, assume it's good.",
            "Why assume it's good and not bad because you can't get near optimality if you assume it's bad.",
            "There is a field that does the opposite for good reason, right?",
            "So people who do like safety critical systems, they assume that the unknown is very, very bad, and that's that's that's a good.",
            "We're glad I was able to fly across the Atlantic last night, and I'm here safe.",
            "And I think partly it's because they make these bad assumptions know these, make these assumptions that things could be bad, and so they do.",
            "Avoid the bad cases, and then you know we're safe and sound if the plane is flying over the attic.",
            "Oh, I've never seen that kind of turbulence before.",
            "I bet it's good.",
            "That that would suck so.",
            "Yeah, but so but anyway, but that's because the goals are different in this case.",
            "If the goal really is we want near optimality with high probability, we have to assume these algorithms seem to have to assume that what's unknown is good.",
            "This is a well known idea.",
            "It dates way back to the beginning and reinforcement learning.",
            "People were doing it in practice before they had.",
            "Any kind of theoretical story about why it was a good idea, but they had a lot of experience showing that when they tuned their algorithms in that way it did better than if they didn't, yeah.",
            "No, but you have to assume that you have some sense of what big really is.",
            "Yeah, yeah, so you have.",
            "So usually in fact, one of the algorithms that I'm going to talk about is called our Max.",
            "It's called our Max because it's given as a parameter, the maximum reward.",
            "OK, so it turns out that if you do this you can develop algorithms and theorems and so forth.",
            "It showed that you can actually achieve these kinds of pack MDP guarantees and the key ideas for getting these proofs to go through are basically these two the simulation lemma, which just says that if you have an approximate model of an MVP and you do what you think is optimal for that approximate model, then you're near optimal for the real MVP, right?",
            "So basically?",
            "Perfection in in in reward gathering plus approximate model equals approximation and reward gathering with imperfect model.",
            "So what that means is you can actually estimate the parameters of the model.",
            "Pretend that that's reality and do pretty well.",
            "If your estimate is good, the other one is the explore exploit lemon.",
            "This is very clever actually, which says that if you can't reach unknown states quickly then you can achieve near optimal reward.",
            "So the basic idea here is what keeps you from.",
            "So let's say that there's a state that's really very inaccessible.",
            "I bet there's a state that's in excess, so there's these cool boxes on the side here, little triangular boxes.",
            "If you can see them, there could be something very valuable for the invited speakers there.",
            "In fact, I was told there actually called speaker cabinets, so they must be for me.",
            "So The thing is that it would be really probably very costly for me to actually find this out, like I'd have to rip apart the thing and risk called prosecution and expulsion.",
            "So why is it that if I'm trying to get near optimal reward, why don't I just go and do that?",
            "Well, the point is that getting even finding out whether there's anything good in there is going to be very costly in and of itself.",
            "So what the Explore exploit lemma says is if it's really costly to get new information.",
            "That's because you can actually get near optimal reward without it.",
            "And so this maybe this may not be entirely true with the real world, but it is true of MDP's.",
            "If you have a bound on the maximum reward.",
            "So when these two things get put together, what you can show is these algorithms are going to explore as much as they need to explore, and once they can't, once it becomes too expensive to explore their actually behaving near optimally.",
            "And this idea can be extended in various ways, and that's what I'm going to focus on for the rest of this slot, so I haven't found o'clock yet.",
            "Oh there's a Clock.",
            "What do we think is the ending time between?",
            "We started a bit late so 1240 OK alright good alright so that's that's my plan is to then flesh that story out with."
        ],
        [
            "The time that I've got.",
            "I should say one thing though.",
            "As a side Note, pack MVP.",
            "Pretty much all the pack MVP algorithms are for these model based methods.",
            "There's a real question as well.",
            "Can you even do model freepac MVP and it turns out I guess it's not that recent anymore, but relatively recent result is that you can and the way that's done is by taking Q learning and actually kind of smoothing it out a bit so that it doesn't get too excited about any one piece of data.",
            "And then you can show that it actually the total number of mistakes that it makes is Paul.",
            "It's like number of states, times, number of actions.",
            "Defective Horizon race to the 8th power the approximation concentrate to the 4th power.",
            "This is in some ways better in some ways worse than what you get with model based methods in model based methods there's always this pesky squared that shows up in the number of States and partly if you think about it, a model free method estimates function from States and actions to values so the data structure itself is N * K model based method is estimating the transitions which is states by states times actions.",
            "So it's N ^2 K. So this is the size of the data structure in a model based approach.",
            "So again, it's not so surprising that that's going to show up in the bounds, so the model free pack MDP bound is actually better, at least in terms of number of states, but it's worse in terms of horizon lengthen approximation constants.",
            "At least what we've got right now, and some of these components actually match in a lower bound that was able to be shown.",
            "So these ideas are more general than just the model based method, but I'm going to focus on the model based methods in the back first.",
            "On continuous state space.",
            "I'll say some about continuous state spaces.",
            "Yeah, so in fact on the previous slide, I think I even used the word.",
            "I said metric spaces.",
            "Yeah, so some of the cube stuff.",
            "Some of this work had been extended to the continuous case.",
            "Things get awkward and the assumptions start to pile up in the continuous case, but in the rest of what I talk about today and tomorrow I will be mentioning continuous now and again.",
            "There's another hand to do it.",
            "So the the end square K result yes.",
            "If you have a typical gridworld where you can't get from every state, every other state, OK. To end on the number of states can get through to actually know that about so.",
            "So the way that I just presented this in terms of the size of data structures.",
            "If it's the case that there's a constant number of next states you can reach, call it.",
            "Z for zoom in so we have N * y * K as the size of the data structure.",
            "The question is, does that actually show up in the bound as well?",
            "I don't know.",
            "They may have a pen anyway, remind me I'll follow up on that.",
            "That's actually that's very cool idea.",
            "Yeah, and in particular a lot of lot of problems actually have that structure.",
            "Certainly the grid worlds that that reinforcement learning people get fixated on have local connectivity, right?",
            "So given state can only reach a small number of neighbors, not the full state space, yes?",
            "6.",
            "Please get up let me know that you already optimal after some haha."
        ],
        [
            "So I will.",
            "I will, I'll address that, so that's a good question.",
            "I don't know what the.",
            "I don't know how I feel about that, but I like the question, but I'm going to ask it again when I get to the part where I say oh and this is going to show that you're not actually optimal after any given time step.",
            "When I get there is a context, I'm going to set up shortly that will make it easier to, I know, but I'm going to get it will be easier for me to explain it later anyway.",
            "So it seems silly to repeat it now anyway, not that.",
            "So OK, so the question was, I've been shamed into it, so the question was.",
            "The mistake bound and which the pack MDP framework sort of sets up is basically one that says the total number of mistakes in an infinite run is bounded by a certain number.",
            "But what it doesn't say is that after this amount of learning from here on forward, you're optimal.",
            "OK, so those are two kinds of things that you might think about is learning right?",
            "One says that you're going to do all your training and learning in some fixed.",
            "And then from then on you've got it and the other one says you could make a mistake.",
            "It could happen today it could happen next week, but each time you make a mistake, you're getting closer and closer to perfection.",
            "Now if you look at any fixed block of time, if it's a big enough block, the fraction of mistakes that you make in that block has to be going to 0, right?",
            "But the fact of the matter is in the bounds that I'm talking about.",
            "There is no last moment at which you can make a mistake, and part of the reason for this sort of inevitable in these MVP cases.",
            "'cause imagine if there's a transition that's really really low probability, so most of your life you kind of live would just at home.",
            "We just started watching the movie The Matrix, which I assume most people have seen, but my kids hadn't seen it yet, so they're being there.",
            "Lines are being bent, but let's say that you live your life, you live your life and then one day you discover.",
            "No, that's not reality.",
            "There's this other reality over here.",
            "Suddenly you're not getting near optimal roared anymore.",
            "Even though you were in the old space and so the moment at which that transition happens, you can't put it down.",
            "It's a low probability event.",
            "It could happen anytime in the future, so it's I think it's impossible to get the kind of bound that you were hoping for.",
            "But but I've come to grips with this one.",
            "I think it's OK. Alright, So what I'd like to do is to draw to kind of extend this idea so we can talk about continuous spaces.",
            "We can talk about spaces where you have more structure, we can talk about lots of things like that by expanding this idea to focusing on the profiling problem.",
            "Model learning is a supervised learning problem.",
            "You're given examples of the form, state, action and inputs of state action, and you're supposed to output next date or probability of next date.",
            "It's not specific.",
            "This idea is not specific to table.",
            "Look up like just building a big literal explicit graph.",
            "So what we'd like to do is extend these kinds of efficient learning results by generalizing the notion of the transition function to be just some function approximators, something that Maps States and actions to next days.",
            "But the problem is that the existing or kind of standard machine learning theory doesn't give us quite the right guarantees for us to build."
        ],
        [
            "Reinforcement learning out of algorithm out.",
            "Alright, so this is sort of the context that I thought I'd be able to use a moment ago.",
            "So here's the deal.",
            "So we talked a little bit about pack.",
            "So in the supervised setting.",
            "Pack is set up like this.",
            "Inputs are drawn from some fixed distribution.",
            "We get to observe labels for the first set of them, say M of them, and then future inputs are drawn from the distribution.",
            "Oh, so once we've done that once, we've gone through our training period, we're not going to make mistakes with high probability.",
            "We're not going to make big mistakes for any future things drawn from that distribution.",
            "If the pack guarantee holds so graphically, I drew it this way.",
            "Imagine each each row here is another training example coming through the first block of them are blue.",
            "Those are the training examples and after that.",
            "It's a bunch of things where the algorithm makes no mistakes.",
            "It correctly classifies or outputs after the initial training period, so that seems like a good kind of guarantee.",
            "It seems like a useful thing to have.",
            "We could imagine the learner robots scrambling around in the world collecting data about, oh, I tried this in this state.",
            "I tried this in this State Building up a training set, and then predicting things correctly after that point.",
            "That would be really great.",
            "So here's the first thing that goes wrong.",
            "Building, putting a pack learner as our transition function learner in a reinforcement learning algorithm.",
            "Isn't going to give us a pack MVP guarantee and the reason for this is?",
            "I find this somewhat amusing.",
            "The IID assumption that so critical for getting kind of these classic results to go through implies that the learner can't improve and change in any way.",
            "It's behavior, right?",
            "So these examples, where is the robot getting this examples from its wandering around the world and collecting them?",
            "What's the distribution of examples?",
            "What is a function of how it wanders around in the world?",
            "So if it collects for awhile and then says, oh, I know how things work now, it starts to behave differently, the distribution changes.",
            "So really, the only way that it can use this great learning that it is that it just it just it just made so it makes perfect predictions now and it wanders around the world the same way that it's always wandered out in the world.",
            "Saying, you know, I know how I would have optimized that.",
            "I know how I would have optimized that, but it can't make use to that.",
            "'cause then it's learner goes away.",
            "It doesn't make accurate predictions anymore, so that's unfortunate.",
            "OK, so has another model.",
            "There's a better model.",
            "The mistake bound model of computational learning theory says that we're not going to have any dependence on a distribution.",
            "Inputs are going to be presented online, perhaps by an adversary an for each one.",
            "The learner is supposed to predict the output.",
            "If it makes a mistake, it gets to observe what the correct label is, and it's not allowed over the course of its lifetime to make more than M mistakes.",
            "So it's sort of a victim of the adversary.",
            "The adversary cannot show it some example for arbitrarily long into the future.",
            "But it when it eventually gets that example, it might make a mistake on it, but the total number of mistakes over its lifetime has to be bounded, so that seems good, right?",
            "We got rid of this notion of depending desperately on the distribution of inputs.",
            "The problem with this one though, if you stick it into a reinforcement learning algorithm, is again it's not pack MVP and that's because it can make mistakes.",
            "The predictor of next states can make mistakes.",
            "An as I showed in that little two by two example that a mistake could mean that there's some place out there that has high reward that our current function predicts has low reward, and therefore we're not going to learn.",
            "We're not going to visit it.",
            "We're not going to learn, and we can be permanently suboptimal, so we won't satisfy the pack MDP guarantee of getting near optimal reward after some number of mistakes.",
            "So alright, so we said OK, well maybe we could get the best of both worlds here and we sort of did.",
            "We had developed a.",
            "A learning framework to sound like pack called Quick, which stands for knows what it knows.",
            "So here the basic idea is the inputs are presented online like in the mistake bound model for each.",
            "The learner can actually predict the output, but it has to be accurate or it's allowed to say I don't know right?",
            "We can't expect it to be accurate without learning anything, so it's allowed to say I don't know and observe the label it's not allowed to make mistakes and up to that point it's actually really easy to do this right?",
            "You just say I don't know forever so we have to put a bound on the total number of times it's allowed to say.",
            "I don't know.",
            "OK, so that means it's allowed to get training examples.",
            "It's allowed to be clueless for a little while, but it can't be clueless forever so graphically.",
            "Oh, I didn't.",
            "I didn't point out the graphic representation in the mistake bound case.",
            "So in this takedown case it's getting examples.",
            "Some of them are wrong red and some of them are right green, but the total number of red bars has to be bounded and the other at the top.",
            "When the total number of blue bars was founded.",
            "This is now in the quick model.",
            "The total number of blue bars is bounded.",
            "So what's happening is it's getting an input.",
            "It's making a prediction it.",
            "But only if it's sure that it's right.",
            "Otherwise it can say I don't know and request an example and the total number of blue bars has to be relatively small.",
            "So that's.",
            "Quick quickly.",
            "And this can we can plug this into a reinforcement learning algorithm and we can get our pack MDP guarantee YYO.",
            "Question.",
            "So the difference between the mistakes on it quick is that quick predicts something, and if it's uncertain, that says I want to observe that label right where is mistake?",
            "What the mistake bound algorithms do is if it's not sure, it just guesses and some of the really effective mistake bound algorithms, what they do is they not playing the odds, but they play the hypothesis space they say basically, if more things, many things in my hypothesis space agree, I'm going to go with that.",
            "And if I'm wrong, then I learn a lot.",
            "And it makes very, very dramatic progress towards learning the real hypothesis, but but the quick quick algorithm can't do that if it's not sure, it has to say I don't know.",
            "And.",
            "Confident.",
            "It if it's too confident.",
            "Oh, it's not allowed to be wrong.",
            "It's simply not OK. Well, it's a.",
            "It has a pack like it has an epsilon Delta sort of thing in it, so a quick algorithm.",
            "You can't guarantee that any problem with any amount of stochasticity is going to be 100% guaranteed correct?",
            "So we have to hold out some probability that the whole thing could just fail, but when it's not failing it has to be right.",
            "Yeah, otherwise it's."
        ],
        [
            "Quick.",
            "So it's like like the background is not allowed to make mistakes like the mistake bound.",
            "There's no distribution assumption.",
            "Not surprisingly, this is a harder problem, right?",
            "So anything that can be quick learned can be mistake bound learned, because whenever you say I don't know, you can just guess anything that turns out anything mistake that you can learn.",
            "Another mistake model you can pack learn, but it's not the other way.",
            "There's problems that you can mistake bound.",
            "Learn that you can't quickly learn, and it's especially when they are taking advantage of the fact that you make a guess an if you're wrong, you learn a lot.",
            "Quick is not allowed to make that guess, and so an adversary can.",
            "Just kind of whittle away at the hypothesis space and take a very very long time forcing it to make lots and lots of I don't knows yes.",
            "So what I'm going to do is, I'm going to present a bunch of examples.",
            "It'll be easier with the examples how how, I just say it's not allowed to make a mistake.",
            "It's as simple as that.",
            "We we could do some examples but but but if this is the way that's going to be, here's why this is very well suited to model learning.",
            "So what we're going to do is.",
            "Have our exploration being driven by this known unknown distinction so anything in the world that are learner can't predict we're going to have to assume that something that's good and get the optimism in the face of uncertainty.",
            "Get it to go out there and get that data once it gets there.",
            "It'll get that example.",
            "Find out the right answer and then it won't not know that anymore.",
            "It won't.",
            "Yes, it will know that now."
        ],
        [
            "Alright, so here's an example of something that we can quick learn, really simple that we should all be able to get on board with, which is learning the probability that a weighted coin is going to come up heads.",
            "OK, so we all know that if we observe for M trials and we've had X successes, we can estimate the probability P. The cartoon version of it is just X / M. Anne, with the Husting bound does is it tells us.",
            "As a function of epsilon, how likely it is that our estimate is going to be at least at least epsilon close to the real answer so?",
            "I assume that you've been doing hafting bounds earlier in the course they come up before.",
            "No interesting, OK. OK, well then let me let me say a little bit more about it then.",
            "So sort of standard concentration bounds sort of thing.",
            "So.",
            "So what we're saying is.",
            "No matter how big XMR we can still get that kind of estimate right?",
            "So I can flip the coin 30 times and it comes up heads five times.",
            "So I could say, well, I'd estimate 5 / 30 is the probability that comes up heads.",
            "The problem is that if we haven't flipped the coin very much, we shouldn't be all that certain that the answer really is 5 / 30.",
            "So, but as we get more and more data with high probability that the average that we compute is really going to be very close to the real average, the law of large numbers sorts of things.",
            "So it's this.",
            "Just quantify that it says that if you've got a sample of size M and you're interested in variable that varies between.",
            "Being a right.",
            "So that's the range of values that can spit out.",
            "So how is the probability that our estimate is going to be within epsilon of that and it's this expression here so you can take this and solve for what the probability is so.",
            "Sorry, this is the probability you can take this and say if I'm saying I want to be Delta Shore 1 minus Delta, sure that I'm within epsilon you can solve for what M what sample size is going to give you that assurance.",
            "OK, So what does this let you know is that once if I want to be 95% sure that I'm within .0, one of the right probability we solve for M, we say.",
            "Well, here's how many examples you'd have to do to be that sure.",
            "So now the quick algorithm, the quick learner is going to say I don't know M times to gather that data, and once it's gathered that data that estimate with high probability is epsilon close, and so we can just use that app that estimate for then on.",
            "OK, I'm not gonna tell you the function.",
            "But I'm going to give you and we're going to play this as you're going to be a quick learner.",
            "We hope this goes quickly and so that means that I give you the input and you have to give me the output and it has to be correct.",
            "Or you can say I don't know.",
            "OK, but you can't say I don't know forever 'cause that would be cheating.",
            "Alright, so first input one.",
            "OK, OK, that's fair.",
            "The output is 1 for that case.",
            "OK, next input is 1.",
            "I heard it, I don't know.",
            "Yes, the output is one that's correct alright.",
            "The next input is 0.",
            "I don't know.",
            "Alright, that was one also turns out alright.",
            "Next input is 1.",
            "Good next input is zero.",
            "There we go alright, and we've learned the function and you've learned it with no mistakes.",
            "By the way, congratulations.",
            "So an how but how many times did you have sort of a ridiculous example, but it turns out lots of lots of functions have elaborations of that idea in them, so for this particular case, how many mistakes?",
            "Not mistake?",
            "Sorry, how many I don't know is did you use?",
            "Two and in fact is that a bound for this class of functions.",
            "Yeah, you shouldn't.",
            "You could do worse, but you shouldn't.",
            "You don't need to because you just memorize it, right?",
            "You just start to fill in the table in your head.",
            "So some of the things that we can quick learn have that kind of form to it.",
            "You just basically fill in the table as you find out what the right answer is.",
            "You know it, and you can write it down, yes.",
            "Learning the concept.",
            "No, they could actually the not not input.",
            "That would have been fine function to learn to.",
            "No.",
            "Why'd you say that?",
            "Because you said I don't know twice you had you said I don't know for the zero and you said I don't know for the one.",
            "Yeah, but then like if we just see that the outcome is always wrong, and if you say if you say 0 at some point and we say one and actually the function is not constant, then the outcome is.",
            "So in the function that I was just doing this deterministic is that, does that fix it?",
            "Yeah no.",
            "So I let that example off saying it's a function from bit to bit.",
            "Oreo is that when you say constant, you meant you mean like everytime its input is zero.",
            "It's always a one out yeah terministic function.",
            "In that example, if it was a probabilistic function, you could have learned the probabilities.",
            "There's two probabilities how well for each one you run a separate copy of this algorithm, right?",
            "So you keep count of when you've heard zeros in input, how many successes, how many failures?",
            "When I've heard one is an input, how many successes, how many code?",
            "So even though this feels sort of silly because you say you know, I don't know 30 times in a row and then you know it.",
            "It's a little more interesting if there's an input involved.",
            "So if I tell you zero, you have to learn the probability of 0 and you have to learn probably for one, you're keeping two separate counts, and so you might say I don't know for awhile and then start knowing one of them and then start saying I don't know again when I give you different inputs so it starts off simple.",
            "But as you start to compose these learners it gets more and more interesting.",
            "I did see a hand or partial hands.",
            "There's a whole hand.",
            "And then.",
            "At some point you have to stop saying I don't know, but you still have probability of landing and indifferent or making a mistake.",
            "30% in lending it 0 and 7% is ending on one right?",
            "OK. And now I know.",
            "So the game here is if we're trying to predict the probability the thing you're supposed to output to me is the probability right?",
            "And so that probability can be arbitrarily close to the true answer, even though none of the examples you get, so you know if it's a fair coin.",
            "Eventually you should learn to say something like 1/2, but none of your data was a half, right?",
            "I give you just zeros and ones as an example as example, right?",
            "So yeah, so I could define this more formally, but I'm being forced to do so gradually.",
            "Apologize for their hands.",
            "OK, so so there's just a couple of things that we can quick learn effectively.",
            "So far we can just a single coin and a single coin.",
            "Conditioned on a small set of finite set of inputs.",
            "For example, we just learn them separately."
        ],
        [
            "We could learn.",
            "Let's say that the output is, there's no input, but the output is a bunch of numbers which are.",
            "The probabilities and all the examples that I give you are draws from that vector, right?",
            "So you like like.",
            "Like a bunch of coins, almost have a bunch of weighted coins in coin, one has a probability coin, two has a problem probability coin three has a probability and the examples that I give for training are while I go and flip those coins and tell you which ones came up heads in which ones tales free on each trial.",
            "But you can start to pull those statistics and learn the same way again.",
            "So if the output is a vector, each component is a probability, then as long as you could quick learn each of the components of that vector, you can quick learn the entire vector.",
            "If the function that you're trying to learn is a mapping from big input space and it's partitioned in some way, and each within each partition you have to learn some quick learnable class, then you could learn the whole class because you just running a separate quick learner for each piece of that partition.",
            "So these seem like really kind of simple, obvious examples, but what's nice is that this is exactly what you need to make a standard transition function in reinforcement learning, right?",
            "So all that is, it's a mapping that takes SNA as input, which partitions the space, and then when you get to observe, are these outcomes you know was it state S7 that came up as a result, yes or no.",
            "So just with the pieces that we have so far, we can quick learn a transition function, and because we can quick learner transition function, we're going to be able to make a pack MDP algorithm out of it.",
            "Now people already knew how to do that without any of these quick learning.",
            "Mumbo jumbo in between, but we're going to start composing them in more interesting ways and get some new algorithms that hadn't been.",
            "Discovered before another thing, you can also quick learning the union of two quick, learnable classes.",
            "Just various ways of composing these things that all workout.",
            "OK, I'm more or less said this."
        ],
        [
            "I think already that the R Max algorithm for Bath and attend and what it does leave didn't say it this way, but you can think of it as what they're doing is a quick learner transition function T, and for anything that's not known, it assumes that the Q values for the state action pairs have maximum reward, so anything that it doesn't know about yet because it hasn't visited enough times is assumed to be really high reward.",
            "Nothing could be higher, and So what it does is it goes out and visits those if it can, and if it can't then it stays among the states that it can visit.",
            "An gets near optimal reward.",
            "So it's very simple, elegant algorithm.",
            "All it's doing is counting the number of times it's tried.",
            "Each state action pair.",
            "When that count goes over threshold, it uses the empirical distribution that it found as truth.",
            "Before that point it just uses some optimistic value.",
            "Doesn't even try to estimate the probabilities.",
            "Alright, so that works.",
            "It gets you a pack, MDP bound and the total mistakes are the bound that I had up before.",
            "Anne, that works out yes.",
            "Samples, yeah, right right?",
            "So in this particular case, what is the transition function?",
            "Well, for some state like you know, the robot is at 30 degrees.",
            "Say it's discretized, so 30 degrees.",
            "It's going to try some action left and we were trying to estimate the probability that actually goes left.",
            "Say, well, we've only tried it 6 times.",
            "Yeah, it's mostly gone left, but we're going to try it for more times before we can say.",
            "We've estimated that probability with extremely high accuracy with high probability.",
            "Not surprisingly, if we want to have some kind of bound that the overall failure probability, the whole learning algorithm is low, it's going to have to estimate the probability of failure of any of the components is going to be much lower than that 'cause all the component.",
            "All the things that have to estimate have to be correct for the overall algorithm to work.",
            "So the papers that talk about this stuff goes through the derivations and say exactly how those quantities relate to each other, and so a lot of uses of the Union bound for example.",
            "So if you want, yeah, if you want the group to succeed then you have to make sure that.",
            "Well, you then make sure that each component succeeds and you estimate the probability of the failures being the sum, which is a little over conservative, but.",
            "It gets polynomial bounds.",
            "The bounds tend to be really I mean like you can see with the 1 minus gamma to the eights and things like that.",
            "The bounds tend to be really.",
            "Not useful in the sense that the number if you literally look at the numbers that they give out.",
            "It's like, Oh yes, yes, you'll have it all you have to do is learn for 772 trillion trials and you'll have an accurate estimate.",
            "But that's not what we in practice.",
            "We don't actually run that many.",
            "It seems like it seems like it tends to work fine with much smaller numbers, but the analysis does seem to lead us towards the useful algorithms.",
            "So even if the numbers aren't exactly right, the overall framework seems to be right.",
            "So a bit weird in your practice meeting framework, and you asked me for million and one penetration.",
            "Yeah, so we're going, so we're not.",
            "We're getting away from that here.",
            "'cause now?",
            "The bound if you actually look at the general quick based learning algorithm, it ends up being polynomial in the number of.",
            "I don't knows that the quick algorithm makes as opposed to the number of states in the world, so that's a big advantage.",
            "So if there's something that you can learn faster than separately estimating a quantity for each state, then the learning actually ends up happening faster."
        ],
        [
            "But we're not quite there yet, so here's an example of using our Max in robotic example where we discretized everything so it looks continuous, but we're not treating it continuous.",
            "This is a Sony AIBO wearing a fetching Raspberry Beret and green Fanny pack so that it can be tracked from above.",
            "So the state information is the position orientation are coming from an overhead camera, and it's trying to escape from the box and its actions are.",
            "Forward backward turn left turn right kind of sidell left inside alright.",
            "And it takes awhile for it to learn about all 4000 states in the world.",
            "But it eventually learns enough that it can get itself out of the box.",
            "So we compared this.",
            "We didn't compare this to Q learning 'cause that would have been.",
            "Well, we would have put the robot at risk.",
            "It takes a very long time for Q learning to learn on the robots, but we did something similar, which is we learn to model.",
            "But we explored by taking random actions occasionally, which is often how Q learning is implemented an it just it is a lot slower but taking random actions in the beginning is really bad because it doesn't know anything yet and it should be taking.",
            "It should be trying everything.",
            "Sorry, taking random actions biased towards what you think is the best is a bad idea.",
            "'cause in the beginning it shouldn't be biased and at the end it should be very heavily biased towards towards the.",
            "Actions that are actually correct.",
            "An arm acts more or less does that automatically because it's estimating it's making these decisions based on how many times it tried actions in each of the states.",
            "So there's things that hasn't tried yet.",
            "It actually will walk all the way across the world to try it right?",
            "It's not waiting for some random sequence of events to happen to get into this new situation.",
            "It's much more directed.",
            "Yeah.",
            "What is the don't pay attention to the graph?",
            "I actually don't remember these are these are unpublished results, so we never really quite cleaned it up, but it's supposed to be like trials, yeah, but why is this 3000?",
            "I think it steps like actual robot actions.",
            "Versus average number of steps."
        ],
        [
            "To get to the goal for trial, all right, but this is not generalizing transitions at all.",
            "So let's start talking about what does it mean to start getting away from detailed information about each separate state.",
            "To start generalizing across states.",
            "OK, so there's lots of ways we can do this.",
            "I'm going to talk about several of them.",
            "The very first one I'm going to talk about is a simple kind of merging of this kind of this robot example with simple.",
            "Graphical model representation.",
            "So here the basic idea is similar.",
            "Similar talent show you a similar robot test to the previous one where there's a Sony Aibo.",
            "It's got its position XY and its orientation data, but there's also in the world aballa Bumble ball that just kind of wobbles around, ignoring the robot.",
            "But it gets in the way and we have a penalty if the robot comes in contact with the ball.",
            "It doesn't literally heard it.",
            "'cause that would be, that would be wrong ethically I guess but but we do give it a -- 1 which is really just as bad and what it's trying to predict from this is the change in the balls position, the robot's position and the robots orientation.",
            "And what we do is in advance.",
            "In this case we had the grad student who built this, created a little independence model that says OK, the ball's position.",
            "That seems wrong.",
            "So the other grad student made a model and then I broke it.",
            "Which I think this edge is is what we want to fix.",
            "The barn at the ball doesn't depend on anything.",
            "I think that edge just should go away.",
            "Well, let's talk about that.",
            "So, so the ball is the ball really is just independent, it just moves around.",
            "So the ball's position should only depend on the ball's old position.",
            "This is this is how the variable changes as a function of what it is now.",
            "So that seems right.",
            "The robots X&Y changes.",
            "Should depend on its orientation.",
            "I guess it depends on what action we're doing turning versus going straight, but I'm going to say maybe this edge should be reversed.",
            "Right, so the robot's orientation impacts its change in position and orientation.",
            "But the robot's position only infects its next position, so this is for something like a.",
            "Like a turn or straight.",
            "Alright, anyway, the point is that we don't have to estimate the full cross product of influences between all the variables and how they're going to change in the next time step.",
            "We can factor them out in this way and then each of these pieces so where the parameters here, so we need to estimate how the ball changes as a function of where it was.",
            "And in fact I think we even ignore that.",
            "We just say the change in the balls position is independent of its current position.",
            "Its new position depends on the old position, but it's changed.",
            "Is independent, the robots X&Y coordinate that, or the change in the XY coordinate don't have any effect, are not affected by where it is because there's no obstacles or anything in this world other than the ball, and that only has an impact on the reward anyway, so this picture I think you should just ignore that picture entirely, 'cause it seems to bear no relation, but the higher the high order thing that I'm trying to get across is that that we start assuming that there's independence in various pieces, and once we've made those assumptions, we can say let's quick learn each of those components.",
            "OK, by observing it enough times and estimating those parameters.",
            "And the bounds for doing that depend on the number of parameters, not on the actual size of the state space."
        ],
        [
            "That's not a robot.",
            "Sorry, that was just that was a motivational shot there.",
            "Yeah alright so this is again this is being tracked by an overhead camera.",
            "The robot is trying to get to the goal position which is in the corner here and the Bumble ball.",
            "OK, hasn't learned much yet.",
            "The Bumble ball really is.",
            "We've plotted the motion of it.",
            "It's very brownian.",
            "It actually moves really very very nicely.",
            "It's almost markov.",
            "I mean, it really seems Markov, which is pretty cool considering that it's just the Bumble ball.",
            "Alright, and we have a bunch of learning trials.",
            "Yeah yeah we actually re implemented some standard SRL simulator simulations, but we did them with the actual robot instead of simulator.",
            "Alright and so.",
            "There it is.",
            "The robot heading for the goal here goes to the goal position OK success?",
            "Notice anything interesting it doesn't know where its face is right, because it's all the positional information comes from the overhead camera, even though there's a camera on the end of it snows, we're not using that for any state information, it's trying to move in whatever way gets it to the goal.",
            "Fastest 'cause it's sort of watching itself as an out of body experience and.",
            "It does it.",
            "It turned out that because of the way it was programmed, the backward walking was slightly faster and so the learning algorithm figured that out.",
            "An would use backwards walking.",
            "This is an unfortunate trial where the Bumble ball kind of parked itself in the way so the robot.",
            "It has a policy which Maps all positions and orientations to where it should go and these low probability events keep happening.",
            "Get out of my face.",
            "I think this is more or less what robot frustration might look like.",
            "At that point, you start checking it's like is it really random?",
            "Is it really random?",
            "It doesn't seem random anymore.",
            "But eventually eventually come on.",
            "You can do it.",
            "Do.",
            "Squeeze in there before the ball gets in the way.",
            "And there we go, alright so.",
            "The reason this was able so that was like trial 15 or something like that is that what it said.",
            "The reason it was able to learn in such a small number of trials is because it totally took advantage of all this conditional independence that we built into the function approx."
        ],
        [
            "Later for the transition function.",
            "So that's one sort of function that you can build.",
            "OK, I think OK, so there's I think if I think we're saying like 10 more minutes or something like that, right?",
            "So if I want to speed through this thing 'cause I think I'm getting.",
            "I got myself a little bit down but there's other kinds of ways that you can introduce various kinds of structure to learn things faster.",
            "In the example that I just gave, there was in spite of that horrible picture that I do, there was no independence of the state on how things were going to change.",
            "Which makes no sense.",
            "Surely there must be some influence, but but let's say that there's another way that you can think about.",
            "It.",
            "Is every individual state has an influence.",
            "That's back to the original model.",
            "Well, what if there's different types of States and within each type there's a mapping to how things are going to change, But this this is a separate types can be learned separately.",
            "So we took a model that had been published in the reinforcement learning literature for doing that with MVP's kind of factorizing the MVP into these types.",
            "And we took advantage of it and were able to learn."
        ],
        [
            "Gridworld a lot faster.",
            "Have the right kind of generalization, speed things up.",
            "We did the same thing on a robot example, so here this one is actually being tracked using Motion tracker things little silver balls.",
            "And it's trying to get to this white box.",
            "You might not be able to see in the corner here an we told it that different color regions may have different dynamics, so it learns a separate action model for what all these actions do on the sandbag versus on the black painted wood.",
            "And the result of that is actually able to take advantage of this in a substantial way.",
            "But if we if we ignore those distinctions, certainly if we learn each individual place in the world and map that to an action model, it takes forever to learn.",
            "If we take the whole world and say no, it's really just one big space and just learn the same action model for the whole space.",
            "That doesn't work well either, because the sand is actually a little bit slippy, and So what tends to do is that it tries to point itself to the goal while on the sand, and it often overshoots, and then it falls out of the world and it's minus one.",
            "So being able to condition on these two types ends up.",
            "Learning a more accurate model but still very, very quickly.",
            "We did something similar.",
            "The Grad student who did this spent almost all of her time trying to find different surface is.",
            "It turns out that it's really easy to find a surface that the robot can't move on at all, but that doesn't make for good learning experiments.",
            "And of the ones that it can move on, they all kind of act the same, so she eventually she and her mom apparently melted wax on their stove at home and then embedded smooth rocks from an aquarium in it, and so that's what you just saw there a moment ago and it was like just bumpy enough, just smooth enough that it could move, but it.",
            "Move really clunkily there you can see it there.",
            "But again, what it learns in this case is based on image analysis.",
            "It breaks the two regions into two separate piles, and that's built in right?",
            "It's not learning that, it's just taking whatever the texture analysis says as the two regions or however many regions it finds, and learning separate models for each of those.",
            "But it really does make a difference.",
            "It learns a very different model for turning on the rocks, then turning on the carpet, and it really, it's like much happier on the carpet, just it gets its groove on."
        ],
        [
            "Alright, so wait, this is the payoff to that one.",
            "Yeah, so more or less.",
            "Here's the robot, but we built a sort of a funky world that if it was to use the same model for the Roxanne the carpet, it wouldn't know that it could make this tight turn to get to the goal over here because it would say Oh well, it's one big noisy surface.",
            "Well then I can't navigate in this tight space."
        ],
        [
            "So that worked out OK. Another form of generalization that you can get is by factoring using graphical model representations of the transitions.",
            "This is the taxi problem which I don't know if people hadn't seen it before the example.",
            "The video game example in the beginning is actually a classic reinforcement learning example that people have used in like 300 papers.",
            "I was able to find that use this example.",
            "So obviously you haven't read any of those papers if you didn't know what was going on.",
            "But this phrase is a taxi problem.",
            "The thing that for us was like a big orange square or something like that is the taxi.",
            "The little red dot is the passenger, and those colored spots are actually labeled with their colored labels in their literature, and there's walls and so forth.",
            "One thing that I notice is you guys didn't run into any walls right now.",
            "The reinforcement learning algorithms tend not to do that right.",
            "They tend to run into walls over and over again, and in fact, if they see a wall that they hadn't hit before, our Max gets all excited.",
            "It's like, oh, something new.",
            "I could bash my head against.",
            "So it tends to learn relatively slowly, but if you give it a factor Rep."
        ],
        [
            "Mentation, so in this case I think I drew this one correct.",
            "I edited last night.",
            "We'll see if I broke it or not.",
            "The taxi has position.",
            "The passenger has a position and the passenger has a desired destination.",
            "Taking the action North, the new position for the taxi only depends on the old one.",
            "The new of the passenger only depends on the old one and the destination.",
            "It remains unchanged.",
            "It only depends on the old one, so there's a lot of conditional independence in this model, which is why even though there's 500 some odd states, we were able to solve that problem as a group.",
            "In fewer than 500 steps right, a lot fewer than 500 steps.",
            "In fact, if I gave you a whole series of these problems now, you'd be performing pretty much optimally on all of them after the first one or two.",
            "So having this kind of conditional independence information is really helpful there.",
            "Huh?",
            "I should have credited Kerns and Kohler for showing that you could learn these kinds of models efficiently given the graphical structure.",
            "So if you have, if you're told so, the learner is told, here's the conditional independence structure.",
            "Now go estimate the probabilities and figure out how to behave in it.",
            "It can do that efficiently in the size of these structures, not the size of the state space.",
            "The state space could be combinatorially large, but it only has to estimate the probabilities of the."
        ],
        [
            "So net.",
            "But you could say, but you didn't.",
            "'cause I'm going so fast.",
            "But what if you don't have the structure?",
            "Well, it turns out that learning an unknown structure is actually a fundamentally different problem because it turns, whereas when you have the the conditional independence structure each each experience that you get can be kind of credited to some piece of the network, so you can actually just keep counts and estimate all the probabilities that you need.",
            "If you don't know the structure, it's really hard to know how to apportion that data, so it ends up being a very different kind of problem.",
            "So one thing I could do is put this off till tomorrow, but some of the some of this stuff up tomorrow, so that would that be OK.",
            "So let me see if I can at least get to some kind of a punch line.",
            "Alright, I'll return to this piece.",
            "It another interactive demo thing.",
            "Yeah, don't look, it's too late.",
            "So."
        ],
        [
            "One of the things that we did is to look at the taxi problem.",
            "The other thing that you do in the taxi problem is not just pay attention to conditional independence, but the fact that there's object right.",
            "So once in fact you could see that there were walls and you just knew how walls work.",
            "You didn't even try to pass through the walls, but you can imagine a learner that was a little bit more naive, at least discovering how walls work and then transmitting that information to all the other walls that it sees, kind of generalizing its experience to walls and elsewhere in the environment.",
            "And so we did look at.",
            "We created a learner that takes advantage of that kind of information by.",
            "Assigning representing the model in terms of the objects in the world.",
            "So there's ataxia, passenger there's walls as destinations and instead of keeping track of explicit states or even features of states, we talked about objects and attributes of objects.",
            "So the learner specifically tries to represent his transitions in terms of those objects and we do get much more human like kinds of expl."
        ],
        [
            "Patient, in that case, it learns the rules that it learns or things like if you go North and you're not touching to the North.",
            "Or sorry the taxi is not touching the wall to its North, then the Y coordinate of the taxi gets incremented, right?",
            "That's sort of what it means to go North.",
            "Sort of.",
            "It's closer, certainly closer than what these other models learn, which is well, here's what go North means.",
            "If you're in State 532, that means you go to State 511.",
            "If you're in State 26 that this is much more generalizable, dropping off a passenger has a similar representation, and the quick bound that we get for learning this kind of structure is polynomial, and the types of objects in the world, it's exponential, and how complex their conditions are 'cause it does have to check different ways that they could interact.",
            "But if we now compare how fast it takes to learn optimal behavior in the taxi problem.",
            "Q Learning with epsilon greedy exploration where it just tries the thing that it thinks is right, but with some probability something random.",
            "Takes about 50,000 steps to get this task down.",
            "Flat are Max.",
            "We're actually keeping track of which states it's been too, and it's pushing itself to new states to visit them, but still representing the states is just a big giant graph.",
            "It goes down by an order of magnitude two, like 4000.",
            "Factored our Max would actually give it the conditional independent structures so it only has to pay attention to features in the world, so it can actually learn about a transition that it's never seen before because it's seen the different pieces of that transition already occur.",
            "Then it goes down to about half of that to 2000 steps.",
            "If we do it based on objects, the objects in the world, we can get down to like 140, three, which seems like OK, but you've now built in everything you know, which is sort of true.",
            "But people man the experiments that I've done maybe 50 ish steps people get.",
            "It depends a lot on whether you spend a lot of your life playing.",
            "Video games turns out there was one person who says he claims that I showed the slide.",
            "He'd never seen the taxi problem before he showed I showed in the slide.",
            "He said, well, that's the same color.",
            "Is that so?",
            "Probably it's navigation.",
            "The A and the B is going to pick up a drop off of that object, so he solved it with no trials at all.",
            "He claimed to but but you know, but you can almost believe that you get a lot of it just by looking.",
            "So that's pretty remark."
        ],
        [
            "So we use this idea.",
            "This is this is going to be my punch line and then I'll sign off.",
            "So we use this idea to learn the game pitfall.",
            "Has anybody wasted time on this particular problem?",
            "Thank you, thank you so here.",
            "So what's going on here is this guy is trying to get off the right side of the world.",
            "There's a ladder he can get in.",
            "There's a there's a wall that's going to block his way here.",
            "There's a log that costs points.",
            "He's got North, South East, West diagonals and all those with the button press so you can jump.",
            "And what we do is we actually took.",
            "An emulator, an actual Atari emulator for this game and plugged it into a reinforcement learner by saying we just we go through the pixels and and find regions that are of the same color and will say that's an object.",
            "So objects become rectangles.",
            "It doesn't look as beautiful as the graphics were.",
            "We simplified them substantially by making everything just colored rectangles, but then the learner had to predict how those color rectangles were going to move as a function of the actions of the joystick actions.",
            "So we actually then plug the learner into the Atari emulator and let it play Atari games, 'cause you know.",
            "We don't have time for it anymore, but our programs do and so.",
            "So, So what?",
            "It's trying to learn is, well, what is jumping do when these two objects are touching?",
            "What is jumping do when these two objects are not touching or when this one's above that one is trying to learn how the object interactions?",
            "Caused the objects to change so.",
            "Sorry, so as you can see what it ends up doing is it explores in this really interesting way, so it's so in the beginning it tries all kinds of crazy jumps, but then it gets tired of that and just walks across the ground.",
            "Because this is all the same it predicts.",
            "This is all the same, but then when it gets to the whole it's like ooh something new and it tries all the different jumps around the whole and it's like oh the ladder.",
            "I don't know what that does and it goes down.",
            "There is hopping around on the bottom floor a little bit 'cause it's never tried that before, but then it then it then it just beelines right to the wall, right?",
            "It doesn't try all the states in between.",
            "It doesn't try jumping in all possible configurations.",
            "It tries jumping next to the wall just like you guys wanted to go to the colored square, and when you were playing the video game, you didn't try all the squares in the grid.",
            "He's back at the hole and kind of fixated on that.",
            "And now there's the Logn.",
            "He starts kind of rubbing himself up against the log in all possible ways.",
            "But then he gets off the screen.",
            "But now he's learned about every object.",
            "There's nothing else to do except win this game, so it just goes right across.",
            "It jumps over the hole.",
            "It jumps over the log and then it does a happy dance so.",
            "And I will.",
            "I will sign off for now.",
            "I've got more to say tomorrow, but thanks a lot for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everybody, I'm Michael littman.",
                    "label": 0
                },
                {
                    "sent": "I would wear the badge, but they say that it interferes with the microphones.",
                    "label": 0
                },
                {
                    "sent": "I've run out of places to hang things off of myself, got pocket pocket, other pocket.",
                    "label": 0
                },
                {
                    "sent": "So I first met Zubeen before he went to grad school.",
                    "label": 0
                },
                {
                    "sent": "I think he was doing a summer internship where I was a researcher in New Jersey in the US, and it's just so wonderful to see him, you know.",
                    "label": 0
                },
                {
                    "sent": "As the big shot everywhere, he's such a great guy so I really I'm just really flattered that that he invited me to be here and I'll do my best to convey some of the sorts of things that I work on to all of you.",
                    "label": 0
                },
                {
                    "sent": "Alright, so good.",
                    "label": 0
                },
                {
                    "sent": "So I was my title was reinforcement learning and then when I looked at what I actually was going to talk about, I decided I should probably modify it a little bit less.",
                    "label": 0
                },
                {
                    "sent": "You think I've covered all of reinforcement learning, so I'm going to focus on model based reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "But I'll say a little bit about reinforcement learning more generally, so you at least can put that into some kind of context.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the basic pitch, so I'm going to have two days to talk to you guys.",
                    "label": 0
                },
                {
                    "sent": "First day I'm going to focus on some background information introduction specifically about reinforcement learning and some classical methods like you learning things that we know about these methods, and then I'll focus on this sort of stuff that I've been really interested in lately, which is model based reinforcement learning and how it relates to that.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about the pack MVP framework and quick learning now looking at the schedule of what's been going on before I came.",
                    "label": 0
                },
                {
                    "sent": "I expected to see lots of stuff like I studied when I was studying machine learning back in grad school, but it seems to be completely different Now, so I don't even know if it's the same stuff anyway.",
                    "label": 0
                },
                {
                    "sent": "So did you guys look at pack at all?",
                    "label": 0
                },
                {
                    "sent": "Have we talked about have you talked about probably approximately OK, good, alright?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take that idea and apply it in the reinforcement learning setting.",
                    "label": 0
                },
                {
                    "sent": "Explain what that means and tomorrow I'll talk about more current trends, other kinds of more scattered topics that connect in various ways but are a little bit less coherent and foundational, but are kind of the things that people are really working on now.",
                    "label": 0
                },
                {
                    "sent": "And if you've got great ideas there.",
                    "label": 0
                },
                {
                    "sent": "There's a community who would very much like to hear from you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that's the basic pitch.",
                    "label": 0
                },
                {
                    "sent": "So let me start off just, you know, since we're at Microsoft and and they make computer software, we're going to play a little game.",
                    "label": 0
                },
                {
                    "sent": "Alright, so see if I can get this up and running.",
                    "label": 0
                },
                {
                    "sent": "This screen is awesome by the way.",
                    "label": 0
                },
                {
                    "sent": "I've never seen such a good projector look at that look how sharp man makes my makes my demo look good.",
                    "label": 0
                },
                {
                    "sent": "Alright, so alright so this is the game and the rules.",
                    "label": 0
                },
                {
                    "sent": "So here's the thing.",
                    "label": 0
                },
                {
                    "sent": "So I don't know how we're going to play it all as like how many of us are there 120 or whatever?",
                    "label": 0
                },
                {
                    "sent": "That may be a little tricky so we're going to try it anyway.",
                    "label": 0
                },
                {
                    "sent": "See how that goes.",
                    "label": 0
                },
                {
                    "sent": "So here's the rules of the game.",
                    "label": 0
                },
                {
                    "sent": "There's you're trying to end the game you're trying to win the game.",
                    "label": 0
                },
                {
                    "sent": "That's the rule so far, and you've got your actions.",
                    "label": 0
                },
                {
                    "sent": "The things that you can do are.",
                    "label": 0
                },
                {
                    "sent": "Up, down, left, right A&B.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Begin.",
                    "label": 0
                },
                {
                    "sent": "Hey we got it up.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "B.",
                    "label": 0
                },
                {
                    "sent": "Up I heard up down here up again left.",
                    "label": 0
                },
                {
                    "sent": "Down.",
                    "label": 0
                },
                {
                    "sent": "Down a.",
                    "label": 0
                },
                {
                    "sent": "Wait wait wait, wait, OK, so here's the thing.",
                    "label": 0
                },
                {
                    "sent": "So I've done this demo many many, many times and at that moment everybody every time I've ever done this they make a noise, but you guys didn't make any noise, so you're supposed to go or something like that, but maybe you didn't notice.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you're just so blase about the world that this like, Oh yeah, well, you know.",
                    "label": 0
                },
                {
                    "sent": "Course the red circle was going to get small van.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is clearly a tough crowd so I will.",
                    "label": 0
                },
                {
                    "sent": "I will do OK.",
                    "label": 0
                },
                {
                    "sent": "So the last thing that I hit was a the last thing that happened is there was a red circle and it got smaller.",
                    "label": 0
                },
                {
                    "sent": "And it sort of seemed vaguely.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this is exactly true, but I heard I heard of what sounded like a female voice saying B and then a whole bunch of male voices saying hey, but it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It wasn't just it wasn't just like let's do a now.",
                    "label": 0
                },
                {
                    "sent": "It's like no, no no.",
                    "label": 0
                },
                {
                    "sent": "I really want it to be OK Alright, so here we are we hit a last.",
                    "label": 0
                },
                {
                    "sent": "I hear it up and I'm going to hit.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna hit up.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right.",
                    "label": 0
                },
                {
                    "sent": "Here there's disagreement.",
                    "label": 0
                },
                {
                    "sent": "Let's just let's let's follow this thread.",
                    "label": 0
                },
                {
                    "sent": "Go ahead good.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Up be good OK good thought.",
                    "label": 0
                },
                {
                    "sent": "Left you were supposed to make a noise again, but that's OK.",
                    "label": 0
                },
                {
                    "sent": "I'll let I'll let it go.",
                    "label": 0
                },
                {
                    "sent": "Write a.",
                    "label": 0
                },
                {
                    "sent": "Down down down down left.",
                    "label": 0
                },
                {
                    "sent": "Right, so somebody who just said know what tell me what you're thinking?",
                    "label": 0
                },
                {
                    "sent": "What would you like?",
                    "label": 0
                },
                {
                    "sent": "Take it to the right, alright?",
                    "label": 0
                },
                {
                    "sent": "So let's lol just will do be I heard that.",
                    "label": 0
                },
                {
                    "sent": "A up up left left.",
                    "label": 0
                },
                {
                    "sent": "Left up up.",
                    "label": 0
                },
                {
                    "sent": "You win alright.",
                    "label": 0
                },
                {
                    "sent": "Well done, well done.",
                    "label": 0
                },
                {
                    "sent": "Well that went that went about as well as could be expected for a group of 100 people playing a video game.",
                    "label": 0
                },
                {
                    "sent": "You don't get to do that every day.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's how do I get my screen back though?",
                    "label": 0
                },
                {
                    "sent": "It says keynote.",
                    "label": 0
                },
                {
                    "sent": "Interesting oono",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep, I just didn't see it.",
                    "label": 0
                },
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": "OK good so OK.",
                    "label": 0
                },
                {
                    "sent": "So now you know reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "This was really great.",
                    "label": 0
                },
                {
                    "sent": "No wait.",
                    "label": 0
                },
                {
                    "sent": "So let me let me let me say a little bit more.",
                    "label": 0
                },
                {
                    "sent": "So that was an example of of we were basically faced with a reinforcement learning task.",
                    "label": 0
                },
                {
                    "sent": "We were given the goal at least the knowledge of when we achieved the goal or not.",
                    "label": 0
                },
                {
                    "sent": "We were given a set of actions we were given.",
                    "label": 0
                },
                {
                    "sent": "State were given some information about what's going on right now we had to choose actions to lead us through and get to a goal.",
                    "label": 0
                },
                {
                    "sent": "That's not the only kind of game we can play in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "Very simple example of a robotic system that we built in our lab, but in general reinforcement learning we have an agent interacting with some environment.",
                    "label": 0
                },
                {
                    "sent": "It was you and the screen a minute ago perceptions or state actions, things that you can do, rewards and this sequence repeats.",
                    "label": 0
                },
                {
                    "sent": "They get a reward, but then you get you.",
                    "label": 0
                },
                {
                    "sent": "You have to live in the bed that you just made for yourself, right?",
                    "label": 0
                },
                {
                    "sent": "So whatever state you actually brought yourself too, that's when you.",
                    "label": 0
                },
                {
                    "sent": "That's where you get to make your next decision, so this whole thing just keeps looping over and over again and your task is to choose actions to maximize rewards.",
                    "label": 1
                },
                {
                    "sent": "Now in the game that we just played, the reward was really simple is basically.",
                    "label": 0
                },
                {
                    "sent": "No cost, no cost until you win, and then you win and we get to go on with the lecture.",
                    "label": 0
                },
                {
                    "sent": "What makes us learning and not, say planning.",
                    "label": 0
                },
                {
                    "sent": "There's a whole field in artificial intelligence that studies planning is that there's something missing in the background knowledge or something about the rules of the game that we don't necessarily know.",
                    "label": 1
                },
                {
                    "sent": "So in this concrete example with the robot, it's actually learning which way to turn.",
                    "label": 0
                },
                {
                    "sent": "It's only got 2 actions, which we could think of his left and right, but it just thinks of, as you know, 615 and 937.",
                    "label": 0
                },
                {
                    "sent": "It's trying to minimize time so it's keeping track of the Clock as it's going, and we're trying to do is see this bar that doesn't know that, just like you didn't know what the goal was at the end of your game, but but it knows it when it sees it, so we have a little piece of code in there that detects whenever that unnatural pink color comes up that it has achieved its goal and it gets a plus one for that likes those plus ones and what it's using to guide itself is.",
                    "label": 0
                },
                {
                    "sent": "It's got camera input, which it takes to be in this case, just a histogram of colors, so each direction that it looks out of its.",
                    "label": 0
                },
                {
                    "sent": "Nose camera thiebeau.",
                    "label": 0
                },
                {
                    "sent": "It sees out and it counts up how many of each different kind of color it sees from a set of something like 15 that we just listed out, and that's how it represents the world.",
                    "label": 0
                },
                {
                    "sent": "And then it takes an action and it sees how that changes.",
                    "label": 0
                },
                {
                    "sent": "An based on this experience of connecting.",
                    "label": 0
                },
                {
                    "sent": "Here's what I saw.",
                    "label": 0
                },
                {
                    "sent": "Here's what I did.",
                    "label": 0
                },
                {
                    "sent": "Here's what I saw next.",
                    "label": 0
                },
                {
                    "sent": "did I get a reward or not?",
                    "label": 0
                },
                {
                    "sent": "It can start to get better and better at its task.",
                    "label": 0
                },
                {
                    "sent": "And what this video is supposed to show is the Aibo actually getting better at figuring out based on what it sees, which way it should turn to face the pink ball.",
                    "label": 0
                },
                {
                    "sent": "In the beginning, it doesn't know.",
                    "label": 0
                },
                {
                    "sent": "It doesn't even know that the actions are inverses of each other, right?",
                    "label": 0
                },
                {
                    "sent": "It just knows that.",
                    "label": 0
                },
                {
                    "sent": "It does, it does the first action and then it does the second action and things look sort of vaguely like they did a moment ago, and it's going to have to put those together and eventually start acting well in the environment.",
                    "label": 0
                },
                {
                    "sent": "So this I think this example is really nice because it's.",
                    "label": 0
                },
                {
                    "sent": "It's real in the sense that it's actually working.",
                    "label": 0
                },
                {
                    "sent": "The robot here is dealing with real data from the real world.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily what you call a practical problem now that we've solved this, it's not clear how to sell it or monetize it, but the point is that we can connect up these ideas with data actually coming.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From a real data source.",
                    "label": 0
                },
                {
                    "sent": "Alright, so mainly if you want what do you need to do?",
                    "label": 0
                },
                {
                    "sent": "If you're trying to solve one of these reinforcement learning problems so couple of things are really important and I think you can see them in both the examples that we've looked at so far.",
                    "label": 0
                },
                {
                    "sent": "One is that you have to generalize your experience in the little video game thing that we played.",
                    "label": 0
                },
                {
                    "sent": "There weren't very many states that we visited more than once, and in fact the total number of states that were reachable is on the order of about 500.",
                    "label": 0
                },
                {
                    "sent": "So there was.",
                    "label": 0
                },
                {
                    "sent": "That would have been really tedious, even maybe even more tedious, to actually have to go through and actually experience all those states.",
                    "label": 0
                },
                {
                    "sent": "But that's not what you did.",
                    "label": 0
                },
                {
                    "sent": "You started to have a higher level picture of how the States related to each other.",
                    "label": 0
                },
                {
                    "sent": "And how we might get to some new and interesting States and you know, similar things happen in the robot.",
                    "label": 0
                },
                {
                    "sent": "Example, you can think of this as being the learning problem, taking data from the world and coming up figuring out with the regularity's are so you can generalize from that.",
                    "label": 0
                },
                {
                    "sent": "That's one part of the problem.",
                    "label": 0
                },
                {
                    "sent": "In reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We have this other side of the problem as well, which is about making sequential decisions.",
                    "label": 0
                },
                {
                    "sent": "So dealing properly with delayed gratification.",
                    "label": 1
                },
                {
                    "sent": "So you may can you take an action and it may not pay off right away.",
                    "label": 0
                },
                {
                    "sent": "So in the video game example, at one point we had as a group subgoal.",
                    "label": 0
                },
                {
                    "sent": "We need to get to the Red Square.",
                    "label": 0
                },
                {
                    "sent": "Why the Red Square by the way?",
                    "label": 0
                },
                {
                    "sent": "'cause the ball was red.",
                    "label": 0
                },
                {
                    "sent": "Yes Sir, you didn't know this, but if we had played the game again, that ball might have been a different color.",
                    "label": 0
                },
                {
                    "sent": "But that's cool that you figured that out.",
                    "label": 0
                },
                {
                    "sent": "Even without seeing the second example.",
                    "label": 0
                },
                {
                    "sent": "So, but we had, so we had an idea of where we wanted to go, and no single action was going to accomplish that, so we had to chain them together to get to where we wanted to go.",
                    "label": 0
                },
                {
                    "sent": "And you know, the planning community has thought about these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "There's lots of problems in the world that have to have this kind of structure where you actually have to generate a sequence to achieve your end.",
                    "label": 0
                },
                {
                    "sent": "And what glues these two things together is the problem of exploration and exploitation because the learner is actually embedded in the problem itself and it's trying to win the game or get high reward and it's trying to figure out how to win the game and get high reward.",
                    "label": 0
                },
                {
                    "sent": "Each action that it takes is sort of playing both of those roles.",
                    "label": 0
                },
                {
                    "sent": "And sometimes there's some.",
                    "label": 0
                },
                {
                    "sent": "There's some tension or tradeoffs between them where you want to take an action 'cause you're going to learn something from it, and you want to take an action because you're going to gain something from it, doesn't directly.",
                    "label": 0
                },
                {
                    "sent": "So this problem in some sense, if reinforcement learning gets to claim anything again.",
                    "label": 0
                },
                {
                    "sent": "For itself, the learning people think about learning, planning people, think about planning reinforcement, learning people can think about how these two things are going to do together in a way that's going to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The whole system work.",
                    "label": 0
                },
                {
                    "sent": "The the standard formal mathematical model that people in the field uses that of the Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "It's very tempting to equate the two and CEO reinforcement learning that MVP's, but I don't think people really should think about it that way.",
                    "label": 0
                },
                {
                    "sent": "You really want to think of MVP's as being a very convenient mathematical model that captures a lot of what we care bout in the reinforcement learning setting and therefore coming up with better algorithms and better ways of dealing with these things is a good thing.",
                    "label": 0
                },
                {
                    "sent": "So probably for the rest of my two lectures, I will equate the two, but I just I should at least make it clear that you really shouldn't think about it that way.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this model dates back to the 50s.",
                    "label": 0
                },
                {
                    "sent": "Richard Bellman and model sequential environments.",
                    "label": 0
                },
                {
                    "sent": "The way to think about this, and we'll have a couple examples of this as we go.",
                    "label": 0
                },
                {
                    "sent": "We've got some kind of Markov system with N states from each state.",
                    "label": 0
                },
                {
                    "sent": "You can choose any of K different actions.",
                    "label": 0
                },
                {
                    "sent": "And just to make things well defined, we have some notion of a discount factor that says we're going to be getting rewards as we move through the state space.",
                    "label": 0
                },
                {
                    "sent": "Future rewards are worth a little bit less than current rewards and their discounted in this case, geometrically at what's going to happen is that each step each time step T the agent is going to be informed of what the state of the system is SMT and it gets to choose an action which will call a sub T, and there's going to be then as a result of that, some payoff are some tea whose expected value is our of SPM.",
                    "label": 0
                },
                {
                    "sent": "Hating these chairs so much.",
                    "label": 0
                },
                {
                    "sent": "Are of SMTHNG City is going to be the expected reward, and then there's going to be a transition to an exit.",
                    "label": 0
                },
                {
                    "sent": "Notice that the expected reward only depends on the state and the action.",
                    "label": 0
                },
                {
                    "sent": "That's the sense in which it's mark off the other sensor which is Markov, is the probability the next date is determined by this function.",
                    "label": 0
                },
                {
                    "sent": "Capital T which says.",
                    "label": 0
                },
                {
                    "sent": "If you're in some state SMT and you choose some action, a sub T, the probability of reaching some next state S prime is this expression, and so really you can think about what's going on here is, it's an extended conversation between the agent who's making the decisions and deciding what to do and the environment which is the transition and reward functions, and it goes like this right state goes this way.",
                    "label": 0
                },
                {
                    "sent": "Action goes this way.",
                    "label": 0
                },
                {
                    "sent": "Reward goes this way and then we get.",
                    "label": 0
                },
                {
                    "sent": "Then we play again.",
                    "label": 0
                },
                {
                    "sent": "That round just goes on and on and on.",
                    "label": 0
                },
                {
                    "sent": "Like that alright so?",
                    "label": 0
                },
                {
                    "sent": "Given that set up and given these quantities as we've we've defined them, we can.",
                    "label": 0
                },
                {
                    "sent": "We can define what it means to behave optimally in this environment.",
                    "label": 0
                },
                {
                    "sent": "In this class of environments, and the way we do that is, we say, well we can define QSAQ is the the Q function, or sometimes people call it quality function, but I think it's just a convenient letter that says what is the future expected reward for an agent that acts optimally, maximize its reward starting from state S, assuming the very first thing it does is action A.",
                    "label": 0
                },
                {
                    "sent": "Which might not be the best thing to do, but let's just say we're going to take one action A and then from then on we're going to smartly and take, take, take the right actions.",
                    "label": 0
                },
                {
                    "sent": "We could say what this is going to be, whatever the immediate reward is for taking that action in that state, plus the rest, and then what the rest is is where we're going to end up in some new state, in particular state S prime is going to be reached with probability, SAS prime and from that state we're going to be choosing actions optimally, so will take whichever action has the highest value in that resulting state.",
                    "label": 0
                },
                {
                    "sent": "So Max overall actions.",
                    "label": 0
                },
                {
                    "sent": "Of the Q values of the state that we end up in, take the expected value of that.",
                    "label": 0
                },
                {
                    "sent": "So we're going to sum it up overall next states to average to average all the next dates together by their probability, and that's that's our future.",
                    "label": 0
                },
                {
                    "sent": "And then we discount that because that's one step into the future.",
                    "label": 0
                },
                {
                    "sent": "So this gets us our geometric discounting.",
                    "label": 0
                },
                {
                    "sent": "This gets us our expected value, and somewhat inconveniently, there's a queue on both sides of this equation.",
                    "label": 0
                },
                {
                    "sent": "I get to be between it for a moment and so that we have a system of simultaneous system of equations which is almost linear.",
                    "label": 0
                },
                {
                    "sent": "You know if we could just blot out the Max, this would be a system of simultaneous system of linear equations, which would be very convenient.",
                    "label": 0
                },
                {
                    "sent": "Well, the Max makes it a bit more inconvenient, but nonetheless this has a unique solution and there's a bunch of algorithms that can be used to solve that.",
                    "label": 0
                },
                {
                    "sent": "If you know the transition function and the reward function.",
                    "label": 0
                },
                {
                    "sent": "OK, those algorithms tend to run in time like polynomial in the total number of states in action, so NNK.",
                    "label": 0
                },
                {
                    "sent": "Which is sometimes inconvenient because and like for example in the little.",
                    "label": 0
                },
                {
                    "sent": "Video game example that we did before.",
                    "label": 0
                },
                {
                    "sent": "If the grid is big enough and there's enough different objects going on, the number of states can be astronomically large.",
                    "label": 0
                },
                {
                    "sent": "So being polynomial in that is, it's not that much to be proud of, but nevertheless, it's nice that when the state space is small that you can actually define what the optimal solution is, and it's all very convenient.",
                    "label": 0
                },
                {
                    "sent": "Any any questions about that?",
                    "label": 0
                },
                {
                    "sent": "How we doing so far?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Expected fewer what about another policy which gives you slightly or expected reward but waited over parties?",
                    "label": 0
                },
                {
                    "sent": "Ah, right, so this is completely focused on this sort of risk neutral case.",
                    "label": 0
                },
                {
                    "sent": "We're just going to take expected value across the board, yes, so there has been some work on looking at things, ways of redefining this that are more risk sensitive in various ways.",
                    "label": 0
                },
                {
                    "sent": "Get things, get uglier and it's not been as popular, but certainly if you're thinking about, for example, financial applications, it's really critical to factor in some of that variance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I won't say much more about that, but that's it's a valid point.",
                    "label": 0
                },
                {
                    "sent": "I can point you at some literature if you if you like.",
                    "label": 0
                },
                {
                    "sent": "OK, alright so but anyway, given this this function that we could solve for if you know this value if you have these values Now we know what we need to do to behave optimally.",
                    "label": 0
                },
                {
                    "sent": "We just choose at time T when we're in state St, we look at all the actions we can take from there, and whichever has the highest Q value.",
                    "label": 0
                },
                {
                    "sent": "That's the one we choose at time T. So as long as we keep acting that way, then we maximize our cumulative expected discounted reward.",
                    "label": 0
                },
                {
                    "sent": "Now what makes us not just the computational problem or straight up computational problem is that we're going to assume that the reward function and the transition function are to some extent unknown, so it can't be that we can just take some specification closer eyes to the real world, grind away on a supercomputer for a bit, and then act optimally.",
                    "label": 0
                },
                {
                    "sent": "We're going to have to interact with the environment to find out what really happens when we take actions, and so that's in some sense some amount of experimentation, some amount of learning is going to be needed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here's that robot finding the ball problem again.",
                    "label": 1
                },
                {
                    "sent": "In two forms, this is sort of how it would look from above.",
                    "label": 0
                },
                {
                    "sent": "This my best attempt at drawing and I bow from above, so this is the eyeball is at different orientations at different times.",
                    "label": 0
                },
                {
                    "sent": "Some of the orientations that can see the pink ball, some of them it can't.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here's how you might view that as a Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Each of those orientations now becomes a state, and the actions are transitions between these states.",
                    "label": 0
                },
                {
                    "sent": "The arrows could be that in some cases some actions actually have some probability of keeping you in the same state.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they move you a little bit one way or the other in the chain.",
                    "label": 0
                },
                {
                    "sent": "Maybe it skips in some places, but some of the states actually have reward associated with them.",
                    "label": 0
                },
                {
                    "sent": "Some of them have sort of mild time penalties for just making those transitions from the perspective of how we're going to be thinking about these for the rest of.",
                    "label": 0
                },
                {
                    "sent": "Today we can sort of imagine that this is.",
                    "label": 0
                },
                {
                    "sent": "This is the representation that we get.",
                    "label": 0
                },
                {
                    "sent": "We get States and actions and transitions.",
                    "label": 0
                },
                {
                    "sent": "We get like a graph structure like this and we're going to work on that in various ways.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so OK.",
                    "label": 0
                },
                {
                    "sent": "So now we have a basic notion of what it means to be in an environment.",
                    "label": 0
                },
                {
                    "sent": "What it is that we're trying to do to maximize behavior environment.",
                    "label": 0
                },
                {
                    "sent": "How do we do this as a learning problem if we don't know the transition and reward function?",
                    "label": 0
                },
                {
                    "sent": "How do we go about actually acting to do that?",
                    "label": 0
                },
                {
                    "sent": "It turns out there's other families of approaches in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "We're going to be focused on.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm going to be focused on model based reinforcement learning, but it's really helpful and we're going to mention I'm going to mention all these other ones as well, so let me sketch it out for you.",
                    "label": 0
                },
                {
                    "sent": "So the one class of methods.",
                    "label": 0
                },
                {
                    "sent": "Are what could be called policy search methods and policy search methods are trying to learn the mapping from states to actions.",
                    "label": 0
                },
                {
                    "sent": "When you see this, what should you do to maximize reward and we represent that mapping as a policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "That's a good thing, because that really is what we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Ultimately is how to behave how to actually act in the world to maximize reward.",
                    "label": 0
                },
                {
                    "sent": "So that seems like a very convenient thing to be trying to learn.",
                    "label": 0
                },
                {
                    "sent": "The problem with it is that we don't actually get direct experience of this form.",
                    "label": 0
                },
                {
                    "sent": "There's nothing in the world that says, hey, you were just in that state.",
                    "label": 0
                },
                {
                    "sent": "I know you took Action 7 but you really should have taken action one.",
                    "label": 0
                },
                {
                    "sent": "So when you set up your learning set of learning examples, make sure that you tell yourself to go for action.",
                    "label": 0
                },
                {
                    "sent": "1 yes.",
                    "label": 0
                },
                {
                    "sent": "Is it obvious that we should always take the same action in the same state?",
                    "label": 0
                },
                {
                    "sent": "So because of discounting so I don't know if it should be obvious or not?",
                    "label": 0
                },
                {
                    "sent": "I always shy away from using that word 'cause you always offend people.",
                    "label": 0
                },
                {
                    "sent": "So yes, of course it's obvious.",
                    "label": 0
                },
                {
                    "sent": "OK, moving on no no, no.",
                    "label": 0
                },
                {
                    "sent": "So because of the geometric discounting and the fact that we're maximizing expected value and the fact that we have the Markov property, it is the case that when you return to if in the running of a policy for some reason, you return to the same state, you should do the same thing again during learning.",
                    "label": 0
                },
                {
                    "sent": "Of course that's not the case, because during learning you may want to try something and discover that's not really what I want to do there, so your policy could be changing during learning, But yeah, usually what we take the target of learning to be is the mapping that maximizes expected discounted reward.",
                    "label": 0
                },
                {
                    "sent": "And there is such a stationary mapping that always map states to a given state to a given action.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK so policy search so alright, so the problem with this is we don't actually get training examples directly that tell us what action we should have taken given the state that we were just in, yes.",
                    "label": 0
                },
                {
                    "sent": "Time is that we said.",
                    "label": 0
                },
                {
                    "sent": "Ah, so the equation that I had before is you could call it Infinite Horizon or indeterminate horizon.",
                    "label": 0
                },
                {
                    "sent": "It's essentially we're summing an infinite sequence of rewards, but it's decaying, so the effect of future is.",
                    "label": 0
                },
                {
                    "sent": "Sort of truncated asymptotically.",
                    "label": 0
                },
                {
                    "sent": "Same, actually.",
                    "label": 0
                },
                {
                    "sent": "I say yes, if there is a So what happens in the model as I described it and there's variance of this?",
                    "label": 0
                },
                {
                    "sent": "But is that the horizon is always a fixed distance away?",
                    "label": 0
                },
                {
                    "sent": "Right, so if it was the case that as you take a step, you're actually getting closer and closer to the end, then it could be the case that you want to start acting differently in the same state, because it really isn't the same state you're in that state, but closer to death, as it were.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you may want to act differently as you get closer to death.",
                    "label": 0
                },
                {
                    "sent": "OK, on that on that note, so because we don't have a direct training, examples of this form, researchers have looked at other ways that we might be able to get better data and train those functions more accurately.",
                    "label": 0
                },
                {
                    "sent": "So one type, another set of methods are value function based methods.",
                    "label": 0
                },
                {
                    "sent": "These are sort of the most popular.",
                    "label": 0
                },
                {
                    "sent": "If you turn to a random reinforcement learning paper published it at a machine learning conference, it probably is mainly about this kind of learning.",
                    "label": 0
                },
                {
                    "sent": "So what's happening here is what we're going to try to learn.",
                    "label": 0
                },
                {
                    "sent": "Is the Q function.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn to map States and actions to future expected rewards.",
                    "label": 0
                },
                {
                    "sent": "And if we had that, we could use it to act optimally, right?",
                    "label": 0
                },
                {
                    "sent": "If you actually had the right Q function.",
                    "label": 0
                },
                {
                    "sent": "If you could learn that, you could use it to act on, because you could just say OK, I'm in some state, which action would gives me the highest predicted reward?",
                    "label": 0
                },
                {
                    "sent": "That's the action that I'll choose, so we can kind of map one of these the Q function into the pie.",
                    "label": 0
                },
                {
                    "sent": "By searching for the action that maximizes value so that so we haven't really lost much.",
                    "label": 1
                },
                {
                    "sent": "There's a little bit of work in getting recovering the policy, but we do get much more direct kind of feedback because at some point we're in some state we try some action and then were acting in the world and we can keep track of how much reward we got and then just send it back and say, OK, well now I have sort of training information that I could use to train that example backward in time for me, and so most of the methods here actually focus on exactly how do you propagate that information backwards so you can set up training examples.",
                    "label": 0
                },
                {
                    "sent": "You know, moderately successful.",
                    "label": 0
                },
                {
                    "sent": "When I talk about model based reinforcement learning, the problem is is different yet again here, what we're saying is, let's learn, let's focus our learning on the mapping from States and actions to next dates and rewards.",
                    "label": 0
                },
                {
                    "sent": "This is a learning problem in the sense that or more traditional kind of learning problem, because we're imagining that.",
                    "label": 0
                },
                {
                    "sent": "That these functions actually capture this information, and each time we act in the world, we get a training example like a real legitimate training example.",
                    "label": 0
                },
                {
                    "sent": "We don't have back propagated in time or very far in time anyway, because we tried that state, we tried that action.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it was a good thing or a bad thing, but I know where I ended up.",
                    "label": 0
                },
                {
                    "sent": "I can see that I'm in some new state S prime and then I got some immediate reward R. So from a learning standpoint, I think this is a much more direct and much easier to work with set of methods from a computational standpoint.",
                    "label": 0
                },
                {
                    "sent": "It's tricky though, right?",
                    "label": 0
                },
                {
                    "sent": "So we could learn these, but now how do you actually?",
                    "label": 0
                },
                {
                    "sent": "Act like I know how the world works.",
                    "label": 0
                },
                {
                    "sent": "What should I do now?",
                    "label": 0
                },
                {
                    "sent": "You have to do something like solving those Bellman equations for Q, which is fairly computationally intensive to get Q and then we have Q we can use that to get the policy.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of tradeoffs here.",
                    "label": 1
                },
                {
                    "sent": "Here on this side we have much more direct use.",
                    "label": 0
                },
                {
                    "sent": "We're actually learning the thing we really want, but the learning is much more indirect because we don't get exact training examples from the environment on this side.",
                    "label": 0
                },
                {
                    "sent": "The learning itself is very direct.",
                    "label": 0
                },
                {
                    "sent": "Every time we take a step in the environment, it's another training example.",
                    "label": 0
                },
                {
                    "sent": "But to use it now requires a lot of grinding away in computation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Depending on the kind of problem that you're working on where you fit on this, you know what the appropriate method to use it varies.",
                    "label": 0
                },
                {
                    "sent": "So in a lot of the robot stuff that we've done, computation is much faster than the robot getting from point A to point B.",
                    "label": 0
                },
                {
                    "sent": "The robot is very slow in the world.",
                    "label": 0
                },
                {
                    "sent": "Oh, I said like why is the world purple?",
                    "label": 0
                },
                {
                    "sent": "'cause I'm in the purple box, there we go so.",
                    "label": 0
                },
                {
                    "sent": "Apparently purple erases my context.",
                    "label": 0
                },
                {
                    "sent": "Right, so in these robot examples data is really expensive.",
                    "label": 0
                },
                {
                    "sent": "It takes a very long time for the robot to actually make it across the room, but compared to that computation is relatively cheap, so a lot of our stuff that we've done in my lab has focused on model based approaches on this side, though I heard a really great talk recently, I think it was a Microsoft guy actually too.",
                    "label": 0
                },
                {
                    "sent": "And what he was working on was low level architectural decisions in the circuit of a computer.",
                    "label": 0
                },
                {
                    "sent": "The memory circuit like deciding which memory accesses to do at which time, when you've got a multithreaded architecture.",
                    "label": 0
                },
                {
                    "sent": "Let's say something like that and what was in his case, he had absolutely no computation time at all, every every learning every piece of learning had to happen at Clock speed, right?",
                    "label": 0
                },
                {
                    "sent": "So you have no time.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, in three seconds you get 2 million training examples or something like that, right so?",
                    "label": 0
                },
                {
                    "sent": "Tons and tons of data.",
                    "label": 0
                },
                {
                    "sent": "Data is basically free computations, expensive.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, he ended up using a actually wasn't even policy search method using a value function based method, so he was able to do this Max fast enough and he was able to back propagate information and train fast enough and we asked him questions like, well, you know what happens if you when you retrain it.",
                    "label": 0
                },
                {
                    "sent": "He's like I don't retrain it, it just constantly retraining itself because the data is just flowing in at such an incredible rate.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to leave anybody with depression that I'm here to bash.",
                    "label": 0
                },
                {
                    "sent": "Any methods on this side?",
                    "label": 0
                },
                {
                    "sent": "But I'm going to focus on methods on that side anyway.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause they're interesting.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to tell you about Q learning, which is kind of the classic prototypical method in that middle column, and the value phase function value function based method because it, for two reasons.",
                    "label": 0
                },
                {
                    "sent": "One is it's almost hard not to go out and try this.",
                    "label": 0
                },
                {
                    "sent": "'cause this is, the algorithm is already on.",
                    "label": 0
                },
                {
                    "sent": "It's like 2 lines of the slide.",
                    "label": 0
                },
                {
                    "sent": "And it really does some remarkable things.",
                    "label": 0
                },
                {
                    "sent": "It's just it's just very slow to learn.",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "What's happening is we've got an agent.",
                    "label": 0
                },
                {
                    "sent": "It's at some state.",
                    "label": 0
                },
                {
                    "sent": "It chooses some action it finds itself in some next state, and with some rewards.",
                    "label": 0
                },
                {
                    "sent": "So we have that SRS piece of data.",
                    "label": 0
                },
                {
                    "sent": "And now the question is, what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, what the Q learning algorithm does is it keeps a table or keep some kind of function that Maps state and it's the Q function map States and actions to future reward and it's going to update that function in the face of this new data as follows.",
                    "label": 0
                },
                {
                    "sent": "So I have this sort of cartoon version of Q because there's the real Q that I defined before.",
                    "label": 0
                },
                {
                    "sent": "That's the true if you actually knew what the environment was, that's the real Q function.",
                    "label": 0
                },
                {
                    "sent": "This is some kind of approximation of it.",
                    "label": 0
                },
                {
                    "sent": "Some cartoon of it, and so we're going to update that by taking the old value and adding a little bit of learning rate.",
                    "label": 0
                },
                {
                    "sent": "Amount of it to what we add to it is the immediate reward plus the discounted approximation of what the value of the next state is state plus one minus what the old value was.",
                    "label": 0
                },
                {
                    "sent": "So this is the difference in.",
                    "label": 0
                },
                {
                    "sent": "You could look at it as a temporal difference as how does my prediction now differ from what it would be one step later?",
                    "label": 0
                },
                {
                    "sent": "So right, so remember that the value as the essay value there S at time.",
                    "label": 0
                },
                {
                    "sent": "TSA value should be equal to the expected value of the reward plus the discounted value of the future states.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is just trying to bring those two things back in line with each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so, but that's the whole thing, right?",
                    "label": 0
                },
                {
                    "sent": "That's the whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "You just keep doing that over and over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "If it's the case that we visit all states in actions infinitely often and we decay the learning rates according to the standard rules of decaying learning rates, then this cartoon value version of Q actually converges.",
                    "label": 0
                },
                {
                    "sent": "2 The real Q function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It solves the whole MDP.",
                    "label": 0
                },
                {
                    "sent": "The only way that we know to solve MPs in polynomial time is using linear programming, so it's a fairly sophisticated computational procedure, but really you can get the exact same answer in the limit by just doing these little updates here, so that's kind of neat.",
                    "label": 0
                },
                {
                    "sent": "So so one thing, one thing that one affected that had is because it is so easy to implement.",
                    "label": 0
                },
                {
                    "sent": "There was A and I guess there's sort of continues to be a lot of people who just implement this and try it in various ways, and so there's just lots of papers that some of them are not as well thought out as others where where they're like.",
                    "label": 0
                },
                {
                    "sent": "Oh, so we applied Q learning to problem X.",
                    "label": 0
                },
                {
                    "sent": "Which is good and we got a lot of experience from that, but it's not always the best tool.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The job.",
                    "label": 0
                },
                {
                    "sent": "So let me contrast this with us with a more complicated thing, which is the model based learner.",
                    "label": 0
                },
                {
                    "sent": "So just give it the same set up here.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to learn a cartoon version of the reward function, R transition function T, and the Q function Q, and we're going to get the same kind of experience coming in SA next Sr. No next day, sorry.",
                    "label": 0
                },
                {
                    "sent": "SARC next S and we're going to do is update the reward to be like the average of the rewards that we've seen for that state action pair.",
                    "label": 0
                },
                {
                    "sent": "We're going to update the transition function to be to represent the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "As we we have observed it and then.",
                    "label": 0
                },
                {
                    "sent": "So we're just keeping kind of an empirical model of the transitions and rewards.",
                    "label": 0
                },
                {
                    "sent": "Then we'll take the Q function to be.",
                    "label": 0
                },
                {
                    "sent": "The Q function that we get by solving the model that comes from our estimated model.",
                    "label": 0
                },
                {
                    "sent": "So instead of actually solving for the Q function with the real model, which we don't know, we just take our best guess based on our estimates that we've done so far and solve it based on that.",
                    "label": 0
                },
                {
                    "sent": "OK, so not surprisingly.",
                    "label": 0
                },
                {
                    "sent": "Again, if we visit all states actions infinitely often and we decay are learning rates, then this cartoon value cartoon version of the Q function goes to the real Q function.",
                    "label": 0
                },
                {
                    "sent": "I cited me there, but really this is.",
                    "label": 0
                },
                {
                    "sent": "I think this is kind of obvious, I just I just happened to have written it down in my thesis so I didn't know what else to say, but.",
                    "label": 0
                },
                {
                    "sent": "At least I read that right, so I do know that it's citable, alright so.",
                    "label": 0
                },
                {
                    "sent": "This seems similar, right?",
                    "label": 0
                },
                {
                    "sent": "So we have Q learning which was really, really simple and converges in the limit to the Q function and we've got this big mess which converges in the limit to the real Q function, so you know it seems like the QQ value of Q learning version of this is at least simpler.",
                    "label": 0
                },
                {
                    "sent": "But convergence in the limit to the optimal Q function is not the only goal that you might want.",
                    "label": 0
                },
                {
                    "sent": "The field was focused on this for a number of years, I think because convert converging to the real Q function is better than not converging to the real Q function.",
                    "label": 0
                },
                {
                    "sent": "And there was certainly a lot of methods that were out and about before Chris Watkins thesis that didn't right.",
                    "label": 0
                },
                {
                    "sent": "There's like well, and then we'll kind of increase this value and will decrease this other value.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it gets stuck sometimes, but not often.",
                    "label": 0
                },
                {
                    "sent": "Works AI is often guilty of what I call best case analysis.",
                    "label": 0
                },
                {
                    "sent": "There's like in computer science.",
                    "label": 0
                },
                {
                    "sent": "Worst case analysis will say, well, this is you know N cubed in the worst case it will give us the answer.",
                    "label": 0
                },
                {
                    "sent": "Best case analysis says, well, we're going to run this and I can show you an example where it works.",
                    "label": 0
                },
                {
                    "sent": "And there's value in that, but you just have to realize that it's a complementary kind of value to the worst case analysis.",
                    "label": 0
                },
                {
                    "sent": "In the worst case, it doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mark alright, so that's one kind of goal is convergence in the limit to the optimal Q function.",
                    "label": 0
                },
                {
                    "sent": "Here's another goal.",
                    "label": 0
                },
                {
                    "sent": "We're going to call pack MDP reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So what do we mean by this will pack as you told me, you may have looked at already probably approximately correct is an idea that was developed in the context of learning classifiers, but the probably in the approximately in the correct part are very general concepts, and so we can apply them in other ways.",
                    "label": 0
                },
                {
                    "sent": "So here's how you might apply these ideas in the reinforcement learning setting.",
                    "label": 0
                },
                {
                    "sent": "A number of people have done this.",
                    "label": 0
                },
                {
                    "sent": "We're going to be given just like impact.",
                    "label": 0
                },
                {
                    "sent": "We're going to be given epsilon in a Delta.",
                    "label": 0
                },
                {
                    "sent": "Let's say we know that we're going to learn in some MVP.",
                    "label": 0
                },
                {
                    "sent": "It really is an MDP, so we're going to assume the Markov property and so forth.",
                    "label": 0
                },
                {
                    "sent": "We know that there's K actions and states, we just don't know what they do, and we've got a discount factor.",
                    "label": 1
                },
                {
                    "sent": "We're going to say a strategy of learning a learning algorithm is going to be acting in the world.",
                    "label": 1
                },
                {
                    "sent": "It's going to reach a state, and it's going to take some action, and it's going to repeat that over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "We're going to say it makes a mistake each time step T, where the action that it took at time step T in the state that it was actually in time zippity.",
                    "label": 0
                },
                {
                    "sent": "The true Q value of that action in that state is worse than the best that you could have done in that state, minus R epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this is the approximately correct part of the pack.",
                    "label": 0
                },
                {
                    "sent": "This is saying that an action is approximately correct if the Q value is close to the optimal Q value.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so that's an if you don't do that then that's a mistake and we're going to let Em be abound on the total number of mistakes, and that bound has to hold with high probability 1 minus Delta.",
                    "label": 1
                },
                {
                    "sent": "So this is the probably part of the pack.",
                    "label": 0
                },
                {
                    "sent": "Phrase what we'd like is, can we come up with some kind of way of acting in the world?",
                    "label": 0
                },
                {
                    "sent": "Some kind of learning algorithm, such that there's an M that holds for that learning algorithm that's polynomial in the number of action states, one over epsilon, one over Delta.",
                    "label": 0
                },
                {
                    "sent": "And let's say the effective horizon 1 /, 1 minus gamma.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you really want, if you really care if gamma is close to one, that means you really care bout far distant rewards.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so maybe that's going to make your learning take longer 'cause you care about things that are far away.",
                    "label": 0
                },
                {
                    "sent": "If the discount factor is closer to 0.",
                    "label": 0
                },
                {
                    "sent": "This is a smaller number and you can actually would like to be able to learn faster in that case, if you don't care too much about the future.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the only way that you're going to be able to achieve this kind of bound is by striking some kind of balance between exploration and exploitation.",
                    "label": 0
                },
                {
                    "sent": "An algorithm that takes its best guess of what it thinks the world is and always acts according to that could end up making tons and tons of mistakes because it hasn't learned what the correct model is yet.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, an algorithm that painstakingly goes out and checks every estimates every parameter to the NTH degree, you can't estimate any of these exactly, but you can get them.",
                    "label": 0
                },
                {
                    "sent": "Very very very very accurate is wasting a lot of time making a lot of mistakes, doing experiments, that time that it could have been spent exploiting gaining near optimal reward.",
                    "label": 0
                },
                {
                    "sent": "So you really do have to strike a balance between these two, so that's kind of challenging.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing to point out is that Q learning is not pack MDP.",
                    "label": 0
                },
                {
                    "sent": "Now that's not entirely fair to say.",
                    "label": 0
                },
                {
                    "sent": "I said it anyway, but it's not entirely fair to say on the part of the reason for that is Q learning.",
                    "label": 0
                },
                {
                    "sent": "There is no Q learning Q learning is actually a family of algorithms and you get very different kinds of behavior depending on how you initialize the Q values.",
                    "label": 0
                },
                {
                    "sent": "Initialize the Q function.",
                    "label": 0
                },
                {
                    "sent": "How you do exploration.",
                    "label": 0
                },
                {
                    "sent": "That is to say, remember that what the theorem said is that you try all states, all actions in all states infinitely often, but at any moment in time you have to choose some action based on your current Q values and how you do that has an impact on how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "And how you decay the learning rates turns out to matter too.",
                    "label": 0
                },
                {
                    "sent": "So different different versions of Q learning decay in different ways actually have different mathematical properties.",
                    "label": 0
                },
                {
                    "sent": "But that being said, here's what we know.",
                    "label": 0
                },
                {
                    "sent": "Many of the versions, every version that we've been able to study, is not pack MDP, and there's a couple where the jury is still out, but there's no versions that we've been able to show actually achieve that pack MDP guarantee.",
                    "label": 0
                },
                {
                    "sent": "And most of the counter examples for why it doesn't work boil down to the combination lock, so here's this.",
                    "label": 0
                },
                {
                    "sent": "I'll show you the combination lock, 'cause then you could have seen it.",
                    "label": 0
                },
                {
                    "sent": "So we have a chain of states.",
                    "label": 0
                },
                {
                    "sent": "There's one action that pushes you this way on the chain and there's the other action that no matter where you are, resets you back to the beginning.",
                    "label": 0
                },
                {
                    "sent": "Except for the very very last state in the chain, which is a self loop and you can get lots of reward from their high reward.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the reset action has a very tiny little reward and the go forward action has no roll cake.",
                    "label": 0
                },
                {
                    "sent": "As you can see how this maybe is a little bit tricky in the beginning if you don't know that you're trying to go out there and get this high reward at the end, you're just kind of bumbling around exploring and what you're seeing.",
                    "label": 0
                },
                {
                    "sent": "A lot of the states that act more or less the same, and if you're choosing actions 'cause you don't know which action is better than the other one.",
                    "label": 0
                },
                {
                    "sent": "So if you're choosing actions randomly, you're going to be spending basically your entire life in this state.",
                    "label": 0
                },
                {
                    "sent": "Going to wander out a little bit and then you'll randomly try the blue action and it takes you all the way back to the beginning.",
                    "label": 0
                },
                {
                    "sent": "Bumbling around randomly like this that I'm the expected time that it's going to take to get all the way to the end of this chain and actually experienced this high reward for the first time is going to be exponential in the size of the chain link.",
                    "label": 0
                },
                {
                    "sent": "So unless it's actually pushing itself, a learning algorithm is pushing itself out on the chain to see if there's something else out there then it's not going to be able to get that high reward, and it's not necessary.",
                    "label": 0
                },
                {
                    "sent": "Well, depending on exactly what the discount rate is and how big that 20 is, and those probabilities and so forth.",
                    "label": 0
                },
                {
                    "sent": "It will not be getting near optimal or it will be making mistakes pretty much forever.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's the bad kind of example.",
                    "label": 0
                },
                {
                    "sent": "So you really do need and if you want to get this guarantee of near optimal reward then you really have to push out and explore more than what Q learning is able to do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Without some direction.",
                    "label": 0
                },
                {
                    "sent": "So what about a model based approach?",
                    "label": 0
                },
                {
                    "sent": "Well, turns out model based approaches can be pack MVP that they don't necessarily have to be 'cause.",
                    "label": 1
                },
                {
                    "sent": "Again, you can think of this as being a family of algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to our combination lock example.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine that we've explored for awhile, and here's what our model based learner has learned so far that it knows about these six states.",
                    "label": 0
                },
                {
                    "sent": "But it's never tried that black action from this last state.",
                    "label": 0
                },
                {
                    "sent": "It's just never tried it, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it's learning as it goes, so it has no idea what's out there.",
                    "label": 0
                },
                {
                    "sent": "So now what happens?",
                    "label": 0
                },
                {
                    "sent": "Well, depending on what it assumes.",
                    "label": 1
                },
                {
                    "sent": "Might be out there.",
                    "label": 0
                },
                {
                    "sent": "The model based method is going to behave very differently, so let's look at a little 2 by 2.",
                    "label": 0
                },
                {
                    "sent": "It can either assume that what's out there has relatively high reward, like more than what it's getting in the rest of the chain, or that it has relatively low reward.",
                    "label": 0
                },
                {
                    "sent": "But the truth is, it could either be low or high.",
                    "label": 0
                },
                {
                    "sent": "And those are disconnected from each other.",
                    "label": 0
                },
                {
                    "sent": "So if it's the case that really what's out there is low reward and the algorithm assumes that what's out there is low reward, then it's not going to even try to go out there and find it.",
                    "label": 0
                },
                {
                    "sent": "And that actually is optimal in this case.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the truth is that it's really low out there, there's nothing good out there, but it assumes there is something good out there.",
                    "label": 0
                },
                {
                    "sent": "What's going to happen?",
                    "label": 0
                },
                {
                    "sent": "It's going to say, hey, the optimal policy, as far as I know, is to go out there and get that reward so it goes out there.",
                    "label": 0
                },
                {
                    "sent": "It gets to the question mark, and then it's sadly disappointed 'cause there's really no good reward out there.",
                    "label": 0
                },
                {
                    "sent": "'cause the truth is.",
                    "label": 0
                },
                {
                    "sent": "In this case the reward was low.",
                    "label": 0
                },
                {
                    "sent": "So what happened is it went visited the question Mark.",
                    "label": 0
                },
                {
                    "sent": "It explored if you will it learn something new about it, and then.",
                    "label": 0
                },
                {
                    "sent": "Like many of us, it learned that it wasn't all that it's cracked up to be, and so, but now that's good, because in its model it now represents that information and it won't make that mistake again.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, let's say that the truth of the matter is that there's high reward out there.",
                    "label": 0
                },
                {
                    "sent": "Well, if there's high reward out there, and it assumes that the reward is high, then that's great.",
                    "label": 0
                },
                {
                    "sent": "It's going to go out there and find that reward, and that was the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "But, and here's where the asymmetry comes in.",
                    "label": 0
                },
                {
                    "sent": "If the truth is that there's high reward out there, but the algorithm assumes there's not, it's not going to go out there and get it.",
                    "label": 0
                },
                {
                    "sent": "It's never going to be getting near optimal reward.",
                    "label": 0
                },
                {
                    "sent": "It's going to be sub optimal, and it's going to stay sub optimal because it's not learning, it's not getting the experience it needs to find out that it's wrong.",
                    "label": 0
                },
                {
                    "sent": "So you can see there's sort of irritating asymmetry that comes up if it assumes stuff that it doesn't know has low reward, then we don't get a pack MVP guarantee.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we can, if we assume that things that are out there that we don't know about are relatively good.",
                    "label": 0
                },
                {
                    "sent": "And in fact it can be packed MVP as long as the each time it learns something that actually get something out of that, learning right?",
                    "label": 0
                },
                {
                    "sent": "If it's the case that that.",
                    "label": 0
                },
                {
                    "sent": "It's truly low.",
                    "label": 0
                },
                {
                    "sent": "It thinks it's high.",
                    "label": 0
                },
                {
                    "sent": "It goes out there, discovers that it really is low, and then forgets that right.",
                    "label": 0
                },
                {
                    "sent": "Then sometime later it's going to be Oh yeah, and it's going to go out there again and it's gotta actually you have to put bounds on how quickly it learns from its mistakes.",
                    "label": 0
                },
                {
                    "sent": "But if you have that then you can actually get these nice guarantees that with quickly with high probability it's.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe near optimal.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea that drives the story that I was just telling a lot of the algorithms that actually have been shown to have these guarantees is the notion of optimism under uncertainty.",
                    "label": 1
                },
                {
                    "sent": "If you don't know any better, assume it's good.",
                    "label": 0
                },
                {
                    "sent": "Why assume it's good and not bad because you can't get near optimality if you assume it's bad.",
                    "label": 0
                },
                {
                    "sent": "There is a field that does the opposite for good reason, right?",
                    "label": 0
                },
                {
                    "sent": "So people who do like safety critical systems, they assume that the unknown is very, very bad, and that's that's that's a good.",
                    "label": 0
                },
                {
                    "sent": "We're glad I was able to fly across the Atlantic last night, and I'm here safe.",
                    "label": 0
                },
                {
                    "sent": "And I think partly it's because they make these bad assumptions know these, make these assumptions that things could be bad, and so they do.",
                    "label": 0
                },
                {
                    "sent": "Avoid the bad cases, and then you know we're safe and sound if the plane is flying over the attic.",
                    "label": 0
                },
                {
                    "sent": "Oh, I've never seen that kind of turbulence before.",
                    "label": 0
                },
                {
                    "sent": "I bet it's good.",
                    "label": 0
                },
                {
                    "sent": "That that would suck so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but so but anyway, but that's because the goals are different in this case.",
                    "label": 0
                },
                {
                    "sent": "If the goal really is we want near optimality with high probability, we have to assume these algorithms seem to have to assume that what's unknown is good.",
                    "label": 0
                },
                {
                    "sent": "This is a well known idea.",
                    "label": 1
                },
                {
                    "sent": "It dates way back to the beginning and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "People were doing it in practice before they had.",
                    "label": 0
                },
                {
                    "sent": "Any kind of theoretical story about why it was a good idea, but they had a lot of experience showing that when they tuned their algorithms in that way it did better than if they didn't, yeah.",
                    "label": 0
                },
                {
                    "sent": "No, but you have to assume that you have some sense of what big really is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so you have.",
                    "label": 0
                },
                {
                    "sent": "So usually in fact, one of the algorithms that I'm going to talk about is called our Max.",
                    "label": 0
                },
                {
                    "sent": "It's called our Max because it's given as a parameter, the maximum reward.",
                    "label": 0
                },
                {
                    "sent": "OK, so it turns out that if you do this you can develop algorithms and theorems and so forth.",
                    "label": 0
                },
                {
                    "sent": "It showed that you can actually achieve these kinds of pack MDP guarantees and the key ideas for getting these proofs to go through are basically these two the simulation lemma, which just says that if you have an approximate model of an MVP and you do what you think is optimal for that approximate model, then you're near optimal for the real MVP, right?",
                    "label": 0
                },
                {
                    "sent": "So basically?",
                    "label": 0
                },
                {
                    "sent": "Perfection in in in reward gathering plus approximate model equals approximation and reward gathering with imperfect model.",
                    "label": 0
                },
                {
                    "sent": "So what that means is you can actually estimate the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "Pretend that that's reality and do pretty well.",
                    "label": 0
                },
                {
                    "sent": "If your estimate is good, the other one is the explore exploit lemon.",
                    "label": 0
                },
                {
                    "sent": "This is very clever actually, which says that if you can't reach unknown states quickly then you can achieve near optimal reward.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea here is what keeps you from.",
                    "label": 0
                },
                {
                    "sent": "So let's say that there's a state that's really very inaccessible.",
                    "label": 0
                },
                {
                    "sent": "I bet there's a state that's in excess, so there's these cool boxes on the side here, little triangular boxes.",
                    "label": 0
                },
                {
                    "sent": "If you can see them, there could be something very valuable for the invited speakers there.",
                    "label": 0
                },
                {
                    "sent": "In fact, I was told there actually called speaker cabinets, so they must be for me.",
                    "label": 0
                },
                {
                    "sent": "So The thing is that it would be really probably very costly for me to actually find this out, like I'd have to rip apart the thing and risk called prosecution and expulsion.",
                    "label": 0
                },
                {
                    "sent": "So why is it that if I'm trying to get near optimal reward, why don't I just go and do that?",
                    "label": 0
                },
                {
                    "sent": "Well, the point is that getting even finding out whether there's anything good in there is going to be very costly in and of itself.",
                    "label": 0
                },
                {
                    "sent": "So what the Explore exploit lemma says is if it's really costly to get new information.",
                    "label": 0
                },
                {
                    "sent": "That's because you can actually get near optimal reward without it.",
                    "label": 0
                },
                {
                    "sent": "And so this maybe this may not be entirely true with the real world, but it is true of MDP's.",
                    "label": 0
                },
                {
                    "sent": "If you have a bound on the maximum reward.",
                    "label": 0
                },
                {
                    "sent": "So when these two things get put together, what you can show is these algorithms are going to explore as much as they need to explore, and once they can't, once it becomes too expensive to explore their actually behaving near optimally.",
                    "label": 0
                },
                {
                    "sent": "And this idea can be extended in various ways, and that's what I'm going to focus on for the rest of this slot, so I haven't found o'clock yet.",
                    "label": 0
                },
                {
                    "sent": "Oh there's a Clock.",
                    "label": 0
                },
                {
                    "sent": "What do we think is the ending time between?",
                    "label": 0
                },
                {
                    "sent": "We started a bit late so 1240 OK alright good alright so that's that's my plan is to then flesh that story out with.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The time that I've got.",
                    "label": 0
                },
                {
                    "sent": "I should say one thing though.",
                    "label": 0
                },
                {
                    "sent": "As a side Note, pack MVP.",
                    "label": 0
                },
                {
                    "sent": "Pretty much all the pack MVP algorithms are for these model based methods.",
                    "label": 0
                },
                {
                    "sent": "There's a real question as well.",
                    "label": 0
                },
                {
                    "sent": "Can you even do model freepac MVP and it turns out I guess it's not that recent anymore, but relatively recent result is that you can and the way that's done is by taking Q learning and actually kind of smoothing it out a bit so that it doesn't get too excited about any one piece of data.",
                    "label": 0
                },
                {
                    "sent": "And then you can show that it actually the total number of mistakes that it makes is Paul.",
                    "label": 0
                },
                {
                    "sent": "It's like number of states, times, number of actions.",
                    "label": 0
                },
                {
                    "sent": "Defective Horizon race to the 8th power the approximation concentrate to the 4th power.",
                    "label": 0
                },
                {
                    "sent": "This is in some ways better in some ways worse than what you get with model based methods in model based methods there's always this pesky squared that shows up in the number of States and partly if you think about it, a model free method estimates function from States and actions to values so the data structure itself is N * K model based method is estimating the transitions which is states by states times actions.",
                    "label": 0
                },
                {
                    "sent": "So it's N ^2 K. So this is the size of the data structure in a model based approach.",
                    "label": 0
                },
                {
                    "sent": "So again, it's not so surprising that that's going to show up in the bounds, so the model free pack MDP bound is actually better, at least in terms of number of states, but it's worse in terms of horizon lengthen approximation constants.",
                    "label": 0
                },
                {
                    "sent": "At least what we've got right now, and some of these components actually match in a lower bound that was able to be shown.",
                    "label": 0
                },
                {
                    "sent": "So these ideas are more general than just the model based method, but I'm going to focus on the model based methods in the back first.",
                    "label": 0
                },
                {
                    "sent": "On continuous state space.",
                    "label": 0
                },
                {
                    "sent": "I'll say some about continuous state spaces.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in fact on the previous slide, I think I even used the word.",
                    "label": 0
                },
                {
                    "sent": "I said metric spaces.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so some of the cube stuff.",
                    "label": 0
                },
                {
                    "sent": "Some of this work had been extended to the continuous case.",
                    "label": 0
                },
                {
                    "sent": "Things get awkward and the assumptions start to pile up in the continuous case, but in the rest of what I talk about today and tomorrow I will be mentioning continuous now and again.",
                    "label": 0
                },
                {
                    "sent": "There's another hand to do it.",
                    "label": 0
                },
                {
                    "sent": "So the the end square K result yes.",
                    "label": 0
                },
                {
                    "sent": "If you have a typical gridworld where you can't get from every state, every other state, OK. To end on the number of states can get through to actually know that about so.",
                    "label": 0
                },
                {
                    "sent": "So the way that I just presented this in terms of the size of data structures.",
                    "label": 0
                },
                {
                    "sent": "If it's the case that there's a constant number of next states you can reach, call it.",
                    "label": 0
                },
                {
                    "sent": "Z for zoom in so we have N * y * K as the size of the data structure.",
                    "label": 0
                },
                {
                    "sent": "The question is, does that actually show up in the bound as well?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "They may have a pen anyway, remind me I'll follow up on that.",
                    "label": 0
                },
                {
                    "sent": "That's actually that's very cool idea.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and in particular a lot of lot of problems actually have that structure.",
                    "label": 0
                },
                {
                    "sent": "Certainly the grid worlds that that reinforcement learning people get fixated on have local connectivity, right?",
                    "label": 0
                },
                {
                    "sent": "So given state can only reach a small number of neighbors, not the full state space, yes?",
                    "label": 0
                },
                {
                    "sent": "6.",
                    "label": 0
                },
                {
                    "sent": "Please get up let me know that you already optimal after some haha.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will.",
                    "label": 0
                },
                {
                    "sent": "I will, I'll address that, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I don't know what the.",
                    "label": 0
                },
                {
                    "sent": "I don't know how I feel about that, but I like the question, but I'm going to ask it again when I get to the part where I say oh and this is going to show that you're not actually optimal after any given time step.",
                    "label": 0
                },
                {
                    "sent": "When I get there is a context, I'm going to set up shortly that will make it easier to, I know, but I'm going to get it will be easier for me to explain it later anyway.",
                    "label": 0
                },
                {
                    "sent": "So it seems silly to repeat it now anyway, not that.",
                    "label": 0
                },
                {
                    "sent": "So OK, so the question was, I've been shamed into it, so the question was.",
                    "label": 0
                },
                {
                    "sent": "The mistake bound and which the pack MDP framework sort of sets up is basically one that says the total number of mistakes in an infinite run is bounded by a certain number.",
                    "label": 0
                },
                {
                    "sent": "But what it doesn't say is that after this amount of learning from here on forward, you're optimal.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are two kinds of things that you might think about is learning right?",
                    "label": 0
                },
                {
                    "sent": "One says that you're going to do all your training and learning in some fixed.",
                    "label": 0
                },
                {
                    "sent": "And then from then on you've got it and the other one says you could make a mistake.",
                    "label": 0
                },
                {
                    "sent": "It could happen today it could happen next week, but each time you make a mistake, you're getting closer and closer to perfection.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at any fixed block of time, if it's a big enough block, the fraction of mistakes that you make in that block has to be going to 0, right?",
                    "label": 0
                },
                {
                    "sent": "But the fact of the matter is in the bounds that I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "There is no last moment at which you can make a mistake, and part of the reason for this sort of inevitable in these MVP cases.",
                    "label": 0
                },
                {
                    "sent": "'cause imagine if there's a transition that's really really low probability, so most of your life you kind of live would just at home.",
                    "label": 0
                },
                {
                    "sent": "We just started watching the movie The Matrix, which I assume most people have seen, but my kids hadn't seen it yet, so they're being there.",
                    "label": 0
                },
                {
                    "sent": "Lines are being bent, but let's say that you live your life, you live your life and then one day you discover.",
                    "label": 0
                },
                {
                    "sent": "No, that's not reality.",
                    "label": 0
                },
                {
                    "sent": "There's this other reality over here.",
                    "label": 0
                },
                {
                    "sent": "Suddenly you're not getting near optimal roared anymore.",
                    "label": 0
                },
                {
                    "sent": "Even though you were in the old space and so the moment at which that transition happens, you can't put it down.",
                    "label": 0
                },
                {
                    "sent": "It's a low probability event.",
                    "label": 0
                },
                {
                    "sent": "It could happen anytime in the future, so it's I think it's impossible to get the kind of bound that you were hoping for.",
                    "label": 0
                },
                {
                    "sent": "But but I've come to grips with this one.",
                    "label": 0
                },
                {
                    "sent": "I think it's OK. Alright, So what I'd like to do is to draw to kind of extend this idea so we can talk about continuous spaces.",
                    "label": 0
                },
                {
                    "sent": "We can talk about spaces where you have more structure, we can talk about lots of things like that by expanding this idea to focusing on the profiling problem.",
                    "label": 0
                },
                {
                    "sent": "Model learning is a supervised learning problem.",
                    "label": 1
                },
                {
                    "sent": "You're given examples of the form, state, action and inputs of state action, and you're supposed to output next date or probability of next date.",
                    "label": 0
                },
                {
                    "sent": "It's not specific.",
                    "label": 0
                },
                {
                    "sent": "This idea is not specific to table.",
                    "label": 0
                },
                {
                    "sent": "Look up like just building a big literal explicit graph.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is extend these kinds of efficient learning results by generalizing the notion of the transition function to be just some function approximators, something that Maps States and actions to next days.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that the existing or kind of standard machine learning theory doesn't give us quite the right guarantees for us to build.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reinforcement learning out of algorithm out.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is sort of the context that I thought I'd be able to use a moment ago.",
                    "label": 0
                },
                {
                    "sent": "So here's the deal.",
                    "label": 0
                },
                {
                    "sent": "So we talked a little bit about pack.",
                    "label": 0
                },
                {
                    "sent": "So in the supervised setting.",
                    "label": 0
                },
                {
                    "sent": "Pack is set up like this.",
                    "label": 0
                },
                {
                    "sent": "Inputs are drawn from some fixed distribution.",
                    "label": 0
                },
                {
                    "sent": "We get to observe labels for the first set of them, say M of them, and then future inputs are drawn from the distribution.",
                    "label": 1
                },
                {
                    "sent": "Oh, so once we've done that once, we've gone through our training period, we're not going to make mistakes with high probability.",
                    "label": 0
                },
                {
                    "sent": "We're not going to make big mistakes for any future things drawn from that distribution.",
                    "label": 0
                },
                {
                    "sent": "If the pack guarantee holds so graphically, I drew it this way.",
                    "label": 0
                },
                {
                    "sent": "Imagine each each row here is another training example coming through the first block of them are blue.",
                    "label": 0
                },
                {
                    "sent": "Those are the training examples and after that.",
                    "label": 0
                },
                {
                    "sent": "It's a bunch of things where the algorithm makes no mistakes.",
                    "label": 0
                },
                {
                    "sent": "It correctly classifies or outputs after the initial training period, so that seems like a good kind of guarantee.",
                    "label": 0
                },
                {
                    "sent": "It seems like a useful thing to have.",
                    "label": 0
                },
                {
                    "sent": "We could imagine the learner robots scrambling around in the world collecting data about, oh, I tried this in this state.",
                    "label": 0
                },
                {
                    "sent": "I tried this in this State Building up a training set, and then predicting things correctly after that point.",
                    "label": 0
                },
                {
                    "sent": "That would be really great.",
                    "label": 0
                },
                {
                    "sent": "So here's the first thing that goes wrong.",
                    "label": 0
                },
                {
                    "sent": "Building, putting a pack learner as our transition function learner in a reinforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Isn't going to give us a pack MVP guarantee and the reason for this is?",
                    "label": 0
                },
                {
                    "sent": "I find this somewhat amusing.",
                    "label": 0
                },
                {
                    "sent": "The IID assumption that so critical for getting kind of these classic results to go through implies that the learner can't improve and change in any way.",
                    "label": 0
                },
                {
                    "sent": "It's behavior, right?",
                    "label": 0
                },
                {
                    "sent": "So these examples, where is the robot getting this examples from its wandering around the world and collecting them?",
                    "label": 0
                },
                {
                    "sent": "What's the distribution of examples?",
                    "label": 0
                },
                {
                    "sent": "What is a function of how it wanders around in the world?",
                    "label": 0
                },
                {
                    "sent": "So if it collects for awhile and then says, oh, I know how things work now, it starts to behave differently, the distribution changes.",
                    "label": 0
                },
                {
                    "sent": "So really, the only way that it can use this great learning that it is that it just it just it just made so it makes perfect predictions now and it wanders around the world the same way that it's always wandered out in the world.",
                    "label": 0
                },
                {
                    "sent": "Saying, you know, I know how I would have optimized that.",
                    "label": 0
                },
                {
                    "sent": "I know how I would have optimized that, but it can't make use to that.",
                    "label": 0
                },
                {
                    "sent": "'cause then it's learner goes away.",
                    "label": 0
                },
                {
                    "sent": "It doesn't make accurate predictions anymore, so that's unfortunate.",
                    "label": 0
                },
                {
                    "sent": "OK, so has another model.",
                    "label": 0
                },
                {
                    "sent": "There's a better model.",
                    "label": 0
                },
                {
                    "sent": "The mistake bound model of computational learning theory says that we're not going to have any dependence on a distribution.",
                    "label": 0
                },
                {
                    "sent": "Inputs are going to be presented online, perhaps by an adversary an for each one.",
                    "label": 0
                },
                {
                    "sent": "The learner is supposed to predict the output.",
                    "label": 0
                },
                {
                    "sent": "If it makes a mistake, it gets to observe what the correct label is, and it's not allowed over the course of its lifetime to make more than M mistakes.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a victim of the adversary.",
                    "label": 0
                },
                {
                    "sent": "The adversary cannot show it some example for arbitrarily long into the future.",
                    "label": 0
                },
                {
                    "sent": "But it when it eventually gets that example, it might make a mistake on it, but the total number of mistakes over its lifetime has to be bounded, so that seems good, right?",
                    "label": 0
                },
                {
                    "sent": "We got rid of this notion of depending desperately on the distribution of inputs.",
                    "label": 0
                },
                {
                    "sent": "The problem with this one though, if you stick it into a reinforcement learning algorithm, is again it's not pack MVP and that's because it can make mistakes.",
                    "label": 0
                },
                {
                    "sent": "The predictor of next states can make mistakes.",
                    "label": 0
                },
                {
                    "sent": "An as I showed in that little two by two example that a mistake could mean that there's some place out there that has high reward that our current function predicts has low reward, and therefore we're not going to learn.",
                    "label": 0
                },
                {
                    "sent": "We're not going to visit it.",
                    "label": 0
                },
                {
                    "sent": "We're not going to learn, and we can be permanently suboptimal, so we won't satisfy the pack MDP guarantee of getting near optimal reward after some number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "So alright, so we said OK, well maybe we could get the best of both worlds here and we sort of did.",
                    "label": 0
                },
                {
                    "sent": "We had developed a.",
                    "label": 0
                },
                {
                    "sent": "A learning framework to sound like pack called Quick, which stands for knows what it knows.",
                    "label": 0
                },
                {
                    "sent": "So here the basic idea is the inputs are presented online like in the mistake bound model for each.",
                    "label": 0
                },
                {
                    "sent": "The learner can actually predict the output, but it has to be accurate or it's allowed to say I don't know right?",
                    "label": 0
                },
                {
                    "sent": "We can't expect it to be accurate without learning anything, so it's allowed to say I don't know and observe the label it's not allowed to make mistakes and up to that point it's actually really easy to do this right?",
                    "label": 0
                },
                {
                    "sent": "You just say I don't know forever so we have to put a bound on the total number of times it's allowed to say.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means it's allowed to get training examples.",
                    "label": 0
                },
                {
                    "sent": "It's allowed to be clueless for a little while, but it can't be clueless forever so graphically.",
                    "label": 0
                },
                {
                    "sent": "Oh, I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't point out the graphic representation in the mistake bound case.",
                    "label": 0
                },
                {
                    "sent": "So in this takedown case it's getting examples.",
                    "label": 0
                },
                {
                    "sent": "Some of them are wrong red and some of them are right green, but the total number of red bars has to be bounded and the other at the top.",
                    "label": 0
                },
                {
                    "sent": "When the total number of blue bars was founded.",
                    "label": 0
                },
                {
                    "sent": "This is now in the quick model.",
                    "label": 0
                },
                {
                    "sent": "The total number of blue bars is bounded.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is it's getting an input.",
                    "label": 0
                },
                {
                    "sent": "It's making a prediction it.",
                    "label": 0
                },
                {
                    "sent": "But only if it's sure that it's right.",
                    "label": 1
                },
                {
                    "sent": "Otherwise it can say I don't know and request an example and the total number of blue bars has to be relatively small.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Quick quickly.",
                    "label": 0
                },
                {
                    "sent": "And this can we can plug this into a reinforcement learning algorithm and we can get our pack MDP guarantee YYO.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So the difference between the mistakes on it quick is that quick predicts something, and if it's uncertain, that says I want to observe that label right where is mistake?",
                    "label": 0
                },
                {
                    "sent": "What the mistake bound algorithms do is if it's not sure, it just guesses and some of the really effective mistake bound algorithms, what they do is they not playing the odds, but they play the hypothesis space they say basically, if more things, many things in my hypothesis space agree, I'm going to go with that.",
                    "label": 0
                },
                {
                    "sent": "And if I'm wrong, then I learn a lot.",
                    "label": 0
                },
                {
                    "sent": "And it makes very, very dramatic progress towards learning the real hypothesis, but but the quick quick algorithm can't do that if it's not sure, it has to say I don't know.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Confident.",
                    "label": 0
                },
                {
                    "sent": "It if it's too confident.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's not allowed to be wrong.",
                    "label": 0
                },
                {
                    "sent": "It's simply not OK. Well, it's a.",
                    "label": 0
                },
                {
                    "sent": "It has a pack like it has an epsilon Delta sort of thing in it, so a quick algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can't guarantee that any problem with any amount of stochasticity is going to be 100% guaranteed correct?",
                    "label": 0
                },
                {
                    "sent": "So we have to hold out some probability that the whole thing could just fail, but when it's not failing it has to be right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, otherwise it's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quick.",
                    "label": 0
                },
                {
                    "sent": "So it's like like the background is not allowed to make mistakes like the mistake bound.",
                    "label": 1
                },
                {
                    "sent": "There's no distribution assumption.",
                    "label": 1
                },
                {
                    "sent": "Not surprisingly, this is a harder problem, right?",
                    "label": 0
                },
                {
                    "sent": "So anything that can be quick learned can be mistake bound learned, because whenever you say I don't know, you can just guess anything that turns out anything mistake that you can learn.",
                    "label": 0
                },
                {
                    "sent": "Another mistake model you can pack learn, but it's not the other way.",
                    "label": 1
                },
                {
                    "sent": "There's problems that you can mistake bound.",
                    "label": 0
                },
                {
                    "sent": "Learn that you can't quickly learn, and it's especially when they are taking advantage of the fact that you make a guess an if you're wrong, you learn a lot.",
                    "label": 0
                },
                {
                    "sent": "Quick is not allowed to make that guess, and so an adversary can.",
                    "label": 0
                },
                {
                    "sent": "Just kind of whittle away at the hypothesis space and take a very very long time forcing it to make lots and lots of I don't knows yes.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is, I'm going to present a bunch of examples.",
                    "label": 0
                },
                {
                    "sent": "It'll be easier with the examples how how, I just say it's not allowed to make a mistake.",
                    "label": 0
                },
                {
                    "sent": "It's as simple as that.",
                    "label": 0
                },
                {
                    "sent": "We we could do some examples but but but if this is the way that's going to be, here's why this is very well suited to model learning.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "Have our exploration being driven by this known unknown distinction so anything in the world that are learner can't predict we're going to have to assume that something that's good and get the optimism in the face of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Get it to go out there and get that data once it gets there.",
                    "label": 0
                },
                {
                    "sent": "It'll get that example.",
                    "label": 0
                },
                {
                    "sent": "Find out the right answer and then it won't not know that anymore.",
                    "label": 0
                },
                {
                    "sent": "It won't.",
                    "label": 0
                },
                {
                    "sent": "Yes, it will know that now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here's an example of something that we can quick learn, really simple that we should all be able to get on board with, which is learning the probability that a weighted coin is going to come up heads.",
                    "label": 0
                },
                {
                    "sent": "OK, so we all know that if we observe for M trials and we've had X successes, we can estimate the probability P. The cartoon version of it is just X / M. Anne, with the Husting bound does is it tells us.",
                    "label": 0
                },
                {
                    "sent": "As a function of epsilon, how likely it is that our estimate is going to be at least at least epsilon close to the real answer so?",
                    "label": 0
                },
                {
                    "sent": "I assume that you've been doing hafting bounds earlier in the course they come up before.",
                    "label": 0
                },
                {
                    "sent": "No interesting, OK. OK, well then let me let me say a little bit more about it then.",
                    "label": 0
                },
                {
                    "sent": "So sort of standard concentration bounds sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying is.",
                    "label": 0
                },
                {
                    "sent": "No matter how big XMR we can still get that kind of estimate right?",
                    "label": 0
                },
                {
                    "sent": "So I can flip the coin 30 times and it comes up heads five times.",
                    "label": 0
                },
                {
                    "sent": "So I could say, well, I'd estimate 5 / 30 is the probability that comes up heads.",
                    "label": 0
                },
                {
                    "sent": "The problem is that if we haven't flipped the coin very much, we shouldn't be all that certain that the answer really is 5 / 30.",
                    "label": 0
                },
                {
                    "sent": "So, but as we get more and more data with high probability that the average that we compute is really going to be very close to the real average, the law of large numbers sorts of things.",
                    "label": 0
                },
                {
                    "sent": "So it's this.",
                    "label": 0
                },
                {
                    "sent": "Just quantify that it says that if you've got a sample of size M and you're interested in variable that varies between.",
                    "label": 0
                },
                {
                    "sent": "Being a right.",
                    "label": 0
                },
                {
                    "sent": "So that's the range of values that can spit out.",
                    "label": 0
                },
                {
                    "sent": "So how is the probability that our estimate is going to be within epsilon of that and it's this expression here so you can take this and solve for what the probability is so.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this is the probability you can take this and say if I'm saying I want to be Delta Shore 1 minus Delta, sure that I'm within epsilon you can solve for what M what sample size is going to give you that assurance.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does this let you know is that once if I want to be 95% sure that I'm within .0, one of the right probability we solve for M, we say.",
                    "label": 0
                },
                {
                    "sent": "Well, here's how many examples you'd have to do to be that sure.",
                    "label": 0
                },
                {
                    "sent": "So now the quick algorithm, the quick learner is going to say I don't know M times to gather that data, and once it's gathered that data that estimate with high probability is epsilon close, and so we can just use that app that estimate for then on.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not gonna tell you the function.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to give you and we're going to play this as you're going to be a quick learner.",
                    "label": 0
                },
                {
                    "sent": "We hope this goes quickly and so that means that I give you the input and you have to give me the output and it has to be correct.",
                    "label": 0
                },
                {
                    "sent": "Or you can say I don't know.",
                    "label": 1
                },
                {
                    "sent": "OK, but you can't say I don't know forever 'cause that would be cheating.",
                    "label": 0
                },
                {
                    "sent": "Alright, so first input one.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, that's fair.",
                    "label": 0
                },
                {
                    "sent": "The output is 1 for that case.",
                    "label": 0
                },
                {
                    "sent": "OK, next input is 1.",
                    "label": 0
                },
                {
                    "sent": "I heard it, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yes, the output is one that's correct alright.",
                    "label": 0
                },
                {
                    "sent": "The next input is 0.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Alright, that was one also turns out alright.",
                    "label": 0
                },
                {
                    "sent": "Next input is 1.",
                    "label": 0
                },
                {
                    "sent": "Good next input is zero.",
                    "label": 0
                },
                {
                    "sent": "There we go alright, and we've learned the function and you've learned it with no mistakes.",
                    "label": 0
                },
                {
                    "sent": "By the way, congratulations.",
                    "label": 0
                },
                {
                    "sent": "So an how but how many times did you have sort of a ridiculous example, but it turns out lots of lots of functions have elaborations of that idea in them, so for this particular case, how many mistakes?",
                    "label": 0
                },
                {
                    "sent": "Not mistake?",
                    "label": 0
                },
                {
                    "sent": "Sorry, how many I don't know is did you use?",
                    "label": 0
                },
                {
                    "sent": "Two and in fact is that a bound for this class of functions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You could do worse, but you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You don't need to because you just memorize it, right?",
                    "label": 0
                },
                {
                    "sent": "You just start to fill in the table in your head.",
                    "label": 0
                },
                {
                    "sent": "So some of the things that we can quick learn have that kind of form to it.",
                    "label": 0
                },
                {
                    "sent": "You just basically fill in the table as you find out what the right answer is.",
                    "label": 0
                },
                {
                    "sent": "You know it, and you can write it down, yes.",
                    "label": 0
                },
                {
                    "sent": "Learning the concept.",
                    "label": 0
                },
                {
                    "sent": "No, they could actually the not not input.",
                    "label": 0
                },
                {
                    "sent": "That would have been fine function to learn to.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Why'd you say that?",
                    "label": 0
                },
                {
                    "sent": "Because you said I don't know twice you had you said I don't know for the zero and you said I don't know for the one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but then like if we just see that the outcome is always wrong, and if you say if you say 0 at some point and we say one and actually the function is not constant, then the outcome is.",
                    "label": 0
                },
                {
                    "sent": "So in the function that I was just doing this deterministic is that, does that fix it?",
                    "label": 0
                },
                {
                    "sent": "Yeah no.",
                    "label": 0
                },
                {
                    "sent": "So I let that example off saying it's a function from bit to bit.",
                    "label": 0
                },
                {
                    "sent": "Oreo is that when you say constant, you meant you mean like everytime its input is zero.",
                    "label": 0
                },
                {
                    "sent": "It's always a one out yeah terministic function.",
                    "label": 0
                },
                {
                    "sent": "In that example, if it was a probabilistic function, you could have learned the probabilities.",
                    "label": 0
                },
                {
                    "sent": "There's two probabilities how well for each one you run a separate copy of this algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So you keep count of when you've heard zeros in input, how many successes, how many failures?",
                    "label": 0
                },
                {
                    "sent": "When I've heard one is an input, how many successes, how many code?",
                    "label": 0
                },
                {
                    "sent": "So even though this feels sort of silly because you say you know, I don't know 30 times in a row and then you know it.",
                    "label": 0
                },
                {
                    "sent": "It's a little more interesting if there's an input involved.",
                    "label": 0
                },
                {
                    "sent": "So if I tell you zero, you have to learn the probability of 0 and you have to learn probably for one, you're keeping two separate counts, and so you might say I don't know for awhile and then start knowing one of them and then start saying I don't know again when I give you different inputs so it starts off simple.",
                    "label": 0
                },
                {
                    "sent": "But as you start to compose these learners it gets more and more interesting.",
                    "label": 0
                },
                {
                    "sent": "I did see a hand or partial hands.",
                    "label": 0
                },
                {
                    "sent": "There's a whole hand.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "At some point you have to stop saying I don't know, but you still have probability of landing and indifferent or making a mistake.",
                    "label": 0
                },
                {
                    "sent": "30% in lending it 0 and 7% is ending on one right?",
                    "label": 0
                },
                {
                    "sent": "OK. And now I know.",
                    "label": 0
                },
                {
                    "sent": "So the game here is if we're trying to predict the probability the thing you're supposed to output to me is the probability right?",
                    "label": 0
                },
                {
                    "sent": "And so that probability can be arbitrarily close to the true answer, even though none of the examples you get, so you know if it's a fair coin.",
                    "label": 0
                },
                {
                    "sent": "Eventually you should learn to say something like 1/2, but none of your data was a half, right?",
                    "label": 0
                },
                {
                    "sent": "I give you just zeros and ones as an example as example, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I could define this more formally, but I'm being forced to do so gradually.",
                    "label": 0
                },
                {
                    "sent": "Apologize for their hands.",
                    "label": 0
                },
                {
                    "sent": "OK, so so there's just a couple of things that we can quick learn effectively.",
                    "label": 0
                },
                {
                    "sent": "So far we can just a single coin and a single coin.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on a small set of finite set of inputs.",
                    "label": 0
                },
                {
                    "sent": "For example, we just learn them separately.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could learn.",
                    "label": 0
                },
                {
                    "sent": "Let's say that the output is, there's no input, but the output is a bunch of numbers which are.",
                    "label": 0
                },
                {
                    "sent": "The probabilities and all the examples that I give you are draws from that vector, right?",
                    "label": 0
                },
                {
                    "sent": "So you like like.",
                    "label": 0
                },
                {
                    "sent": "Like a bunch of coins, almost have a bunch of weighted coins in coin, one has a probability coin, two has a problem probability coin three has a probability and the examples that I give for training are while I go and flip those coins and tell you which ones came up heads in which ones tales free on each trial.",
                    "label": 0
                },
                {
                    "sent": "But you can start to pull those statistics and learn the same way again.",
                    "label": 0
                },
                {
                    "sent": "So if the output is a vector, each component is a probability, then as long as you could quick learn each of the components of that vector, you can quick learn the entire vector.",
                    "label": 1
                },
                {
                    "sent": "If the function that you're trying to learn is a mapping from big input space and it's partitioned in some way, and each within each partition you have to learn some quick learnable class, then you could learn the whole class because you just running a separate quick learner for each piece of that partition.",
                    "label": 0
                },
                {
                    "sent": "So these seem like really kind of simple, obvious examples, but what's nice is that this is exactly what you need to make a standard transition function in reinforcement learning, right?",
                    "label": 1
                },
                {
                    "sent": "So all that is, it's a mapping that takes SNA as input, which partitions the space, and then when you get to observe, are these outcomes you know was it state S7 that came up as a result, yes or no.",
                    "label": 0
                },
                {
                    "sent": "So just with the pieces that we have so far, we can quick learn a transition function, and because we can quick learner transition function, we're going to be able to make a pack MDP algorithm out of it.",
                    "label": 0
                },
                {
                    "sent": "Now people already knew how to do that without any of these quick learning.",
                    "label": 0
                },
                {
                    "sent": "Mumbo jumbo in between, but we're going to start composing them in more interesting ways and get some new algorithms that hadn't been.",
                    "label": 0
                },
                {
                    "sent": "Discovered before another thing, you can also quick learning the union of two quick, learnable classes.",
                    "label": 1
                },
                {
                    "sent": "Just various ways of composing these things that all workout.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm more or less said this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think already that the R Max algorithm for Bath and attend and what it does leave didn't say it this way, but you can think of it as what they're doing is a quick learner transition function T, and for anything that's not known, it assumes that the Q values for the state action pairs have maximum reward, so anything that it doesn't know about yet because it hasn't visited enough times is assumed to be really high reward.",
                    "label": 0
                },
                {
                    "sent": "Nothing could be higher, and So what it does is it goes out and visits those if it can, and if it can't then it stays among the states that it can visit.",
                    "label": 0
                },
                {
                    "sent": "An gets near optimal reward.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple, elegant algorithm.",
                    "label": 0
                },
                {
                    "sent": "All it's doing is counting the number of times it's tried.",
                    "label": 0
                },
                {
                    "sent": "Each state action pair.",
                    "label": 0
                },
                {
                    "sent": "When that count goes over threshold, it uses the empirical distribution that it found as truth.",
                    "label": 0
                },
                {
                    "sent": "Before that point it just uses some optimistic value.",
                    "label": 0
                },
                {
                    "sent": "Doesn't even try to estimate the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that works.",
                    "label": 0
                },
                {
                    "sent": "It gets you a pack, MDP bound and the total mistakes are the bound that I had up before.",
                    "label": 0
                },
                {
                    "sent": "Anne, that works out yes.",
                    "label": 0
                },
                {
                    "sent": "Samples, yeah, right right?",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, what is the transition function?",
                    "label": 0
                },
                {
                    "sent": "Well, for some state like you know, the robot is at 30 degrees.",
                    "label": 0
                },
                {
                    "sent": "Say it's discretized, so 30 degrees.",
                    "label": 0
                },
                {
                    "sent": "It's going to try some action left and we were trying to estimate the probability that actually goes left.",
                    "label": 0
                },
                {
                    "sent": "Say, well, we've only tried it 6 times.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's mostly gone left, but we're going to try it for more times before we can say.",
                    "label": 0
                },
                {
                    "sent": "We've estimated that probability with extremely high accuracy with high probability.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, if we want to have some kind of bound that the overall failure probability, the whole learning algorithm is low, it's going to have to estimate the probability of failure of any of the components is going to be much lower than that 'cause all the component.",
                    "label": 0
                },
                {
                    "sent": "All the things that have to estimate have to be correct for the overall algorithm to work.",
                    "label": 0
                },
                {
                    "sent": "So the papers that talk about this stuff goes through the derivations and say exactly how those quantities relate to each other, and so a lot of uses of the Union bound for example.",
                    "label": 0
                },
                {
                    "sent": "So if you want, yeah, if you want the group to succeed then you have to make sure that.",
                    "label": 0
                },
                {
                    "sent": "Well, you then make sure that each component succeeds and you estimate the probability of the failures being the sum, which is a little over conservative, but.",
                    "label": 0
                },
                {
                    "sent": "It gets polynomial bounds.",
                    "label": 0
                },
                {
                    "sent": "The bounds tend to be really I mean like you can see with the 1 minus gamma to the eights and things like that.",
                    "label": 0
                },
                {
                    "sent": "The bounds tend to be really.",
                    "label": 0
                },
                {
                    "sent": "Not useful in the sense that the number if you literally look at the numbers that they give out.",
                    "label": 0
                },
                {
                    "sent": "It's like, Oh yes, yes, you'll have it all you have to do is learn for 772 trillion trials and you'll have an accurate estimate.",
                    "label": 0
                },
                {
                    "sent": "But that's not what we in practice.",
                    "label": 0
                },
                {
                    "sent": "We don't actually run that many.",
                    "label": 0
                },
                {
                    "sent": "It seems like it seems like it tends to work fine with much smaller numbers, but the analysis does seem to lead us towards the useful algorithms.",
                    "label": 0
                },
                {
                    "sent": "So even if the numbers aren't exactly right, the overall framework seems to be right.",
                    "label": 0
                },
                {
                    "sent": "So a bit weird in your practice meeting framework, and you asked me for million and one penetration.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we're going, so we're not.",
                    "label": 0
                },
                {
                    "sent": "We're getting away from that here.",
                    "label": 0
                },
                {
                    "sent": "'cause now?",
                    "label": 0
                },
                {
                    "sent": "The bound if you actually look at the general quick based learning algorithm, it ends up being polynomial in the number of.",
                    "label": 0
                },
                {
                    "sent": "I don't knows that the quick algorithm makes as opposed to the number of states in the world, so that's a big advantage.",
                    "label": 0
                },
                {
                    "sent": "So if there's something that you can learn faster than separately estimating a quantity for each state, then the learning actually ends up happening faster.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we're not quite there yet, so here's an example of using our Max in robotic example where we discretized everything so it looks continuous, but we're not treating it continuous.",
                    "label": 0
                },
                {
                    "sent": "This is a Sony AIBO wearing a fetching Raspberry Beret and green Fanny pack so that it can be tracked from above.",
                    "label": 0
                },
                {
                    "sent": "So the state information is the position orientation are coming from an overhead camera, and it's trying to escape from the box and its actions are.",
                    "label": 0
                },
                {
                    "sent": "Forward backward turn left turn right kind of sidell left inside alright.",
                    "label": 1
                },
                {
                    "sent": "And it takes awhile for it to learn about all 4000 states in the world.",
                    "label": 0
                },
                {
                    "sent": "But it eventually learns enough that it can get itself out of the box.",
                    "label": 0
                },
                {
                    "sent": "So we compared this.",
                    "label": 0
                },
                {
                    "sent": "We didn't compare this to Q learning 'cause that would have been.",
                    "label": 0
                },
                {
                    "sent": "Well, we would have put the robot at risk.",
                    "label": 0
                },
                {
                    "sent": "It takes a very long time for Q learning to learn on the robots, but we did something similar, which is we learn to model.",
                    "label": 0
                },
                {
                    "sent": "But we explored by taking random actions occasionally, which is often how Q learning is implemented an it just it is a lot slower but taking random actions in the beginning is really bad because it doesn't know anything yet and it should be taking.",
                    "label": 0
                },
                {
                    "sent": "It should be trying everything.",
                    "label": 0
                },
                {
                    "sent": "Sorry, taking random actions biased towards what you think is the best is a bad idea.",
                    "label": 0
                },
                {
                    "sent": "'cause in the beginning it shouldn't be biased and at the end it should be very heavily biased towards towards the.",
                    "label": 0
                },
                {
                    "sent": "Actions that are actually correct.",
                    "label": 0
                },
                {
                    "sent": "An arm acts more or less does that automatically because it's estimating it's making these decisions based on how many times it tried actions in each of the states.",
                    "label": 0
                },
                {
                    "sent": "So there's things that hasn't tried yet.",
                    "label": 0
                },
                {
                    "sent": "It actually will walk all the way across the world to try it right?",
                    "label": 0
                },
                {
                    "sent": "It's not waiting for some random sequence of events to happen to get into this new situation.",
                    "label": 0
                },
                {
                    "sent": "It's much more directed.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What is the don't pay attention to the graph?",
                    "label": 0
                },
                {
                    "sent": "I actually don't remember these are these are unpublished results, so we never really quite cleaned it up, but it's supposed to be like trials, yeah, but why is this 3000?",
                    "label": 0
                },
                {
                    "sent": "I think it steps like actual robot actions.",
                    "label": 0
                },
                {
                    "sent": "Versus average number of steps.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get to the goal for trial, all right, but this is not generalizing transitions at all.",
                    "label": 0
                },
                {
                    "sent": "So let's start talking about what does it mean to start getting away from detailed information about each separate state.",
                    "label": 0
                },
                {
                    "sent": "To start generalizing across states.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's lots of ways we can do this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about several of them.",
                    "label": 0
                },
                {
                    "sent": "The very first one I'm going to talk about is a simple kind of merging of this kind of this robot example with simple.",
                    "label": 0
                },
                {
                    "sent": "Graphical model representation.",
                    "label": 0
                },
                {
                    "sent": "So here the basic idea is similar.",
                    "label": 0
                },
                {
                    "sent": "Similar talent show you a similar robot test to the previous one where there's a Sony Aibo.",
                    "label": 0
                },
                {
                    "sent": "It's got its position XY and its orientation data, but there's also in the world aballa Bumble ball that just kind of wobbles around, ignoring the robot.",
                    "label": 0
                },
                {
                    "sent": "But it gets in the way and we have a penalty if the robot comes in contact with the ball.",
                    "label": 0
                },
                {
                    "sent": "It doesn't literally heard it.",
                    "label": 0
                },
                {
                    "sent": "'cause that would be, that would be wrong ethically I guess but but we do give it a -- 1 which is really just as bad and what it's trying to predict from this is the change in the balls position, the robot's position and the robots orientation.",
                    "label": 0
                },
                {
                    "sent": "And what we do is in advance.",
                    "label": 0
                },
                {
                    "sent": "In this case we had the grad student who built this, created a little independence model that says OK, the ball's position.",
                    "label": 0
                },
                {
                    "sent": "That seems wrong.",
                    "label": 0
                },
                {
                    "sent": "So the other grad student made a model and then I broke it.",
                    "label": 0
                },
                {
                    "sent": "Which I think this edge is is what we want to fix.",
                    "label": 0
                },
                {
                    "sent": "The barn at the ball doesn't depend on anything.",
                    "label": 0
                },
                {
                    "sent": "I think that edge just should go away.",
                    "label": 0
                },
                {
                    "sent": "Well, let's talk about that.",
                    "label": 0
                },
                {
                    "sent": "So, so the ball is the ball really is just independent, it just moves around.",
                    "label": 0
                },
                {
                    "sent": "So the ball's position should only depend on the ball's old position.",
                    "label": 0
                },
                {
                    "sent": "This is this is how the variable changes as a function of what it is now.",
                    "label": 0
                },
                {
                    "sent": "So that seems right.",
                    "label": 0
                },
                {
                    "sent": "The robots X&Y changes.",
                    "label": 0
                },
                {
                    "sent": "Should depend on its orientation.",
                    "label": 0
                },
                {
                    "sent": "I guess it depends on what action we're doing turning versus going straight, but I'm going to say maybe this edge should be reversed.",
                    "label": 0
                },
                {
                    "sent": "Right, so the robot's orientation impacts its change in position and orientation.",
                    "label": 0
                },
                {
                    "sent": "But the robot's position only infects its next position, so this is for something like a.",
                    "label": 0
                },
                {
                    "sent": "Like a turn or straight.",
                    "label": 0
                },
                {
                    "sent": "Alright, anyway, the point is that we don't have to estimate the full cross product of influences between all the variables and how they're going to change in the next time step.",
                    "label": 0
                },
                {
                    "sent": "We can factor them out in this way and then each of these pieces so where the parameters here, so we need to estimate how the ball changes as a function of where it was.",
                    "label": 0
                },
                {
                    "sent": "And in fact I think we even ignore that.",
                    "label": 0
                },
                {
                    "sent": "We just say the change in the balls position is independent of its current position.",
                    "label": 0
                },
                {
                    "sent": "Its new position depends on the old position, but it's changed.",
                    "label": 0
                },
                {
                    "sent": "Is independent, the robots X&Y coordinate that, or the change in the XY coordinate don't have any effect, are not affected by where it is because there's no obstacles or anything in this world other than the ball, and that only has an impact on the reward anyway, so this picture I think you should just ignore that picture entirely, 'cause it seems to bear no relation, but the higher the high order thing that I'm trying to get across is that that we start assuming that there's independence in various pieces, and once we've made those assumptions, we can say let's quick learn each of those components.",
                    "label": 0
                },
                {
                    "sent": "OK, by observing it enough times and estimating those parameters.",
                    "label": 0
                },
                {
                    "sent": "And the bounds for doing that depend on the number of parameters, not on the actual size of the state space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's not a robot.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that was just that was a motivational shot there.",
                    "label": 0
                },
                {
                    "sent": "Yeah alright so this is again this is being tracked by an overhead camera.",
                    "label": 0
                },
                {
                    "sent": "The robot is trying to get to the goal position which is in the corner here and the Bumble ball.",
                    "label": 0
                },
                {
                    "sent": "OK, hasn't learned much yet.",
                    "label": 0
                },
                {
                    "sent": "The Bumble ball really is.",
                    "label": 0
                },
                {
                    "sent": "We've plotted the motion of it.",
                    "label": 0
                },
                {
                    "sent": "It's very brownian.",
                    "label": 0
                },
                {
                    "sent": "It actually moves really very very nicely.",
                    "label": 0
                },
                {
                    "sent": "It's almost markov.",
                    "label": 0
                },
                {
                    "sent": "I mean, it really seems Markov, which is pretty cool considering that it's just the Bumble ball.",
                    "label": 0
                },
                {
                    "sent": "Alright, and we have a bunch of learning trials.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah we actually re implemented some standard SRL simulator simulations, but we did them with the actual robot instead of simulator.",
                    "label": 0
                },
                {
                    "sent": "Alright and so.",
                    "label": 0
                },
                {
                    "sent": "There it is.",
                    "label": 0
                },
                {
                    "sent": "The robot heading for the goal here goes to the goal position OK success?",
                    "label": 0
                },
                {
                    "sent": "Notice anything interesting it doesn't know where its face is right, because it's all the positional information comes from the overhead camera, even though there's a camera on the end of it snows, we're not using that for any state information, it's trying to move in whatever way gets it to the goal.",
                    "label": 0
                },
                {
                    "sent": "Fastest 'cause it's sort of watching itself as an out of body experience and.",
                    "label": 0
                },
                {
                    "sent": "It does it.",
                    "label": 0
                },
                {
                    "sent": "It turned out that because of the way it was programmed, the backward walking was slightly faster and so the learning algorithm figured that out.",
                    "label": 0
                },
                {
                    "sent": "An would use backwards walking.",
                    "label": 0
                },
                {
                    "sent": "This is an unfortunate trial where the Bumble ball kind of parked itself in the way so the robot.",
                    "label": 0
                },
                {
                    "sent": "It has a policy which Maps all positions and orientations to where it should go and these low probability events keep happening.",
                    "label": 0
                },
                {
                    "sent": "Get out of my face.",
                    "label": 0
                },
                {
                    "sent": "I think this is more or less what robot frustration might look like.",
                    "label": 0
                },
                {
                    "sent": "At that point, you start checking it's like is it really random?",
                    "label": 0
                },
                {
                    "sent": "Is it really random?",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem random anymore.",
                    "label": 0
                },
                {
                    "sent": "But eventually eventually come on.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "Squeeze in there before the ball gets in the way.",
                    "label": 0
                },
                {
                    "sent": "And there we go, alright so.",
                    "label": 0
                },
                {
                    "sent": "The reason this was able so that was like trial 15 or something like that is that what it said.",
                    "label": 0
                },
                {
                    "sent": "The reason it was able to learn in such a small number of trials is because it totally took advantage of all this conditional independence that we built into the function approx.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later for the transition function.",
                    "label": 0
                },
                {
                    "sent": "So that's one sort of function that you can build.",
                    "label": 0
                },
                {
                    "sent": "OK, I think OK, so there's I think if I think we're saying like 10 more minutes or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So if I want to speed through this thing 'cause I think I'm getting.",
                    "label": 0
                },
                {
                    "sent": "I got myself a little bit down but there's other kinds of ways that you can introduce various kinds of structure to learn things faster.",
                    "label": 0
                },
                {
                    "sent": "In the example that I just gave, there was in spite of that horrible picture that I do, there was no independence of the state on how things were going to change.",
                    "label": 0
                },
                {
                    "sent": "Which makes no sense.",
                    "label": 0
                },
                {
                    "sent": "Surely there must be some influence, but but let's say that there's another way that you can think about.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Is every individual state has an influence.",
                    "label": 0
                },
                {
                    "sent": "That's back to the original model.",
                    "label": 0
                },
                {
                    "sent": "Well, what if there's different types of States and within each type there's a mapping to how things are going to change, But this this is a separate types can be learned separately.",
                    "label": 0
                },
                {
                    "sent": "So we took a model that had been published in the reinforcement learning literature for doing that with MVP's kind of factorizing the MVP into these types.",
                    "label": 0
                },
                {
                    "sent": "And we took advantage of it and were able to learn.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gridworld a lot faster.",
                    "label": 0
                },
                {
                    "sent": "Have the right kind of generalization, speed things up.",
                    "label": 0
                },
                {
                    "sent": "We did the same thing on a robot example, so here this one is actually being tracked using Motion tracker things little silver balls.",
                    "label": 0
                },
                {
                    "sent": "And it's trying to get to this white box.",
                    "label": 0
                },
                {
                    "sent": "You might not be able to see in the corner here an we told it that different color regions may have different dynamics, so it learns a separate action model for what all these actions do on the sandbag versus on the black painted wood.",
                    "label": 0
                },
                {
                    "sent": "And the result of that is actually able to take advantage of this in a substantial way.",
                    "label": 0
                },
                {
                    "sent": "But if we if we ignore those distinctions, certainly if we learn each individual place in the world and map that to an action model, it takes forever to learn.",
                    "label": 0
                },
                {
                    "sent": "If we take the whole world and say no, it's really just one big space and just learn the same action model for the whole space.",
                    "label": 0
                },
                {
                    "sent": "That doesn't work well either, because the sand is actually a little bit slippy, and So what tends to do is that it tries to point itself to the goal while on the sand, and it often overshoots, and then it falls out of the world and it's minus one.",
                    "label": 0
                },
                {
                    "sent": "So being able to condition on these two types ends up.",
                    "label": 0
                },
                {
                    "sent": "Learning a more accurate model but still very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "We did something similar.",
                    "label": 0
                },
                {
                    "sent": "The Grad student who did this spent almost all of her time trying to find different surface is.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's really easy to find a surface that the robot can't move on at all, but that doesn't make for good learning experiments.",
                    "label": 0
                },
                {
                    "sent": "And of the ones that it can move on, they all kind of act the same, so she eventually she and her mom apparently melted wax on their stove at home and then embedded smooth rocks from an aquarium in it, and so that's what you just saw there a moment ago and it was like just bumpy enough, just smooth enough that it could move, but it.",
                    "label": 0
                },
                {
                    "sent": "Move really clunkily there you can see it there.",
                    "label": 0
                },
                {
                    "sent": "But again, what it learns in this case is based on image analysis.",
                    "label": 0
                },
                {
                    "sent": "It breaks the two regions into two separate piles, and that's built in right?",
                    "label": 0
                },
                {
                    "sent": "It's not learning that, it's just taking whatever the texture analysis says as the two regions or however many regions it finds, and learning separate models for each of those.",
                    "label": 0
                },
                {
                    "sent": "But it really does make a difference.",
                    "label": 0
                },
                {
                    "sent": "It learns a very different model for turning on the rocks, then turning on the carpet, and it really, it's like much happier on the carpet, just it gets its groove on.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so wait, this is the payoff to that one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so more or less.",
                    "label": 0
                },
                {
                    "sent": "Here's the robot, but we built a sort of a funky world that if it was to use the same model for the Roxanne the carpet, it wouldn't know that it could make this tight turn to get to the goal over here because it would say Oh well, it's one big noisy surface.",
                    "label": 0
                },
                {
                    "sent": "Well then I can't navigate in this tight space.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that worked out OK. Another form of generalization that you can get is by factoring using graphical model representations of the transitions.",
                    "label": 0
                },
                {
                    "sent": "This is the taxi problem which I don't know if people hadn't seen it before the example.",
                    "label": 0
                },
                {
                    "sent": "The video game example in the beginning is actually a classic reinforcement learning example that people have used in like 300 papers.",
                    "label": 0
                },
                {
                    "sent": "I was able to find that use this example.",
                    "label": 0
                },
                {
                    "sent": "So obviously you haven't read any of those papers if you didn't know what was going on.",
                    "label": 0
                },
                {
                    "sent": "But this phrase is a taxi problem.",
                    "label": 0
                },
                {
                    "sent": "The thing that for us was like a big orange square or something like that is the taxi.",
                    "label": 0
                },
                {
                    "sent": "The little red dot is the passenger, and those colored spots are actually labeled with their colored labels in their literature, and there's walls and so forth.",
                    "label": 0
                },
                {
                    "sent": "One thing that I notice is you guys didn't run into any walls right now.",
                    "label": 0
                },
                {
                    "sent": "The reinforcement learning algorithms tend not to do that right.",
                    "label": 0
                },
                {
                    "sent": "They tend to run into walls over and over again, and in fact, if they see a wall that they hadn't hit before, our Max gets all excited.",
                    "label": 0
                },
                {
                    "sent": "It's like, oh, something new.",
                    "label": 0
                },
                {
                    "sent": "I could bash my head against.",
                    "label": 0
                },
                {
                    "sent": "So it tends to learn relatively slowly, but if you give it a factor Rep.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mentation, so in this case I think I drew this one correct.",
                    "label": 0
                },
                {
                    "sent": "I edited last night.",
                    "label": 0
                },
                {
                    "sent": "We'll see if I broke it or not.",
                    "label": 0
                },
                {
                    "sent": "The taxi has position.",
                    "label": 0
                },
                {
                    "sent": "The passenger has a position and the passenger has a desired destination.",
                    "label": 0
                },
                {
                    "sent": "Taking the action North, the new position for the taxi only depends on the old one.",
                    "label": 0
                },
                {
                    "sent": "The new of the passenger only depends on the old one and the destination.",
                    "label": 0
                },
                {
                    "sent": "It remains unchanged.",
                    "label": 0
                },
                {
                    "sent": "It only depends on the old one, so there's a lot of conditional independence in this model, which is why even though there's 500 some odd states, we were able to solve that problem as a group.",
                    "label": 0
                },
                {
                    "sent": "In fewer than 500 steps right, a lot fewer than 500 steps.",
                    "label": 0
                },
                {
                    "sent": "In fact, if I gave you a whole series of these problems now, you'd be performing pretty much optimally on all of them after the first one or two.",
                    "label": 0
                },
                {
                    "sent": "So having this kind of conditional independence information is really helpful there.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "I should have credited Kerns and Kohler for showing that you could learn these kinds of models efficiently given the graphical structure.",
                    "label": 0
                },
                {
                    "sent": "So if you have, if you're told so, the learner is told, here's the conditional independence structure.",
                    "label": 0
                },
                {
                    "sent": "Now go estimate the probabilities and figure out how to behave in it.",
                    "label": 0
                },
                {
                    "sent": "It can do that efficiently in the size of these structures, not the size of the state space.",
                    "label": 0
                },
                {
                    "sent": "The state space could be combinatorially large, but it only has to estimate the probabilities of the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So net.",
                    "label": 0
                },
                {
                    "sent": "But you could say, but you didn't.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm going so fast.",
                    "label": 0
                },
                {
                    "sent": "But what if you don't have the structure?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that learning an unknown structure is actually a fundamentally different problem because it turns, whereas when you have the the conditional independence structure each each experience that you get can be kind of credited to some piece of the network, so you can actually just keep counts and estimate all the probabilities that you need.",
                    "label": 0
                },
                {
                    "sent": "If you don't know the structure, it's really hard to know how to apportion that data, so it ends up being a very different kind of problem.",
                    "label": 1
                },
                {
                    "sent": "So one thing I could do is put this off till tomorrow, but some of the some of this stuff up tomorrow, so that would that be OK.",
                    "label": 0
                },
                {
                    "sent": "So let me see if I can at least get to some kind of a punch line.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'll return to this piece.",
                    "label": 0
                },
                {
                    "sent": "It another interactive demo thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, don't look, it's too late.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the things that we did is to look at the taxi problem.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you do in the taxi problem is not just pay attention to conditional independence, but the fact that there's object right.",
                    "label": 0
                },
                {
                    "sent": "So once in fact you could see that there were walls and you just knew how walls work.",
                    "label": 0
                },
                {
                    "sent": "You didn't even try to pass through the walls, but you can imagine a learner that was a little bit more naive, at least discovering how walls work and then transmitting that information to all the other walls that it sees, kind of generalizing its experience to walls and elsewhere in the environment.",
                    "label": 0
                },
                {
                    "sent": "And so we did look at.",
                    "label": 0
                },
                {
                    "sent": "We created a learner that takes advantage of that kind of information by.",
                    "label": 0
                },
                {
                    "sent": "Assigning representing the model in terms of the objects in the world.",
                    "label": 1
                },
                {
                    "sent": "So there's ataxia, passenger there's walls as destinations and instead of keeping track of explicit states or even features of states, we talked about objects and attributes of objects.",
                    "label": 1
                },
                {
                    "sent": "So the learner specifically tries to represent his transitions in terms of those objects and we do get much more human like kinds of expl.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient, in that case, it learns the rules that it learns or things like if you go North and you're not touching to the North.",
                    "label": 0
                },
                {
                    "sent": "Or sorry the taxi is not touching the wall to its North, then the Y coordinate of the taxi gets incremented, right?",
                    "label": 0
                },
                {
                    "sent": "That's sort of what it means to go North.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "It's closer, certainly closer than what these other models learn, which is well, here's what go North means.",
                    "label": 0
                },
                {
                    "sent": "If you're in State 532, that means you go to State 511.",
                    "label": 0
                },
                {
                    "sent": "If you're in State 26 that this is much more generalizable, dropping off a passenger has a similar representation, and the quick bound that we get for learning this kind of structure is polynomial, and the types of objects in the world, it's exponential, and how complex their conditions are 'cause it does have to check different ways that they could interact.",
                    "label": 0
                },
                {
                    "sent": "But if we now compare how fast it takes to learn optimal behavior in the taxi problem.",
                    "label": 0
                },
                {
                    "sent": "Q Learning with epsilon greedy exploration where it just tries the thing that it thinks is right, but with some probability something random.",
                    "label": 0
                },
                {
                    "sent": "Takes about 50,000 steps to get this task down.",
                    "label": 0
                },
                {
                    "sent": "Flat are Max.",
                    "label": 0
                },
                {
                    "sent": "We're actually keeping track of which states it's been too, and it's pushing itself to new states to visit them, but still representing the states is just a big giant graph.",
                    "label": 0
                },
                {
                    "sent": "It goes down by an order of magnitude two, like 4000.",
                    "label": 0
                },
                {
                    "sent": "Factored our Max would actually give it the conditional independent structures so it only has to pay attention to features in the world, so it can actually learn about a transition that it's never seen before because it's seen the different pieces of that transition already occur.",
                    "label": 0
                },
                {
                    "sent": "Then it goes down to about half of that to 2000 steps.",
                    "label": 0
                },
                {
                    "sent": "If we do it based on objects, the objects in the world, we can get down to like 140, three, which seems like OK, but you've now built in everything you know, which is sort of true.",
                    "label": 0
                },
                {
                    "sent": "But people man the experiments that I've done maybe 50 ish steps people get.",
                    "label": 0
                },
                {
                    "sent": "It depends a lot on whether you spend a lot of your life playing.",
                    "label": 0
                },
                {
                    "sent": "Video games turns out there was one person who says he claims that I showed the slide.",
                    "label": 0
                },
                {
                    "sent": "He'd never seen the taxi problem before he showed I showed in the slide.",
                    "label": 0
                },
                {
                    "sent": "He said, well, that's the same color.",
                    "label": 0
                },
                {
                    "sent": "Is that so?",
                    "label": 0
                },
                {
                    "sent": "Probably it's navigation.",
                    "label": 0
                },
                {
                    "sent": "The A and the B is going to pick up a drop off of that object, so he solved it with no trials at all.",
                    "label": 0
                },
                {
                    "sent": "He claimed to but but you know, but you can almost believe that you get a lot of it just by looking.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty remark.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use this idea.",
                    "label": 0
                },
                {
                    "sent": "This is this is going to be my punch line and then I'll sign off.",
                    "label": 0
                },
                {
                    "sent": "So we use this idea to learn the game pitfall.",
                    "label": 0
                },
                {
                    "sent": "Has anybody wasted time on this particular problem?",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you so here.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here is this guy is trying to get off the right side of the world.",
                    "label": 0
                },
                {
                    "sent": "There's a ladder he can get in.",
                    "label": 0
                },
                {
                    "sent": "There's a there's a wall that's going to block his way here.",
                    "label": 0
                },
                {
                    "sent": "There's a log that costs points.",
                    "label": 0
                },
                {
                    "sent": "He's got North, South East, West diagonals and all those with the button press so you can jump.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we actually took.",
                    "label": 0
                },
                {
                    "sent": "An emulator, an actual Atari emulator for this game and plugged it into a reinforcement learner by saying we just we go through the pixels and and find regions that are of the same color and will say that's an object.",
                    "label": 0
                },
                {
                    "sent": "So objects become rectangles.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look as beautiful as the graphics were.",
                    "label": 0
                },
                {
                    "sent": "We simplified them substantially by making everything just colored rectangles, but then the learner had to predict how those color rectangles were going to move as a function of the actions of the joystick actions.",
                    "label": 0
                },
                {
                    "sent": "So we actually then plug the learner into the Atari emulator and let it play Atari games, 'cause you know.",
                    "label": 0
                },
                {
                    "sent": "We don't have time for it anymore, but our programs do and so.",
                    "label": 0
                },
                {
                    "sent": "So, So what?",
                    "label": 0
                },
                {
                    "sent": "It's trying to learn is, well, what is jumping do when these two objects are touching?",
                    "label": 0
                },
                {
                    "sent": "What is jumping do when these two objects are not touching or when this one's above that one is trying to learn how the object interactions?",
                    "label": 0
                },
                {
                    "sent": "Caused the objects to change so.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so as you can see what it ends up doing is it explores in this really interesting way, so it's so in the beginning it tries all kinds of crazy jumps, but then it gets tired of that and just walks across the ground.",
                    "label": 0
                },
                {
                    "sent": "Because this is all the same it predicts.",
                    "label": 0
                },
                {
                    "sent": "This is all the same, but then when it gets to the whole it's like ooh something new and it tries all the different jumps around the whole and it's like oh the ladder.",
                    "label": 0
                },
                {
                    "sent": "I don't know what that does and it goes down.",
                    "label": 0
                },
                {
                    "sent": "There is hopping around on the bottom floor a little bit 'cause it's never tried that before, but then it then it then it just beelines right to the wall, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't try all the states in between.",
                    "label": 0
                },
                {
                    "sent": "It doesn't try jumping in all possible configurations.",
                    "label": 0
                },
                {
                    "sent": "It tries jumping next to the wall just like you guys wanted to go to the colored square, and when you were playing the video game, you didn't try all the squares in the grid.",
                    "label": 0
                },
                {
                    "sent": "He's back at the hole and kind of fixated on that.",
                    "label": 0
                },
                {
                    "sent": "And now there's the Logn.",
                    "label": 0
                },
                {
                    "sent": "He starts kind of rubbing himself up against the log in all possible ways.",
                    "label": 0
                },
                {
                    "sent": "But then he gets off the screen.",
                    "label": 0
                },
                {
                    "sent": "But now he's learned about every object.",
                    "label": 0
                },
                {
                    "sent": "There's nothing else to do except win this game, so it just goes right across.",
                    "label": 0
                },
                {
                    "sent": "It jumps over the hole.",
                    "label": 0
                },
                {
                    "sent": "It jumps over the log and then it does a happy dance so.",
                    "label": 0
                },
                {
                    "sent": "And I will.",
                    "label": 0
                },
                {
                    "sent": "I will sign off for now.",
                    "label": 0
                },
                {
                    "sent": "I've got more to say tomorrow, but thanks a lot for your attention.",
                    "label": 0
                }
            ]
        }
    }
}