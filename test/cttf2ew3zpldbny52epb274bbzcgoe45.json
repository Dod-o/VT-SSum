{
    "id": "cttf2ew3zpldbny52epb274bbzcgoe45",
    "title": "Thompson Sampling for Learning Parameterized Markov Decision Processes",
    "info": {
        "author": [
            "Aditya Gopalan, Indian Institute of Science Bangalore"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_gopalan_markov_decision/",
    "segmentation": [
        [
            "This is joint work with Simon or."
        ],
        [
            "So here is the problem.",
            "The problem is the one of online reinforcement learning.",
            "So let's suppose you're dealing with trying to play actions in an unknown Markov decision process.",
            "So what does that mean?",
            "You have a bunch of states you."
        ],
        [
            "A bunch of actions that you can play at each state and you have a model."
        ],
        [
            "For the environment, which is fixed but unknown to you, so you have transition probabilities starting from each state and taking a certain action that leads to a distribution on the next state."
        ],
        [
            "And you have a reward function that is basically telling you how much reward you collect by making the transition."
        ],
        [
            "So basically what you do is you start out at fixed initial state, let's say."
        ],
        [
            "At each point you take decide to take some action like you're trying to learn and collect maximum reward."
        ],
        [
            "Move to some state you are."
        ],
        [
            "Have some reward.",
            "You keep doing this."
        ],
        [
            "Great to keep observing."
        ],
        [
            "Rewards and."
        ],
        [
            "Undergoing random."
        ],
        [
            "Positions."
        ],
        [
            "And you do this."
        ],
        [
            "Or some amount of time."
        ],
        [
            "And let."
        ],
        [
            "Do you want to try and Maxim?"
        ],
        [
            "Is your net reward."
        ],
        [
            "Over the horizon of."
        ],
        [
            "OK, so that's basically online reinforcement learning problem.",
            "So you have to maximize the trying maximize.",
            "I didn't expectation or with high probability the reward that you collect.",
            "Equivalently it's you can try and minimize the regret between what you get and what is the what is a particular optimal average cost policy for this environment that you're working in.",
            "OK, so the interesting case here is when the mark of decision process may be really large, but you know that it has an implicit parameterization, so there's several examples of this sort where you might sort of work in an abstract.",
            "Amateur space that exactly specifies all your empty structure, so how would you try and learn as well as exploit and explore such as to maximize your reward?"
        ],
        [
            "So in this situation, Thompson sampling is a very well known sort of natural approach that translates to this setting.",
            "It was basically a heuristic for a large time until a few years ago, so the idea is very simple.",
            "So basically imagine that you have the space of all parameterized NDPS you try and just start with an artificial prior over it.",
            "At each point we just sample."
        ],
        [
            "An instance of an MDP from this prior, once you."
        ],
        [
            "MVP just put all your trust into it.",
            "That is, try and find plan the best policy for that MDP and just use it for a few instance of time.",
            "Use whatever observations you get, update your posterior belief and then continue OK.",
            "So just repeat this prescription right so the question is."
        ],
        [
            "This seems touristic, but does this really give this really work?",
            "Well, it doesn't really give you the right balance of exploration and exploitation to try and solve online reinforcement learning problems, which might be complicated."
        ],
        [
            "We actually show that the answer is yes, we actually can get problem dependent regret bound that basically grows as essentially see times locked your constant times locked.",
            "If the number of intervals that you played this game and most importantly the constant E, The scaling of the regret in fact captures the exact structure of the parameterization in complicated but explicitly characterizable way and this."
        ],
        [
            "It has the implication that you can actually accelerate the rate of learning and regret minimization drastically if you have much more structure in the MD.",
            "So thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Simon or.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the problem.",
                    "label": 0
                },
                {
                    "sent": "The problem is the one of online reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So let's suppose you're dealing with trying to play actions in an unknown Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of states you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bunch of actions that you can play at each state and you have a model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the environment, which is fixed but unknown to you, so you have transition probabilities starting from each state and taking a certain action that leads to a distribution on the next state.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you have a reward function that is basically telling you how much reward you collect by making the transition.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what you do is you start out at fixed initial state, let's say.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At each point you take decide to take some action like you're trying to learn and collect maximum reward.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to some state you are.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have some reward.",
                    "label": 0
                },
                {
                    "sent": "You keep doing this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great to keep observing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rewards and.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Undergoing random.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you do this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or some amount of time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you want to try and Maxim?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is your net reward.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the horizon of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's basically online reinforcement learning problem.",
                    "label": 1
                },
                {
                    "sent": "So you have to maximize the trying maximize.",
                    "label": 0
                },
                {
                    "sent": "I didn't expectation or with high probability the reward that you collect.",
                    "label": 0
                },
                {
                    "sent": "Equivalently it's you can try and minimize the regret between what you get and what is the what is a particular optimal average cost policy for this environment that you're working in.",
                    "label": 0
                },
                {
                    "sent": "OK, so the interesting case here is when the mark of decision process may be really large, but you know that it has an implicit parameterization, so there's several examples of this sort where you might sort of work in an abstract.",
                    "label": 0
                },
                {
                    "sent": "Amateur space that exactly specifies all your empty structure, so how would you try and learn as well as exploit and explore such as to maximize your reward?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this situation, Thompson sampling is a very well known sort of natural approach that translates to this setting.",
                    "label": 0
                },
                {
                    "sent": "It was basically a heuristic for a large time until a few years ago, so the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "So basically imagine that you have the space of all parameterized NDPS you try and just start with an artificial prior over it.",
                    "label": 0
                },
                {
                    "sent": "At each point we just sample.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An instance of an MDP from this prior, once you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "MVP just put all your trust into it.",
                    "label": 0
                },
                {
                    "sent": "That is, try and find plan the best policy for that MDP and just use it for a few instance of time.",
                    "label": 0
                },
                {
                    "sent": "Use whatever observations you get, update your posterior belief and then continue OK.",
                    "label": 0
                },
                {
                    "sent": "So just repeat this prescription right so the question is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This seems touristic, but does this really give this really work?",
                    "label": 0
                },
                {
                    "sent": "Well, it doesn't really give you the right balance of exploration and exploitation to try and solve online reinforcement learning problems, which might be complicated.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually show that the answer is yes, we actually can get problem dependent regret bound that basically grows as essentially see times locked your constant times locked.",
                    "label": 0
                },
                {
                    "sent": "If the number of intervals that you played this game and most importantly the constant E, The scaling of the regret in fact captures the exact structure of the parameterization in complicated but explicitly characterizable way and this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has the implication that you can actually accelerate the rate of learning and regret minimization drastically if you have much more structure in the MD.",
                    "label": 0
                },
                {
                    "sent": "So thanks.",
                    "label": 0
                }
            ]
        }
    }
}