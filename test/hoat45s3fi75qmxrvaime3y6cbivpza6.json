{
    "id": "hoat45s3fi75qmxrvaime3y6cbivpza6",
    "title": "Bounds on individual risk for log-loss predictors",
    "info": {
        "author": [
            "Peter Gr\u00fcnwald, Centrum Wiskunde & Informatica (CWI)"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_grunwald_bounds/",
    "segmentation": [
        [
            "Hi everybody, it's good to be back at quote.",
            "This is a joint question."
        ],
        [
            "With Voytek Kottlowski who's Mr Open problem because he will also present the next open problem.",
            "Um, so why tech is part of my group at CWI, and."
        ],
        [
            "As some of you know, we're working both in machine learning and statistics, which is usually a way of getting publications rather easily.",
            "Because you transfer, you have a result in statistics and you transferred to learning or vice versa, but sometimes it doesn't work out very well and you think you have a very nice result and you send it to a statistics Journal, but they say no because you have a nice algorithm, you know it's a learning algorithm, they call it an estimator, you get data, it outputs a hypothesis, but you bound the cumulative lock loss of the algorithm.",
            "We don't want that or it's meaningless we want.",
            "The cube and the individual risk, and then you cannot publish your paper.",
            "And actually it gets.",
            "This is hard, but usually I combined with it."
        ],
        [
            "Actually gets worse because we also do a lot of information theory and then we have to sometimes do some really impossible juggling around to get people to understand what we're even talking about.",
            "But sometimes these questions are just annoying, and sometimes if you think about them, they're actually rather deep and not stupid at all, and actually much harder than they seem, so I'll talk about one of those questions here, which is about the quantity of interest when you learn something."
        ],
        [
            "From data, so in statistics, this is often the so called individual risk.",
            "I'll explain in a moment what that is.",
            "It's a notion for a fixed sample size.",
            "Your learning algorithm outputs some hypothesis and you want to see how well is it for a fixed sample size.",
            "In machine learning.",
            "These now I'm talking about prediction with expert advice kind of things you'll often bound regrets in a worst case or something, but it's usually communitive like what is the sum of the regrets over everything you've done so far.",
            "And information.",
            "Siri, you're usually interested in the redundancy, which is also a communitive notion, and what we're interested in is with the tools that have been developed in information theory and in the cold community we can often give nice bounds in very easy manners on cumulative cumulative log.",
            "Lawson on redundancy.",
            "And intuitively, this should transfer to individual risks, which is what the statisticians like, but it's not at all easy to show that they do."
        ],
        [
            "So let me define individual risk, so I'll simplify things here.",
            "Assume data are IID according to some distribution remains interesting if they're not, but I'll talk about ID, and I'll also be a statistician.",
            "I'll assume that there is a model, a set of distributions, and that the data is actually sampled from an element of that set.",
            "So this is a model, but it can be very large.",
            "It could be the set of all computable distributions, or some huge nonparametric set of distributions.",
            "But we assume that we know the model, so we know a set from which the distribution comes.",
            "Now learning algorithm which is very similar to what statisticians call an estimator, is something which takes input a finite sequence of data and it outputs a distribution which is viewed as an estimate of the true distribution.",
            "P star.",
            "Now know that in general the output.",
            "So the output of the learning algorithm doesn't have to be in the model M. It often is, but for example, if you do Bayesian learning use Bayesian inference, then you can do the map maximum a posteriori distribution.",
            "Then you would have an output inside the model.",
            "But you could also.",
            "Use the base and predictive distribution as an estimator, so that's an average weighted average according to the posterior of all distributions in your model, so it's in a convex closure of your model rather than your model, right?",
            "So will.",
            "Look at those things.",
            "Also as learning algorithms or estimators."
        ],
        [
            "Now the risk is the quantity that statisticians are interested in and this is an usually defined, not as it was defined here today as an expectation relative to a loss function button expectation relative to a distance function, similar of course.",
            "So the risk, which depends on your learning algorithm, but I'll suppressed it in the notation and the true distribution at sample size.",
            "N is the expected distance between the thing your algorithm learned from the data and the true distribution, averaged over everything that happened in the past.",
            "And this D. So it also the definition depends on the distance you're interested in.",
            "This could be mean squared mean squared error between parameters.",
            "It could be hell, injure distance.",
            "That's what it often is, or KL divergent.",
            "And I'll look at KLF versions here."
        ],
        [
            "So for a good learning algorithm, we would want to prove that the risk converges to zero uniformly in N. Something like that.",
            "That would means that it works well because it's the expected distance of what it learns from the data to the true distribution is small.",
            "So how is this related to the information theoretic redundancy?",
            "So first I'll define the risk for a specific distance function, so note."
        ],
        [
            "We had the expectation overall previous data of a distance between true distribution and the estimate for the case of the KL Divergent.",
            "This distance is itself an expectation on the next outcome, so it can itself be written as an expectation over the end outcome.",
            "Of the minus logarithm of the probability you assign to the next outcome with the distribution you learn from the past and the true distribution of the next out."
        ],
        [
            "Come.",
            "Now the redundancy in information theory, which will be noted by Capital R, is defined as the expectation overall sequences of length M of the extra number.",
            "The additional number of bits you need to quote the data.",
            "If you code them using your learning algorithm compared to the code, that would be optimal among all quotes in your model set of distributions.",
            "You can also think of it as inducing a set of codes.",
            "So in terms of low class, this means you use what you learn from your data iteratively to predict the next outcome.",
            "You record the log loss and you look at the expected difference between your predictions loss and the loss you get if you use the best distribution in your model.",
            "So now the output is not really thought of as an estimate anymore, but rather as a prediction, but it's the same kind of thing."
        ],
        [
            "So, um, since we're looking at the case where the true distribution is in the model, the best predictions can be obtained if you use the true distr."
        ],
        [
            "Bution soapie stars Q and then we get as it's very easy to show from these two formulas that the individual statistical risk, which is what statisticians are interested in.",
            "Is the same thing if you sum it from one to N, you get the information theoretic redundancy which is the expected log loss you have Additionally over if you would use the true distribution for predicting.",
            "Right, so in this case there's a very simple relation between the redundancy and the individual risk.",
            "The one is the sum of the other and we are good at bounding the sum and the statisticians want to have a bound on each of the terms in the sum.",
            "So can we use?",
            "That's our question in for."
        ],
        [
            "We stated can we use the balance in the sum to get good bounds on the individual things?",
            "So here I'll only ask this probation learning algorithms since for that it's really easy to prove great cumulative bounds.",
            "And with an algorithm with a proof method."
        ],
        [
            "Most of us know presumably.",
            "So suppose we have a countable model.",
            "Again, it can be extremely big.",
            "Say all computable distributions, and we have some prior on it.",
            "So then the Bayesian predictions are defined as this probability according to the base mixture of the full data, including the next outcome divided by the date of the previous outcome."
        ],
        [
            "And then we can evaluate the total loss we make if we predict sequentially by base.",
            "So now this is not an expectation, but on arbitrary sequences, in the worst case, it's some of the minus logarithm minus log of the product, but the product if you look at the definition of the conditional basion predictive distributions, telescopes, it's one large factor and everything on the diagonal cancels.",
            "So you get that the sum of the logarithmic prediction errors is just minus lock.",
            "The probability that base assigns to the full data sequence.",
            "By definition it's minus log of the sum over all the priors of the P of the probability of P of the data sum is larger than each of its terms.",
            "So minus log of sum is smaller than each of its terms, so it's smaller than minus lock.",
            "The prior you assign to the true distribution times the probability the true distribution gives to the data.",
            "So now if you look at the two formulas in blue, you see that the total logarithmic loss is bounded by the logarithmic loss you get if you sequentially use the best distribution.",
            "The true distribution minus the lock prior there off, and this holds for every sequence in particular.",
            "It also is if you take expectations and if you then move the minus log probability of the true distribution to the left."
        ],
        [
            "You get that the cumulative risk which is the sum of the individual risks is bounded by minus lock.",
            "The prior assigned to the true distribution.",
            "That's the basic cold proof technique for proving finite regret bounds, but here it's not regret it's redundancy.",
            "So we know for countable sets that we have this property.",
            "The regrets if we have accountable model, the data are sampled from one of the distributions in the.",
            "The total redundancy will remain finite no matter how large N is, since every individual term in the sum, every individual risk must be positive.",
            "And we have that even if N goes to Infinity, the total cumulative risk remains bounded.",
            "We can of course show that in the limit, the individual risk must become zero, so we say fixed we have a bound on a cumulative risk, it's finite, so we can show that individual risk goes to 0, but we have no clue at all in what way it goes to zero.",
            "It could go to zero immediately, or it could be 0 four time and then jump up and then jump down again and be 0 again.",
            "What happens and the statisticians want us to say something about finite N?",
            "About the small aravan for finite an."
        ],
        [
            "Something known about as well.",
            "There's one result by Tongxiang, who shows that if you look at the risk in terms of hell injure squared distance rather than KL, Divergent sits always bounded by this minus log prior divided by N. Now if all distributions in your model are uniformly bounded in terms of their densities, so the ratio of the densities is bounded away from Infinity and zero, then abound on Hollinger transfers into abound on the KL diversions.",
            "So then that bound on the KL divergent is the risk we are interested in becomes smaller, equal and constant times minus locria divided by N. So that seems nice.",
            "But this is probably very not tight.",
            "Why will consider the case where there are is just a finite number of distributions.",
            "If you have a finite number of distributions, you can use the Uniform law of large numbers and then you can show that the log likelihoods of the data according to the different distributions, they will become different by a linear amount, which means that the base and posterior will give exponentially small posterior probability because it's E to the minus log likelihood.",
            "To all distributions except the good one, and then you will actually get an exponential convergence rate.",
            "It's about it's problem dependent.",
            "Yeah, yeah?",
            "Yeah, of course.",
            "But So what I'm wondering is, are there weak conditions you can come up with for which you can get something better than this?",
            "Exponential would be too much to ask for, but I have some inclination that under some conditions you can do better than.",
            "Something divided by N will see in a moment you will see why."
        ],
        [
            "So there's another known result, which is if you get if you don't use base but the cisero average.",
            "It's also called progressive mixture rule, so this is a method that many people use instead of base because it's easier to prove things about it, but in reality you probably wouldn't want to use it because what you do there is you average overall basing predictions done so far.",
            "And that's probably not something which works very well in practice.",
            "You get the same bound.",
            "So the first question is, can we get something better than minus look prior divided by N and I'll hand out a price which depends on the exponent.",
            "You get into an.",
            "If you can solve this at all an asset, it's probably you need some extra conditions in order to get something."
        ],
        [
            "Enter.",
            "So second and final question, what about parametric families?",
            "So suppose we have an exponential family, so this could be like a Bayesian network with K parameters and we sample from it ID.",
            "Then the scale risk the cumulative risk.",
            "The redundancy satisfies.",
            "There's a famous formula in information theory and minimum description length for that.",
            "It's equal to K / 2 log N, where K is the number of free parameters in your model.",
            "Plus some constant involving Fisher information prior density you put on the true distribution.",
            "I won't bore you with the details, plus something of order 1 / N. So this suggests that the individual risk should be K / 2 M plus something much smaller, because if you sum K / 2 N you get something of the order K / 2 Logn times constant, right?",
            "And for regression, the KL divergences equal to a squared type of distance and then you can actually work this out precisely without asymptotics.",
            "And then that you actually get K / 2 two N precisely.",
            "So you would expect that it's always K over."
        ],
        [
            "UN but can you prove this well for one?"
        ],
        [
            "Dimensional models you can do a Taylor expansion and then you find it's very involved Taylor expansion.",
            "You actually find that this constant comes in 1 / N ^2 term, which is surprisingly small.",
            "Actually it's not.",
            "There's nothing between one of random 1 / N ^2 and then you get one over into the third.",
            "At term, So what do you do for K dimension?"
        ],
        [
            "All models.",
            "So we conjecture that it's always K / 2 N plus something small, and for the applications we have in mind, we actually need some explicit bounds on the smaller of one, and if we do this for K larger than one with the Taylor expansion, we will not get these explicit bounds.",
            "Moreover, we get a horrible Taylor expansions which involve tensors and all kinds of things.",
            "There's actually a paper I think they do this, but I can't even read it.",
            "So and again it has all kinds of higher order terms, which are.",
            "It's not clear how large they are.",
            "Only that they go to 0."
        ],
        [
            "So more interesting.",
            "Possible approach would be if we could show that the individual risk is decreasing.",
            "So in terms which I only recently heard about, this would mean that base is smart in some sense.",
            "If you use a base unit."
        ],
        [
            "Learning algorithm here.",
            "The intuition here is that if you could show that you know, we know that this sum, that's the redundancy, the sum of the individual risks, is bounded by K. / 2 log N plus something came over to Logan minus something, and we can even evaluate that something rather easily.",
            "So if we can show that the individual terms are all positive and non increasing, then it's rather easy to show that they actually know it's gone.",
            "How do I get back?",
            "That the individual terms must be on the order K / 2.",
            "And basically this is like the sum.",
            "Behaves like a continuous function of N, and so you just can take the derivative to get the terms of the sum that's more or less what happens here.",
            "So if we could show that this thing is not increasing, which would be interesting in its own right, we would have solved the problem."
        ],
        [
            "The problem is in general it is.",
            "It can actually increase.",
            "So even this is very simple observation due to Andrew Barron.",
            "You have a Bernoulli model.",
            "Biased coin flips, you put a uniform prior on them, and you do the basin predictions.",
            "And suppose the data are generated by independent fair coin flips.",
            "So then your first prediction will have zero risk.",
            "It will be perfect because your priors uniform overall theater probability one between zero and one, and because that symmetric around 1/2 your first prediction will be to say probably why is one is 1/2, which is the correct prediction.",
            "Your risk will be 0.",
            "After having seen one outcome, if it's a one, you will say that the probability that the next outcome is one is 2/3 and otherwise you will say it's in 1/3.",
            "In both cases, your risk is about 0.64, so your risk goes up.",
            "But this seems rather to have something to do with the discreteness of the data, then something fundamental."
        ],
        [
            "So the question is maybe if you are a bit more careful, maybe you can show that base is almost smart that you that the risk goes down fast enough or that some good tight upper bound on the risk is always decreasing and that would also be good enough.",
            "So the second question is can we show for parametric models that if you use Bayesian prediction, let's say with a uniform prior that it's smart that the risk goes down more or less monotonically.",
            "Look at the case where it's not just a 0 prediction, which is good, but the next prediction is still.",
            "I think it's possible, but I get your.",
            "I have to start doing pretty complicated things so it all suggests that this is an artifact that it is not something inherent in, but I think you can."
        ],
        [
            "So, um, as a final remark, even if you're only interested in cumulative risk results, this saying things about the individual risk can sometimes be important.",
            "We now have a paper where this is actually exactly, that's why I got this question is a few years old.",
            "I got interested in it again because I needed this result to say something about cumulative risk for very large nonparametric models.",
            "Um, so Monday I'll talk about the situation when the true distribution is not in the model, and then it gets a lot hairier.",
            "But maybe more interesting.",
            "And of course this is not just interesting for Bashan predictors.",
            "You are learning algorithms, you can ask the same question for any learning algorithm you like, and it remains an interesting question.",
            "Thank you.",
            "Any yes?",
            "Open question out to online papers.",
            "Sorry online.",
            "Is it?",
            "Yes, that's what you're actually asking how to do.",
            "Online device conversion for your particular loss function?",
            "Yes, I think you could see it.",
            "It's more or less the same problem.",
            "Yeah, but, but particularly for the log loss then.",
            "Divorce is this is.",
            "Folklore result, so to say.",
            "Well, I don't know what what focal result you're referring to, but even if everything is so even, if all densities are bounded, then the local loss becomes bounded.",
            "We don't want to modify the algorithms right?",
            "We want to prove.",
            "We want to prove what the original algorithm does in terms of individual risk, so we don't do cisero averaging or things like that, right?",
            "So there's no modification of the original algorithm, and that I think makes it a lot harder.",
            "That seems like an article restriction.",
            "Well, ask the referees of my statistics paper.",
            "Well, they say that we don't understand.",
            "But apart from that, let me say one more thing about it, because this is about this, so I'm now actually we've been working on a new kind of model selection method where you basically you mix between a whole lot of parametric models to get some sophisticated predictor.",
            "And there are actually even to prove bounds on the cumulative loss.",
            "You need to have the property that for the individual models you look at, the risks are nonincreasing.",
            "So I would need 15 more minutes to explain why, but so this decreasing property of the risk to prove that would be really nice.",
            "And So what we now do, we now have a kind of really ugly way of solving that.",
            "We allow our learners to freeze their predictions.",
            "And then we mix over a frozen and unfrozen version of the algorithm.",
            "So the frozen version stops learning at some point and it makes it all very inelegant.",
            "So you would really like to know is this really necessary to stop learning at some point and do things like that?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everybody, it's good to be back at quote.",
                    "label": 0
                },
                {
                    "sent": "This is a joint question.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With Voytek Kottlowski who's Mr Open problem because he will also present the next open problem.",
                    "label": 0
                },
                {
                    "sent": "Um, so why tech is part of my group at CWI, and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As some of you know, we're working both in machine learning and statistics, which is usually a way of getting publications rather easily.",
                    "label": 1
                },
                {
                    "sent": "Because you transfer, you have a result in statistics and you transferred to learning or vice versa, but sometimes it doesn't work out very well and you think you have a very nice result and you send it to a statistics Journal, but they say no because you have a nice algorithm, you know it's a learning algorithm, they call it an estimator, you get data, it outputs a hypothesis, but you bound the cumulative lock loss of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "We don't want that or it's meaningless we want.",
                    "label": 0
                },
                {
                    "sent": "The cube and the individual risk, and then you cannot publish your paper.",
                    "label": 0
                },
                {
                    "sent": "And actually it gets.",
                    "label": 0
                },
                {
                    "sent": "This is hard, but usually I combined with it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually gets worse because we also do a lot of information theory and then we have to sometimes do some really impossible juggling around to get people to understand what we're even talking about.",
                    "label": 0
                },
                {
                    "sent": "But sometimes these questions are just annoying, and sometimes if you think about them, they're actually rather deep and not stupid at all, and actually much harder than they seem, so I'll talk about one of those questions here, which is about the quantity of interest when you learn something.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From data, so in statistics, this is often the so called individual risk.",
                    "label": 1
                },
                {
                    "sent": "I'll explain in a moment what that is.",
                    "label": 0
                },
                {
                    "sent": "It's a notion for a fixed sample size.",
                    "label": 0
                },
                {
                    "sent": "Your learning algorithm outputs some hypothesis and you want to see how well is it for a fixed sample size.",
                    "label": 0
                },
                {
                    "sent": "In machine learning.",
                    "label": 0
                },
                {
                    "sent": "These now I'm talking about prediction with expert advice kind of things you'll often bound regrets in a worst case or something, but it's usually communitive like what is the sum of the regrets over everything you've done so far.",
                    "label": 0
                },
                {
                    "sent": "And information.",
                    "label": 0
                },
                {
                    "sent": "Siri, you're usually interested in the redundancy, which is also a communitive notion, and what we're interested in is with the tools that have been developed in information theory and in the cold community we can often give nice bounds in very easy manners on cumulative cumulative log.",
                    "label": 0
                },
                {
                    "sent": "Lawson on redundancy.",
                    "label": 0
                },
                {
                    "sent": "And intuitively, this should transfer to individual risks, which is what the statisticians like, but it's not at all easy to show that they do.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me define individual risk, so I'll simplify things here.",
                    "label": 1
                },
                {
                    "sent": "Assume data are IID according to some distribution remains interesting if they're not, but I'll talk about ID, and I'll also be a statistician.",
                    "label": 0
                },
                {
                    "sent": "I'll assume that there is a model, a set of distributions, and that the data is actually sampled from an element of that set.",
                    "label": 1
                },
                {
                    "sent": "So this is a model, but it can be very large.",
                    "label": 0
                },
                {
                    "sent": "It could be the set of all computable distributions, or some huge nonparametric set of distributions.",
                    "label": 0
                },
                {
                    "sent": "But we assume that we know the model, so we know a set from which the distribution comes.",
                    "label": 0
                },
                {
                    "sent": "Now learning algorithm which is very similar to what statisticians call an estimator, is something which takes input a finite sequence of data and it outputs a distribution which is viewed as an estimate of the true distribution.",
                    "label": 0
                },
                {
                    "sent": "P star.",
                    "label": 0
                },
                {
                    "sent": "Now know that in general the output.",
                    "label": 0
                },
                {
                    "sent": "So the output of the learning algorithm doesn't have to be in the model M. It often is, but for example, if you do Bayesian learning use Bayesian inference, then you can do the map maximum a posteriori distribution.",
                    "label": 0
                },
                {
                    "sent": "Then you would have an output inside the model.",
                    "label": 0
                },
                {
                    "sent": "But you could also.",
                    "label": 0
                },
                {
                    "sent": "Use the base and predictive distribution as an estimator, so that's an average weighted average according to the posterior of all distributions in your model, so it's in a convex closure of your model rather than your model, right?",
                    "label": 0
                },
                {
                    "sent": "So will.",
                    "label": 0
                },
                {
                    "sent": "Look at those things.",
                    "label": 0
                },
                {
                    "sent": "Also as learning algorithms or estimators.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the risk is the quantity that statisticians are interested in and this is an usually defined, not as it was defined here today as an expectation relative to a loss function button expectation relative to a distance function, similar of course.",
                    "label": 0
                },
                {
                    "sent": "So the risk, which depends on your learning algorithm, but I'll suppressed it in the notation and the true distribution at sample size.",
                    "label": 0
                },
                {
                    "sent": "N is the expected distance between the thing your algorithm learned from the data and the true distribution, averaged over everything that happened in the past.",
                    "label": 0
                },
                {
                    "sent": "And this D. So it also the definition depends on the distance you're interested in.",
                    "label": 0
                },
                {
                    "sent": "This could be mean squared mean squared error between parameters.",
                    "label": 0
                },
                {
                    "sent": "It could be hell, injure distance.",
                    "label": 0
                },
                {
                    "sent": "That's what it often is, or KL divergent.",
                    "label": 0
                },
                {
                    "sent": "And I'll look at KLF versions here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for a good learning algorithm, we would want to prove that the risk converges to zero uniformly in N. Something like that.",
                    "label": 1
                },
                {
                    "sent": "That would means that it works well because it's the expected distance of what it learns from the data to the true distribution is small.",
                    "label": 0
                },
                {
                    "sent": "So how is this related to the information theoretic redundancy?",
                    "label": 0
                },
                {
                    "sent": "So first I'll define the risk for a specific distance function, so note.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had the expectation overall previous data of a distance between true distribution and the estimate for the case of the KL Divergent.",
                    "label": 0
                },
                {
                    "sent": "This distance is itself an expectation on the next outcome, so it can itself be written as an expectation over the end outcome.",
                    "label": 0
                },
                {
                    "sent": "Of the minus logarithm of the probability you assign to the next outcome with the distribution you learn from the past and the true distribution of the next out.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come.",
                    "label": 0
                },
                {
                    "sent": "Now the redundancy in information theory, which will be noted by Capital R, is defined as the expectation overall sequences of length M of the extra number.",
                    "label": 1
                },
                {
                    "sent": "The additional number of bits you need to quote the data.",
                    "label": 0
                },
                {
                    "sent": "If you code them using your learning algorithm compared to the code, that would be optimal among all quotes in your model set of distributions.",
                    "label": 0
                },
                {
                    "sent": "You can also think of it as inducing a set of codes.",
                    "label": 0
                },
                {
                    "sent": "So in terms of low class, this means you use what you learn from your data iteratively to predict the next outcome.",
                    "label": 0
                },
                {
                    "sent": "You record the log loss and you look at the expected difference between your predictions loss and the loss you get if you use the best distribution in your model.",
                    "label": 1
                },
                {
                    "sent": "So now the output is not really thought of as an estimate anymore, but rather as a prediction, but it's the same kind of thing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um, since we're looking at the case where the true distribution is in the model, the best predictions can be obtained if you use the true distr.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bution soapie stars Q and then we get as it's very easy to show from these two formulas that the individual statistical risk, which is what statisticians are interested in.",
                    "label": 0
                },
                {
                    "sent": "Is the same thing if you sum it from one to N, you get the information theoretic redundancy which is the expected log loss you have Additionally over if you would use the true distribution for predicting.",
                    "label": 0
                },
                {
                    "sent": "Right, so in this case there's a very simple relation between the redundancy and the individual risk.",
                    "label": 0
                },
                {
                    "sent": "The one is the sum of the other and we are good at bounding the sum and the statisticians want to have a bound on each of the terms in the sum.",
                    "label": 0
                },
                {
                    "sent": "So can we use?",
                    "label": 0
                },
                {
                    "sent": "That's our question in for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We stated can we use the balance in the sum to get good bounds on the individual things?",
                    "label": 0
                },
                {
                    "sent": "So here I'll only ask this probation learning algorithms since for that it's really easy to prove great cumulative bounds.",
                    "label": 1
                },
                {
                    "sent": "And with an algorithm with a proof method.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most of us know presumably.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have a countable model.",
                    "label": 0
                },
                {
                    "sent": "Again, it can be extremely big.",
                    "label": 0
                },
                {
                    "sent": "Say all computable distributions, and we have some prior on it.",
                    "label": 0
                },
                {
                    "sent": "So then the Bayesian predictions are defined as this probability according to the base mixture of the full data, including the next outcome divided by the date of the previous outcome.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can evaluate the total loss we make if we predict sequentially by base.",
                    "label": 0
                },
                {
                    "sent": "So now this is not an expectation, but on arbitrary sequences, in the worst case, it's some of the minus logarithm minus log of the product, but the product if you look at the definition of the conditional basion predictive distributions, telescopes, it's one large factor and everything on the diagonal cancels.",
                    "label": 0
                },
                {
                    "sent": "So you get that the sum of the logarithmic prediction errors is just minus lock.",
                    "label": 0
                },
                {
                    "sent": "The probability that base assigns to the full data sequence.",
                    "label": 0
                },
                {
                    "sent": "By definition it's minus log of the sum over all the priors of the P of the probability of P of the data sum is larger than each of its terms.",
                    "label": 0
                },
                {
                    "sent": "So minus log of sum is smaller than each of its terms, so it's smaller than minus lock.",
                    "label": 0
                },
                {
                    "sent": "The prior you assign to the true distribution times the probability the true distribution gives to the data.",
                    "label": 0
                },
                {
                    "sent": "So now if you look at the two formulas in blue, you see that the total logarithmic loss is bounded by the logarithmic loss you get if you sequentially use the best distribution.",
                    "label": 0
                },
                {
                    "sent": "The true distribution minus the lock prior there off, and this holds for every sequence in particular.",
                    "label": 0
                },
                {
                    "sent": "It also is if you take expectations and if you then move the minus log probability of the true distribution to the left.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You get that the cumulative risk which is the sum of the individual risks is bounded by minus lock.",
                    "label": 0
                },
                {
                    "sent": "The prior assigned to the true distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the basic cold proof technique for proving finite regret bounds, but here it's not regret it's redundancy.",
                    "label": 0
                },
                {
                    "sent": "So we know for countable sets that we have this property.",
                    "label": 0
                },
                {
                    "sent": "The regrets if we have accountable model, the data are sampled from one of the distributions in the.",
                    "label": 0
                },
                {
                    "sent": "The total redundancy will remain finite no matter how large N is, since every individual term in the sum, every individual risk must be positive.",
                    "label": 0
                },
                {
                    "sent": "And we have that even if N goes to Infinity, the total cumulative risk remains bounded.",
                    "label": 0
                },
                {
                    "sent": "We can of course show that in the limit, the individual risk must become zero, so we say fixed we have a bound on a cumulative risk, it's finite, so we can show that individual risk goes to 0, but we have no clue at all in what way it goes to zero.",
                    "label": 0
                },
                {
                    "sent": "It could go to zero immediately, or it could be 0 four time and then jump up and then jump down again and be 0 again.",
                    "label": 0
                },
                {
                    "sent": "What happens and the statisticians want us to say something about finite N?",
                    "label": 1
                },
                {
                    "sent": "About the small aravan for finite an.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something known about as well.",
                    "label": 0
                },
                {
                    "sent": "There's one result by Tongxiang, who shows that if you look at the risk in terms of hell injure squared distance rather than KL, Divergent sits always bounded by this minus log prior divided by N. Now if all distributions in your model are uniformly bounded in terms of their densities, so the ratio of the densities is bounded away from Infinity and zero, then abound on Hollinger transfers into abound on the KL diversions.",
                    "label": 0
                },
                {
                    "sent": "So then that bound on the KL divergent is the risk we are interested in becomes smaller, equal and constant times minus locria divided by N. So that seems nice.",
                    "label": 0
                },
                {
                    "sent": "But this is probably very not tight.",
                    "label": 0
                },
                {
                    "sent": "Why will consider the case where there are is just a finite number of distributions.",
                    "label": 0
                },
                {
                    "sent": "If you have a finite number of distributions, you can use the Uniform law of large numbers and then you can show that the log likelihoods of the data according to the different distributions, they will become different by a linear amount, which means that the base and posterior will give exponentially small posterior probability because it's E to the minus log likelihood.",
                    "label": 0
                },
                {
                    "sent": "To all distributions except the good one, and then you will actually get an exponential convergence rate.",
                    "label": 0
                },
                {
                    "sent": "It's about it's problem dependent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course.",
                    "label": 0
                },
                {
                    "sent": "But So what I'm wondering is, are there weak conditions you can come up with for which you can get something better than this?",
                    "label": 0
                },
                {
                    "sent": "Exponential would be too much to ask for, but I have some inclination that under some conditions you can do better than.",
                    "label": 0
                },
                {
                    "sent": "Something divided by N will see in a moment you will see why.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's another known result, which is if you get if you don't use base but the cisero average.",
                    "label": 1
                },
                {
                    "sent": "It's also called progressive mixture rule, so this is a method that many people use instead of base because it's easier to prove things about it, but in reality you probably wouldn't want to use it because what you do there is you average overall basing predictions done so far.",
                    "label": 1
                },
                {
                    "sent": "And that's probably not something which works very well in practice.",
                    "label": 0
                },
                {
                    "sent": "You get the same bound.",
                    "label": 0
                },
                {
                    "sent": "So the first question is, can we get something better than minus look prior divided by N and I'll hand out a price which depends on the exponent.",
                    "label": 0
                },
                {
                    "sent": "You get into an.",
                    "label": 0
                },
                {
                    "sent": "If you can solve this at all an asset, it's probably you need some extra conditions in order to get something.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "So second and final question, what about parametric families?",
                    "label": 0
                },
                {
                    "sent": "So suppose we have an exponential family, so this could be like a Bayesian network with K parameters and we sample from it ID.",
                    "label": 0
                },
                {
                    "sent": "Then the scale risk the cumulative risk.",
                    "label": 0
                },
                {
                    "sent": "The redundancy satisfies.",
                    "label": 0
                },
                {
                    "sent": "There's a famous formula in information theory and minimum description length for that.",
                    "label": 0
                },
                {
                    "sent": "It's equal to K / 2 log N, where K is the number of free parameters in your model.",
                    "label": 1
                },
                {
                    "sent": "Plus some constant involving Fisher information prior density you put on the true distribution.",
                    "label": 0
                },
                {
                    "sent": "I won't bore you with the details, plus something of order 1 / N. So this suggests that the individual risk should be K / 2 M plus something much smaller, because if you sum K / 2 N you get something of the order K / 2 Logn times constant, right?",
                    "label": 0
                },
                {
                    "sent": "And for regression, the KL divergences equal to a squared type of distance and then you can actually work this out precisely without asymptotics.",
                    "label": 0
                },
                {
                    "sent": "And then that you actually get K / 2 two N precisely.",
                    "label": 0
                },
                {
                    "sent": "So you would expect that it's always K over.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "UN but can you prove this well for one?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensional models you can do a Taylor expansion and then you find it's very involved Taylor expansion.",
                    "label": 0
                },
                {
                    "sent": "You actually find that this constant comes in 1 / N ^2 term, which is surprisingly small.",
                    "label": 0
                },
                {
                    "sent": "Actually it's not.",
                    "label": 0
                },
                {
                    "sent": "There's nothing between one of random 1 / N ^2 and then you get one over into the third.",
                    "label": 0
                },
                {
                    "sent": "At term, So what do you do for K dimension?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All models.",
                    "label": 0
                },
                {
                    "sent": "So we conjecture that it's always K / 2 N plus something small, and for the applications we have in mind, we actually need some explicit bounds on the smaller of one, and if we do this for K larger than one with the Taylor expansion, we will not get these explicit bounds.",
                    "label": 1
                },
                {
                    "sent": "Moreover, we get a horrible Taylor expansions which involve tensors and all kinds of things.",
                    "label": 0
                },
                {
                    "sent": "There's actually a paper I think they do this, but I can't even read it.",
                    "label": 0
                },
                {
                    "sent": "So and again it has all kinds of higher order terms, which are.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how large they are.",
                    "label": 0
                },
                {
                    "sent": "Only that they go to 0.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more interesting.",
                    "label": 0
                },
                {
                    "sent": "Possible approach would be if we could show that the individual risk is decreasing.",
                    "label": 1
                },
                {
                    "sent": "So in terms which I only recently heard about, this would mean that base is smart in some sense.",
                    "label": 0
                },
                {
                    "sent": "If you use a base unit.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning algorithm here.",
                    "label": 0
                },
                {
                    "sent": "The intuition here is that if you could show that you know, we know that this sum, that's the redundancy, the sum of the individual risks, is bounded by K. / 2 log N plus something came over to Logan minus something, and we can even evaluate that something rather easily.",
                    "label": 0
                },
                {
                    "sent": "So if we can show that the individual terms are all positive and non increasing, then it's rather easy to show that they actually know it's gone.",
                    "label": 1
                },
                {
                    "sent": "How do I get back?",
                    "label": 1
                },
                {
                    "sent": "That the individual terms must be on the order K / 2.",
                    "label": 0
                },
                {
                    "sent": "And basically this is like the sum.",
                    "label": 0
                },
                {
                    "sent": "Behaves like a continuous function of N, and so you just can take the derivative to get the terms of the sum that's more or less what happens here.",
                    "label": 0
                },
                {
                    "sent": "So if we could show that this thing is not increasing, which would be interesting in its own right, we would have solved the problem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is in general it is.",
                    "label": 0
                },
                {
                    "sent": "It can actually increase.",
                    "label": 0
                },
                {
                    "sent": "So even this is very simple observation due to Andrew Barron.",
                    "label": 0
                },
                {
                    "sent": "You have a Bernoulli model.",
                    "label": 1
                },
                {
                    "sent": "Biased coin flips, you put a uniform prior on them, and you do the basin predictions.",
                    "label": 1
                },
                {
                    "sent": "And suppose the data are generated by independent fair coin flips.",
                    "label": 0
                },
                {
                    "sent": "So then your first prediction will have zero risk.",
                    "label": 0
                },
                {
                    "sent": "It will be perfect because your priors uniform overall theater probability one between zero and one, and because that symmetric around 1/2 your first prediction will be to say probably why is one is 1/2, which is the correct prediction.",
                    "label": 0
                },
                {
                    "sent": "Your risk will be 0.",
                    "label": 0
                },
                {
                    "sent": "After having seen one outcome, if it's a one, you will say that the probability that the next outcome is one is 2/3 and otherwise you will say it's in 1/3.",
                    "label": 0
                },
                {
                    "sent": "In both cases, your risk is about 0.64, so your risk goes up.",
                    "label": 0
                },
                {
                    "sent": "But this seems rather to have something to do with the discreteness of the data, then something fundamental.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is maybe if you are a bit more careful, maybe you can show that base is almost smart that you that the risk goes down fast enough or that some good tight upper bound on the risk is always decreasing and that would also be good enough.",
                    "label": 1
                },
                {
                    "sent": "So the second question is can we show for parametric models that if you use Bayesian prediction, let's say with a uniform prior that it's smart that the risk goes down more or less monotonically.",
                    "label": 0
                },
                {
                    "sent": "Look at the case where it's not just a 0 prediction, which is good, but the next prediction is still.",
                    "label": 0
                },
                {
                    "sent": "I think it's possible, but I get your.",
                    "label": 0
                },
                {
                    "sent": "I have to start doing pretty complicated things so it all suggests that this is an artifact that it is not something inherent in, but I think you can.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um, as a final remark, even if you're only interested in cumulative risk results, this saying things about the individual risk can sometimes be important.",
                    "label": 1
                },
                {
                    "sent": "We now have a paper where this is actually exactly, that's why I got this question is a few years old.",
                    "label": 0
                },
                {
                    "sent": "I got interested in it again because I needed this result to say something about cumulative risk for very large nonparametric models.",
                    "label": 1
                },
                {
                    "sent": "Um, so Monday I'll talk about the situation when the true distribution is not in the model, and then it gets a lot hairier.",
                    "label": 0
                },
                {
                    "sent": "But maybe more interesting.",
                    "label": 0
                },
                {
                    "sent": "And of course this is not just interesting for Bashan predictors.",
                    "label": 0
                },
                {
                    "sent": "You are learning algorithms, you can ask the same question for any learning algorithm you like, and it remains an interesting question.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Any yes?",
                    "label": 0
                },
                {
                    "sent": "Open question out to online papers.",
                    "label": 0
                },
                {
                    "sent": "Sorry online.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Yes, that's what you're actually asking how to do.",
                    "label": 0
                },
                {
                    "sent": "Online device conversion for your particular loss function?",
                    "label": 0
                },
                {
                    "sent": "Yes, I think you could see it.",
                    "label": 0
                },
                {
                    "sent": "It's more or less the same problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but, but particularly for the log loss then.",
                    "label": 0
                },
                {
                    "sent": "Divorce is this is.",
                    "label": 0
                },
                {
                    "sent": "Folklore result, so to say.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know what what focal result you're referring to, but even if everything is so even, if all densities are bounded, then the local loss becomes bounded.",
                    "label": 0
                },
                {
                    "sent": "We don't want to modify the algorithms right?",
                    "label": 0
                },
                {
                    "sent": "We want to prove.",
                    "label": 0
                },
                {
                    "sent": "We want to prove what the original algorithm does in terms of individual risk, so we don't do cisero averaging or things like that, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no modification of the original algorithm, and that I think makes it a lot harder.",
                    "label": 0
                },
                {
                    "sent": "That seems like an article restriction.",
                    "label": 0
                },
                {
                    "sent": "Well, ask the referees of my statistics paper.",
                    "label": 0
                },
                {
                    "sent": "Well, they say that we don't understand.",
                    "label": 0
                },
                {
                    "sent": "But apart from that, let me say one more thing about it, because this is about this, so I'm now actually we've been working on a new kind of model selection method where you basically you mix between a whole lot of parametric models to get some sophisticated predictor.",
                    "label": 0
                },
                {
                    "sent": "And there are actually even to prove bounds on the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "You need to have the property that for the individual models you look at, the risks are nonincreasing.",
                    "label": 0
                },
                {
                    "sent": "So I would need 15 more minutes to explain why, but so this decreasing property of the risk to prove that would be really nice.",
                    "label": 0
                },
                {
                    "sent": "And So what we now do, we now have a kind of really ugly way of solving that.",
                    "label": 0
                },
                {
                    "sent": "We allow our learners to freeze their predictions.",
                    "label": 0
                },
                {
                    "sent": "And then we mix over a frozen and unfrozen version of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the frozen version stops learning at some point and it makes it all very inelegant.",
                    "label": 0
                },
                {
                    "sent": "So you would really like to know is this really necessary to stop learning at some point and do things like that?",
                    "label": 0
                }
            ]
        }
    }
}