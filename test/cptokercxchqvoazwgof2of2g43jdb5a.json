{
    "id": "cptokercxchqvoazwgof2of2g43jdb5a",
    "title": "Semidefinite ranking on graphs",
    "info": {
        "author": [
            "Shankar Vembu, Department of Computer Science, University of Illinois at Urbana-Champaign"
        ],
        "published": "Sept. 7, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/sicgt07_vembu_srog/",
    "segmentation": [
        [
            "OK, so ready to roll.",
            "It's the next speaker is Shane Carden.",
            "Frankie, thanks.",
            "Alright, I'm a PhD student at the University of one and also at the front of the Institute in something else, and I'm going to talk about a semi definite formulation for ranking the nodes of an interactive graph given some additional information in the form of reference constraint and this is joint work with Thomas Kiernan and Steven Weber, who are my PhD advisors.",
            "Thomas is here with us.",
            "OK."
        ],
        [
            "Right, so this is the outline of it all.",
            "I'll first describe the problem setting and then I'll talk about and provide some motivation behind our approaches.",
            "After that, I'll talk about a quadratic programming relaxation for the same problem that was proposed last year at ICL, and after that I'll talk about a semi definite formulation for the same problem, which is the main contribution of our work, and then I'll provide some empirical."
        ],
        [
            "It's at the end.",
            "OK, so what's the problem?",
            "So we are given two inputs here.",
            "One is an underrated graph and the other one is a directed graph.",
            "Pretty simple example, the web search, so the nodes of the undirected graph are the web pages, right?",
            "And the linkage information in the under graph is the hyperlinks between your web pages.",
            "What do we have in this static trap in this traffic graph?",
            "We have some preference constraints, so if you're trying to learn a ranking function for a search engine, you would need some kind of need to provide some kind of training information to the learner right?",
            "And this is what you have in the directed graph, so it actually contains information as such.",
            "Pairwise comparison.",
            "So you would say that I prefer web page 8 with PHP and then you have a directed edge from A to B.",
            "So this information is there in that row.",
            "So what do we need at the output?",
            "I'm basically looking for a bijection from the set of vertices to the set of integers from one to N. OK, so which is basically a permutation and this permutation should satisfy two things.",
            "We need to have as few backward edges as possible in there with respect to the directed graph.",
            "And apart from that I also need to make sure that the.",
            "Function that I'm trying to learn so should be smooth with respect to the undirected graph, right?",
            "So this is the problem setting and let's."
        ],
        [
            "Let's see how we could solve this problem.",
            "It actually boils down to this following optimization problem.",
            "So I've got two terms in my optimization problem.",
            "The first term takes care of your preference constraints.",
            "Alright, and the second term is your regularizer.",
            "OK, so that is step function over there Sigma.",
            "So if I minimize these two summons simultaneously, I can make sure that the final permutation has as few backward edges as possible in the respect to the directed graph.",
            "And also I can ensure that the function that I the permutation is smooth with respect to the undirected graph.",
            "OK."
        ],
        [
            "Alright, let's see how we could solve this problem.",
            "The motivation, I mean the motivation actually comes from the vertex ordering algorithms by the so-called layout problems from graph theory.",
            "So this ranking on graphs problem is very much related to the vertex ordering problems that you would find in the literature.",
            "I've just listed three of them here.",
            "For example, you could at the top you have the minimum bandwidth where you just try to minimize the maximum of the dilation.",
            "The dilation is just the length of the edge with respect to the permutation, and you could also have a different form of cost function which is called the minimum linear arrangement where you just try to minimize over the sum of the directions.",
            "Or you could also minimize over the sum of the square dilations, which is the minimum length modern problem.",
            "So this this layer problems are I see them as different flavors of regularization from a machine learning perspective.",
            "All these problems are NP hard, but of course approximation algorithms do exist."
        ],
        [
            "OK, what's the idea?",
            "Now the idea is we could generalize one of these vertex ordering problems an make it suitable for our problem at hand.",
            "But there is one problem.",
            "The vertex ordering algorithms are unsupervised, so to say you only have the regularizer here, right?"
        ],
        [
            "No, here you only have the regularizer there, but there is no way to incorporate the preference constraints.",
            "That is which is appears in the first term here."
        ],
        [
            "So the question is how to incorporate preference constraints into this algorithms so the main contribution of our work is to modify the minimum length ordering algorithm which was proposed by our emblem and colleagues in the late 90s to handle preference constraints."
        ],
        [
            "OK, the motivation also comes from a different angle.",
            "And in recent years in the machine learning literature, you will find that.",
            "People have come up with SGP formulations for different learning tasks like for example in classification we have this spectral graph transducer right which is.",
            "Quadratic programming problem and on the other hand you also have an SDP formulation for the same problem.",
            "So till maybe he comes up with a family of history relaxations for classification problems.",
            "So all these algorithms, such as transductive in nature.",
            "For the classification and in classroom you have this normalized cuts algorithm, which basically takes the spectrum of yellow pricing to compute their cut.",
            "And on the other hand, you also have an SDP formulation for the same problem.",
            "And there they show that the SDP relaxation works better than this.",
            "Better relaxation for ranking last year.",
            "I CML they proposed a quadratic programming formation.",
            "I would not call it a spectral ranking IT security problem.",
            "And so in this work we investigate and history formulation for the same problem."
        ],
        [
            "OK. Alright, so before I get into the details of our algorithm, let's let's make a brief detour on graph based clustering.",
            "I'll see I'll show you how any stabilisation works better than spectral relaxation for clustering algorithms.",
            "OK, let's talk about the optimal ratio ratio code for this so called the Congress graph for the ladder network graph.",
            "The optimal ratio that is shown over there, so I just need a horizontal sorry the vertical cut.",
            "OK so this is balanced and the number of edges in the cut is too.",
            "If you take the spectral relaxation to solve this problem."
        ],
        [
            "Last week you get something like this.",
            "OK, so this is.",
            "This cut is also balanced, but the number of edges and that is for.",
            "If you."
        ],
        [
            "Take the stupid relaxation.",
            "I'm sorry so these are the two clusters that you get.",
            "OK, one of the top and one in the upper.",
            "But if I take it, it's cheap."
        ],
        [
            "Relaxation to the same problem.",
            "I'm hurrying up.",
            "OK, so this is the length of the artery.",
            "OK, so from from the eigenvector from the Fiedler vector you could actually get an ordering and then if you compute the minimum length ordering cost you get this 184.",
            "And the ordering is something like this one.",
            "210 you get at the top and then 11:20 you get the gate at the bottom.",
            "So it's oriented in that way.",
            "So this is what spectral clustering does."
        ],
        [
            "OK, now let's see what SCP clustering is.",
            "Stupid relaxation for the same problem.",
            "Best so STP gives me the exact count that I want.",
            "This is this is the ratio cut given by the SDP relaxation OK, it's balanced again and the number of edges and that is only two."
        ],
        [
            "And you get these two clusters.",
            "Intuitively, I would expect these two clusters, right?"
        ],
        [
            "And the length of the ordering is 173.",
            "If you look at the orderings that you compute from the stabilisation numbers from one to 10 are on the left side numbers from 10 to 20, or on the right side.",
            "OK, and the length of the ordering is of course less than what what you got in the previous slide that was 184.",
            "This is 173.",
            "OK, so this shows that a stripper relaxations are tighter than spectral relaxations, so they say so it makes sense to investigate SDP based relaxations for other problems so."
        ],
        [
            "In this case we do it for ranking.",
            "OK, before I get into the SDP formulation for ranking, let's have a quick look at the the quadratic programming relaxation that was proposed last year and I see you there.",
            "So what what do they do there?",
            "I take the objective of the optimization problem that I mentioned in the second slide and I replaced the step function with the convex upper bound.",
            "OK, and then I introduce Slack variables.",
            "It's the normal machine learning SVM kind of setting and then I of course I relaxed a real numbers, so this is what I get at the end.",
            "This is QP.",
            "Right, so the first term is your preference constraints and the second term is your regularizer, and then I have a regularization parameter there.",
            "OK, now let's see how I could.",
            "We could formulate an SDP.",
            "Best relaxation for the same problem."
        ],
        [
            "So this is there similar franking on graph.",
            "This is actually the minimum length ordering problem that was proposed by Bloom and Cousins.",
            "The minimum length ordering problem actually has an SDP based solution with an approximation factor of oh log squared N where N is the number of vertice is.",
            "The concept shown differences instead of searching for an embedding in a single dimension, you are basically looking for an embedding in R&D dimensional space.",
            "OK, and once you get the embedding, what you do is you just choose a random vector and then project all the embedded embeddings into that random vector and we just open whatever ordering you get there.",
            "That's the minimum rank ordering algorithm.",
            "And this is an approximation factor for this algorithm is all square.",
            "But the good news over here is you could exploit the geometry of the embedding to handle the preference constraint to incorporate the preference constraints that we need.",
            "So from a machine learning perspective, this is what I need.",
            "I need a regularizer and I also need to make sure that the empirical risk is minimized, but this could be done by exploiting the geometry of memory."
        ],
        [
            "Let's see how this is done.",
            "There is this nice property in the meeting.",
            "Say for example I have this preference.",
            "I have let's consider three nodes ZZU&V and that's what I have in mind directed graph.",
            "So I would like to have you ranked to be higher than V right?",
            "And that is just a dummy node that I would include for all the directed edges in the directed graph.",
            "And in the embedded space I have FCF of you end up off me.",
            "The probability that after the year projection onto the random line, the vector, the node you lies between Z&B is actually determined by the angle made by the two vectors shown there.",
            "OK, and if I make this angle to 180 degrees I can be sure of its probability, one that you lies between serenbe, which is what I need to incorporate this constraint.",
            "This is a single constraint.",
            "OK, so this translates to making the cosine of the angle to minus one, so the sum of the dot product and the norms should be 0.",
            "Alright.",
            "So this is for a single reference constraint.",
            "If I want it for all the constraints, I just send them out."
        ],
        [
            "So this is the optimization problem.",
            "The empirical error term is actually shown in the in the in the first equation there.",
            "So you have you have a sum over all the directed edges.",
            "And you have to someone said, one is the angle and the other one is enough.",
            "That constitutes your that represents your.",
            "Empirical restaurant and of course I have the regularization term.",
            "But it's not difficult to see that the summons and empirical error terms are not convex.",
            "So what we do is we could actually make a change of variables from vectors to inner product between vectors.",
            "So now we are moving into an SDP formulation, so I could do that for the first term.",
            "OK, and then I get in history.",
            "I can write down and see proper formulation, but of course I can't do that with the second term because you have two names over there.",
            "I can't write it in the form of inner products and so we just ignore that term.",
            "We just minimize the first time in our formulation.",
            "OK."
        ],
        [
            "So this is the final optimization problem written in the SG.",
            "Perform the Canonical form.",
            "So I have two terms that PR is a trace the metrics trays so you have the Laplacian which is a regularizer and then you have those first term.",
            "P is just the incidence matrix, an zed is the matrix and appropriately constructed mattress that takes care of the dummy node that I introduced.",
            "So the first term is the empirical error term and the second term is a regularizer written in the form of inner products.",
            "And so I have a nasty problem.",
            "I have a constraint which is basically to make sure that the norms are bounded.",
            "OK, so just just a trace of XX Instagram matrix.",
            "Sorry I forgot to mention that, so it's just F into F transpose, so it's a gram matrix of your embedding."
        ],
        [
            "OK. Alright, so this is the final element.",
            "I solve the optimization problem and then I factorize the gram matrix using incomplete color scheme method and then after that I project all the embeddings onto a randomly chosen unit vector and output the ordering.",
            "So that's what I need, that's my.",
            "Final function."
        ],
        [
            "OK, so we perform experiments on ordinal regression datasets, so you could think of ordinal regression as a form of ranking where the number of ordinal values or the number of ratings.",
            "So I would I would say that audio and #5 should be ranked lower than ordinal #5 number one.",
            "In this way you have a ranking problem.",
            "And we used Kendall to reverse the valuation metric and also is just an evaluation metric to compare two different rankings.",
            "It just measures the number of disagreements between your 2 rankings."
        ],
        [
            "OK, so we compared the SDP formulation with the QP formulation that was proposed last year and this is where I get to.",
            "The conductor actually varies from minus one to plus 1 + 1 means perfect correlation and minus one.",
            "Is the other extreme, so tall prime is just.",
            "I just cleaned it up to so that the metric lies between zero and one.",
            "So if I get a value of 1, I'm doing fine.",
            "If I get a value of 0 then we have problems OK. Yeah, so this is the benchmark data set on metric regression.",
            "You could of course convert them into eternal regression by discretizing the target values.",
            "And we see that the SDP formulation works better than QP and the results are significant."
        ],
        [
            "OK. Alright.",
            "In the future, I would like to investigate something called spreading metrics.",
            "We didn't implement the spreading metrics, pretty metrics are used to make sure that the ordering the ticket at the end are spread apart.",
            "OK. Of course, the number of spreading metrics is actually exponential in number, but this problem admits a so-called separation Oracle using which you could.",
            "Solve the problem in polynomial time.",
            "So you don't really have exponential number of constraints.",
            "So that's One Direction.",
            "And the alternative?",
            "We could also think about alternative formulations for the same problem.",
            "For example, the low rank STP.",
            "So instead of optimizing over the gram matrix, you optimize over the embedding matrix itself.",
            "So this is what they do in low rank SDP solvers.",
            "And of course, you could also think about other approximation algorithms that could be used that could be modified to handle this ranking on graphs.",
            "Problem from a machine learning perspective.",
            "And finally, we could think about large scale implementations using petrol model methods, which is a solver or loskill.",
            "Implementations of SCP.",
            "OK, that's it, I'm done, thanks."
        ],
        [
            "Well, if it is curassow is STP for spectral clustering always better than?",
            "Then sorry is SDP based clustering versus just taking say the second eigenvector.",
            "It's strictly better for ratio cut.",
            "Or is it just done that example?",
            "Well, for power law graphs and social network graphs, it works better than spectral clustering that was shown in one of the papers.",
            "And of course there is this cockroach graph.",
            "There's also at this point that I said I would say in general is GP works better than spectral clustering?",
            "Yes, I would make that statement.",
            "So I know of no datasets where spectral works well and the SDP doesn't work.",
            "It's not known whether it always has a strictly smaller ratio cut.",
            "Or it's not?",
            "No no.",
            "There are some.",
            "Then there are some theorem thing that under some conditions I think it tighter.",
            "Approximation.",
            "But I don't know.",
            "But the SDP best relaxation is actually tighter than spectral relaxation.",
            "So I think I would claim that the ratio country get from this will always be smaller than the ratio corrupted from spectral clustering.",
            "So some of your results likely works right on the."
        ],
        [
            "Frank say stocks.",
            "Yes.",
            "Giving, that's because of the fact you didn't optimize the correct thing.",
            "Are you know you have to drop that?",
            "The second term, then, um, yes, yes we do have a relaxation.",
            "I maybe relaxed the problem or we drop the second time.",
            "That could be one of the reasons, but there is no way to incorporate that second term at the moment.",
            "And of course, we don't have spreading metrics yet.",
            "So.",
            "The minimum length ordering problem is randomized approximation of yes.",
            "Yes.",
            "So for this for for our results we just take 50 projections and take the best star.",
            "In the cross validation, in the inverse full cross, when inside the fold.",
            "And then use that same one.",
            "We don't really run the whole side.",
            "There's no way of choosing that brand new direction when we do it on the whole friends.",
            "No, I just fix it.",
            "I take 50 or 100 possessions, yes.",
            "I can speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so ready to roll.",
                    "label": 0
                },
                {
                    "sent": "It's the next speaker is Shane Carden.",
                    "label": 0
                },
                {
                    "sent": "Frankie, thanks.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'm a PhD student at the University of one and also at the front of the Institute in something else, and I'm going to talk about a semi definite formulation for ranking the nodes of an interactive graph given some additional information in the form of reference constraint and this is joint work with Thomas Kiernan and Steven Weber, who are my PhD advisors.",
                    "label": 0
                },
                {
                    "sent": "Thomas is here with us.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this is the outline of it all.",
                    "label": 0
                },
                {
                    "sent": "I'll first describe the problem setting and then I'll talk about and provide some motivation behind our approaches.",
                    "label": 0
                },
                {
                    "sent": "After that, I'll talk about a quadratic programming relaxation for the same problem that was proposed last year at ICL, and after that I'll talk about a semi definite formulation for the same problem, which is the main contribution of our work, and then I'll provide some empirical.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the problem?",
                    "label": 0
                },
                {
                    "sent": "So we are given two inputs here.",
                    "label": 0
                },
                {
                    "sent": "One is an underrated graph and the other one is a directed graph.",
                    "label": 0
                },
                {
                    "sent": "Pretty simple example, the web search, so the nodes of the undirected graph are the web pages, right?",
                    "label": 0
                },
                {
                    "sent": "And the linkage information in the under graph is the hyperlinks between your web pages.",
                    "label": 0
                },
                {
                    "sent": "What do we have in this static trap in this traffic graph?",
                    "label": 0
                },
                {
                    "sent": "We have some preference constraints, so if you're trying to learn a ranking function for a search engine, you would need some kind of need to provide some kind of training information to the learner right?",
                    "label": 0
                },
                {
                    "sent": "And this is what you have in the directed graph, so it actually contains information as such.",
                    "label": 0
                },
                {
                    "sent": "Pairwise comparison.",
                    "label": 0
                },
                {
                    "sent": "So you would say that I prefer web page 8 with PHP and then you have a directed edge from A to B.",
                    "label": 0
                },
                {
                    "sent": "So this information is there in that row.",
                    "label": 0
                },
                {
                    "sent": "So what do we need at the output?",
                    "label": 0
                },
                {
                    "sent": "I'm basically looking for a bijection from the set of vertices to the set of integers from one to N. OK, so which is basically a permutation and this permutation should satisfy two things.",
                    "label": 0
                },
                {
                    "sent": "We need to have as few backward edges as possible in there with respect to the directed graph.",
                    "label": 1
                },
                {
                    "sent": "And apart from that I also need to make sure that the.",
                    "label": 0
                },
                {
                    "sent": "Function that I'm trying to learn so should be smooth with respect to the undirected graph, right?",
                    "label": 1
                },
                {
                    "sent": "So this is the problem setting and let's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see how we could solve this problem.",
                    "label": 0
                },
                {
                    "sent": "It actually boils down to this following optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So I've got two terms in my optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The first term takes care of your preference constraints.",
                    "label": 0
                },
                {
                    "sent": "Alright, and the second term is your regularizer.",
                    "label": 0
                },
                {
                    "sent": "OK, so that is step function over there Sigma.",
                    "label": 1
                },
                {
                    "sent": "So if I minimize these two summons simultaneously, I can make sure that the final permutation has as few backward edges as possible in the respect to the directed graph.",
                    "label": 0
                },
                {
                    "sent": "And also I can ensure that the function that I the permutation is smooth with respect to the undirected graph.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, let's see how we could solve this problem.",
                    "label": 0
                },
                {
                    "sent": "The motivation, I mean the motivation actually comes from the vertex ordering algorithms by the so-called layout problems from graph theory.",
                    "label": 0
                },
                {
                    "sent": "So this ranking on graphs problem is very much related to the vertex ordering problems that you would find in the literature.",
                    "label": 1
                },
                {
                    "sent": "I've just listed three of them here.",
                    "label": 0
                },
                {
                    "sent": "For example, you could at the top you have the minimum bandwidth where you just try to minimize the maximum of the dilation.",
                    "label": 0
                },
                {
                    "sent": "The dilation is just the length of the edge with respect to the permutation, and you could also have a different form of cost function which is called the minimum linear arrangement where you just try to minimize over the sum of the directions.",
                    "label": 0
                },
                {
                    "sent": "Or you could also minimize over the sum of the square dilations, which is the minimum length modern problem.",
                    "label": 0
                },
                {
                    "sent": "So this this layer problems are I see them as different flavors of regularization from a machine learning perspective.",
                    "label": 0
                },
                {
                    "sent": "All these problems are NP hard, but of course approximation algorithms do exist.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Now the idea is we could generalize one of these vertex ordering problems an make it suitable for our problem at hand.",
                    "label": 1
                },
                {
                    "sent": "But there is one problem.",
                    "label": 0
                },
                {
                    "sent": "The vertex ordering algorithms are unsupervised, so to say you only have the regularizer here, right?",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, here you only have the regularizer there, but there is no way to incorporate the preference constraints.",
                    "label": 0
                },
                {
                    "sent": "That is which is appears in the first term here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is how to incorporate preference constraints into this algorithms so the main contribution of our work is to modify the minimum length ordering algorithm which was proposed by our emblem and colleagues in the late 90s to handle preference constraints.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the motivation also comes from a different angle.",
                    "label": 0
                },
                {
                    "sent": "And in recent years in the machine learning literature, you will find that.",
                    "label": 1
                },
                {
                    "sent": "People have come up with SGP formulations for different learning tasks like for example in classification we have this spectral graph transducer right which is.",
                    "label": 0
                },
                {
                    "sent": "Quadratic programming problem and on the other hand you also have an SDP formulation for the same problem.",
                    "label": 0
                },
                {
                    "sent": "So till maybe he comes up with a family of history relaxations for classification problems.",
                    "label": 0
                },
                {
                    "sent": "So all these algorithms, such as transductive in nature.",
                    "label": 0
                },
                {
                    "sent": "For the classification and in classroom you have this normalized cuts algorithm, which basically takes the spectrum of yellow pricing to compute their cut.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, you also have an SDP formulation for the same problem.",
                    "label": 0
                },
                {
                    "sent": "And there they show that the SDP relaxation works better than this.",
                    "label": 0
                },
                {
                    "sent": "Better relaxation for ranking last year.",
                    "label": 0
                },
                {
                    "sent": "I CML they proposed a quadratic programming formation.",
                    "label": 0
                },
                {
                    "sent": "I would not call it a spectral ranking IT security problem.",
                    "label": 1
                },
                {
                    "sent": "And so in this work we investigate and history formulation for the same problem.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright, so before I get into the details of our algorithm, let's let's make a brief detour on graph based clustering.",
                    "label": 1
                },
                {
                    "sent": "I'll see I'll show you how any stabilisation works better than spectral relaxation for clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, let's talk about the optimal ratio ratio code for this so called the Congress graph for the ladder network graph.",
                    "label": 0
                },
                {
                    "sent": "The optimal ratio that is shown over there, so I just need a horizontal sorry the vertical cut.",
                    "label": 0
                },
                {
                    "sent": "OK so this is balanced and the number of edges in the cut is too.",
                    "label": 0
                },
                {
                    "sent": "If you take the spectral relaxation to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last week you get something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This cut is also balanced, but the number of edges and that is for.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the stupid relaxation.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry so these are the two clusters that you get.",
                    "label": 0
                },
                {
                    "sent": "OK, one of the top and one in the upper.",
                    "label": 0
                },
                {
                    "sent": "But if I take it, it's cheap.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relaxation to the same problem.",
                    "label": 0
                },
                {
                    "sent": "I'm hurrying up.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the length of the artery.",
                    "label": 1
                },
                {
                    "sent": "OK, so from from the eigenvector from the Fiedler vector you could actually get an ordering and then if you compute the minimum length ordering cost you get this 184.",
                    "label": 1
                },
                {
                    "sent": "And the ordering is something like this one.",
                    "label": 0
                },
                {
                    "sent": "210 you get at the top and then 11:20 you get the gate at the bottom.",
                    "label": 0
                },
                {
                    "sent": "So it's oriented in that way.",
                    "label": 0
                },
                {
                    "sent": "So this is what spectral clustering does.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now let's see what SCP clustering is.",
                    "label": 0
                },
                {
                    "sent": "Stupid relaxation for the same problem.",
                    "label": 0
                },
                {
                    "sent": "Best so STP gives me the exact count that I want.",
                    "label": 0
                },
                {
                    "sent": "This is this is the ratio cut given by the SDP relaxation OK, it's balanced again and the number of edges and that is only two.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you get these two clusters.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, I would expect these two clusters, right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the length of the ordering is 173.",
                    "label": 1
                },
                {
                    "sent": "If you look at the orderings that you compute from the stabilisation numbers from one to 10 are on the left side numbers from 10 to 20, or on the right side.",
                    "label": 0
                },
                {
                    "sent": "OK, and the length of the ordering is of course less than what what you got in the previous slide that was 184.",
                    "label": 0
                },
                {
                    "sent": "This is 173.",
                    "label": 0
                },
                {
                    "sent": "OK, so this shows that a stripper relaxations are tighter than spectral relaxations, so they say so it makes sense to investigate SDP based relaxations for other problems so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this case we do it for ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, before I get into the SDP formulation for ranking, let's have a quick look at the the quadratic programming relaxation that was proposed last year and I see you there.",
                    "label": 0
                },
                {
                    "sent": "So what what do they do there?",
                    "label": 0
                },
                {
                    "sent": "I take the objective of the optimization problem that I mentioned in the second slide and I replaced the step function with the convex upper bound.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I introduce Slack variables.",
                    "label": 1
                },
                {
                    "sent": "It's the normal machine learning SVM kind of setting and then I of course I relaxed a real numbers, so this is what I get at the end.",
                    "label": 0
                },
                {
                    "sent": "This is QP.",
                    "label": 0
                },
                {
                    "sent": "Right, so the first term is your preference constraints and the second term is your regularizer, and then I have a regularization parameter there.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's see how I could.",
                    "label": 0
                },
                {
                    "sent": "We could formulate an SDP.",
                    "label": 0
                },
                {
                    "sent": "Best relaxation for the same problem.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is there similar franking on graph.",
                    "label": 0
                },
                {
                    "sent": "This is actually the minimum length ordering problem that was proposed by Bloom and Cousins.",
                    "label": 0
                },
                {
                    "sent": "The minimum length ordering problem actually has an SDP based solution with an approximation factor of oh log squared N where N is the number of vertice is.",
                    "label": 1
                },
                {
                    "sent": "The concept shown differences instead of searching for an embedding in a single dimension, you are basically looking for an embedding in R&D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, and once you get the embedding, what you do is you just choose a random vector and then project all the embedded embeddings into that random vector and we just open whatever ordering you get there.",
                    "label": 0
                },
                {
                    "sent": "That's the minimum rank ordering algorithm.",
                    "label": 0
                },
                {
                    "sent": "And this is an approximation factor for this algorithm is all square.",
                    "label": 0
                },
                {
                    "sent": "But the good news over here is you could exploit the geometry of the embedding to handle the preference constraint to incorporate the preference constraints that we need.",
                    "label": 1
                },
                {
                    "sent": "So from a machine learning perspective, this is what I need.",
                    "label": 0
                },
                {
                    "sent": "I need a regularizer and I also need to make sure that the empirical risk is minimized, but this could be done by exploiting the geometry of memory.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see how this is done.",
                    "label": 0
                },
                {
                    "sent": "There is this nice property in the meeting.",
                    "label": 0
                },
                {
                    "sent": "Say for example I have this preference.",
                    "label": 0
                },
                {
                    "sent": "I have let's consider three nodes ZZU&V and that's what I have in mind directed graph.",
                    "label": 0
                },
                {
                    "sent": "So I would like to have you ranked to be higher than V right?",
                    "label": 0
                },
                {
                    "sent": "And that is just a dummy node that I would include for all the directed edges in the directed graph.",
                    "label": 0
                },
                {
                    "sent": "And in the embedded space I have FCF of you end up off me.",
                    "label": 0
                },
                {
                    "sent": "The probability that after the year projection onto the random line, the vector, the node you lies between Z&B is actually determined by the angle made by the two vectors shown there.",
                    "label": 0
                },
                {
                    "sent": "OK, and if I make this angle to 180 degrees I can be sure of its probability, one that you lies between serenbe, which is what I need to incorporate this constraint.",
                    "label": 0
                },
                {
                    "sent": "This is a single constraint.",
                    "label": 0
                },
                {
                    "sent": "OK, so this translates to making the cosine of the angle to minus one, so the sum of the dot product and the norms should be 0.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So this is for a single reference constraint.",
                    "label": 0
                },
                {
                    "sent": "If I want it for all the constraints, I just send them out.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The empirical error term is actually shown in the in the in the first equation there.",
                    "label": 1
                },
                {
                    "sent": "So you have you have a sum over all the directed edges.",
                    "label": 0
                },
                {
                    "sent": "And you have to someone said, one is the angle and the other one is enough.",
                    "label": 0
                },
                {
                    "sent": "That constitutes your that represents your.",
                    "label": 0
                },
                {
                    "sent": "Empirical restaurant and of course I have the regularization term.",
                    "label": 1
                },
                {
                    "sent": "But it's not difficult to see that the summons and empirical error terms are not convex.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we could actually make a change of variables from vectors to inner product between vectors.",
                    "label": 0
                },
                {
                    "sent": "So now we are moving into an SDP formulation, so I could do that for the first term.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I get in history.",
                    "label": 0
                },
                {
                    "sent": "I can write down and see proper formulation, but of course I can't do that with the second term because you have two names over there.",
                    "label": 0
                },
                {
                    "sent": "I can't write it in the form of inner products and so we just ignore that term.",
                    "label": 0
                },
                {
                    "sent": "We just minimize the first time in our formulation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the final optimization problem written in the SG.",
                    "label": 0
                },
                {
                    "sent": "Perform the Canonical form.",
                    "label": 0
                },
                {
                    "sent": "So I have two terms that PR is a trace the metrics trays so you have the Laplacian which is a regularizer and then you have those first term.",
                    "label": 0
                },
                {
                    "sent": "P is just the incidence matrix, an zed is the matrix and appropriately constructed mattress that takes care of the dummy node that I introduced.",
                    "label": 0
                },
                {
                    "sent": "So the first term is the empirical error term and the second term is a regularizer written in the form of inner products.",
                    "label": 1
                },
                {
                    "sent": "And so I have a nasty problem.",
                    "label": 0
                },
                {
                    "sent": "I have a constraint which is basically to make sure that the norms are bounded.",
                    "label": 0
                },
                {
                    "sent": "OK, so just just a trace of XX Instagram matrix.",
                    "label": 0
                },
                {
                    "sent": "Sorry I forgot to mention that, so it's just F into F transpose, so it's a gram matrix of your embedding.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright, so this is the final element.",
                    "label": 0
                },
                {
                    "sent": "I solve the optimization problem and then I factorize the gram matrix using incomplete color scheme method and then after that I project all the embeddings onto a randomly chosen unit vector and output the ordering.",
                    "label": 1
                },
                {
                    "sent": "So that's what I need, that's my.",
                    "label": 0
                },
                {
                    "sent": "Final function.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we perform experiments on ordinal regression datasets, so you could think of ordinal regression as a form of ranking where the number of ordinal values or the number of ratings.",
                    "label": 0
                },
                {
                    "sent": "So I would I would say that audio and #5 should be ranked lower than ordinal #5 number one.",
                    "label": 0
                },
                {
                    "sent": "In this way you have a ranking problem.",
                    "label": 0
                },
                {
                    "sent": "And we used Kendall to reverse the valuation metric and also is just an evaluation metric to compare two different rankings.",
                    "label": 0
                },
                {
                    "sent": "It just measures the number of disagreements between your 2 rankings.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we compared the SDP formulation with the QP formulation that was proposed last year and this is where I get to.",
                    "label": 0
                },
                {
                    "sent": "The conductor actually varies from minus one to plus 1 + 1 means perfect correlation and minus one.",
                    "label": 0
                },
                {
                    "sent": "Is the other extreme, so tall prime is just.",
                    "label": 0
                },
                {
                    "sent": "I just cleaned it up to so that the metric lies between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So if I get a value of 1, I'm doing fine.",
                    "label": 0
                },
                {
                    "sent": "If I get a value of 0 then we have problems OK. Yeah, so this is the benchmark data set on metric regression.",
                    "label": 0
                },
                {
                    "sent": "You could of course convert them into eternal regression by discretizing the target values.",
                    "label": 0
                },
                {
                    "sent": "And we see that the SDP formulation works better than QP and the results are significant.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright.",
                    "label": 0
                },
                {
                    "sent": "In the future, I would like to investigate something called spreading metrics.",
                    "label": 0
                },
                {
                    "sent": "We didn't implement the spreading metrics, pretty metrics are used to make sure that the ordering the ticket at the end are spread apart.",
                    "label": 0
                },
                {
                    "sent": "OK. Of course, the number of spreading metrics is actually exponential in number, but this problem admits a so-called separation Oracle using which you could.",
                    "label": 1
                },
                {
                    "sent": "Solve the problem in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "So you don't really have exponential number of constraints.",
                    "label": 0
                },
                {
                    "sent": "So that's One Direction.",
                    "label": 1
                },
                {
                    "sent": "And the alternative?",
                    "label": 0
                },
                {
                    "sent": "We could also think about alternative formulations for the same problem.",
                    "label": 0
                },
                {
                    "sent": "For example, the low rank STP.",
                    "label": 0
                },
                {
                    "sent": "So instead of optimizing over the gram matrix, you optimize over the embedding matrix itself.",
                    "label": 0
                },
                {
                    "sent": "So this is what they do in low rank SDP solvers.",
                    "label": 0
                },
                {
                    "sent": "And of course, you could also think about other approximation algorithms that could be used that could be modified to handle this ranking on graphs.",
                    "label": 0
                },
                {
                    "sent": "Problem from a machine learning perspective.",
                    "label": 0
                },
                {
                    "sent": "And finally, we could think about large scale implementations using petrol model methods, which is a solver or loskill.",
                    "label": 0
                },
                {
                    "sent": "Implementations of SCP.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it, I'm done, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, if it is curassow is STP for spectral clustering always better than?",
                    "label": 0
                },
                {
                    "sent": "Then sorry is SDP based clustering versus just taking say the second eigenvector.",
                    "label": 0
                },
                {
                    "sent": "It's strictly better for ratio cut.",
                    "label": 0
                },
                {
                    "sent": "Or is it just done that example?",
                    "label": 0
                },
                {
                    "sent": "Well, for power law graphs and social network graphs, it works better than spectral clustering that was shown in one of the papers.",
                    "label": 0
                },
                {
                    "sent": "And of course there is this cockroach graph.",
                    "label": 0
                },
                {
                    "sent": "There's also at this point that I said I would say in general is GP works better than spectral clustering?",
                    "label": 0
                },
                {
                    "sent": "Yes, I would make that statement.",
                    "label": 0
                },
                {
                    "sent": "So I know of no datasets where spectral works well and the SDP doesn't work.",
                    "label": 0
                },
                {
                    "sent": "It's not known whether it always has a strictly smaller ratio cut.",
                    "label": 0
                },
                {
                    "sent": "Or it's not?",
                    "label": 0
                },
                {
                    "sent": "No no.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "Then there are some theorem thing that under some conditions I think it tighter.",
                    "label": 0
                },
                {
                    "sent": "Approximation.",
                    "label": 0
                },
                {
                    "sent": "But I don't know.",
                    "label": 0
                },
                {
                    "sent": "But the SDP best relaxation is actually tighter than spectral relaxation.",
                    "label": 0
                },
                {
                    "sent": "So I think I would claim that the ratio country get from this will always be smaller than the ratio corrupted from spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So some of your results likely works right on the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frank say stocks.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Giving, that's because of the fact you didn't optimize the correct thing.",
                    "label": 0
                },
                {
                    "sent": "Are you know you have to drop that?",
                    "label": 0
                },
                {
                    "sent": "The second term, then, um, yes, yes we do have a relaxation.",
                    "label": 0
                },
                {
                    "sent": "I maybe relaxed the problem or we drop the second time.",
                    "label": 0
                },
                {
                    "sent": "That could be one of the reasons, but there is no way to incorporate that second term at the moment.",
                    "label": 0
                },
                {
                    "sent": "And of course, we don't have spreading metrics yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The minimum length ordering problem is randomized approximation of yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So for this for for our results we just take 50 projections and take the best star.",
                    "label": 0
                },
                {
                    "sent": "In the cross validation, in the inverse full cross, when inside the fold.",
                    "label": 0
                },
                {
                    "sent": "And then use that same one.",
                    "label": 0
                },
                {
                    "sent": "We don't really run the whole side.",
                    "label": 0
                },
                {
                    "sent": "There's no way of choosing that brand new direction when we do it on the whole friends.",
                    "label": 0
                },
                {
                    "sent": "No, I just fix it.",
                    "label": 0
                },
                {
                    "sent": "I take 50 or 100 possessions, yes.",
                    "label": 0
                },
                {
                    "sent": "I can speak again.",
                    "label": 0
                }
            ]
        }
    }
}