{
    "id": "ot6izbah4apd3v4x55ajp3lwsjrdlf6y",
    "title": "Linear Projections and Gaussian Process Reconstructions",
    "info": {
        "author": [
            "Joaquin Qui\u00f1onero Candela, Facebook"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/learning06_candela_lpgpr/",
    "segmentation": [
        [
            "Sorry for the delay, so this is the first time.",
            "And it's not a cheap excuse that this laptop doesn't doesn't work when I. Plug it to a beamer.",
            "So I'm gonna try and and show you the demos which I have like this OK, and then if you're if you're interested then you can maybe sort of gather around right after the talk and I can show you the demos so the talk is going to be fairly brief 'cause it did rely sort of heavily on on demos since we're going to have lots of time then I'm going to give you a very exciting talk about affiliations.",
            "Does anyone have a laser pointer or does this thing?",
            "Alright.",
            "OK, so my name is cooking milk and Ella.",
            "I am now visiting the Carlos after University in Madrid.",
            "I am really at the Technical University of Berlin and that hungover 1st and from January this year you will be able to find me at Microsoft Research in Cambridge.",
            "This is John work with Neil Lawrence and Cosmos and both of them are also moving, so Neil will go from Sheffield to Manchester.",
            "That's not a very long trip to make.",
            "And Carson will actually join in Cambridge, which is which is great, except he'll be at Cambridge University.",
            "OK, so I'm going to try to tell you about.",
            "About linear projections and Gaussian process reconstructions, that's actually the actual talk after this very exciting talk on affiliations."
        ],
        [
            "Alright, so first of all, I had forgotten this one.",
            "First of all, some acknowledgement, so we'd like to thank Pascal once more.",
            "You never think Pascal enough for a visit day they funded, which I paid to new Lawrence in the summer last year, where actually the idea of the back constraints, which will tell you about was cooked up."
        ],
        [
            "Alright, consider the following situation.",
            "I am sure all of you do data modeling so you find yourselves in this situation quite often.",
            "Where you have some sort of a fairly uncomfortably high dimensional data out there, and you'd like to look at your data right so?",
            "Data we're going to call here Y alright so have this sort of matrix capital Y and is there a stick or a cane or anything?",
            "I have my stick.",
            "OK, OK.",
            "But it doesn't have graduates, so this is all I can.",
            "OK, that's a good old robust sexually not, not, not really long enough, so I think that's OK. Alright, so as you can see I have a half capital N vectors in a quite high dimensional space.",
            "The dimension is capital D alright, and what I'm searching is for a low dimensional representation.",
            "Alright so I'll have.",
            "Capital N vectors in my low dimensional space as well and here the dimension is going to be Q alright and the idea of course is that Q is fairly small, right?",
            "Say Q = 2 so that I can actually look at my data right OK?",
            "So.",
            "One thing that most of you would do first without even thinking about it would probably be to search for a linear projection.",
            "OK, So what do you need to make a linear projection where you need?",
            "You need a matrix capital P which is of size Q * D, right?",
            "If you let your data lived in 100 dimensional space and you want to look at it into 2D space, you need to sort of come up with the 2 * 100 matrix, which you're going to use a projector data.",
            "Now what I would do first and I guess.",
            "It would be the same for you would be to actually search for a principle component, sort of the composition of your of your sort of data in order to make the projection.",
            "And as you all know what you would do, you would build this covariance matrix here.",
            "Why terms Y transpose OK?",
            "Which, as you can see is a D times the object and you would sort of search for the first Q.",
            "First, Q eigenvectors in here OK and you of course have carefully removed the mean in here first, alright to get a covariance matrix?",
            "OK now.",
            "What does PCA do?",
            "What what PCA does and I want to reduce the term reconstruction OK, what PC does it says?",
            "Well, assume.",
            "Fair enough, you want to have these two dimensional space here where you're going to look at your data.",
            "Now you could look at this in a different way.",
            "You could say well.",
            "I want to I want to reconstruct linearly from my 2 dimensional space.",
            "Alright, I want to be able to sort of recreate my high dimensional points OK and I want to do this linearly.",
            "OK so PCA and I suppose this is no surprise to any of you what it achieves actually is it achieves a minimum mean squared reconstruction error.",
            "Sort of knowing that you're going to do things linearly.",
            "OK, it gets you the best linear projection, knowing that you're going to be reconstructing linearly.",
            "Alright, in the optimal in the mean squared sense."
        ],
        [
            "Alright, so let's talk a bit more about.",
            "Reconstructions here OK so so I just said that I'm going to be reconstructing linearly now.",
            "Obviously any sort of reconstructed high dimensional object from my my embedding is going to be a composition of the first Q eigenvectors that I chose.",
            "Alright, so I'm going to be spanning.",
            "I'm going to be spending my high dimensional sort of manifold by a handful of vectors, OK?",
            "I'm going to show you a figure in a moment.",
            "So very often this is a very poor reconstruction actually, and this is also not surprised because you're approximating some some sort of manifold just by just by a hyperplane.",
            "OK of dimension Q in your D dimensional space.",
            "On the other hand, you might argue that most dimensional reduction methods don't really even offer you a mapping to go back from your low dimensional space to the sort of high dimensional space, OK?",
            "So let's use an example.",
            "I'm sure you all know the handwritten digits database.",
            "OK, so this is a bunch of sort of handwritten digits scan onto images which are 16 * 16."
        ],
        [
            "And what we're going to do now?",
            "Is we're going to look at?",
            "We're going to sort of perform an embedding of those data using PCA onto a 2 dimensional space.",
            "OK, now what I've done is after doing the embedding OK, I have labeled the little points with the actual digit they correspond to all right?",
            "And these are these.",
            "Are these points in here?",
            "Alright, what I have chosen is I have chosen a bunch of numbers.",
            "I have chosen twos, threes, fours and fives and just a handful of them, about 100 of each.",
            "You can see there's a fair amount of overlapping between the three is in the fives Now, what do our eigenvectors look like in this case?",
            "Well, if I if I had been careful enough.",
            "I would have.",
            "I would have added the origin which is about here.",
            "This is sort of the origin of this figure.",
            "OK so you can see that in this axis here what I'm doing is I'm using one eigenvector which is actually this guy.",
            "You can see it here in positive and you can see it pretty much here in negative OK. And basically the other eigenvector is sort of proportional to this guy here, which looks sort of strangely between three and five.",
            "OK, these are the directions of maximum variation in my space in my high dimensional Space, 16 * 16.",
            "Of digits.",
            "OK, now if I wanted to reconstruct OK and I went in my 2 dimensional space and I said, well, you know, generate whichever digit sort of corresponds to this location in my low dimensional embedding I would get sort of the little cross here OK, and so on for the other four points alright?",
            "Now, that's not very, very exciting.",
            "Is it as a reconstruction?",
            "But here in this column I have shown you a reconstruction obtained by other means, which looks a lot cooler, doesn't it?",
            "So what I'm going to do now is I'm going to tell you a bit about how I managed to obtain this reconstruction here, OK?",
            "So GP stands for Gaussian process."
        ],
        [
            "Alright.",
            "To obtain that reconstruction, the idea is fair enough.",
            "I have projected linearly OK, but now I actually have a set of pairs of X&Y's.",
            "OK, so I can actually pose a regression problem.",
            "Can't I write?",
            "I have inputs and outputs earlier I only had sort of data, right?",
            "So now basically what I can do is I can fit a nonlinear a flexible, powerful nonlinear regression method that basically learns how to go from the low dimensional to the high dimensional data space, OK?",
            "And the axis the projections have been given, for example by PCA, OK?",
            "Everybody is happy with that."
        ],
        [
            "So the method I would choose.",
            "Because of my past would be to use Gaussian processes OK.",
            "So now I'm going to try to tell you in two slides about Gaussian processes.",
            "And Bayesian regression with Gaussian process is OK if you're going to do.",
            "Bayesian regression.",
            "What is the first thing that you need?",
            "Well, you need a prior over functions OK?",
            "Now suppose you were able to write a prior distribution over functions OK. Now you can draw samples from this prior and this is what is shown on the figure top left.",
            "These are simply some samples out of a Gaussian process prior.",
            "OK, I'll tell you in the next slide how you build a Gaussian process.",
            "Just think a prior over functions.",
            "Now the next thing that happens is that you actually see some data OK, and now we're going to do a sort of a 1 dimensional to 1 dimensional regression problem.",
            "Now we see three data points.",
            "OK, now what you have to do is you have to come up with the likelihood or with the noise model.",
            "OK, how much do you trust the data?",
            "And let's assume here you sort of decide that you decide that the noise is uniform and it's centered around the data point that you observe around the Y. OK, so you see this little.",
            "I'm not sure I can climb that up.",
            "You see, these two little parallel bars around around the data point that basically shows you the noise distribution OK, which is centered around the way you have observed, alright?",
            "Now.",
            "The next thing in the Bayesian sort of a machinery is to look at to obtain your posterior distribution.",
            "Your posterior over functions.",
            "OK, so how do you do that?",
            "Well, you can apply Bayes rule.",
            "In this sort of toy example, you can.",
            "You can apply it sort of very intuitively.",
            "You could you could obtain samples from your posterior by doing the following.",
            "You take samples from your prior alright, and for each sample you take.",
            "You check whether it satisfies.",
            "Your likelihood assumption.",
            "Does it go through all three little tunnels OK?",
            "Because I have a uniform noise distribution, meaning if it goes outside one of those sort of little intervals, then the likelihood is going to be 0 so that that sample has no probability under my posterior distribution.",
            "OK, so basically that's the game and I do that lots of times and actually to obtain the 25 samples from the posterior that I'm showing you in the middle, I had to sample 60,000 samples from my posterior at random.",
            "It seems like it doesn't seem like the most clever way of doing things, but it works.",
            "And actually, that's not really the way you would do it in practice.",
            "This is just to illustrate Bayes rule.",
            "Now, once I have enough samples from my posterior, these samples represent my knowledge right given my prior sort of some of the hypothesis from my prior have been restricted by the data I have observed.",
            "OK, I can actually summarize the information I have.",
            "So if someone asks me to predict somewhere, let's say someone asks me, what do you think would be the value of this function at zero?",
            "OK, well, what I can do is I can go to my posterior sort of look at zero, and I could sort of I could sort of provide some summary OK?",
            "I could decide to give sort of the empirical mean and the empirical variance at that point OK computed from the samples from the posterior, so that's it.",
            "That's based on regression.",
            "It's as easy as that.",
            "Of course there are some.",
            "There are some subtleties like and this is a question that everybody should sort of be asking immediately.",
            "Now, how do I choose my prior and stuff like that, alright?",
            "Well.",
            "If you if you sort of go the hardcore basin way.",
            "You define sort of hyperparameters on hyper priors on anything you sort of.",
            "You're not willing to give a value to OK. Or you can Alternatively maximize the evidence so I can also tell you from these figures what the evidence would be here.",
            "So the evidence is also known as a marginal likelihood.",
            "What it is, it's the normalizing constant of the posterior.",
            "Now I have chosen this noise model here because it's very easy to compute the normalizing constant of the posterior.",
            "Here you know what it is, it's simply the ratio of the functions that were accepted from the prior to the total number of functions that I sampled.",
            "So the evidence here would be 25 /, 60,000 in this case.",
            "OK, you can see that if I had chosen a prior that was less appropriate that generated more crazy functions, it would be less likely that a function makes it through the likelihood and therefore the evidence would be lower OK.",
            "So in a way, the evidence sort of makes sure that you don't choose priors that allow too crazy things because things have to normalize, OK?",
            "We can talk about that later, over coffee or."
        ],
        [
            "OK. Now, how did I build my prior?",
            "Well, it's called a Gaussian process, so it has.",
            "It's got to have a Gaussian in it, doesn't it?",
            "Let's consider a joint prior over 2 function values evaluated at XI and XJ.",
            "OK, so here I'm directly building a prior on function values.",
            "OK, now you see the joint prior over 2 function values would be a sort of a joint multivariate Gaussian.",
            "The mean has been set to 0 here, but this is really not limiting.",
            "You shouldn't worry too much about that.",
            "You can define other sort of sort of.",
            "Mean functions.",
            "Now the very important thing.",
            "Of course, here the only thing we've got left to define our model is a covariance, and the covariance is obtained from the covariance function.",
            "In this case, for those of you sort of into kernels, kernel in general, a semi definite kernel is a is a valid covariance function as well.",
            "OK, you can see that the key thing is that the covariance function.",
            "What it tells me is how similar and in which way should function values B as a function of the corresponding inputs.",
            "OK.",
            "So this is going to tell me if I take here the so-called squared exponential or Gaussian covariance function.",
            "You can see that the covariance between the two function values is going to depend on how far away the inputs are.",
            "If the inputs are far away enough, this is going to be 0 and then the two function values are free to do whatever they want.",
            "If they are close together, they're not free to do whatever they want.",
            "They have to sort of be similar.",
            "Now what I have illustrated here are functions run at random from sort of three different covariance functions.",
            "The two first ones are the squared exponential with two different sort of length scale values, so the length scale would be this thing which tells you how quickly do I forget things.",
            "If this is very large, I'm going to have a very long memory.",
            "If this is very small, things are going today very quickly, and this is a completely different covariance function.",
            "This is a neural network covariance function."
        ],
        [
            "OK, so that's almost it for Gaussian processes.",
            "Now the noise model I told you about earlier.",
            "Sort of forced us to sample to obtain samples from the posterior.",
            "We could not compute the posterior analytically with that with that sort of uniform noise model.",
            "However, one quite neat thing is that if we choose instead an independent Gaussian noise model, OK, then if you look at if you look at things, the posterior is going to be actually the product of the likelihood terms of prior.",
            "Now this object here is a product of two Gaussians, right?",
            "So the product of two Gaussians has has a very nice property.",
            "The product of two Gaussians is Gaussian and it has a Gaussian normalizing constant, which means the posterior Gaussian and the evidence is also Gaussian and the predictive distribution is also Gaussian.",
            "So I'm not going to go much into the details here.",
            "I want to I want to point out the fact that you can analytically compute the evidence here because we're going to use that later, OK?"
        ],
        [
            "Alright.",
            "So until now, what I have told you is that I could use Gaussian processes.",
            "To learn this mapping between the X is otherwise OK and the axis.",
            "Had been given OK, they had been given by some linear projection.",
            "Now the question is.",
            "Could I actually could I actually forget about this linear projection that I was doing earlier and directly use this Gaussian processes to actually learn the location of the low dimensional variables.",
            "Well, yes I can."
        ],
        [
            "And this is what was proposed by Neil Lawrence.",
            "Sort of recently, what he called the Gaussian process latent variable model.",
            "So here's the idea.",
            "The Gaussian processes have shown you are able to do a 1 dimensional output regression.",
            "OK, now what I'm going to do is if I had 100 dimensions in my head dimensional data space, I'm going to take 100 Gaussian processes and each of them is going to predict sort of the high dimensional data along one of the dimensions.",
            "OK, now the thing that's going to be common to our models are the inputs.",
            "OK, so all 100.",
            "Gaussian processes in order to predict this point here in high dimensional space, they have to share the same input in my 2 dimensional space.",
            "So there are coupled by the inputs.",
            "They're not really totally independent, OK?",
            "Now the crazy idea here is.",
            "That you do regression in a way where you sort of give the targets and you ask the model to learn the inputs.",
            "OK, so this might sound like a completely unreasonable and crazy idea.",
            "Well, actually it sort of is."
        ],
        [
            "So I'm going to show you here.",
            "Here's where the difficulties come right?",
            "I was going to show you an example on motion capture data to give you an impression of a high dimensional manifold, and I'm going to try to.",
            "I'm going to try to do that somehow.",
            "Alright.",
            "I don't know if you can see this, but let me just re run it to see if you can see it.",
            "Alright, So what you can see.",
            "So motion capture data.",
            "You can see a little little man sort of running, and there's a number of markers attached to him to his body.",
            "OK, there's a total of 50.",
            "Quite remember 30 something markers OK, and each marker gives you a 3 dimensional vector, namely the position.",
            "OK, the 3D position, so the data is about 100 dimensional.",
            "OK, now obviously you would suspect that this data set cannot fill the 100 dimensional space right?",
            "Because because when you're running.",
            "There's actually only a few variables that are determining your whole sort of act of running OK. Now let's see if I use the Gaussian process latent variable model carefully initialized by PCA.",
            "So I perform initialization of my axis with PCA, and then I sort of maximize the evidence of the Gaussian processes with respect to the inputs.",
            "This is what I get.",
            "I get this embedding here.",
            "I'm going to try to explain it to you.",
            "So what I have done is.",
            "The little crosses, which you probably cannot see, represent the sort of all the frames.",
            "OK, so all the data points in high dimension space, they have been embedding embedded there and then I have connected everything with the solid line just to show you the course of time, OK?",
            "So this information was not given to the algorithm.",
            "Time was not given to it, but as you can imagine, this is a movie with a handful of frames, OK?",
            "Now the cool thing here is that I can be tough.",
            "I can go in here.",
            "And I can sort of move my mouse and I can.",
            "I can generate new high dimensional objects right?",
            "Because I have.",
            "Anyway, I have some sort of a regression mechanism.",
            "So here you can see the.",
            "The angle has been sort of encoded in this direction here and here.",
            "You sort of have different sort of phases of the cycle of running now.",
            "This embedding is a bit bit strange, and it's perhaps not the most satisfying embedding you could have dreamt of alright.",
            "And why is that?",
            "Well, that's because there are points like like this point here.",
            "At this point there they belong very close together, because these are two consecutive frames.",
            "But somehow they have been embedded quite far apart, right?",
            "And this happens again and again in this embedding.",
            "What is 1 nice thing about this embedding is that it doesn't overlap or collapse.",
            "It doesn't embed things that were different in data space, it doesn't embed them together in here.",
            "OK, that's nice.",
            "What's not nice is that it's breaking similarities.",
            "OK, so let's try to understand a bit more why this is."
        ],
        [
            "OK, let me let me in general.",
            "So before we come to that, I'll say that as you can imagine, I'm learning the inputs.",
            "OK, so if my low dimensional space was a dimension Q and I had any data points, I have an optimization problem which is N * Q, that's big.",
            "On top of that, because I'm doing the crazy thing of giving you some targets and asking you OK, just find me some inputs for this target using your regression model you have loads of local minima alright.",
            "Another thing is that because we sort of throw away our linear mapping from data to latent.",
            "I I don't have an explicit mapping anymore, right?",
            "Going from data space to latent?",
            "And now the thing that I wanted to tell you about is that the Gaussian process latent variable model is not similarity preserving.",
            "So for all of you who know about locally linear embedding.",
            "So LLE Isomap any of those nonlinear dimensionality reduction methods, what they do is that they basically instead of of sort of minimizing the linear reconstruction costs, OK, they minimize something like that.",
            "But they say you know in the sum of the squared error terms I'm going to pay a lot more attention to those distances that correspond to close things in high dimensional space.",
            "And I don't care so much about the guys that are far apart.",
            "OK and this is what allows you to have a nonlinear sort of embedding, OK?",
            "And that's what I would call similarity preserving.",
            "OK, at the risk of sometimes embedding things that are far apart close together.",
            "OK, now the GPL VM does exactly the opposite.",
            "And why is that?",
            "Well, as you saw from the from, the functions that I draw from my Gaussian process prior, if two inputs are close together for a Gaussian process, then the two corresponding outputs are also necessarily close together.",
            "Now what does that mean?",
            "That means that.",
            "If I have two outputs or two high dimensional objects which are very dissimilar, they cannot be embedded as two inputs that are close together.",
            "This is a bit like A implies B implies implies no way alright.",
            "did I say correctly?",
            "Well, you know what I mean?",
            "You cannot.",
            "You cannot have.",
            "You cannot have because two inputs must generate two inputs that are similar, must generate 2 high dimensional objects which are similar.",
            "Then it cannot be that two dissimilar objects.",
            "Our embedded onto 2 close together points OK and This is why the GP LVM what it does.",
            "It loves to separate things.",
            "That's his his big thing right?",
            "He just separate things.",
            "Now the problem with that is that you would lose you sort of lose your the structure.",
            "The local structure of your manifold.",
            "Imagine your data was contained onto a scarf that was sort of floating in the wind.",
            "OK, now what the GP LV is going to do?",
            "It has no problems in shredding your scarf into pieces and just embedding little patches here and there.",
            "It's not gonna.",
            "It's not going to sort of collapse apart of the scarf.",
            "That was far apart from another one together that is not going to do.",
            "Which is something that most of the other methods actually will do.",
            "OK, so there's something interesting in this in this property of being the similarity preserving, but it's like something which is at the same time good and bad.",
            "OK, I put here in this slide that another advantage is that sometimes when you have a very noisy manifold, some of these methods that focus on preserving similarity of course are going to be a bit sort of in difficulty, right?",
            "So what does what does Neil Lawrence?",
            "How did?",
            "How did the Lawrence manage to get his paper accepted at nips right?",
            "Well, because he initialized very carefully using PCA alright?",
            "And that sort of and that sort of work, 'cause if you if you set up give it sort of the right inputs, then the model doesn't get completely lost, right?",
            "It's not going to try to do something ridiculous.",
            "OK, is just if you initialize it at random, there's no way OK.",
            "Which is why I sort of said to me that I thought his model sucked when I visited him in in Sheffield.",
            "We were friends.",
            "Actually I can say to him things like that."
        ],
        [
            "Right?",
            "Now, so I chose to sort of combine the GP LVM with linear projections here OK.",
            "There might be a sort of a symbiosis here, right?",
            "You might think they need each other right?",
            "So why is that?",
            "Well?",
            "It would seem that that if you're projecting linearly, you don't want to reconstruct linearly.",
            "If you want to sort of generate sort of nice high dimensional objects, OK, so in that sense linear projections need Gaussian processes, OK, but the GP LVM needs some sort of PCA initialization anyway, so then want to do something different.",
            "There might be a better linear projection sort of for Gaussian processes reconstruction, right?",
            "Because?",
            "The PCA projection is optimal if you're going to be reconstructing linearly.",
            "OK, but you're not going to be reconstructing linearly.",
            "You're going to be reconstructing using Gaussian processes, so you want to find the linear projection, which is optimal.",
            "Knowing that you're going to be reconstructing using Gaussian processes.",
            "OK, So what you can do is very simple, and actually you solve quite a few problems just at once.",
            "If you say, well, you know what I don't really feel like learning the whole inputs X, right?",
            "So what I'm going to do is climb on the chair I'm going to.",
            "I'm going to sort of.",
            "Change my problem and I'm going to say, well, the axis cannot just be whatever they want, they exist, must be a linear projection of the wise OK?",
            "And now my parameter.",
            "My free parameter is Pete, the projection matrix.",
            "Now this is nice because queue times D is probably a lot smaller than Q * N. OK, so if I have more data points than the dimension of my high dimensional manifold, then this is a smaller optimization problem and it's also an optimization problem which is very very heavily constrained and I can tell you that I can now initialize at random and it works very nicely.",
            "OK, so now the other things that I was going to show you is how does it work?",
            "And actually I think I pretty much have no more slides with text.",
            "I think everything that comes in our figures.",
            "OK. Let's go back to our motion capture data example.",
            "How would the PCA projection look like of that little running guy?",
            "OK, well this is how it would look like.",
            "Let me find it.",
            "Here is OK.",
            "This is how it would look like.",
            "What you know, you might argue.",
            "Well, this is fantastic.",
            "This is a very nice embedding, so why do I need to bother with all this crap?",
            "Will be talking talking about is it a very nice embedding?",
            "I don't know, so this would be taking the first 2 sort of principle components.",
            "OK, the first 2 eigenvectors.",
            "Now, why is it maybe not so nice as it may seem?",
            "Well, the nice part is you can see that it seems to have captured some cyclic structure, which makes sense if you're running.",
            "You're performing a cyclic movement.",
            "OK, now what is perhaps not so satisfying about this?",
            "If I manage to use my left hand for this and move this guy, you can see if I now go onto a 3D, you can see it actually all these points here.",
            "If I add one more, one more principal component you can see on 2D.",
            "Using the two first principle components I had sort of collapsed a lot of points which are actually different.",
            "I had sort of collapse them together.",
            "OK, now if I use 3 dimensions, things look a lot better, alright?",
            "You can also see that.",
            "Yeah, so this sort of this is where you can see right?",
            "Is approximately using the two first components.",
            "OK, maybe this is not exactly what Gaussian processes will be happy about now.",
            "What will Gaussian processes be happy about?",
            "Well, let's see what happens if I now do this game of seeking for the linear projection, which is sort of optimal for Gaussian process reconstruction.",
            "What I get is this.",
            "I'm sorry if you can see this.",
            "I get this this sort of this sort of linear projection here, which looks very different from the previous one, and you can see there's no, there's no real overlapping of things so that it has searched for the linear projection, which tries to separate things the most, and I can.",
            "Again I can again, of course play this nice game of going in here and sort of.",
            "You know, you can see that around here.",
            "It has sort of found, sort of some cyclic structure.",
            "It has used the radius to encode for the inclination of the runner OK.",
            "This might this might not be sort of a perfect embedding either.",
            "But for Gaussian process, for the purpose of Gaussian process reconstruction, it's a much better embedding.",
            "OK, what comes next?",
            "We can."
        ],
        [
            "Go back to the to the digits, so this is the.",
            "This is a projection of the digits that I showed you earlier, so it's a linear projection.",
            "OK, PCA.",
            "Now what you see is that the three is under fives.",
            "They're sort of collapsed onto into each other, and the reason for that is because one of the two eigenvalues looks a bit like it.",
            "Sort of looks like between the three of the five.",
            "Just removing the upper bar of the three under 5 right, which is on one on either side.",
            "So this is the reason why they both get projected on top of each other.",
            "Now what happens if I search for a linear projection that that sort of is optimal?"
        ],
        [
            "For Gaussian process reconstructions, well is not perfect, but you can see that it's a linear projection that at least seems to separate the three is in the fives a lot better, and the reason is because if I if I was using this embedding and I was so earlier when I was generating when I was reconstructing with the Gaussian process, I very carefully avoided to put a query point in here, right?",
            "And that's because the GP doesn't really know what to do there, it's going to give you something which is between 3:00 and 5:00, right?",
            "However, here is going to do a much better job.",
            "And then finally, I think my last or almost less."
        ],
        [
            "Example is this one.",
            "You know, perhaps this data set the Swiss roll.",
            "Or the robust role if you will.",
            "So this is this is the example that was in the locally linear embedding in the elderly.",
            "Paper, which is a science paper.",
            "Imagine your data set looks like this.",
            "I don't know if I can manage to do this.",
            "OK, perhaps that's why it's easier so.",
            "Imagine you had this kind of data set.",
            "This is called the Swiss roll.",
            "OK, it's something you can eat.",
            "Actually.",
            "Sort of a pastry that looks like this.",
            "Now this is intrinsically.",
            "This is a 2 dimensional data set.",
            "Alright?",
            "I mean it's in 3D, but really if I manage to do it, probably this has, you know the data lives in a 2 dimensional manifold.",
            "Now if I did a PCA sort of embedding of this, why would I get well, where is where is the direction of maximum variation?",
            "Where it's this one?",
            "I would get this as one of the directions and you know.",
            "A perpendicular to this, as the other one, so it would be basically as if I was sitting on my role.",
            "OK, so we have a lot of overlapping of points.",
            "And what does an what does this model do?",
            "What it does?",
            "What you can see there is still a linear projection, but it seems to be a linear projection.",
            "I mean necessarily.",
            "You're going to have overlapping, right?",
            "Because it's a linear projection, right?",
            "But you see that it minimizes it.",
            "You have it sort of along along this axis here.",
            "OK, so it tries to separate things apart as much as possible.",
            "And I think I'm sort of finishing now."
        ],
        [
            "So what I've told you about is I've told you about, sort of.",
            "I didn't mention the probabilistic part of it.",
            "I forgot to say that.",
            "OK, if you look at this figure again here, you have dark and sort of and white areas.",
            "Actually, this guy here.",
            "This is only the mean reconstruction, so I told you that if I if I have an input somewhere here.",
            "What I have is I have see if this is 100 dimensional space.",
            "I have 100 dimensional Gaussian processes, each of which give me a Gaussian predictive distribution.",
            "They don't give me just a point here.",
            "I'm showing the mean, but here there's an uncertainty associated to that and actually sort of the sum of the log uncertainties or a measure like that of overall confidence is sort of what is sort of what is used to obtain the shading here.",
            "So in the white regions, the reconstruction model is quite certain about his Gaussian reconstruction in the dark areas is not so sure.",
            "Which means that if you if you go and ask for a point in some areas, it seems that I don't have a nice example.",
            "Sometimes you get these examples where he's doing things that are impossible, right?",
            "Doesn't matter, because this is the mean of a Gaussian that has a very wide variance.",
            "OK, so you have this sort of probabilistic reconstructions.",
            "Nice applications of this are of the fact that you can generate from your low dimensional space in computer sort of animated graphics.",
            "There's this very cool example of people wanting to have a face that sort of looks more or less human and sort of talks to you or whatever in your computer.",
            "So what you would do is you sort of record a number of data points of a person making different kind of characteristic faces.",
            "Angry, sad, excited, whatever you want, OK?",
            "And then what you do is that you embed those things onto a very low dimensional space of dimension 2, and you have this very powerful tool for reconstructing.",
            "So you can actually slide your mouse in there and you can have the guy sort of make different facial expressions, right?",
            "Some sort of, some sort of interpolation if you will between between those faces it's also been used as prior distributions for tracking.",
            "OK, so for those of you who work on tracking, you know what I mean.",
            "And I think I OK, yes yes.",
            "So the last thing I wanted to say actually, I'm going to skip this one.",
            "So if someone wants to ask this question, you're welcome to do it.",
            "I was just putting it there in case you didn't have questions.",
            "Yes, this is a particular example.",
            "A particular case what I've told you about is a particular case of the work that need learns, and I published at ICML this year where actually instead of having a linear mapping going from high dimensional data to the low dimensional space, we just have a neural network and RBF sort of network whatever you want.",
            "We have a flexible model for going down as well.",
            "Now if you do that.",
            "You get nice things like like this.",
            "This is the kind of embeddings you get, which are which are sort of nicer even than than the other things right here.",
            "You do manage to do both things.",
            "You do manage to keep to preserve similarity, and you manage to preserve the similarity at the same time.",
            "OK, you can again play the little game over.",
            "Of generating sort of stuff.",
            "So here you have the cyclic.",
            "I'm not so good with my left hand.",
            "But you can.",
            "You can see what I mean, right?",
            "So it seems like quite nice.",
            "Embedding and I think I am finished.",
            "When you say the.",
            "The first approach, with the results from the second approach.",
            "Can you say it again, please?",
            "The first part you said you were using PCA training.",
            "I got some personal.",
            "Yeah, so yes the inputs yeah.",
            "With the result of using the back of planes.",
            "OK, so I don't know if this is a question, but I'm going to try to answer it anyway.",
            "Yes, so if you.",
            "So perhaps I can go back to this to this slide.",
            "Here let me see.",
            "OK, I don't quite know how this.",
            "Operating system works.",
            "Yes, yes.",
            "OK, so this would be fine initialized by PCA, but my learning problem would be OK. Now I have a number of inputs and the corresponding little figure for each of those points are my targets for my regression problem and I trained GP OK. Now the performance I get which I can measure basically to evaluate performance.",
            "What I can do?",
            "I have a model that gives me probabilistic predictions where I can simply plug the truth under my predictive distribution and I get sort of get a density in there, right?",
            "I can take logarithms to be able to take averages and stuff like that, so I have a measure to evaluate things.",
            "Now this is going to get much worse score than this.",
            "Which is a result of having sort of initialize of having sort of a couple the linear and the GP together.",
            "Here they're sort of separate right, and the reason why this is worse is cause in this critical region in there where the trees in the fives overlap.",
            "What a GP does, you can sort of very roughly, think of a GP, something that does smoothing is not entirely true, because it doesn't only do sort of interpolation between between things, it can do more than that, it can overshoot, and it can do other things, but.",
            "The thing is, what it's going to use?",
            "It is going to use it of the neighboring targets, but now here everything is mixed up.",
            "OK, so it's going to be very uncertain what it's going to do is going to predict you something that is a mixture between three and five in there and with very sort of big uncertainty.",
            "However, here is going to be able to do a much better job because the trees in the fives are sort of more separate, right?",
            "So the kind of embeddings you need so overlapping is not good if you're going to be using the GPU for reconstruction.",
            "If things are completely apart, even if they should be neighbors, it's actually another problem and This is why the unconstrained GP LVM tends to separate things.",
            "When you use these, this is still a projection of the highly emotional space, and I mean everything.",
            "Yes, that's correct.",
            "Being here and you deleted that video game over this data, yes.",
            "Perform and only a.",
            "Right, so two things.",
            "So if you do it in an unconstrained way, if you just do it as I was sort of showing before, then you get things, then you might get here a picture where you have a little Patch of trees here a little Patch of trees here.",
            "Maybe there's a Patch of fives in between and stuff like that so you're unable to keep sort of.",
            "You're unable to follow the high dimensional manifold.",
            "You can sort of get cut in pieces OK.",
            "However, if you constrain it at the same time, perhaps in a nonlinear way, as this is sort of the talk that I didn't give today.",
            "Was where you constrain it sort of non linearly.",
            "Then you get very nice thing.",
            "Then this is sort of the way we obtained this one as opposed to.",
            "As opposed to this one, this would be an unconstrained GP LVM and this would be sort of the.",
            "The one that I didn't talk about, sort of the one that is constrained in a nonlinear way.",
            "Appreciate.",
            "Now here I initialize at random.",
            "So so, so here there's no.",
            "Alright, OK, OK so you're So what you're asking me about is OK if I initialize at random I get a catastrophe.",
            "I get something that looks much worse than this.",
            "So GP LVM you cannot initialize at random.",
            "But not even a notice a also.",
            "OK, then how?",
            "Can you initially said with the base model Camry?",
            "I mean, I condition initialized using Isomap, using Ellie, or using some other things, but that's not.",
            "It's not going to stay there.",
            "Oh, actually damn it.",
            "I think I oh, I had another.",
            "I had another example.",
            "I don't know if we should take this discussion offline.",
            "I mean, I hope that there.",
            "Perhaps those people are sort of wanting to go for lunch or something.",
            "If you want.",
            "I mean, I'm happy to just spare the lives of other people.",
            "You have questions.",
            "Excellent question, yes, I'm happy about that question.",
            "OK, so so to obtain kernel PCA?",
            "So kernel PCA is a similarity preserving measure because you have a smooth mapping the other way around.",
            "So you have a smooth mapping that goes from high dimensional.",
            "So basically it's taking a GP LVM and turning it on his head.",
            "And actually if you sort of maximize the evidence of that trying to put some constraint that prevents the trivial sort of everything is O solution.",
            "You would get you would actually get kernel PCA.",
            "As a as a sort of as a solution.",
            "Sorry.",
            "Yes.",
            "OK, yes.",
            "Yes, PC asymmetric is.",
            "Yes, yes.",
            "Yes, exactly so this is actually very reminiscent of autoencoders and this kind of models where you go from the data you sort of predict onto a sort of hidden layer, and then you try to re predict your data from there.",
            "You could sort of unfold this.",
            "The second quarter reconstruction.",
            "Correct?",
            "Yes.",
            "That's correct yes, yes.",
            "Later, yes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry for the delay, so this is the first time.",
                    "label": 0
                },
                {
                    "sent": "And it's not a cheap excuse that this laptop doesn't doesn't work when I. Plug it to a beamer.",
                    "label": 0
                },
                {
                    "sent": "So I'm gonna try and and show you the demos which I have like this OK, and then if you're if you're interested then you can maybe sort of gather around right after the talk and I can show you the demos so the talk is going to be fairly brief 'cause it did rely sort of heavily on on demos since we're going to have lots of time then I'm going to give you a very exciting talk about affiliations.",
                    "label": 0
                },
                {
                    "sent": "Does anyone have a laser pointer or does this thing?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "OK, so my name is cooking milk and Ella.",
                    "label": 0
                },
                {
                    "sent": "I am now visiting the Carlos after University in Madrid.",
                    "label": 0
                },
                {
                    "sent": "I am really at the Technical University of Berlin and that hungover 1st and from January this year you will be able to find me at Microsoft Research in Cambridge.",
                    "label": 1
                },
                {
                    "sent": "This is John work with Neil Lawrence and Cosmos and both of them are also moving, so Neil will go from Sheffield to Manchester.",
                    "label": 0
                },
                {
                    "sent": "That's not a very long trip to make.",
                    "label": 0
                },
                {
                    "sent": "And Carson will actually join in Cambridge, which is which is great, except he'll be at Cambridge University.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to try to tell you about.",
                    "label": 0
                },
                {
                    "sent": "About linear projections and Gaussian process reconstructions, that's actually the actual talk after this very exciting talk on affiliations.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so first of all, I had forgotten this one.",
                    "label": 0
                },
                {
                    "sent": "First of all, some acknowledgement, so we'd like to thank Pascal once more.",
                    "label": 0
                },
                {
                    "sent": "You never think Pascal enough for a visit day they funded, which I paid to new Lawrence in the summer last year, where actually the idea of the back constraints, which will tell you about was cooked up.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, consider the following situation.",
                    "label": 0
                },
                {
                    "sent": "I am sure all of you do data modeling so you find yourselves in this situation quite often.",
                    "label": 0
                },
                {
                    "sent": "Where you have some sort of a fairly uncomfortably high dimensional data out there, and you'd like to look at your data right so?",
                    "label": 0
                },
                {
                    "sent": "Data we're going to call here Y alright so have this sort of matrix capital Y and is there a stick or a cane or anything?",
                    "label": 0
                },
                {
                    "sent": "I have my stick.",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't have graduates, so this is all I can.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a good old robust sexually not, not, not really long enough, so I think that's OK. Alright, so as you can see I have a half capital N vectors in a quite high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The dimension is capital D alright, and what I'm searching is for a low dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "Alright so I'll have.",
                    "label": 0
                },
                {
                    "sent": "Capital N vectors in my low dimensional space as well and here the dimension is going to be Q alright and the idea of course is that Q is fairly small, right?",
                    "label": 0
                },
                {
                    "sent": "Say Q = 2 so that I can actually look at my data right OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One thing that most of you would do first without even thinking about it would probably be to search for a linear projection.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you need to make a linear projection where you need?",
                    "label": 0
                },
                {
                    "sent": "You need a matrix capital P which is of size Q * D, right?",
                    "label": 1
                },
                {
                    "sent": "If you let your data lived in 100 dimensional space and you want to look at it into 2D space, you need to sort of come up with the 2 * 100 matrix, which you're going to use a projector data.",
                    "label": 0
                },
                {
                    "sent": "Now what I would do first and I guess.",
                    "label": 0
                },
                {
                    "sent": "It would be the same for you would be to actually search for a principle component, sort of the composition of your of your sort of data in order to make the projection.",
                    "label": 0
                },
                {
                    "sent": "And as you all know what you would do, you would build this covariance matrix here.",
                    "label": 0
                },
                {
                    "sent": "Why terms Y transpose OK?",
                    "label": 1
                },
                {
                    "sent": "Which, as you can see is a D times the object and you would sort of search for the first Q.",
                    "label": 0
                },
                {
                    "sent": "First, Q eigenvectors in here OK and you of course have carefully removed the mean in here first, alright to get a covariance matrix?",
                    "label": 0
                },
                {
                    "sent": "OK now.",
                    "label": 0
                },
                {
                    "sent": "What does PCA do?",
                    "label": 0
                },
                {
                    "sent": "What what PCA does and I want to reduce the term reconstruction OK, what PC does it says?",
                    "label": 0
                },
                {
                    "sent": "Well, assume.",
                    "label": 0
                },
                {
                    "sent": "Fair enough, you want to have these two dimensional space here where you're going to look at your data.",
                    "label": 0
                },
                {
                    "sent": "Now you could look at this in a different way.",
                    "label": 0
                },
                {
                    "sent": "You could say well.",
                    "label": 0
                },
                {
                    "sent": "I want to I want to reconstruct linearly from my 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Alright, I want to be able to sort of recreate my high dimensional points OK and I want to do this linearly.",
                    "label": 1
                },
                {
                    "sent": "OK so PCA and I suppose this is no surprise to any of you what it achieves actually is it achieves a minimum mean squared reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "Sort of knowing that you're going to do things linearly.",
                    "label": 0
                },
                {
                    "sent": "OK, it gets you the best linear projection, knowing that you're going to be reconstructing linearly.",
                    "label": 0
                },
                {
                    "sent": "Alright, in the optimal in the mean squared sense.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's talk a bit more about.",
                    "label": 0
                },
                {
                    "sent": "Reconstructions here OK so so I just said that I'm going to be reconstructing linearly now.",
                    "label": 0
                },
                {
                    "sent": "Obviously any sort of reconstructed high dimensional object from my my embedding is going to be a composition of the first Q eigenvectors that I chose.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to be spanning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be spending my high dimensional sort of manifold by a handful of vectors, OK?",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you a figure in a moment.",
                    "label": 0
                },
                {
                    "sent": "So very often this is a very poor reconstruction actually, and this is also not surprised because you're approximating some some sort of manifold just by just by a hyperplane.",
                    "label": 1
                },
                {
                    "sent": "OK of dimension Q in your D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you might argue that most dimensional reduction methods don't really even offer you a mapping to go back from your low dimensional space to the sort of high dimensional space, OK?",
                    "label": 1
                },
                {
                    "sent": "So let's use an example.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you all know the handwritten digits database.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a bunch of sort of handwritten digits scan onto images which are 16 * 16.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we're going to do now?",
                    "label": 0
                },
                {
                    "sent": "Is we're going to look at?",
                    "label": 0
                },
                {
                    "sent": "We're going to sort of perform an embedding of those data using PCA onto a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, now what I've done is after doing the embedding OK, I have labeled the little points with the actual digit they correspond to all right?",
                    "label": 0
                },
                {
                    "sent": "And these are these.",
                    "label": 0
                },
                {
                    "sent": "Are these points in here?",
                    "label": 0
                },
                {
                    "sent": "Alright, what I have chosen is I have chosen a bunch of numbers.",
                    "label": 0
                },
                {
                    "sent": "I have chosen twos, threes, fours and fives and just a handful of them, about 100 of each.",
                    "label": 0
                },
                {
                    "sent": "You can see there's a fair amount of overlapping between the three is in the fives Now, what do our eigenvectors look like in this case?",
                    "label": 0
                },
                {
                    "sent": "Well, if I if I had been careful enough.",
                    "label": 0
                },
                {
                    "sent": "I would have.",
                    "label": 0
                },
                {
                    "sent": "I would have added the origin which is about here.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the origin of this figure.",
                    "label": 0
                },
                {
                    "sent": "OK so you can see that in this axis here what I'm doing is I'm using one eigenvector which is actually this guy.",
                    "label": 0
                },
                {
                    "sent": "You can see it here in positive and you can see it pretty much here in negative OK. And basically the other eigenvector is sort of proportional to this guy here, which looks sort of strangely between three and five.",
                    "label": 0
                },
                {
                    "sent": "OK, these are the directions of maximum variation in my space in my high dimensional Space, 16 * 16.",
                    "label": 0
                },
                {
                    "sent": "Of digits.",
                    "label": 0
                },
                {
                    "sent": "OK, now if I wanted to reconstruct OK and I went in my 2 dimensional space and I said, well, you know, generate whichever digit sort of corresponds to this location in my low dimensional embedding I would get sort of the little cross here OK, and so on for the other four points alright?",
                    "label": 0
                },
                {
                    "sent": "Now, that's not very, very exciting.",
                    "label": 0
                },
                {
                    "sent": "Is it as a reconstruction?",
                    "label": 0
                },
                {
                    "sent": "But here in this column I have shown you a reconstruction obtained by other means, which looks a lot cooler, doesn't it?",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do now is I'm going to tell you a bit about how I managed to obtain this reconstruction here, OK?",
                    "label": 0
                },
                {
                    "sent": "So GP stands for Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "To obtain that reconstruction, the idea is fair enough.",
                    "label": 0
                },
                {
                    "sent": "I have projected linearly OK, but now I actually have a set of pairs of X&Y's.",
                    "label": 1
                },
                {
                    "sent": "OK, so I can actually pose a regression problem.",
                    "label": 1
                },
                {
                    "sent": "Can't I write?",
                    "label": 0
                },
                {
                    "sent": "I have inputs and outputs earlier I only had sort of data, right?",
                    "label": 0
                },
                {
                    "sent": "So now basically what I can do is I can fit a nonlinear a flexible, powerful nonlinear regression method that basically learns how to go from the low dimensional to the high dimensional data space, OK?",
                    "label": 0
                },
                {
                    "sent": "And the axis the projections have been given, for example by PCA, OK?",
                    "label": 0
                },
                {
                    "sent": "Everybody is happy with that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the method I would choose.",
                    "label": 0
                },
                {
                    "sent": "Because of my past would be to use Gaussian processes OK.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to try to tell you in two slides about Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "And Bayesian regression with Gaussian process is OK if you're going to do.",
                    "label": 1
                },
                {
                    "sent": "Bayesian regression.",
                    "label": 0
                },
                {
                    "sent": "What is the first thing that you need?",
                    "label": 0
                },
                {
                    "sent": "Well, you need a prior over functions OK?",
                    "label": 0
                },
                {
                    "sent": "Now suppose you were able to write a prior distribution over functions OK. Now you can draw samples from this prior and this is what is shown on the figure top left.",
                    "label": 0
                },
                {
                    "sent": "These are simply some samples out of a Gaussian process prior.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll tell you in the next slide how you build a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Just think a prior over functions.",
                    "label": 0
                },
                {
                    "sent": "Now the next thing that happens is that you actually see some data OK, and now we're going to do a sort of a 1 dimensional to 1 dimensional regression problem.",
                    "label": 0
                },
                {
                    "sent": "Now we see three data points.",
                    "label": 0
                },
                {
                    "sent": "OK, now what you have to do is you have to come up with the likelihood or with the noise model.",
                    "label": 0
                },
                {
                    "sent": "OK, how much do you trust the data?",
                    "label": 0
                },
                {
                    "sent": "And let's assume here you sort of decide that you decide that the noise is uniform and it's centered around the data point that you observe around the Y. OK, so you see this little.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I can climb that up.",
                    "label": 0
                },
                {
                    "sent": "You see, these two little parallel bars around around the data point that basically shows you the noise distribution OK, which is centered around the way you have observed, alright?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The next thing in the Bayesian sort of a machinery is to look at to obtain your posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Your posterior over functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do you do that?",
                    "label": 0
                },
                {
                    "sent": "Well, you can apply Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "In this sort of toy example, you can.",
                    "label": 0
                },
                {
                    "sent": "You can apply it sort of very intuitively.",
                    "label": 0
                },
                {
                    "sent": "You could you could obtain samples from your posterior by doing the following.",
                    "label": 0
                },
                {
                    "sent": "You take samples from your prior alright, and for each sample you take.",
                    "label": 0
                },
                {
                    "sent": "You check whether it satisfies.",
                    "label": 0
                },
                {
                    "sent": "Your likelihood assumption.",
                    "label": 0
                },
                {
                    "sent": "Does it go through all three little tunnels OK?",
                    "label": 0
                },
                {
                    "sent": "Because I have a uniform noise distribution, meaning if it goes outside one of those sort of little intervals, then the likelihood is going to be 0 so that that sample has no probability under my posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically that's the game and I do that lots of times and actually to obtain the 25 samples from the posterior that I'm showing you in the middle, I had to sample 60,000 samples from my posterior at random.",
                    "label": 0
                },
                {
                    "sent": "It seems like it doesn't seem like the most clever way of doing things, but it works.",
                    "label": 0
                },
                {
                    "sent": "And actually, that's not really the way you would do it in practice.",
                    "label": 0
                },
                {
                    "sent": "This is just to illustrate Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "Now, once I have enough samples from my posterior, these samples represent my knowledge right given my prior sort of some of the hypothesis from my prior have been restricted by the data I have observed.",
                    "label": 0
                },
                {
                    "sent": "OK, I can actually summarize the information I have.",
                    "label": 0
                },
                {
                    "sent": "So if someone asks me to predict somewhere, let's say someone asks me, what do you think would be the value of this function at zero?",
                    "label": 0
                },
                {
                    "sent": "OK, well, what I can do is I can go to my posterior sort of look at zero, and I could sort of I could sort of provide some summary OK?",
                    "label": 0
                },
                {
                    "sent": "I could decide to give sort of the empirical mean and the empirical variance at that point OK computed from the samples from the posterior, so that's it.",
                    "label": 1
                },
                {
                    "sent": "That's based on regression.",
                    "label": 0
                },
                {
                    "sent": "It's as easy as that.",
                    "label": 0
                },
                {
                    "sent": "Of course there are some.",
                    "label": 0
                },
                {
                    "sent": "There are some subtleties like and this is a question that everybody should sort of be asking immediately.",
                    "label": 0
                },
                {
                    "sent": "Now, how do I choose my prior and stuff like that, alright?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If you if you sort of go the hardcore basin way.",
                    "label": 0
                },
                {
                    "sent": "You define sort of hyperparameters on hyper priors on anything you sort of.",
                    "label": 0
                },
                {
                    "sent": "You're not willing to give a value to OK. Or you can Alternatively maximize the evidence so I can also tell you from these figures what the evidence would be here.",
                    "label": 0
                },
                {
                    "sent": "So the evidence is also known as a marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "What it is, it's the normalizing constant of the posterior.",
                    "label": 0
                },
                {
                    "sent": "Now I have chosen this noise model here because it's very easy to compute the normalizing constant of the posterior.",
                    "label": 0
                },
                {
                    "sent": "Here you know what it is, it's simply the ratio of the functions that were accepted from the prior to the total number of functions that I sampled.",
                    "label": 0
                },
                {
                    "sent": "So the evidence here would be 25 /, 60,000 in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, you can see that if I had chosen a prior that was less appropriate that generated more crazy functions, it would be less likely that a function makes it through the likelihood and therefore the evidence would be lower OK.",
                    "label": 0
                },
                {
                    "sent": "So in a way, the evidence sort of makes sure that you don't choose priors that allow too crazy things because things have to normalize, OK?",
                    "label": 0
                },
                {
                    "sent": "We can talk about that later, over coffee or.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now, how did I build my prior?",
                    "label": 0
                },
                {
                    "sent": "Well, it's called a Gaussian process, so it has.",
                    "label": 0
                },
                {
                    "sent": "It's got to have a Gaussian in it, doesn't it?",
                    "label": 0
                },
                {
                    "sent": "Let's consider a joint prior over 2 function values evaluated at XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I'm directly building a prior on function values.",
                    "label": 0
                },
                {
                    "sent": "OK, now you see the joint prior over 2 function values would be a sort of a joint multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The mean has been set to 0 here, but this is really not limiting.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't worry too much about that.",
                    "label": 0
                },
                {
                    "sent": "You can define other sort of sort of.",
                    "label": 0
                },
                {
                    "sent": "Mean functions.",
                    "label": 0
                },
                {
                    "sent": "Now the very important thing.",
                    "label": 0
                },
                {
                    "sent": "Of course, here the only thing we've got left to define our model is a covariance, and the covariance is obtained from the covariance function.",
                    "label": 0
                },
                {
                    "sent": "In this case, for those of you sort of into kernels, kernel in general, a semi definite kernel is a is a valid covariance function as well.",
                    "label": 0
                },
                {
                    "sent": "OK, you can see that the key thing is that the covariance function.",
                    "label": 0
                },
                {
                    "sent": "What it tells me is how similar and in which way should function values B as a function of the corresponding inputs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is going to tell me if I take here the so-called squared exponential or Gaussian covariance function.",
                    "label": 0
                },
                {
                    "sent": "You can see that the covariance between the two function values is going to depend on how far away the inputs are.",
                    "label": 0
                },
                {
                    "sent": "If the inputs are far away enough, this is going to be 0 and then the two function values are free to do whatever they want.",
                    "label": 0
                },
                {
                    "sent": "If they are close together, they're not free to do whatever they want.",
                    "label": 0
                },
                {
                    "sent": "They have to sort of be similar.",
                    "label": 0
                },
                {
                    "sent": "Now what I have illustrated here are functions run at random from sort of three different covariance functions.",
                    "label": 0
                },
                {
                    "sent": "The two first ones are the squared exponential with two different sort of length scale values, so the length scale would be this thing which tells you how quickly do I forget things.",
                    "label": 0
                },
                {
                    "sent": "If this is very large, I'm going to have a very long memory.",
                    "label": 0
                },
                {
                    "sent": "If this is very small, things are going today very quickly, and this is a completely different covariance function.",
                    "label": 0
                },
                {
                    "sent": "This is a neural network covariance function.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's almost it for Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Now the noise model I told you about earlier.",
                    "label": 0
                },
                {
                    "sent": "Sort of forced us to sample to obtain samples from the posterior.",
                    "label": 0
                },
                {
                    "sent": "We could not compute the posterior analytically with that with that sort of uniform noise model.",
                    "label": 0
                },
                {
                    "sent": "However, one quite neat thing is that if we choose instead an independent Gaussian noise model, OK, then if you look at if you look at things, the posterior is going to be actually the product of the likelihood terms of prior.",
                    "label": 1
                },
                {
                    "sent": "Now this object here is a product of two Gaussians, right?",
                    "label": 0
                },
                {
                    "sent": "So the product of two Gaussians has has a very nice property.",
                    "label": 0
                },
                {
                    "sent": "The product of two Gaussians is Gaussian and it has a Gaussian normalizing constant, which means the posterior Gaussian and the evidence is also Gaussian and the predictive distribution is also Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So I'm not going to go much into the details here.",
                    "label": 0
                },
                {
                    "sent": "I want to I want to point out the fact that you can analytically compute the evidence here because we're going to use that later, OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So until now, what I have told you is that I could use Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "To learn this mapping between the X is otherwise OK and the axis.",
                    "label": 0
                },
                {
                    "sent": "Had been given OK, they had been given by some linear projection.",
                    "label": 0
                },
                {
                    "sent": "Now the question is.",
                    "label": 0
                },
                {
                    "sent": "Could I actually could I actually forget about this linear projection that I was doing earlier and directly use this Gaussian processes to actually learn the location of the low dimensional variables.",
                    "label": 0
                },
                {
                    "sent": "Well, yes I can.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what was proposed by Neil Lawrence.",
                    "label": 0
                },
                {
                    "sent": "Sort of recently, what he called the Gaussian process latent variable model.",
                    "label": 1
                },
                {
                    "sent": "So here's the idea.",
                    "label": 1
                },
                {
                    "sent": "The Gaussian processes have shown you are able to do a 1 dimensional output regression.",
                    "label": 0
                },
                {
                    "sent": "OK, now what I'm going to do is if I had 100 dimensions in my head dimensional data space, I'm going to take 100 Gaussian processes and each of them is going to predict sort of the high dimensional data along one of the dimensions.",
                    "label": 1
                },
                {
                    "sent": "OK, now the thing that's going to be common to our models are the inputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so all 100.",
                    "label": 0
                },
                {
                    "sent": "Gaussian processes in order to predict this point here in high dimensional space, they have to share the same input in my 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So there are coupled by the inputs.",
                    "label": 0
                },
                {
                    "sent": "They're not really totally independent, OK?",
                    "label": 0
                },
                {
                    "sent": "Now the crazy idea here is.",
                    "label": 0
                },
                {
                    "sent": "That you do regression in a way where you sort of give the targets and you ask the model to learn the inputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so this might sound like a completely unreasonable and crazy idea.",
                    "label": 0
                },
                {
                    "sent": "Well, actually it sort of is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to show you here.",
                    "label": 0
                },
                {
                    "sent": "Here's where the difficulties come right?",
                    "label": 0
                },
                {
                    "sent": "I was going to show you an example on motion capture data to give you an impression of a high dimensional manifold, and I'm going to try to.",
                    "label": 1
                },
                {
                    "sent": "I'm going to try to do that somehow.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see this, but let me just re run it to see if you can see it.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what you can see.",
                    "label": 0
                },
                {
                    "sent": "So motion capture data.",
                    "label": 0
                },
                {
                    "sent": "You can see a little little man sort of running, and there's a number of markers attached to him to his body.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a total of 50.",
                    "label": 0
                },
                {
                    "sent": "Quite remember 30 something markers OK, and each marker gives you a 3 dimensional vector, namely the position.",
                    "label": 1
                },
                {
                    "sent": "OK, the 3D position, so the data is about 100 dimensional.",
                    "label": 0
                },
                {
                    "sent": "OK, now obviously you would suspect that this data set cannot fill the 100 dimensional space right?",
                    "label": 0
                },
                {
                    "sent": "Because because when you're running.",
                    "label": 0
                },
                {
                    "sent": "There's actually only a few variables that are determining your whole sort of act of running OK. Now let's see if I use the Gaussian process latent variable model carefully initialized by PCA.",
                    "label": 0
                },
                {
                    "sent": "So I perform initialization of my axis with PCA, and then I sort of maximize the evidence of the Gaussian processes with respect to the inputs.",
                    "label": 1
                },
                {
                    "sent": "This is what I get.",
                    "label": 0
                },
                {
                    "sent": "I get this embedding here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to explain it to you.",
                    "label": 0
                },
                {
                    "sent": "So what I have done is.",
                    "label": 0
                },
                {
                    "sent": "The little crosses, which you probably cannot see, represent the sort of all the frames.",
                    "label": 0
                },
                {
                    "sent": "OK, so all the data points in high dimension space, they have been embedding embedded there and then I have connected everything with the solid line just to show you the course of time, OK?",
                    "label": 0
                },
                {
                    "sent": "So this information was not given to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Time was not given to it, but as you can imagine, this is a movie with a handful of frames, OK?",
                    "label": 0
                },
                {
                    "sent": "Now the cool thing here is that I can be tough.",
                    "label": 0
                },
                {
                    "sent": "I can go in here.",
                    "label": 0
                },
                {
                    "sent": "And I can sort of move my mouse and I can.",
                    "label": 0
                },
                {
                    "sent": "I can generate new high dimensional objects right?",
                    "label": 0
                },
                {
                    "sent": "Because I have.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I have some sort of a regression mechanism.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the.",
                    "label": 0
                },
                {
                    "sent": "The angle has been sort of encoded in this direction here and here.",
                    "label": 0
                },
                {
                    "sent": "You sort of have different sort of phases of the cycle of running now.",
                    "label": 0
                },
                {
                    "sent": "This embedding is a bit bit strange, and it's perhaps not the most satisfying embedding you could have dreamt of alright.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, that's because there are points like like this point here.",
                    "label": 0
                },
                {
                    "sent": "At this point there they belong very close together, because these are two consecutive frames.",
                    "label": 0
                },
                {
                    "sent": "But somehow they have been embedded quite far apart, right?",
                    "label": 0
                },
                {
                    "sent": "And this happens again and again in this embedding.",
                    "label": 0
                },
                {
                    "sent": "What is 1 nice thing about this embedding is that it doesn't overlap or collapse.",
                    "label": 0
                },
                {
                    "sent": "It doesn't embed things that were different in data space, it doesn't embed them together in here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's nice.",
                    "label": 0
                },
                {
                    "sent": "What's not nice is that it's breaking similarities.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's try to understand a bit more why this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me let me in general.",
                    "label": 0
                },
                {
                    "sent": "So before we come to that, I'll say that as you can imagine, I'm learning the inputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so if my low dimensional space was a dimension Q and I had any data points, I have an optimization problem which is N * Q, that's big.",
                    "label": 0
                },
                {
                    "sent": "On top of that, because I'm doing the crazy thing of giving you some targets and asking you OK, just find me some inputs for this target using your regression model you have loads of local minima alright.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that because we sort of throw away our linear mapping from data to latent.",
                    "label": 1
                },
                {
                    "sent": "I I don't have an explicit mapping anymore, right?",
                    "label": 0
                },
                {
                    "sent": "Going from data space to latent?",
                    "label": 1
                },
                {
                    "sent": "And now the thing that I wanted to tell you about is that the Gaussian process latent variable model is not similarity preserving.",
                    "label": 0
                },
                {
                    "sent": "So for all of you who know about locally linear embedding.",
                    "label": 0
                },
                {
                    "sent": "So LLE Isomap any of those nonlinear dimensionality reduction methods, what they do is that they basically instead of of sort of minimizing the linear reconstruction costs, OK, they minimize something like that.",
                    "label": 0
                },
                {
                    "sent": "But they say you know in the sum of the squared error terms I'm going to pay a lot more attention to those distances that correspond to close things in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And I don't care so much about the guys that are far apart.",
                    "label": 0
                },
                {
                    "sent": "OK and this is what allows you to have a nonlinear sort of embedding, OK?",
                    "label": 0
                },
                {
                    "sent": "And that's what I would call similarity preserving.",
                    "label": 0
                },
                {
                    "sent": "OK, at the risk of sometimes embedding things that are far apart close together.",
                    "label": 0
                },
                {
                    "sent": "OK, now the GPL VM does exactly the opposite.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, as you saw from the from, the functions that I draw from my Gaussian process prior, if two inputs are close together for a Gaussian process, then the two corresponding outputs are also necessarily close together.",
                    "label": 0
                },
                {
                    "sent": "Now what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that.",
                    "label": 0
                },
                {
                    "sent": "If I have two outputs or two high dimensional objects which are very dissimilar, they cannot be embedded as two inputs that are close together.",
                    "label": 0
                },
                {
                    "sent": "This is a bit like A implies B implies implies no way alright.",
                    "label": 0
                },
                {
                    "sent": "did I say correctly?",
                    "label": 0
                },
                {
                    "sent": "Well, you know what I mean?",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "You cannot have.",
                    "label": 0
                },
                {
                    "sent": "You cannot have because two inputs must generate two inputs that are similar, must generate 2 high dimensional objects which are similar.",
                    "label": 0
                },
                {
                    "sent": "Then it cannot be that two dissimilar objects.",
                    "label": 0
                },
                {
                    "sent": "Our embedded onto 2 close together points OK and This is why the GP LVM what it does.",
                    "label": 0
                },
                {
                    "sent": "It loves to separate things.",
                    "label": 0
                },
                {
                    "sent": "That's his his big thing right?",
                    "label": 0
                },
                {
                    "sent": "He just separate things.",
                    "label": 1
                },
                {
                    "sent": "Now the problem with that is that you would lose you sort of lose your the structure.",
                    "label": 0
                },
                {
                    "sent": "The local structure of your manifold.",
                    "label": 0
                },
                {
                    "sent": "Imagine your data was contained onto a scarf that was sort of floating in the wind.",
                    "label": 0
                },
                {
                    "sent": "OK, now what the GP LV is going to do?",
                    "label": 0
                },
                {
                    "sent": "It has no problems in shredding your scarf into pieces and just embedding little patches here and there.",
                    "label": 0
                },
                {
                    "sent": "It's not gonna.",
                    "label": 0
                },
                {
                    "sent": "It's not going to sort of collapse apart of the scarf.",
                    "label": 0
                },
                {
                    "sent": "That was far apart from another one together that is not going to do.",
                    "label": 0
                },
                {
                    "sent": "Which is something that most of the other methods actually will do.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's something interesting in this in this property of being the similarity preserving, but it's like something which is at the same time good and bad.",
                    "label": 0
                },
                {
                    "sent": "OK, I put here in this slide that another advantage is that sometimes when you have a very noisy manifold, some of these methods that focus on preserving similarity of course are going to be a bit sort of in difficulty, right?",
                    "label": 0
                },
                {
                    "sent": "So what does what does Neil Lawrence?",
                    "label": 0
                },
                {
                    "sent": "How did?",
                    "label": 0
                },
                {
                    "sent": "How did the Lawrence manage to get his paper accepted at nips right?",
                    "label": 0
                },
                {
                    "sent": "Well, because he initialized very carefully using PCA alright?",
                    "label": 0
                },
                {
                    "sent": "And that sort of and that sort of work, 'cause if you if you set up give it sort of the right inputs, then the model doesn't get completely lost, right?",
                    "label": 0
                },
                {
                    "sent": "It's not going to try to do something ridiculous.",
                    "label": 0
                },
                {
                    "sent": "OK, is just if you initialize it at random, there's no way OK.",
                    "label": 0
                },
                {
                    "sent": "Which is why I sort of said to me that I thought his model sucked when I visited him in in Sheffield.",
                    "label": 0
                },
                {
                    "sent": "We were friends.",
                    "label": 0
                },
                {
                    "sent": "Actually I can say to him things like that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now, so I chose to sort of combine the GP LVM with linear projections here OK.",
                    "label": 0
                },
                {
                    "sent": "There might be a sort of a symbiosis here, right?",
                    "label": 0
                },
                {
                    "sent": "You might think they need each other right?",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "It would seem that that if you're projecting linearly, you don't want to reconstruct linearly.",
                    "label": 0
                },
                {
                    "sent": "If you want to sort of generate sort of nice high dimensional objects, OK, so in that sense linear projections need Gaussian processes, OK, but the GP LVM needs some sort of PCA initialization anyway, so then want to do something different.",
                    "label": 1
                },
                {
                    "sent": "There might be a better linear projection sort of for Gaussian processes reconstruction, right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "The PCA projection is optimal if you're going to be reconstructing linearly.",
                    "label": 0
                },
                {
                    "sent": "OK, but you're not going to be reconstructing linearly.",
                    "label": 0
                },
                {
                    "sent": "You're going to be reconstructing using Gaussian processes, so you want to find the linear projection, which is optimal.",
                    "label": 0
                },
                {
                    "sent": "Knowing that you're going to be reconstructing using Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you can do is very simple, and actually you solve quite a few problems just at once.",
                    "label": 0
                },
                {
                    "sent": "If you say, well, you know what I don't really feel like learning the whole inputs X, right?",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is climb on the chair I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of.",
                    "label": 0
                },
                {
                    "sent": "Change my problem and I'm going to say, well, the axis cannot just be whatever they want, they exist, must be a linear projection of the wise OK?",
                    "label": 0
                },
                {
                    "sent": "And now my parameter.",
                    "label": 0
                },
                {
                    "sent": "My free parameter is Pete, the projection matrix.",
                    "label": 0
                },
                {
                    "sent": "Now this is nice because queue times D is probably a lot smaller than Q * N. OK, so if I have more data points than the dimension of my high dimensional manifold, then this is a smaller optimization problem and it's also an optimization problem which is very very heavily constrained and I can tell you that I can now initialize at random and it works very nicely.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the other things that I was going to show you is how does it work?",
                    "label": 0
                },
                {
                    "sent": "And actually I think I pretty much have no more slides with text.",
                    "label": 0
                },
                {
                    "sent": "I think everything that comes in our figures.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's go back to our motion capture data example.",
                    "label": 0
                },
                {
                    "sent": "How would the PCA projection look like of that little running guy?",
                    "label": 0
                },
                {
                    "sent": "OK, well this is how it would look like.",
                    "label": 0
                },
                {
                    "sent": "Let me find it.",
                    "label": 0
                },
                {
                    "sent": "Here is OK.",
                    "label": 0
                },
                {
                    "sent": "This is how it would look like.",
                    "label": 0
                },
                {
                    "sent": "What you know, you might argue.",
                    "label": 0
                },
                {
                    "sent": "Well, this is fantastic.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice embedding, so why do I need to bother with all this crap?",
                    "label": 0
                },
                {
                    "sent": "Will be talking talking about is it a very nice embedding?",
                    "label": 0
                },
                {
                    "sent": "I don't know, so this would be taking the first 2 sort of principle components.",
                    "label": 0
                },
                {
                    "sent": "OK, the first 2 eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Now, why is it maybe not so nice as it may seem?",
                    "label": 0
                },
                {
                    "sent": "Well, the nice part is you can see that it seems to have captured some cyclic structure, which makes sense if you're running.",
                    "label": 0
                },
                {
                    "sent": "You're performing a cyclic movement.",
                    "label": 0
                },
                {
                    "sent": "OK, now what is perhaps not so satisfying about this?",
                    "label": 0
                },
                {
                    "sent": "If I manage to use my left hand for this and move this guy, you can see if I now go onto a 3D, you can see it actually all these points here.",
                    "label": 0
                },
                {
                    "sent": "If I add one more, one more principal component you can see on 2D.",
                    "label": 0
                },
                {
                    "sent": "Using the two first principle components I had sort of collapsed a lot of points which are actually different.",
                    "label": 0
                },
                {
                    "sent": "I had sort of collapse them together.",
                    "label": 0
                },
                {
                    "sent": "OK, now if I use 3 dimensions, things look a lot better, alright?",
                    "label": 0
                },
                {
                    "sent": "You can also see that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this sort of this is where you can see right?",
                    "label": 0
                },
                {
                    "sent": "Is approximately using the two first components.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe this is not exactly what Gaussian processes will be happy about now.",
                    "label": 0
                },
                {
                    "sent": "What will Gaussian processes be happy about?",
                    "label": 0
                },
                {
                    "sent": "Well, let's see what happens if I now do this game of seeking for the linear projection, which is sort of optimal for Gaussian process reconstruction.",
                    "label": 0
                },
                {
                    "sent": "What I get is this.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry if you can see this.",
                    "label": 0
                },
                {
                    "sent": "I get this this sort of this sort of linear projection here, which looks very different from the previous one, and you can see there's no, there's no real overlapping of things so that it has searched for the linear projection, which tries to separate things the most, and I can.",
                    "label": 0
                },
                {
                    "sent": "Again I can again, of course play this nice game of going in here and sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, you can see that around here.",
                    "label": 0
                },
                {
                    "sent": "It has sort of found, sort of some cyclic structure.",
                    "label": 0
                },
                {
                    "sent": "It has used the radius to encode for the inclination of the runner OK.",
                    "label": 0
                },
                {
                    "sent": "This might this might not be sort of a perfect embedding either.",
                    "label": 0
                },
                {
                    "sent": "But for Gaussian process, for the purpose of Gaussian process reconstruction, it's a much better embedding.",
                    "label": 0
                },
                {
                    "sent": "OK, what comes next?",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back to the to the digits, so this is the.",
                    "label": 0
                },
                {
                    "sent": "This is a projection of the digits that I showed you earlier, so it's a linear projection.",
                    "label": 0
                },
                {
                    "sent": "OK, PCA.",
                    "label": 0
                },
                {
                    "sent": "Now what you see is that the three is under fives.",
                    "label": 0
                },
                {
                    "sent": "They're sort of collapsed onto into each other, and the reason for that is because one of the two eigenvalues looks a bit like it.",
                    "label": 0
                },
                {
                    "sent": "Sort of looks like between the three of the five.",
                    "label": 0
                },
                {
                    "sent": "Just removing the upper bar of the three under 5 right, which is on one on either side.",
                    "label": 0
                },
                {
                    "sent": "So this is the reason why they both get projected on top of each other.",
                    "label": 0
                },
                {
                    "sent": "Now what happens if I search for a linear projection that that sort of is optimal?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For Gaussian process reconstructions, well is not perfect, but you can see that it's a linear projection that at least seems to separate the three is in the fives a lot better, and the reason is because if I if I was using this embedding and I was so earlier when I was generating when I was reconstructing with the Gaussian process, I very carefully avoided to put a query point in here, right?",
                    "label": 0
                },
                {
                    "sent": "And that's because the GP doesn't really know what to do there, it's going to give you something which is between 3:00 and 5:00, right?",
                    "label": 0
                },
                {
                    "sent": "However, here is going to do a much better job.",
                    "label": 0
                },
                {
                    "sent": "And then finally, I think my last or almost less.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example is this one.",
                    "label": 0
                },
                {
                    "sent": "You know, perhaps this data set the Swiss roll.",
                    "label": 0
                },
                {
                    "sent": "Or the robust role if you will.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the example that was in the locally linear embedding in the elderly.",
                    "label": 0
                },
                {
                    "sent": "Paper, which is a science paper.",
                    "label": 0
                },
                {
                    "sent": "Imagine your data set looks like this.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I can manage to do this.",
                    "label": 0
                },
                {
                    "sent": "OK, perhaps that's why it's easier so.",
                    "label": 0
                },
                {
                    "sent": "Imagine you had this kind of data set.",
                    "label": 0
                },
                {
                    "sent": "This is called the Swiss roll.",
                    "label": 1
                },
                {
                    "sent": "OK, it's something you can eat.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Sort of a pastry that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Now this is intrinsically.",
                    "label": 0
                },
                {
                    "sent": "This is a 2 dimensional data set.",
                    "label": 0
                },
                {
                    "sent": "Alright?",
                    "label": 0
                },
                {
                    "sent": "I mean it's in 3D, but really if I manage to do it, probably this has, you know the data lives in a 2 dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Now if I did a PCA sort of embedding of this, why would I get well, where is where is the direction of maximum variation?",
                    "label": 0
                },
                {
                    "sent": "Where it's this one?",
                    "label": 0
                },
                {
                    "sent": "I would get this as one of the directions and you know.",
                    "label": 0
                },
                {
                    "sent": "A perpendicular to this, as the other one, so it would be basically as if I was sitting on my role.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a lot of overlapping of points.",
                    "label": 0
                },
                {
                    "sent": "And what does an what does this model do?",
                    "label": 0
                },
                {
                    "sent": "What it does?",
                    "label": 0
                },
                {
                    "sent": "What you can see there is still a linear projection, but it seems to be a linear projection.",
                    "label": 0
                },
                {
                    "sent": "I mean necessarily.",
                    "label": 0
                },
                {
                    "sent": "You're going to have overlapping, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's a linear projection, right?",
                    "label": 0
                },
                {
                    "sent": "But you see that it minimizes it.",
                    "label": 0
                },
                {
                    "sent": "You have it sort of along along this axis here.",
                    "label": 0
                },
                {
                    "sent": "OK, so it tries to separate things apart as much as possible.",
                    "label": 0
                },
                {
                    "sent": "And I think I'm sort of finishing now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I've told you about is I've told you about, sort of.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention the probabilistic part of it.",
                    "label": 0
                },
                {
                    "sent": "I forgot to say that.",
                    "label": 0
                },
                {
                    "sent": "OK, if you look at this figure again here, you have dark and sort of and white areas.",
                    "label": 0
                },
                {
                    "sent": "Actually, this guy here.",
                    "label": 0
                },
                {
                    "sent": "This is only the mean reconstruction, so I told you that if I if I have an input somewhere here.",
                    "label": 0
                },
                {
                    "sent": "What I have is I have see if this is 100 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I have 100 dimensional Gaussian processes, each of which give me a Gaussian predictive distribution.",
                    "label": 1
                },
                {
                    "sent": "They don't give me just a point here.",
                    "label": 0
                },
                {
                    "sent": "I'm showing the mean, but here there's an uncertainty associated to that and actually sort of the sum of the log uncertainties or a measure like that of overall confidence is sort of what is sort of what is used to obtain the shading here.",
                    "label": 0
                },
                {
                    "sent": "So in the white regions, the reconstruction model is quite certain about his Gaussian reconstruction in the dark areas is not so sure.",
                    "label": 0
                },
                {
                    "sent": "Which means that if you if you go and ask for a point in some areas, it seems that I don't have a nice example.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you get these examples where he's doing things that are impossible, right?",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter, because this is the mean of a Gaussian that has a very wide variance.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have this sort of probabilistic reconstructions.",
                    "label": 0
                },
                {
                    "sent": "Nice applications of this are of the fact that you can generate from your low dimensional space in computer sort of animated graphics.",
                    "label": 0
                },
                {
                    "sent": "There's this very cool example of people wanting to have a face that sort of looks more or less human and sort of talks to you or whatever in your computer.",
                    "label": 0
                },
                {
                    "sent": "So what you would do is you sort of record a number of data points of a person making different kind of characteristic faces.",
                    "label": 0
                },
                {
                    "sent": "Angry, sad, excited, whatever you want, OK?",
                    "label": 0
                },
                {
                    "sent": "And then what you do is that you embed those things onto a very low dimensional space of dimension 2, and you have this very powerful tool for reconstructing.",
                    "label": 0
                },
                {
                    "sent": "So you can actually slide your mouse in there and you can have the guy sort of make different facial expressions, right?",
                    "label": 0
                },
                {
                    "sent": "Some sort of, some sort of interpolation if you will between between those faces it's also been used as prior distributions for tracking.",
                    "label": 0
                },
                {
                    "sent": "OK, so for those of you who work on tracking, you know what I mean.",
                    "label": 0
                },
                {
                    "sent": "And I think I OK, yes yes.",
                    "label": 0
                },
                {
                    "sent": "So the last thing I wanted to say actually, I'm going to skip this one.",
                    "label": 0
                },
                {
                    "sent": "So if someone wants to ask this question, you're welcome to do it.",
                    "label": 0
                },
                {
                    "sent": "I was just putting it there in case you didn't have questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is a particular example.",
                    "label": 0
                },
                {
                    "sent": "A particular case what I've told you about is a particular case of the work that need learns, and I published at ICML this year where actually instead of having a linear mapping going from high dimensional data to the low dimensional space, we just have a neural network and RBF sort of network whatever you want.",
                    "label": 1
                },
                {
                    "sent": "We have a flexible model for going down as well.",
                    "label": 0
                },
                {
                    "sent": "Now if you do that.",
                    "label": 0
                },
                {
                    "sent": "You get nice things like like this.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of embeddings you get, which are which are sort of nicer even than than the other things right here.",
                    "label": 0
                },
                {
                    "sent": "You do manage to do both things.",
                    "label": 0
                },
                {
                    "sent": "You do manage to keep to preserve similarity, and you manage to preserve the similarity at the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, you can again play the little game over.",
                    "label": 0
                },
                {
                    "sent": "Of generating sort of stuff.",
                    "label": 0
                },
                {
                    "sent": "So here you have the cyclic.",
                    "label": 0
                },
                {
                    "sent": "I'm not so good with my left hand.",
                    "label": 0
                },
                {
                    "sent": "But you can.",
                    "label": 0
                },
                {
                    "sent": "You can see what I mean, right?",
                    "label": 0
                },
                {
                    "sent": "So it seems like quite nice.",
                    "label": 0
                },
                {
                    "sent": "Embedding and I think I am finished.",
                    "label": 0
                },
                {
                    "sent": "When you say the.",
                    "label": 0
                },
                {
                    "sent": "The first approach, with the results from the second approach.",
                    "label": 0
                },
                {
                    "sent": "Can you say it again, please?",
                    "label": 0
                },
                {
                    "sent": "The first part you said you were using PCA training.",
                    "label": 0
                },
                {
                    "sent": "I got some personal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yes the inputs yeah.",
                    "label": 0
                },
                {
                    "sent": "With the result of using the back of planes.",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't know if this is a question, but I'm going to try to answer it anyway.",
                    "label": 0
                },
                {
                    "sent": "Yes, so if you.",
                    "label": 0
                },
                {
                    "sent": "So perhaps I can go back to this to this slide.",
                    "label": 0
                },
                {
                    "sent": "Here let me see.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't quite know how this.",
                    "label": 0
                },
                {
                    "sent": "Operating system works.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this would be fine initialized by PCA, but my learning problem would be OK. Now I have a number of inputs and the corresponding little figure for each of those points are my targets for my regression problem and I trained GP OK. Now the performance I get which I can measure basically to evaluate performance.",
                    "label": 0
                },
                {
                    "sent": "What I can do?",
                    "label": 0
                },
                {
                    "sent": "I have a model that gives me probabilistic predictions where I can simply plug the truth under my predictive distribution and I get sort of get a density in there, right?",
                    "label": 0
                },
                {
                    "sent": "I can take logarithms to be able to take averages and stuff like that, so I have a measure to evaluate things.",
                    "label": 0
                },
                {
                    "sent": "Now this is going to get much worse score than this.",
                    "label": 0
                },
                {
                    "sent": "Which is a result of having sort of initialize of having sort of a couple the linear and the GP together.",
                    "label": 0
                },
                {
                    "sent": "Here they're sort of separate right, and the reason why this is worse is cause in this critical region in there where the trees in the fives overlap.",
                    "label": 0
                },
                {
                    "sent": "What a GP does, you can sort of very roughly, think of a GP, something that does smoothing is not entirely true, because it doesn't only do sort of interpolation between between things, it can do more than that, it can overshoot, and it can do other things, but.",
                    "label": 0
                },
                {
                    "sent": "The thing is, what it's going to use?",
                    "label": 0
                },
                {
                    "sent": "It is going to use it of the neighboring targets, but now here everything is mixed up.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's going to be very uncertain what it's going to do is going to predict you something that is a mixture between three and five in there and with very sort of big uncertainty.",
                    "label": 0
                },
                {
                    "sent": "However, here is going to be able to do a much better job because the trees in the fives are sort of more separate, right?",
                    "label": 0
                },
                {
                    "sent": "So the kind of embeddings you need so overlapping is not good if you're going to be using the GPU for reconstruction.",
                    "label": 0
                },
                {
                    "sent": "If things are completely apart, even if they should be neighbors, it's actually another problem and This is why the unconstrained GP LVM tends to separate things.",
                    "label": 0
                },
                {
                    "sent": "When you use these, this is still a projection of the highly emotional space, and I mean everything.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's correct.",
                    "label": 0
                },
                {
                    "sent": "Being here and you deleted that video game over this data, yes.",
                    "label": 0
                },
                {
                    "sent": "Perform and only a.",
                    "label": 0
                },
                {
                    "sent": "Right, so two things.",
                    "label": 0
                },
                {
                    "sent": "So if you do it in an unconstrained way, if you just do it as I was sort of showing before, then you get things, then you might get here a picture where you have a little Patch of trees here a little Patch of trees here.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a Patch of fives in between and stuff like that so you're unable to keep sort of.",
                    "label": 0
                },
                {
                    "sent": "You're unable to follow the high dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "You can sort of get cut in pieces OK.",
                    "label": 0
                },
                {
                    "sent": "However, if you constrain it at the same time, perhaps in a nonlinear way, as this is sort of the talk that I didn't give today.",
                    "label": 0
                },
                {
                    "sent": "Was where you constrain it sort of non linearly.",
                    "label": 0
                },
                {
                    "sent": "Then you get very nice thing.",
                    "label": 0
                },
                {
                    "sent": "Then this is sort of the way we obtained this one as opposed to.",
                    "label": 0
                },
                {
                    "sent": "As opposed to this one, this would be an unconstrained GP LVM and this would be sort of the.",
                    "label": 0
                },
                {
                    "sent": "The one that I didn't talk about, sort of the one that is constrained in a nonlinear way.",
                    "label": 0
                },
                {
                    "sent": "Appreciate.",
                    "label": 0
                },
                {
                    "sent": "Now here I initialize at random.",
                    "label": 0
                },
                {
                    "sent": "So so, so here there's no.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK, OK so you're So what you're asking me about is OK if I initialize at random I get a catastrophe.",
                    "label": 0
                },
                {
                    "sent": "I get something that looks much worse than this.",
                    "label": 0
                },
                {
                    "sent": "So GP LVM you cannot initialize at random.",
                    "label": 0
                },
                {
                    "sent": "But not even a notice a also.",
                    "label": 0
                },
                {
                    "sent": "OK, then how?",
                    "label": 0
                },
                {
                    "sent": "Can you initially said with the base model Camry?",
                    "label": 0
                },
                {
                    "sent": "I mean, I condition initialized using Isomap, using Ellie, or using some other things, but that's not.",
                    "label": 0
                },
                {
                    "sent": "It's not going to stay there.",
                    "label": 0
                },
                {
                    "sent": "Oh, actually damn it.",
                    "label": 0
                },
                {
                    "sent": "I think I oh, I had another.",
                    "label": 0
                },
                {
                    "sent": "I had another example.",
                    "label": 0
                },
                {
                    "sent": "I don't know if we should take this discussion offline.",
                    "label": 0
                },
                {
                    "sent": "I mean, I hope that there.",
                    "label": 0
                },
                {
                    "sent": "Perhaps those people are sort of wanting to go for lunch or something.",
                    "label": 0
                },
                {
                    "sent": "If you want.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm happy to just spare the lives of other people.",
                    "label": 0
                },
                {
                    "sent": "You have questions.",
                    "label": 0
                },
                {
                    "sent": "Excellent question, yes, I'm happy about that question.",
                    "label": 0
                },
                {
                    "sent": "OK, so so to obtain kernel PCA?",
                    "label": 0
                },
                {
                    "sent": "So kernel PCA is a similarity preserving measure because you have a smooth mapping the other way around.",
                    "label": 0
                },
                {
                    "sent": "So you have a smooth mapping that goes from high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So basically it's taking a GP LVM and turning it on his head.",
                    "label": 0
                },
                {
                    "sent": "And actually if you sort of maximize the evidence of that trying to put some constraint that prevents the trivial sort of everything is O solution.",
                    "label": 0
                },
                {
                    "sent": "You would get you would actually get kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "As a as a sort of as a solution.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, PC asymmetric is.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly so this is actually very reminiscent of autoencoders and this kind of models where you go from the data you sort of predict onto a sort of hidden layer, and then you try to re predict your data from there.",
                    "label": 0
                },
                {
                    "sent": "You could sort of unfold this.",
                    "label": 0
                },
                {
                    "sent": "The second quarter reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Correct?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's correct yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Later, yes.",
                    "label": 0
                }
            ]
        }
    }
}