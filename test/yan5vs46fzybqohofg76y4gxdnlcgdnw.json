{
    "id": "yan5vs46fzybqohofg76y4gxdnlcgdnw",
    "title": "Cheeger Cuts and p-Spectral Clustering",
    "info": {
        "author": [
            "Matthias Hein, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09us_hein_ccpsc/",
    "segmentation": [
        [
            "OK good yeah, thanks to the organizers for the invitation.",
            "So."
        ],
        [
            "Basically, the talk will be.",
            "Two parts.",
            "First parts is a study of how the choice of similarity graphs in machine learning effect actually.",
            "The result of your learning algorithm, or to make it more provocative, I would say that often the choice of the craft structure as a larger influence on the solution then the algorithm on top.",
            "For example, in supervised learning or spectral clustering.",
            "And I want to show you at least the difference of the choice of graph types.",
            "So difference between K nearest neighbor graph and epsilon graph for the example of the normalized cut criterion.",
            "And.",
            "In the second part of my talk, also related to the normalized cut country and then is so called P spectral clustering.",
            "And maybe I felt in the target customer that of that.",
            "One can use the eigenvectors of the graph person to do clustering and P spectral clustering.",
            "Basically it's the same, it just uses the eigenvectors of the nonlinear generalization of the of the graph Laplacian.",
            "The so called pill ablation.",
            "And the interesting thing is to do that.",
            "We can show that the eigenvector.",
            "Off this piece back to the class of P Spectra, clustering or the partition obtained in this way converges to the optimal one in some limit, as I will show.",
            "OK.",
            "But first."
        ],
        [
            "Let's talk about graphs.",
            "OK, it's clear that graphs are basically.",
            "Everywhere I mean we have a lot of data structures where which are given naturally as crowds.",
            "They have web graph, social networks, protein, international website citation networks and the nice thing about graph is that we that it actually melts relations between data.",
            "It does not.",
            "We do not need any absolute feature representation of the data, but we just need.",
            "Relations between data points.",
            "And in the meantime, in the recent guests, a lot of craft based methods have been proposed.",
            "And OK, the emphasis of this talk will be on clustering, but often actually in practice or in machine learning.",
            "We do not use given graphs, but actually given some data we construct the graphs and how it usually works is that."
        ],
        [
            "Construct so-called similarity graphs.",
            "That is, given some data and a similarity measure which measures similarity between two instances.",
            "In this space, we construct the graph.",
            "As follows, we use the data points as vertices of our graph and then we connect similar points.",
            "And the idea behind this construction is that we built kind of the global structure inherent in the data from local structure.",
            "So by connecting here locally all similar points we see somehow the global structure inherent in the data and OK, this is this famous Toyota Moon data set where OK in Euclidean space these two points are quite close.",
            "But then once you have constructed with cloud graph, you see actually that you have to travel along.",
            "Way along the craft in order to get through these two points, and in particular it's even closer to travel along this high density region.",
            "I mean making always very small jumps.",
            "You see that this points are somehow more closer than these two points because you have to jump over this Valley somehow.",
            "And this idea also."
        ],
        [
            "Over to manyfold learning.",
            "So what is meant for learning in one sentence?",
            "Basically, you make the assumption that due to strong dependencies of your features, the data is actually concentrated or around the low dimensional structure.",
            "You again given some, just some samples in Rd and you construct again the similarity graph and then right inside you can see this similarity graph as a discrete approximation of the continuous manifold and in recent years there have been proven many.",
            "Results rigorous results about how actually the limit behaves from going somehow from discrete to continuous.",
            "Well, the obvious question is now.",
            "I mean we have this similarity measure.",
            "We have data but and I've just always said we construct this similarity graph.",
            "But actually, how should we construct this similarity graph?",
            "And they are."
        ],
        [
            "Basically, two ways are done in the literature.",
            "OK, I switch over to neighborhood crafts.",
            "I mean basically similar to graphs and neighborhood graphs are the same.",
            "I assume that we are given actually a dissimilarity measure, but actually, once you have a similarity measure, we can overall vehicle over to dissimilarity measure.",
            "Assume for simplicity that symmetric, but that's actually not necessary, so there for SpaceX and we have a metric defined on it and how we construct two types of neighbor graphs.",
            "Once this the K nearest neighbor graphs.",
            "So for each point KNN of XI denotes the set of K nearest neighbor of the Point XI, and now there are three types of K nearest neighbor crafts.",
            "Once we connect Point Zion XJ, if basically XJ is among the K nearest neighbor of Exile that these two directed graph and well basically we have two operations now to symmetrize the graph.",
            "Logical end and oral, so we connect either.",
            "If both are neighbors of each other, that's leads them to the so called mutual K nearest neighbor graph, or we just connect the two points if one is neighbor of each other and we call this symmetric canius neighbor craft, it's a little bit notation is not good here.",
            "But if you have a better name for this graph, I'm happy to know how you would call it.",
            "OK, and then the aircrafts are very simple.",
            "We just connect two points if their distance is smaller than R and it's leads naturally again to another graph.",
            "And OK, usually are in the following I assume that we always in a statistical setting that means our data is actually an IID sample from some probability measure on the space, and in this case all these graphs, also called random geometric graphs.",
            "One should distinguish them to the standard random graphs which are dealt with most of the time in certain computer science.",
            "There the data, I mean the vertices are fixed and then the edges are basically random.",
            "Here the randomness comes from the randomness of the sample.",
            "So given the sample, the graph is actually deterministic.",
            "OK."
        ],
        [
            "So here on top now.",
            "Examples for points how these graphs are look like?",
            "Once he's actually that there are already major difference, in particular of the mutual canius naval craft and the symmetric key news network graph.",
            "And well, I want to get into status more proactive statement that the choice of this graph structure that means witchcraft you choose and how you choose the perimetre of this graph actually has more influence on on your learning performance as the choice of the algorithm on top.",
            "I mean, that's that's clearly not hold always, but I would say for some.",
            "Problems it clearly holds, and astonishingly this is not a well studied problem.",
            "Now in machine learning I mean up to now.",
            "The Bell Research has concentrated on the question.",
            "Given the graph, how should we learn on top?",
            "But the the question?",
            "How we should?",
            "We should actually construct construct this graph and how should we choose this?",
            "Para meters have actually not much tackled.",
            "And in the following I just want to give a partial answer or least tackle for one."
        ],
        [
            "Problem this question, which graph type one should choose?",
            "OK, and as I said, most of the talk will be about clustering.",
            "So let's step back a little bit and think about what clustering actually means.",
            "So one could define clustering in that way that you clustering means you try to prove points such that points in each group are similar and in different groups are dissimilar.",
            "And it's clear that from from this kind of definition that it's actually not a mathematical definition, so we have for clustering no clear objective.",
            "And this is different to supervised learning.",
            "We have once we have at least specified the loss function.",
            "Well, we have clear objective.",
            "And while the same now applies to clustering, once we have not specified what actually our objective is.",
            "For clustering, clustering is completely defined.",
            "OK, so your clustering might not agree with my clustering.",
            "Anne.",
            "And there are not too many models about what clustering should be in the sense of formulated in terms of the data generating probability measure.",
            "Most of the time, on reading the literature the what I call the statistical model for clustering that the clusters are trusted, connected components of the level set of the density.",
            "OK, you assume that the probability measure has a density.",
            "For example, in Rd and you just cut the density at some level T. And basically then the connected components of this cut function is are then just your clusters.",
            "Opposites well what we use most of the time.",
            "For example clustering bit I mean, spectral clustering is based on a notion of clustering, where we try to partition a graph.",
            "But for this clustering techniques, there exists no interpretation in terms of the data generating probability measure, and it's clear that we fear in machine learning and we want to study the statistical performance of these algorithms.",
            "We have to state actually what is the objective of this algorithms in terms of really the data generating probability measure.",
            "OK."
        ],
        [
            "And well, as I said, I want to study now the normalized cut criterion.",
            "And just as a repetition.",
            "Some definitions for once we want to find clustering is graph partition.",
            "We denote by a bar the complement of a set.",
            "A subset of vertices A.",
            "Three function is just the sum of the adjacent edge rates and the cut of the sets A and a bar.",
            "Of the of the partition A in a bar is then just the sum of the edge weights, which go from one partition, one for one thread from the set A to its complement, and then we have two measures of volume.",
            "Once the cardinality of a set and here volume is defined as the sum over the decrease in this set a. OK, and then there are two well known balance graph cut criteria.",
            "The ratio cuts and the normalized cut I mean.",
            "Asian sometimes differs in the literature, but I called him racial cut.",
            "The ratio cut is around since the 80s.",
            "And normalized cut has been proposed by Sheena Malik in think it was 2000.",
            "And OK, why are we interested in Ben's craft cuts valid cause it's well known that I mean min cuts and just just minimizing the cut can be efficiently computed, but usually leads to useless clustering results.",
            "But because often one or a few points actually disconnected from the rest of the set OK, and that's why I want to introduce these balancing constraints in order to get partitions which are basically roughly balanced.",
            "Nevertheless, one should say that these balancing terms here usually kick in only if the cardinality or if the volume of these sets are very small.",
            "Cuts never less dominates.",
            "Usually what is selected for for doing the partition.",
            "It just keeps us away from the more extreme cases where the partitions are really very much unbalanced.",
            "OK, so the question is now.",
            "I mean we have this balance graph cut criteria and fell.",
            "We try to find this optimal graph cuts.",
            "Actually I should say finding all these post problems, finding the optimal cuts are both NP hard problems.",
            "So what we do in the end in practices we use relaxations of these problems and speculation.",
            "Is one of them.",
            "But for the moment I just want to assume that we could solve these problems and just try to see what is the limit of this normalized Korean in terms of the.",
            "Data generating probability measure."
        ],
        [
            "Or make it more precise, we want to answer 2 questions.",
            "The first one is what is really the clustering objective corresponding to the normalized cut formulated in terms of the data generating probability measure and then the second question is that it depends on the graph type.",
            "So will we get different results once we are using for example, canius neighbor graph and the aircraft?",
            "OK, and the setting is that we have again an ID sample.",
            "Now from with probability measure in Euclidean space where we have assumed that we have also density.",
            "Neighborhood graphs are in this case unweighted and we restrict possible cuts.",
            "Now two of the craft cuts induced by hyperplanes.",
            "So basically we have a hyperplane S. I mean, this cuts the graph into two halves.",
            "One is basically lying in the half space H plus and the other one is laying lying in the edge.",
            "How space H minus?",
            "So every basically hyperplane?",
            "This is a certain cut of the graph lying in Rd.",
            "All the points are laying in Hardy, and here are already 2 examples.",
            "Here's the for this data set, so data points are the same, so the sample is the same.",
            "We just the graph constructed on top is different.",
            "Here's the canius never graph is K = 8, and here's the corresponding aircraft with our chosen as the average K nearest neighbor distance.",
            "So this K equals to 8 in this case, and you see already did the certain differences of these two cross.",
            "OK, and here's the."
        ],
        [
            "Loads.",
            "So again, it's obtained for one fixed hyperplane.",
            "We can write the normalized cuts in this way.",
            "We have this cut induced by the hyperplane.",
            "We have filled the volume terms.",
            "And we show the limit for the two graph types for the directed K nearest neighbor graph and for the aircraft.",
            "And.",
            "So we showed the limit under these conditions.",
            "Soil sample size cost Infinity and we have always two conditions here.",
            "K has to be has to grow faster than log N and smaller than N basically.",
            "And for the aircraft, well, clearly the neighborhood size goes to 0.",
            "But the neighborhood size has to go to 0 slow enough so that this term basically diverges.",
            "What one sees.",
            "Immediately OK, when has these two limits?",
            "Actually, these four factors both.",
            "Go to Infinity, which means that the normalized cut itself always converges to zero once N goes to Infinity.",
            "So we have to always renormalize it in order to get to some interesting limit.",
            "And OK, let's look now at.",
            "Have a look at the limit.",
            "So basically the cut term looks roughly the same, so we just integrate the density over the hyperplane, but.",
            "The power of the density has is different for the cainan, for the aircraft.",
            "And basically the same holds also for the volume terms, so the volume term here converges to the volume, basically just the probability mass of the hyperplane of the half space age plus.",
            "And here we actually integrate for the archive square of the density difference.",
            "Do not look basically very significant, maybe, but the limit results shows that basically limit of these two graphs are different.",
            "So what we have shown here is that this is basically the criterion you're trying to really optimize when you're doing clustering with the normalized cut coherent in the limit.",
            "As you get more and more data.",
            "And basically the limits are different.",
            "So the question is now these are just some powers of the density, so does it really matter for practice?",
            "So do we find examples where?"
        ],
        [
            "This matters, and the answer is yes.",
            "And here is actually very simple one.",
            "It's just the density is given as a mixture of three Gaussians in two dimensions.",
            "And the means of these three Gaussians lie all on one line.",
            "They have different balances, but the means are basically aligned and then it's clear that the optimal hyperplane.",
            "The optimal normalized cut if you restrict yourself to hyperplanes should be basically orthogonal to the access connecting the means.",
            "So then it's simple.",
            "You cannot.",
            "Basically compute this limits or this expressions analytically, but at least numerical computation is easy.",
            "So we just basically tried all possible cuts in this direction and then we got this result that for the KNN graph this Red Cross is basically optimal, minimizing the normalized cap and for the aircraft it's this.",
            "This blue line here.",
            "OK so once he said.",
            "There's really a major difference, then, between the optimal cut found by a Canaan, Grafton, the optimal cut found by another graph.",
            "Now you could ask value.",
            "We have here mixture of three cautions and we look for a partition into two clusters.",
            "So maybe this is the reason why we have get here different two different limits.",
            "But actually you can also find examples where you have just two a mixture of two Gaussians and never less.",
            "The optimal cat is a different places.",
            "OK, so this is just the limit results.",
            "Clearly we are interested in also in finite sample performance.",
            "Or basically it doesn't matter for."
        ],
        [
            "For yeah, if you have some reasonable sample size.",
            "Now the problem is now that now we cannot simply do it.",
            "As before, because optimizing normal cat is, as I said, NP hard.",
            "So what we do is for this experiment we use spectral clustering.",
            "And I come back actually later on to how spectral clustering is related as a regulation of normalized cut.",
            "But basically one sees if one just uses partitioning into 2.",
            "Yeah.",
            "Into two sets of this of this graph and we sample basically from this mixture of Gaussians.",
            "We sat here for the K nearest neighbor.",
            "Graph K equals to 150 and the aircraft the Alpha is chosen again as the average K nearest neighbor distance, so that the graph are roughly come parable and you'll see that the optimal cut is exactly at the places where it was predicted from the limit.",
            "So basically this is this first bump and this is this small.",
            "Bump on the right.",
            "OK, so these are just examples in two dimensions.",
            "So the question is, does this generalize to higher dimensions and?",
            "The thing is, we can."
        ],
        [
            "Find examples also in higher dimensions where the results of spectral clustering are different.",
            "The problem is only that the optimal cut is not anymore at the place where we basically predict it, and the reason is well, if we are in high dimensions, we just have not enough samples so that we are already in a regime where basically the normalized cut is the empirical normalized cut is close to this limit expression.",
            "Because basically in high dimension, then the normalized cut criterion is mainly affected by boundary boundary effects in high dimension cause any point is basically close to the boundary of the domain in high dimensions and this leads to the fact that then our limit does not kick in at this time.",
            "OK, the same thing can actually be easily generalized to other balanced graph cut criteria.",
            "So you can use the same limit results to derive the limit of racial cut normalized Chico cut.",
            "I will define Chico cut in the second.",
            "Major limitation is at the moment that the results or holds only for unweighted graphs, but I think that we can generalize it to weighted graph and.",
            "I think that in particular weighted cross could be interesting be caused by different renormalization of the weights.",
            "I think that we can modify the influence of the density, which means that hopefully in the limit we should be able to get any power here and here, which we would like to have.",
            "So that we can basically any model we want to have for spectral class.",
            "For clustering we can reach by renormalizing the weights of the graph.",
            "OK, so that was basically the first part of the talk, so the message is different.",
            "Graphs can lead to different results for spectral clustering.",
            "Well, basically for most cuts, but then also for spectral clustering.",
            "And now the second part of."
        ],
        [
            "Talk I want to answer the question.",
            "Basically, is standard spectral clustering really the best approximation to the normalized cut?",
            "And I guess most of you in the audience will already say no because there are relaxations based on semidefinite programming which.",
            "At least for small datasets, perform better.",
            "Problem is why it does not not vice that not so much used, because usually the better the approximation of semidefinite programming, the harder it is to optimize.",
            "So basically then at some point you are restricted to a very small set of vertices in order to still be able to solve it.",
            "OK, and I want to talk about another relaxation.",
            "But"
        ],
        [
            "Before I come to that, I want to basically review quickly the how you will get spectral clustering as a relaxation of normalized cuts and racial cut, and I was tricked myself in the following two actually to the ratio cuts because the formulas for the normalized cut a little bit more actually, and I think it's easier to understand for the ratio cut.",
            "OK, thanks to Gary Miller.",
            "I don't have to go over this again, so this is our craft Laplacian.",
            "He also cited realization functional looks like this or the associated quadratic form.",
            "And it's well known that if the graph is connected, then only the first eigenvalue zero and the corresponding eigenvector is just the constant eigenvector, so all components are the same.",
            "OK. Now."
        ],
        [
            "So how do we get spectral clustering as a reputation of the ratio cut in this case?",
            "Well, we define for each partition of our graph into CNC bar, a function FC which is basically constant on.",
            "On the set C, an constant on the set C bar.",
            "And attains these values and then you can show that this quadratic form of the function FC basically just this.",
            "Seeing boils down to the ratio cuts.",
            "Why?",
            "'cause first of all, because the function is constant on one partition and constant on the other half.",
            "Basically only the terms survive where you have points from different clusters.",
            "And then OK, the rest of the balancing concerned comes from the specific form of dysfunction and then so you can show that this quadratic form is end times the ratio cut the squared normal of this function is just N and each function in this of this form fulfills that it's orthogonal to the constant vector and then you can rewrite the optimal or the problem of finding the optimal ratio cut as minimizing this kind of coefficient here where you have this kind of redundant constraint that.",
            "Which is anyway fulfill for any this function of this form.",
            "And how do we get now?",
            "The relaxation.",
            "Well, if we instead of minimizing trust of our functions of this form of this specific form, we allow any function.",
            "OK, and if you allow any function, it's clearly a larger set, so we optimize over larger sets.",
            "So relax the problem to this larger set so the optimum of this problem here will be smaller than the solution of this problem, and it's well known that from the radio responsible that the solution of this problem is just the second eigenvalue of the graph Laplacian.",
            "OK, and as I said, there are other relaxations possible, which usually then lead to semidefinite programs.",
            "OK, so this is basically the duration of the standard spectral clustering.",
            "Now the immediate question is, how good is this relax."
        ],
        [
            "So how close is the?",
            "The cut then found by.",
            "Thresholding actually this eigenvector.",
            "Compared to the optimal cut compared to the optimization cut and actually we, it's a little bit disturbing, but I will not compared to the optimal ratio cut, but so the optimal ratio, Cheeger cut, and this slightly discrepancy will become clear in the rest of the talk 'cause the ratio cut is actually the more natural criterion, then the ratio cut, but for the moment you just have to believe me here.",
            "OK so the racial Cheeger cut is also a balanced graph cut.",
            "Well, we have also the cuts, but then instead of for the ratio cut, we had this balancing term.",
            "Now we have basically one over the minimum of the canal tease of the two sets.",
            "It's just a different way how to hold the balance the.",
            "The Commonality's and basically we will see later on the disk corresponds more to kind of level norm, and this corresponds more to an Infinity norm.",
            "OK, but I will come back to that and then we define the optimal Geo cut just as the infineum over all possible partitions of this racial chicken cut.",
            "Then we have to say we have got the second eigenvector of the graph Laplacian.",
            "But now we have not any partitioning yet.",
            "So how do we get the partitioning while we transform?",
            "Just cut off.",
            "At some point the second item vector, so we just just do thresholding and we do thresholding by minimizing the ratio cut over all possible thresholds.",
            "OK. And then one can show that using the standard isoperimetric inequality or basically the proof of Spicer, parametric inequality that this relation holds.",
            "So they cut found by thresholding this second eigenvector of the graph Laplacian fulfills this relation.",
            "So this lower bound is trivial because clearly the cat found by thresholding is worse than the optimal cut, and so this upper bound is interesting.",
            "So it looks a little bit, you know, sent that we have here the square root of this of our left hand side.",
            "But actually this makes a huge difference, so the upper bound is quite loose, and it's even the more loose.",
            "Basically the better the optimal ratio achiever cut is.",
            "So basically the smaller this is, the larger will be the square root.",
            "So basically the worse the true or the best partition is.",
            "The verses despond and it actually has been shown in a paper by Catherine Miller that roughly the order of this upper bound can be achieved by this whole called Tree Cross part.",
            "So it it's a it's a cross product of two crowds.",
            "There one is the tree and the other is a path, and by careful selection of the size of the tree and the path you can get basically.",
            "Yeah, across which achieves this upper bounds roughly.",
            "OK. And in the following I basically.",
            "So this is for the standard spectral clustering and what I was shown in the following is that by going over to this P Laplacian we can actually abound here and upper bound which can becomes tight in the limit as we go to.",
            "Basically P goes towards form, so that means the cut found by optimal thresholding of the pyaing vector of the second peeing vector will converge to the optimal ratio Cheeger cut.",
            "OK.",
            "But yeah, let's first."
        ],
        [
            "Fix some technical details so we would like to have an operator which generalizes basically the functional induced by the graph Laplacian.",
            "So we will have and want to have an operator which infuses here this kind of functional where we have replaced the.",
            "Two for the standard practice standard krafla passing by AP.",
            "And so if you ask this question, that's the existing operator which induces this functional.",
            "Here the answer is yes, and this is the graph Laplacian and it has this form here.",
            "But one first sees that is that one recovers the standard graph Laplacian for the case P = 2.",
            "That's easier, just plug in P = 2 and then you get back this wonderful blessing.",
            "In general, this P Laplacian is is a non linear operator, which means is you plug in Alpha times F. You won't give Alpha times the operator times F, so it's a non linear operator.",
            "Kitty would like to proceed in the same way as before.",
            "Also, we want to use eigenvectors of this operator in order to get a class ring.",
            "Now the problem is, well, we have learned how to define eigenvectors of a linear operator, which basically in our case is a metrics.",
            "So what are eigenvectors of a nonlinear operator?",
            "OK, so."
        ],
        [
            "Just the definition of such an icon vector of.",
            "Well, this is just a definition.",
            "You can define anything.",
            "The question is, is this definition of reasonable?",
            "The first thing to note is OK here you have the standard pizza.",
            "I mean the P Laplacian times the vector has to be the eigenvalue and the eigenvalue appeal ablation is then always denoted by Lambda P now.",
            "The.",
            "At times the right hand side, which has this form, you see that the right hand side is not linear anymore as in the standard case.",
            "It's interesting to note that for this definition, at least eigenvectors are involved in the linear rescaling under rescaling.",
            "Which means at least that on array.",
            "Basically from the original you have the standard property that if you multiply your eigenvector by, uh, some constant, it will be again and eigenvectors, so that property at least is preserved.",
            "This definition is actually motivated by kind of a generalized responsibl.",
            "Where?",
            "You know that for standards I mean for symmetric matrix you can always find the eigenvectors by.",
            "Yeah, finding all the critical points of this of this quotient here.",
            "And what we basically just it is to generalize this quotient now to this nonlinear case.",
            "So for the for the form here, we just plugged in the form of the P Laplacian and instead of dividing by the squared Euclidean norm, we just now divide by the piece power of the P norm.",
            "And well, what you can prove now is that.",
            "Which is actually.",
            "Done already in a paper, but I'm giving back in 2003 for the normalized case.",
            "This is just an adaptation to the to the ratio cut case.",
            "So basically through the unnormalized pila plus.",
            "In that this function has a critical point at some vector V if and only if this isn't peeing vector of this peel ablation and the eigenvalues, then just the value of this functional.",
            "OK, so it's just as in the standard.",
            "Case for the radio, it's principle and OK. You have the same result.",
            "So basically this generalizes reside or feed lot to the peel up Lawson.",
            "If the graph is connected only first eigenvalue is 0.",
            "And the first time vector is again the considering vector.",
            "OK, so this is basically carries over from the standard case to this more general case, and it's clear that if the first eigenvector vector is constant, then we cannot use it for clustering.",
            "So we need at least the second vector again.",
            "OK, so the question is how do we get the second eigenvector?",
            "Well, we could just find all critical points of this functional, but this is a very difficult task, so we would like to go for something else.",
            "And.",
            "Well, we can go again in another cheetah.",
            "The this.",
            "Case for the standard graph Laplacian, we know that we can find the second eigenvector as the argument."
        ],
        [
            "Of.",
            "So the standard richemond will we have we optimize overall functions which are similar to the basically first eigenvector?",
            "OK, so we go just over to the subspace.",
            "Which is orthogonal to the constant one eigenvector.",
            "We minimize over this subspace and then get back the second eigenvector.",
            "Now the problem is for the P Laplacian that this constraint here cannot directly be carried over now to the pillar person because you cannot prove that the eigenvectors of this P Laplacian orthogonal to each other.",
            "So in particular you have no relation like that.",
            "The the Second Act of TP Lab lesson has to be orthogonal to the constant wonder Eigenvector.",
            "But the nice thing is that they exist an equivalent formulation of this of the vector for the standard graph Laplacian, so that you can equivalently formulate this as an optimization problem where we optimize this ratio here.",
            "And what does this minimum?",
            "Thing do here, it's basically you project out the component of F in the direction of the one vector.",
            "OK, so that's what this minimization problem in the denominator does, so it's that's why it's basically I mean it's equivalent to the first case, and this now can be generalized to the Pila blessing, because what we do is OK, we know how to replace this thing.",
            "And we know also how to replace degrees in normal, just again replace it by the norm by the P norm and we take the piece power.",
            "OK. And the yeah, no one can prove that the second eigenvalue of of the P Laplacian is just a global minimum of this functional.",
            "And the second eigenvector off the peel of lost income can be computed using the global minimizer of dysfunctional.",
            "OK, so it's not directly that Eigenvector.",
            "The second argument is not directly to minimize of dysfunctional, but it can be directly computed.",
            "OK, and the nice thing is now compared to the previous situation, we don't have to use search for critical points.",
            "Offer such a function, which would be very difficult, but we just have to find for the global minimum of such a function, which is a much easier task.",
            "It's not turns out not to be very easy, but it's much easier to find all critical points.",
            "OK, from a theoretical point is now interesting.",
            "I mean we know that the standard case P = 2 corresponds to a balanced graph cut cut here, which is the ratio cut.",
            "So we can ask the same question again to which balance graph cut cut here, does the PDF blasting correspond to or respectively the second eigenvalue of the PDF lesson and one can answer this question."
        ],
        [
            "So basically for each P there exists a corresponding organization.",
            "So what you do in the peel up lesson you can see if you find the second eigenvalue appeal updates and it's and relaxation of this problem here where you have a balanced graph cut cut here are defined it in this way.",
            "And with the special cases, if you plug in P = 2, you recover the ratio cut and if you go to limit P equals one who will cover the Rachel Chico cut?",
            "OK, in this photos by.",
            "This relationship here.",
            "OK, that's why I said that.",
            "It basically corresponds to a kind of interpolation.",
            "Also the P Laplacian basically interpolates between the Rachel cut and the ratio Chico cuts.",
            "And you can see this as a kind of interpolation between the Infinity norm and the L1 norm for the balancing term.",
            "OK, so well, now this is nice theory, but the question is now again, how good is this relaxation?",
            "And OK, we do the same thing we."
        ],
        [
            "Defined again the partition by optimal thresholding so we minimize the racial Cheeger cut over all possible thresholds.",
            "Then basically OK one can extend and so called as a permanent inequality by I'm good base which has proven for the normalized case to the Unnormalized case.",
            "Well, I think I'm already short of time, so I skip this and I go directly over to the main result.",
            "Basically by looking into this proof you can see that you.",
            "Can prove this result and this is our actually also our main motivation for spectral clustering.",
            "Namely, you get basically abound, which looks very similar to the case P = 2 and actually you recover also for P equals to the bound I've shown before.",
            "But now The thing is S. P goes to towards one basically.",
            "So, uh, this upper bound becomes tight.",
            "So basically you can approximate.",
            "The optimal ratio cut or the optimization Chico Carter should say by the thresholded eigenvector of the P Laplacian.",
            "Arbitrary well in this limit as P goes towards one.",
            "OK.",
            "So now you might have come a little bit suspicious, because as I told you the problem of finding the optimal cut is NP hard.",
            "So it seems that we should have to pay a price for this that we can approximate this arbitrary battle and yes."
        ],
        [
            "Because as I said, you know to find the second eigenvector of this period last time we have to minimize this functional.",
            "Or we have to find the global minimum of this function.",
            "Now it's might look still OK, Becausw at least the numerator is convex and also the.",
            "So both parts are convex, but the total function is actually."
        ],
        [
            "Non convex and it's also optimized over non convex domain, so it's maximally actually I would say.",
            "And that's also a problem, because if you directly try to minimize this functional for small values of P and clearly we are interested in small values of P because we want to get very good approximation, then we often end up very quickly in a suboptimal local minima OK.",
            "The idea of the algorithm we propose is that we know that for P = 2, we have the global optimizer of this problem because it's just a second I'm vector of the standard graph Laplacian, which can be effectively I mean it very efficiently computed.",
            "And Additionally, we know that the function is continuous in P. OK, so that means that at least local minor minimum for close P should be close.",
            "So that means if we start with P = 2 with the global and we know the global minima.",
            "Now we lower up here a little bit so.",
            "Then local media should at least be close of the smaller value of P, and we can even hope that also the global minimizer is still closed.",
            "We cannot guarantee this, and this is at the moment the problem of this algorithm.",
            "We can only show that we get really good cuts, but we have no proof yet that this thing converges in any case, and actually we can use the same counterexample for the three cross path graph.",
            "Also as a counterexample for our algorithm so.",
            "Yeah.",
            "So the idea is yeah, one use decreases P in small steps and optimize for each P visib pseudo Newton method to denote method 'cause then we get this power system.",
            "The empirical observation is that we can solve still large scale problems.",
            "Large scale means emnace basically.",
            "The runtime increases dramatically as you go towards piggles one because basically the problem becomes.",
            "Not much more difficult.",
            "So basically landscape of this function, it becomes increasingly ugly as you go towards one, which is clear because you basically approximate this combinatorial problem in the limit.",
            "So you see that for these are times for USPS.",
            "So in order to find the second eigenvector, we just need 10 seconds, or basically this is for the complete clustering, but for P = 2, one point 2, we need already 1 1/2 hour.",
            "OK, and the result is for the experiments which I will show in the second district to found chiggers cut this off is as good as the standard spectral clustering cut, but often much better.",
            "OK, up to now, I did not say how we do multi class."
        ],
        [
            "Also, after now it was only partitioned into 2.",
            "The cluster, so we get multiple cluster by doing recursive spectrum doing recursive splitting because at the moment we have not any idea actually how to get higher order N vectors in order to do the standard scheme of doing an embedding and then do for example K means in this embedding.",
            "On the other hand, I have to say that.",
            "It seems not to be that good idea to, for example, do the K means because we can show that by doing the optimal thresholding we recover.",
            "Actually the optimal cut for this case, and I think there exists another result for K means for this case, even for for two clusters.",
            "OK, so.",
            "Experiments."
        ],
        [
            "Fall experience we used the symmetric anus Democrats with a fix to 10 so we basically did not want to interfere with the graph specials.",
            "We fix just the graph structure.",
            "We have this kind of Gaussian weights and we evaluate our method by using it for supervised datasets where we know the labels.",
            "So that means if we have K classes we also cluster into K clusters and we measure the agreement of the found cluster with hourglass structure.",
            "In this way we determine for each cluster the dominant label.",
            "So basically.",
            "We do a majority vote for this cluster and then give it this label.",
            "And yeah, compute the error in this way.",
            "So that corresponds to a setting where you basically given the partitioning obtained by piece Berkeley clustering.",
            "Somebody gives you a few labels and then you just do the final labeling by majority vote."
        ],
        [
            "OK so here is again for this toy data set, but actually it's not so much toy data set 'cause this is the two going to moon data set in 100 dimensions.",
            "Where we have added full Gaussian noise to each point and you see here always the two dimensional projection that you can see that the problem is not so easy from this graph here where you have actually a lot of connections between the two clusters, so it's not this standard toy problems where you have five connections between the crew clusters and whatever connections inside the cluster you see on top.",
            "The eigenvector obtained by P spectral clustering for this for different values of P where the.",
            "Color encodes basically the value of the component of this point.",
            "You see here the histogram of the values of the eigenvectors, and here you see the final clustering obtained by optimal thresholding.",
            "What you can see is basically from this histogram and also from the colors that basically the eigenvector is very smooth for the case P = 2.",
            "You see also that you obtain basically all kind of values in between.",
            "So basically the eigenvector various various moves along the graph and you see that this basically it converges to kind of step function.",
            "OK, so for P = 1.1, basically the function is the actors basically constant on one component that has a very sharp transition there.",
            "Only very few components in this region, and then it's again constant for the other part.",
            "And you see also from this the values of the cat actually hits the normalized case, so it's normal that you cut.",
            "It goes from 0.07 to 0.05.",
            "And in particular, also the error decreases, which you can see actually, probably not.",
            "It decreases from 16% as P varies from 2 to one to something like 4%.",
            "And you see also, this blue curve is the normalized cuts and this red cough is enormous.",
            "She cut all basically.",
            "Go down, SP goes towards one.",
            "And we repeated this experiment several times, so this is not just an artifact of this one.",
            "Data set we just sampled 70 of these datasets and this is a.",
            "Consistent observation OK?",
            "Now, yeah, skip this.",
            "USPS&M NIS has said we class sorry then into 2 into 10.",
            "I mean we have 10 classes so we cluster also into 10 clusters.",
            "Again."
        ],
        [
            "USPS the Rachel cut decreases quite a bit and Interestingly, minimizing the cut seems to be related to also minimizing the error of the partition.",
            "So basically the error goes down and the same happens also for amnesty ratio cut goes down and the also the arrow goes down and actually it could have been even better because you can see from the consultation confusion table that the clustering obtained was actually very good.",
            "The only problem was that one class, namely.",
            "The digits one were split into two clusters, and that implied because we forced it to have 1010 glasses that two classes for immersion.",
            "This were basically he class 49 OK.",
            "The.",
            "Yeah, that."
        ],
        [
            "We conclude, so I presented you basically in this talk.",
            "Two things.",
            "One thing, one message is that the related graph type, the choice of the craft type metals in in graph based application and this thing is an interesting topic which should be studied more.",
            "I think our result at least gives a kind of 1st provides first understanding of the modeling aspect of the of the graph type for machine learning.",
            "But I think there's much more potential to get out of here and the second part of the talk is basically completely independent of that.",
            "It works for any kind of graph and I proposed this piece spectral clustering as a generalization of spectral clustering, and empirical observation is that we get all this or.",
            "Most of the time, much better cut values and we have the theoretic result that we converge in the limit as P cause the one towards the optimal Geo cut.",
            "If we were able to compute this second eigenvector, yeah.",
            "And at the moment we are working on several extensions or a spectral clustering has been originally or denormalized controversially proposed for image segmentation.",
            "So we try to do the same for P spectral clustering.",
            "The problem is, as I've shown you, the runtime is quite terrible.",
            "So at the moment we tried to speed it up by kind.",
            "Of course, Frank Rage, current approach of crops and we are working also on higher order eigenvectors to use it for dimensionality reduction.",
            "OK, thank you very much."
        ],
        [
            "So we can take a few questions.",
            "Yep.",
            "They know the main theorem.",
            "Sorry.",
            "Atlanta to second I connector always.",
            "Has the property that if you look at all those nodes are positive connected together that aren't connected component of the negative one from another.",
            "Yeah, I know I notice the result, but and this is also on my list of things to prove for it, but I have no idea.",
            "I mean, the empirical observation is that it holds.",
            "In the same way As for the standard graph Laplacian.",
            "But it's only an empirical observation.",
            "Have you tried any of these simple lactations then?",
            "OK, so at the moment we yeah.",
            "Yeah.",
            "We have, well.",
            "We have not tried yet, so we have started with first comparing to some formulations where where it has been relaxed, a semidefinite program, they are the results were OK. Usually the lower bound you obtained by semidefinite program is actually really much better than for the.",
            "Then obtained from this second language to graph lesson, but the cuts found really well by doing kind of random rounding, or this all these things which you can do is not much better than the standard.",
            "Cut obtained by spectral clustering.",
            "The even more simple methods how to get cuts we have not tried yet, but that's also on the list to do.",
            "OK.",
            "So then we can thank the speaker and close the session."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK good yeah, thanks to the organizers for the invitation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, the talk will be.",
                    "label": 0
                },
                {
                    "sent": "Two parts.",
                    "label": 0
                },
                {
                    "sent": "First parts is a study of how the choice of similarity graphs in machine learning effect actually.",
                    "label": 1
                },
                {
                    "sent": "The result of your learning algorithm, or to make it more provocative, I would say that often the choice of the craft structure as a larger influence on the solution then the algorithm on top.",
                    "label": 1
                },
                {
                    "sent": "For example, in supervised learning or spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "And I want to show you at least the difference of the choice of graph types.",
                    "label": 1
                },
                {
                    "sent": "So difference between K nearest neighbor graph and epsilon graph for the example of the normalized cut criterion.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In the second part of my talk, also related to the normalized cut country and then is so called P spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "And maybe I felt in the target customer that of that.",
                    "label": 0
                },
                {
                    "sent": "One can use the eigenvectors of the graph person to do clustering and P spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Basically it's the same, it just uses the eigenvectors of the nonlinear generalization of the of the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "The so called pill ablation.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is to do that.",
                    "label": 0
                },
                {
                    "sent": "We can show that the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Off this piece back to the class of P Spectra, clustering or the partition obtained in this way converges to the optimal one in some limit, as I will show.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But first.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's talk about graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, it's clear that graphs are basically.",
                    "label": 0
                },
                {
                    "sent": "Everywhere I mean we have a lot of data structures where which are given naturally as crowds.",
                    "label": 0
                },
                {
                    "sent": "They have web graph, social networks, protein, international website citation networks and the nice thing about graph is that we that it actually melts relations between data.",
                    "label": 1
                },
                {
                    "sent": "It does not.",
                    "label": 0
                },
                {
                    "sent": "We do not need any absolute feature representation of the data, but we just need.",
                    "label": 0
                },
                {
                    "sent": "Relations between data points.",
                    "label": 0
                },
                {
                    "sent": "And in the meantime, in the recent guests, a lot of craft based methods have been proposed.",
                    "label": 1
                },
                {
                    "sent": "And OK, the emphasis of this talk will be on clustering, but often actually in practice or in machine learning.",
                    "label": 0
                },
                {
                    "sent": "We do not use given graphs, but actually given some data we construct the graphs and how it usually works is that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Construct so-called similarity graphs.",
                    "label": 0
                },
                {
                    "sent": "That is, given some data and a similarity measure which measures similarity between two instances.",
                    "label": 0
                },
                {
                    "sent": "In this space, we construct the graph.",
                    "label": 1
                },
                {
                    "sent": "As follows, we use the data points as vertices of our graph and then we connect similar points.",
                    "label": 1
                },
                {
                    "sent": "And the idea behind this construction is that we built kind of the global structure inherent in the data from local structure.",
                    "label": 0
                },
                {
                    "sent": "So by connecting here locally all similar points we see somehow the global structure inherent in the data and OK, this is this famous Toyota Moon data set where OK in Euclidean space these two points are quite close.",
                    "label": 0
                },
                {
                    "sent": "But then once you have constructed with cloud graph, you see actually that you have to travel along.",
                    "label": 0
                },
                {
                    "sent": "Way along the craft in order to get through these two points, and in particular it's even closer to travel along this high density region.",
                    "label": 0
                },
                {
                    "sent": "I mean making always very small jumps.",
                    "label": 0
                },
                {
                    "sent": "You see that this points are somehow more closer than these two points because you have to jump over this Valley somehow.",
                    "label": 0
                },
                {
                    "sent": "And this idea also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over to manyfold learning.",
                    "label": 0
                },
                {
                    "sent": "So what is meant for learning in one sentence?",
                    "label": 0
                },
                {
                    "sent": "Basically, you make the assumption that due to strong dependencies of your features, the data is actually concentrated or around the low dimensional structure.",
                    "label": 1
                },
                {
                    "sent": "You again given some, just some samples in Rd and you construct again the similarity graph and then right inside you can see this similarity graph as a discrete approximation of the continuous manifold and in recent years there have been proven many.",
                    "label": 0
                },
                {
                    "sent": "Results rigorous results about how actually the limit behaves from going somehow from discrete to continuous.",
                    "label": 0
                },
                {
                    "sent": "Well, the obvious question is now.",
                    "label": 0
                },
                {
                    "sent": "I mean we have this similarity measure.",
                    "label": 0
                },
                {
                    "sent": "We have data but and I've just always said we construct this similarity graph.",
                    "label": 0
                },
                {
                    "sent": "But actually, how should we construct this similarity graph?",
                    "label": 0
                },
                {
                    "sent": "And they are.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, two ways are done in the literature.",
                    "label": 0
                },
                {
                    "sent": "OK, I switch over to neighborhood crafts.",
                    "label": 0
                },
                {
                    "sent": "I mean basically similar to graphs and neighborhood graphs are the same.",
                    "label": 0
                },
                {
                    "sent": "I assume that we are given actually a dissimilarity measure, but actually, once you have a similarity measure, we can overall vehicle over to dissimilarity measure.",
                    "label": 0
                },
                {
                    "sent": "Assume for simplicity that symmetric, but that's actually not necessary, so there for SpaceX and we have a metric defined on it and how we construct two types of neighbor graphs.",
                    "label": 0
                },
                {
                    "sent": "Once this the K nearest neighbor graphs.",
                    "label": 1
                },
                {
                    "sent": "So for each point KNN of XI denotes the set of K nearest neighbor of the Point XI, and now there are three types of K nearest neighbor crafts.",
                    "label": 0
                },
                {
                    "sent": "Once we connect Point Zion XJ, if basically XJ is among the K nearest neighbor of Exile that these two directed graph and well basically we have two operations now to symmetrize the graph.",
                    "label": 0
                },
                {
                    "sent": "Logical end and oral, so we connect either.",
                    "label": 0
                },
                {
                    "sent": "If both are neighbors of each other, that's leads them to the so called mutual K nearest neighbor graph, or we just connect the two points if one is neighbor of each other and we call this symmetric canius neighbor craft, it's a little bit notation is not good here.",
                    "label": 0
                },
                {
                    "sent": "But if you have a better name for this graph, I'm happy to know how you would call it.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the aircrafts are very simple.",
                    "label": 0
                },
                {
                    "sent": "We just connect two points if their distance is smaller than R and it's leads naturally again to another graph.",
                    "label": 0
                },
                {
                    "sent": "And OK, usually are in the following I assume that we always in a statistical setting that means our data is actually an IID sample from some probability measure on the space, and in this case all these graphs, also called random geometric graphs.",
                    "label": 1
                },
                {
                    "sent": "One should distinguish them to the standard random graphs which are dealt with most of the time in certain computer science.",
                    "label": 0
                },
                {
                    "sent": "There the data, I mean the vertices are fixed and then the edges are basically random.",
                    "label": 0
                },
                {
                    "sent": "Here the randomness comes from the randomness of the sample.",
                    "label": 0
                },
                {
                    "sent": "So given the sample, the graph is actually deterministic.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here on top now.",
                    "label": 0
                },
                {
                    "sent": "Examples for points how these graphs are look like?",
                    "label": 0
                },
                {
                    "sent": "Once he's actually that there are already major difference, in particular of the mutual canius naval craft and the symmetric key news network graph.",
                    "label": 0
                },
                {
                    "sent": "And well, I want to get into status more proactive statement that the choice of this graph structure that means witchcraft you choose and how you choose the perimetre of this graph actually has more influence on on your learning performance as the choice of the algorithm on top.",
                    "label": 1
                },
                {
                    "sent": "I mean, that's that's clearly not hold always, but I would say for some.",
                    "label": 0
                },
                {
                    "sent": "Problems it clearly holds, and astonishingly this is not a well studied problem.",
                    "label": 1
                },
                {
                    "sent": "Now in machine learning I mean up to now.",
                    "label": 0
                },
                {
                    "sent": "The Bell Research has concentrated on the question.",
                    "label": 0
                },
                {
                    "sent": "Given the graph, how should we learn on top?",
                    "label": 0
                },
                {
                    "sent": "But the the question?",
                    "label": 0
                },
                {
                    "sent": "How we should?",
                    "label": 0
                },
                {
                    "sent": "We should actually construct construct this graph and how should we choose this?",
                    "label": 0
                },
                {
                    "sent": "Para meters have actually not much tackled.",
                    "label": 0
                },
                {
                    "sent": "And in the following I just want to give a partial answer or least tackle for one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem this question, which graph type one should choose?",
                    "label": 0
                },
                {
                    "sent": "OK, and as I said, most of the talk will be about clustering.",
                    "label": 0
                },
                {
                    "sent": "So let's step back a little bit and think about what clustering actually means.",
                    "label": 0
                },
                {
                    "sent": "So one could define clustering in that way that you clustering means you try to prove points such that points in each group are similar and in different groups are dissimilar.",
                    "label": 1
                },
                {
                    "sent": "And it's clear that from from this kind of definition that it's actually not a mathematical definition, so we have for clustering no clear objective.",
                    "label": 1
                },
                {
                    "sent": "And this is different to supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We have once we have at least specified the loss function.",
                    "label": 0
                },
                {
                    "sent": "Well, we have clear objective.",
                    "label": 1
                },
                {
                    "sent": "And while the same now applies to clustering, once we have not specified what actually our objective is.",
                    "label": 0
                },
                {
                    "sent": "For clustering, clustering is completely defined.",
                    "label": 0
                },
                {
                    "sent": "OK, so your clustering might not agree with my clustering.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And there are not too many models about what clustering should be in the sense of formulated in terms of the data generating probability measure.",
                    "label": 1
                },
                {
                    "sent": "Most of the time, on reading the literature the what I call the statistical model for clustering that the clusters are trusted, connected components of the level set of the density.",
                    "label": 0
                },
                {
                    "sent": "OK, you assume that the probability measure has a density.",
                    "label": 0
                },
                {
                    "sent": "For example, in Rd and you just cut the density at some level T. And basically then the connected components of this cut function is are then just your clusters.",
                    "label": 0
                },
                {
                    "sent": "Opposites well what we use most of the time.",
                    "label": 1
                },
                {
                    "sent": "For example clustering bit I mean, spectral clustering is based on a notion of clustering, where we try to partition a graph.",
                    "label": 0
                },
                {
                    "sent": "But for this clustering techniques, there exists no interpretation in terms of the data generating probability measure, and it's clear that we fear in machine learning and we want to study the statistical performance of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "We have to state actually what is the objective of this algorithms in terms of really the data generating probability measure.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And well, as I said, I want to study now the normalized cut criterion.",
                    "label": 0
                },
                {
                    "sent": "And just as a repetition.",
                    "label": 0
                },
                {
                    "sent": "Some definitions for once we want to find clustering is graph partition.",
                    "label": 0
                },
                {
                    "sent": "We denote by a bar the complement of a set.",
                    "label": 1
                },
                {
                    "sent": "A subset of vertices A.",
                    "label": 0
                },
                {
                    "sent": "Three function is just the sum of the adjacent edge rates and the cut of the sets A and a bar.",
                    "label": 1
                },
                {
                    "sent": "Of the of the partition A in a bar is then just the sum of the edge weights, which go from one partition, one for one thread from the set A to its complement, and then we have two measures of volume.",
                    "label": 0
                },
                {
                    "sent": "Once the cardinality of a set and here volume is defined as the sum over the decrease in this set a. OK, and then there are two well known balance graph cut criteria.",
                    "label": 1
                },
                {
                    "sent": "The ratio cuts and the normalized cut I mean.",
                    "label": 0
                },
                {
                    "sent": "Asian sometimes differs in the literature, but I called him racial cut.",
                    "label": 0
                },
                {
                    "sent": "The ratio cut is around since the 80s.",
                    "label": 0
                },
                {
                    "sent": "And normalized cut has been proposed by Sheena Malik in think it was 2000.",
                    "label": 0
                },
                {
                    "sent": "And OK, why are we interested in Ben's craft cuts valid cause it's well known that I mean min cuts and just just minimizing the cut can be efficiently computed, but usually leads to useless clustering results.",
                    "label": 0
                },
                {
                    "sent": "But because often one or a few points actually disconnected from the rest of the set OK, and that's why I want to introduce these balancing constraints in order to get partitions which are basically roughly balanced.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, one should say that these balancing terms here usually kick in only if the cardinality or if the volume of these sets are very small.",
                    "label": 0
                },
                {
                    "sent": "Cuts never less dominates.",
                    "label": 0
                },
                {
                    "sent": "Usually what is selected for for doing the partition.",
                    "label": 0
                },
                {
                    "sent": "It just keeps us away from the more extreme cases where the partitions are really very much unbalanced.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is now.",
                    "label": 0
                },
                {
                    "sent": "I mean we have this balance graph cut criteria and fell.",
                    "label": 0
                },
                {
                    "sent": "We try to find this optimal graph cuts.",
                    "label": 0
                },
                {
                    "sent": "Actually I should say finding all these post problems, finding the optimal cuts are both NP hard problems.",
                    "label": 0
                },
                {
                    "sent": "So what we do in the end in practices we use relaxations of these problems and speculation.",
                    "label": 0
                },
                {
                    "sent": "Is one of them.",
                    "label": 0
                },
                {
                    "sent": "But for the moment I just want to assume that we could solve these problems and just try to see what is the limit of this normalized Korean in terms of the.",
                    "label": 0
                },
                {
                    "sent": "Data generating probability measure.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or make it more precise, we want to answer 2 questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is what is really the clustering objective corresponding to the normalized cut formulated in terms of the data generating probability measure and then the second question is that it depends on the graph type.",
                    "label": 1
                },
                {
                    "sent": "So will we get different results once we are using for example, canius neighbor graph and the aircraft?",
                    "label": 0
                },
                {
                    "sent": "OK, and the setting is that we have again an ID sample.",
                    "label": 0
                },
                {
                    "sent": "Now from with probability measure in Euclidean space where we have assumed that we have also density.",
                    "label": 1
                },
                {
                    "sent": "Neighborhood graphs are in this case unweighted and we restrict possible cuts.",
                    "label": 1
                },
                {
                    "sent": "Now two of the craft cuts induced by hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a hyperplane S. I mean, this cuts the graph into two halves.",
                    "label": 0
                },
                {
                    "sent": "One is basically lying in the half space H plus and the other one is laying lying in the edge.",
                    "label": 0
                },
                {
                    "sent": "How space H minus?",
                    "label": 0
                },
                {
                    "sent": "So every basically hyperplane?",
                    "label": 0
                },
                {
                    "sent": "This is a certain cut of the graph lying in Rd.",
                    "label": 0
                },
                {
                    "sent": "All the points are laying in Hardy, and here are already 2 examples.",
                    "label": 0
                },
                {
                    "sent": "Here's the for this data set, so data points are the same, so the sample is the same.",
                    "label": 0
                },
                {
                    "sent": "We just the graph constructed on top is different.",
                    "label": 0
                },
                {
                    "sent": "Here's the canius never graph is K = 8, and here's the corresponding aircraft with our chosen as the average K nearest neighbor distance.",
                    "label": 0
                },
                {
                    "sent": "So this K equals to 8 in this case, and you see already did the certain differences of these two cross.",
                    "label": 0
                },
                {
                    "sent": "OK, and here's the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Loads.",
                    "label": 0
                },
                {
                    "sent": "So again, it's obtained for one fixed hyperplane.",
                    "label": 1
                },
                {
                    "sent": "We can write the normalized cuts in this way.",
                    "label": 0
                },
                {
                    "sent": "We have this cut induced by the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "We have filled the volume terms.",
                    "label": 0
                },
                {
                    "sent": "And we show the limit for the two graph types for the directed K nearest neighbor graph and for the aircraft.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So we showed the limit under these conditions.",
                    "label": 0
                },
                {
                    "sent": "Soil sample size cost Infinity and we have always two conditions here.",
                    "label": 1
                },
                {
                    "sent": "K has to be has to grow faster than log N and smaller than N basically.",
                    "label": 0
                },
                {
                    "sent": "And for the aircraft, well, clearly the neighborhood size goes to 0.",
                    "label": 0
                },
                {
                    "sent": "But the neighborhood size has to go to 0 slow enough so that this term basically diverges.",
                    "label": 0
                },
                {
                    "sent": "What one sees.",
                    "label": 0
                },
                {
                    "sent": "Immediately OK, when has these two limits?",
                    "label": 0
                },
                {
                    "sent": "Actually, these four factors both.",
                    "label": 0
                },
                {
                    "sent": "Go to Infinity, which means that the normalized cut itself always converges to zero once N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So we have to always renormalize it in order to get to some interesting limit.",
                    "label": 0
                },
                {
                    "sent": "And OK, let's look now at.",
                    "label": 0
                },
                {
                    "sent": "Have a look at the limit.",
                    "label": 0
                },
                {
                    "sent": "So basically the cut term looks roughly the same, so we just integrate the density over the hyperplane, but.",
                    "label": 0
                },
                {
                    "sent": "The power of the density has is different for the cainan, for the aircraft.",
                    "label": 0
                },
                {
                    "sent": "And basically the same holds also for the volume terms, so the volume term here converges to the volume, basically just the probability mass of the hyperplane of the half space age plus.",
                    "label": 0
                },
                {
                    "sent": "And here we actually integrate for the archive square of the density difference.",
                    "label": 0
                },
                {
                    "sent": "Do not look basically very significant, maybe, but the limit results shows that basically limit of these two graphs are different.",
                    "label": 0
                },
                {
                    "sent": "So what we have shown here is that this is basically the criterion you're trying to really optimize when you're doing clustering with the normalized cut coherent in the limit.",
                    "label": 0
                },
                {
                    "sent": "As you get more and more data.",
                    "label": 0
                },
                {
                    "sent": "And basically the limits are different.",
                    "label": 0
                },
                {
                    "sent": "So the question is now these are just some powers of the density, so does it really matter for practice?",
                    "label": 0
                },
                {
                    "sent": "So do we find examples where?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This matters, and the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "And here is actually very simple one.",
                    "label": 0
                },
                {
                    "sent": "It's just the density is given as a mixture of three Gaussians in two dimensions.",
                    "label": 1
                },
                {
                    "sent": "And the means of these three Gaussians lie all on one line.",
                    "label": 0
                },
                {
                    "sent": "They have different balances, but the means are basically aligned and then it's clear that the optimal hyperplane.",
                    "label": 0
                },
                {
                    "sent": "The optimal normalized cut if you restrict yourself to hyperplanes should be basically orthogonal to the access connecting the means.",
                    "label": 1
                },
                {
                    "sent": "So then it's simple.",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "Basically compute this limits or this expressions analytically, but at least numerical computation is easy.",
                    "label": 0
                },
                {
                    "sent": "So we just basically tried all possible cuts in this direction and then we got this result that for the KNN graph this Red Cross is basically optimal, minimizing the normalized cap and for the aircraft it's this.",
                    "label": 0
                },
                {
                    "sent": "This blue line here.",
                    "label": 0
                },
                {
                    "sent": "OK so once he said.",
                    "label": 0
                },
                {
                    "sent": "There's really a major difference, then, between the optimal cut found by a Canaan, Grafton, the optimal cut found by another graph.",
                    "label": 0
                },
                {
                    "sent": "Now you could ask value.",
                    "label": 0
                },
                {
                    "sent": "We have here mixture of three cautions and we look for a partition into two clusters.",
                    "label": 0
                },
                {
                    "sent": "So maybe this is the reason why we have get here different two different limits.",
                    "label": 0
                },
                {
                    "sent": "But actually you can also find examples where you have just two a mixture of two Gaussians and never less.",
                    "label": 0
                },
                {
                    "sent": "The optimal cat is a different places.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just the limit results.",
                    "label": 0
                },
                {
                    "sent": "Clearly we are interested in also in finite sample performance.",
                    "label": 0
                },
                {
                    "sent": "Or basically it doesn't matter for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For yeah, if you have some reasonable sample size.",
                    "label": 1
                },
                {
                    "sent": "Now the problem is now that now we cannot simply do it.",
                    "label": 0
                },
                {
                    "sent": "As before, because optimizing normal cat is, as I said, NP hard.",
                    "label": 0
                },
                {
                    "sent": "So what we do is for this experiment we use spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "And I come back actually later on to how spectral clustering is related as a regulation of normalized cut.",
                    "label": 1
                },
                {
                    "sent": "But basically one sees if one just uses partitioning into 2.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Into two sets of this of this graph and we sample basically from this mixture of Gaussians.",
                    "label": 1
                },
                {
                    "sent": "We sat here for the K nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "Graph K equals to 150 and the aircraft the Alpha is chosen again as the average K nearest neighbor distance, so that the graph are roughly come parable and you'll see that the optimal cut is exactly at the places where it was predicted from the limit.",
                    "label": 0
                },
                {
                    "sent": "So basically this is this first bump and this is this small.",
                    "label": 0
                },
                {
                    "sent": "Bump on the right.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are just examples in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "So the question is, does this generalize to higher dimensions and?",
                    "label": 0
                },
                {
                    "sent": "The thing is, we can.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find examples also in higher dimensions where the results of spectral clustering are different.",
                    "label": 1
                },
                {
                    "sent": "The problem is only that the optimal cut is not anymore at the place where we basically predict it, and the reason is well, if we are in high dimensions, we just have not enough samples so that we are already in a regime where basically the normalized cut is the empirical normalized cut is close to this limit expression.",
                    "label": 0
                },
                {
                    "sent": "Because basically in high dimension, then the normalized cut criterion is mainly affected by boundary boundary effects in high dimension cause any point is basically close to the boundary of the domain in high dimensions and this leads to the fact that then our limit does not kick in at this time.",
                    "label": 0
                },
                {
                    "sent": "OK, the same thing can actually be easily generalized to other balanced graph cut criteria.",
                    "label": 0
                },
                {
                    "sent": "So you can use the same limit results to derive the limit of racial cut normalized Chico cut.",
                    "label": 0
                },
                {
                    "sent": "I will define Chico cut in the second.",
                    "label": 0
                },
                {
                    "sent": "Major limitation is at the moment that the results or holds only for unweighted graphs, but I think that we can generalize it to weighted graph and.",
                    "label": 1
                },
                {
                    "sent": "I think that in particular weighted cross could be interesting be caused by different renormalization of the weights.",
                    "label": 0
                },
                {
                    "sent": "I think that we can modify the influence of the density, which means that hopefully in the limit we should be able to get any power here and here, which we would like to have.",
                    "label": 0
                },
                {
                    "sent": "So that we can basically any model we want to have for spectral class.",
                    "label": 0
                },
                {
                    "sent": "For clustering we can reach by renormalizing the weights of the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was basically the first part of the talk, so the message is different.",
                    "label": 0
                },
                {
                    "sent": "Graphs can lead to different results for spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Well, basically for most cuts, but then also for spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "And now the second part of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk I want to answer the question.",
                    "label": 0
                },
                {
                    "sent": "Basically, is standard spectral clustering really the best approximation to the normalized cut?",
                    "label": 1
                },
                {
                    "sent": "And I guess most of you in the audience will already say no because there are relaxations based on semidefinite programming which.",
                    "label": 0
                },
                {
                    "sent": "At least for small datasets, perform better.",
                    "label": 0
                },
                {
                    "sent": "Problem is why it does not not vice that not so much used, because usually the better the approximation of semidefinite programming, the harder it is to optimize.",
                    "label": 0
                },
                {
                    "sent": "So basically then at some point you are restricted to a very small set of vertices in order to still be able to solve it.",
                    "label": 0
                },
                {
                    "sent": "OK, and I want to talk about another relaxation.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before I come to that, I want to basically review quickly the how you will get spectral clustering as a relaxation of normalized cuts and racial cut, and I was tricked myself in the following two actually to the ratio cuts because the formulas for the normalized cut a little bit more actually, and I think it's easier to understand for the ratio cut.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks to Gary Miller.",
                    "label": 0
                },
                {
                    "sent": "I don't have to go over this again, so this is our craft Laplacian.",
                    "label": 0
                },
                {
                    "sent": "He also cited realization functional looks like this or the associated quadratic form.",
                    "label": 0
                },
                {
                    "sent": "And it's well known that if the graph is connected, then only the first eigenvalue zero and the corresponding eigenvector is just the constant eigenvector, so all components are the same.",
                    "label": 1
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we get spectral clustering as a reputation of the ratio cut in this case?",
                    "label": 1
                },
                {
                    "sent": "Well, we define for each partition of our graph into CNC bar, a function FC which is basically constant on.",
                    "label": 0
                },
                {
                    "sent": "On the set C, an constant on the set C bar.",
                    "label": 1
                },
                {
                    "sent": "And attains these values and then you can show that this quadratic form of the function FC basically just this.",
                    "label": 0
                },
                {
                    "sent": "Seeing boils down to the ratio cuts.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "'cause first of all, because the function is constant on one partition and constant on the other half.",
                    "label": 0
                },
                {
                    "sent": "Basically only the terms survive where you have points from different clusters.",
                    "label": 0
                },
                {
                    "sent": "And then OK, the rest of the balancing concerned comes from the specific form of dysfunction and then so you can show that this quadratic form is end times the ratio cut the squared normal of this function is just N and each function in this of this form fulfills that it's orthogonal to the constant vector and then you can rewrite the optimal or the problem of finding the optimal ratio cut as minimizing this kind of coefficient here where you have this kind of redundant constraint that.",
                    "label": 0
                },
                {
                    "sent": "Which is anyway fulfill for any this function of this form.",
                    "label": 0
                },
                {
                    "sent": "And how do we get now?",
                    "label": 0
                },
                {
                    "sent": "The relaxation.",
                    "label": 0
                },
                {
                    "sent": "Well, if we instead of minimizing trust of our functions of this form of this specific form, we allow any function.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you allow any function, it's clearly a larger set, so we optimize over larger sets.",
                    "label": 0
                },
                {
                    "sent": "So relax the problem to this larger set so the optimum of this problem here will be smaller than the solution of this problem, and it's well known that from the radio responsible that the solution of this problem is just the second eigenvalue of the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "OK, and as I said, there are other relaxations possible, which usually then lead to semidefinite programs.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically the duration of the standard spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Now the immediate question is, how good is this relax.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how close is the?",
                    "label": 0
                },
                {
                    "sent": "The cut then found by.",
                    "label": 0
                },
                {
                    "sent": "Thresholding actually this eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Compared to the optimal cut compared to the optimization cut and actually we, it's a little bit disturbing, but I will not compared to the optimal ratio cut, but so the optimal ratio, Cheeger cut, and this slightly discrepancy will become clear in the rest of the talk 'cause the ratio cut is actually the more natural criterion, then the ratio cut, but for the moment you just have to believe me here.",
                    "label": 0
                },
                {
                    "sent": "OK so the racial Cheeger cut is also a balanced graph cut.",
                    "label": 1
                },
                {
                    "sent": "Well, we have also the cuts, but then instead of for the ratio cut, we had this balancing term.",
                    "label": 0
                },
                {
                    "sent": "Now we have basically one over the minimum of the canal tease of the two sets.",
                    "label": 0
                },
                {
                    "sent": "It's just a different way how to hold the balance the.",
                    "label": 0
                },
                {
                    "sent": "The Commonality's and basically we will see later on the disk corresponds more to kind of level norm, and this corresponds more to an Infinity norm.",
                    "label": 0
                },
                {
                    "sent": "OK, but I will come back to that and then we define the optimal Geo cut just as the infineum over all possible partitions of this racial chicken cut.",
                    "label": 0
                },
                {
                    "sent": "Then we have to say we have got the second eigenvector of the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "But now we have not any partitioning yet.",
                    "label": 0
                },
                {
                    "sent": "So how do we get the partitioning while we transform?",
                    "label": 0
                },
                {
                    "sent": "Just cut off.",
                    "label": 0
                },
                {
                    "sent": "At some point the second item vector, so we just just do thresholding and we do thresholding by minimizing the ratio cut over all possible thresholds.",
                    "label": 0
                },
                {
                    "sent": "OK. And then one can show that using the standard isoperimetric inequality or basically the proof of Spicer, parametric inequality that this relation holds.",
                    "label": 1
                },
                {
                    "sent": "So they cut found by thresholding this second eigenvector of the graph Laplacian fulfills this relation.",
                    "label": 0
                },
                {
                    "sent": "So this lower bound is trivial because clearly the cat found by thresholding is worse than the optimal cut, and so this upper bound is interesting.",
                    "label": 0
                },
                {
                    "sent": "So it looks a little bit, you know, sent that we have here the square root of this of our left hand side.",
                    "label": 0
                },
                {
                    "sent": "But actually this makes a huge difference, so the upper bound is quite loose, and it's even the more loose.",
                    "label": 1
                },
                {
                    "sent": "Basically the better the optimal ratio achiever cut is.",
                    "label": 0
                },
                {
                    "sent": "So basically the smaller this is, the larger will be the square root.",
                    "label": 0
                },
                {
                    "sent": "So basically the worse the true or the best partition is.",
                    "label": 0
                },
                {
                    "sent": "The verses despond and it actually has been shown in a paper by Catherine Miller that roughly the order of this upper bound can be achieved by this whole called Tree Cross part.",
                    "label": 0
                },
                {
                    "sent": "So it it's a it's a cross product of two crowds.",
                    "label": 0
                },
                {
                    "sent": "There one is the tree and the other is a path, and by careful selection of the size of the tree and the path you can get basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, across which achieves this upper bounds roughly.",
                    "label": 0
                },
                {
                    "sent": "OK. And in the following I basically.",
                    "label": 0
                },
                {
                    "sent": "So this is for the standard spectral clustering and what I was shown in the following is that by going over to this P Laplacian we can actually abound here and upper bound which can becomes tight in the limit as we go to.",
                    "label": 0
                },
                {
                    "sent": "Basically P goes towards form, so that means the cut found by optimal thresholding of the pyaing vector of the second peeing vector will converge to the optimal ratio Cheeger cut.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But yeah, let's first.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fix some technical details so we would like to have an operator which generalizes basically the functional induced by the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So we will have and want to have an operator which infuses here this kind of functional where we have replaced the.",
                    "label": 0
                },
                {
                    "sent": "Two for the standard practice standard krafla passing by AP.",
                    "label": 0
                },
                {
                    "sent": "And so if you ask this question, that's the existing operator which induces this functional.",
                    "label": 0
                },
                {
                    "sent": "Here the answer is yes, and this is the graph Laplacian and it has this form here.",
                    "label": 0
                },
                {
                    "sent": "But one first sees that is that one recovers the standard graph Laplacian for the case P = 2.",
                    "label": 1
                },
                {
                    "sent": "That's easier, just plug in P = 2 and then you get back this wonderful blessing.",
                    "label": 0
                },
                {
                    "sent": "In general, this P Laplacian is is a non linear operator, which means is you plug in Alpha times F. You won't give Alpha times the operator times F, so it's a non linear operator.",
                    "label": 0
                },
                {
                    "sent": "Kitty would like to proceed in the same way as before.",
                    "label": 0
                },
                {
                    "sent": "Also, we want to use eigenvectors of this operator in order to get a class ring.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is, well, we have learned how to define eigenvectors of a linear operator, which basically in our case is a metrics.",
                    "label": 0
                },
                {
                    "sent": "So what are eigenvectors of a nonlinear operator?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just the definition of such an icon vector of.",
                    "label": 0
                },
                {
                    "sent": "Well, this is just a definition.",
                    "label": 0
                },
                {
                    "sent": "You can define anything.",
                    "label": 0
                },
                {
                    "sent": "The question is, is this definition of reasonable?",
                    "label": 0
                },
                {
                    "sent": "The first thing to note is OK here you have the standard pizza.",
                    "label": 0
                },
                {
                    "sent": "I mean the P Laplacian times the vector has to be the eigenvalue and the eigenvalue appeal ablation is then always denoted by Lambda P now.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "At times the right hand side, which has this form, you see that the right hand side is not linear anymore as in the standard case.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to note that for this definition, at least eigenvectors are involved in the linear rescaling under rescaling.",
                    "label": 0
                },
                {
                    "sent": "Which means at least that on array.",
                    "label": 0
                },
                {
                    "sent": "Basically from the original you have the standard property that if you multiply your eigenvector by, uh, some constant, it will be again and eigenvectors, so that property at least is preserved.",
                    "label": 0
                },
                {
                    "sent": "This definition is actually motivated by kind of a generalized responsibl.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "You know that for standards I mean for symmetric matrix you can always find the eigenvectors by.",
                    "label": 0
                },
                {
                    "sent": "Yeah, finding all the critical points of this of this quotient here.",
                    "label": 0
                },
                {
                    "sent": "And what we basically just it is to generalize this quotient now to this nonlinear case.",
                    "label": 0
                },
                {
                    "sent": "So for the for the form here, we just plugged in the form of the P Laplacian and instead of dividing by the squared Euclidean norm, we just now divide by the piece power of the P norm.",
                    "label": 0
                },
                {
                    "sent": "And well, what you can prove now is that.",
                    "label": 0
                },
                {
                    "sent": "Which is actually.",
                    "label": 0
                },
                {
                    "sent": "Done already in a paper, but I'm giving back in 2003 for the normalized case.",
                    "label": 0
                },
                {
                    "sent": "This is just an adaptation to the to the ratio cut case.",
                    "label": 0
                },
                {
                    "sent": "So basically through the unnormalized pila plus.",
                    "label": 0
                },
                {
                    "sent": "In that this function has a critical point at some vector V if and only if this isn't peeing vector of this peel ablation and the eigenvalues, then just the value of this functional.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just as in the standard.",
                    "label": 0
                },
                {
                    "sent": "Case for the radio, it's principle and OK. You have the same result.",
                    "label": 0
                },
                {
                    "sent": "So basically this generalizes reside or feed lot to the peel up Lawson.",
                    "label": 0
                },
                {
                    "sent": "If the graph is connected only first eigenvalue is 0.",
                    "label": 1
                },
                {
                    "sent": "And the first time vector is again the considering vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically carries over from the standard case to this more general case, and it's clear that if the first eigenvector vector is constant, then we cannot use it for clustering.",
                    "label": 0
                },
                {
                    "sent": "So we need at least the second vector again.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is how do we get the second eigenvector?",
                    "label": 0
                },
                {
                    "sent": "Well, we could just find all critical points of this functional, but this is a very difficult task, so we would like to go for something else.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, we can go again in another cheetah.",
                    "label": 0
                },
                {
                    "sent": "The this.",
                    "label": 0
                },
                {
                    "sent": "Case for the standard graph Laplacian, we know that we can find the second eigenvector as the argument.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "So the standard richemond will we have we optimize overall functions which are similar to the basically first eigenvector?",
                    "label": 0
                },
                {
                    "sent": "OK, so we go just over to the subspace.",
                    "label": 0
                },
                {
                    "sent": "Which is orthogonal to the constant one eigenvector.",
                    "label": 0
                },
                {
                    "sent": "We minimize over this subspace and then get back the second eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is for the P Laplacian that this constraint here cannot directly be carried over now to the pillar person because you cannot prove that the eigenvectors of this P Laplacian orthogonal to each other.",
                    "label": 0
                },
                {
                    "sent": "So in particular you have no relation like that.",
                    "label": 0
                },
                {
                    "sent": "The the Second Act of TP Lab lesson has to be orthogonal to the constant wonder Eigenvector.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing is that they exist an equivalent formulation of this of the vector for the standard graph Laplacian, so that you can equivalently formulate this as an optimization problem where we optimize this ratio here.",
                    "label": 0
                },
                {
                    "sent": "And what does this minimum?",
                    "label": 0
                },
                {
                    "sent": "Thing do here, it's basically you project out the component of F in the direction of the one vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what this minimization problem in the denominator does, so it's that's why it's basically I mean it's equivalent to the first case, and this now can be generalized to the Pila blessing, because what we do is OK, we know how to replace this thing.",
                    "label": 0
                },
                {
                    "sent": "And we know also how to replace degrees in normal, just again replace it by the norm by the P norm and we take the piece power.",
                    "label": 0
                },
                {
                    "sent": "OK. And the yeah, no one can prove that the second eigenvalue of of the P Laplacian is just a global minimum of this functional.",
                    "label": 1
                },
                {
                    "sent": "And the second eigenvector off the peel of lost income can be computed using the global minimizer of dysfunctional.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's not directly that Eigenvector.",
                    "label": 0
                },
                {
                    "sent": "The second argument is not directly to minimize of dysfunctional, but it can be directly computed.",
                    "label": 0
                },
                {
                    "sent": "OK, and the nice thing is now compared to the previous situation, we don't have to use search for critical points.",
                    "label": 0
                },
                {
                    "sent": "Offer such a function, which would be very difficult, but we just have to find for the global minimum of such a function, which is a much easier task.",
                    "label": 0
                },
                {
                    "sent": "It's not turns out not to be very easy, but it's much easier to find all critical points.",
                    "label": 1
                },
                {
                    "sent": "OK, from a theoretical point is now interesting.",
                    "label": 0
                },
                {
                    "sent": "I mean we know that the standard case P = 2 corresponds to a balanced graph cut cut here, which is the ratio cut.",
                    "label": 0
                },
                {
                    "sent": "So we can ask the same question again to which balance graph cut cut here, does the PDF blasting correspond to or respectively the second eigenvalue of the PDF lesson and one can answer this question.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically for each P there exists a corresponding organization.",
                    "label": 0
                },
                {
                    "sent": "So what you do in the peel up lesson you can see if you find the second eigenvalue appeal updates and it's and relaxation of this problem here where you have a balanced graph cut cut here are defined it in this way.",
                    "label": 1
                },
                {
                    "sent": "And with the special cases, if you plug in P = 2, you recover the ratio cut and if you go to limit P equals one who will cover the Rachel Chico cut?",
                    "label": 1
                },
                {
                    "sent": "OK, in this photos by.",
                    "label": 0
                },
                {
                    "sent": "This relationship here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why I said that.",
                    "label": 0
                },
                {
                    "sent": "It basically corresponds to a kind of interpolation.",
                    "label": 0
                },
                {
                    "sent": "Also the P Laplacian basically interpolates between the Rachel cut and the ratio Chico cuts.",
                    "label": 1
                },
                {
                    "sent": "And you can see this as a kind of interpolation between the Infinity norm and the L1 norm for the balancing term.",
                    "label": 0
                },
                {
                    "sent": "OK, so well, now this is nice theory, but the question is now again, how good is this relaxation?",
                    "label": 0
                },
                {
                    "sent": "And OK, we do the same thing we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Defined again the partition by optimal thresholding so we minimize the racial Cheeger cut over all possible thresholds.",
                    "label": 0
                },
                {
                    "sent": "Then basically OK one can extend and so called as a permanent inequality by I'm good base which has proven for the normalized case to the Unnormalized case.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I'm already short of time, so I skip this and I go directly over to the main result.",
                    "label": 0
                },
                {
                    "sent": "Basically by looking into this proof you can see that you.",
                    "label": 0
                },
                {
                    "sent": "Can prove this result and this is our actually also our main motivation for spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Namely, you get basically abound, which looks very similar to the case P = 2 and actually you recover also for P equals to the bound I've shown before.",
                    "label": 0
                },
                {
                    "sent": "But now The thing is S. P goes to towards one basically.",
                    "label": 0
                },
                {
                    "sent": "So, uh, this upper bound becomes tight.",
                    "label": 0
                },
                {
                    "sent": "So basically you can approximate.",
                    "label": 0
                },
                {
                    "sent": "The optimal ratio cut or the optimization Chico Carter should say by the thresholded eigenvector of the P Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary well in this limit as P goes towards one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now you might have come a little bit suspicious, because as I told you the problem of finding the optimal cut is NP hard.",
                    "label": 0
                },
                {
                    "sent": "So it seems that we should have to pay a price for this that we can approximate this arbitrary battle and yes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because as I said, you know to find the second eigenvector of this period last time we have to minimize this functional.",
                    "label": 0
                },
                {
                    "sent": "Or we have to find the global minimum of this function.",
                    "label": 0
                },
                {
                    "sent": "Now it's might look still OK, Becausw at least the numerator is convex and also the.",
                    "label": 0
                },
                {
                    "sent": "So both parts are convex, but the total function is actually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Non convex and it's also optimized over non convex domain, so it's maximally actually I would say.",
                    "label": 0
                },
                {
                    "sent": "And that's also a problem, because if you directly try to minimize this functional for small values of P and clearly we are interested in small values of P because we want to get very good approximation, then we often end up very quickly in a suboptimal local minima OK.",
                    "label": 0
                },
                {
                    "sent": "The idea of the algorithm we propose is that we know that for P = 2, we have the global optimizer of this problem because it's just a second I'm vector of the standard graph Laplacian, which can be effectively I mean it very efficiently computed.",
                    "label": 0
                },
                {
                    "sent": "And Additionally, we know that the function is continuous in P. OK, so that means that at least local minor minimum for close P should be close.",
                    "label": 0
                },
                {
                    "sent": "So that means if we start with P = 2 with the global and we know the global minima.",
                    "label": 0
                },
                {
                    "sent": "Now we lower up here a little bit so.",
                    "label": 0
                },
                {
                    "sent": "Then local media should at least be close of the smaller value of P, and we can even hope that also the global minimizer is still closed.",
                    "label": 0
                },
                {
                    "sent": "We cannot guarantee this, and this is at the moment the problem of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can only show that we get really good cuts, but we have no proof yet that this thing converges in any case, and actually we can use the same counterexample for the three cross path graph.",
                    "label": 0
                },
                {
                    "sent": "Also as a counterexample for our algorithm so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the idea is yeah, one use decreases P in small steps and optimize for each P visib pseudo Newton method to denote method 'cause then we get this power system.",
                    "label": 0
                },
                {
                    "sent": "The empirical observation is that we can solve still large scale problems.",
                    "label": 0
                },
                {
                    "sent": "Large scale means emnace basically.",
                    "label": 0
                },
                {
                    "sent": "The runtime increases dramatically as you go towards piggles one because basically the problem becomes.",
                    "label": 0
                },
                {
                    "sent": "Not much more difficult.",
                    "label": 0
                },
                {
                    "sent": "So basically landscape of this function, it becomes increasingly ugly as you go towards one, which is clear because you basically approximate this combinatorial problem in the limit.",
                    "label": 0
                },
                {
                    "sent": "So you see that for these are times for USPS.",
                    "label": 0
                },
                {
                    "sent": "So in order to find the second eigenvector, we just need 10 seconds, or basically this is for the complete clustering, but for P = 2, one point 2, we need already 1 1/2 hour.",
                    "label": 0
                },
                {
                    "sent": "OK, and the result is for the experiments which I will show in the second district to found chiggers cut this off is as good as the standard spectral clustering cut, but often much better.",
                    "label": 0
                },
                {
                    "sent": "OK, up to now, I did not say how we do multi class.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, after now it was only partitioned into 2.",
                    "label": 0
                },
                {
                    "sent": "The cluster, so we get multiple cluster by doing recursive spectrum doing recursive splitting because at the moment we have not any idea actually how to get higher order N vectors in order to do the standard scheme of doing an embedding and then do for example K means in this embedding.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I have to say that.",
                    "label": 0
                },
                {
                    "sent": "It seems not to be that good idea to, for example, do the K means because we can show that by doing the optimal thresholding we recover.",
                    "label": 0
                },
                {
                    "sent": "Actually the optimal cut for this case, and I think there exists another result for K means for this case, even for for two clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Experiments.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fall experience we used the symmetric anus Democrats with a fix to 10 so we basically did not want to interfere with the graph specials.",
                    "label": 1
                },
                {
                    "sent": "We fix just the graph structure.",
                    "label": 0
                },
                {
                    "sent": "We have this kind of Gaussian weights and we evaluate our method by using it for supervised datasets where we know the labels.",
                    "label": 0
                },
                {
                    "sent": "So that means if we have K classes we also cluster into K clusters and we measure the agreement of the found cluster with hourglass structure.",
                    "label": 1
                },
                {
                    "sent": "In this way we determine for each cluster the dominant label.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "We do a majority vote for this cluster and then give it this label.",
                    "label": 0
                },
                {
                    "sent": "And yeah, compute the error in this way.",
                    "label": 0
                },
                {
                    "sent": "So that corresponds to a setting where you basically given the partitioning obtained by piece Berkeley clustering.",
                    "label": 0
                },
                {
                    "sent": "Somebody gives you a few labels and then you just do the final labeling by majority vote.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is again for this toy data set, but actually it's not so much toy data set 'cause this is the two going to moon data set in 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Where we have added full Gaussian noise to each point and you see here always the two dimensional projection that you can see that the problem is not so easy from this graph here where you have actually a lot of connections between the two clusters, so it's not this standard toy problems where you have five connections between the crew clusters and whatever connections inside the cluster you see on top.",
                    "label": 0
                },
                {
                    "sent": "The eigenvector obtained by P spectral clustering for this for different values of P where the.",
                    "label": 0
                },
                {
                    "sent": "Color encodes basically the value of the component of this point.",
                    "label": 0
                },
                {
                    "sent": "You see here the histogram of the values of the eigenvectors, and here you see the final clustering obtained by optimal thresholding.",
                    "label": 0
                },
                {
                    "sent": "What you can see is basically from this histogram and also from the colors that basically the eigenvector is very smooth for the case P = 2.",
                    "label": 0
                },
                {
                    "sent": "You see also that you obtain basically all kind of values in between.",
                    "label": 0
                },
                {
                    "sent": "So basically the eigenvector various various moves along the graph and you see that this basically it converges to kind of step function.",
                    "label": 0
                },
                {
                    "sent": "OK, so for P = 1.1, basically the function is the actors basically constant on one component that has a very sharp transition there.",
                    "label": 0
                },
                {
                    "sent": "Only very few components in this region, and then it's again constant for the other part.",
                    "label": 0
                },
                {
                    "sent": "And you see also from this the values of the cat actually hits the normalized case, so it's normal that you cut.",
                    "label": 0
                },
                {
                    "sent": "It goes from 0.07 to 0.05.",
                    "label": 0
                },
                {
                    "sent": "And in particular, also the error decreases, which you can see actually, probably not.",
                    "label": 0
                },
                {
                    "sent": "It decreases from 16% as P varies from 2 to one to something like 4%.",
                    "label": 0
                },
                {
                    "sent": "And you see also, this blue curve is the normalized cuts and this red cough is enormous.",
                    "label": 0
                },
                {
                    "sent": "She cut all basically.",
                    "label": 0
                },
                {
                    "sent": "Go down, SP goes towards one.",
                    "label": 0
                },
                {
                    "sent": "And we repeated this experiment several times, so this is not just an artifact of this one.",
                    "label": 0
                },
                {
                    "sent": "Data set we just sampled 70 of these datasets and this is a.",
                    "label": 0
                },
                {
                    "sent": "Consistent observation OK?",
                    "label": 0
                },
                {
                    "sent": "Now, yeah, skip this.",
                    "label": 0
                },
                {
                    "sent": "USPS&M NIS has said we class sorry then into 2 into 10.",
                    "label": 0
                },
                {
                    "sent": "I mean we have 10 classes so we cluster also into 10 clusters.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "USPS the Rachel cut decreases quite a bit and Interestingly, minimizing the cut seems to be related to also minimizing the error of the partition.",
                    "label": 0
                },
                {
                    "sent": "So basically the error goes down and the same happens also for amnesty ratio cut goes down and the also the arrow goes down and actually it could have been even better because you can see from the consultation confusion table that the clustering obtained was actually very good.",
                    "label": 0
                },
                {
                    "sent": "The only problem was that one class, namely.",
                    "label": 0
                },
                {
                    "sent": "The digits one were split into two clusters, and that implied because we forced it to have 1010 glasses that two classes for immersion.",
                    "label": 0
                },
                {
                    "sent": "This were basically he class 49 OK.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We conclude, so I presented you basically in this talk.",
                    "label": 0
                },
                {
                    "sent": "Two things.",
                    "label": 0
                },
                {
                    "sent": "One thing, one message is that the related graph type, the choice of the craft type metals in in graph based application and this thing is an interesting topic which should be studied more.",
                    "label": 0
                },
                {
                    "sent": "I think our result at least gives a kind of 1st provides first understanding of the modeling aspect of the of the graph type for machine learning.",
                    "label": 0
                },
                {
                    "sent": "But I think there's much more potential to get out of here and the second part of the talk is basically completely independent of that.",
                    "label": 0
                },
                {
                    "sent": "It works for any kind of graph and I proposed this piece spectral clustering as a generalization of spectral clustering, and empirical observation is that we get all this or.",
                    "label": 0
                },
                {
                    "sent": "Most of the time, much better cut values and we have the theoretic result that we converge in the limit as P cause the one towards the optimal Geo cut.",
                    "label": 0
                },
                {
                    "sent": "If we were able to compute this second eigenvector, yeah.",
                    "label": 0
                },
                {
                    "sent": "And at the moment we are working on several extensions or a spectral clustering has been originally or denormalized controversially proposed for image segmentation.",
                    "label": 0
                },
                {
                    "sent": "So we try to do the same for P spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "The problem is, as I've shown you, the runtime is quite terrible.",
                    "label": 0
                },
                {
                    "sent": "So at the moment we tried to speed it up by kind.",
                    "label": 0
                },
                {
                    "sent": "Of course, Frank Rage, current approach of crops and we are working also on higher order eigenvectors to use it for dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can take a few questions.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "They know the main theorem.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Atlanta to second I connector always.",
                    "label": 0
                },
                {
                    "sent": "Has the property that if you look at all those nodes are positive connected together that aren't connected component of the negative one from another.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know I notice the result, but and this is also on my list of things to prove for it, but I have no idea.",
                    "label": 0
                },
                {
                    "sent": "I mean, the empirical observation is that it holds.",
                    "label": 0
                },
                {
                    "sent": "In the same way As for the standard graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "But it's only an empirical observation.",
                    "label": 0
                },
                {
                    "sent": "Have you tried any of these simple lactations then?",
                    "label": 0
                },
                {
                    "sent": "OK, so at the moment we yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We have, well.",
                    "label": 0
                },
                {
                    "sent": "We have not tried yet, so we have started with first comparing to some formulations where where it has been relaxed, a semidefinite program, they are the results were OK. Usually the lower bound you obtained by semidefinite program is actually really much better than for the.",
                    "label": 0
                },
                {
                    "sent": "Then obtained from this second language to graph lesson, but the cuts found really well by doing kind of random rounding, or this all these things which you can do is not much better than the standard.",
                    "label": 0
                },
                {
                    "sent": "Cut obtained by spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "The even more simple methods how to get cuts we have not tried yet, but that's also on the list to do.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then we can thank the speaker and close the session.",
                    "label": 0
                }
            ]
        }
    }
}