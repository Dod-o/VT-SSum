{
    "id": "y5oogpzu7owksunovz26fascj7tnrcsk",
    "title": "Model Selection in Markovian Processes",
    "info": {
        "author": [
            "Shie Mannor, Department of Electrical Engineering, Technion - Israel Institute of Technology"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_mannor_markovian/",
    "segmentation": [
        [
            "So thanks for coming this early.",
            "You look a little bit sleepy.",
            "Me too.",
            "So I'm going to talk about more selection in planning problems.",
            "This is a joint work with my postdoc Tan and with a colleague at Business School at MIT.",
            "And, um.",
            "I'm going to talk about planning and all this planning in large large markets in process in SIX plane.",
            "Why more selection is essential?",
            "Also show official sun examples and basically this talk is really not a reinforcement learning talk.",
            "So many of you know if my work in RL, but this is not in our relative planning a planning talk."
        ],
        [
            "Alright, so when you tried to shoot the policy and you want to plan under center, we typically want to maximize the expected average or discounted reward that has been the long term goal of all planning algorithms they dating back to bellemans time in the 50s and basically when we teach students about planning, that's what you show them you always assume."
        ],
        [
            "But the model is known.",
            "So that's also ubiquitous assumption in the planning literature, and as well as we'll see, this is a destructive assumption.",
            "So assuming that the mall is no known, as it is something that is very fundamental to planning or things you make assumption.",
            "Now there is a whole bunch of other things you can use to solve the optimization problem, so let's one assumption that is dubious and will explain."
        ],
        [
            "Why there is another assumption that is a sound problematic is that you assume that you can trade off different elements and then you can monetize.",
            "So we always have a single valued reward.",
            "It's not not a vector value.",
            "I always assume that I can take all the different elements of in my system and monetize it to a single scalar reward.",
            "So that's a sort of fundamental assumption that I make.",
            "It's justified under some some decision theoretical elements, but it's definitely in practice.",
            "I don't really doesn't really work so.",
            "Those are the two assumptions that we essentially always make.",
            "In this talk.",
            "I'm not going to discuss the second assumption, so the second assumption is a big assumption that you can monetize different elements.",
            "But this is not going to be a. I mean not going to discussed after this, we're just going to talk about what happens when you have more uncertainty."
        ],
        [
            "I would like to mention that we said here you know what we look at.",
            "The expected average word expectation can be a tricky thing.",
            "So when you do expectation, specially when you have heavy tailed distributions for reward, you assume that you really know what's going on in terms of internal probability distributions, and that's our risk thing.",
            "Once your model is not is not known, or once you try to infer things from data."
        ],
        [
            "And last but not least, there is the issue of their events, so very advance or commonly now called black Swans.",
            "They can be an extremely meaningful an extremely destructive elements in your in your system, and there are events can happen from just not knowing the probability distribution, so we don't know the distribution.",
            "There is a high low probability event that is very impactful.",
            "We'll see we'll see one in the next few slides.",
            "And then this can basically destroy your whole your whole system here.",
            "So by looking at expected reward we basically these are those assumptions that we make an I will attack just the model is known."
        ],
        [
            "Here.",
            "So let me just tell you a little bit about types of uncertainty and situate this talk and this research in the global world of planning uncertainty.",
            "So sometimes we have a market decision process and we just don't know the parameters.",
            "That's quite common and there is a whole bunch of literature called robust decision process or rust.",
            "The marketing process where you just you don't know the parameters, but you know that the lion sound set that is known to you.",
            "So there is a transition probability you have a nominal value.",
            "And there is an error that is around this value, and that if that's your problem then you know there are many good ways to solve it.",
            "As long as the model is known and that."
        ],
        [
            "Something that can be done there is another type of concerns that have a probabilistic assignment in the parameter, so you don't know the parameters value, but you have some distribution, typically beige and beige, and type of analysis, so you have a prior or you see some data will be able to get the posterior and you have probabilistic uncertainty of the parameters and then you can.",
            "You can ask what how to optimize my policy using using this such environment.",
            "Sometimes problems are easy sampling that hard.",
            "But in any case, you assume that the model is known.",
            "This is also not going to."
        ],
        [
            "In this talk.",
            "A third time conservative.",
            "What usually people think about us not to when they have a marketing process in mind, is uncertainty, which is due to random traditional reward.",
            "So you have a single trajectory and then they just rewards a random traditional random.",
            "So basically you can look at expected reward that you can also look at other criteria that are risk sensitive, mean variance tradeoffs, percentile optimization, coherent risk measures, and there are many other other such a risk measures.",
            "Summer easy, summer hard.",
            "All of them assume that the mall is known, so if you know the model, there are different ways to model the uncertainty.",
            "Either that uncertainty is deterministic or this is probabilistic, or you just care about the distribution of your of your reward.",
            "All those leads to interesting, interesting questions and definitely worth the whole talk."
        ],
        [
            "On their own, we're going to discuss a different problem here, so we're going to talk about when you don't know the model, so you have.",
            "I mean, this is a model selection, new Frontier and model selection Workshop.",
            "We're going to focus on this problem, so let me motivate it by giving you some real applications where more selection is."
        ],
        [
            "Is essential so when I started at McGill you know it was the young faculty member and the third thing to tell you is that you need to get money.",
            "So where do you go to get money?",
            "You go to companies that dig out gold.",
            "So this is a project that was done with PHP built on that Fortune 500 mining company.",
            "They don't only gold with a dig, but there I mean they mainly gold.",
            "Those companies are huge.",
            "I mean my mind is not, you know like 5 different miners working day in day out and mine is this is.",
            "Mining in Australia is the size of the mine is I mean the area is roughly the area for Rhode Island, so it's a huge area with this is an open pit mine by the way, so there are places here when you can actually dig.",
            "Dig out the gold you dig and then you get something.",
            "You get dirt and check the dirt and you take it to a factory and then you clean it and then you put cyanide in it and you do all sort of things and then sometimes you get a little bit of gold.",
            "So the objective of."
        ],
        [
            "Of of the company is to keep the throughput constant.",
            "There is our severens constraint and this is a very dynamic problem.",
            "It's endemic problem because there was not only the mind that is uncertain.",
            "You can't dig where you want.",
            "I mean think about it just a geographical structure.",
            "Here you have to sort of dig anywhere you like if you want to dig in one place, you have to make sure that you've dug in other places close to it.",
            "But also there is a whole supply chain following, so there's a whole large supply chain that includes trains factories in their employees that actually are in charge of moving the dirt and then later on the OR from one place to the other.",
            "So this is a. I mean there's a whole supply chain that is."
        ],
        [
            "Fairly complicated, the model is not is not known for mining, but it's known for the rest of the supply chain, and here by no by no name I'm putting your.",
            "I mean it's not really known and the model is terribly complicated.",
            "So the model here once you write it down and you try to solve it and companies like mining companies that do have OR groups so they have operations research teams and they have quite a few researchers that work on those topics.",
            "You get usually a very complicated mix.",
            "IP LP problem and then you solve it and you hope for the best.",
            "In this problem, from a dynamic perspective, it just, I mean, does it make sense to try and solve the whole thing?",
            "I mean just there is very large.",
            "It's very very large problem and you don't really know much about about the structure of the level of uncertainty, and this problem is very very dynamic and I also want to mention that here the objective is not to maximize your reward.",
            "I mean you don't care so much about your average reward, you care most about your throughput because you have a supply chain.",
            "Once your supply chain is broken then all hell breaks loose.",
            "Basically you lose a lot of money, so the reason objective here that you want to keep throughput reasonable and you want your variance to keep throughput implies money.",
            "This is cash flow when you dig out gold, so more gold, more money.",
            "But you have severe virus constraint 'cause you don't want your supply chain to collapse.",
            "So this is the first example."
        ],
        [
            "The second example, that's an ongoing work with HP HP Labs and that's, uh, they have a problem here.",
            "That is what's known as PSP or print service providers.",
            "In the PSPS there are.",
            "Basically this is a print shop so it's not a moms and moms and pops kind of shocked that there are many machines and the machines are some machines can print.",
            "Some machines, can cut, some machines, can collect the pages and every such print shop can print.",
            "Thousands of books PR.",
            "And that's a true list."
        ],
        [
            "Scheduling problem, so it's a scheduling problem.",
            "You have many machines.",
            "The machines you can sort of future decide how to schedule each job so to which machines you want to send a different job."
        ],
        [
            "And again here you have, you want to maximize your reward, which means how many?",
            "How many books or or cards or envelopes get out of your system and system as you see here.",
            "It's pretty complicated system.",
            "And and but there are also some operational constraints because machines breakdown sometimes.",
            "There are things that are possible you cannot do and you have a large set of constraints that you can you can use."
        ],
        [
            "So there is a question where does the model come from?",
            "Where did the model for the machines come from?",
            "Machines breakdown?",
            "They have stochastic behavior, that's something that might be a little bit surprising, but if you give us a single machine the same job to print twice, it will not.",
            "It will make it may take a different amount of time and the reason that this machine is very complicated than the previous machine or what happened in the system affect what's going to happen now.",
            "I mean, those machines are.",
            "I don't know if you've ever seen the huge big printers and there are fairly.",
            "Complex, so I mean just modeling them is the problem and how you can build some very naive models of how the machines work, but that seems like a."
        ],
        [
            "Challenge and I should mention this is fairly stochastic problem, so this is not a deterministic scheduling problem, but rather refers to classic problem and you can have models for different machines for the behavior of the press line.",
            "So this is a second motivate."
        ],
        [
            "And the third one that's a motivation will touch more later on in stock.",
            "Let's work with a large US or Taylor, whose name will remain disclosed.",
            "And this is a Mail order catalog problem I've been working on this for several years.",
            "This is just to show you this is the sort of the first Mail order catalog ever, and this is in the the 18 I think in 1884.",
            "That's known as the Eaton catalog, so this problem has been around for many many years."
        ],
        [
            "Before and the question here is very simple, it's a marketing problem and that's why by the way in this work there is someone from the Business School.",
            "So you want to know if you want to send or not.",
            "To send this case there Miller catalog to a client, and I mean sending or not something that's a fair fairly isn't easy decision, but it's very simple if you if you send it to the client and he doesn't buy, then you lost $2 or so.",
            "If you send it to the customer handed by then you gain some money depending on what the customer bought.",
            "And this is just to give you an idea on how much data we have for this problem, so this is a problem where you have records of something like 2,000,000 customers over five years.",
            "Records of what happened every two weeks, so that's a lot of data.",
            "Of course the data is noisy and this this is a whole field in marketing."
        ],
        [
            "So there is.",
            "There's a common wisdom and people sort of have a good intuitive idea of what to do in this problem, so there is a.",
            "The wisdom is to do what's known as the FM method, so its recency, frequency and monetary value.",
            "Basically you look at every customer and you look at the recently of the left left.",
            "Purchase the frequency.",
            "So how often this customer buys buys a good from you and the monetary value.",
            "So how much you actually spends an you build?",
            "Or you discretize somehow when you build a state space based on these measures?",
            "And maybe some some other parameters.",
            "You have an and you try to solve the MDP.",
            "So that's what has been done in the past, and it's not very.",
            "I mean, it's very clear hard to do that.",
            "So how do you decide to discretize?"
        ],
        [
            "I should mention that in this problem dynamics matter, so you might think.",
            "Well, maybe that's just a classification problem, I just need to know if the customer is going to buy or not buy and that maybe I should.",
            "I should be solve it as a classification problem.",
            "So now you shouldn't because there is a whole issue here of retaining customers and developing a relationship with the customer.",
            "So basically think about life, you know somebody.",
            "There's somebody who sends you a miracle, but if you don't buy today, maybe you will buy it.",
            "You know, in a couple of weeks or in a month when you flip over the pages so Dynamics Matter and there is the issue of retainment of customer that seems seems crucial, and I said the problem here is how to how to discretize and I should mention that in this problem as well as well as in the other problems, nobody is going to use to let you use exploration.",
            "So exploration is something that is completely out of the question even in this realm.",
            "Certainly in the other two problems.",
            "There is a lot of money on the line, so so basically what's going to those around that you have lots of data, but you can't really touch it on."
        ],
        [
            "Other ones so, the problem.",
            "So this is a said young faculty member.",
            "You need to raise money, so there's a lot of lot of money on the line.",
            "Those are real real problems.",
            "The state space is huge, so in all the problems I mentioned the real state space is gigantic.",
            "There's a lot of uncertainty.",
            "There are many parameters and quite frankly the problems should probably not even mark off, so their problems himself a very complicated structure and we can't really, really expect to be able to solve them with.",
            "Tools from for from with tool, for applying for MVP's."
        ],
        [
            "The other other distinctive feature that batch data are available you have lots of data, probably more than you actually want, but you cannot do exploration, so there is really no channel and we think about the mining problem.",
            "I mean, there is my real money on the line here, and you really cannot do exploration.",
            "You might be able to suggest the plan and this plan will be also examined in the other approaches, but there is no opportunity.",
            "It's not a real enforcement learning problem, it's a real planning problem."
        ],
        [
            "And this is what people do.",
            "So the operative solution is you build a small MDP that you solve and then you apply.",
            "The reason that this is an operative solution is there are several reasons for that.",
            "You really need to find the number of parameters and just if you have 300 states with lots of data then it's really easy to solve.",
            "If you have.",
            "I mean I think.",
            "Realistically, we can solve problems with up to 10,000 states quite easily, but we don't want to get their records of our information is just not good enough so we never don't get good enough information there and, but that's what people do here."
        ],
        [
            "For those of you who are in the know in RL or are planning approximate planning problems, there is a soul approach of function approximation.",
            "There's function approximation mean that we write our value function in some functional form and try to solve an approximate version of a Bellman equation, at least to the best of my experience.",
            "This doesn't really help us in such problems.",
            "There are several reasons for that.",
            "At the computational problem is not an issue.",
            "So if you get your computational problem and you want to solve it, you will solve it.",
            "I mean just just there is enough enough computational power to solve with specialized solvers.",
            "I mean such rather setting the mining problem, we use linear programming with an integer program together with a very large problems, and this works quite well.",
            "So function approximation doesn't buy you much in terms of computation.",
            "The question is whether it buys you much much in terms of generalization, so whether or not if you can find a good approximation that would actually solve a generalization problem.",
            "To my experience, the answer is no.",
            "So basically if you know how to find a good function approximator.",
            "Then you also know how to solve the problem by finding a smart states based representation.",
            "This is my experience and and I'm not aware of any any works except for some works in in some rich location problems where function approximation really value by something."
        ],
        [
            "So just to conclude, one of that and that was perhaps the lesson.",
            "The most important aspect that risk uncertainty of are of the essence and in all the problems that we mentioned.",
            "It's not really the average reward you care about the most, but rather there is some risk adjusted or suggested objective that you care about more.",
            "And I will not.",
            "Want to get into risk risk related optimization?",
            "Those are very important and interesting aspects, but will talk today about the marszalek."
        ],
        [
            "Problem.",
            "So the two important question is that we have is what model to use.",
            "That's a question we're going to ask her today an if I choose a model.",
            "How do I optimize the model with a risk adjusted objective in mind?",
            "That's not for today that if you want to talk to me about it, I'll be happy to take it offline.",
            "Many, many works on that.",
            "So we'll talk about the problem of what model to use.",
            "Before is get into the details and questions about the applications.",
            "Too early in the morning."
        ],
        [
            "Alright, so this is a we formulate some all selection problem.",
            "In these applications, is the model order of the model itself the issue both so in how can you sort of distinguish between them?",
            "Because some model might have a completely different structure and then different orders, then the other one.",
            "That's right, so so I mean I'll discuss smaller order problems.",
            "You do have them also model selection problem itself, which different basically the way it works is that you go to different consultants.",
            "And they suggest from the company for outside and just suggest ideas for variables that would influence the dynamics in a crucial way.",
            "And this is I mean the variables can be different and sometimes overlapping.",
            "Sometimes I mean completely out of the blue and you need to choose a model so we both problems are interest and you need to address both and it's.",
            "This is the way it is OK, but the but the MODIS operandi in these applications is that the consultants gives you the model and you solve them or lower for them.",
            "They give you the variables so they say they know it's very important that certain viable.",
            "This is the reason to have the purchase is very important because of some psychological reason that you know someone like me can't really understand and then you say OK, recency is important.",
            "Now what do you mean by recency?",
            "This is last.",
            "How do I do that?",
            "But sometimes of course they suggest meaningless.",
            "I mean as features that you need to find a way to eliminate.",
            "OK.",
            "So."
        ],
        [
            "Yeah.",
            "I understand why you wouldn't have.",
            "Expiration is an issue in that mining problem, and if there was one part of the mind that you were pretty confident you have, model would suggest it's not going to have much cold.",
            "Does that mean you never know?",
            "So you do have exploration in your policy eventually, but this is not explosion in terms of which features are important or which features the features are not, so you can't suggest a radical policy.",
            "OK, the policies have to be more or less within.",
            "What's acceptable just because I mean you can suggest it, but it's never going to happen, so whatever he's going to propose is going to be treated with a grain of salt and and the exploration is, I mean, expression within your model definitely is going to happen.",
            "So if your policy tells you to explore that something that will happen, this something that they will do anyway.",
            "I mean, they have to dig out the whole mind.",
            "the Patriots will dig it all out, but in mining problems is the state is sort of for every.",
            "I mean the divide the mind to the mind to essentially 3 dimensional cubes and there is the cubes himself form something which is like an like an MRF.",
            "So their relationship of if you have gold here and here, you probably are going to find gold here as well.",
            "And then there is the question is how do you?",
            "How do you make?",
            "How do plan what, what cubes are explored and what which cubes are not explored?"
        ],
        [
            "So that's let's talk about the mall selection problem.",
            "So this is a simplification, and this is the model that we're going to consider, so we'll focus on a very simple problem, and we're going to ignore action completely, so this is not MVP, but rather what's known as MRP and Markov reward process.",
            "So in a marker word process we have state we essentially something happens, we get a reward, and then we go to the next day.",
            "So this is a simplified version of Markov chain processes, and we observe a sequence of three observations.",
            "New words that occur in the in some space, so this space is rotated as or across R. So R is a reward and always just us is just a sample preservation space that can and usually will be fairly complicated, so we have this.",
            "This is our the data that we have, so we have this observation reward opposition reward and so forth and we are given K mappings, so this mapping their mappings from observation to state spaces.",
            "So somebody has an idea of K. State spaces that belong to KAMRPS, so those are the mappings, and each mapping describes a model, so those are the mappings is from observation to state spaces, and this is really just the state size.",
            "The state space of MRP, MI.",
            "Now in this talk we're not going to talk about how to have to construct the mapping, so we assume that those mappings that were given to us.",
            "Possibly there are many such models.",
            "OK, K can be possibly very very large.",
            "So the mappings are given to us and basically our problem is to find the best mapping in."
        ],
        [
            "Sense of the word best.",
            "So I'm all the selection criteria and it gets it takes as input a DT so it takes as input the stream of observations, the models and the functions.",
            "An functions HK an returns one of those says there supposed to be a true model and the definition here is that the model selection is.",
            "We're consistent if if when model I.",
            "Is it true model than the probabilities that will choose some other model goes to 0.",
            "So that's I think, a fairly natural and and simple definition of the model selection problem.",
            "So now we have K models and you want to choose one of those."
        ],
        [
            "OK, so the first thing that you would try to do is to try to probably take a penalized likelihood criteria approach, such as MDL, Akaike information criteria based on information criteria and so forth.",
            "So, just to recap how it works well, you look at the likelihood of observations given model I.",
            "So you maximize overall parameters and here just look at the log likelihood of the trajectory given the model and the dimension of the parameter is just.",
            "The number of parameters of the marker reward process.",
            "It's anyway linear in that and then in MD LM DL type estimator is simply has the following structure.",
            "So you want to you want to look at the MDF model.",
            "I is the norm of.",
            "The number of parameters time some function FT minus LFT, where FT is some sub linear function and this is sort of classical recent type analysis and there are many other related criteria and different justification that something I will hear more about later on today.",
            "Vision on beige and complexity complexity type.",
            "Capitalization and so forth.",
            "This is sort of not really what we care about here.",
            "This is just the shape that the way this model looks like.",
            "What's important here that we have the number of parameters times a sublinear element minus the log likelihood, so so far this is.",
            "This slide is supposed to be known to to all of you.",
            "Let's be honest, black cloud."
        ],
        [
            "Now the first thing possibly resulted that there does not exist a consistent or even a weekly consistent MD like Criterium and the following example is an example show that it's very simple example.",
            "So here we have two MRP's.",
            "The first one it has a single state where so you always stay in this state and the reward is a uniform between minus two and one.",
            "The second one has two states, says Zero State one and you move between them with probability.",
            "Half the reward in State Zero is minus one one.",
            "The awarding state one is.",
            "Between zero and two.",
            "So if I'll give you data that essentially the data will have some, there will be.",
            "There will be some very complicated observation here.",
            "Maybe in very high dimensional space.",
            "Then it will be mapped either to a constant stream of 0 or an alternate stream of zeros and one somehow.",
            "I don't don't really care how this transformation happens, and then I'll see rewards.",
            "So their words in all cases that are going to be between minus one and two so.",
            "Every stream of observation or it can be interpreted as belonging as if it belongs to model one, or as if it belongs to Malta.",
            "Oh sorry, so it should be minus one and two.",
            "So this is an essential property because otherwise I mean I need to be able to interpret every data point with probability that is bigger than 0.",
            "Otherwise of course the problem is uninteresting.",
            "So suppose now that Model 2 is the more complicated model, Model 2 is a true model.",
            "Let's see what the likelihood ratio is.",
            "So here basically the probability of every trajectory internal transition is 1.",
            "The probability of the reward is minus 1/3 to the power of T. So here we always get.",
            "1/3 so.",
            "So Model 1.",
            "So it's 1/3 of the power of T Model 2.",
            "So here we get basically half for every half a probability of half for every transition and probability, half for the reward.",
            "So basically here we have just one over half one over half to the power of T. So given our definitions.",
            "When you look at the log likelihoods of the log likelihood of model one is going to behave like something that grows linearly in T and and same for Model 2.",
            "So the linear term is going to be the likelihood term is going to be dominant, and it's always going to be bigger.",
            "For Model 1, just."
        ],
        [
            "Think about the definition.",
            "So here we have something sub linear.",
            "So this is the first time is going to be to go to vanish eventually and the."
        ],
        [
            "Second term, the lack of the term is always going to be bigger Formula One.",
            "Therefore we will always prefer the trivial model.",
            "So in MD like Criterion that the matter how you're going to choose the function is also going to prefer the first model as young, as long as you look at the likelihood of.",
            "Of the of the trajectory.",
            "So basically, and that's a kind of slightly surprising result, you cannot do something which is MD like, yes.",
            "All the way.",
            "The way you are applying DL, I mean you apply in DL 2 States and rewards for states are not observations.",
            "So if you're one mapping which Maps to huge state space and the other two small one, you try to code with MPL completely different things.",
            "Because I agree, I agree I didn't say I didn't.",
            "I didn't say that.",
            "I mean will show how to fix it shortly.",
            "The Navy Navy application.",
            "The Navy application of MDL doesn't work there."
        ],
        [
            "My point, so I mean how to make it, how to make it work.",
            "So there basically in this problem we can look at something that we call the accurate prediction error and then we can do we accurate errors.",
            "The reward prediction error of every every model and there are two types of regression toward the condition in transition probability aggression and this talk just to simplify things, I want to talk about reward aggregation.",
            "Everything can also can also be be done with transition probability aggregation.",
            "And basically we take their, we collapse their offer different states, we encode it.",
            "In a different way, and then we do something which is like M deal.",
            "So we do M deal in a different fashion, not on the on the on the not on the trajectory is but rather on the quality of our prediction of the reward.",
            "And to make things even simpler we will focus on refined models.",
            "So we say that Model 2 which finds Model 1 basically if we can.",
            "In Split in a consistent manner, states in M1 and Gator 2M2.",
            "So this is a certain intuitive definitions.",
            "But I mean the more precise definition is just a little bit a little bit annoying, but I think about it.",
            "So if you have one and then just just take a take a state split in two or three days, take a state and split it into some pieces and then you get a new a new model which is a refinement."
        ],
        [
            "Previous one so now we can do we order a creation.",
            "So we define the reward mean square error of this is a reward of model I for the data, so we sum over the data and now we found the error in State Jane Model I for the reward estimate.",
            "So basically we just collect the reward estimates for every for every state we look at the mean square error an essentially this is this is it?",
            "What happens here?",
            "That in the limit we have that reward, the criterion as T goes to Infinity goes in every probabilistic manner that you like to the sum over over the states.",
            "This is a steady state distribution.",
            "This is just the variance of the reward.",
            "When you get when you are in state State X.",
            "So.",
            "So basically if you like we look at just the sum of variances.",
            "Empirical variances of the rewards per state so.",
            "OK, so that's our criterion and now we look at that.",
            "We have a very simple result, but still it's a it's kind of interesting.",
            "So if MI, MI MI contains MK.",
            "So basically we have a refinement here then for a single trajectory we are also increases.",
            "This is not like a probabilistic clean.",
            "This is the claim that always happens.",
            "Well the reason is that if you divide rewards too, if you have a bunch of batch of rewards and divide 2 pieces then your virus is going to.",
            "They were on.",
            "Your average virus is going to decrease.",
            "So so basically, and we get the following corollary.",
            "So if we have the series of refining, also M1 is less refined than M2 and so forth until MK, then the RMC criterion of the model one is higher than mbtu an so forth.",
            "So basically the more refined the model is, the lower the reward.",
            "Mean square error is.",
            "So that's our we.",
            "Basically we took a problem and we converted it.",
            "To just estimates a reward error that's very simple operation to do an in the case of refined models, we have this this inequality in the case of rewards, that of models are not refined.",
            "This does not hold and you need to do a little bit more aerobics to solve the problem."
        ],
        [
            "OK, so now we have an MVL type criterion, so the our criterion is we'll look at that, something that is like the complexity of the model times a sub linear function plus the loss the RMC loss for every model, and we just pick them all with the lowest lowest reward MSE.",
            "So that's basically our GNU GNU Model order cartoon and this is very simple to do, very simple to implement, you just need to collect.",
            "The reward in every state computer mean square error and just find the one with the lowest lowest.",
            "Lowest model, so the theorem and the result which I will not prove here today at this small selection selector is actually consistent and you can also get a finite time analysis rates.",
            "And I mean basically now this here is you know you can get whatever you can imagine with many exponential and write it over three slides.",
            "So, so we have now a criterion that allows you to select select a model based on diversity in the reward.",
            "So of course, if you're always always zero, you cannot choose.",
            "I mean, there's nothing you can do, so you can do the same criterion with probability distributions, but now it's."
        ],
        [
            "I was looking at instead of looking at expected error here, you have to look at something which is a little bit more difficult, and that's a vector valued expected error.",
            "Fewer at transition probabilities becomes a little bit measure, but you can still get a similar result."
        ],
        [
            "Eventually you can combine the two and then you can.",
            "You can distinguish between every two models that have are different, have some difference in terms of reward or transition probability.",
            "So as far as we're concerned the problem is solved as long as the true model is one of your candidate models in the batch."
        ],
        [
            "OK, so let me just mention something which is kind of nice here, that the test that we have is really comparative test.",
            "So you can compare two models and ask which model is a better one.",
            "And here is just an example of here.",
            "You have a model with a single state, so 1, two and three in a row collect are all collapsed together.",
            "This is a coherent will have to state spaces that have two states each and have a different different collapsing structure.",
            "So here.",
            "We have the two F2 and three together.",
            "He will want it together and so forth, and you can build an algorithm that would do pairwise comparisons so you get pairwise comparison and then you get better convergence rates and better guarantees.",
            "That you sent essentially traverse along a tree and you start from other from from Baltimore from top and you can prove that even if you have a fairly complicated model selection with thousands and thousands of models.",
            "So the number here can be exponential in principle.",
            "Then you can still find this problem.",
            "The reason that you're going to get an exponential number of models is when sometimes you have you really trying to do a discretization problem.",
            "So I wanna descriptor just.",
            "There are many ways to discretize.",
            "So you know, even if the problem itself is fairly simple, the number of models that you might want to consider eventually is going to be huge mean, possibly exponentially in the parameters of the problem.",
            "So there is a hierarchical way to do it because of pairwise."
        ],
        [
            "Comparisons, so just make sure you just experiment with artificial data.",
            "So here we generated the.",
            "There was a random generated randomly generated.",
            "RP with us.",
            "30 States and you see this hockey stick figure that will you know.",
            "Love to love to observe and this is basically what model or selection criteria is going to get you and and the rate here.",
            "The rate of the slope will be determined by the way you normalize things, so this is going to be determined.",
            "This function F T / T that you choose.",
            "So this is artificial data an this is."
        ],
        [
            "Will happen if you look at the trajectory and do something like VICS or AIC.",
            "Basically this is inverted, so both criteria going to prefer to select.",
            "They're going to be preferred to select very small state spaces.",
            "Actually I just."
        ],
        [
            "My previous one that you you ran, this standard deviation of the the error bars, right?",
            "That's over several trajectory right over sample trajectory is in.",
            "So let's say something which is a little bit more."
        ],
        [
            "Interesting, so this is an experiment with real data, and that's with the mailing Mail order company Ann.",
            "And here we have our fair measures.",
            "So just to remind you, there is.",
            "It's a it's a retailer and it needs to decide whether or not to send Mail order catalogs to customers.",
            "There are three 3 axis here.",
            "Recently frequency and monetary value and we're going to focus on on recent suggests because this is the only thing I can I can show you on the graph.",
            "Of course we do that for all for all criteria and the question that you can ask is how to aggregate.",
            "So now you have in principle you have a very fine discretization and you want to know how should I activate my the recency effect so I can add with randomly.",
            "Basically I can just say well, I start saying with you know 500 different states.",
            "I mean just for recency.",
            "So 500 recently numbers.",
            "I can decide just to pick two at random algorithm.",
            "I can answer it according to most recent.",
            "So the most recent.",
            "Aggregation most recent states represent most recent acquisitions are going to be algorithm aggregated or the least reason so the one that were, you know happen that the way way in the past are going to be aggregated.",
            "So of course list recent is makes the most sense because if somebody bought you know 500 weeks ago or 498 weeks ago, they probably don't even remember the name of the company.",
            "So at least this makes most sense.",
            "Most recent seems like a silly idea and randomly we don't really know whether that's going to how that's going to work.",
            "So most recent means that you know if somebody bought a week ago or two weeks ago will put them in the same bin.",
            "Probably don't want to do that because there are some customers that buy every every."
        ],
        [
            "Rick.",
            "Surprisingly, so.",
            "This is the score that we get.",
            "And there are three.",
            "Three colors are for different values of F T / T We don't have.",
            "We don't have the luxury here of changing the size of the data.",
            "This is just the data that we have.",
            "And you see this.",
            "As you can see that if you look at the random random aggregation, so you just take the random recency, you get pretty much reasonably the number of states that you need is around.",
            "You know, 60 and if you do it according to the lowest frequency you get this.",
            "Slatted Lowell number so around that 40 and if you do it according to the highest and things just don't look don't make sense anymore so that's just you know this is this seems like a bad idea.",
            "Sounds like a good idea at this.",
            "Seems like an OK idea, but probably not the best, but something which is The thing is is rather remarkable is that you do observe that there seems to be something which is very reasonable to be done like there seems to be a. I mean, if you want the model order here can be reasonably reduce from therrell hundreds.",
            "Two, I know 3040 something like that.",
            "Which, by the way, later on will be fed into an MDP type problem, an optimized and then used to find an optimal or supposedly optimal model selection policy.",
            "Sorry, what is the score here?",
            "Through the score of the signing, which is the right state?",
            "So this is our MDA like score with the RMC."
        ],
        [
            "OK so so let me conclude.",
            "I want you to mention some things about this problem, so this is a very special and stylized more selection problem.",
            "I think we still made some headway stalling it.",
            "Very vanilla approach fails, but modification of the MDL philosophy here works.",
            "There is a big lie here that I made in the lies that I assumed in the analysis that the true model is one of my models.",
            "That's of course the big lie it can be worked out.",
            "At least we have some idea on how to work hard to make it work when the true model is very close to one of the models that you have when the model is far, at least we are.",
            "We are at loss.",
            "We don't know how to solve this problem.",
            "I'll be happy to hear idea of what to do when when the true model is not even close to one of the malls in your batch.",
            "I didn't say anything about optimization, so the optimization of course is lurking here behind an.",
            "I mean, this eventually should lead to an optimization problem that has to be solved and there is a question of how to aggregate.",
            "So we gave an example that how to do aggregation or how to do feature, how to, how to do it in the recency, access for the Miller Kaplan problem.",
            "But it's just one example and the question is the general question of how to aggregate.",
            "Seems to be a quite."
        ],
        [
            "Quite interesting, I just wanted to finish with an outlook so so when you do learning from batch as opposed to reinforcement learning, the real question here of what is the objective?",
            "So are you really trying to optimize your expected reward?",
            "The answer is often known.",
            "The answer is often I have a risk aware reward and this is something that you really you really need to be aware of this work, I mean is geared towards finding the right model.",
            "If such a model exists and then try to solve the objective with a separate optimization problem, this may not be the cleverest thing to do, so it maybe you need to do both.",
            "Maybe you think about optimization and more selection."
        ],
        [
            "In the same time.",
            "Finding the model is easy as long as you have the right model.",
            "That's the conclusion here.",
            "Once you don't have the right model or the right set of models, and that's a different story."
        ],
        [
            "So a little longer today.",
            "I think we'll hear and witness a.",
            "There was a poster about learning the model actively, so that's a different problem.",
            "And once you do, you can actively select action that will tell you something that whole different ball game.",
            "Very interesting.",
            "There is a great question here.",
            "How to combine things so when you have a lot of data and when you somebody tells you I'm going to allow you to sample a few, a few samples actively.",
            "So the Miller catalog tells you, you know there we have 2 million customers.",
            "Here are 500.",
            "Play with them.",
            "Do whatever you like is not going to affect us anyway, so there's a question here.",
            "What can be done?"
        ],
        [
            "Your.",
            "From an optimization problem, I just want to mention that we know how to handle parametric uncertainty quite well and inherent uncertainties.",
            "So those services uncertain the parameter that is not something that is related to the model, so that having just random transitions.",
            "How to do all of them together?",
            "That's something that seems to be beyond us at this point."
        ],
        [
            "And finally, there is the issue of of large state spaces.",
            "So what we did here today is geared toward finding something small.",
            "Question here is, who really cares if you have a large state space you can't solve it anyway and you have a lot of difficulties, but that still may be from a statistical perspective an interesting questions.",
            "So thank you for your time.",
            "I think I'm more less about time.",
            "So the question is, how does this approach or this method relates to previous works?",
            "An extension of MDL to important learning?",
            "Yeah, of course so.",
            "Our goal here is on our focus, either on statistical properties and eventually on the planning problem.",
            "So this is really not to work in RLI mean.",
            "There is nothing here to do with RL, it's more about understanding the dynamics of the problem, their relationship.",
            "But we don't.",
            "We don't care so much about about the feature selection problem.",
            "Just because we care about the model selection problem, they're related, but at least to the best of my understanding, you cannot get a result, specially not the rate results that we obtain, and which I did not.",
            "Present here.",
            "From the very beginning I didn't understand, so I mean if you have these features you know, and you know if you select these features or other features or more or less, you get a different state space and then you want to select them right, right, right?",
            "So that's exactly what feature of enforcement us.",
            "It selects a feature or not, or with features.",
            "Or.",
            "I mean you can start without it limited number of features and asking how should this feature be included or not.",
            "Yeah yeah.",
            "As I said, I think at least to the best of my knowledge.",
            "Maybe you can correct me afterwards and an enlightened me.",
            "But the rate of convergence rates that we have and the probabilistic guarantees aren't are not given in RL approaches that you are usually geared.",
            "Sure, that reducing some sort of Bellman error type criterion to the best of my knowledge.",
            "Let's take it offline.",
            "For features, right, well with long socks feature will select models so you can think about it as a special type of features, but as far as I know, because those are some very special features you can get convergence rates and guarantees.",
            "Other questions.",
            "OK, thank you, just get home.",
            "Order I missed this problem that the moment where you are the true model is not in your.",
            "You said we can achieve good result in the true model is closing right?",
            "What mean close so close here means in a in a sense of.",
            "So if you model is, think about it.",
            "If you have a perturbation of the model, we do parameter estimation anyway, But if you add more or a few more state spaces than states states to the system, as long as the reward the RMC, so the reward mean square is kind of similar, then we're fine.",
            "Once it's hit some value, then at least our analysis collapses.",
            "I'm not saying it's impossible, just I don't know how to do it.",
            "You don't use distance notion, no, because it's a distance between different state spaces, so you need to say to figure out what the distance should be.",
            "The distance notion is between the reward mean square errors.",
            "Quickly when you have the real data, you just have one structure on trajectory.",
            "Yeah, and a long one, but you didn't split it.",
            "You know.",
            "I mean, you can split it for for your own your own reasons, but that's a statistical tool, basically.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thanks for coming this early.",
                    "label": 0
                },
                {
                    "sent": "You look a little bit sleepy.",
                    "label": 0
                },
                {
                    "sent": "Me too.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about more selection in planning problems.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work with my postdoc Tan and with a colleague at Business School at MIT.",
                    "label": 1
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about planning and all this planning in large large markets in process in SIX plane.",
                    "label": 0
                },
                {
                    "sent": "Why more selection is essential?",
                    "label": 0
                },
                {
                    "sent": "Also show official sun examples and basically this talk is really not a reinforcement learning talk.",
                    "label": 0
                },
                {
                    "sent": "So many of you know if my work in RL, but this is not in our relative planning a planning talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so when you tried to shoot the policy and you want to plan under center, we typically want to maximize the expected average or discounted reward that has been the long term goal of all planning algorithms they dating back to bellemans time in the 50s and basically when we teach students about planning, that's what you show them you always assume.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the model is known.",
                    "label": 1
                },
                {
                    "sent": "So that's also ubiquitous assumption in the planning literature, and as well as we'll see, this is a destructive assumption.",
                    "label": 0
                },
                {
                    "sent": "So assuming that the mall is no known, as it is something that is very fundamental to planning or things you make assumption.",
                    "label": 0
                },
                {
                    "sent": "Now there is a whole bunch of other things you can use to solve the optimization problem, so let's one assumption that is dubious and will explain.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why there is another assumption that is a sound problematic is that you assume that you can trade off different elements and then you can monetize.",
                    "label": 1
                },
                {
                    "sent": "So we always have a single valued reward.",
                    "label": 0
                },
                {
                    "sent": "It's not not a vector value.",
                    "label": 0
                },
                {
                    "sent": "I always assume that I can take all the different elements of in my system and monetize it to a single scalar reward.",
                    "label": 0
                },
                {
                    "sent": "So that's a sort of fundamental assumption that I make.",
                    "label": 0
                },
                {
                    "sent": "It's justified under some some decision theoretical elements, but it's definitely in practice.",
                    "label": 0
                },
                {
                    "sent": "I don't really doesn't really work so.",
                    "label": 0
                },
                {
                    "sent": "Those are the two assumptions that we essentially always make.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to discuss the second assumption, so the second assumption is a big assumption that you can monetize different elements.",
                    "label": 0
                },
                {
                    "sent": "But this is not going to be a. I mean not going to discussed after this, we're just going to talk about what happens when you have more uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would like to mention that we said here you know what we look at.",
                    "label": 0
                },
                {
                    "sent": "The expected average word expectation can be a tricky thing.",
                    "label": 1
                },
                {
                    "sent": "So when you do expectation, specially when you have heavy tailed distributions for reward, you assume that you really know what's going on in terms of internal probability distributions, and that's our risk thing.",
                    "label": 0
                },
                {
                    "sent": "Once your model is not is not known, or once you try to infer things from data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And last but not least, there is the issue of their events, so very advance or commonly now called black Swans.",
                    "label": 1
                },
                {
                    "sent": "They can be an extremely meaningful an extremely destructive elements in your in your system, and there are events can happen from just not knowing the probability distribution, so we don't know the distribution.",
                    "label": 1
                },
                {
                    "sent": "There is a high low probability event that is very impactful.",
                    "label": 0
                },
                {
                    "sent": "We'll see we'll see one in the next few slides.",
                    "label": 0
                },
                {
                    "sent": "And then this can basically destroy your whole your whole system here.",
                    "label": 0
                },
                {
                    "sent": "So by looking at expected reward we basically these are those assumptions that we make an I will attack just the model is known.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So let me just tell you a little bit about types of uncertainty and situate this talk and this research in the global world of planning uncertainty.",
                    "label": 1
                },
                {
                    "sent": "So sometimes we have a market decision process and we just don't know the parameters.",
                    "label": 0
                },
                {
                    "sent": "That's quite common and there is a whole bunch of literature called robust decision process or rust.",
                    "label": 0
                },
                {
                    "sent": "The marketing process where you just you don't know the parameters, but you know that the lion sound set that is known to you.",
                    "label": 0
                },
                {
                    "sent": "So there is a transition probability you have a nominal value.",
                    "label": 0
                },
                {
                    "sent": "And there is an error that is around this value, and that if that's your problem then you know there are many good ways to solve it.",
                    "label": 0
                },
                {
                    "sent": "As long as the model is known and that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something that can be done there is another type of concerns that have a probabilistic assignment in the parameter, so you don't know the parameters value, but you have some distribution, typically beige and beige, and type of analysis, so you have a prior or you see some data will be able to get the posterior and you have probabilistic uncertainty of the parameters and then you can.",
                    "label": 1
                },
                {
                    "sent": "You can ask what how to optimize my policy using using this such environment.",
                    "label": 0
                },
                {
                    "sent": "Sometimes problems are easy sampling that hard.",
                    "label": 0
                },
                {
                    "sent": "But in any case, you assume that the model is known.",
                    "label": 0
                },
                {
                    "sent": "This is also not going to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "A third time conservative.",
                    "label": 0
                },
                {
                    "sent": "What usually people think about us not to when they have a marketing process in mind, is uncertainty, which is due to random traditional reward.",
                    "label": 1
                },
                {
                    "sent": "So you have a single trajectory and then they just rewards a random traditional random.",
                    "label": 0
                },
                {
                    "sent": "So basically you can look at expected reward that you can also look at other criteria that are risk sensitive, mean variance tradeoffs, percentile optimization, coherent risk measures, and there are many other other such a risk measures.",
                    "label": 1
                },
                {
                    "sent": "Summer easy, summer hard.",
                    "label": 0
                },
                {
                    "sent": "All of them assume that the mall is known, so if you know the model, there are different ways to model the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Either that uncertainty is deterministic or this is probabilistic, or you just care about the distribution of your of your reward.",
                    "label": 0
                },
                {
                    "sent": "All those leads to interesting, interesting questions and definitely worth the whole talk.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On their own, we're going to discuss a different problem here, so we're going to talk about when you don't know the model, so you have.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a model selection, new Frontier and model selection Workshop.",
                    "label": 0
                },
                {
                    "sent": "We're going to focus on this problem, so let me motivate it by giving you some real applications where more selection is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is essential so when I started at McGill you know it was the young faculty member and the third thing to tell you is that you need to get money.",
                    "label": 0
                },
                {
                    "sent": "So where do you go to get money?",
                    "label": 0
                },
                {
                    "sent": "You go to companies that dig out gold.",
                    "label": 1
                },
                {
                    "sent": "So this is a project that was done with PHP built on that Fortune 500 mining company.",
                    "label": 0
                },
                {
                    "sent": "They don't only gold with a dig, but there I mean they mainly gold.",
                    "label": 0
                },
                {
                    "sent": "Those companies are huge.",
                    "label": 1
                },
                {
                    "sent": "I mean my mind is not, you know like 5 different miners working day in day out and mine is this is.",
                    "label": 1
                },
                {
                    "sent": "Mining in Australia is the size of the mine is I mean the area is roughly the area for Rhode Island, so it's a huge area with this is an open pit mine by the way, so there are places here when you can actually dig.",
                    "label": 0
                },
                {
                    "sent": "Dig out the gold you dig and then you get something.",
                    "label": 0
                },
                {
                    "sent": "You get dirt and check the dirt and you take it to a factory and then you clean it and then you put cyanide in it and you do all sort of things and then sometimes you get a little bit of gold.",
                    "label": 0
                },
                {
                    "sent": "So the objective of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of of the company is to keep the throughput constant.",
                    "label": 1
                },
                {
                    "sent": "There is our severens constraint and this is a very dynamic problem.",
                    "label": 0
                },
                {
                    "sent": "It's endemic problem because there was not only the mind that is uncertain.",
                    "label": 0
                },
                {
                    "sent": "You can't dig where you want.",
                    "label": 0
                },
                {
                    "sent": "I mean think about it just a geographical structure.",
                    "label": 0
                },
                {
                    "sent": "Here you have to sort of dig anywhere you like if you want to dig in one place, you have to make sure that you've dug in other places close to it.",
                    "label": 0
                },
                {
                    "sent": "But also there is a whole supply chain following, so there's a whole large supply chain that includes trains factories in their employees that actually are in charge of moving the dirt and then later on the OR from one place to the other.",
                    "label": 1
                },
                {
                    "sent": "So this is a. I mean there's a whole supply chain that is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fairly complicated, the model is not is not known for mining, but it's known for the rest of the supply chain, and here by no by no name I'm putting your.",
                    "label": 1
                },
                {
                    "sent": "I mean it's not really known and the model is terribly complicated.",
                    "label": 0
                },
                {
                    "sent": "So the model here once you write it down and you try to solve it and companies like mining companies that do have OR groups so they have operations research teams and they have quite a few researchers that work on those topics.",
                    "label": 0
                },
                {
                    "sent": "You get usually a very complicated mix.",
                    "label": 0
                },
                {
                    "sent": "IP LP problem and then you solve it and you hope for the best.",
                    "label": 0
                },
                {
                    "sent": "In this problem, from a dynamic perspective, it just, I mean, does it make sense to try and solve the whole thing?",
                    "label": 0
                },
                {
                    "sent": "I mean just there is very large.",
                    "label": 0
                },
                {
                    "sent": "It's very very large problem and you don't really know much about about the structure of the level of uncertainty, and this problem is very very dynamic and I also want to mention that here the objective is not to maximize your reward.",
                    "label": 0
                },
                {
                    "sent": "I mean you don't care so much about your average reward, you care most about your throughput because you have a supply chain.",
                    "label": 0
                },
                {
                    "sent": "Once your supply chain is broken then all hell breaks loose.",
                    "label": 0
                },
                {
                    "sent": "Basically you lose a lot of money, so the reason objective here that you want to keep throughput reasonable and you want your variance to keep throughput implies money.",
                    "label": 0
                },
                {
                    "sent": "This is cash flow when you dig out gold, so more gold, more money.",
                    "label": 0
                },
                {
                    "sent": "But you have severe virus constraint 'cause you don't want your supply chain to collapse.",
                    "label": 0
                },
                {
                    "sent": "So this is the first example.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second example, that's an ongoing work with HP HP Labs and that's, uh, they have a problem here.",
                    "label": 0
                },
                {
                    "sent": "That is what's known as PSP or print service providers.",
                    "label": 1
                },
                {
                    "sent": "In the PSPS there are.",
                    "label": 0
                },
                {
                    "sent": "Basically this is a print shop so it's not a moms and moms and pops kind of shocked that there are many machines and the machines are some machines can print.",
                    "label": 0
                },
                {
                    "sent": "Some machines, can cut, some machines, can collect the pages and every such print shop can print.",
                    "label": 0
                },
                {
                    "sent": "Thousands of books PR.",
                    "label": 0
                },
                {
                    "sent": "And that's a true list.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scheduling problem, so it's a scheduling problem.",
                    "label": 1
                },
                {
                    "sent": "You have many machines.",
                    "label": 0
                },
                {
                    "sent": "The machines you can sort of future decide how to schedule each job so to which machines you want to send a different job.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again here you have, you want to maximize your reward, which means how many?",
                    "label": 0
                },
                {
                    "sent": "How many books or or cards or envelopes get out of your system and system as you see here.",
                    "label": 0
                },
                {
                    "sent": "It's pretty complicated system.",
                    "label": 0
                },
                {
                    "sent": "And and but there are also some operational constraints because machines breakdown sometimes.",
                    "label": 0
                },
                {
                    "sent": "There are things that are possible you cannot do and you have a large set of constraints that you can you can use.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a question where does the model come from?",
                    "label": 1
                },
                {
                    "sent": "Where did the model for the machines come from?",
                    "label": 0
                },
                {
                    "sent": "Machines breakdown?",
                    "label": 0
                },
                {
                    "sent": "They have stochastic behavior, that's something that might be a little bit surprising, but if you give us a single machine the same job to print twice, it will not.",
                    "label": 0
                },
                {
                    "sent": "It will make it may take a different amount of time and the reason that this machine is very complicated than the previous machine or what happened in the system affect what's going to happen now.",
                    "label": 0
                },
                {
                    "sent": "I mean, those machines are.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you've ever seen the huge big printers and there are fairly.",
                    "label": 0
                },
                {
                    "sent": "Complex, so I mean just modeling them is the problem and how you can build some very naive models of how the machines work, but that seems like a.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Challenge and I should mention this is fairly stochastic problem, so this is not a deterministic scheduling problem, but rather refers to classic problem and you can have models for different machines for the behavior of the press line.",
                    "label": 0
                },
                {
                    "sent": "So this is a second motivate.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the third one that's a motivation will touch more later on in stock.",
                    "label": 0
                },
                {
                    "sent": "Let's work with a large US or Taylor, whose name will remain disclosed.",
                    "label": 0
                },
                {
                    "sent": "And this is a Mail order catalog problem I've been working on this for several years.",
                    "label": 0
                },
                {
                    "sent": "This is just to show you this is the sort of the first Mail order catalog ever, and this is in the the 18 I think in 1884.",
                    "label": 0
                },
                {
                    "sent": "That's known as the Eaton catalog, so this problem has been around for many many years.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before and the question here is very simple, it's a marketing problem and that's why by the way in this work there is someone from the Business School.",
                    "label": 0
                },
                {
                    "sent": "So you want to know if you want to send or not.",
                    "label": 1
                },
                {
                    "sent": "To send this case there Miller catalog to a client, and I mean sending or not something that's a fair fairly isn't easy decision, but it's very simple if you if you send it to the client and he doesn't buy, then you lost $2 or so.",
                    "label": 0
                },
                {
                    "sent": "If you send it to the customer handed by then you gain some money depending on what the customer bought.",
                    "label": 0
                },
                {
                    "sent": "And this is just to give you an idea on how much data we have for this problem, so this is a problem where you have records of something like 2,000,000 customers over five years.",
                    "label": 0
                },
                {
                    "sent": "Records of what happened every two weeks, so that's a lot of data.",
                    "label": 0
                },
                {
                    "sent": "Of course the data is noisy and this this is a whole field in marketing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "There's a common wisdom and people sort of have a good intuitive idea of what to do in this problem, so there is a.",
                    "label": 0
                },
                {
                    "sent": "The wisdom is to do what's known as the FM method, so its recency, frequency and monetary value.",
                    "label": 1
                },
                {
                    "sent": "Basically you look at every customer and you look at the recently of the left left.",
                    "label": 0
                },
                {
                    "sent": "Purchase the frequency.",
                    "label": 0
                },
                {
                    "sent": "So how often this customer buys buys a good from you and the monetary value.",
                    "label": 0
                },
                {
                    "sent": "So how much you actually spends an you build?",
                    "label": 0
                },
                {
                    "sent": "Or you discretize somehow when you build a state space based on these measures?",
                    "label": 0
                },
                {
                    "sent": "And maybe some some other parameters.",
                    "label": 0
                },
                {
                    "sent": "You have an and you try to solve the MDP.",
                    "label": 0
                },
                {
                    "sent": "So that's what has been done in the past, and it's not very.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's very clear hard to do that.",
                    "label": 1
                },
                {
                    "sent": "So how do you decide to discretize?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I should mention that in this problem dynamics matter, so you might think.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe that's just a classification problem, I just need to know if the customer is going to buy or not buy and that maybe I should.",
                    "label": 0
                },
                {
                    "sent": "I should be solve it as a classification problem.",
                    "label": 0
                },
                {
                    "sent": "So now you shouldn't because there is a whole issue here of retaining customers and developing a relationship with the customer.",
                    "label": 0
                },
                {
                    "sent": "So basically think about life, you know somebody.",
                    "label": 0
                },
                {
                    "sent": "There's somebody who sends you a miracle, but if you don't buy today, maybe you will buy it.",
                    "label": 0
                },
                {
                    "sent": "You know, in a couple of weeks or in a month when you flip over the pages so Dynamics Matter and there is the issue of retainment of customer that seems seems crucial, and I said the problem here is how to how to discretize and I should mention that in this problem as well as well as in the other problems, nobody is going to use to let you use exploration.",
                    "label": 1
                },
                {
                    "sent": "So exploration is something that is completely out of the question even in this realm.",
                    "label": 0
                },
                {
                    "sent": "Certainly in the other two problems.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of money on the line, so so basically what's going to those around that you have lots of data, but you can't really touch it on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other ones so, the problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a said young faculty member.",
                    "label": 0
                },
                {
                    "sent": "You need to raise money, so there's a lot of lot of money on the line.",
                    "label": 1
                },
                {
                    "sent": "Those are real real problems.",
                    "label": 1
                },
                {
                    "sent": "The state space is huge, so in all the problems I mentioned the real state space is gigantic.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "There are many parameters and quite frankly the problems should probably not even mark off, so their problems himself a very complicated structure and we can't really, really expect to be able to solve them with.",
                    "label": 0
                },
                {
                    "sent": "Tools from for from with tool, for applying for MVP's.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other other distinctive feature that batch data are available you have lots of data, probably more than you actually want, but you cannot do exploration, so there is really no channel and we think about the mining problem.",
                    "label": 1
                },
                {
                    "sent": "I mean, there is my real money on the line here, and you really cannot do exploration.",
                    "label": 0
                },
                {
                    "sent": "You might be able to suggest the plan and this plan will be also examined in the other approaches, but there is no opportunity.",
                    "label": 0
                },
                {
                    "sent": "It's not a real enforcement learning problem, it's a real planning problem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what people do.",
                    "label": 0
                },
                {
                    "sent": "So the operative solution is you build a small MDP that you solve and then you apply.",
                    "label": 1
                },
                {
                    "sent": "The reason that this is an operative solution is there are several reasons for that.",
                    "label": 1
                },
                {
                    "sent": "You really need to find the number of parameters and just if you have 300 states with lots of data then it's really easy to solve.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "I mean I think.",
                    "label": 0
                },
                {
                    "sent": "Realistically, we can solve problems with up to 10,000 states quite easily, but we don't want to get their records of our information is just not good enough so we never don't get good enough information there and, but that's what people do here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For those of you who are in the know in RL or are planning approximate planning problems, there is a soul approach of function approximation.",
                    "label": 0
                },
                {
                    "sent": "There's function approximation mean that we write our value function in some functional form and try to solve an approximate version of a Bellman equation, at least to the best of my experience.",
                    "label": 0
                },
                {
                    "sent": "This doesn't really help us in such problems.",
                    "label": 0
                },
                {
                    "sent": "There are several reasons for that.",
                    "label": 0
                },
                {
                    "sent": "At the computational problem is not an issue.",
                    "label": 0
                },
                {
                    "sent": "So if you get your computational problem and you want to solve it, you will solve it.",
                    "label": 0
                },
                {
                    "sent": "I mean just just there is enough enough computational power to solve with specialized solvers.",
                    "label": 0
                },
                {
                    "sent": "I mean such rather setting the mining problem, we use linear programming with an integer program together with a very large problems, and this works quite well.",
                    "label": 0
                },
                {
                    "sent": "So function approximation doesn't buy you much in terms of computation.",
                    "label": 0
                },
                {
                    "sent": "The question is whether it buys you much much in terms of generalization, so whether or not if you can find a good approximation that would actually solve a generalization problem.",
                    "label": 0
                },
                {
                    "sent": "To my experience, the answer is no.",
                    "label": 0
                },
                {
                    "sent": "So basically if you know how to find a good function approximator.",
                    "label": 0
                },
                {
                    "sent": "Then you also know how to solve the problem by finding a smart states based representation.",
                    "label": 0
                },
                {
                    "sent": "This is my experience and and I'm not aware of any any works except for some works in in some rich location problems where function approximation really value by something.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to conclude, one of that and that was perhaps the lesson.",
                    "label": 0
                },
                {
                    "sent": "The most important aspect that risk uncertainty of are of the essence and in all the problems that we mentioned.",
                    "label": 1
                },
                {
                    "sent": "It's not really the average reward you care about the most, but rather there is some risk adjusted or suggested objective that you care about more.",
                    "label": 0
                },
                {
                    "sent": "And I will not.",
                    "label": 0
                },
                {
                    "sent": "Want to get into risk risk related optimization?",
                    "label": 0
                },
                {
                    "sent": "Those are very important and interesting aspects, but will talk today about the marszalek.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So the two important question is that we have is what model to use.",
                    "label": 0
                },
                {
                    "sent": "That's a question we're going to ask her today an if I choose a model.",
                    "label": 0
                },
                {
                    "sent": "How do I optimize the model with a risk adjusted objective in mind?",
                    "label": 0
                },
                {
                    "sent": "That's not for today that if you want to talk to me about it, I'll be happy to take it offline.",
                    "label": 0
                },
                {
                    "sent": "Many, many works on that.",
                    "label": 0
                },
                {
                    "sent": "So we'll talk about the problem of what model to use.",
                    "label": 0
                },
                {
                    "sent": "Before is get into the details and questions about the applications.",
                    "label": 0
                },
                {
                    "sent": "Too early in the morning.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is a we formulate some all selection problem.",
                    "label": 0
                },
                {
                    "sent": "In these applications, is the model order of the model itself the issue both so in how can you sort of distinguish between them?",
                    "label": 0
                },
                {
                    "sent": "Because some model might have a completely different structure and then different orders, then the other one.",
                    "label": 0
                },
                {
                    "sent": "That's right, so so I mean I'll discuss smaller order problems.",
                    "label": 0
                },
                {
                    "sent": "You do have them also model selection problem itself, which different basically the way it works is that you go to different consultants.",
                    "label": 1
                },
                {
                    "sent": "And they suggest from the company for outside and just suggest ideas for variables that would influence the dynamics in a crucial way.",
                    "label": 0
                },
                {
                    "sent": "And this is I mean the variables can be different and sometimes overlapping.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I mean completely out of the blue and you need to choose a model so we both problems are interest and you need to address both and it's.",
                    "label": 1
                },
                {
                    "sent": "This is the way it is OK, but the but the MODIS operandi in these applications is that the consultants gives you the model and you solve them or lower for them.",
                    "label": 0
                },
                {
                    "sent": "They give you the variables so they say they know it's very important that certain viable.",
                    "label": 0
                },
                {
                    "sent": "This is the reason to have the purchase is very important because of some psychological reason that you know someone like me can't really understand and then you say OK, recency is important.",
                    "label": 0
                },
                {
                    "sent": "Now what do you mean by recency?",
                    "label": 0
                },
                {
                    "sent": "This is last.",
                    "label": 0
                },
                {
                    "sent": "How do I do that?",
                    "label": 0
                },
                {
                    "sent": "But sometimes of course they suggest meaningless.",
                    "label": 0
                },
                {
                    "sent": "I mean as features that you need to find a way to eliminate.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I understand why you wouldn't have.",
                    "label": 0
                },
                {
                    "sent": "Expiration is an issue in that mining problem, and if there was one part of the mind that you were pretty confident you have, model would suggest it's not going to have much cold.",
                    "label": 0
                },
                {
                    "sent": "Does that mean you never know?",
                    "label": 0
                },
                {
                    "sent": "So you do have exploration in your policy eventually, but this is not explosion in terms of which features are important or which features the features are not, so you can't suggest a radical policy.",
                    "label": 0
                },
                {
                    "sent": "OK, the policies have to be more or less within.",
                    "label": 0
                },
                {
                    "sent": "What's acceptable just because I mean you can suggest it, but it's never going to happen, so whatever he's going to propose is going to be treated with a grain of salt and and the exploration is, I mean, expression within your model definitely is going to happen.",
                    "label": 0
                },
                {
                    "sent": "So if your policy tells you to explore that something that will happen, this something that they will do anyway.",
                    "label": 0
                },
                {
                    "sent": "I mean, they have to dig out the whole mind.",
                    "label": 0
                },
                {
                    "sent": "the Patriots will dig it all out, but in mining problems is the state is sort of for every.",
                    "label": 0
                },
                {
                    "sent": "I mean the divide the mind to the mind to essentially 3 dimensional cubes and there is the cubes himself form something which is like an like an MRF.",
                    "label": 0
                },
                {
                    "sent": "So their relationship of if you have gold here and here, you probably are going to find gold here as well.",
                    "label": 0
                },
                {
                    "sent": "And then there is the question is how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you make?",
                    "label": 0
                },
                {
                    "sent": "How do plan what, what cubes are explored and what which cubes are not explored?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's let's talk about the mall selection problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a simplification, and this is the model that we're going to consider, so we'll focus on a very simple problem, and we're going to ignore action completely, so this is not MVP, but rather what's known as MRP and Markov reward process.",
                    "label": 0
                },
                {
                    "sent": "So in a marker word process we have state we essentially something happens, we get a reward, and then we go to the next day.",
                    "label": 0
                },
                {
                    "sent": "So this is a simplified version of Markov chain processes, and we observe a sequence of three observations.",
                    "label": 0
                },
                {
                    "sent": "New words that occur in the in some space, so this space is rotated as or across R. So R is a reward and always just us is just a sample preservation space that can and usually will be fairly complicated, so we have this.",
                    "label": 0
                },
                {
                    "sent": "This is our the data that we have, so we have this observation reward opposition reward and so forth and we are given K mappings, so this mapping their mappings from observation to state spaces.",
                    "label": 0
                },
                {
                    "sent": "So somebody has an idea of K. State spaces that belong to KAMRPS, so those are the mappings, and each mapping describes a model, so those are the mappings is from observation to state spaces, and this is really just the state size.",
                    "label": 0
                },
                {
                    "sent": "The state space of MRP, MI.",
                    "label": 0
                },
                {
                    "sent": "Now in this talk we're not going to talk about how to have to construct the mapping, so we assume that those mappings that were given to us.",
                    "label": 0
                },
                {
                    "sent": "Possibly there are many such models.",
                    "label": 0
                },
                {
                    "sent": "OK, K can be possibly very very large.",
                    "label": 0
                },
                {
                    "sent": "So the mappings are given to us and basically our problem is to find the best mapping in.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sense of the word best.",
                    "label": 0
                },
                {
                    "sent": "So I'm all the selection criteria and it gets it takes as input a DT so it takes as input the stream of observations, the models and the functions.",
                    "label": 0
                },
                {
                    "sent": "An functions HK an returns one of those says there supposed to be a true model and the definition here is that the model selection is.",
                    "label": 1
                },
                {
                    "sent": "We're consistent if if when model I.",
                    "label": 0
                },
                {
                    "sent": "Is it true model than the probabilities that will choose some other model goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So that's I think, a fairly natural and and simple definition of the model selection problem.",
                    "label": 1
                },
                {
                    "sent": "So now we have K models and you want to choose one of those.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first thing that you would try to do is to try to probably take a penalized likelihood criteria approach, such as MDL, Akaike information criteria based on information criteria and so forth.",
                    "label": 0
                },
                {
                    "sent": "So, just to recap how it works well, you look at the likelihood of observations given model I.",
                    "label": 1
                },
                {
                    "sent": "So you maximize overall parameters and here just look at the log likelihood of the trajectory given the model and the dimension of the parameter is just.",
                    "label": 1
                },
                {
                    "sent": "The number of parameters of the marker reward process.",
                    "label": 0
                },
                {
                    "sent": "It's anyway linear in that and then in MD LM DL type estimator is simply has the following structure.",
                    "label": 0
                },
                {
                    "sent": "So you want to you want to look at the MDF model.",
                    "label": 0
                },
                {
                    "sent": "I is the norm of.",
                    "label": 1
                },
                {
                    "sent": "The number of parameters time some function FT minus LFT, where FT is some sub linear function and this is sort of classical recent type analysis and there are many other related criteria and different justification that something I will hear more about later on today.",
                    "label": 0
                },
                {
                    "sent": "Vision on beige and complexity complexity type.",
                    "label": 0
                },
                {
                    "sent": "Capitalization and so forth.",
                    "label": 0
                },
                {
                    "sent": "This is sort of not really what we care about here.",
                    "label": 0
                },
                {
                    "sent": "This is just the shape that the way this model looks like.",
                    "label": 0
                },
                {
                    "sent": "What's important here that we have the number of parameters times a sublinear element minus the log likelihood, so so far this is.",
                    "label": 0
                },
                {
                    "sent": "This slide is supposed to be known to to all of you.",
                    "label": 0
                },
                {
                    "sent": "Let's be honest, black cloud.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the first thing possibly resulted that there does not exist a consistent or even a weekly consistent MD like Criterium and the following example is an example show that it's very simple example.",
                    "label": 0
                },
                {
                    "sent": "So here we have two MRP's.",
                    "label": 0
                },
                {
                    "sent": "The first one it has a single state where so you always stay in this state and the reward is a uniform between minus two and one.",
                    "label": 0
                },
                {
                    "sent": "The second one has two states, says Zero State one and you move between them with probability.",
                    "label": 0
                },
                {
                    "sent": "Half the reward in State Zero is minus one one.",
                    "label": 0
                },
                {
                    "sent": "The awarding state one is.",
                    "label": 0
                },
                {
                    "sent": "Between zero and two.",
                    "label": 0
                },
                {
                    "sent": "So if I'll give you data that essentially the data will have some, there will be.",
                    "label": 0
                },
                {
                    "sent": "There will be some very complicated observation here.",
                    "label": 0
                },
                {
                    "sent": "Maybe in very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then it will be mapped either to a constant stream of 0 or an alternate stream of zeros and one somehow.",
                    "label": 0
                },
                {
                    "sent": "I don't don't really care how this transformation happens, and then I'll see rewards.",
                    "label": 0
                },
                {
                    "sent": "So their words in all cases that are going to be between minus one and two so.",
                    "label": 0
                },
                {
                    "sent": "Every stream of observation or it can be interpreted as belonging as if it belongs to model one, or as if it belongs to Malta.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, so it should be minus one and two.",
                    "label": 0
                },
                {
                    "sent": "So this is an essential property because otherwise I mean I need to be able to interpret every data point with probability that is bigger than 0.",
                    "label": 0
                },
                {
                    "sent": "Otherwise of course the problem is uninteresting.",
                    "label": 0
                },
                {
                    "sent": "So suppose now that Model 2 is the more complicated model, Model 2 is a true model.",
                    "label": 0
                },
                {
                    "sent": "Let's see what the likelihood ratio is.",
                    "label": 0
                },
                {
                    "sent": "So here basically the probability of every trajectory internal transition is 1.",
                    "label": 0
                },
                {
                    "sent": "The probability of the reward is minus 1/3 to the power of T. So here we always get.",
                    "label": 0
                },
                {
                    "sent": "1/3 so.",
                    "label": 0
                },
                {
                    "sent": "So Model 1.",
                    "label": 0
                },
                {
                    "sent": "So it's 1/3 of the power of T Model 2.",
                    "label": 0
                },
                {
                    "sent": "So here we get basically half for every half a probability of half for every transition and probability, half for the reward.",
                    "label": 0
                },
                {
                    "sent": "So basically here we have just one over half one over half to the power of T. So given our definitions.",
                    "label": 0
                },
                {
                    "sent": "When you look at the log likelihoods of the log likelihood of model one is going to behave like something that grows linearly in T and and same for Model 2.",
                    "label": 0
                },
                {
                    "sent": "So the linear term is going to be the likelihood term is going to be dominant, and it's always going to be bigger.",
                    "label": 0
                },
                {
                    "sent": "For Model 1, just.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think about the definition.",
                    "label": 0
                },
                {
                    "sent": "So here we have something sub linear.",
                    "label": 0
                },
                {
                    "sent": "So this is the first time is going to be to go to vanish eventually and the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second term, the lack of the term is always going to be bigger Formula One.",
                    "label": 0
                },
                {
                    "sent": "Therefore we will always prefer the trivial model.",
                    "label": 0
                },
                {
                    "sent": "So in MD like Criterion that the matter how you're going to choose the function is also going to prefer the first model as young, as long as you look at the likelihood of.",
                    "label": 0
                },
                {
                    "sent": "Of the of the trajectory.",
                    "label": 0
                },
                {
                    "sent": "So basically, and that's a kind of slightly surprising result, you cannot do something which is MD like, yes.",
                    "label": 0
                },
                {
                    "sent": "All the way.",
                    "label": 0
                },
                {
                    "sent": "The way you are applying DL, I mean you apply in DL 2 States and rewards for states are not observations.",
                    "label": 0
                },
                {
                    "sent": "So if you're one mapping which Maps to huge state space and the other two small one, you try to code with MPL completely different things.",
                    "label": 0
                },
                {
                    "sent": "Because I agree, I agree I didn't say I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't say that.",
                    "label": 0
                },
                {
                    "sent": "I mean will show how to fix it shortly.",
                    "label": 0
                },
                {
                    "sent": "The Navy Navy application.",
                    "label": 0
                },
                {
                    "sent": "The Navy application of MDL doesn't work there.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My point, so I mean how to make it, how to make it work.",
                    "label": 0
                },
                {
                    "sent": "So there basically in this problem we can look at something that we call the accurate prediction error and then we can do we accurate errors.",
                    "label": 0
                },
                {
                    "sent": "The reward prediction error of every every model and there are two types of regression toward the condition in transition probability aggression and this talk just to simplify things, I want to talk about reward aggregation.",
                    "label": 0
                },
                {
                    "sent": "Everything can also can also be be done with transition probability aggregation.",
                    "label": 0
                },
                {
                    "sent": "And basically we take their, we collapse their offer different states, we encode it.",
                    "label": 0
                },
                {
                    "sent": "In a different way, and then we do something which is like M deal.",
                    "label": 0
                },
                {
                    "sent": "So we do M deal in a different fashion, not on the on the on the not on the trajectory is but rather on the quality of our prediction of the reward.",
                    "label": 0
                },
                {
                    "sent": "And to make things even simpler we will focus on refined models.",
                    "label": 0
                },
                {
                    "sent": "So we say that Model 2 which finds Model 1 basically if we can.",
                    "label": 0
                },
                {
                    "sent": "In Split in a consistent manner, states in M1 and Gator 2M2.",
                    "label": 1
                },
                {
                    "sent": "So this is a certain intuitive definitions.",
                    "label": 0
                },
                {
                    "sent": "But I mean the more precise definition is just a little bit a little bit annoying, but I think about it.",
                    "label": 0
                },
                {
                    "sent": "So if you have one and then just just take a take a state split in two or three days, take a state and split it into some pieces and then you get a new a new model which is a refinement.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous one so now we can do we order a creation.",
                    "label": 0
                },
                {
                    "sent": "So we define the reward mean square error of this is a reward of model I for the data, so we sum over the data and now we found the error in State Jane Model I for the reward estimate.",
                    "label": 0
                },
                {
                    "sent": "So basically we just collect the reward estimates for every for every state we look at the mean square error an essentially this is this is it?",
                    "label": 1
                },
                {
                    "sent": "What happens here?",
                    "label": 0
                },
                {
                    "sent": "That in the limit we have that reward, the criterion as T goes to Infinity goes in every probabilistic manner that you like to the sum over over the states.",
                    "label": 1
                },
                {
                    "sent": "This is a steady state distribution.",
                    "label": 0
                },
                {
                    "sent": "This is just the variance of the reward.",
                    "label": 0
                },
                {
                    "sent": "When you get when you are in state State X.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically if you like we look at just the sum of variances.",
                    "label": 0
                },
                {
                    "sent": "Empirical variances of the rewards per state so.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's our criterion and now we look at that.",
                    "label": 0
                },
                {
                    "sent": "We have a very simple result, but still it's a it's kind of interesting.",
                    "label": 1
                },
                {
                    "sent": "So if MI, MI MI contains MK.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a refinement here then for a single trajectory we are also increases.",
                    "label": 0
                },
                {
                    "sent": "This is not like a probabilistic clean.",
                    "label": 0
                },
                {
                    "sent": "This is the claim that always happens.",
                    "label": 0
                },
                {
                    "sent": "Well the reason is that if you divide rewards too, if you have a bunch of batch of rewards and divide 2 pieces then your virus is going to.",
                    "label": 0
                },
                {
                    "sent": "They were on.",
                    "label": 0
                },
                {
                    "sent": "Your average virus is going to decrease.",
                    "label": 0
                },
                {
                    "sent": "So so basically, and we get the following corollary.",
                    "label": 0
                },
                {
                    "sent": "So if we have the series of refining, also M1 is less refined than M2 and so forth until MK, then the RMC criterion of the model one is higher than mbtu an so forth.",
                    "label": 0
                },
                {
                    "sent": "So basically the more refined the model is, the lower the reward.",
                    "label": 0
                },
                {
                    "sent": "Mean square error is.",
                    "label": 0
                },
                {
                    "sent": "So that's our we.",
                    "label": 0
                },
                {
                    "sent": "Basically we took a problem and we converted it.",
                    "label": 0
                },
                {
                    "sent": "To just estimates a reward error that's very simple operation to do an in the case of refined models, we have this this inequality in the case of rewards, that of models are not refined.",
                    "label": 0
                },
                {
                    "sent": "This does not hold and you need to do a little bit more aerobics to solve the problem.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we have an MVL type criterion, so the our criterion is we'll look at that, something that is like the complexity of the model times a sub linear function plus the loss the RMC loss for every model, and we just pick them all with the lowest lowest reward MSE.",
                    "label": 0
                },
                {
                    "sent": "So that's basically our GNU GNU Model order cartoon and this is very simple to do, very simple to implement, you just need to collect.",
                    "label": 0
                },
                {
                    "sent": "The reward in every state computer mean square error and just find the one with the lowest lowest.",
                    "label": 1
                },
                {
                    "sent": "Lowest model, so the theorem and the result which I will not prove here today at this small selection selector is actually consistent and you can also get a finite time analysis rates.",
                    "label": 0
                },
                {
                    "sent": "And I mean basically now this here is you know you can get whatever you can imagine with many exponential and write it over three slides.",
                    "label": 1
                },
                {
                    "sent": "So, so we have now a criterion that allows you to select select a model based on diversity in the reward.",
                    "label": 0
                },
                {
                    "sent": "So of course, if you're always always zero, you cannot choose.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's nothing you can do, so you can do the same criterion with probability distributions, but now it's.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was looking at instead of looking at expected error here, you have to look at something which is a little bit more difficult, and that's a vector valued expected error.",
                    "label": 0
                },
                {
                    "sent": "Fewer at transition probabilities becomes a little bit measure, but you can still get a similar result.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eventually you can combine the two and then you can.",
                    "label": 0
                },
                {
                    "sent": "You can distinguish between every two models that have are different, have some difference in terms of reward or transition probability.",
                    "label": 0
                },
                {
                    "sent": "So as far as we're concerned the problem is solved as long as the true model is one of your candidate models in the batch.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just mention something which is kind of nice here, that the test that we have is really comparative test.",
                    "label": 0
                },
                {
                    "sent": "So you can compare two models and ask which model is a better one.",
                    "label": 0
                },
                {
                    "sent": "And here is just an example of here.",
                    "label": 0
                },
                {
                    "sent": "You have a model with a single state, so 1, two and three in a row collect are all collapsed together.",
                    "label": 0
                },
                {
                    "sent": "This is a coherent will have to state spaces that have two states each and have a different different collapsing structure.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "We have the two F2 and three together.",
                    "label": 0
                },
                {
                    "sent": "He will want it together and so forth, and you can build an algorithm that would do pairwise comparisons so you get pairwise comparison and then you get better convergence rates and better guarantees.",
                    "label": 0
                },
                {
                    "sent": "That you sent essentially traverse along a tree and you start from other from from Baltimore from top and you can prove that even if you have a fairly complicated model selection with thousands and thousands of models.",
                    "label": 0
                },
                {
                    "sent": "So the number here can be exponential in principle.",
                    "label": 0
                },
                {
                    "sent": "Then you can still find this problem.",
                    "label": 0
                },
                {
                    "sent": "The reason that you're going to get an exponential number of models is when sometimes you have you really trying to do a discretization problem.",
                    "label": 0
                },
                {
                    "sent": "So I wanna descriptor just.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to discretize.",
                    "label": 0
                },
                {
                    "sent": "So you know, even if the problem itself is fairly simple, the number of models that you might want to consider eventually is going to be huge mean, possibly exponentially in the parameters of the problem.",
                    "label": 0
                },
                {
                    "sent": "So there is a hierarchical way to do it because of pairwise.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comparisons, so just make sure you just experiment with artificial data.",
                    "label": 0
                },
                {
                    "sent": "So here we generated the.",
                    "label": 0
                },
                {
                    "sent": "There was a random generated randomly generated.",
                    "label": 0
                },
                {
                    "sent": "RP with us.",
                    "label": 0
                },
                {
                    "sent": "30 States and you see this hockey stick figure that will you know.",
                    "label": 0
                },
                {
                    "sent": "Love to love to observe and this is basically what model or selection criteria is going to get you and and the rate here.",
                    "label": 0
                },
                {
                    "sent": "The rate of the slope will be determined by the way you normalize things, so this is going to be determined.",
                    "label": 0
                },
                {
                    "sent": "This function F T / T that you choose.",
                    "label": 0
                },
                {
                    "sent": "So this is artificial data an this is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will happen if you look at the trajectory and do something like VICS or AIC.",
                    "label": 0
                },
                {
                    "sent": "Basically this is inverted, so both criteria going to prefer to select.",
                    "label": 0
                },
                {
                    "sent": "They're going to be preferred to select very small state spaces.",
                    "label": 0
                },
                {
                    "sent": "Actually I just.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My previous one that you you ran, this standard deviation of the the error bars, right?",
                    "label": 0
                },
                {
                    "sent": "That's over several trajectory right over sample trajectory is in.",
                    "label": 0
                },
                {
                    "sent": "So let's say something which is a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting, so this is an experiment with real data, and that's with the mailing Mail order company Ann.",
                    "label": 0
                },
                {
                    "sent": "And here we have our fair measures.",
                    "label": 0
                },
                {
                    "sent": "So just to remind you, there is.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a retailer and it needs to decide whether or not to send Mail order catalogs to customers.",
                    "label": 0
                },
                {
                    "sent": "There are three 3 axis here.",
                    "label": 0
                },
                {
                    "sent": "Recently frequency and monetary value and we're going to focus on on recent suggests because this is the only thing I can I can show you on the graph.",
                    "label": 0
                },
                {
                    "sent": "Of course we do that for all for all criteria and the question that you can ask is how to aggregate.",
                    "label": 0
                },
                {
                    "sent": "So now you have in principle you have a very fine discretization and you want to know how should I activate my the recency effect so I can add with randomly.",
                    "label": 0
                },
                {
                    "sent": "Basically I can just say well, I start saying with you know 500 different states.",
                    "label": 0
                },
                {
                    "sent": "I mean just for recency.",
                    "label": 0
                },
                {
                    "sent": "So 500 recently numbers.",
                    "label": 0
                },
                {
                    "sent": "I can decide just to pick two at random algorithm.",
                    "label": 0
                },
                {
                    "sent": "I can answer it according to most recent.",
                    "label": 0
                },
                {
                    "sent": "So the most recent.",
                    "label": 0
                },
                {
                    "sent": "Aggregation most recent states represent most recent acquisitions are going to be algorithm aggregated or the least reason so the one that were, you know happen that the way way in the past are going to be aggregated.",
                    "label": 0
                },
                {
                    "sent": "So of course list recent is makes the most sense because if somebody bought you know 500 weeks ago or 498 weeks ago, they probably don't even remember the name of the company.",
                    "label": 0
                },
                {
                    "sent": "So at least this makes most sense.",
                    "label": 0
                },
                {
                    "sent": "Most recent seems like a silly idea and randomly we don't really know whether that's going to how that's going to work.",
                    "label": 0
                },
                {
                    "sent": "So most recent means that you know if somebody bought a week ago or two weeks ago will put them in the same bin.",
                    "label": 0
                },
                {
                    "sent": "Probably don't want to do that because there are some customers that buy every every.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rick.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, so.",
                    "label": 0
                },
                {
                    "sent": "This is the score that we get.",
                    "label": 0
                },
                {
                    "sent": "And there are three.",
                    "label": 0
                },
                {
                    "sent": "Three colors are for different values of F T / T We don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have the luxury here of changing the size of the data.",
                    "label": 0
                },
                {
                    "sent": "This is just the data that we have.",
                    "label": 0
                },
                {
                    "sent": "And you see this.",
                    "label": 0
                },
                {
                    "sent": "As you can see that if you look at the random random aggregation, so you just take the random recency, you get pretty much reasonably the number of states that you need is around.",
                    "label": 0
                },
                {
                    "sent": "You know, 60 and if you do it according to the lowest frequency you get this.",
                    "label": 0
                },
                {
                    "sent": "Slatted Lowell number so around that 40 and if you do it according to the highest and things just don't look don't make sense anymore so that's just you know this is this seems like a bad idea.",
                    "label": 0
                },
                {
                    "sent": "Sounds like a good idea at this.",
                    "label": 0
                },
                {
                    "sent": "Seems like an OK idea, but probably not the best, but something which is The thing is is rather remarkable is that you do observe that there seems to be something which is very reasonable to be done like there seems to be a. I mean, if you want the model order here can be reasonably reduce from therrell hundreds.",
                    "label": 0
                },
                {
                    "sent": "Two, I know 3040 something like that.",
                    "label": 0
                },
                {
                    "sent": "Which, by the way, later on will be fed into an MDP type problem, an optimized and then used to find an optimal or supposedly optimal model selection policy.",
                    "label": 0
                },
                {
                    "sent": "Sorry, what is the score here?",
                    "label": 0
                },
                {
                    "sent": "Through the score of the signing, which is the right state?",
                    "label": 0
                },
                {
                    "sent": "So this is our MDA like score with the RMC.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so so let me conclude.",
                    "label": 0
                },
                {
                    "sent": "I want you to mention some things about this problem, so this is a very special and stylized more selection problem.",
                    "label": 0
                },
                {
                    "sent": "I think we still made some headway stalling it.",
                    "label": 0
                },
                {
                    "sent": "Very vanilla approach fails, but modification of the MDL philosophy here works.",
                    "label": 0
                },
                {
                    "sent": "There is a big lie here that I made in the lies that I assumed in the analysis that the true model is one of my models.",
                    "label": 0
                },
                {
                    "sent": "That's of course the big lie it can be worked out.",
                    "label": 0
                },
                {
                    "sent": "At least we have some idea on how to work hard to make it work when the true model is very close to one of the models that you have when the model is far, at least we are.",
                    "label": 0
                },
                {
                    "sent": "We are at loss.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "I'll be happy to hear idea of what to do when when the true model is not even close to one of the malls in your batch.",
                    "label": 0
                },
                {
                    "sent": "I didn't say anything about optimization, so the optimization of course is lurking here behind an.",
                    "label": 0
                },
                {
                    "sent": "I mean, this eventually should lead to an optimization problem that has to be solved and there is a question of how to aggregate.",
                    "label": 0
                },
                {
                    "sent": "So we gave an example that how to do aggregation or how to do feature, how to, how to do it in the recency, access for the Miller Kaplan problem.",
                    "label": 0
                },
                {
                    "sent": "But it's just one example and the question is the general question of how to aggregate.",
                    "label": 0
                },
                {
                    "sent": "Seems to be a quite.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite interesting, I just wanted to finish with an outlook so so when you do learning from batch as opposed to reinforcement learning, the real question here of what is the objective?",
                    "label": 0
                },
                {
                    "sent": "So are you really trying to optimize your expected reward?",
                    "label": 0
                },
                {
                    "sent": "The answer is often known.",
                    "label": 0
                },
                {
                    "sent": "The answer is often I have a risk aware reward and this is something that you really you really need to be aware of this work, I mean is geared towards finding the right model.",
                    "label": 0
                },
                {
                    "sent": "If such a model exists and then try to solve the objective with a separate optimization problem, this may not be the cleverest thing to do, so it maybe you need to do both.",
                    "label": 0
                },
                {
                    "sent": "Maybe you think about optimization and more selection.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the same time.",
                    "label": 0
                },
                {
                    "sent": "Finding the model is easy as long as you have the right model.",
                    "label": 1
                },
                {
                    "sent": "That's the conclusion here.",
                    "label": 0
                },
                {
                    "sent": "Once you don't have the right model or the right set of models, and that's a different story.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a little longer today.",
                    "label": 0
                },
                {
                    "sent": "I think we'll hear and witness a.",
                    "label": 0
                },
                {
                    "sent": "There was a poster about learning the model actively, so that's a different problem.",
                    "label": 1
                },
                {
                    "sent": "And once you do, you can actively select action that will tell you something that whole different ball game.",
                    "label": 0
                },
                {
                    "sent": "Very interesting.",
                    "label": 0
                },
                {
                    "sent": "There is a great question here.",
                    "label": 0
                },
                {
                    "sent": "How to combine things so when you have a lot of data and when you somebody tells you I'm going to allow you to sample a few, a few samples actively.",
                    "label": 0
                },
                {
                    "sent": "So the Miller catalog tells you, you know there we have 2 million customers.",
                    "label": 0
                },
                {
                    "sent": "Here are 500.",
                    "label": 0
                },
                {
                    "sent": "Play with them.",
                    "label": 0
                },
                {
                    "sent": "Do whatever you like is not going to affect us anyway, so there's a question here.",
                    "label": 0
                },
                {
                    "sent": "What can be done?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your.",
                    "label": 0
                },
                {
                    "sent": "From an optimization problem, I just want to mention that we know how to handle parametric uncertainty quite well and inherent uncertainties.",
                    "label": 0
                },
                {
                    "sent": "So those services uncertain the parameter that is not something that is related to the model, so that having just random transitions.",
                    "label": 0
                },
                {
                    "sent": "How to do all of them together?",
                    "label": 0
                },
                {
                    "sent": "That's something that seems to be beyond us at this point.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, there is the issue of of large state spaces.",
                    "label": 1
                },
                {
                    "sent": "So what we did here today is geared toward finding something small.",
                    "label": 0
                },
                {
                    "sent": "Question here is, who really cares if you have a large state space you can't solve it anyway and you have a lot of difficulties, but that still may be from a statistical perspective an interesting questions.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your time.",
                    "label": 0
                },
                {
                    "sent": "I think I'm more less about time.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how does this approach or this method relates to previous works?",
                    "label": 0
                },
                {
                    "sent": "An extension of MDL to important learning?",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course so.",
                    "label": 0
                },
                {
                    "sent": "Our goal here is on our focus, either on statistical properties and eventually on the planning problem.",
                    "label": 0
                },
                {
                    "sent": "So this is really not to work in RLI mean.",
                    "label": 0
                },
                {
                    "sent": "There is nothing here to do with RL, it's more about understanding the dynamics of the problem, their relationship.",
                    "label": 0
                },
                {
                    "sent": "But we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't care so much about about the feature selection problem.",
                    "label": 1
                },
                {
                    "sent": "Just because we care about the model selection problem, they're related, but at least to the best of my understanding, you cannot get a result, specially not the rate results that we obtain, and which I did not.",
                    "label": 0
                },
                {
                    "sent": "Present here.",
                    "label": 0
                },
                {
                    "sent": "From the very beginning I didn't understand, so I mean if you have these features you know, and you know if you select these features or other features or more or less, you get a different state space and then you want to select them right, right, right?",
                    "label": 0
                },
                {
                    "sent": "So that's exactly what feature of enforcement us.",
                    "label": 0
                },
                {
                    "sent": "It selects a feature or not, or with features.",
                    "label": 0
                },
                {
                    "sent": "Or.",
                    "label": 0
                },
                {
                    "sent": "I mean you can start without it limited number of features and asking how should this feature be included or not.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "As I said, I think at least to the best of my knowledge.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can correct me afterwards and an enlightened me.",
                    "label": 0
                },
                {
                    "sent": "But the rate of convergence rates that we have and the probabilistic guarantees aren't are not given in RL approaches that you are usually geared.",
                    "label": 0
                },
                {
                    "sent": "Sure, that reducing some sort of Bellman error type criterion to the best of my knowledge.",
                    "label": 0
                },
                {
                    "sent": "Let's take it offline.",
                    "label": 0
                },
                {
                    "sent": "For features, right, well with long socks feature will select models so you can think about it as a special type of features, but as far as I know, because those are some very special features you can get convergence rates and guarantees.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, just get home.",
                    "label": 0
                },
                {
                    "sent": "Order I missed this problem that the moment where you are the true model is not in your.",
                    "label": 0
                },
                {
                    "sent": "You said we can achieve good result in the true model is closing right?",
                    "label": 0
                },
                {
                    "sent": "What mean close so close here means in a in a sense of.",
                    "label": 1
                },
                {
                    "sent": "So if you model is, think about it.",
                    "label": 0
                },
                {
                    "sent": "If you have a perturbation of the model, we do parameter estimation anyway, But if you add more or a few more state spaces than states states to the system, as long as the reward the RMC, so the reward mean square is kind of similar, then we're fine.",
                    "label": 0
                },
                {
                    "sent": "Once it's hit some value, then at least our analysis collapses.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying it's impossible, just I don't know how to do it.",
                    "label": 0
                },
                {
                    "sent": "You don't use distance notion, no, because it's a distance between different state spaces, so you need to say to figure out what the distance should be.",
                    "label": 0
                },
                {
                    "sent": "The distance notion is between the reward mean square errors.",
                    "label": 0
                },
                {
                    "sent": "Quickly when you have the real data, you just have one structure on trajectory.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and a long one, but you didn't split it.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can split it for for your own your own reasons, but that's a statistical tool, basically.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}