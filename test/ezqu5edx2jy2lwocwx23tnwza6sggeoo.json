{
    "id": "ezqu5edx2jy2lwocwx23tnwza6sggeoo",
    "title": "Distance Metric Learning for Kernel Machines",
    "info": {
        "author": [
            "Kilian Q. Weinberger, Department of Computer Science, Cornell University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Learning to Rank",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_weinberger_dml/",
    "segmentation": [
        [
            "So this is joint work with some team member and this is joint work with Eddie due is my student Olivia Chappelle who's at Yahoo Research and facial?",
            "Who's at University of Southern California?",
            "And quick warning.",
            "This talk may contain amounts of newer network."
        ],
        [
            "And traces of nonconvexity anyone leaving good heavens?",
            "So yeah, alright."
        ],
        [
            "I'm talking bout metric learning and metric learning is really kind of about learning of similarities, so dis similarities and so of course the question is.",
            "What do we mean by similarity and well, let me."
        ],
        [
            "Give you an example.",
            "So imagine you have a database of images.",
            "And imagine if this image here on the left and you would like to find the most similar image in the database to that query and so well, do you know?",
            "Depends you know what you measure.",
            "Similarity is so you know once I want."
        ],
        [
            "I'd say, well, you know, I'm really care about similarity in gender.",
            "Well then this person here on the right might be a very good match.",
            "Someone else might have the same image database, but makes you say well gender is not what I care about at all.",
            "I care about age, so then the person here in the."
        ],
        [
            "It is actually a much, much better match in the third person might come along and have yet another different."
        ],
        [
            "And of similarity.",
            "So what I claim is, what do you mean by 7?"
        ],
        [
            "Pretty really depends on the context, so you know what is our problem that we are trying to solve.",
            "What is the data that we have, etc.",
            "Very often people when they."
        ],
        [
            "You know, compare data instances.",
            "The default way to do this is the Euclidean distance.",
            "A measure of dissimilarity.",
            "And very simple extension of the Euclidean distances.",
            "The Mahalanobis distance.",
            "My nervous system is basically the same thing as Euclidean distance.",
            "The only difference is that you stick a matrix M here inside of this product, and this matrix M has to be positive semidefinite."
        ],
        [
            "So what does that do to my numbers?",
            "Mr. Metrics metric.",
            "In some sense, this matrix M here scales some dimensions off in the Euclidean distance that say you have some point in me.",
            "Make a set of points at all have exactly the same distance to the center.",
            "That's a circle, whereas in the Mount of assistance the same circuit would actually form to an ellipsoid.",
            "So in this case for example, you would stretch this dimension.",
            "Here we would squeeze this dimension.",
            "And so that's important.",
            "In this talk we're talking about learning this may."
        ],
        [
            "This matrix here.",
            "And by the way, the positive definite constraints that always a little scary in this talk actually not to worry about it, so it turns out a matrix is positive definite if and only if you can decompose it into square of two real matrices.",
            "And this will just focus about these real matrices, so I will not actually mentioned this constraint anymore.",
            "You can just get around this no problem.",
            "Alright, so this is important."
        ],
        [
            "Go to important for many machine learning algorithms.",
            "In particular, the prime example is K nearest neighbor classification.",
            "So K nearest neighbor classification.",
            "Basically you know 100% relies on a measure of similarity.",
            "The algorithm is very simple.",
            "You have a training data set and imagine you have test data point you would like to know the label of the test data point.",
            "What do you do?",
            "You just look at the nearest neighbor.",
            "In this case it would be this fall.",
            "Yeah just assign the label of the nearest neighbor and if you have K is larger than one then you take the K nearest neighbors and you take the majority vote amongst those and.",
            "In this case, for example, you can see well if we had this point here, it would be classified as a soccer ball, but that's probably wrong, right?",
            "Most of you would ask you what is the label of this guy?",
            "You would probably say it's actually a basketball.",
            "And why is that?",
            "Because if you look at this data set, you realize that this dimension, the vertical dimension here, actually is much more informative towards class membership than the horizontal dimension.",
            "So data points could be could have very small horizontal distance, but they still actually in different classes.",
            "But the data points that have large vertical distance.",
            "You know the neighbors with the vertical distance are always in the same class, so if you want to run."
        ],
        [
            "Metric that as well on this this data said she would run over metric something like this where basically now you know all the points on the yellow circle here at the same distance from the query and now the nearest neighbor is this point here.",
            "This point is much closer than this point, so we would classify the point correct.",
            "That's been a lot of work on this in the last five.",
            "I don't know.",
            "I guess 8."
        ],
        [
            "Years by now, people proposing all sorts of algorithms to learn these maneuvers.",
            "Metrics for K nearest neighbors and for K means another algorithms that heavily rely on these metrics, and the all of these algorithms have different advantages and disadvantages.",
            "Not going to go over them.",
            "And but one thing that a lot of people ask me is, well, you know, once you learn that kind of metric, right?",
            "So now we can use seniors neighbors but could also use a different algorithms.",
            "So for my data set, maybe 10 years neighbors is not the best algorithm.",
            "You know.",
            "I want to support Vector Machine could also use a support vector machine with the metric that you learned here."
        ],
        [
            "And so that's kind of what I investigated here for actually, especially for this workshop.",
            "And I called the support."
        ],
        [
            "Back to metric learning.",
            "So why is that a good idea?",
            "While if you look at support vector machines and in particular let's look at the RBF kernel, the RBF kernel is defined as this equation here.",
            "So basically all it is basically the Euclidean."
        ],
        [
            "Distance if you expansion, exponentiate the Euclidean distance.",
            "So support vector machines only access data points in terms of the kernels or indirectly in terms of the Euclidean distance.",
            "So it's a very natural question to ask.",
            "Well, why don't we just take this distance here?",
            "And instead of using Euclidean Dist."
        ],
        [
            "And stick in the matrix M and that's the monovisc distance that we learned with one of these algorithms.",
            "And you know, it's definitely worth trying that out, so I did."
        ],
        [
            "Here's a usage couple of datasets just to explain the baseline.",
            "Everything is reported relative to support Vector machine under the Euclidean distance where I chose the kernel width and the private parameters with cross validation.",
            "So here a bunch of UCI datasets and we just chose me to try to choose all the all the binary datasets and.",
            "These are different algorithms, different metric learning algorithms, information theoretic, metric learning."
        ],
        [
            "I buy Brian, who is also here in the audience this NCA.",
            "The samurais also did an element of the algorithms that I worked on and so this is the error relative to the Euclidean metric so lower is better in this case and so one thing you realize is, well, you know it doesn't really work right.",
            "Like you know, if you see these bars, most of them are actually higher than Euclidean metric.",
            "Here we have a little bit of improvement but nothing to write home about and actually the worst part about this graph is that my algorithm is the worst, so you know something has to be done here.",
            "I'm in.",
            "That's right, so I'm first learning at first learn using this metric learning algorithms, learn a metric and then I use SVM using that metric.",
            "That's the only difference.",
            "I just started playing this case.",
            "Oh yeah, so sorry we did that.",
            "Sorry yeah OK. Alright so Francis question was what we just plug in that."
        ],
        [
            "Metric here, here that you know you still need to learn that hyperparameter here, right?",
            "That was kind of parameter.",
            "We did that by cross validation.",
            "But it still didn't really."
        ],
        [
            "To work on so the so why is that?",
            "And my explanation for this as well these algorithms, all of these in one way or another, mimic the leave one out loss of K nearest neighbors, so they are a little bit too specialized to K nearest neighbors, and so the RBF kernel is similar to nearest neighbors.",
            "You basically put a Gaussian around you test point and then you measure similarity in that waited for him.",
            "But one thing is that in super vector machine you have support vector, so most of the data points actually have zero wait, so you don't really care about them and so that's really something you should take into consideration when you're learning metric.",
            "So the question of course, is well, can we learn a specific metric that you know works better with support vector machines?",
            "And so."
        ],
        [
            "Remind you this year.",
            "Support vector machine.",
            "I think most of you familiar with this.",
            "It's basically if you're not familiar with this doesn't matter, but there's the Kernel workshop, but the basically just Wan optimization problems because automation problem.",
            "So the idea is, well, how can we learn a metric for that was better for in this optimization problem and the metric appears inside this kernel function here and the idea is very simple.",
            "We just do a wrapper around it so."
        ],
        [
            "Kind of stick this optimization problem inside another optimization problem where we learn a metric unit matrix M and what is the objective of this optimization problem?",
            "Here we just decided well, we just try to do Bell on the validation data set.",
            "So we split our data set and training and validation.",
            "This year is trained on the training data set and then we say overall well.",
            "You know it's kind of like cross validation.",
            "We want to have a metrics on the validation data set.",
            "We get low error.",
            "Now we can express this here.",
            "You know in Chile."
        ],
        [
            "Creations this is what it looks like.",
            "It's relatively simple.",
            "This year is basically just the output of the SVM and this year is just a 01 loss.",
            "And as I said, the metric M appears in here.",
            "So basically, how do you optimize this while you just take a gradient step with respect to this?",
            "This function here and then every time you have done a gradient step you have to resolve this one space the Rapper method and people have mentioned this before.",
            "I think the first time I olyvia powders used this in his thesis in 2002.",
            "Now by the way, one thing you might notice is this function.",
            "Here that 01 loss actually is non differentiable and non continuous, so it's not so easy to do this gradient step.",
            "But one thing we can do is we just."
        ],
        [
            "Make it soft so we kind of put a sigmoid around it."
        ],
        [
            "Be substituted by a sigmoid, so instead of the 01 loss we kind of you know he's a smooth function that's very very similar to this number 01 less.",
            "And now we can take gradient steps down here.",
            "One thing of course at this point you know convexity is out of the window, right?",
            "There is no longer convex, but well.",
            "That's right."
        ],
        [
            "OK, so just two side notes during the optimization."
        ],
        [
            "One thing is to use the squared hinge loss.",
            "Why do you squared handles this very simple trick?",
            "The user squared loss."
        ],
        [
            "You can actually absorb the slack variables into the kernel.",
            "So instead of writing kmu just right, K M + 1 / C times the identity matrix.",
            "So a lot of you are familiar with this trick I'm sure, and the nice thing is, now we can just use the optimization problem without Slack variables.",
            "We have one more parameter C. And what do we do with this one while we just?"
        ],
        [
            "The gradient of the two right?",
            "So we just optimize that one.",
            "As well, and there's no reason not to do this.",
            "So basically an optimization problem.",
            "Yeah, now I've kind of around that SDP be wrapped.",
            "Another optimization problem very basically take the gradient respect to M and with C."
        ],
        [
            "Question how do we done the gradient?",
            "Not that important at the end, they just you know simple derivative, but they basically the only tricky part.",
            "If you take the predictor or you know the SVM predictor and take the difference effect of M that decomposes and there's one little trick that makes that possible.",
            "This one is maybe not so obvious.",
            "How do you take the results of the Alpha Victor M or be with respect to M and so there's this one little trick and that basically.",
            "Again, a lot of you probably know this, but if you don't, there's this one thing that it's really kind of.",
            "A neat rule is if you, you know, once you solve the SDP, then you know that the support vectors lie exactly on the margin.",
            "So you know that the inequality constraints are strict.",
            "So what does that mean?"
        ],
        [
            "That means that we can actually write the inequality constraints, equality constraints and basically can write them in some matrix times.",
            "Alpha B is 1 and 0, so this is just the margin one here and.",
            "So why is that useful?",
            "That's useful, because actually this matrix turned out is invertible.",
            "So one thing we can do is we can now."
        ],
        [
            "Tried Alpha and B is H inverse times this vector and now we have Alpha and B kind of enclosed form of this matrix H which contains this case.",
            "Slight modification of K which contains M. So actually we can just take that."
        ],
        [
            "But if Alpha MB perspective M so it's a very very simple trick to actually, you know, take the derivative.",
            "You know, despite the fact that you have these constraints.",
            "And that's something that Olivia already did this thesis in 2002 to learn kernel parameters.",
            "Alright, so."
        ],
        [
            "Now if we have this great information, we can just go ahead and do gradient descent.",
            "So here on the left you see as you know the losses going down as number of variations increases and you on the right you can see the training error.",
            "That's the red thing which is going up.",
            "That's fine.",
            "The validation error which is going down nicely.",
            "That's what we're mimicking with this loss function.",
            "So the validation errors going down the test error is following suit.",
            "Also, going down and training are you know the thing?",
            "That's a good time.",
            "You're not overfitting so much.",
            "This is on UCI credit card data set.",
            "Now we use this.",
            "Oh no one quick question.",
            "So one question, of course is if you look at this month concern, you might have is that?",
            "Well, every time you take a gradient."
        ],
        [
            "We actually have to solve an entire SVM, so that seems pretty slow.",
            "I hope it turns out that if you use instead of gradient descent, use conjugate gradient descent.",
            "In fact, use minimized M by call.",
            "It was Rasmussen, but actually if you just take a small holdout data set you can do early stopping after four or five grading iterations, so you don't really need very much.",
            "It's very very straightforward optimization and then actually makes it a lot faster than cross validation and all of the data sets to be tested on average about four times faster than cross validation."
        ],
        [
            "So here the results.",
            "Here again, these are, you know these are all the previous algorithms.",
            "But now we see the red line is the new algorithm and now this is very nicely always below that red line, which is the baseline SVM and the Euclidean metric.",
            "So on every data set we either improving or you know nothing is happening and we're definitely doing much better than all these nearest metric learning algorithms that are tuned for nearest neighbors.",
            "One thing.",
            "That it's also kind of interesting is.",
            "You could constrain the matrix M to be either diagonal."
        ],
        [
            "Vehicle that's particularly interesting if you have very high dimensional data, because this is quadratic in the dimension, so this is linear dimension and this is basically the same thing is tuning the hyper V kernel width, like waiting to send an turns out.",
            "Well, that kind of fluctuates around this, usually using the full matrix.",
            "That's the best and except this data set where I know this seems like an outlier.",
            "Oh no, because yeah, good question.",
            "So this is not the same as the baseline, because here actually we actually learning this year with cross validation.",
            "This is Ivy actually learning this is waiting to send our matrix.",
            "Actually, is this matrix here.",
            "Whereas here we actually using cross validation over just 25 parameters.",
            "Sorry.",
            "Clear.",
            "Finding Sigma because yeah, that's right.",
            "Yeah, that's right.",
            "Yeah, this is a set of doing grid search, right?",
            "You solve for that Sigma by gradient descent.",
            "Well you, I don't know there's something yeah.",
            "These are sorted by size.",
            "Actually, this is a pretty small data set, but the well you might also overfit, I don't know.",
            "We use 25 values, But then we started with me.",
            "By the way, we normally we rescaled all the features, so we did everything kind of you can do to get this baseline as low as possible.",
            "OK, so.",
            "If you look at this well, you know in some sense my my you know the conclusion that I draw from this is that actually metric learning could be kind of a nice alternative to trickier parameters of cross validation, so in many datasets is faster and actually tends to give you slightly better results.",
            "But one thing I just want to point out is these datasets are not so big.",
            "This is UCI Irvine datasets, so all of them have less than 10,000 data points.",
            "Or maybe this one is slightly bigger and these results here basically insignificant.",
            "So one thing I was interested as well.",
            "What if we apply this to really hard data set like not?",
            "You know UC Irvine, but you know something that's really challenging.",
            "So where do you get really challenging datasets from and?"
        ],
        [
            "Trick is you just go to the deep learning workshop.",
            "And so why are these datasets challenging?",
            "Because the deep learning community you know has come up with very, very different.",
            "Benchmark datasets and so part of the agenda, and I'm not sure you know.",
            "I hope I'm not insulting anyone, but part of the agenda is somewhat to design datasets such as VM's.",
            "Don't do that well, I think.",
            "And so these three datasets that they created, the first ones rectangle stated.",
            "So you have an image and then there is a rectangle and the task is to predict whether the vertical line, whether the rectangle is kind of longer vertically and horizontally.",
            "And this is the same task, but the rectangles actually image patches on top of other image patches.",
            "So for example here as you might be able to see this as a rectangle, that's look different than the background, and so you should be able to predict that here the vertical line is longer than this horizontal line, so that's you know, even reasonably challenging for humans.",
            "And the last data set is the convex data set, so here they created image patches and the task is to determine if the white points form a convex set.",
            "So in this case, yes, yes yes no no no.",
            "And so one thing that's very nice about these datasets number one.",
            "They hired a number 2.",
            "Actually they have a huge test set, so there are 50,000 test points.",
            "The previous one, the idea said they only had a couple 100.",
            "So evaluation is much nicer and the only thing is you know they purposely don't have that many training points.",
            "Alright."
        ],
        [
            "The only problem with this kind of data set is that you need good features, so if you would just think that in I doubt that you would do very well.",
            "You can see it afterwards.",
            "So how do the deep learning people create good?"
        ],
        [
            "And so, how do they even solve this in the 1st place?",
            "And so basically what they do is they take an image Patch and they first they do this unsupervised pretraining phase.",
            "So this year I'm following the approach by Yoshua Bengio, Pascal Vinson.",
            "So they take an image and the."
        ],
        [
            "Add noise to it so they randomly said 10% or 20 or 30% depends off the pixels off the features to 0.",
            "And then they train an autoencoder.",
            "To reconstruct the original image where these pixels are not set to 0.",
            "Now, why is that a good idea?",
            "Well, that's a good idea, because basically what you?"
        ],
        [
            "Learning here is that to reconstruct any pixel that's missing here from other pixels that are highly correlated with that pixel.",
            "So in some sense, doing is you're doing local convolution, so you're looking.",
            "It's kind of like a convolutional network, except that you're learning the structure.",
            "And we know the conversion units work pretty well, and datasets are designed for.",
            "But this even you know, you don't have to know the structure or you kind of learning it.",
            "And So what do they say?",
            "This is the first part of the learning process.",
            "They basically learn this Rep network to reconstruct the image.",
            "By the way, here we have 1003 hidden layers for thousand nodes each.",
            "That's what we use now.",
            "Our result.",
            "And by the way, we just used the Theano package from Joshua Bengio.",
            "And then after they are done with this, they use back propagation to the fine tuning.",
            "So then the."
        ],
        [
            "Stick in the image and then for the first time they actually use the label and the chop off half the network and add a new last layer here with only one one node and that's basically the prediction of.",
            "Making and now you just use back prop to train the whole thing."
        ],
        [
            "Alright, so that's what we don't want to do, right?",
            "I'm, you know, I don't want to go there, but."
        ],
        [
            "What seems like a good idea is to take these features that those guys are learning and stick them into an SVM.",
            "And how do we do this?",
            "Well, for every image I compute the value for the hidden."
        ],
        [
            "Layers and I concatenate them and."
        ],
        [
            "Stick them into a support vector machine.",
            "And.",
            "That probably doesn't do well because."
        ],
        [
            "The.",
            "You know that kind of different.",
            "They have different scale and compute different things.",
            "We don't even know how useful they are, so we use metric learning to basically re scale these hidden layers and combine them.",
            "And here you see diagonal matrix just for efficiency."
        ],
        [
            "Right, and that is our matrix M from the.",
            "From earlier"
        ],
        [
            "You can rewrite this.",
            "As basically a matrix where you basically have a different Sigma for every single hidden layer.",
            "Like there's a kernel based, have different kernel parameter for everything here hidden layer.",
            "So if you would cross validation he would have a lot of these sigmas and will take forever right at the time that cross validation takes is exponential."
        ],
        [
            "In terms of how many in the number of parameters?",
            "OK, so we tried this and here are results here.",
            "3 datasets.",
            "The convex image, rectangular and rectangular, and so this is all relative to the deep net performance.",
            "And one thing we did is we looked at the results that you also enjoy yogurt with.",
            "This network can be reported.",
            "He managed to reproduce them exactly.",
            "So the blue line here is what you get with SVM.",
            "If you just stick in the raw data and you doing RBF kernel, you're very fine.",
            "Very careful tweaking of the hyperparameters with cross validation, so that doesn't seem too high.",
            "The difference here, but actually these are very challenging is a very large test set.",
            "So that actually means a lot.",
            "And one thing you observe is that basically without you know with metric learning here in this case we actually match the deep networks at their own game.",
            "In this case we actually outperformed them by 4%.",
            "In this case, actually something crazy is happening.",
            "So you know we kind of having the error.",
            "So that was very unexpected that we're doing so well.",
            "Yeah, and for my letter was convinced there must be a bug is impossible.",
            "But actually then I realized it was a very good explanation why we're doing so well here.",
            "And that's because this data set actually is the smallest one.",
            "1200 training points is a lot of test points, but only 1000 training points and it's kind of a tricky data set, so this rectangle is only one pixel wide and so.",
            "That's that's kind of tricky footing machines, because if you have a support vector, any other test point if it doesn't have exactly the same rectangle is probably going to look probably going to have the same distance to every other other image, right?",
            "Because if the rectangles evenly one pixel off right, it doesn't help you at all.",
            "It looks very, very different.",
            "But if you actually have this, denoising that we're doing, and then actually you can have blurring these and actually now that you know, even if your rectangle is moved a little bit, you basically have local invariants of translation of rectangles moves a little stretched a little.",
            "You might still actually be very similar to one of the support vectors.",
            "So one question about criticism someone could have at this point is, well, you know you really need that metric.",
            "Learning right?",
            "Like you know you're doing that autoencoder generating this good features, but maybe that's all that's going on here, right?",
            "Maybe it's only the autoencoder that you're using and?"
        ],
        [
            "So that so, here are three more results.",
            "We basically just taking the first layer first and second layer first, second and third layer and without metric learning.",
            "And these are the results that we get in here.",
            "You see, for example, here we actually doing doing worse in this case is only 1% improvements that have five year from SVM.",
            "In this case you see this blurring effect actually does do something, but again, we're improving once you do metric learning.",
            "Improving over this, and again, this is a very very large test set, so this is actually.",
            "This absolutely is significant.",
            "Alright."
        ],
        [
            "So in conclusion.",
            "Anne.",
            "From basically I made several points in this talk, so the first one is if you want to metric learning for support vector machines.",
            "One thing you have to do is you have to incorporate the metric learning algorithm into these support vector optimization.",
            "You can't just use something out of the box and then stick it in the SVM.",
            "Two is doing that.",
            "We called IT support vector metric learning.",
            "Maybe a good good alternative to cross validation so you actually learning a metric and that's actually can be very fast and actually does an average slightly better than tweaking your parameters cross validation.",
            "So there's really there's not really a good reason not to do this in some sense.",
            "And if you have a challenging problem that features come from incompatible sources, than actually this can help quite significantly.",
            "And finally, last side Noyd side note, if you use denoising autoencoder, actually they make quite good features for support vector machines.",
            "Thanks, that's all."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with some team member and this is joint work with Eddie due is my student Olivia Chappelle who's at Yahoo Research and facial?",
                    "label": 0
                },
                {
                    "sent": "Who's at University of Southern California?",
                    "label": 0
                },
                {
                    "sent": "And quick warning.",
                    "label": 0
                },
                {
                    "sent": "This talk may contain amounts of newer network.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And traces of nonconvexity anyone leaving good heavens?",
                    "label": 0
                },
                {
                    "sent": "So yeah, alright.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm talking bout metric learning and metric learning is really kind of about learning of similarities, so dis similarities and so of course the question is.",
                    "label": 0
                },
                {
                    "sent": "What do we mean by similarity and well, let me.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give you an example.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a database of images.",
                    "label": 0
                },
                {
                    "sent": "And imagine if this image here on the left and you would like to find the most similar image in the database to that query and so well, do you know?",
                    "label": 1
                },
                {
                    "sent": "Depends you know what you measure.",
                    "label": 0
                },
                {
                    "sent": "Similarity is so you know once I want.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd say, well, you know, I'm really care about similarity in gender.",
                    "label": 0
                },
                {
                    "sent": "Well then this person here on the right might be a very good match.",
                    "label": 0
                },
                {
                    "sent": "Someone else might have the same image database, but makes you say well gender is not what I care about at all.",
                    "label": 0
                },
                {
                    "sent": "I care about age, so then the person here in the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is actually a much, much better match in the third person might come along and have yet another different.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of similarity.",
                    "label": 0
                },
                {
                    "sent": "So what I claim is, what do you mean by 7?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pretty really depends on the context, so you know what is our problem that we are trying to solve.",
                    "label": 1
                },
                {
                    "sent": "What is the data that we have, etc.",
                    "label": 0
                },
                {
                    "sent": "Very often people when they.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, compare data instances.",
                    "label": 0
                },
                {
                    "sent": "The default way to do this is the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "A measure of dissimilarity.",
                    "label": 0
                },
                {
                    "sent": "And very simple extension of the Euclidean distances.",
                    "label": 0
                },
                {
                    "sent": "The Mahalanobis distance.",
                    "label": 0
                },
                {
                    "sent": "My nervous system is basically the same thing as Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that you stick a matrix M here inside of this product, and this matrix M has to be positive semidefinite.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does that do to my numbers?",
                    "label": 0
                },
                {
                    "sent": "Mr. Metrics metric.",
                    "label": 0
                },
                {
                    "sent": "In some sense, this matrix M here scales some dimensions off in the Euclidean distance that say you have some point in me.",
                    "label": 0
                },
                {
                    "sent": "Make a set of points at all have exactly the same distance to the center.",
                    "label": 0
                },
                {
                    "sent": "That's a circle, whereas in the Mount of assistance the same circuit would actually form to an ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So in this case for example, you would stretch this dimension.",
                    "label": 0
                },
                {
                    "sent": "Here we would squeeze this dimension.",
                    "label": 0
                },
                {
                    "sent": "And so that's important.",
                    "label": 0
                },
                {
                    "sent": "In this talk we're talking about learning this may.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This matrix here.",
                    "label": 0
                },
                {
                    "sent": "And by the way, the positive definite constraints that always a little scary in this talk actually not to worry about it, so it turns out a matrix is positive definite if and only if you can decompose it into square of two real matrices.",
                    "label": 0
                },
                {
                    "sent": "And this will just focus about these real matrices, so I will not actually mentioned this constraint anymore.",
                    "label": 0
                },
                {
                    "sent": "You can just get around this no problem.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is important.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go to important for many machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "In particular, the prime example is K nearest neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "So K nearest neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "Basically you know 100% relies on a measure of similarity.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is very simple.",
                    "label": 0
                },
                {
                    "sent": "You have a training data set and imagine you have test data point you would like to know the label of the test data point.",
                    "label": 0
                },
                {
                    "sent": "What do you do?",
                    "label": 0
                },
                {
                    "sent": "You just look at the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "In this case it would be this fall.",
                    "label": 0
                },
                {
                    "sent": "Yeah just assign the label of the nearest neighbor and if you have K is larger than one then you take the K nearest neighbors and you take the majority vote amongst those and.",
                    "label": 0
                },
                {
                    "sent": "In this case, for example, you can see well if we had this point here, it would be classified as a soccer ball, but that's probably wrong, right?",
                    "label": 0
                },
                {
                    "sent": "Most of you would ask you what is the label of this guy?",
                    "label": 0
                },
                {
                    "sent": "You would probably say it's actually a basketball.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Because if you look at this data set, you realize that this dimension, the vertical dimension here, actually is much more informative towards class membership than the horizontal dimension.",
                    "label": 0
                },
                {
                    "sent": "So data points could be could have very small horizontal distance, but they still actually in different classes.",
                    "label": 0
                },
                {
                    "sent": "But the data points that have large vertical distance.",
                    "label": 1
                },
                {
                    "sent": "You know the neighbors with the vertical distance are always in the same class, so if you want to run.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Metric that as well on this this data said she would run over metric something like this where basically now you know all the points on the yellow circle here at the same distance from the query and now the nearest neighbor is this point here.",
                    "label": 0
                },
                {
                    "sent": "This point is much closer than this point, so we would classify the point correct.",
                    "label": 0
                },
                {
                    "sent": "That's been a lot of work on this in the last five.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I guess 8.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Years by now, people proposing all sorts of algorithms to learn these maneuvers.",
                    "label": 0
                },
                {
                    "sent": "Metrics for K nearest neighbors and for K means another algorithms that heavily rely on these metrics, and the all of these algorithms have different advantages and disadvantages.",
                    "label": 0
                },
                {
                    "sent": "Not going to go over them.",
                    "label": 0
                },
                {
                    "sent": "And but one thing that a lot of people ask me is, well, you know, once you learn that kind of metric, right?",
                    "label": 0
                },
                {
                    "sent": "So now we can use seniors neighbors but could also use a different algorithms.",
                    "label": 0
                },
                {
                    "sent": "So for my data set, maybe 10 years neighbors is not the best algorithm.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "I want to support Vector Machine could also use a support vector machine with the metric that you learned here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that's kind of what I investigated here for actually, especially for this workshop.",
                    "label": 0
                },
                {
                    "sent": "And I called the support.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back to metric learning.",
                    "label": 0
                },
                {
                    "sent": "So why is that a good idea?",
                    "label": 0
                },
                {
                    "sent": "While if you look at support vector machines and in particular let's look at the RBF kernel, the RBF kernel is defined as this equation here.",
                    "label": 1
                },
                {
                    "sent": "So basically all it is basically the Euclidean.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distance if you expansion, exponentiate the Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "So support vector machines only access data points in terms of the kernels or indirectly in terms of the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "So it's a very natural question to ask.",
                    "label": 0
                },
                {
                    "sent": "Well, why don't we just take this distance here?",
                    "label": 0
                },
                {
                    "sent": "And instead of using Euclidean Dist.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And stick in the matrix M and that's the monovisc distance that we learned with one of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "And you know, it's definitely worth trying that out, so I did.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a usage couple of datasets just to explain the baseline.",
                    "label": 0
                },
                {
                    "sent": "Everything is reported relative to support Vector machine under the Euclidean distance where I chose the kernel width and the private parameters with cross validation.",
                    "label": 1
                },
                {
                    "sent": "So here a bunch of UCI datasets and we just chose me to try to choose all the all the binary datasets and.",
                    "label": 1
                },
                {
                    "sent": "These are different algorithms, different metric learning algorithms, information theoretic, metric learning.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I buy Brian, who is also here in the audience this NCA.",
                    "label": 0
                },
                {
                    "sent": "The samurais also did an element of the algorithms that I worked on and so this is the error relative to the Euclidean metric so lower is better in this case and so one thing you realize is, well, you know it doesn't really work right.",
                    "label": 0
                },
                {
                    "sent": "Like you know, if you see these bars, most of them are actually higher than Euclidean metric.",
                    "label": 0
                },
                {
                    "sent": "Here we have a little bit of improvement but nothing to write home about and actually the worst part about this graph is that my algorithm is the worst, so you know something has to be done here.",
                    "label": 0
                },
                {
                    "sent": "I'm in.",
                    "label": 0
                },
                {
                    "sent": "That's right, so I'm first learning at first learn using this metric learning algorithms, learn a metric and then I use SVM using that metric.",
                    "label": 0
                },
                {
                    "sent": "That's the only difference.",
                    "label": 0
                },
                {
                    "sent": "I just started playing this case.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so sorry we did that.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah OK. Alright so Francis question was what we just plug in that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Metric here, here that you know you still need to learn that hyperparameter here, right?",
                    "label": 0
                },
                {
                    "sent": "That was kind of parameter.",
                    "label": 0
                },
                {
                    "sent": "We did that by cross validation.",
                    "label": 0
                },
                {
                    "sent": "But it still didn't really.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To work on so the so why is that?",
                    "label": 0
                },
                {
                    "sent": "And my explanation for this as well these algorithms, all of these in one way or another, mimic the leave one out loss of K nearest neighbors, so they are a little bit too specialized to K nearest neighbors, and so the RBF kernel is similar to nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "You basically put a Gaussian around you test point and then you measure similarity in that waited for him.",
                    "label": 0
                },
                {
                    "sent": "But one thing is that in super vector machine you have support vector, so most of the data points actually have zero wait, so you don't really care about them and so that's really something you should take into consideration when you're learning metric.",
                    "label": 0
                },
                {
                    "sent": "So the question of course, is well, can we learn a specific metric that you know works better with support vector machines?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remind you this year.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine.",
                    "label": 0
                },
                {
                    "sent": "I think most of you familiar with this.",
                    "label": 0
                },
                {
                    "sent": "It's basically if you're not familiar with this doesn't matter, but there's the Kernel workshop, but the basically just Wan optimization problems because automation problem.",
                    "label": 0
                },
                {
                    "sent": "So the idea is, well, how can we learn a metric for that was better for in this optimization problem and the metric appears inside this kernel function here and the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "We just do a wrapper around it so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of stick this optimization problem inside another optimization problem where we learn a metric unit matrix M and what is the objective of this optimization problem?",
                    "label": 0
                },
                {
                    "sent": "Here we just decided well, we just try to do Bell on the validation data set.",
                    "label": 0
                },
                {
                    "sent": "So we split our data set and training and validation.",
                    "label": 0
                },
                {
                    "sent": "This year is trained on the training data set and then we say overall well.",
                    "label": 0
                },
                {
                    "sent": "You know it's kind of like cross validation.",
                    "label": 0
                },
                {
                    "sent": "We want to have a metrics on the validation data set.",
                    "label": 0
                },
                {
                    "sent": "We get low error.",
                    "label": 0
                },
                {
                    "sent": "Now we can express this here.",
                    "label": 0
                },
                {
                    "sent": "You know in Chile.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creations this is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "It's relatively simple.",
                    "label": 0
                },
                {
                    "sent": "This year is basically just the output of the SVM and this year is just a 01 loss.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the metric M appears in here.",
                    "label": 0
                },
                {
                    "sent": "So basically, how do you optimize this while you just take a gradient step with respect to this?",
                    "label": 0
                },
                {
                    "sent": "This function here and then every time you have done a gradient step you have to resolve this one space the Rapper method and people have mentioned this before.",
                    "label": 0
                },
                {
                    "sent": "I think the first time I olyvia powders used this in his thesis in 2002.",
                    "label": 0
                },
                {
                    "sent": "Now by the way, one thing you might notice is this function.",
                    "label": 0
                },
                {
                    "sent": "Here that 01 loss actually is non differentiable and non continuous, so it's not so easy to do this gradient step.",
                    "label": 0
                },
                {
                    "sent": "But one thing we can do is we just.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make it soft so we kind of put a sigmoid around it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be substituted by a sigmoid, so instead of the 01 loss we kind of you know he's a smooth function that's very very similar to this number 01 less.",
                    "label": 0
                },
                {
                    "sent": "And now we can take gradient steps down here.",
                    "label": 0
                },
                {
                    "sent": "One thing of course at this point you know convexity is out of the window, right?",
                    "label": 0
                },
                {
                    "sent": "There is no longer convex, but well.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just two side notes during the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing is to use the squared hinge loss.",
                    "label": 1
                },
                {
                    "sent": "Why do you squared handles this very simple trick?",
                    "label": 0
                },
                {
                    "sent": "The user squared loss.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can actually absorb the slack variables into the kernel.",
                    "label": 0
                },
                {
                    "sent": "So instead of writing kmu just right, K M + 1 / C times the identity matrix.",
                    "label": 1
                },
                {
                    "sent": "So a lot of you are familiar with this trick I'm sure, and the nice thing is, now we can just use the optimization problem without Slack variables.",
                    "label": 0
                },
                {
                    "sent": "We have one more parameter C. And what do we do with this one while we just?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradient of the two right?",
                    "label": 0
                },
                {
                    "sent": "So we just optimize that one.",
                    "label": 0
                },
                {
                    "sent": "As well, and there's no reason not to do this.",
                    "label": 0
                },
                {
                    "sent": "So basically an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, now I've kind of around that SDP be wrapped.",
                    "label": 0
                },
                {
                    "sent": "Another optimization problem very basically take the gradient respect to M and with C.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question how do we done the gradient?",
                    "label": 0
                },
                {
                    "sent": "Not that important at the end, they just you know simple derivative, but they basically the only tricky part.",
                    "label": 0
                },
                {
                    "sent": "If you take the predictor or you know the SVM predictor and take the difference effect of M that decomposes and there's one little trick that makes that possible.",
                    "label": 0
                },
                {
                    "sent": "This one is maybe not so obvious.",
                    "label": 0
                },
                {
                    "sent": "How do you take the results of the Alpha Victor M or be with respect to M and so there's this one little trick and that basically.",
                    "label": 0
                },
                {
                    "sent": "Again, a lot of you probably know this, but if you don't, there's this one thing that it's really kind of.",
                    "label": 0
                },
                {
                    "sent": "A neat rule is if you, you know, once you solve the SDP, then you know that the support vectors lie exactly on the margin.",
                    "label": 0
                },
                {
                    "sent": "So you know that the inequality constraints are strict.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That means that we can actually write the inequality constraints, equality constraints and basically can write them in some matrix times.",
                    "label": 0
                },
                {
                    "sent": "Alpha B is 1 and 0, so this is just the margin one here and.",
                    "label": 0
                },
                {
                    "sent": "So why is that useful?",
                    "label": 0
                },
                {
                    "sent": "That's useful, because actually this matrix turned out is invertible.",
                    "label": 0
                },
                {
                    "sent": "So one thing we can do is we can now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tried Alpha and B is H inverse times this vector and now we have Alpha and B kind of enclosed form of this matrix H which contains this case.",
                    "label": 0
                },
                {
                    "sent": "Slight modification of K which contains M. So actually we can just take that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if Alpha MB perspective M so it's a very very simple trick to actually, you know, take the derivative.",
                    "label": 0
                },
                {
                    "sent": "You know, despite the fact that you have these constraints.",
                    "label": 0
                },
                {
                    "sent": "And that's something that Olivia already did this thesis in 2002 to learn kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if we have this great information, we can just go ahead and do gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So here on the left you see as you know the losses going down as number of variations increases and you on the right you can see the training error.",
                    "label": 0
                },
                {
                    "sent": "That's the red thing which is going up.",
                    "label": 0
                },
                {
                    "sent": "That's fine.",
                    "label": 0
                },
                {
                    "sent": "The validation error which is going down nicely.",
                    "label": 1
                },
                {
                    "sent": "That's what we're mimicking with this loss function.",
                    "label": 1
                },
                {
                    "sent": "So the validation errors going down the test error is following suit.",
                    "label": 0
                },
                {
                    "sent": "Also, going down and training are you know the thing?",
                    "label": 0
                },
                {
                    "sent": "That's a good time.",
                    "label": 0
                },
                {
                    "sent": "You're not overfitting so much.",
                    "label": 0
                },
                {
                    "sent": "This is on UCI credit card data set.",
                    "label": 1
                },
                {
                    "sent": "Now we use this.",
                    "label": 0
                },
                {
                    "sent": "Oh no one quick question.",
                    "label": 0
                },
                {
                    "sent": "So one question, of course is if you look at this month concern, you might have is that?",
                    "label": 0
                },
                {
                    "sent": "Well, every time you take a gradient.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We actually have to solve an entire SVM, so that seems pretty slow.",
                    "label": 0
                },
                {
                    "sent": "I hope it turns out that if you use instead of gradient descent, use conjugate gradient descent.",
                    "label": 1
                },
                {
                    "sent": "In fact, use minimized M by call.",
                    "label": 0
                },
                {
                    "sent": "It was Rasmussen, but actually if you just take a small holdout data set you can do early stopping after four or five grading iterations, so you don't really need very much.",
                    "label": 0
                },
                {
                    "sent": "It's very very straightforward optimization and then actually makes it a lot faster than cross validation and all of the data sets to be tested on average about four times faster than cross validation.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here the results.",
                    "label": 0
                },
                {
                    "sent": "Here again, these are, you know these are all the previous algorithms.",
                    "label": 0
                },
                {
                    "sent": "But now we see the red line is the new algorithm and now this is very nicely always below that red line, which is the baseline SVM and the Euclidean metric.",
                    "label": 0
                },
                {
                    "sent": "So on every data set we either improving or you know nothing is happening and we're definitely doing much better than all these nearest metric learning algorithms that are tuned for nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "One thing.",
                    "label": 0
                },
                {
                    "sent": "That it's also kind of interesting is.",
                    "label": 0
                },
                {
                    "sent": "You could constrain the matrix M to be either diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vehicle that's particularly interesting if you have very high dimensional data, because this is quadratic in the dimension, so this is linear dimension and this is basically the same thing is tuning the hyper V kernel width, like waiting to send an turns out.",
                    "label": 0
                },
                {
                    "sent": "Well, that kind of fluctuates around this, usually using the full matrix.",
                    "label": 0
                },
                {
                    "sent": "That's the best and except this data set where I know this seems like an outlier.",
                    "label": 0
                },
                {
                    "sent": "Oh no, because yeah, good question.",
                    "label": 0
                },
                {
                    "sent": "So this is not the same as the baseline, because here actually we actually learning this year with cross validation.",
                    "label": 0
                },
                {
                    "sent": "This is Ivy actually learning this is waiting to send our matrix.",
                    "label": 0
                },
                {
                    "sent": "Actually, is this matrix here.",
                    "label": 0
                },
                {
                    "sent": "Whereas here we actually using cross validation over just 25 parameters.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "Finding Sigma because yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a set of doing grid search, right?",
                    "label": 0
                },
                {
                    "sent": "You solve for that Sigma by gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Well you, I don't know there's something yeah.",
                    "label": 0
                },
                {
                    "sent": "These are sorted by size.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is a pretty small data set, but the well you might also overfit, I don't know.",
                    "label": 0
                },
                {
                    "sent": "We use 25 values, But then we started with me.",
                    "label": 0
                },
                {
                    "sent": "By the way, we normally we rescaled all the features, so we did everything kind of you can do to get this baseline as low as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you look at this well, you know in some sense my my you know the conclusion that I draw from this is that actually metric learning could be kind of a nice alternative to trickier parameters of cross validation, so in many datasets is faster and actually tends to give you slightly better results.",
                    "label": 0
                },
                {
                    "sent": "But one thing I just want to point out is these datasets are not so big.",
                    "label": 0
                },
                {
                    "sent": "This is UCI Irvine datasets, so all of them have less than 10,000 data points.",
                    "label": 0
                },
                {
                    "sent": "Or maybe this one is slightly bigger and these results here basically insignificant.",
                    "label": 0
                },
                {
                    "sent": "So one thing I was interested as well.",
                    "label": 0
                },
                {
                    "sent": "What if we apply this to really hard data set like not?",
                    "label": 0
                },
                {
                    "sent": "You know UC Irvine, but you know something that's really challenging.",
                    "label": 0
                },
                {
                    "sent": "So where do you get really challenging datasets from and?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trick is you just go to the deep learning workshop.",
                    "label": 0
                },
                {
                    "sent": "And so why are these datasets challenging?",
                    "label": 0
                },
                {
                    "sent": "Because the deep learning community you know has come up with very, very different.",
                    "label": 0
                },
                {
                    "sent": "Benchmark datasets and so part of the agenda, and I'm not sure you know.",
                    "label": 0
                },
                {
                    "sent": "I hope I'm not insulting anyone, but part of the agenda is somewhat to design datasets such as VM's.",
                    "label": 0
                },
                {
                    "sent": "Don't do that well, I think.",
                    "label": 0
                },
                {
                    "sent": "And so these three datasets that they created, the first ones rectangle stated.",
                    "label": 0
                },
                {
                    "sent": "So you have an image and then there is a rectangle and the task is to predict whether the vertical line, whether the rectangle is kind of longer vertically and horizontally.",
                    "label": 1
                },
                {
                    "sent": "And this is the same task, but the rectangles actually image patches on top of other image patches.",
                    "label": 0
                },
                {
                    "sent": "So for example here as you might be able to see this as a rectangle, that's look different than the background, and so you should be able to predict that here the vertical line is longer than this horizontal line, so that's you know, even reasonably challenging for humans.",
                    "label": 1
                },
                {
                    "sent": "And the last data set is the convex data set, so here they created image patches and the task is to determine if the white points form a convex set.",
                    "label": 0
                },
                {
                    "sent": "So in this case, yes, yes yes no no no.",
                    "label": 0
                },
                {
                    "sent": "And so one thing that's very nice about these datasets number one.",
                    "label": 0
                },
                {
                    "sent": "They hired a number 2.",
                    "label": 0
                },
                {
                    "sent": "Actually they have a huge test set, so there are 50,000 test points.",
                    "label": 0
                },
                {
                    "sent": "The previous one, the idea said they only had a couple 100.",
                    "label": 0
                },
                {
                    "sent": "So evaluation is much nicer and the only thing is you know they purposely don't have that many training points.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The only problem with this kind of data set is that you need good features, so if you would just think that in I doubt that you would do very well.",
                    "label": 0
                },
                {
                    "sent": "You can see it afterwards.",
                    "label": 0
                },
                {
                    "sent": "So how do the deep learning people create good?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, how do they even solve this in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "And so basically what they do is they take an image Patch and they first they do this unsupervised pretraining phase.",
                    "label": 0
                },
                {
                    "sent": "So this year I'm following the approach by Yoshua Bengio, Pascal Vinson.",
                    "label": 0
                },
                {
                    "sent": "So they take an image and the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add noise to it so they randomly said 10% or 20 or 30% depends off the pixels off the features to 0.",
                    "label": 0
                },
                {
                    "sent": "And then they train an autoencoder.",
                    "label": 0
                },
                {
                    "sent": "To reconstruct the original image where these pixels are not set to 0.",
                    "label": 0
                },
                {
                    "sent": "Now, why is that a good idea?",
                    "label": 0
                },
                {
                    "sent": "Well, that's a good idea, because basically what you?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning here is that to reconstruct any pixel that's missing here from other pixels that are highly correlated with that pixel.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, doing is you're doing local convolution, so you're looking.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like a convolutional network, except that you're learning the structure.",
                    "label": 0
                },
                {
                    "sent": "And we know the conversion units work pretty well, and datasets are designed for.",
                    "label": 0
                },
                {
                    "sent": "But this even you know, you don't have to know the structure or you kind of learning it.",
                    "label": 0
                },
                {
                    "sent": "And So what do they say?",
                    "label": 0
                },
                {
                    "sent": "This is the first part of the learning process.",
                    "label": 0
                },
                {
                    "sent": "They basically learn this Rep network to reconstruct the image.",
                    "label": 0
                },
                {
                    "sent": "By the way, here we have 1003 hidden layers for thousand nodes each.",
                    "label": 0
                },
                {
                    "sent": "That's what we use now.",
                    "label": 0
                },
                {
                    "sent": "Our result.",
                    "label": 0
                },
                {
                    "sent": "And by the way, we just used the Theano package from Joshua Bengio.",
                    "label": 0
                },
                {
                    "sent": "And then after they are done with this, they use back propagation to the fine tuning.",
                    "label": 0
                },
                {
                    "sent": "So then the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick in the image and then for the first time they actually use the label and the chop off half the network and add a new last layer here with only one one node and that's basically the prediction of.",
                    "label": 0
                },
                {
                    "sent": "Making and now you just use back prop to train the whole thing.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's what we don't want to do, right?",
                    "label": 0
                },
                {
                    "sent": "I'm, you know, I don't want to go there, but.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What seems like a good idea is to take these features that those guys are learning and stick them into an SVM.",
                    "label": 0
                },
                {
                    "sent": "And how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, for every image I compute the value for the hidden.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Layers and I concatenate them and.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick them into a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That probably doesn't do well because.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "You know that kind of different.",
                    "label": 0
                },
                {
                    "sent": "They have different scale and compute different things.",
                    "label": 0
                },
                {
                    "sent": "We don't even know how useful they are, so we use metric learning to basically re scale these hidden layers and combine them.",
                    "label": 1
                },
                {
                    "sent": "And here you see diagonal matrix just for efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and that is our matrix M from the.",
                    "label": 0
                },
                {
                    "sent": "From earlier",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can rewrite this.",
                    "label": 0
                },
                {
                    "sent": "As basically a matrix where you basically have a different Sigma for every single hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Like there's a kernel based, have different kernel parameter for everything here hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So if you would cross validation he would have a lot of these sigmas and will take forever right at the time that cross validation takes is exponential.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of how many in the number of parameters?",
                    "label": 0
                },
                {
                    "sent": "OK, so we tried this and here are results here.",
                    "label": 0
                },
                {
                    "sent": "3 datasets.",
                    "label": 0
                },
                {
                    "sent": "The convex image, rectangular and rectangular, and so this is all relative to the deep net performance.",
                    "label": 0
                },
                {
                    "sent": "And one thing we did is we looked at the results that you also enjoy yogurt with.",
                    "label": 0
                },
                {
                    "sent": "This network can be reported.",
                    "label": 0
                },
                {
                    "sent": "He managed to reproduce them exactly.",
                    "label": 0
                },
                {
                    "sent": "So the blue line here is what you get with SVM.",
                    "label": 0
                },
                {
                    "sent": "If you just stick in the raw data and you doing RBF kernel, you're very fine.",
                    "label": 0
                },
                {
                    "sent": "Very careful tweaking of the hyperparameters with cross validation, so that doesn't seem too high.",
                    "label": 0
                },
                {
                    "sent": "The difference here, but actually these are very challenging is a very large test set.",
                    "label": 0
                },
                {
                    "sent": "So that actually means a lot.",
                    "label": 0
                },
                {
                    "sent": "And one thing you observe is that basically without you know with metric learning here in this case we actually match the deep networks at their own game.",
                    "label": 0
                },
                {
                    "sent": "In this case we actually outperformed them by 4%.",
                    "label": 0
                },
                {
                    "sent": "In this case, actually something crazy is happening.",
                    "label": 0
                },
                {
                    "sent": "So you know we kind of having the error.",
                    "label": 0
                },
                {
                    "sent": "So that was very unexpected that we're doing so well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and for my letter was convinced there must be a bug is impossible.",
                    "label": 0
                },
                {
                    "sent": "But actually then I realized it was a very good explanation why we're doing so well here.",
                    "label": 0
                },
                {
                    "sent": "And that's because this data set actually is the smallest one.",
                    "label": 0
                },
                {
                    "sent": "1200 training points is a lot of test points, but only 1000 training points and it's kind of a tricky data set, so this rectangle is only one pixel wide and so.",
                    "label": 0
                },
                {
                    "sent": "That's that's kind of tricky footing machines, because if you have a support vector, any other test point if it doesn't have exactly the same rectangle is probably going to look probably going to have the same distance to every other other image, right?",
                    "label": 0
                },
                {
                    "sent": "Because if the rectangles evenly one pixel off right, it doesn't help you at all.",
                    "label": 0
                },
                {
                    "sent": "It looks very, very different.",
                    "label": 0
                },
                {
                    "sent": "But if you actually have this, denoising that we're doing, and then actually you can have blurring these and actually now that you know, even if your rectangle is moved a little bit, you basically have local invariants of translation of rectangles moves a little stretched a little.",
                    "label": 0
                },
                {
                    "sent": "You might still actually be very similar to one of the support vectors.",
                    "label": 0
                },
                {
                    "sent": "So one question about criticism someone could have at this point is, well, you know you really need that metric.",
                    "label": 0
                },
                {
                    "sent": "Learning right?",
                    "label": 0
                },
                {
                    "sent": "Like you know you're doing that autoencoder generating this good features, but maybe that's all that's going on here, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe it's only the autoencoder that you're using and?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that so, here are three more results.",
                    "label": 0
                },
                {
                    "sent": "We basically just taking the first layer first and second layer first, second and third layer and without metric learning.",
                    "label": 0
                },
                {
                    "sent": "And these are the results that we get in here.",
                    "label": 0
                },
                {
                    "sent": "You see, for example, here we actually doing doing worse in this case is only 1% improvements that have five year from SVM.",
                    "label": 0
                },
                {
                    "sent": "In this case you see this blurring effect actually does do something, but again, we're improving once you do metric learning.",
                    "label": 0
                },
                {
                    "sent": "Improving over this, and again, this is a very very large test set, so this is actually.",
                    "label": 0
                },
                {
                    "sent": "This absolutely is significant.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "From basically I made several points in this talk, so the first one is if you want to metric learning for support vector machines.",
                    "label": 0
                },
                {
                    "sent": "One thing you have to do is you have to incorporate the metric learning algorithm into these support vector optimization.",
                    "label": 0
                },
                {
                    "sent": "You can't just use something out of the box and then stick it in the SVM.",
                    "label": 0
                },
                {
                    "sent": "Two is doing that.",
                    "label": 0
                },
                {
                    "sent": "We called IT support vector metric learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe a good good alternative to cross validation so you actually learning a metric and that's actually can be very fast and actually does an average slightly better than tweaking your parameters cross validation.",
                    "label": 1
                },
                {
                    "sent": "So there's really there's not really a good reason not to do this in some sense.",
                    "label": 0
                },
                {
                    "sent": "And if you have a challenging problem that features come from incompatible sources, than actually this can help quite significantly.",
                    "label": 1
                },
                {
                    "sent": "And finally, last side Noyd side note, if you use denoising autoencoder, actually they make quite good features for support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Thanks, that's all.",
                    "label": 0
                }
            ]
        }
    }
}