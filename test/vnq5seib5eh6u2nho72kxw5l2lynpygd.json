{
    "id": "vnq5seib5eh6u2nho72kxw5l2lynpygd",
    "title": "Manifold Tangent Classifier",
    "info": {
        "author": [
            "Yann Dauphin, Universit\u00e9 de Montr\u00e9al"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Manifold Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_dauphin_manifold/",
    "segmentation": [
        [
            "Alright, so hi everyone, my name is Jan Dofe and today I'm going to talk about how to build a classifier from a manifold perspective.",
            "This is joint work with seller.",
            "If I pass canvas on your show banjo and zavia Miller."
        ],
        [
            "So.",
            "To start off, let me state the key idea what we're going to do is that we're going to extract the tendence of the manifold using deep learning, and we're going to make a classifier invariant to these directions, and the reason that makes any sense is because these tangents of the manifold in the case of vision, for example, correspond to small translations and rotations, for example, so we know we want our classifier to be invariant to such things.",
            "We extract these tangents in an unsupervised manner and to do this we leverage and we combine three powerful but generic prior hypothesis."
        ],
        [
            "The first one is the semisupervised hypothesis.",
            "I'm sure all of you know it.",
            "It's that the probability distribution of the input shares some structure with the conditional probability of the class labels given the input."
        ],
        [
            "So by that I mean that an SVN."
        ],
        [
            "Give you such a decision boundary, but."
        ],
        [
            "If you use the unlabeled data here in black."
        ],
        [
            "Then you're able to come up with a much better decision boundary, and."
        ],
        [
            "This is the idea that is used in intense 2006 algorithm to train deep networks and this result is further extended by your show banjo."
        ],
        [
            "And Marco Rubio ranzato to autoencoders.",
            "So there's a lot to say about deep learning.",
            "But in a few words, the basic idea is that you want to learn a year article representation of your input."
        ],
        [
            "That's it."
        ],
        [
            "So that's the first hypothesis.",
            "The second is the manifold hypothesis.",
            "By that I mean that your examples concentrate near or lower dimensional manifold.",
            "So here we have the manifold of faces in the space of images, so most images are pretty random."
        ],
        [
            "3rd Hypothesis is the classification manifold aybo."
        ],
        [
            "This is an here I found a pretty cool visualization using T sne, which is dimensionality reduction technique, and this is on Caltech 101.",
            "And the idea here is that if examples from different classes belong to different manifolds.",
            "So for example the manifold if you will or cluster faces at the bottom is separated from all the other classes by a low density region, and the idea is that if you are at a point in the manifold of faces, then any direction that keeps you within that manifold then it doesn't change the class label.",
            "So you should be invariant to it.",
            "So.",
            "What we're going to try to do is to extract these tangents in an unsupervised fashion and train a classifier to be invariant to them."
        ],
        [
            "So here's how we do it."
        ],
        [
            "So our algorithm is within the framework of autoencoders.",
            "Because of time constraints, I'm only going to remind you the notation.",
            "So you have an input X which you map to the hidden code H bold using the encoder function H you take the hidden code H bold and you map it back to Reconstruction Z using the decoder function G and the global objective is that you're trying to minimize the difference between a given input and its reconstruction.",
            "For all the data set."
        ],
        [
            "So this is all good and well when you're simply trying to compress the input.",
            "But in the case where you want to learn rich information rich representations about your input, we turned to the contractive autoencoder, which was introduced early this year."
        ],
        [
            "And the idea is that instead of simply reconstructing your input, you are adding a second term."
        ],
        [
            "Which we call the contraction.",
            "So the first time is the reconstruction, which is the usual cost of the auto encoder and the second term, which is the new term is called the contraction an it's simply the Frobenius norm of the derivative of your encoder with respect to X.",
            "So basically what you're trying to measure is how much you're eating code varies when you vary around the input just a little bit.",
            "So basically that term is trying to ignore all the information in the input, whereas the reconstruction is guaranteeing.",
            "That you keep all of the information about the input an the interaction between these two terms is sort of like a tug of war."
        ],
        [
            "And let me illustrate that.",
            "So this is a 2 dimensional space.",
            "The slide and you have embedded within it a 1 dimensional manner."
        ],
        [
            "So if we concentrate on this point."
        ],
        [
            "Here in green."
        ],
        [
            "You can move basically in two directions.",
            "What the contraction will do is try to be invariant to when you move that point in any direction, so any move.",
            "In any combination of those directions would give you the same hidden code.",
            "So let's say it tries to contract."
        ],
        [
            "The first dimension it will succeed, so it will no longer be invariant when you are moving in that direction.",
            "It will also try to be invariant to changes in."
        ],
        [
            "The other direction, but it's not possible because that direction is necessary to reconstruct your input properly.",
            "So what happens?",
            "This is just a simple explanation, a simple visualization, but what happens is that the encoder function of your contracted with encoder will only be sensible to the directions that are within the manifold that are tangent to the manifold.",
            "And by using this."
        ],
        [
            "Then we're able to define an Atlas of charts, and to do this we simply need to recover the directions to which the encoder is most sensitive to.",
            "And you can do this by taking the Jacobian, which is the derivative of your H with respect to X.",
            "Doing an eigendecomposition an your attention directions will be found in the left eigenvectors, so EU vector with eigenvalues above a certain threshold.",
            "So now we're able to make a prediction, right?",
            "We're able to say that if in fact we're extracting a lower dimensional manifold, then there should be only few eigenvalues that are non negligible for a given problem."
        ],
        [
            "So we evaluate both the autoencoder and the contractive autoencoder on Cifar 10, which is an object recognition data set, and what we observe is that the auto encoder is sensitive to pretty much every direction in the input.",
            "However the contracted with encoder is only sensible to a few of them, meaning that it has in fact extracted a lower dimensional manifold.",
            "So this is just visualizing the distribution of eigenvalues on cifar, but I think we can do better right?",
            "We can also look."
        ],
        [
            "Look at the tangents.",
            "So this is for M Nest which is N written character recognition digit recognition data set.",
            "So on the left you have an input point and on the right you have the tender directions which are extracted by the contracted with 1/4 and this is interesting because now we're able to move around in the neighborhood of that of that zero and generate completely new examples using basic math.",
            "So you simply take a linear combination.",
            "Of the attendance with the input.",
            "So this is a."
        ],
        [
            "Slightly translated to the right zero.",
            "And I we can already think of some ways to improve classification using this method, one of which would be to simply augment your training set with new examples using this technique, but I think."
        ],
        [
            "We can do better.",
            "So for instance we can use tension distance which was introduced by Patrice Imma in 1993, where he uses tensions that use apriori knowledge.",
            "I think he uses etherized amusing analytic knowledge an you do use that to define a better distance measure.",
            "I don't have enough time to go into it, so I'm going to skip the explanation.",
            "The more interesting version I think is tender and propagation again by battery SEMA.",
            "Which is a framework to make any gradient based classifier invariant to tension directions and all you need to do is add a penalty to your objective and it's."
        ],
        [
            "Shown here on the right.",
            "So if your classifier performs the function F, then all you need to do is take the derivative of F with respect to X and project it in the tangent directions for that point.",
            "So in an illustration, what that means is."
        ],
        [
            "But if you know that the tangent direction of the manifold of blue points is slightly slanted, then you would figure that your decision boundary should also be slanted."
        ],
        [
            "Right, and this is done by this penalty because it tries to make the normal of the decision surface perpendicular to the tension space."
        ],
        [
            "And this is what we use to build the manifold tangent classifier."
        ],
        [
            "So you only need three steps at the first step you train a stack of CLL."
        ],
        [
            "So tough and finally at second step you compute the tangents of each example by doing the SVD of the derivative of H1 with respect to X.",
            "So we're going to have more layers.",
            "I just put two layers because it fits on the page and the last step is to try."
        ],
        [
            "In a logistic regression, so you stuck a logistic regression on top of the representation and you train that logistic regression is by adding also the tangent propagation penalty an.",
            "Also we also fine tune all the weights.",
            "So we do global fine tuning.",
            "Alright, so that's the manifold tangent classifier.",
            "Let's look at some."
        ],
        [
            "Experiments."
        ],
        [
            "So, as I've shown you before, we have some pretty nice tangents on ennist, but we can also look to a more complete."
        ],
        [
            "This problem, so here this is.",
            "This is a secret and this is not our method, I assure you.",
            "So here you have the input point, which is a dog with a shirt for some reason.",
            "Anne.",
            "Here you have the tensions using local PCA, which is just PCA on a neighborhood of the examples, so we're trying to see how how well can you approximate attendance using local methods.",
            "You could argue with."
        ],
        [
            "OK, but this is what we get using the contractive autoencoder.",
            "So the difference is pretty striking when you're going to serve is that, for example, some of the tenants correspond to small translations of the legs.",
            "For example, the next to last tenant for example."
        ],
        [
            "We've also evaluated it on text, so this is our CV one where you have documents as bag of words.",
            "I'm not going to get into it, but you can check the paper if you want to know more."
        ],
        [
            "About that, so let's talk a little bit more about classification.",
            "So first, let's see if we can improve the performance of a simple K nearest neighbor classifier using the tension distance proposed by Patrice.",
            "Imma using the tensions that we extract using the contractive auto encoder.",
            "So unless you have the regular cameras neighbor with Euclidean distance on the right, you have key nearest neighbor using our attention distance and what you can see is that on the data set that we've considered, we get an improvement using these tenants.",
            "But knowledge, let's talk about the manifold tension classifier.",
            "So this is an amnesty."
        ],
        [
            "With 106 hundred, 1000 and 3000 labels and 50,000 unlabeled examples, so this is a semi supervised setting.",
            "And here we're comparing with from left to right, traditional neural networks, SVM's convolutional neural networks, transductive SVM's deep belief networks, regularize neighborhood component analysis, embed neural network and.",
            "The contract if autoencoder and finally the manifold tangent classifier, so it's a bunch of numbers.",
            "Of course some of the results taken from other papers, but I think the more interesting results are that the manifold tension classifier in all the cases that we've considered reduces the error by half when you compare it with the traditional neural network.",
            "So for example, with 100 labels examples you get 25.81% of error.",
            "Using the traditional network and you can reduce that to 12.03% using the manifold and classifier.",
            "Also notable is the fact that we also reduced the error by half when compared with convolutional networks and SVM's in some cases, so this isn't in a semi supervised setting.",
            "We've also evaluated it in a setting where we have all the data that is."
        ],
        [
            "So this is just the regular ennist.",
            "Here I show the results only for the permutation invariant models.",
            "So we have the.",
            "We have the manifold engine classifier on the left we achieve 0.81% of error and this is close to Africa duction of the error again when compared with traditional neural network and it's also competitive when you compare it with deep belief networks and deep Boltzmann machine 1 result which shouldn't be here.",
            "But hey I had it because I thought it was.",
            "It was cool.",
            "So you have the convolutional neural network, the basic convolutional neural network at zero point 85% error.",
            "An we we beat that using the manifold tension classifier even though the convolutional networks use prior information about images.",
            "And finally."
        ],
        [
            "We also evaluated the model on cover type an.",
            "Again it compares pretty well with other models.",
            "So in this case as VMS and distributed as moms, which are stacks of SPMS."
        ],
        [
            "Alright, so in conclusion, we've presented an algorithm that learns a representation that captures the structure of the manifold.",
            "We also learn the tensions of the manifold using deep learning an we training classifier that uses that representation and that also learns to be invariant to the tension directions which correspond to things, local transformations.",
            "And finally we show that it's possible to achieve state of the art results using this method.",
            "So Lastly, I'd like to think Pascal Vasselon an you show up Andrew and I'll take questions.",
            "Thank you.",
            "So I do have a question actually.",
            "So what do you think is the most important thing you bring compared to other manifold learning approaches?",
            "One thing is that we're able to discriminatively fine tune the representation that is learned, so we don't just give you the manifold coordinates, you're able to adapt these features to the classification problem.",
            "That's one thing also.",
            "We learn that entrance in an unsupervised fashion.",
            "I think I'm not aware of another algorithm that explicitly tries to learn the tension of the manifold.",
            "Usually they are interested in the learning the coordinates, but already I simply learn also the attendance because we know there they contain interesting information about the input.",
            "Right, thank you, and so all these methods you compare the results to.",
            "They all have free para meters, some more, some less so.",
            "Yes, belief networks have a lot of free parameters and so on.",
            "Can you give us an idea how many free para meters you have to tune and how difficult it is?",
            "Yeah, so.",
            "So let's say, let's consider the deep belief networks that contracted with encoders, the depots machine.",
            "So the deep deep belief network I think had a about.",
            "A million parameters, if I'm not mistaken, but the manifold tension classifier we have about 10 million from that long.",
            "If I remember correctly, so it's still on the same order and the number of free parameters.",
            "But of course it's not limited, right?",
            "We cross validate the number of parameters in the model."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so hi everyone, my name is Jan Dofe and today I'm going to talk about how to build a classifier from a manifold perspective.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with seller.",
                    "label": 0
                },
                {
                    "sent": "If I pass canvas on your show banjo and zavia Miller.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To start off, let me state the key idea what we're going to do is that we're going to extract the tendence of the manifold using deep learning, and we're going to make a classifier invariant to these directions, and the reason that makes any sense is because these tangents of the manifold in the case of vision, for example, correspond to small translations and rotations, for example, so we know we want our classifier to be invariant to such things.",
                    "label": 0
                },
                {
                    "sent": "We extract these tangents in an unsupervised manner and to do this we leverage and we combine three powerful but generic prior hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one is the semisupervised hypothesis.",
                    "label": 0
                },
                {
                    "sent": "I'm sure all of you know it.",
                    "label": 0
                },
                {
                    "sent": "It's that the probability distribution of the input shares some structure with the conditional probability of the class labels given the input.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So by that I mean that an SVN.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give you such a decision boundary, but.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you use the unlabeled data here in black.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you're able to come up with a much better decision boundary, and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the idea that is used in intense 2006 algorithm to train deep networks and this result is further extended by your show banjo.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Marco Rubio ranzato to autoencoders.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot to say about deep learning.",
                    "label": 1
                },
                {
                    "sent": "But in a few words, the basic idea is that you want to learn a year article representation of your input.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the first hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The second is the manifold hypothesis.",
                    "label": 0
                },
                {
                    "sent": "By that I mean that your examples concentrate near or lower dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "So here we have the manifold of faces in the space of images, so most images are pretty random.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd Hypothesis is the classification manifold aybo.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an here I found a pretty cool visualization using T sne, which is dimensionality reduction technique, and this is on Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is that if examples from different classes belong to different manifolds.",
                    "label": 0
                },
                {
                    "sent": "So for example the manifold if you will or cluster faces at the bottom is separated from all the other classes by a low density region, and the idea is that if you are at a point in the manifold of faces, then any direction that keeps you within that manifold then it doesn't change the class label.",
                    "label": 0
                },
                {
                    "sent": "So you should be invariant to it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we're going to try to do is to extract these tangents in an unsupervised fashion and train a classifier to be invariant to them.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how we do it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our algorithm is within the framework of autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Because of time constraints, I'm only going to remind you the notation.",
                    "label": 0
                },
                {
                    "sent": "So you have an input X which you map to the hidden code H bold using the encoder function H you take the hidden code H bold and you map it back to Reconstruction Z using the decoder function G and the global objective is that you're trying to minimize the difference between a given input and its reconstruction.",
                    "label": 0
                },
                {
                    "sent": "For all the data set.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is all good and well when you're simply trying to compress the input.",
                    "label": 0
                },
                {
                    "sent": "But in the case where you want to learn rich information rich representations about your input, we turned to the contractive autoencoder, which was introduced early this year.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea is that instead of simply reconstructing your input, you are adding a second term.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which we call the contraction.",
                    "label": 0
                },
                {
                    "sent": "So the first time is the reconstruction, which is the usual cost of the auto encoder and the second term, which is the new term is called the contraction an it's simply the Frobenius norm of the derivative of your encoder with respect to X.",
                    "label": 0
                },
                {
                    "sent": "So basically what you're trying to measure is how much you're eating code varies when you vary around the input just a little bit.",
                    "label": 0
                },
                {
                    "sent": "So basically that term is trying to ignore all the information in the input, whereas the reconstruction is guaranteeing.",
                    "label": 0
                },
                {
                    "sent": "That you keep all of the information about the input an the interaction between these two terms is sort of like a tug of war.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me illustrate that.",
                    "label": 0
                },
                {
                    "sent": "So this is a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The slide and you have embedded within it a 1 dimensional manner.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we concentrate on this point.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here in green.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can move basically in two directions.",
                    "label": 0
                },
                {
                    "sent": "What the contraction will do is try to be invariant to when you move that point in any direction, so any move.",
                    "label": 0
                },
                {
                    "sent": "In any combination of those directions would give you the same hidden code.",
                    "label": 0
                },
                {
                    "sent": "So let's say it tries to contract.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first dimension it will succeed, so it will no longer be invariant when you are moving in that direction.",
                    "label": 0
                },
                {
                    "sent": "It will also try to be invariant to changes in.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other direction, but it's not possible because that direction is necessary to reconstruct your input properly.",
                    "label": 0
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "This is just a simple explanation, a simple visualization, but what happens is that the encoder function of your contracted with encoder will only be sensible to the directions that are within the manifold that are tangent to the manifold.",
                    "label": 0
                },
                {
                    "sent": "And by using this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we're able to define an Atlas of charts, and to do this we simply need to recover the directions to which the encoder is most sensitive to.",
                    "label": 1
                },
                {
                    "sent": "And you can do this by taking the Jacobian, which is the derivative of your H with respect to X.",
                    "label": 0
                },
                {
                    "sent": "Doing an eigendecomposition an your attention directions will be found in the left eigenvectors, so EU vector with eigenvalues above a certain threshold.",
                    "label": 0
                },
                {
                    "sent": "So now we're able to make a prediction, right?",
                    "label": 0
                },
                {
                    "sent": "We're able to say that if in fact we're extracting a lower dimensional manifold, then there should be only few eigenvalues that are non negligible for a given problem.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we evaluate both the autoencoder and the contractive autoencoder on Cifar 10, which is an object recognition data set, and what we observe is that the auto encoder is sensitive to pretty much every direction in the input.",
                    "label": 0
                },
                {
                    "sent": "However the contracted with encoder is only sensible to a few of them, meaning that it has in fact extracted a lower dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "So this is just visualizing the distribution of eigenvalues on cifar, but I think we can do better right?",
                    "label": 0
                },
                {
                    "sent": "We can also look.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the tangents.",
                    "label": 0
                },
                {
                    "sent": "So this is for M Nest which is N written character recognition digit recognition data set.",
                    "label": 0
                },
                {
                    "sent": "So on the left you have an input point and on the right you have the tender directions which are extracted by the contracted with 1/4 and this is interesting because now we're able to move around in the neighborhood of that of that zero and generate completely new examples using basic math.",
                    "label": 0
                },
                {
                    "sent": "So you simply take a linear combination.",
                    "label": 0
                },
                {
                    "sent": "Of the attendance with the input.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slightly translated to the right zero.",
                    "label": 0
                },
                {
                    "sent": "And I we can already think of some ways to improve classification using this method, one of which would be to simply augment your training set with new examples using this technique, but I think.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do better.",
                    "label": 0
                },
                {
                    "sent": "So for instance we can use tension distance which was introduced by Patrice Imma in 1993, where he uses tensions that use apriori knowledge.",
                    "label": 0
                },
                {
                    "sent": "I think he uses etherized amusing analytic knowledge an you do use that to define a better distance measure.",
                    "label": 1
                },
                {
                    "sent": "I don't have enough time to go into it, so I'm going to skip the explanation.",
                    "label": 0
                },
                {
                    "sent": "The more interesting version I think is tender and propagation again by battery SEMA.",
                    "label": 0
                },
                {
                    "sent": "Which is a framework to make any gradient based classifier invariant to tension directions and all you need to do is add a penalty to your objective and it's.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shown here on the right.",
                    "label": 0
                },
                {
                    "sent": "So if your classifier performs the function F, then all you need to do is take the derivative of F with respect to X and project it in the tangent directions for that point.",
                    "label": 0
                },
                {
                    "sent": "So in an illustration, what that means is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you know that the tangent direction of the manifold of blue points is slightly slanted, then you would figure that your decision boundary should also be slanted.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and this is done by this penalty because it tries to make the normal of the decision surface perpendicular to the tension space.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what we use to build the manifold tangent classifier.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you only need three steps at the first step you train a stack of CLL.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So tough and finally at second step you compute the tangents of each example by doing the SVD of the derivative of H1 with respect to X.",
                    "label": 1
                },
                {
                    "sent": "So we're going to have more layers.",
                    "label": 0
                },
                {
                    "sent": "I just put two layers because it fits on the page and the last step is to try.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a logistic regression, so you stuck a logistic regression on top of the representation and you train that logistic regression is by adding also the tangent propagation penalty an.",
                    "label": 1
                },
                {
                    "sent": "Also we also fine tune all the weights.",
                    "label": 0
                },
                {
                    "sent": "So we do global fine tuning.",
                    "label": 1
                },
                {
                    "sent": "Alright, so that's the manifold tangent classifier.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, as I've shown you before, we have some pretty nice tangents on ennist, but we can also look to a more complete.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem, so here this is.",
                    "label": 0
                },
                {
                    "sent": "This is a secret and this is not our method, I assure you.",
                    "label": 0
                },
                {
                    "sent": "So here you have the input point, which is a dog with a shirt for some reason.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Here you have the tensions using local PCA, which is just PCA on a neighborhood of the examples, so we're trying to see how how well can you approximate attendance using local methods.",
                    "label": 0
                },
                {
                    "sent": "You could argue with.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but this is what we get using the contractive autoencoder.",
                    "label": 1
                },
                {
                    "sent": "So the difference is pretty striking when you're going to serve is that, for example, some of the tenants correspond to small translations of the legs.",
                    "label": 0
                },
                {
                    "sent": "For example, the next to last tenant for example.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've also evaluated it on text, so this is our CV one where you have documents as bag of words.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into it, but you can check the paper if you want to know more.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About that, so let's talk a little bit more about classification.",
                    "label": 0
                },
                {
                    "sent": "So first, let's see if we can improve the performance of a simple K nearest neighbor classifier using the tension distance proposed by Patrice.",
                    "label": 0
                },
                {
                    "sent": "Imma using the tensions that we extract using the contractive auto encoder.",
                    "label": 0
                },
                {
                    "sent": "So unless you have the regular cameras neighbor with Euclidean distance on the right, you have key nearest neighbor using our attention distance and what you can see is that on the data set that we've considered, we get an improvement using these tenants.",
                    "label": 0
                },
                {
                    "sent": "But knowledge, let's talk about the manifold tension classifier.",
                    "label": 0
                },
                {
                    "sent": "So this is an amnesty.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With 106 hundred, 1000 and 3000 labels and 50,000 unlabeled examples, so this is a semi supervised setting.",
                    "label": 0
                },
                {
                    "sent": "And here we're comparing with from left to right, traditional neural networks, SVM's convolutional neural networks, transductive SVM's deep belief networks, regularize neighborhood component analysis, embed neural network and.",
                    "label": 0
                },
                {
                    "sent": "The contract if autoencoder and finally the manifold tangent classifier, so it's a bunch of numbers.",
                    "label": 0
                },
                {
                    "sent": "Of course some of the results taken from other papers, but I think the more interesting results are that the manifold tension classifier in all the cases that we've considered reduces the error by half when you compare it with the traditional neural network.",
                    "label": 0
                },
                {
                    "sent": "So for example, with 100 labels examples you get 25.81% of error.",
                    "label": 0
                },
                {
                    "sent": "Using the traditional network and you can reduce that to 12.03% using the manifold and classifier.",
                    "label": 0
                },
                {
                    "sent": "Also notable is the fact that we also reduced the error by half when compared with convolutional networks and SVM's in some cases, so this isn't in a semi supervised setting.",
                    "label": 0
                },
                {
                    "sent": "We've also evaluated it in a setting where we have all the data that is.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just the regular ennist.",
                    "label": 0
                },
                {
                    "sent": "Here I show the results only for the permutation invariant models.",
                    "label": 1
                },
                {
                    "sent": "So we have the.",
                    "label": 0
                },
                {
                    "sent": "We have the manifold engine classifier on the left we achieve 0.81% of error and this is close to Africa duction of the error again when compared with traditional neural network and it's also competitive when you compare it with deep belief networks and deep Boltzmann machine 1 result which shouldn't be here.",
                    "label": 0
                },
                {
                    "sent": "But hey I had it because I thought it was.",
                    "label": 0
                },
                {
                    "sent": "It was cool.",
                    "label": 0
                },
                {
                    "sent": "So you have the convolutional neural network, the basic convolutional neural network at zero point 85% error.",
                    "label": 0
                },
                {
                    "sent": "An we we beat that using the manifold tension classifier even though the convolutional networks use prior information about images.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also evaluated the model on cover type an.",
                    "label": 0
                },
                {
                    "sent": "Again it compares pretty well with other models.",
                    "label": 0
                },
                {
                    "sent": "So in this case as VMS and distributed as moms, which are stacks of SPMS.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so in conclusion, we've presented an algorithm that learns a representation that captures the structure of the manifold.",
                    "label": 1
                },
                {
                    "sent": "We also learn the tensions of the manifold using deep learning an we training classifier that uses that representation and that also learns to be invariant to the tension directions which correspond to things, local transformations.",
                    "label": 1
                },
                {
                    "sent": "And finally we show that it's possible to achieve state of the art results using this method.",
                    "label": 0
                },
                {
                    "sent": "So Lastly, I'd like to think Pascal Vasselon an you show up Andrew and I'll take questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So I do have a question actually.",
                    "label": 0
                },
                {
                    "sent": "So what do you think is the most important thing you bring compared to other manifold learning approaches?",
                    "label": 0
                },
                {
                    "sent": "One thing is that we're able to discriminatively fine tune the representation that is learned, so we don't just give you the manifold coordinates, you're able to adapt these features to the classification problem.",
                    "label": 0
                },
                {
                    "sent": "That's one thing also.",
                    "label": 0
                },
                {
                    "sent": "We learn that entrance in an unsupervised fashion.",
                    "label": 0
                },
                {
                    "sent": "I think I'm not aware of another algorithm that explicitly tries to learn the tension of the manifold.",
                    "label": 0
                },
                {
                    "sent": "Usually they are interested in the learning the coordinates, but already I simply learn also the attendance because we know there they contain interesting information about the input.",
                    "label": 0
                },
                {
                    "sent": "Right, thank you, and so all these methods you compare the results to.",
                    "label": 0
                },
                {
                    "sent": "They all have free para meters, some more, some less so.",
                    "label": 0
                },
                {
                    "sent": "Yes, belief networks have a lot of free parameters and so on.",
                    "label": 0
                },
                {
                    "sent": "Can you give us an idea how many free para meters you have to tune and how difficult it is?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So let's say, let's consider the deep belief networks that contracted with encoders, the depots machine.",
                    "label": 0
                },
                {
                    "sent": "So the deep deep belief network I think had a about.",
                    "label": 0
                },
                {
                    "sent": "A million parameters, if I'm not mistaken, but the manifold tension classifier we have about 10 million from that long.",
                    "label": 0
                },
                {
                    "sent": "If I remember correctly, so it's still on the same order and the number of free parameters.",
                    "label": 0
                },
                {
                    "sent": "But of course it's not limited, right?",
                    "label": 0
                },
                {
                    "sent": "We cross validate the number of parameters in the model.",
                    "label": 0
                }
            ]
        }
    }
}