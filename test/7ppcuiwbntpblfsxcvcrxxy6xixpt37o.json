{
    "id": "7ppcuiwbntpblfsxcvcrxxy6xixpt37o",
    "title": "Incremental and Decremental Training for Linear Classification",
    "info": {
        "author": [
            "Ching-Pei Lee, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Knowledge Extraction",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2014_lee_linear_classification/",
    "segmentation": [
        [
            "I'm presenting incremental and incremental learning trending for linear classification, and the authors are Taj Mahal India and from National Taiwan University."
        ],
        [
            "OK, so here is the outline of my presentation today."
        ],
        [
            "So first, when in classification if we have some very few data points added or removed, then we can use incremental or decremental learning to quickly update our model.",
            "But there are some issues saying using this techniques.",
            "The first one is that it's not guaranteed to be faster than directory training a new model.",
            "And Secondly the framework of incorporating this techniques with our original optimization procedure may be very.",
            "Complicated and Thirdly, the scenario of applying this techniques may be application dependent.",
            "For designing software for general use, it would be very difficult."
        ],
        [
            "So an in this work we will focus on linear classification, cause for kernel methods.",
            "The model is is a linear combination of our training instances and therefore our optimization methods is very restrictive.",
            "And also when data points are added or removed to deal with the kernel matrix and its operation is very complicated.",
            "An on the other hand, for linear classification model is very simple weight vector of the features.",
            "So we have many different optimization methods that we can use an also linear models can give incomparable accuracy to kernel methods.",
            "Some applications such as documented possible."
        ],
        [
            "In our work, we assume that there are different instances in our training data and all of them are with N dimensional feature an binary label.",
            "So our model will be will be consist of 1L2 regularization term and the last term here and the last term can be one loss SVM or SVM or.",
            "Logistic regression.",
            "So the dual problem of our problem is can we see sorry."
        ],
        [
            "The problem is can be shown here and there are different parameters of D, an, U and the function edge depending on which last term where you think."
        ],
        [
            "So our setting for incremental earnings shown here for incremental learning, was saying that there are K instances at their index from L + 1, two L + K. For decrementing learning, we assume that the first K instances are removed and our strategy is this is a warm start setting that we utilized the optimal solution of our.",
            "Our original optimization problem."
        ],
        [
            "And the data are here.",
            "We assume that the optimal solution of our original primal and dual problems RW Star in August, are respectively for Primal, Primal, Primal.",
            "Problem will use W Star S or or new new initial solution of the new optimization problem for both incremental and decremental learning, and the reason of that we can use this double stars that even though the data points are.",
            "Added or removed the.",
            "The features are remained the same, so there is no problem of using it for the dual problem.",
            "We will use the star for incremental learning for in the old instances we use we will use the same of a star for the newly added instances, will simply assign 0 for them.",
            "For decremental learning we will just remove those.",
            "Those entries of alphastar related to both data points we do not use anymore."
        ],
        [
            "So.",
            "Seems our strategy is very simple.",
            "There are still some issues that we should."
        ],
        [
            "So we will invest three different issues.",
            "The first one is that should solve the primal problem or the dual problem and the second one is that which kind of optimization method should we choose?",
            "And the third one is that we will check some implementation issues."
        ],
        [
            "So for."
        ],
        [
            "For choosing to solve the prime or the dual problem, we think that we should consider the problem that has a better initial point that is closer to its optimal.",
            "So the training time should be less.",
            "And after some analyzed as shown in later slides, we found that the best for the better one is the primal because its initial point is closer to its optimal when warm start is applied."
        ],
        [
            "So for incremental learning we can see here.",
            "The optimization problem is shown here with."
        ],
        [
            "We added the last term for the newly added instances.",
            "Anne."
        ],
        [
            "The primal initial objective value is shown.",
            "Here.",
            "We use double star as our initial solution."
        ],
        [
            "And for the dual problem, we can say that substituting of a star and or initial solution we can see that.",
            "Formed here, an S0C is their role for all the three different losses."
        ],
        [
            "And although there are nearly added entrance entries for the metrics in the latter term, there are multiplying 0, so it will not affect our initial value, so."
        ],
        [
            "So eventually it's the same or optimal solution of the original problem.",
            "So which one should better we consider a scaled version of our original problem with simply divides our problem by CL.",
            "So John here.",
            "The latter term becomes an average loss term, so if K is not very, very large, then we can say that balance between the two terms will not will not be affected a lot.",
            "And also if.",
            "Describe the virtual as well and the data distribution of the old instances in the new instances are the same.",
            "Then double Star should still describe our average loss of the new problem.",
            "Well, so under this idea would think that the primal initial objective value should be very close to our new optimal."
        ],
        [
            "On the other hand, for the dual initial value, we can see that.",
            "Because it does not consider the last term for the newly added instances.",
            "So we think that it should be too small in comparison with our primary initial value."
        ],
        [
            "An hour analyzes in decremental learning for the primal.",
            "Problem is the same cause we use the same W star for different learning for the dual dual problem.",
            "We can.",
            "After some derivation we can see that there is a loss term showing there.",
            "So because of this.",
            "Term so because of this minus term, we think that the dual initial valuation should be too small.",
            "So in both cases the primal solving the primal may be better when warm starts used."
        ],
        [
            "Anwen about truth."
        ],
        [
            "Thing or optimization methods?",
            "When we do not have a good initial point like like them, low order information, lower their optimization method may be better because it can quickly converge to an acceptable point as shown from the left view.",
            "Configure here an.",
            "A method using higher order information may converge to that acceptable point slower, but.",
            "For example, as the circle show in there so.",
            "But if we have a better initial point like like the circle, then high order methods can converge to the optimum like the cross there.",
            "More quickly than lower their information method, so we think that if we apply warm stars then we will have a better initial point, so we should.",
            "We should consider a higher order method."
        ],
        [
            "And in our experiment, we will use three different optimization methods.",
            "There are trying MP, CD and E. CD Train is the trust region.",
            "Newton methods that solving the primal problem because because Newton method used is high order information, so like the Hessian matrix.",
            "So we consider it as a high order method, an PCD and Cdr coordinated, some methods and solving the primal and dual problem.",
            "Respectively.",
            "So because Cortana send Mail deal with one variable at a time, so we will consider it at lower their methods."
        ],
        [
            "And for implementation issue."
        ],
        [
            "The well will check that if there are any additional information we should maintained in addition to our original model.",
            "So originally for the prediction, we only need double star for the.",
            "For the original problem.",
            "And so if we are using warm starting or primal problem, which in any any additional thing because we just use W star and for for solving the dual problem then we need of a star as we mentioned earlier.",
            "But maintaining this information is very complicated because when the data is shuffled or something or some data points are removed we have to check them.",
            "The correspondence of the entries and then.",
            "Instances, so we think that implementing a primal solver for warm start is easier."
        ],
        [
            "Regardie"
        ],
        [
            "In our experiment for incremental learning, well randomly defined our data into our hearts and take our minus one part as originally training data.",
            "After obtaining the optimal solution, we will take the whole data set as our training data for incremental learning.",
            "An for the setting for decremental learning is very similar and for both settings smaller are will represent that our data changes larger."
        ],
        [
            "So here are some data the statistics and we will evaluate the different optimization methods by the relative difference to the optimum.",
            "And we will show results of logistic regression with C = 2 one."
        ],
        [
            "Shown here is the distance from the initial solution to the optimum.",
            "So we can say that if we do not use warm start, then.",
            "And then usually dual problem has a better initial point.",
            "But when one star is applied then we can see them primal.",
            "Primal problems has a better initial solution and.",
            "And there is one thing North mentioned that is.",
            "Warm start is more powerful for the problems we can say then when it is applied the reducing the distance is the reduction of the distance is very significant, but it's not that powerful for the dual problem."
        ],
        [
            "And here are some other results.",
            "Anne.",
            "So the X axis is the.",
            "The training time and the Y axis is the objective value, so we can see that.",
            "The Red 1 does not apply warm star then others are using warm start.",
            "We can say that warm stars.",
            "Indeed, more effectively for for the Primal Primal Solvers and also.",
            "There are some convergence issue for the PC, so we do not discuss it here.",
            "And another thing is that the time scale for different servers are different.",
            "So when we do not use warm start.",
            "Then this idea is the best one, but when warm start is applied, actually trunk can be competitive."
        ],
        [
            "The result for decremental learning is very similar."
        ],
        [
            "So."
        ],
        [
            "In conclusion that we think Warm Star is most suitable for a primal based high order optimization methods, and because of the following three reasons, the first one is that it can give warm start can give Primal primal problem a better initial solution than the dual problem.",
            "And it is also easier to implement a primal solver.",
            "For one, start.",
            "And Thirdly, for the high order part, because one star can Canmore effectively speed up high order optimization method.",
            "So we will therefore implement Trump as our incremental and decremental learning extension of the popular linear classification package Lib Linear.",
            "It is released as the following page here.",
            "Thank you very much.",
            "Do you have any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm presenting incremental and incremental learning trending for linear classification, and the authors are Taj Mahal India and from National Taiwan University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the outline of my presentation today.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first, when in classification if we have some very few data points added or removed, then we can use incremental or decremental learning to quickly update our model.",
                    "label": 1
                },
                {
                    "sent": "But there are some issues saying using this techniques.",
                    "label": 1
                },
                {
                    "sent": "The first one is that it's not guaranteed to be faster than directory training a new model.",
                    "label": 0
                },
                {
                    "sent": "And Secondly the framework of incorporating this techniques with our original optimization procedure may be very.",
                    "label": 0
                },
                {
                    "sent": "Complicated and Thirdly, the scenario of applying this techniques may be application dependent.",
                    "label": 0
                },
                {
                    "sent": "For designing software for general use, it would be very difficult.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an in this work we will focus on linear classification, cause for kernel methods.",
                    "label": 1
                },
                {
                    "sent": "The model is is a linear combination of our training instances and therefore our optimization methods is very restrictive.",
                    "label": 1
                },
                {
                    "sent": "And also when data points are added or removed to deal with the kernel matrix and its operation is very complicated.",
                    "label": 0
                },
                {
                    "sent": "An on the other hand, for linear classification model is very simple weight vector of the features.",
                    "label": 1
                },
                {
                    "sent": "So we have many different optimization methods that we can use an also linear models can give incomparable accuracy to kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Some applications such as documented possible.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our work, we assume that there are different instances in our training data and all of them are with N dimensional feature an binary label.",
                    "label": 0
                },
                {
                    "sent": "So our model will be will be consist of 1L2 regularization term and the last term here and the last term can be one loss SVM or SVM or.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So the dual problem of our problem is can we see sorry.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is can be shown here and there are different parameters of D, an, U and the function edge depending on which last term where you think.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our setting for incremental earnings shown here for incremental learning, was saying that there are K instances at their index from L + 1, two L + K. For decrementing learning, we assume that the first K instances are removed and our strategy is this is a warm start setting that we utilized the optimal solution of our.",
                    "label": 0
                },
                {
                    "sent": "Our original optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the data are here.",
                    "label": 0
                },
                {
                    "sent": "We assume that the optimal solution of our original primal and dual problems RW Star in August, are respectively for Primal, Primal, Primal.",
                    "label": 1
                },
                {
                    "sent": "Problem will use W Star S or or new new initial solution of the new optimization problem for both incremental and decremental learning, and the reason of that we can use this double stars that even though the data points are.",
                    "label": 1
                },
                {
                    "sent": "Added or removed the.",
                    "label": 0
                },
                {
                    "sent": "The features are remained the same, so there is no problem of using it for the dual problem.",
                    "label": 1
                },
                {
                    "sent": "We will use the star for incremental learning for in the old instances we use we will use the same of a star for the newly added instances, will simply assign 0 for them.",
                    "label": 0
                },
                {
                    "sent": "For decremental learning we will just remove those.",
                    "label": 0
                },
                {
                    "sent": "Those entries of alphastar related to both data points we do not use anymore.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Seems our strategy is very simple.",
                    "label": 0
                },
                {
                    "sent": "There are still some issues that we should.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we will invest three different issues.",
                    "label": 0
                },
                {
                    "sent": "The first one is that should solve the primal problem or the dual problem and the second one is that which kind of optimization method should we choose?",
                    "label": 0
                },
                {
                    "sent": "And the third one is that we will check some implementation issues.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For choosing to solve the prime or the dual problem, we think that we should consider the problem that has a better initial point that is closer to its optimal.",
                    "label": 1
                },
                {
                    "sent": "So the training time should be less.",
                    "label": 1
                },
                {
                    "sent": "And after some analyzed as shown in later slides, we found that the best for the better one is the primal because its initial point is closer to its optimal when warm start is applied.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for incremental learning we can see here.",
                    "label": 0
                },
                {
                    "sent": "The optimization problem is shown here with.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We added the last term for the newly added instances.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The primal initial objective value is shown.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We use double star as our initial solution.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the dual problem, we can say that substituting of a star and or initial solution we can see that.",
                    "label": 0
                },
                {
                    "sent": "Formed here, an S0C is their role for all the three different losses.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And although there are nearly added entrance entries for the metrics in the latter term, there are multiplying 0, so it will not affect our initial value, so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So eventually it's the same or optimal solution of the original problem.",
                    "label": 1
                },
                {
                    "sent": "So which one should better we consider a scaled version of our original problem with simply divides our problem by CL.",
                    "label": 0
                },
                {
                    "sent": "So John here.",
                    "label": 0
                },
                {
                    "sent": "The latter term becomes an average loss term, so if K is not very, very large, then we can say that balance between the two terms will not will not be affected a lot.",
                    "label": 0
                },
                {
                    "sent": "And also if.",
                    "label": 0
                },
                {
                    "sent": "Describe the virtual as well and the data distribution of the old instances in the new instances are the same.",
                    "label": 1
                },
                {
                    "sent": "Then double Star should still describe our average loss of the new problem.",
                    "label": 0
                },
                {
                    "sent": "Well, so under this idea would think that the primal initial objective value should be very close to our new optimal.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, for the dual initial value, we can see that.",
                    "label": 1
                },
                {
                    "sent": "Because it does not consider the last term for the newly added instances.",
                    "label": 0
                },
                {
                    "sent": "So we think that it should be too small in comparison with our primary initial value.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An hour analyzes in decremental learning for the primal.",
                    "label": 1
                },
                {
                    "sent": "Problem is the same cause we use the same W star for different learning for the dual dual problem.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "After some derivation we can see that there is a loss term showing there.",
                    "label": 0
                },
                {
                    "sent": "So because of this.",
                    "label": 0
                },
                {
                    "sent": "Term so because of this minus term, we think that the dual initial valuation should be too small.",
                    "label": 1
                },
                {
                    "sent": "So in both cases the primal solving the primal may be better when warm starts used.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anwen about truth.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing or optimization methods?",
                    "label": 0
                },
                {
                    "sent": "When we do not have a good initial point like like them, low order information, lower their optimization method may be better because it can quickly converge to an acceptable point as shown from the left view.",
                    "label": 1
                },
                {
                    "sent": "Configure here an.",
                    "label": 0
                },
                {
                    "sent": "A method using higher order information may converge to that acceptable point slower, but.",
                    "label": 0
                },
                {
                    "sent": "For example, as the circle show in there so.",
                    "label": 1
                },
                {
                    "sent": "But if we have a better initial point like like the circle, then high order methods can converge to the optimum like the cross there.",
                    "label": 0
                },
                {
                    "sent": "More quickly than lower their information method, so we think that if we apply warm stars then we will have a better initial point, so we should.",
                    "label": 0
                },
                {
                    "sent": "We should consider a higher order method.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in our experiment, we will use three different optimization methods.",
                    "label": 1
                },
                {
                    "sent": "There are trying MP, CD and E. CD Train is the trust region.",
                    "label": 0
                },
                {
                    "sent": "Newton methods that solving the primal problem because because Newton method used is high order information, so like the Hessian matrix.",
                    "label": 0
                },
                {
                    "sent": "So we consider it as a high order method, an PCD and Cdr coordinated, some methods and solving the primal and dual problem.",
                    "label": 0
                },
                {
                    "sent": "Respectively.",
                    "label": 0
                },
                {
                    "sent": "So because Cortana send Mail deal with one variable at a time, so we will consider it at lower their methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for implementation issue.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The well will check that if there are any additional information we should maintained in addition to our original model.",
                    "label": 1
                },
                {
                    "sent": "So originally for the prediction, we only need double star for the.",
                    "label": 0
                },
                {
                    "sent": "For the original problem.",
                    "label": 1
                },
                {
                    "sent": "And so if we are using warm starting or primal problem, which in any any additional thing because we just use W star and for for solving the dual problem then we need of a star as we mentioned earlier.",
                    "label": 1
                },
                {
                    "sent": "But maintaining this information is very complicated because when the data is shuffled or something or some data points are removed we have to check them.",
                    "label": 0
                },
                {
                    "sent": "The correspondence of the entries and then.",
                    "label": 0
                },
                {
                    "sent": "Instances, so we think that implementing a primal solver for warm start is easier.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regardie",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our experiment for incremental learning, well randomly defined our data into our hearts and take our minus one part as originally training data.",
                    "label": 1
                },
                {
                    "sent": "After obtaining the optimal solution, we will take the whole data set as our training data for incremental learning.",
                    "label": 1
                },
                {
                    "sent": "An for the setting for decremental learning is very similar and for both settings smaller are will represent that our data changes larger.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some data the statistics and we will evaluate the different optimization methods by the relative difference to the optimum.",
                    "label": 0
                },
                {
                    "sent": "And we will show results of logistic regression with C = 2 one.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shown here is the distance from the initial solution to the optimum.",
                    "label": 0
                },
                {
                    "sent": "So we can say that if we do not use warm start, then.",
                    "label": 0
                },
                {
                    "sent": "And then usually dual problem has a better initial point.",
                    "label": 0
                },
                {
                    "sent": "But when one star is applied then we can see them primal.",
                    "label": 0
                },
                {
                    "sent": "Primal problems has a better initial solution and.",
                    "label": 0
                },
                {
                    "sent": "And there is one thing North mentioned that is.",
                    "label": 0
                },
                {
                    "sent": "Warm start is more powerful for the problems we can say then when it is applied the reducing the distance is the reduction of the distance is very significant, but it's not that powerful for the dual problem.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some other results.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So the X axis is the.",
                    "label": 0
                },
                {
                    "sent": "The training time and the Y axis is the objective value, so we can see that.",
                    "label": 0
                },
                {
                    "sent": "The Red 1 does not apply warm star then others are using warm start.",
                    "label": 0
                },
                {
                    "sent": "We can say that warm stars.",
                    "label": 0
                },
                {
                    "sent": "Indeed, more effectively for for the Primal Primal Solvers and also.",
                    "label": 0
                },
                {
                    "sent": "There are some convergence issue for the PC, so we do not discuss it here.",
                    "label": 0
                },
                {
                    "sent": "And another thing is that the time scale for different servers are different.",
                    "label": 0
                },
                {
                    "sent": "So when we do not use warm start.",
                    "label": 0
                },
                {
                    "sent": "Then this idea is the best one, but when warm start is applied, actually trunk can be competitive.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The result for decremental learning is very similar.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion that we think Warm Star is most suitable for a primal based high order optimization methods, and because of the following three reasons, the first one is that it can give warm start can give Primal primal problem a better initial solution than the dual problem.",
                    "label": 1
                },
                {
                    "sent": "And it is also easier to implement a primal solver.",
                    "label": 0
                },
                {
                    "sent": "For one, start.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, for the high order part, because one star can Canmore effectively speed up high order optimization method.",
                    "label": 1
                },
                {
                    "sent": "So we will therefore implement Trump as our incremental and decremental learning extension of the popular linear classification package Lib Linear.",
                    "label": 0
                },
                {
                    "sent": "It is released as the following page here.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                }
            ]
        }
    }
}