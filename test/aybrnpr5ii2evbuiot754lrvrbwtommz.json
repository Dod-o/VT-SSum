{
    "id": "aybrnpr5ii2evbuiot754lrvrbwtommz",
    "title": "Use of variance estimation in the multi-armed bandit problem",
    "info": {
        "author": [
            "Jean Yves Audibert, Center for Education and Research in Computer Science of the \u00c9cole des ponts, \u00c9cole des Ponts ParisTech, MINES ParisTech"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/otee06_audibert_uvema/",
    "segmentation": [
        [
            "Help.",
            "So.",
            "Next door is.",
            "Use a variance estimation in the Northland problem.",
            "Thank you so this is a joint work with him.",
            "You know San Saba Sipos very."
        ],
        [
            "And here's the outline.",
            "First, I will describe the program I'm interesting in, and one particular type of algorithm which solves this program.",
            "And then I will turn to two proposition of UCB policy which use variance estimates in it."
        ],
        [
            "So let us first describe the program so it is a multi armed bandit problem.",
            "You assume that you are in front of the machine which have many arms and at each time step you have to draw an arm and disarm will provide you a reward and your goal is just to maximize your competitive rewards reward.",
            "And the key assumptions.",
            "It's at successive place of an arm.",
            "I did realizations.",
            "Of some unknown distribution so that you can see an arm.",
            "Through its distribution through its reward distribution.",
            "And we assume that the rewards of different arms are independent and, well, we also assume that so we both are bounded to simplifies."
        ],
        [
            "Things.",
            "So let us fix the notation in order to see the quantities which are involved.",
            "So first the number of arms arms will be capital K. So and X Katie is not the reward of RK at time T. It is the reward of RMK when drone for that if time.",
            "So this is an important point.",
            "And well, since we have assumed that the rewards are bounded, you without loss of generality, you can say that the bound to zero and one.",
            "And the distribution of the reward of MK is characterized by its expectation.",
            "Mutai and its variants, Sigmascape Square, so optimal arm is denoted K star, and the expected reward of K star is simply denoted new star."
        ],
        [
            "OK. Deltacare is expected regret of K, so it is the difference between the optimal.",
            "Reward expected reward and the expected reward of K. And once you have drawn teetimes.",
            "K What you can observe is the empirical means of your rewards and the empirical means is just denoted with a bar above SKT.",
            "Is there empirical variance is noted the key KT and policy is just a way of choosing the next arm.",
            "Based on past place and obtain rewards.",
            "And then I did not I see the the unplayed by the policy at time T an maybe an important notation is this one TKT which is the number of times K is chosen by the policy during the first place.",
            "And we define the cumulative regret or cumulative.",
            "So the regret by the sum of the arms of the number of times the arm is played times the expected regret of the arm.",
            "OK, and we are interested in having a bound and this communitive regret and also we are interested in having a bound on its expected value.",
            "So it's expected value, which is the sum of the arms of the expected number of times the arm has been drawn.",
            "Times the expected regret.",
            "And one can see also this quantity this way in which you have any times the optimal expected reward minus the expectation of what?",
            "What happens under some of what happens at anytime?",
            "So well, it seems complicated, but this X, ITT, ITT is just reward you get at time T because at time T you draw the arm it.",
            "And the number of times you have drawn the arm, IT is just TITT.",
            "Under 50 time steps.",
            "This is different than there again I was using because it's yes, yes, yeah that's true and that's why this balance will be really different, yeah?",
            "Well, here I did not put the sum of the rewards you would have obtained if you have drawn more arms and the sum of of rewards is has a division of order square root of N. So you cannot expect bounds which are better than square root of N. If you put this kind of term.",
            "So yeah, this is a definition of micro motive request and the expected comparative regret is really important.",
            "It changes things."
        ],
        [
            "OK, and so let us down to turn to this family of algorithm which is UC policy and social ID is at at time T. From from pass observation and some probabilistic arguments you can build some bound upper bound on the expected reward of any arm and the policy is just you place the arm having the largest upper bound on your expected reward.",
            "So first question, it is why does it make sense and the way to address this question is to ask you."
        ],
        [
            "Well, could we stay a long time drawing wrong?",
            "And in fact you can convince yourself that it is not possible because the more you draw an alarm wrong arm, so closer you're bound gets to the expected reward well.",
            "Well, assuming that you are taking a correct probabilistic upper bound.",
            "And and then, since you're the.",
            "Since the reward to which you converge is strictly smaller than the optimal expected reward, which is also smaller than the upper bound on it.",
            "Then you see that your upper bound on K will go below.",
            "This is the upper bound on Seal team.",
            "So this is the basic idea.",
            "Sorry.",
            "Yeah, well, we don't even use that.",
            "Use the upper bound on new star is.",
            "Close to new stuff which we really used.",
            "It sets the upper bound.",
            "On under one arm will goes to new K If you draw it too much.",
            "And that's a key.",
            "That's a key point.",
            "And then yeah."
        ],
        [
            "I will return to this maybe later.",
            "So the 1st police here we introduce is the beta UCB policy and it is basically based on Bernstein type inequality.",
            "So let us remind what is Bernstein inequality is just?",
            "Well, some kind of know that synthetic version of Central Limit Theorem.",
            "So it just says that the empirical mean is close to the true mean and well closed up to some terms in which typically you have standard deviation over square root of of T. So I'm standing there.",
            "Quality is a background, so it means it is true with high probability and so confidence level.",
            "Beta appears for logarithmic.",
            "OK, and the only modification we do here is.",
            "Well, you cannot use this to talk about your expected reward.",
            "Of of Anima tightly because you don't know Sigma and so the only thing you can say well is.",
            "Sigma is below 1/4 because this is a random variable between text value between zero and one.",
            "But if you want to say something stronger which will be interested when the variance of the arms are small, then you have to.",
            "2 upper bounds.",
            "This is standard deviation with an empirical quantity, and naturally what appears it is empirical standard deviation, which I call S. And to do this, in fact you have to use one small."
        ],
        [
            "Ben Stein inequality.",
            "So the starting point is Bernstein inequality applied with an appropriate confidence level.",
            "So I just rewrite them mentioning equality here and then to control Sigma you consider the IID random variable XI minus expectation of X squared improving.",
            "So sorry, it's what is known is based on equality and what is not known is this bench time studying inequality.",
            "Sorry, Yep, thank you.",
            "And OK, so to control Sigma.",
            "You just notice that Sigma Square can be interpreted as the expectation of this.",
            "This random variable excitement and expectation of X square and you notice that the empirical means of these random variables.",
            "Can be expressed in terms of the empirical volumes esquerre plus some terms and.",
            "Well and if you see this then it is over because this term is what you have already controlled here so well, I don't make the computation here, but what you have to solve it polynomial degree two polynomial in equation and so it's solvable and you get the formula announced."
        ],
        [
            "And to see that this inequality is true, this equality is true.",
            "You just think of the random variable W, which takes value XI with probability 1, / T. And this is just a standard variance decomposition where a place this is just a constant and the expectation of the value is X bar by definition of what I call W. OK, so."
        ],
        [
            "So for the sketch of the proof and now I can define the UCB policy.",
            "And it is based on one parameter beta, which is positive and which represents a confidence level.",
            "And from this can fitness level.",
            "You can define sub confidence levels better S and in fact you will use the bench time type inequality I've just mentioned for each arm and for each number of draws.",
            "Over of the armor, so you have an infinite number of inequalities."
        ],
        [
            "And once you have.",
            "And what we have done here is just to put some weight such that when you sum this is a different confidence level.",
            "You have something of order better.",
            "They just say, you know, you know, inbound over infinite numbers of inequality."
        ],
        [
            "OK, and then you define your bound which on the expected reward of RMK when you observe SS times rewards coming from ARM K which is just coming from bench time.",
            "Bernstein's type inequality.",
            "When we wish we had just inform with one because the rewards are bounded by one, so the expected we want is bounded well by 1.",
            "Yes, that's why it says before in fact OK. And those wanting to enter into his details that way.",
            "OK, and the policy is just to play the maximizing your your bound.",
            "Why is TKT minus one?",
            "It is when you are at time T it is just a number of times you have drawn ARM K. So you use the bound.",
            "For this number of times.",
            "For this number of jobs of amke, OK, this is the policy an."
        ],
        [
            "There's a nice thing about this policy.",
            "Is this result which says that with high probability.",
            "Is your better UCB policy will play?",
            "Non empty more arms only at most UK times where UK is the smallest integer satisfying this inequality.",
            "So OK, this inequality is not that nice because."
        ],
        [
            "OK, appears several times so you can just derive rough about about on it, which has the following term.",
            "One should see this upper bound by concentrated just on the value K log of beta minus one, and the surprising thing here is that it does not depend on the number of.",
            "Of the truth is a number of places.",
            "I mean, you can play infinitely.",
            "You will never well.",
            "Never with high priority.",
            "Place is not empty.",
            "More arms more than some fixed number which is UK.",
            "OK."
        ],
        [
            "So from this control of the number of times you play non optimal arms you can get."
        ],
        [
            "At the bound on your communitive regret.",
            "Just buy something, say something's bounds, waited by the suspected regret Delta K. And so.",
            "For the moment I just gave results with high priority.",
            "Now we will switch to result with for the expected cumulative regret.",
            "And to do this, one should understand that when you have results with high probability, you do not control at all what happens and so.",
            "So the event of probability, bitter and typically what happens here on this event, is that you can have a regret of order N. So if you want to compensate this huge loss because.",
            "Yeah, this is just large loss.",
            "You need to take beta smaller than one of and that's what we have done precisely.",
            "We took better equal to 1 / N and then just buy.",
            "Bye.",
            "By saying that on the low probability event, the worst we can do is N and the high probability event.",
            "I have a very tight bound of the of the number of times to draw nonoptimal arms.",
            "You end up with an upper bound on the expected creative regret.",
            "Log in.",
            "No, because it's not useful.",
            "I mean OK, you can take your log, login over end, but it does not.",
            "Appears to be useful.",
            "You just have to write the tradeoff between the two terms.",
            "One is log better minus one and the other is better times N. So basically it does not change.",
            "Yeah, you can take 1 / N square.",
            "It won't change basically the bound, it's just a matter of constant.",
            "OK, so this will search should be read that as you have a logarithmic cumulative regret.",
            "And you have this summation here.",
            "In which the nice thing is that there is a variant of ARM K which appears.",
            "It means that if non optimal arms has low variances then you will you will have a tighter upper bound.",
            "And here this one should be interpreted as.",
            "Sorry yeah.",
            "Number of times along with creating this one policy, what is it?",
            "Optimal arms would be great.",
            "Well in speaking.",
            "Yeah, yeah yeah yeah, you have this summation.",
            "So forget the summation.",
            "Divide by Delta.",
            "Can you obtain it?",
            "And yeah, and so on here it should be interpreted as a range of the rewards.",
            "And otherwise with Formula One beer genius.",
            "You're saying that you have to know an ahead of time.",
            "Or yeah, I will come to it.",
            "You are ahead of me."
        ],
        [
            "OK, so let's compare it.",
            "To the most simple UCB policy which is UCB one and so basically you have this same logarithmic term, But here this term is smaller will be typically smaller than this one because Delta case always smaller than one.",
            "So up to two it is this one is always smaller than the nice part is the control of the variance here because it could be.",
            "This term could be much smaller than this one.",
            "Well, this is the comparison to the simple."
        ],
        [
            "List you see people see if we compare it to a variation variant of this policy.",
            "Then we have typically the same kind of bound.",
            "But here this bound is a bit better because you don't have the range of the rewards here.",
            "Here you have instead, so the expected regret, which is which could be much smaller than one.",
            "But this result is only true for normal distributions.",
            "And, well, we believe that the fact that the range of the rewards appear here is quite natural.",
            "Well, we're still arguing about this, but.",
            "I believe it's quite natural if you don't have.",
            "How long?",
            "OK. OK so I. I will go to."
        ],
        [
            "So the main drawback of this policy is that that you need to know the number of plays.",
            "Well, you have a standard trick, which is the doubling trick to come over this program.",
            "It just to cut time space into intervals of appropriate appropriate length.",
            "Well here it's 2 to the two to the L2L well, and you launch your algorithm independently on each of these epochs and.",
            "Well this is not."
        ],
        [
            "Guys, you don't want to implement it because you need to restart the algorithm at each epochs.",
            "So we have been in."
        ],
        [
            "Interested in a second algorithm which is UCB tuned policy?",
            "And this algorithm is.",
            "OK, is quite similar to the previous one, but what changes so you should look at this confidence term level terms.",
            "What changes is.",
            "Before we have this bitter S which was just depending on the number of plays of ARM K, so it was independent of the time at which you were the time T. And here you put instead something which so your confidence level.",
            "And what would you need?",
            "For me in equation which holds with highest probability because T goes to Infinity."
        ],
        [
            "And here is the confidence level is basically T -- T to the power minus P. So so policies one small to place.",
            "They are maximizing the bound.",
            "And."
        ],
        [
            "And what you you can prove for.",
            "This policy.",
            "Is this type of bound which is similar to what we have seen?",
            "P is just a parameter, so just takes P = 3 for instance, and what you see is you have constant times logarithmic term.",
            "Times one for this system, so run to the reward press.",
            "Valiance of us is expected.",
            "We regret.",
            "Plus some additional terms which are basically well.",
            "It's 2nd order terms."
        ],
        [
            "OK, Ann, so you can compare it to the bound I've just mentioned, but here we write.",
            "See explicit formula.",
            "So well, it's.",
            "Always better, except when you have in a very particular situation when DD Key.",
            "Is small and.",
            "Sigma K squared.",
            "It is much smaller than Delta key.",
            "But well, and is this bound where assuming certain rewards where normal distributed?"
        ],
        [
            "OK.",
            "So brief view of the proof.",
            "Well, the underlying idea is that.",
            "You have an empirical and upper bound of your expected reward and what you do is to have a theoretical upper bound on your empirical upper bound.",
            "And so this is.",
            "My.",
            "This is how upper bound on how a particular."
        ],
        [
            "Bound and we have this with high probability.",
            "For for any asininity.",
            "And any arm.",
            "And.",
            "OK, and the basic things you do is once more to concentrate on the number of plays."
        ],
        [
            "Love, none of them are arms and zakiya argument.",
            "I mentioned about UCP policy can be found here, while since, since, well, I believe I should skip it.",
            "OK."
        ],
        [
            "So then you you take the expectation and.",
            "Well, basically you you choose.",
            "You end up with what you want.",
            "I don't have time.",
            "I believe 2 today."
        ],
        [
            "So much more.",
            "OK, so to conclude what we propose, its first UCB algorithm which satisfies with high probability that with high probability, the secretive regret after N places is bounded by some constant which has the nice property to be independent from N, and we prove a logarithmic bound and the expected regret of a variant of UCB tune that takes into account variance estimates.",
            "OK, thank you.",
            "Compare.",
            "Is there something like an optimal beta?",
            "OK.",
            "I mean, if you think an expected regret.",
            "Zenda I believe the crucial point is one of our end because you have this traded trade off.",
            "The control of the expected comparative regrets of Zabita UCB policy is of order better N plus logarithm of beta minus one better power minus one.",
            "So the only thing I could say is so critical point is 1 / N and then if you take 1 / N or 1 / N power 10 it just makes a small difference difference into constant.",
            "Yeah yeah, but you should not take his better over the exponential minus and typically awesome.",
            "Yeah, says around of good values, yeah.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Help.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next door is.",
                    "label": 0
                },
                {
                    "sent": "Use a variance estimation in the Northland problem.",
                    "label": 1
                },
                {
                    "sent": "Thank you so this is a joint work with him.",
                    "label": 0
                },
                {
                    "sent": "You know San Saba Sipos very.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the outline.",
                    "label": 0
                },
                {
                    "sent": "First, I will describe the program I'm interesting in, and one particular type of algorithm which solves this program.",
                    "label": 0
                },
                {
                    "sent": "And then I will turn to two proposition of UCB policy which use variance estimates in it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us first describe the program so it is a multi armed bandit problem.",
                    "label": 0
                },
                {
                    "sent": "You assume that you are in front of the machine which have many arms and at each time step you have to draw an arm and disarm will provide you a reward and your goal is just to maximize your competitive rewards reward.",
                    "label": 1
                },
                {
                    "sent": "And the key assumptions.",
                    "label": 1
                },
                {
                    "sent": "It's at successive place of an arm.",
                    "label": 0
                },
                {
                    "sent": "I did realizations.",
                    "label": 0
                },
                {
                    "sent": "Of some unknown distribution so that you can see an arm.",
                    "label": 0
                },
                {
                    "sent": "Through its distribution through its reward distribution.",
                    "label": 1
                },
                {
                    "sent": "And we assume that the rewards of different arms are independent and, well, we also assume that so we both are bounded to simplifies.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "So let us fix the notation in order to see the quantities which are involved.",
                    "label": 0
                },
                {
                    "sent": "So first the number of arms arms will be capital K. So and X Katie is not the reward of RK at time T. It is the reward of RMK when drone for that if time.",
                    "label": 1
                },
                {
                    "sent": "So this is an important point.",
                    "label": 0
                },
                {
                    "sent": "And well, since we have assumed that the rewards are bounded, you without loss of generality, you can say that the bound to zero and one.",
                    "label": 1
                },
                {
                    "sent": "And the distribution of the reward of MK is characterized by its expectation.",
                    "label": 1
                },
                {
                    "sent": "Mutai and its variants, Sigmascape Square, so optimal arm is denoted K star, and the expected reward of K star is simply denoted new star.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Deltacare is expected regret of K, so it is the difference between the optimal.",
                    "label": 1
                },
                {
                    "sent": "Reward expected reward and the expected reward of K. And once you have drawn teetimes.",
                    "label": 0
                },
                {
                    "sent": "K What you can observe is the empirical means of your rewards and the empirical means is just denoted with a bar above SKT.",
                    "label": 0
                },
                {
                    "sent": "Is there empirical variance is noted the key KT and policy is just a way of choosing the next arm.",
                    "label": 1
                },
                {
                    "sent": "Based on past place and obtain rewards.",
                    "label": 0
                },
                {
                    "sent": "And then I did not I see the the unplayed by the policy at time T an maybe an important notation is this one TKT which is the number of times K is chosen by the policy during the first place.",
                    "label": 1
                },
                {
                    "sent": "And we define the cumulative regret or cumulative.",
                    "label": 0
                },
                {
                    "sent": "So the regret by the sum of the arms of the number of times the arm is played times the expected regret of the arm.",
                    "label": 0
                },
                {
                    "sent": "OK, and we are interested in having a bound and this communitive regret and also we are interested in having a bound on its expected value.",
                    "label": 0
                },
                {
                    "sent": "So it's expected value, which is the sum of the arms of the expected number of times the arm has been drawn.",
                    "label": 0
                },
                {
                    "sent": "Times the expected regret.",
                    "label": 0
                },
                {
                    "sent": "And one can see also this quantity this way in which you have any times the optimal expected reward minus the expectation of what?",
                    "label": 0
                },
                {
                    "sent": "What happens under some of what happens at anytime?",
                    "label": 0
                },
                {
                    "sent": "So well, it seems complicated, but this X, ITT, ITT is just reward you get at time T because at time T you draw the arm it.",
                    "label": 0
                },
                {
                    "sent": "And the number of times you have drawn the arm, IT is just TITT.",
                    "label": 0
                },
                {
                    "sent": "Under 50 time steps.",
                    "label": 0
                },
                {
                    "sent": "This is different than there again I was using because it's yes, yes, yeah that's true and that's why this balance will be really different, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well, here I did not put the sum of the rewards you would have obtained if you have drawn more arms and the sum of of rewards is has a division of order square root of N. So you cannot expect bounds which are better than square root of N. If you put this kind of term.",
                    "label": 0
                },
                {
                    "sent": "So yeah, this is a definition of micro motive request and the expected comparative regret is really important.",
                    "label": 0
                },
                {
                    "sent": "It changes things.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so let us down to turn to this family of algorithm which is UC policy and social ID is at at time T. From from pass observation and some probabilistic arguments you can build some bound upper bound on the expected reward of any arm and the policy is just you place the arm having the largest upper bound on your expected reward.",
                    "label": 0
                },
                {
                    "sent": "So first question, it is why does it make sense and the way to address this question is to ask you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, could we stay a long time drawing wrong?",
                    "label": 1
                },
                {
                    "sent": "And in fact you can convince yourself that it is not possible because the more you draw an alarm wrong arm, so closer you're bound gets to the expected reward well.",
                    "label": 0
                },
                {
                    "sent": "Well, assuming that you are taking a correct probabilistic upper bound.",
                    "label": 0
                },
                {
                    "sent": "And and then, since you're the.",
                    "label": 0
                },
                {
                    "sent": "Since the reward to which you converge is strictly smaller than the optimal expected reward, which is also smaller than the upper bound on it.",
                    "label": 0
                },
                {
                    "sent": "Then you see that your upper bound on K will go below.",
                    "label": 0
                },
                {
                    "sent": "This is the upper bound on Seal team.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, we don't even use that.",
                    "label": 0
                },
                {
                    "sent": "Use the upper bound on new star is.",
                    "label": 0
                },
                {
                    "sent": "Close to new stuff which we really used.",
                    "label": 0
                },
                {
                    "sent": "It sets the upper bound.",
                    "label": 0
                },
                {
                    "sent": "On under one arm will goes to new K If you draw it too much.",
                    "label": 0
                },
                {
                    "sent": "And that's a key.",
                    "label": 0
                },
                {
                    "sent": "That's a key point.",
                    "label": 0
                },
                {
                    "sent": "And then yeah.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will return to this maybe later.",
                    "label": 0
                },
                {
                    "sent": "So the 1st police here we introduce is the beta UCB policy and it is basically based on Bernstein type inequality.",
                    "label": 0
                },
                {
                    "sent": "So let us remind what is Bernstein inequality is just?",
                    "label": 0
                },
                {
                    "sent": "Well, some kind of know that synthetic version of Central Limit Theorem.",
                    "label": 0
                },
                {
                    "sent": "So it just says that the empirical mean is close to the true mean and well closed up to some terms in which typically you have standard deviation over square root of of T. So I'm standing there.",
                    "label": 0
                },
                {
                    "sent": "Quality is a background, so it means it is true with high probability and so confidence level.",
                    "label": 0
                },
                {
                    "sent": "Beta appears for logarithmic.",
                    "label": 0
                },
                {
                    "sent": "OK, and the only modification we do here is.",
                    "label": 0
                },
                {
                    "sent": "Well, you cannot use this to talk about your expected reward.",
                    "label": 0
                },
                {
                    "sent": "Of of Anima tightly because you don't know Sigma and so the only thing you can say well is.",
                    "label": 0
                },
                {
                    "sent": "Sigma is below 1/4 because this is a random variable between text value between zero and one.",
                    "label": 0
                },
                {
                    "sent": "But if you want to say something stronger which will be interested when the variance of the arms are small, then you have to.",
                    "label": 0
                },
                {
                    "sent": "2 upper bounds.",
                    "label": 0
                },
                {
                    "sent": "This is standard deviation with an empirical quantity, and naturally what appears it is empirical standard deviation, which I call S. And to do this, in fact you have to use one small.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ben Stein inequality.",
                    "label": 0
                },
                {
                    "sent": "So the starting point is Bernstein inequality applied with an appropriate confidence level.",
                    "label": 0
                },
                {
                    "sent": "So I just rewrite them mentioning equality here and then to control Sigma you consider the IID random variable XI minus expectation of X squared improving.",
                    "label": 0
                },
                {
                    "sent": "So sorry, it's what is known is based on equality and what is not known is this bench time studying inequality.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Yep, thank you.",
                    "label": 0
                },
                {
                    "sent": "And OK, so to control Sigma.",
                    "label": 0
                },
                {
                    "sent": "You just notice that Sigma Square can be interpreted as the expectation of this.",
                    "label": 0
                },
                {
                    "sent": "This random variable excitement and expectation of X square and you notice that the empirical means of these random variables.",
                    "label": 0
                },
                {
                    "sent": "Can be expressed in terms of the empirical volumes esquerre plus some terms and.",
                    "label": 0
                },
                {
                    "sent": "Well and if you see this then it is over because this term is what you have already controlled here so well, I don't make the computation here, but what you have to solve it polynomial degree two polynomial in equation and so it's solvable and you get the formula announced.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to see that this inequality is true, this equality is true.",
                    "label": 0
                },
                {
                    "sent": "You just think of the random variable W, which takes value XI with probability 1, / T. And this is just a standard variance decomposition where a place this is just a constant and the expectation of the value is X bar by definition of what I call W. OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the sketch of the proof and now I can define the UCB policy.",
                    "label": 0
                },
                {
                    "sent": "And it is based on one parameter beta, which is positive and which represents a confidence level.",
                    "label": 0
                },
                {
                    "sent": "And from this can fitness level.",
                    "label": 0
                },
                {
                    "sent": "You can define sub confidence levels better S and in fact you will use the bench time type inequality I've just mentioned for each arm and for each number of draws.",
                    "label": 0
                },
                {
                    "sent": "Over of the armor, so you have an infinite number of inequalities.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once you have.",
                    "label": 0
                },
                {
                    "sent": "And what we have done here is just to put some weight such that when you sum this is a different confidence level.",
                    "label": 0
                },
                {
                    "sent": "You have something of order better.",
                    "label": 0
                },
                {
                    "sent": "They just say, you know, you know, inbound over infinite numbers of inequality.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then you define your bound which on the expected reward of RMK when you observe SS times rewards coming from ARM K which is just coming from bench time.",
                    "label": 0
                },
                {
                    "sent": "Bernstein's type inequality.",
                    "label": 0
                },
                {
                    "sent": "When we wish we had just inform with one because the rewards are bounded by one, so the expected we want is bounded well by 1.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's why it says before in fact OK. And those wanting to enter into his details that way.",
                    "label": 0
                },
                {
                    "sent": "OK, and the policy is just to play the maximizing your your bound.",
                    "label": 0
                },
                {
                    "sent": "Why is TKT minus one?",
                    "label": 0
                },
                {
                    "sent": "It is when you are at time T it is just a number of times you have drawn ARM K. So you use the bound.",
                    "label": 1
                },
                {
                    "sent": "For this number of times.",
                    "label": 0
                },
                {
                    "sent": "For this number of jobs of amke, OK, this is the policy an.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a nice thing about this policy.",
                    "label": 0
                },
                {
                    "sent": "Is this result which says that with high probability.",
                    "label": 0
                },
                {
                    "sent": "Is your better UCB policy will play?",
                    "label": 0
                },
                {
                    "sent": "Non empty more arms only at most UK times where UK is the smallest integer satisfying this inequality.",
                    "label": 1
                },
                {
                    "sent": "So OK, this inequality is not that nice because.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, appears several times so you can just derive rough about about on it, which has the following term.",
                    "label": 0
                },
                {
                    "sent": "One should see this upper bound by concentrated just on the value K log of beta minus one, and the surprising thing here is that it does not depend on the number of.",
                    "label": 0
                },
                {
                    "sent": "Of the truth is a number of places.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can play infinitely.",
                    "label": 0
                },
                {
                    "sent": "You will never well.",
                    "label": 0
                },
                {
                    "sent": "Never with high priority.",
                    "label": 0
                },
                {
                    "sent": "Place is not empty.",
                    "label": 0
                },
                {
                    "sent": "More arms more than some fixed number which is UK.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from this control of the number of times you play non optimal arms you can get.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the bound on your communitive regret.",
                    "label": 0
                },
                {
                    "sent": "Just buy something, say something's bounds, waited by the suspected regret Delta K. And so.",
                    "label": 0
                },
                {
                    "sent": "For the moment I just gave results with high priority.",
                    "label": 0
                },
                {
                    "sent": "Now we will switch to result with for the expected cumulative regret.",
                    "label": 0
                },
                {
                    "sent": "And to do this, one should understand that when you have results with high probability, you do not control at all what happens and so.",
                    "label": 0
                },
                {
                    "sent": "So the event of probability, bitter and typically what happens here on this event, is that you can have a regret of order N. So if you want to compensate this huge loss because.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is just large loss.",
                    "label": 0
                },
                {
                    "sent": "You need to take beta smaller than one of and that's what we have done precisely.",
                    "label": 0
                },
                {
                    "sent": "We took better equal to 1 / N and then just buy.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "By saying that on the low probability event, the worst we can do is N and the high probability event.",
                    "label": 0
                },
                {
                    "sent": "I have a very tight bound of the of the number of times to draw nonoptimal arms.",
                    "label": 0
                },
                {
                    "sent": "You end up with an upper bound on the expected creative regret.",
                    "label": 0
                },
                {
                    "sent": "Log in.",
                    "label": 0
                },
                {
                    "sent": "No, because it's not useful.",
                    "label": 0
                },
                {
                    "sent": "I mean OK, you can take your log, login over end, but it does not.",
                    "label": 0
                },
                {
                    "sent": "Appears to be useful.",
                    "label": 0
                },
                {
                    "sent": "You just have to write the tradeoff between the two terms.",
                    "label": 0
                },
                {
                    "sent": "One is log better minus one and the other is better times N. So basically it does not change.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can take 1 / N square.",
                    "label": 0
                },
                {
                    "sent": "It won't change basically the bound, it's just a matter of constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so this will search should be read that as you have a logarithmic cumulative regret.",
                    "label": 0
                },
                {
                    "sent": "And you have this summation here.",
                    "label": 0
                },
                {
                    "sent": "In which the nice thing is that there is a variant of ARM K which appears.",
                    "label": 0
                },
                {
                    "sent": "It means that if non optimal arms has low variances then you will you will have a tighter upper bound.",
                    "label": 0
                },
                {
                    "sent": "And here this one should be interpreted as.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah.",
                    "label": 0
                },
                {
                    "sent": "Number of times along with creating this one policy, what is it?",
                    "label": 0
                },
                {
                    "sent": "Optimal arms would be great.",
                    "label": 0
                },
                {
                    "sent": "Well in speaking.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah yeah, you have this summation.",
                    "label": 0
                },
                {
                    "sent": "So forget the summation.",
                    "label": 0
                },
                {
                    "sent": "Divide by Delta.",
                    "label": 0
                },
                {
                    "sent": "Can you obtain it?",
                    "label": 0
                },
                {
                    "sent": "And yeah, and so on here it should be interpreted as a range of the rewards.",
                    "label": 0
                },
                {
                    "sent": "And otherwise with Formula One beer genius.",
                    "label": 0
                },
                {
                    "sent": "You're saying that you have to know an ahead of time.",
                    "label": 0
                },
                {
                    "sent": "Or yeah, I will come to it.",
                    "label": 0
                },
                {
                    "sent": "You are ahead of me.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's compare it.",
                    "label": 0
                },
                {
                    "sent": "To the most simple UCB policy which is UCB one and so basically you have this same logarithmic term, But here this term is smaller will be typically smaller than this one because Delta case always smaller than one.",
                    "label": 0
                },
                {
                    "sent": "So up to two it is this one is always smaller than the nice part is the control of the variance here because it could be.",
                    "label": 0
                },
                {
                    "sent": "This term could be much smaller than this one.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the comparison to the simple.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "List you see people see if we compare it to a variation variant of this policy.",
                    "label": 0
                },
                {
                    "sent": "Then we have typically the same kind of bound.",
                    "label": 1
                },
                {
                    "sent": "But here this bound is a bit better because you don't have the range of the rewards here.",
                    "label": 0
                },
                {
                    "sent": "Here you have instead, so the expected regret, which is which could be much smaller than one.",
                    "label": 1
                },
                {
                    "sent": "But this result is only true for normal distributions.",
                    "label": 0
                },
                {
                    "sent": "And, well, we believe that the fact that the range of the rewards appear here is quite natural.",
                    "label": 0
                },
                {
                    "sent": "Well, we're still arguing about this, but.",
                    "label": 0
                },
                {
                    "sent": "I believe it's quite natural if you don't have.",
                    "label": 0
                },
                {
                    "sent": "How long?",
                    "label": 0
                },
                {
                    "sent": "OK. OK so I. I will go to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main drawback of this policy is that that you need to know the number of plays.",
                    "label": 1
                },
                {
                    "sent": "Well, you have a standard trick, which is the doubling trick to come over this program.",
                    "label": 1
                },
                {
                    "sent": "It just to cut time space into intervals of appropriate appropriate length.",
                    "label": 0
                },
                {
                    "sent": "Well here it's 2 to the two to the L2L well, and you launch your algorithm independently on each of these epochs and.",
                    "label": 0
                },
                {
                    "sent": "Well this is not.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, you don't want to implement it because you need to restart the algorithm at each epochs.",
                    "label": 0
                },
                {
                    "sent": "So we have been in.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interested in a second algorithm which is UCB tuned policy?",
                    "label": 0
                },
                {
                    "sent": "And this algorithm is.",
                    "label": 0
                },
                {
                    "sent": "OK, is quite similar to the previous one, but what changes so you should look at this confidence term level terms.",
                    "label": 0
                },
                {
                    "sent": "What changes is.",
                    "label": 0
                },
                {
                    "sent": "Before we have this bitter S which was just depending on the number of plays of ARM K, so it was independent of the time at which you were the time T. And here you put instead something which so your confidence level.",
                    "label": 1
                },
                {
                    "sent": "And what would you need?",
                    "label": 0
                },
                {
                    "sent": "For me in equation which holds with highest probability because T goes to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the confidence level is basically T -- T to the power minus P. So so policies one small to place.",
                    "label": 0
                },
                {
                    "sent": "They are maximizing the bound.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you you can prove for.",
                    "label": 0
                },
                {
                    "sent": "This policy.",
                    "label": 0
                },
                {
                    "sent": "Is this type of bound which is similar to what we have seen?",
                    "label": 0
                },
                {
                    "sent": "P is just a parameter, so just takes P = 3 for instance, and what you see is you have constant times logarithmic term.",
                    "label": 0
                },
                {
                    "sent": "Times one for this system, so run to the reward press.",
                    "label": 0
                },
                {
                    "sent": "Valiance of us is expected.",
                    "label": 0
                },
                {
                    "sent": "We regret.",
                    "label": 0
                },
                {
                    "sent": "Plus some additional terms which are basically well.",
                    "label": 0
                },
                {
                    "sent": "It's 2nd order terms.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, Ann, so you can compare it to the bound I've just mentioned, but here we write.",
                    "label": 0
                },
                {
                    "sent": "See explicit formula.",
                    "label": 0
                },
                {
                    "sent": "So well, it's.",
                    "label": 0
                },
                {
                    "sent": "Always better, except when you have in a very particular situation when DD Key.",
                    "label": 0
                },
                {
                    "sent": "Is small and.",
                    "label": 0
                },
                {
                    "sent": "Sigma K squared.",
                    "label": 0
                },
                {
                    "sent": "It is much smaller than Delta key.",
                    "label": 0
                },
                {
                    "sent": "But well, and is this bound where assuming certain rewards where normal distributed?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So brief view of the proof.",
                    "label": 1
                },
                {
                    "sent": "Well, the underlying idea is that.",
                    "label": 0
                },
                {
                    "sent": "You have an empirical and upper bound of your expected reward and what you do is to have a theoretical upper bound on your empirical upper bound.",
                    "label": 0
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "My.",
                    "label": 0
                },
                {
                    "sent": "This is how upper bound on how a particular.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bound and we have this with high probability.",
                    "label": 0
                },
                {
                    "sent": "For for any asininity.",
                    "label": 0
                },
                {
                    "sent": "And any arm.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, and the basic things you do is once more to concentrate on the number of plays.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love, none of them are arms and zakiya argument.",
                    "label": 0
                },
                {
                    "sent": "I mentioned about UCP policy can be found here, while since, since, well, I believe I should skip it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then you you take the expectation and.",
                    "label": 0
                },
                {
                    "sent": "Well, basically you you choose.",
                    "label": 0
                },
                {
                    "sent": "You end up with what you want.",
                    "label": 0
                },
                {
                    "sent": "I don't have time.",
                    "label": 0
                },
                {
                    "sent": "I believe 2 today.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So much more.",
                    "label": 0
                },
                {
                    "sent": "OK, so to conclude what we propose, its first UCB algorithm which satisfies with high probability that with high probability, the secretive regret after N places is bounded by some constant which has the nice property to be independent from N, and we prove a logarithmic bound and the expected regret of a variant of UCB tune that takes into account variance estimates.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Compare.",
                    "label": 0
                },
                {
                    "sent": "Is there something like an optimal beta?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you think an expected regret.",
                    "label": 0
                },
                {
                    "sent": "Zenda I believe the crucial point is one of our end because you have this traded trade off.",
                    "label": 0
                },
                {
                    "sent": "The control of the expected comparative regrets of Zabita UCB policy is of order better N plus logarithm of beta minus one better power minus one.",
                    "label": 0
                },
                {
                    "sent": "So the only thing I could say is so critical point is 1 / N and then if you take 1 / N or 1 / N power 10 it just makes a small difference difference into constant.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, but you should not take his better over the exponential minus and typically awesome.",
                    "label": 0
                },
                {
                    "sent": "Yeah, says around of good values, yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}