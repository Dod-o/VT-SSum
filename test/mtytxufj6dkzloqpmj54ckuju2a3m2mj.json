{
    "id": "mtytxufj6dkzloqpmj54ckuju2a3m2mj",
    "title": "The complexity of learning halfspaces using generalized linear methods",
    "info": {
        "author": [
            "Amit Daniely, Einstein Institute of Mathematics, The Hebrew University of Jerusalem"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_daniely_methods/",
    "segmentation": [
        [
            "So we'll talk about binary classification standard binary classification.",
            "We have distribution over X * 0 one and based on example form distribution.",
            "We want to find a good classifier.",
            "So many."
        ],
        [
            "A algorithms to this problem that I will collectively called generalized linear methods can be described as follows.",
            "We take our instance, SpaceX, we embed it into a high dimensional space an in that space we find 1/2 space that minimizes certain convex loss, so."
        ],
        [
            "This description encompassed many metal scanner SVM regression.",
            "484 Miss Methods and others and four."
        ],
        [
            "Theoretical perspective it also provides us the best guarantees.",
            "Too many problems to learning halfspaces, learning DNF's and other problems and domain."
        ],
        [
            "Purpose of this work is to prove lower bounds for these algorithms, and we do that, in particular for the program of learning large mining half faces.",
            "So what is that problem?"
        ],
        [
            "Just remind you the interface will be the unit ball vectors of more math than one.",
            "Ford"
        ],
        [
            ", greater than zero.",
            "We define the gamma error of a hyperspace hyperplane as the probability of an example to fall either at the wrong side of edge or a distance less than gamma from edge.",
            "Add."
        ],
        [
            "Also defined the gamma of the distribution as the gamma of the best hyperplane, and we'd like to find a classifier with L is at most the gamma error of the OK, so this is the purpose."
        ],
        [
            "They go to.",
            "The algorithm will be the margin parameter, gamma and their parameter epsilon an access 2 examples from D and also the the complexity parameters of the former will be one over gamma and one over epsilon.",
            "We will want to be polynomials in these two parameters.",
            "Now, as we will see later, we probably can solve this program exactly by efficient algorithms, so we'll have to compromise on approximation algorithms and."
        ],
        [
            "For Alpha greater than one, we said that algorithm has approximation ratio Alpha.",
            "If it is guaranteed to output a hypothesis with arrow at most Alpha times, the gamma error of the and here the error of the faith is simply the 01 error."
        ],
        [
            "And finally, we we.",
            "We say that learning algorithm is efficient if it runs in time polynomial and uses polynomially many samples.",
            "OK, so."
        ],
        [
            "Learning language model half base is one of the most cited problems in machine learning.",
            "What do we know about this problem?",
            "So let's start with upper bounds."
        ],
        [
            "If we simply use SVM with no kernel and the hinge loss, which is not how to see that we get an approximation of ratio of one over gap, which is quite bad approximation ratio and you can ask whether we can do better.",
            "So yes we can."
        ],
        [
            "If we use FM with a particular cannon, we can do slightly better, and this can also."
        ],
        [
            "We achieved using other methods, so this is the state of the out of the upper bounds.",
            "What about lower bounds?"
        ],
        [
            "So under cryptographic assumption, it is possible to show that there are no exact algorithms and in."
        ],
        [
            "Recent results we show that under certain average case complexity assumption, there are no constant approximation ratio algorithms.",
            "OK, so these are the lower ones and I make two remarks.",
            "First, as you can see, there are very very far from the upper bounds and second they are based on not the most standard complexity assumption like base different from NP.",
            "So we can ask what about lower bounds for concrete algorithms?"
        ],
        [
            "So forth, and we also have a matching lower bound of one over gamma.",
            "If we don't use kernel and the slower ones holds also if you use other convex losses, not just the hinges.",
            "But what about more general families of algorithms like kernels?"
        ],
        [
            "PMO general in our algorithms.",
            "So here the best of my knowledge.",
            "We don't have general result and the."
        ],
        [
            "Result of this work is to show that every efficient generalized linear learner has approximation ratio at least one over gamma up to a logarithmic factor and this matches."
        ],
        [
            "The best known upper bound up to the logarithmic factor, and as I said."
        ],
        [
            "In the beginning, this applies to.",
            "Too many techniques."
        ],
        [
            "So in the remaining time at try to."
        ],
        [
            "Give you a glimpse to the proof.",
            "So the main challenge they technical challenge as we see, is to rule out every possible embedding of our data into a high dimensional space and to do."
        ],
        [
            "So we develop."
        ],
        [
            "Technique and we use many mathematical tools from many formalities.",
            "So I won't have the opportunity to talk about all of them, but I tried to give you some idea.",
            "So let."
        ],
        [
            "Well defined precisely, what do we mean by a generalized linear algorithms?",
            "So we have a Hilbert space when we want to embed our data with the unit ball B."
        ],
        [
            "We fixed an embedding fee for eggs to be Anna convex.",
            "Surrogate L can be any nonnegative convex function you want and see can be any function you want with one technical condition that I have enrolled that she should be absolutely continues."
        ],
        [
            "Now for a vector W in the high dimensional space and biosphere, we do not buy HW be the classifier corresponding to the mapping C and the halfspaces defined by W&B and we do not buy LL the surrogate error of this classifier."
        ],
        [
            "Now account base learner corresponding to C&L minimizes the surrogate error over all vectors of normal less than C of gamma will see of gamma.",
            "Is somebody that can be chosen by the algorithm.",
            "This is another parameter of the algorithm."
        ],
        [
            "And when you're done, tell us that every efficient such learner has approximation ratio at least one over gamma up to this log factor."
        ],
        [
            "In order to prove that we need to show that for every choice of three L and three, there is some bad distribution that fails the algorithm."
        ],
        [
            "OK, so the first step maybe is to give an equivalent definition of kernel based learn also generalized linear algorithms."
        ],
        [
            "So we do not buy F. The Hilbert space consisting of all hypothesis.",
            "The algorithm might return and we end all this Hilbert space with the no induced by the Hilbert said we started right by age.",
            "Now in Cleveland."
        ],
        [
            "Definition of algorithm is an algorithm that minimizes the surrogate error over a certain bowl in that Hilbert space.",
            "OK, so this is just an equivalent definition."
        ],
        [
            "And now, instead of quantifying all possible mapping, see we need to quantify over all possible Hilbert phase spaces of that kind and."
        ],
        [
            "Actually, in the powerful will use both formulations.",
            "Some of the arguments will be easier in the first formulation, some of the argument will be easier in the second, but in the talk we will use only the second formulation.",
            "So let me repeat it, we have a Hilbert space of function from X2 R and we minimize the surrogates error over large bowl in that Hilbert space, and we want to show that this algorithm will perform very bad.",
            "OK."
        ],
        [
            "So now I give you a Birds Eye view of the profit can be decomposed into four step."
        ],
        [
            "Step would be to handle this bound C of gamma.",
            "So refer to that sends the sample is polynomial in one over gamma.",
            "C must be polynomial in one over gamma as well so."
        ],
        [
            "It is a Commonwealth of very, very well known theorem tells that if fees polynomial then we can learn with a polynomial sample complexity and I think that is an testing of its own right and."
        ],
        [
            "OK, so I want prove it and just take that this step is prove using a certain geometric arguments."
        ],
        [
            "The second step is to prove the theorem in a very restricted setting where the instance space is 1 dimensional ball, the figment minus one one an hour Hilbert space is simply the space of polynomials of degree at most log one over gamma.",
            "The second step is."
        ],
        [
            "To reduce all of this restriction and to prove it indicates that.",
            "The interface is the unit ball, but now in D dimensions, but they're space will still be a polynomials of bounded degree and the final."
        ],
        [
            "Step of course, is to deal with the general case.",
            "So in the mean time I'll tell you a bit more about Step 224, so let's start with the."
        ],
        [
            "Dimensional case.",
            "So in order to prove that, erm, I need to show you a distribution with a small.",
            "Gamma ello such that the algorithm will be doing very bad hypothesis.",
            "So here is the distribution will have two very heavy points close to the origin.",
            "Each of these points will have more ability, almost half, and the left one will be negative and the right one will be positive and on the rest of the segment will simply have a uniform.",
            "So this is the distribution.",
            "Now."
        ],
        [
            "First, it is easy to see that the gamma always very good.",
            "It is at most two gamma.",
            "Why?",
            "Because if we look on the half space, which is simply the threshold of zero, it's error is less than two gamma.",
            "And now we show you that the error of the hypothesis returned by the algorithm will be almost half, and therefore the ratio will be one over gamma as we need."
        ],
        [
            "OK, so let's be be the polynomial returned by the algorithm, the 1st."
        ],
        [
            "Point to note is that the L2 norm of this polynomial has to be small and the reason is that otherwise if it's big, if it's more than one over gamma, then the arrow on the noisy part will be very large.",
            "The target error and noisy part will be very large.",
            "Therefore the L2 norm is small.",
            "No."
        ],
        [
            "So since we work with polynomials of very low degree, the fact that the L2 norm is small implies that L Infinity norm of the derivative is small as well."
        ],
        [
            "Therefore, the value of the polynomial on the two heavy points will be approximately the same, and therefore the label will be the same, so the algorithm will have to err on at least one of them and therefore."
        ],
        [
            "There will be approximately half OK, so this is the idea of actual proof."
        ],
        [
            "View A is done using a championship polynomial, so this is certain family orthogonal polynomials."
        ],
        [
            "The next step is to go to the D dimensional ball.",
            "So now, then specify the dimensional ball and still a Hilbert space is bounded degree polynomials.",
            "So the idea here I give left."
        ],
        [
            "This is to take the distribution.",
            "We used it before and lift it to the dimension of.",
            "Also with life the ball into slices.",
            "Each slice will be D -- 1 dimensional distribution on each slice will be uniform and we will have two very heavy slices next to the origin.",
            "And you think?"
        ],
        [
            "Conceptually similar argument, but technically much more involved, we show that the theorem holds for this case as well.",
            "The final say."
        ],
        [
            "Is of course to deal with the general case, and this is done using a new symmetrization technique, so I'll try to give you the idea.",
            "I won't give almost any details.",
            "We do not buy ODI.",
            "The group of linear example trees over Rd.",
            "No."
        ],
        [
            "We say that a Hilbert space is symmetric if the following holds for every function in that Hilbert space.",
            "If we rotate it using a linear lizama tree, then we are left with a function in the table space and the North don't change.",
            "Now."
        ],
        [
            "We construct an construct, an operator that takes a Hilbert space and return a symmetric Hilbert space, and this is intuitively that done by averaging over this group.",
            "We take the Hilbert space movies all over the fear or all over the ball, and we take 13 average OK.",
            "So this is a very.",
            "A course description."
        ],
        [
            "And we show two things.",
            "First, is that the approximation ratio of optimizing over the symmetrized space is at least as good as the approximation ratio of the original space, which means that we reduced to the case that the Hilbert space is symmetric and last."
        ],
        [
            "We show that the mental space almost coincides with low degree polynomials, and this again."
        ],
        [
            "When is done using presentation tier, we over the sphere and harmonic analysis over the ship, so I won't get into the details."
        ],
        [
            "So let's summarize.",
            "So we proved lower bounds on a particular family of algorithms an I think that such lower bounds are interesting for various reasons.",
            "The first reason is that such overruns tell us that if we want better guarantees, we have to use other metal, so I."
        ],
        [
            "And ask you to prove upper bounds for other methods.",
            "Second, if the family is general enough, then we might hope or think.",
            "But these are ones holds for every algorithm, so I can."
        ],
        [
            "Ask you to prove lower bounds for general algorithms and."
        ],
        [
            "Use this opportunity to advertise another work of the same authors.",
            "Yeah, well we developed a new technique to do that, so till now proving hardness results for learning was quite hard cause the issue of improper learning.",
            "And here we have a new technique that also applies to this problem where we prove.",
            "That there are no constant approximation ratio algorithms."
        ],
        [
            "And two other."
        ],
        [
            "Quite natural question is to use this technique, which is quite general to prove lower bounds for other problems and to prove all ones for the same problems.",
            "But for more general families of algorithms.",
            "So I think that all the open questions are reachable.",
            "And they."
        ],
        [
            "And thank you and take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll talk about binary classification standard binary classification.",
                    "label": 0
                },
                {
                    "sent": "We have distribution over X * 0 one and based on example form distribution.",
                    "label": 1
                },
                {
                    "sent": "We want to find a good classifier.",
                    "label": 0
                },
                {
                    "sent": "So many.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A algorithms to this problem that I will collectively called generalized linear methods can be described as follows.",
                    "label": 0
                },
                {
                    "sent": "We take our instance, SpaceX, we embed it into a high dimensional space an in that space we find 1/2 space that minimizes certain convex loss, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This description encompassed many metal scanner SVM regression.",
                    "label": 0
                },
                {
                    "sent": "484 Miss Methods and others and four.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theoretical perspective it also provides us the best guarantees.",
                    "label": 0
                },
                {
                    "sent": "Too many problems to learning halfspaces, learning DNF's and other problems and domain.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Purpose of this work is to prove lower bounds for these algorithms, and we do that, in particular for the program of learning large mining half faces.",
                    "label": 0
                },
                {
                    "sent": "So what is that problem?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just remind you the interface will be the unit ball vectors of more math than one.",
                    "label": 0
                },
                {
                    "sent": "Ford",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": ", greater than zero.",
                    "label": 0
                },
                {
                    "sent": "We define the gamma error of a hyperspace hyperplane as the probability of an example to fall either at the wrong side of edge or a distance less than gamma from edge.",
                    "label": 1
                },
                {
                    "sent": "Add.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also defined the gamma of the distribution as the gamma of the best hyperplane, and we'd like to find a classifier with L is at most the gamma error of the OK, so this is the purpose.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They go to.",
                    "label": 0
                },
                {
                    "sent": "The algorithm will be the margin parameter, gamma and their parameter epsilon an access 2 examples from D and also the the complexity parameters of the former will be one over gamma and one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "We will want to be polynomials in these two parameters.",
                    "label": 0
                },
                {
                    "sent": "Now, as we will see later, we probably can solve this program exactly by efficient algorithms, so we'll have to compromise on approximation algorithms and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For Alpha greater than one, we said that algorithm has approximation ratio Alpha.",
                    "label": 0
                },
                {
                    "sent": "If it is guaranteed to output a hypothesis with arrow at most Alpha times, the gamma error of the and here the error of the faith is simply the 01 error.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, we we.",
                    "label": 0
                },
                {
                    "sent": "We say that learning algorithm is efficient if it runs in time polynomial and uses polynomially many samples.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning language model half base is one of the most cited problems in machine learning.",
                    "label": 0
                },
                {
                    "sent": "What do we know about this problem?",
                    "label": 0
                },
                {
                    "sent": "So let's start with upper bounds.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we simply use SVM with no kernel and the hinge loss, which is not how to see that we get an approximation of ratio of one over gap, which is quite bad approximation ratio and you can ask whether we can do better.",
                    "label": 0
                },
                {
                    "sent": "So yes we can.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we use FM with a particular cannon, we can do slightly better, and this can also.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We achieved using other methods, so this is the state of the out of the upper bounds.",
                    "label": 0
                },
                {
                    "sent": "What about lower bounds?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So under cryptographic assumption, it is possible to show that there are no exact algorithms and in.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recent results we show that under certain average case complexity assumption, there are no constant approximation ratio algorithms.",
                    "label": 1
                },
                {
                    "sent": "OK, so these are the lower ones and I make two remarks.",
                    "label": 0
                },
                {
                    "sent": "First, as you can see, there are very very far from the upper bounds and second they are based on not the most standard complexity assumption like base different from NP.",
                    "label": 1
                },
                {
                    "sent": "So we can ask what about lower bounds for concrete algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So forth, and we also have a matching lower bound of one over gamma.",
                    "label": 0
                },
                {
                    "sent": "If we don't use kernel and the slower ones holds also if you use other convex losses, not just the hinges.",
                    "label": 0
                },
                {
                    "sent": "But what about more general families of algorithms like kernels?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "PMO general in our algorithms.",
                    "label": 0
                },
                {
                    "sent": "So here the best of my knowledge.",
                    "label": 0
                },
                {
                    "sent": "We don't have general result and the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result of this work is to show that every efficient generalized linear learner has approximation ratio at least one over gamma up to a logarithmic factor and this matches.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The best known upper bound up to the logarithmic factor, and as I said.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the beginning, this applies to.",
                    "label": 0
                },
                {
                    "sent": "Too many techniques.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the remaining time at try to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give you a glimpse to the proof.",
                    "label": 0
                },
                {
                    "sent": "So the main challenge they technical challenge as we see, is to rule out every possible embedding of our data into a high dimensional space and to do.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we develop.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Technique and we use many mathematical tools from many formalities.",
                    "label": 0
                },
                {
                    "sent": "So I won't have the opportunity to talk about all of them, but I tried to give you some idea.",
                    "label": 0
                },
                {
                    "sent": "So let.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well defined precisely, what do we mean by a generalized linear algorithms?",
                    "label": 0
                },
                {
                    "sent": "So we have a Hilbert space when we want to embed our data with the unit ball B.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We fixed an embedding fee for eggs to be Anna convex.",
                    "label": 0
                },
                {
                    "sent": "Surrogate L can be any nonnegative convex function you want and see can be any function you want with one technical condition that I have enrolled that she should be absolutely continues.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for a vector W in the high dimensional space and biosphere, we do not buy HW be the classifier corresponding to the mapping C and the halfspaces defined by W&B and we do not buy LL the surrogate error of this classifier.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now account base learner corresponding to C&L minimizes the surrogate error over all vectors of normal less than C of gamma will see of gamma.",
                    "label": 0
                },
                {
                    "sent": "Is somebody that can be chosen by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is another parameter of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you're done, tell us that every efficient such learner has approximation ratio at least one over gamma up to this log factor.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to prove that we need to show that for every choice of three L and three, there is some bad distribution that fails the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first step maybe is to give an equivalent definition of kernel based learn also generalized linear algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we do not buy F. The Hilbert space consisting of all hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The algorithm might return and we end all this Hilbert space with the no induced by the Hilbert said we started right by age.",
                    "label": 1
                },
                {
                    "sent": "Now in Cleveland.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definition of algorithm is an algorithm that minimizes the surrogate error over a certain bowl in that Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just an equivalent definition.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now, instead of quantifying all possible mapping, see we need to quantify over all possible Hilbert phase spaces of that kind and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, in the powerful will use both formulations.",
                    "label": 0
                },
                {
                    "sent": "Some of the arguments will be easier in the first formulation, some of the argument will be easier in the second, but in the talk we will use only the second formulation.",
                    "label": 0
                },
                {
                    "sent": "So let me repeat it, we have a Hilbert space of function from X2 R and we minimize the surrogates error over large bowl in that Hilbert space, and we want to show that this algorithm will perform very bad.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I give you a Birds Eye view of the profit can be decomposed into four step.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step would be to handle this bound C of gamma.",
                    "label": 0
                },
                {
                    "sent": "So refer to that sends the sample is polynomial in one over gamma.",
                    "label": 0
                },
                {
                    "sent": "C must be polynomial in one over gamma as well so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is a Commonwealth of very, very well known theorem tells that if fees polynomial then we can learn with a polynomial sample complexity and I think that is an testing of its own right and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I want prove it and just take that this step is prove using a certain geometric arguments.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second step is to prove the theorem in a very restricted setting where the instance space is 1 dimensional ball, the figment minus one one an hour Hilbert space is simply the space of polynomials of degree at most log one over gamma.",
                    "label": 0
                },
                {
                    "sent": "The second step is.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To reduce all of this restriction and to prove it indicates that.",
                    "label": 0
                },
                {
                    "sent": "The interface is the unit ball, but now in D dimensions, but they're space will still be a polynomials of bounded degree and the final.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step of course, is to deal with the general case.",
                    "label": 0
                },
                {
                    "sent": "So in the mean time I'll tell you a bit more about Step 224, so let's start with the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensional case.",
                    "label": 0
                },
                {
                    "sent": "So in order to prove that, erm, I need to show you a distribution with a small.",
                    "label": 0
                },
                {
                    "sent": "Gamma ello such that the algorithm will be doing very bad hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So here is the distribution will have two very heavy points close to the origin.",
                    "label": 0
                },
                {
                    "sent": "Each of these points will have more ability, almost half, and the left one will be negative and the right one will be positive and on the rest of the segment will simply have a uniform.",
                    "label": 0
                },
                {
                    "sent": "So this is the distribution.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, it is easy to see that the gamma always very good.",
                    "label": 0
                },
                {
                    "sent": "It is at most two gamma.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if we look on the half space, which is simply the threshold of zero, it's error is less than two gamma.",
                    "label": 0
                },
                {
                    "sent": "And now we show you that the error of the hypothesis returned by the algorithm will be almost half, and therefore the ratio will be one over gamma as we need.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's be be the polynomial returned by the algorithm, the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point to note is that the L2 norm of this polynomial has to be small and the reason is that otherwise if it's big, if it's more than one over gamma, then the arrow on the noisy part will be very large.",
                    "label": 0
                },
                {
                    "sent": "The target error and noisy part will be very large.",
                    "label": 0
                },
                {
                    "sent": "Therefore the L2 norm is small.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So since we work with polynomials of very low degree, the fact that the L2 norm is small implies that L Infinity norm of the derivative is small as well.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, the value of the polynomial on the two heavy points will be approximately the same, and therefore the label will be the same, so the algorithm will have to err on at least one of them and therefore.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There will be approximately half OK, so this is the idea of actual proof.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "View A is done using a championship polynomial, so this is certain family orthogonal polynomials.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next step is to go to the D dimensional ball.",
                    "label": 1
                },
                {
                    "sent": "So now, then specify the dimensional ball and still a Hilbert space is bounded degree polynomials.",
                    "label": 0
                },
                {
                    "sent": "So the idea here I give left.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is to take the distribution.",
                    "label": 0
                },
                {
                    "sent": "We used it before and lift it to the dimension of.",
                    "label": 1
                },
                {
                    "sent": "Also with life the ball into slices.",
                    "label": 0
                },
                {
                    "sent": "Each slice will be D -- 1 dimensional distribution on each slice will be uniform and we will have two very heavy slices next to the origin.",
                    "label": 0
                },
                {
                    "sent": "And you think?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conceptually similar argument, but technically much more involved, we show that the theorem holds for this case as well.",
                    "label": 0
                },
                {
                    "sent": "The final say.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is of course to deal with the general case, and this is done using a new symmetrization technique, so I'll try to give you the idea.",
                    "label": 0
                },
                {
                    "sent": "I won't give almost any details.",
                    "label": 0
                },
                {
                    "sent": "We do not buy ODI.",
                    "label": 0
                },
                {
                    "sent": "The group of linear example trees over Rd.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We say that a Hilbert space is symmetric if the following holds for every function in that Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "If we rotate it using a linear lizama tree, then we are left with a function in the table space and the North don't change.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We construct an construct, an operator that takes a Hilbert space and return a symmetric Hilbert space, and this is intuitively that done by averaging over this group.",
                    "label": 1
                },
                {
                    "sent": "We take the Hilbert space movies all over the fear or all over the ball, and we take 13 average OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a very.",
                    "label": 0
                },
                {
                    "sent": "A course description.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we show two things.",
                    "label": 0
                },
                {
                    "sent": "First, is that the approximation ratio of optimizing over the symmetrized space is at least as good as the approximation ratio of the original space, which means that we reduced to the case that the Hilbert space is symmetric and last.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We show that the mental space almost coincides with low degree polynomials, and this again.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When is done using presentation tier, we over the sphere and harmonic analysis over the ship, so I won't get into the details.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's summarize.",
                    "label": 0
                },
                {
                    "sent": "So we proved lower bounds on a particular family of algorithms an I think that such lower bounds are interesting for various reasons.",
                    "label": 0
                },
                {
                    "sent": "The first reason is that such overruns tell us that if we want better guarantees, we have to use other metal, so I.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And ask you to prove upper bounds for other methods.",
                    "label": 0
                },
                {
                    "sent": "Second, if the family is general enough, then we might hope or think.",
                    "label": 0
                },
                {
                    "sent": "But these are ones holds for every algorithm, so I can.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask you to prove lower bounds for general algorithms and.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use this opportunity to advertise another work of the same authors.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well we developed a new technique to do that, so till now proving hardness results for learning was quite hard cause the issue of improper learning.",
                    "label": 1
                },
                {
                    "sent": "And here we have a new technique that also applies to this problem where we prove.",
                    "label": 0
                },
                {
                    "sent": "That there are no constant approximation ratio algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And two other.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite natural question is to use this technique, which is quite general to prove lower bounds for other problems and to prove all ones for the same problems.",
                    "label": 1
                },
                {
                    "sent": "But for more general families of algorithms.",
                    "label": 1
                },
                {
                    "sent": "So I think that all the open questions are reachable.",
                    "label": 0
                },
                {
                    "sent": "And they.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And thank you and take questions.",
                    "label": 0
                }
            ]
        }
    }
}