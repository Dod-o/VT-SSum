{
    "id": "jp2qwbmbb53u2fgmto6f4n3mge4vw2ph",
    "title": "Gaussian Process: Practical Course",
    "info": {
        "author": [
            "Dilan G\u00f6r\u00fcr, Yahoo! Research",
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/mlss2012_gorur_gaussian_practical/",
    "segmentation": [
        [
            "So as I said, we will be using this tool box written by Caroline Tennis.",
            "This is the website for the toolbox.",
            "And it runs both on Octave and Matlab.",
            "And basically this this tool box came out of the collection of code which was originally used to demonstrate the main algorithms from this book that John also referred to.",
            "So one important thing is that I think you will like this book is freely available for download from the books web page.",
            "So basically Gaussian processes that are slash GPML after the course when you want to learn more about the.",
            "GPS you want.",
            "You can download it for free.",
            "OK."
        ],
        [
            "So the toolbox.",
            "Has a flexible framework for specifying Gaussian process models.",
            "You can use different mean functions and different covariance functions and the combinations of these so and it has a modular design that is the allows to extend the existing libraries as well.",
            "So if you want to develop further code based on this toolbox.",
            "There are several different life functions that are available, like Gaussian likelihood for regression, for example or the plus for regression or cumulative logistic for classification and so on.",
            "I'll go into more details of these later, and similarly there are several inference methods available, also implemented in the toolbox.",
            "Exact inference for GPS is only possible for the regression case, and this is implemented in the toolbox.",
            "And their approximation techniques that are used for either classification or regression.",
            "You can use them so these are all the algorithms that are implemented.",
            "The laptops well plus approximation, expectation propagation and variational Bayes."
        ],
        [
            "So the toolbox has one function called GP, which you call and that does everything you want to do, so you call with different variables different number of variables tested what the function is going to do for you, whether you're just going to evaluate the.",
            "Log marginal likelihood, for example, or whether you're going to optimize the hyperparameters.",
            "Whether you're going to test some test points and so on.",
            "So everything is done seamlessly within this GP function.",
            "But of course you'll need to format your inputs.",
            "That that will be suitable for that will be compatible with that function and it is going to somehow tell the function what you want it to do.",
            "So this is called.",
            "The function is called GP and there are a number of supporting structures and functions.",
            "As I said, there are different inference methods and these infinite inference methods are going to be specified also into a as an input to the GP function.",
            "So the inference methods will be forgiven.",
            "Model specification and data set they're going to.",
            "Run the specified inference technique on the GP model that you specify with the.",
            "With the parameters, the hyperparameters, it computes the approximate posterior and approximate negative log marginal likelihood and its partial derivatives with respect to the hyperparameters if they are required as outputs.",
            "So what are the hyperparameters?",
            "Hyperparameters are basically given as one single struct structure with three fields controlling the properties of the model.",
            "One field is the likelihood function, so you specify which likelihood to use in the hyperparameter structure.",
            "The other is the mean function, and the other is covariance function.",
            "So you specify everything about the model specifics using the hyperparameter structure."
        ],
        [
            "Specifying model parameters.",
            "So what are the likelihood functions so the likelihood function that we pass to the model specifies the form of the likelihood of the Gaussian process model and computes the terms needed for prediction and inference.",
            "OK, so depending on whether you want to do classification or regression, for example, the likelihood function is going to vary and depending on what you think the best way of classifying the data, for example, is, the likelihood function is going to vary the mean function.",
            "As the name indicates is, it is basically specifying the prior mean for the GP, so it's a salary specifying the mean, and it computes the mean and it's derivatives with respect to the part of the hyperparameters pertaining to the mean.",
            "OK, similarly covariance function is a cell array specifying the GPU covariance function, and it computes the covariance and it's there.",
            "It is with respect to the part of the hyperparameters pertaining to the covariance function."
        ],
        [
            "And then there are several inference techniques that you can use.",
            "As I said, exact inference is possible for Gaussian.",
            "Process is for the regression case, that is, when we have Gaussian likelihoods.",
            "We can, if, in the absence of this, or if you want to try out, for example, if we want to compare how a particular approximate method compares to the exact output, we can use one of the approximation methods.",
            "So let let's approximation basically approximates the posterior by a Gaussian that is centered at its mode, and it matches its curvature.",
            "So it basically approximates the posterior by Gaussian that is fit to the.",
            "Posterior density.",
            "And it is applicable for differentiable likelihoods.",
            "Because it's using the curvature, so expectation propagation is a different approximation algorithm, and it is also doing posterior Gaussian matching.",
            "But instead of fitting the model and the matching the curvature, it does this by moment matching OK and then the.",
            "Less algorithm I'll talk about is the variational Bayes which constructs the joint lower bounds.",
            "Sorry for the typo on the marginal likelihood based on individual lower bounds to every likelihood function.",
            "So basically the variational Bayes works as an optimization algorithm that gradually updates the lower bound to have the as tight lower bound as possible.",
            "It is easy to provide a custom covariance matrix or functions with these tools.",
            "If they, what do you mean by custom?",
            "Like you specify the functional form?",
            "Do you mean you want to specify everything about this?",
            "Already exists in the toolbox is not no.",
            "Yeah yeah.",
            "I think.",
            "I mean I haven't played around with that myself, but I think it is fairly easy.",
            "I mean, the only thing you would need to so if you look at the toolbox folders everything is sorted neatly under for under different folders I covariance functions are also sorted there and then.",
            "All you would need to do is write out what you want to specify in your covariance function.",
            "Was there something else?",
            "Actually, really easy."
        ],
        [
            "OK, so a bit more detail about the likelihood functions that are implemented, so these are the names of the functions.",
            "So basically this is the Gaussian function.",
            "The likelihood function is implemented.",
            "The squared exponential Laplace students T desire for regression and for classification the error function and logistic function are implemented as the.",
            "Like build functions so you can.",
            "You can look up a table like this which is available also in the documentation of the toolbox to decide on which functions you use.",
            "And again you could decide to choose something of your own too."
        ],
        [
            "This is a compatibility of metrics of the likelihood functions versus the inference algorithms to make it easier for you to get the idea of what you can use together with what.",
            "So again, as I said, this column is the likelihood functions and these are the inference methods, so exact inference is only possible for the Gaussian likelihood for regression.",
            "The squared exponential Laplacian students the answer.",
            "Also EP is available for all these methods except for students T. The reason being.",
            "I think the students tease form doesn't allow the EP to be stable when it's updating the moments when it's trying to do moment matching.",
            "That's why this is not available for student T and LA Plus is available for all but Laplacian, which sounds a little funny.",
            "But the reason being Laplace requires continuous derivatives and laplacian's there were two is not continuous at the.",
            "Not an variational.",
            "Bayes is basically available for all the likelihood functions.",
            "Again, these two functions that and are for for profit and logistic regression, which means for classification."
        ],
        [
            "So you can decide to have a zero mean or mean equals one or mean equals some constant.",
            "Mean being linear with some slope parameter basically.",
            "Oh, or you can have composite mean functions, so scale I mean sum sum of 2 means or more than one mean product of means powers of the specified mean functions and masks basically.",
            "Restricted on some components."
        ],
        [
            "And there is a long list of covariance functions.",
            "So that you can try out.",
            "I'm not trying.",
            "I'm not going to try to read all these two.",
            "These are all in the documentation so you can look up what are the covariance functions that are available for you to play around with.",
            "But as you can see from the list there, there there are no."
        ],
        [
            "As plus if this is not."
        ],
        [
            "Enough, then you can still do composite covariance functions by scaling them, summing, taking the product masking, and adding and so on."
        ],
        [
            "OK, so I'll first.",
            "Run in matlab.",
            "The demos and then I'll go on to explain the parts of the code.",
            "So when I'm doing this, you can also do this on your machines just to get an idea and you can raise your hands to like ask questions if you don't understand the specific part of the.",
            "Demo so basically.",
            "The first part.",
            "So it's the mean function, the covariance function they like that function, so this is the regression demo which is called demo regression that M on the document under documents folder I think.",
            "So we are now generating data.",
            "From the model that we specified and the data that we generate looks like this.",
            "OK, so we specified.",
            "We specified the mean function, which was a sum of 2 means a linear mean and a constant mean.",
            "And then we we specified some hyperparameter values.",
            "Basically telling the slope and slope of this linear thing and the custom to add to it.",
            "And then there is a covariance function which we specify as the maternal function set, set its parameters and so on, and likelihood function is Gaussian likelihood.",
            "Then we decided to have 20 data points and we generated data using this function.",
            "Basically, this is the output of at the points X and we plotted that OK, so this is what we get so.",
            "Now when we do inference.",
            "What we get is.",
            "This fit to the data points, so remember the data points are these, so the pluses are exactly at the same location, says where the data points are that we generated.",
            "The Gray area is, as you can imagine, is the variance of the OR the standard deviation of the predictive likelihood.",
            "So basically we see that and the blue curve is the mean of the GP.",
            "OK, so the way we did we plotted this was to generate a grid of points in this graph and we evaluated we took the.",
            "Predictive likelihood all around this graph, and then we show the.",
            "Variance and the mean.",
            "So if.",
            "If we now want to learn a different model.",
            "On the same data.",
            "Basically, specifying a different covariance function this time squared exponential covariance.",
            "So we specify again the hyperparameters of these covariance and the hyperparameters of the likelihood.",
            "Um?",
            "And then we call function minimize, which is a function to minimize.",
            "So we're basically so function to optimize the hyperparameters.",
            "OK, so we call this function.",
            "What happens is we get the.",
            "We get the fit for the.",
            "Later.",
            "Which looks like this now.",
            "So this is, this is what the model fit looks like when you optimize the parameters of this card.",
            "Exponential kernel.",
            "OK, so this this kernel is the.",
            "So this GP is using zero mean and.",
            "Squared exponential is the covariance.",
            "OK, so let's see what happens when we have a nonzero mean.",
            "So this is a fit that we get when we have a nonzero mean.",
            "So because when we have nonzero mean we have more flexibility to model the data, you'll see that this is kind of a better fit.",
            "There is less.",
            "Various here less uncertainty.",
            "OK, so.",
            "That's basically how Gaussian process regression works.",
            "OK, so this is actually what we did.",
            "We have.",
            "We have the GP function which outputs variable number of outputs and input it takes as input these.",
            "Arguments none, not all of them need to be specified.",
            "As I said, depending on what you want to do with the function, you specify more or less number of inputs.",
            "So the first argument is the column vector of hyperparameters.",
            "The second is the function specifying the inference method.",
            "3rd is the prior covariance function.",
            "Forces the prior mean function.",
            "And this is the likelihood function.",
            "And then we have the inputs.",
            "So the training inputs the training targets, the test inputs and the test targets.",
            "OK."
        ],
        [
            "And these are the possible outputs.",
            "So basically the negative log marginal likelihood is the first one.",
            "This is the column vector of partial derivatives of the negative log marginal likelihood with respect to each hyperparameter.",
            "It is returned only if you ask for it or it is computed only if you ask for it.",
            "Column vector of predictive outcome predicted output means predictive output variance is predictive.",
            "Latent means predictive latent variances lock predicted probabilities and finally as structure representation of the approximate posterior.",
            "And depending on whether you're training or in prediction mode, it's either the sword output order successful, so these are all documentation from the code, so you don't need to actually have these in mind.",
            "You can always look it up from the."
        ],
        [
            "GP function OK so.",
            "This is a very high level view of what the GP function does.",
            "So.",
            "We first have this deep function half part with the green lines that I showed you before and then it does initializations and then it does inference and depending on whether you specify test cases or not, it either just reports the log marginal likelihood and their derivatives and the posterior or it computes test predictions.",
            "So this is this is a very compact power level of what the function is doing."
        ],
        [
            "And this is a bit more detailed version of what I showed you.",
            "So this is basically processing the input argument, so you don't need to worry about the details if you don't want to change the function itself, but.",
            "Again, it's well documented, is well commented.",
            "You can go to the code and read it out and see what it's doing.",
            "Basically it's checking to see whether everything is in the proper format, that it should be.",
            "It's initializing the things to their default values if they're not given, and if there are some default values, and so on."
        ],
        [
            "And then.",
            "Again, the next part basically checks on initialize, initializes the hyperparameters.",
            "So we check that the size of the hyper parameters that are supplied in HYP match the expectations, so makes a match what is expected of the of covariance and mean functions and the likelihood function.",
            "So each of these.",
            "Mean covariance and likelihoods are checked separately and they they are empty and entries are determined if they don't set the default values.",
            "If they are not specified basically."
        ],
        [
            "OK, and then finally we do inference.",
            "So this part basically may issue a warning if it fails in training mode and it tries to recover because sometimes because of numerical instabilities the training can just fail just like that and it doesn't have.",
            "Maybe sometimes it doesn't have to do with the fact that you specify something wrong or something so it tries to recover from that it it fails.",
            "During training mode, but if it fails during testing mode then it's issues and error.",
            "OK, so.",
            "Basically.",
            "In inference, you call this function if you're computing the marginal likelihood, then sorry, you're computing the marginal likelihood and is servatus only if it's necessary, so only if you are in the prediction case.",
            "Otherwise, to simply do inference and return the posterior.",
            "And if you're doing, if you specify some test points as well, then you can run it with this.",
            "So if is the inference function as you can imagine."
        ],
        [
            "OK, so.",
            "The test predictions.",
            "So one important thing to keep in mind for the test predictions is that predictions are done in mini batches.",
            "So even if you have a large number of test cases just to not run out of memory, not to overwhelm the memory, the program automatically puts them into smaller batches and does batch testing on those mini batches OK.",
            "So basically this part of the codes handles the sparse representations if it's applicable.",
            "So if you gave some sparse form of inputs to the data, it computes the necessary matrices like the the covariance matrices, or the inverse covariances, and so on.",
            "If they're not provided, it sets the mini batch size, it allocates memory, it makes predictions and assigns outputs."
        ],
        [
            "And to make predictions basically for all test points in a mini batch, this is all you do to the predictions.",
            "So evaluate the South variance, the cross clearances, the predictive means and then use it rollerski decomposition for for the AL matrix.",
            "Compute the predictive variances and.",
            "Yeah, here computer predictive variances remove the numerical noise and output."
        ],
        [
            "OK, and.",
            "As I said, there are different inference techniques available and exact inference is available only for regression.",
            "It is possible only for regression and this is the only code you need to do to get the exact inference running.",
            "So basically it's simple matrix algebra.",
            "You need to just hard code like code it down, code down the matrix algebra and you have your inference technique.",
            "This is in fact M. If you want to look it up.",
            "And."
        ],
        [
            "Just a little word about the marginal likelihood versus cross validation.",
            "Basically this is the marginal likelihood is basically summing over the over the data cases to give the predictive like dude.",
            "And this is the leave one out cross validation likelihood or leave one out like it which basically leaves out one of the data points and evaluates this.",
            "Same function basically so you can think of the marginal likelihood as giving the probability of the data given the model assumptions and.",
            "The live on Earth Cross validation as giving an estimate of the predictive log probability regardless of the model assumptions being fulfilled.",
            "So if you're more worried about your predictive.",
            "Performance then you may be better off using the leave one out cross validation simply because it's more robust against models specification, and that's obvious from the fact that it gives an estimate regardless of the model assumptions being fulfilled or not.",
            "Whereas if you if you really want to have, like if you really believe in your model structure or if you want to have some good.",
            "Model representing your data you're not too worried about predictive performance.",
            "Then you would be better off using the marginal likelihood.",
            "I also have a demo for classification, again from the documentation so I didn't change much from that.",
            "Basically similar to.",
            "Regression we first generate some data.",
            "Some data like this so the Reds pluses are of one class and the blue pluses are of another class.",
            "You see that this is a non separable case.",
            "So.",
            "This is a contour plot of the true generating functions, so this should be what the output looks like if you do perfect fit.",
            "If you, if you had enough data, an if you could fit perfectly to the data, this is what it should look like.",
            "And now we are.",
            "Training the.",
            "Data with squared exponential kernel.",
            "And.",
            "So compared to the original, of course it's not perfect, but.",
            "I would say it's pretty good, so this is using squared exponential kernel and I think this is using the appropriate function so the error function for the likelihood.",
            "And.",
            "So that was using.",
            "Non zero mean I think.",
            "So and that was.",
            "That was using the non zero mean and this time we are using 0 mean.",
            "In which case we still get a quite a good fit.",
            "And now I changed the inference so we were using EP for the inference technique.",
            "In the previous one now.",
            "We're going to try it with.",
            "Variational.",
            "And this is basically what the variational approximation gives us so.",
            "I guess it still is still pretty good, but it does something different at the edges here.",
            "Yep.",
            "So yeah, please don't play around with the code and try to change the covariance functions and the mean functions the approximation algorithms.",
            "And try to get a feeling of it.",
            "And ask questions.",
            "About the iep.",
            "For this student distribution.",
            "2011 R Key Bettyann people from Malta University presented on medication.",
            "That's OK.",
            "In their code they have approvals for OK. OK, OK yeah I don't know that so.",
            "People from Aalto University have implemented the EP version of the Student T Like it.",
            "Sparse approximations OK. Yeah, so sparse approximations are not included in the toolbox.",
            "I'm not sure whether they are planning to include them or not, so do these people at Aalto University have a toolbox available or code available OK?",
            "GP.",
            "OK, the code the toolbox from Aalto University is called GPU stuff.",
            "So I'm assuming it's doing stuff.",
            "OK, do we have the video or no?",
            "OK, so there is a video available.",
            "I think on Carl Rasmussen's webpage.",
            "Which shows how to learn to control the cart and the pole.",
            "Basically, there's a, there's a cart and there's a pole hanging off the cards pretty, and by applying force to the cart you're trying to balance the pendulum at this state rather than swinging state.",
            "And this state is not, of course the stable state, so it's not something easy to do, but they're showing that using Gaussian processes for range.",
            "For reinforcement learning you can.",
            "OK, we have it.",
            "OK can everybody hear me?",
            "So we didn't know who to narrate it, so it's my laptop so I'll narrate it.",
            "But this is work done by Carl Rasmussen, an Marc Deisenroth.",
            "A couple of years ago and basically what what they did here was to learn dynamic model of a an inverted pendulum.",
            "So the idea is you have.",
            "You want to get a controller to learn to take this pendulum and invert it so that it's balanced in the unstable upright position.",
            "And rather than use, this is not a very hard problem from a classical control theory point of view.",
            "If you have a model of the pendulum, you can build a controller to do this, But what what they set out to do was to learn this, knowing almost nothing about the pendulum.",
            "So I think the only thing that they did was measure the length of the pendulum.",
            "But other than that, they didn't build any model of a pendulum in there.",
            "They just used a Gaussian process to model the dynamics of the pendulum an.",
            "What that means is.",
            "The system is going to.",
            "Exert forces on the cart going back and forth and then it can measure the angular position and velocity of the pendulum and it just learns the relationship between the forces and the change in the angular position and velocity.",
            "OK from experience and then it uses that to derive a control policy that inverts the pendulum.",
            "To make it stand upright.",
            "So it's Gaussian processes all everywhere that there is an unknown there bunch of unknown functions in here.",
            "Namely, the dynamics is an unknown function.",
            "They use a Gaussian process to learn the dynamics, and it's important that the model has concept of its uncertainty because the control policy that it applies is based on the uncertainty.",
            "So here's what we're going to look at is from their work just the entire life experience of this learning.",
            "A inverted pendulum robot?",
            "OK, so here we go.",
            "It starts an it applies some random forces and the pendulum moves around in some way and it gets data it's collecting data I think.",
            "At.",
            "5 Hertz, something very slow, like 5 samples per second.",
            "And so each of these little sessions is maybe 2 seconds long.",
            "Um?",
            "And in each of those sessions it's collecting, you know, 10 or 20 samples.",
            "I don't remember exactly the numbers.",
            "And from those samples it's basically probably calling the functions in the GPL toolbox to learn.",
            "The dynamics of this thing.",
            "OK, what's that my phone?",
            "OK, so it's had a few seconds of experience.",
            "And it's starting to try to apply what it thinks is a control policy by doing forward prediction through the dynamics, propagating the uncertainty forward in time and trying to achieve its goal, which is to be stationary in the upright position, so it didn't seem to do very well now.",
            "Each trial stops at the end, so that's why it swings at the end because it's just loose.",
            "OK, so it's had.",
            "Think maybe.",
            "10 seconds of actual experience now.",
            "It's trying clearly.",
            "He's probably getting frustrated.",
            "OK, so that trial ended there.",
            "It seemed to be doing pretty well.",
            "And let's see.",
            "And by now it's basically managed to balance it.",
            "So now you can try to irritate it.",
            "You can sense the frustration.",
            "And that's it basically.",
            "So the the idea, I think behind this is that if you use a flexible function approximator like a Gaussian process, and you try to deal with the uncertainty in that, then you can try to solve control problems without actually having to write down an explicit model of the control system.",
            "So there now scaling this up too much.",
            "More complicated systems like unicycle and a bunch of other systems like that.",
            "Alright, thank you.",
            "That's it for this course.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, we will be using this tool box written by Caroline Tennis.",
                    "label": 0
                },
                {
                    "sent": "This is the website for the toolbox.",
                    "label": 0
                },
                {
                    "sent": "And it runs both on Octave and Matlab.",
                    "label": 0
                },
                {
                    "sent": "And basically this this tool box came out of the collection of code which was originally used to demonstrate the main algorithms from this book that John also referred to.",
                    "label": 0
                },
                {
                    "sent": "So one important thing is that I think you will like this book is freely available for download from the books web page.",
                    "label": 0
                },
                {
                    "sent": "So basically Gaussian processes that are slash GPML after the course when you want to learn more about the.",
                    "label": 1
                },
                {
                    "sent": "GPS you want.",
                    "label": 0
                },
                {
                    "sent": "You can download it for free.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the toolbox.",
                    "label": 0
                },
                {
                    "sent": "Has a flexible framework for specifying Gaussian process models.",
                    "label": 0
                },
                {
                    "sent": "You can use different mean functions and different covariance functions and the combinations of these so and it has a modular design that is the allows to extend the existing libraries as well.",
                    "label": 0
                },
                {
                    "sent": "So if you want to develop further code based on this toolbox.",
                    "label": 0
                },
                {
                    "sent": "There are several different life functions that are available, like Gaussian likelihood for regression, for example or the plus for regression or cumulative logistic for classification and so on.",
                    "label": 0
                },
                {
                    "sent": "I'll go into more details of these later, and similarly there are several inference methods available, also implemented in the toolbox.",
                    "label": 0
                },
                {
                    "sent": "Exact inference for GPS is only possible for the regression case, and this is implemented in the toolbox.",
                    "label": 0
                },
                {
                    "sent": "And their approximation techniques that are used for either classification or regression.",
                    "label": 0
                },
                {
                    "sent": "You can use them so these are all the algorithms that are implemented.",
                    "label": 0
                },
                {
                    "sent": "The laptops well plus approximation, expectation propagation and variational Bayes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the toolbox has one function called GP, which you call and that does everything you want to do, so you call with different variables different number of variables tested what the function is going to do for you, whether you're just going to evaluate the.",
                    "label": 0
                },
                {
                    "sent": "Log marginal likelihood, for example, or whether you're going to optimize the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Whether you're going to test some test points and so on.",
                    "label": 0
                },
                {
                    "sent": "So everything is done seamlessly within this GP function.",
                    "label": 0
                },
                {
                    "sent": "But of course you'll need to format your inputs.",
                    "label": 0
                },
                {
                    "sent": "That that will be suitable for that will be compatible with that function and it is going to somehow tell the function what you want it to do.",
                    "label": 0
                },
                {
                    "sent": "So this is called.",
                    "label": 0
                },
                {
                    "sent": "The function is called GP and there are a number of supporting structures and functions.",
                    "label": 0
                },
                {
                    "sent": "As I said, there are different inference methods and these infinite inference methods are going to be specified also into a as an input to the GP function.",
                    "label": 0
                },
                {
                    "sent": "So the inference methods will be forgiven.",
                    "label": 1
                },
                {
                    "sent": "Model specification and data set they're going to.",
                    "label": 0
                },
                {
                    "sent": "Run the specified inference technique on the GP model that you specify with the.",
                    "label": 0
                },
                {
                    "sent": "With the parameters, the hyperparameters, it computes the approximate posterior and approximate negative log marginal likelihood and its partial derivatives with respect to the hyperparameters if they are required as outputs.",
                    "label": 0
                },
                {
                    "sent": "So what are the hyperparameters?",
                    "label": 0
                },
                {
                    "sent": "Hyperparameters are basically given as one single struct structure with three fields controlling the properties of the model.",
                    "label": 0
                },
                {
                    "sent": "One field is the likelihood function, so you specify which likelihood to use in the hyperparameter structure.",
                    "label": 0
                },
                {
                    "sent": "The other is the mean function, and the other is covariance function.",
                    "label": 1
                },
                {
                    "sent": "So you specify everything about the model specifics using the hyperparameter structure.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Specifying model parameters.",
                    "label": 0
                },
                {
                    "sent": "So what are the likelihood functions so the likelihood function that we pass to the model specifies the form of the likelihood of the Gaussian process model and computes the terms needed for prediction and inference.",
                    "label": 1
                },
                {
                    "sent": "OK, so depending on whether you want to do classification or regression, for example, the likelihood function is going to vary and depending on what you think the best way of classifying the data, for example, is, the likelihood function is going to vary the mean function.",
                    "label": 0
                },
                {
                    "sent": "As the name indicates is, it is basically specifying the prior mean for the GP, so it's a salary specifying the mean, and it computes the mean and it's derivatives with respect to the part of the hyperparameters pertaining to the mean.",
                    "label": 1
                },
                {
                    "sent": "OK, similarly covariance function is a cell array specifying the GPU covariance function, and it computes the covariance and it's there.",
                    "label": 1
                },
                {
                    "sent": "It is with respect to the part of the hyperparameters pertaining to the covariance function.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then there are several inference techniques that you can use.",
                    "label": 0
                },
                {
                    "sent": "As I said, exact inference is possible for Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Process is for the regression case, that is, when we have Gaussian likelihoods.",
                    "label": 0
                },
                {
                    "sent": "We can, if, in the absence of this, or if you want to try out, for example, if we want to compare how a particular approximate method compares to the exact output, we can use one of the approximation methods.",
                    "label": 1
                },
                {
                    "sent": "So let let's approximation basically approximates the posterior by a Gaussian that is centered at its mode, and it matches its curvature.",
                    "label": 1
                },
                {
                    "sent": "So it basically approximates the posterior by Gaussian that is fit to the.",
                    "label": 0
                },
                {
                    "sent": "Posterior density.",
                    "label": 0
                },
                {
                    "sent": "And it is applicable for differentiable likelihoods.",
                    "label": 1
                },
                {
                    "sent": "Because it's using the curvature, so expectation propagation is a different approximation algorithm, and it is also doing posterior Gaussian matching.",
                    "label": 0
                },
                {
                    "sent": "But instead of fitting the model and the matching the curvature, it does this by moment matching OK and then the.",
                    "label": 1
                },
                {
                    "sent": "Less algorithm I'll talk about is the variational Bayes which constructs the joint lower bounds.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the typo on the marginal likelihood based on individual lower bounds to every likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So basically the variational Bayes works as an optimization algorithm that gradually updates the lower bound to have the as tight lower bound as possible.",
                    "label": 0
                },
                {
                    "sent": "It is easy to provide a custom covariance matrix or functions with these tools.",
                    "label": 0
                },
                {
                    "sent": "If they, what do you mean by custom?",
                    "label": 0
                },
                {
                    "sent": "Like you specify the functional form?",
                    "label": 0
                },
                {
                    "sent": "Do you mean you want to specify everything about this?",
                    "label": 0
                },
                {
                    "sent": "Already exists in the toolbox is not no.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 1
                },
                {
                    "sent": "I mean I haven't played around with that myself, but I think it is fairly easy.",
                    "label": 0
                },
                {
                    "sent": "I mean, the only thing you would need to so if you look at the toolbox folders everything is sorted neatly under for under different folders I covariance functions are also sorted there and then.",
                    "label": 0
                },
                {
                    "sent": "All you would need to do is write out what you want to specify in your covariance function.",
                    "label": 0
                },
                {
                    "sent": "Was there something else?",
                    "label": 0
                },
                {
                    "sent": "Actually, really easy.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so a bit more detail about the likelihood functions that are implemented, so these are the names of the functions.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the Gaussian function.",
                    "label": 0
                },
                {
                    "sent": "The likelihood function is implemented.",
                    "label": 0
                },
                {
                    "sent": "The squared exponential Laplace students T desire for regression and for classification the error function and logistic function are implemented as the.",
                    "label": 0
                },
                {
                    "sent": "Like build functions so you can.",
                    "label": 0
                },
                {
                    "sent": "You can look up a table like this which is available also in the documentation of the toolbox to decide on which functions you use.",
                    "label": 0
                },
                {
                    "sent": "And again you could decide to choose something of your own too.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a compatibility of metrics of the likelihood functions versus the inference algorithms to make it easier for you to get the idea of what you can use together with what.",
                    "label": 0
                },
                {
                    "sent": "So again, as I said, this column is the likelihood functions and these are the inference methods, so exact inference is only possible for the Gaussian likelihood for regression.",
                    "label": 0
                },
                {
                    "sent": "The squared exponential Laplacian students the answer.",
                    "label": 0
                },
                {
                    "sent": "Also EP is available for all these methods except for students T. The reason being.",
                    "label": 0
                },
                {
                    "sent": "I think the students tease form doesn't allow the EP to be stable when it's updating the moments when it's trying to do moment matching.",
                    "label": 0
                },
                {
                    "sent": "That's why this is not available for student T and LA Plus is available for all but Laplacian, which sounds a little funny.",
                    "label": 0
                },
                {
                    "sent": "But the reason being Laplace requires continuous derivatives and laplacian's there were two is not continuous at the.",
                    "label": 0
                },
                {
                    "sent": "Not an variational.",
                    "label": 0
                },
                {
                    "sent": "Bayes is basically available for all the likelihood functions.",
                    "label": 1
                },
                {
                    "sent": "Again, these two functions that and are for for profit and logistic regression, which means for classification.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can decide to have a zero mean or mean equals one or mean equals some constant.",
                    "label": 0
                },
                {
                    "sent": "Mean being linear with some slope parameter basically.",
                    "label": 0
                },
                {
                    "sent": "Oh, or you can have composite mean functions, so scale I mean sum sum of 2 means or more than one mean product of means powers of the specified mean functions and masks basically.",
                    "label": 0
                },
                {
                    "sent": "Restricted on some components.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there is a long list of covariance functions.",
                    "label": 0
                },
                {
                    "sent": "So that you can try out.",
                    "label": 0
                },
                {
                    "sent": "I'm not trying.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to try to read all these two.",
                    "label": 0
                },
                {
                    "sent": "These are all in the documentation so you can look up what are the covariance functions that are available for you to play around with.",
                    "label": 0
                },
                {
                    "sent": "But as you can see from the list there, there there are no.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As plus if this is not.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enough, then you can still do composite covariance functions by scaling them, summing, taking the product masking, and adding and so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll first.",
                    "label": 0
                },
                {
                    "sent": "Run in matlab.",
                    "label": 0
                },
                {
                    "sent": "The demos and then I'll go on to explain the parts of the code.",
                    "label": 0
                },
                {
                    "sent": "So when I'm doing this, you can also do this on your machines just to get an idea and you can raise your hands to like ask questions if you don't understand the specific part of the.",
                    "label": 0
                },
                {
                    "sent": "Demo so basically.",
                    "label": 0
                },
                {
                    "sent": "The first part.",
                    "label": 0
                },
                {
                    "sent": "So it's the mean function, the covariance function they like that function, so this is the regression demo which is called demo regression that M on the document under documents folder I think.",
                    "label": 0
                },
                {
                    "sent": "So we are now generating data.",
                    "label": 0
                },
                {
                    "sent": "From the model that we specified and the data that we generate looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so we specified.",
                    "label": 0
                },
                {
                    "sent": "We specified the mean function, which was a sum of 2 means a linear mean and a constant mean.",
                    "label": 0
                },
                {
                    "sent": "And then we we specified some hyperparameter values.",
                    "label": 0
                },
                {
                    "sent": "Basically telling the slope and slope of this linear thing and the custom to add to it.",
                    "label": 0
                },
                {
                    "sent": "And then there is a covariance function which we specify as the maternal function set, set its parameters and so on, and likelihood function is Gaussian likelihood.",
                    "label": 0
                },
                {
                    "sent": "Then we decided to have 20 data points and we generated data using this function.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is the output of at the points X and we plotted that OK, so this is what we get so.",
                    "label": 0
                },
                {
                    "sent": "Now when we do inference.",
                    "label": 0
                },
                {
                    "sent": "What we get is.",
                    "label": 0
                },
                {
                    "sent": "This fit to the data points, so remember the data points are these, so the pluses are exactly at the same location, says where the data points are that we generated.",
                    "label": 0
                },
                {
                    "sent": "The Gray area is, as you can imagine, is the variance of the OR the standard deviation of the predictive likelihood.",
                    "label": 0
                },
                {
                    "sent": "So basically we see that and the blue curve is the mean of the GP.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way we did we plotted this was to generate a grid of points in this graph and we evaluated we took the.",
                    "label": 0
                },
                {
                    "sent": "Predictive likelihood all around this graph, and then we show the.",
                    "label": 0
                },
                {
                    "sent": "Variance and the mean.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "If we now want to learn a different model.",
                    "label": 0
                },
                {
                    "sent": "On the same data.",
                    "label": 0
                },
                {
                    "sent": "Basically, specifying a different covariance function this time squared exponential covariance.",
                    "label": 0
                },
                {
                    "sent": "So we specify again the hyperparameters of these covariance and the hyperparameters of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then we call function minimize, which is a function to minimize.",
                    "label": 0
                },
                {
                    "sent": "So we're basically so function to optimize the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so we call this function.",
                    "label": 0
                },
                {
                    "sent": "What happens is we get the.",
                    "label": 0
                },
                {
                    "sent": "We get the fit for the.",
                    "label": 0
                },
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "Which looks like this now.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what the model fit looks like when you optimize the parameters of this card.",
                    "label": 0
                },
                {
                    "sent": "Exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this kernel is the.",
                    "label": 0
                },
                {
                    "sent": "So this GP is using zero mean and.",
                    "label": 0
                },
                {
                    "sent": "Squared exponential is the covariance.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see what happens when we have a nonzero mean.",
                    "label": 0
                },
                {
                    "sent": "So this is a fit that we get when we have a nonzero mean.",
                    "label": 0
                },
                {
                    "sent": "So because when we have nonzero mean we have more flexibility to model the data, you'll see that this is kind of a better fit.",
                    "label": 0
                },
                {
                    "sent": "There is less.",
                    "label": 0
                },
                {
                    "sent": "Various here less uncertainty.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "That's basically how Gaussian process regression works.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is actually what we did.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "We have the GP function which outputs variable number of outputs and input it takes as input these.",
                    "label": 0
                },
                {
                    "sent": "Arguments none, not all of them need to be specified.",
                    "label": 0
                },
                {
                    "sent": "As I said, depending on what you want to do with the function, you specify more or less number of inputs.",
                    "label": 0
                },
                {
                    "sent": "So the first argument is the column vector of hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "The second is the function specifying the inference method.",
                    "label": 0
                },
                {
                    "sent": "3rd is the prior covariance function.",
                    "label": 0
                },
                {
                    "sent": "Forces the prior mean function.",
                    "label": 0
                },
                {
                    "sent": "And this is the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "And then we have the inputs.",
                    "label": 0
                },
                {
                    "sent": "So the training inputs the training targets, the test inputs and the test targets.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the possible outputs.",
                    "label": 0
                },
                {
                    "sent": "So basically the negative log marginal likelihood is the first one.",
                    "label": 0
                },
                {
                    "sent": "This is the column vector of partial derivatives of the negative log marginal likelihood with respect to each hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "It is returned only if you ask for it or it is computed only if you ask for it.",
                    "label": 0
                },
                {
                    "sent": "Column vector of predictive outcome predicted output means predictive output variance is predictive.",
                    "label": 0
                },
                {
                    "sent": "Latent means predictive latent variances lock predicted probabilities and finally as structure representation of the approximate posterior.",
                    "label": 0
                },
                {
                    "sent": "And depending on whether you're training or in prediction mode, it's either the sword output order successful, so these are all documentation from the code, so you don't need to actually have these in mind.",
                    "label": 0
                },
                {
                    "sent": "You can always look it up from the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "GP function OK so.",
                    "label": 0
                },
                {
                    "sent": "This is a very high level view of what the GP function does.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We first have this deep function half part with the green lines that I showed you before and then it does initializations and then it does inference and depending on whether you specify test cases or not, it either just reports the log marginal likelihood and their derivatives and the posterior or it computes test predictions.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a very compact power level of what the function is doing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a bit more detailed version of what I showed you.",
                    "label": 0
                },
                {
                    "sent": "So this is basically processing the input argument, so you don't need to worry about the details if you don't want to change the function itself, but.",
                    "label": 0
                },
                {
                    "sent": "Again, it's well documented, is well commented.",
                    "label": 0
                },
                {
                    "sent": "You can go to the code and read it out and see what it's doing.",
                    "label": 0
                },
                {
                    "sent": "Basically it's checking to see whether everything is in the proper format, that it should be.",
                    "label": 0
                },
                {
                    "sent": "It's initializing the things to their default values if they're not given, and if there are some default values, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Again, the next part basically checks on initialize, initializes the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So we check that the size of the hyper parameters that are supplied in HYP match the expectations, so makes a match what is expected of the of covariance and mean functions and the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So each of these.",
                    "label": 0
                },
                {
                    "sent": "Mean covariance and likelihoods are checked separately and they they are empty and entries are determined if they don't set the default values.",
                    "label": 0
                },
                {
                    "sent": "If they are not specified basically.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then finally we do inference.",
                    "label": 0
                },
                {
                    "sent": "So this part basically may issue a warning if it fails in training mode and it tries to recover because sometimes because of numerical instabilities the training can just fail just like that and it doesn't have.",
                    "label": 0
                },
                {
                    "sent": "Maybe sometimes it doesn't have to do with the fact that you specify something wrong or something so it tries to recover from that it it fails.",
                    "label": 0
                },
                {
                    "sent": "During training mode, but if it fails during testing mode then it's issues and error.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "In inference, you call this function if you're computing the marginal likelihood, then sorry, you're computing the marginal likelihood and is servatus only if it's necessary, so only if you are in the prediction case.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, to simply do inference and return the posterior.",
                    "label": 0
                },
                {
                    "sent": "And if you're doing, if you specify some test points as well, then you can run it with this.",
                    "label": 0
                },
                {
                    "sent": "So if is the inference function as you can imagine.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The test predictions.",
                    "label": 0
                },
                {
                    "sent": "So one important thing to keep in mind for the test predictions is that predictions are done in mini batches.",
                    "label": 0
                },
                {
                    "sent": "So even if you have a large number of test cases just to not run out of memory, not to overwhelm the memory, the program automatically puts them into smaller batches and does batch testing on those mini batches OK.",
                    "label": 0
                },
                {
                    "sent": "So basically this part of the codes handles the sparse representations if it's applicable.",
                    "label": 0
                },
                {
                    "sent": "So if you gave some sparse form of inputs to the data, it computes the necessary matrices like the the covariance matrices, or the inverse covariances, and so on.",
                    "label": 0
                },
                {
                    "sent": "If they're not provided, it sets the mini batch size, it allocates memory, it makes predictions and assigns outputs.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to make predictions basically for all test points in a mini batch, this is all you do to the predictions.",
                    "label": 0
                },
                {
                    "sent": "So evaluate the South variance, the cross clearances, the predictive means and then use it rollerski decomposition for for the AL matrix.",
                    "label": 0
                },
                {
                    "sent": "Compute the predictive variances and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, here computer predictive variances remove the numerical noise and output.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "As I said, there are different inference techniques available and exact inference is available only for regression.",
                    "label": 0
                },
                {
                    "sent": "It is possible only for regression and this is the only code you need to do to get the exact inference running.",
                    "label": 0
                },
                {
                    "sent": "So basically it's simple matrix algebra.",
                    "label": 0
                },
                {
                    "sent": "You need to just hard code like code it down, code down the matrix algebra and you have your inference technique.",
                    "label": 0
                },
                {
                    "sent": "This is in fact M. If you want to look it up.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a little word about the marginal likelihood versus cross validation.",
                    "label": 0
                },
                {
                    "sent": "Basically this is the marginal likelihood is basically summing over the over the data cases to give the predictive like dude.",
                    "label": 0
                },
                {
                    "sent": "And this is the leave one out cross validation likelihood or leave one out like it which basically leaves out one of the data points and evaluates this.",
                    "label": 0
                },
                {
                    "sent": "Same function basically so you can think of the marginal likelihood as giving the probability of the data given the model assumptions and.",
                    "label": 0
                },
                {
                    "sent": "The live on Earth Cross validation as giving an estimate of the predictive log probability regardless of the model assumptions being fulfilled.",
                    "label": 0
                },
                {
                    "sent": "So if you're more worried about your predictive.",
                    "label": 0
                },
                {
                    "sent": "Performance then you may be better off using the leave one out cross validation simply because it's more robust against models specification, and that's obvious from the fact that it gives an estimate regardless of the model assumptions being fulfilled or not.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you if you really want to have, like if you really believe in your model structure or if you want to have some good.",
                    "label": 0
                },
                {
                    "sent": "Model representing your data you're not too worried about predictive performance.",
                    "label": 0
                },
                {
                    "sent": "Then you would be better off using the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "I also have a demo for classification, again from the documentation so I didn't change much from that.",
                    "label": 0
                },
                {
                    "sent": "Basically similar to.",
                    "label": 0
                },
                {
                    "sent": "Regression we first generate some data.",
                    "label": 0
                },
                {
                    "sent": "Some data like this so the Reds pluses are of one class and the blue pluses are of another class.",
                    "label": 0
                },
                {
                    "sent": "You see that this is a non separable case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a contour plot of the true generating functions, so this should be what the output looks like if you do perfect fit.",
                    "label": 0
                },
                {
                    "sent": "If you, if you had enough data, an if you could fit perfectly to the data, this is what it should look like.",
                    "label": 0
                },
                {
                    "sent": "And now we are.",
                    "label": 0
                },
                {
                    "sent": "Training the.",
                    "label": 0
                },
                {
                    "sent": "Data with squared exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So compared to the original, of course it's not perfect, but.",
                    "label": 0
                },
                {
                    "sent": "I would say it's pretty good, so this is using squared exponential kernel and I think this is using the appropriate function so the error function for the likelihood.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So that was using.",
                    "label": 0
                },
                {
                    "sent": "Non zero mean I think.",
                    "label": 0
                },
                {
                    "sent": "So and that was.",
                    "label": 0
                },
                {
                    "sent": "That was using the non zero mean and this time we are using 0 mean.",
                    "label": 0
                },
                {
                    "sent": "In which case we still get a quite a good fit.",
                    "label": 0
                },
                {
                    "sent": "And now I changed the inference so we were using EP for the inference technique.",
                    "label": 0
                },
                {
                    "sent": "In the previous one now.",
                    "label": 0
                },
                {
                    "sent": "We're going to try it with.",
                    "label": 0
                },
                {
                    "sent": "Variational.",
                    "label": 0
                },
                {
                    "sent": "And this is basically what the variational approximation gives us so.",
                    "label": 0
                },
                {
                    "sent": "I guess it still is still pretty good, but it does something different at the edges here.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So yeah, please don't play around with the code and try to change the covariance functions and the mean functions the approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "And try to get a feeling of it.",
                    "label": 0
                },
                {
                    "sent": "And ask questions.",
                    "label": 0
                },
                {
                    "sent": "About the iep.",
                    "label": 0
                },
                {
                    "sent": "For this student distribution.",
                    "label": 0
                },
                {
                    "sent": "2011 R Key Bettyann people from Malta University presented on medication.",
                    "label": 0
                },
                {
                    "sent": "That's OK.",
                    "label": 0
                },
                {
                    "sent": "In their code they have approvals for OK. OK, OK yeah I don't know that so.",
                    "label": 0
                },
                {
                    "sent": "People from Aalto University have implemented the EP version of the Student T Like it.",
                    "label": 0
                },
                {
                    "sent": "Sparse approximations OK. Yeah, so sparse approximations are not included in the toolbox.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure whether they are planning to include them or not, so do these people at Aalto University have a toolbox available or code available OK?",
                    "label": 0
                },
                {
                    "sent": "GP.",
                    "label": 0
                },
                {
                    "sent": "OK, the code the toolbox from Aalto University is called GPU stuff.",
                    "label": 0
                },
                {
                    "sent": "So I'm assuming it's doing stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, do we have the video or no?",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a video available.",
                    "label": 0
                },
                {
                    "sent": "I think on Carl Rasmussen's webpage.",
                    "label": 0
                },
                {
                    "sent": "Which shows how to learn to control the cart and the pole.",
                    "label": 0
                },
                {
                    "sent": "Basically, there's a, there's a cart and there's a pole hanging off the cards pretty, and by applying force to the cart you're trying to balance the pendulum at this state rather than swinging state.",
                    "label": 0
                },
                {
                    "sent": "And this state is not, of course the stable state, so it's not something easy to do, but they're showing that using Gaussian processes for range.",
                    "label": 0
                },
                {
                    "sent": "For reinforcement learning you can.",
                    "label": 0
                },
                {
                    "sent": "OK, we have it.",
                    "label": 0
                },
                {
                    "sent": "OK can everybody hear me?",
                    "label": 0
                },
                {
                    "sent": "So we didn't know who to narrate it, so it's my laptop so I'll narrate it.",
                    "label": 0
                },
                {
                    "sent": "But this is work done by Carl Rasmussen, an Marc Deisenroth.",
                    "label": 0
                },
                {
                    "sent": "A couple of years ago and basically what what they did here was to learn dynamic model of a an inverted pendulum.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you have.",
                    "label": 0
                },
                {
                    "sent": "You want to get a controller to learn to take this pendulum and invert it so that it's balanced in the unstable upright position.",
                    "label": 0
                },
                {
                    "sent": "And rather than use, this is not a very hard problem from a classical control theory point of view.",
                    "label": 0
                },
                {
                    "sent": "If you have a model of the pendulum, you can build a controller to do this, But what what they set out to do was to learn this, knowing almost nothing about the pendulum.",
                    "label": 0
                },
                {
                    "sent": "So I think the only thing that they did was measure the length of the pendulum.",
                    "label": 0
                },
                {
                    "sent": "But other than that, they didn't build any model of a pendulum in there.",
                    "label": 0
                },
                {
                    "sent": "They just used a Gaussian process to model the dynamics of the pendulum an.",
                    "label": 0
                },
                {
                    "sent": "What that means is.",
                    "label": 0
                },
                {
                    "sent": "The system is going to.",
                    "label": 0
                },
                {
                    "sent": "Exert forces on the cart going back and forth and then it can measure the angular position and velocity of the pendulum and it just learns the relationship between the forces and the change in the angular position and velocity.",
                    "label": 0
                },
                {
                    "sent": "OK from experience and then it uses that to derive a control policy that inverts the pendulum.",
                    "label": 0
                },
                {
                    "sent": "To make it stand upright.",
                    "label": 0
                },
                {
                    "sent": "So it's Gaussian processes all everywhere that there is an unknown there bunch of unknown functions in here.",
                    "label": 0
                },
                {
                    "sent": "Namely, the dynamics is an unknown function.",
                    "label": 0
                },
                {
                    "sent": "They use a Gaussian process to learn the dynamics, and it's important that the model has concept of its uncertainty because the control policy that it applies is based on the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So here's what we're going to look at is from their work just the entire life experience of this learning.",
                    "label": 0
                },
                {
                    "sent": "A inverted pendulum robot?",
                    "label": 0
                },
                {
                    "sent": "OK, so here we go.",
                    "label": 0
                },
                {
                    "sent": "It starts an it applies some random forces and the pendulum moves around in some way and it gets data it's collecting data I think.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "5 Hertz, something very slow, like 5 samples per second.",
                    "label": 0
                },
                {
                    "sent": "And so each of these little sessions is maybe 2 seconds long.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And in each of those sessions it's collecting, you know, 10 or 20 samples.",
                    "label": 0
                },
                {
                    "sent": "I don't remember exactly the numbers.",
                    "label": 0
                },
                {
                    "sent": "And from those samples it's basically probably calling the functions in the GPL toolbox to learn.",
                    "label": 0
                },
                {
                    "sent": "The dynamics of this thing.",
                    "label": 0
                },
                {
                    "sent": "OK, what's that my phone?",
                    "label": 0
                },
                {
                    "sent": "OK, so it's had a few seconds of experience.",
                    "label": 0
                },
                {
                    "sent": "And it's starting to try to apply what it thinks is a control policy by doing forward prediction through the dynamics, propagating the uncertainty forward in time and trying to achieve its goal, which is to be stationary in the upright position, so it didn't seem to do very well now.",
                    "label": 0
                },
                {
                    "sent": "Each trial stops at the end, so that's why it swings at the end because it's just loose.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's had.",
                    "label": 0
                },
                {
                    "sent": "Think maybe.",
                    "label": 0
                },
                {
                    "sent": "10 seconds of actual experience now.",
                    "label": 0
                },
                {
                    "sent": "It's trying clearly.",
                    "label": 0
                },
                {
                    "sent": "He's probably getting frustrated.",
                    "label": 0
                },
                {
                    "sent": "OK, so that trial ended there.",
                    "label": 0
                },
                {
                    "sent": "It seemed to be doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "And let's see.",
                    "label": 0
                },
                {
                    "sent": "And by now it's basically managed to balance it.",
                    "label": 0
                },
                {
                    "sent": "So now you can try to irritate it.",
                    "label": 0
                },
                {
                    "sent": "You can sense the frustration.",
                    "label": 0
                },
                {
                    "sent": "And that's it basically.",
                    "label": 0
                },
                {
                    "sent": "So the the idea, I think behind this is that if you use a flexible function approximator like a Gaussian process, and you try to deal with the uncertainty in that, then you can try to solve control problems without actually having to write down an explicit model of the control system.",
                    "label": 0
                },
                {
                    "sent": "So there now scaling this up too much.",
                    "label": 0
                },
                {
                    "sent": "More complicated systems like unicycle and a bunch of other systems like that.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "That's it for this course.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}