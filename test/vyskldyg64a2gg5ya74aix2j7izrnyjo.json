{
    "id": "vyskldyg64a2gg5ya74aix2j7izrnyjo",
    "title": "Nonparametric Bayesian Models",
    "info": {
        "author": [
            "Yee Whye Teh, Department of Statistics, University of Oxford"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_teh_nbm/",
    "segmentation": [
        [
            "Hello hello.",
            "Can everybody hear me with my mic?",
            "OK, so I'll first like to thank the organizers for giving me the opportunity to to tell you about the area that I've been working on.",
            "My name is Eyt.",
            "I'm at Gatsby unit just South of here in London, and I'll be telling you about an introduction to Bayesian nonparametric modeling.",
            "I think so.",
            "Next week Peter Banks will also tell you about some foundational stuff on basic nonparametrics as well, so this lecture will be kind of more introductory.",
            "Just telling you giving a bit you a bit of an idea of how what are basic nonparametric models.",
            "And then after Peter Robbins Lecture, I will tell you a bit more more advanced models.",
            "On Thursday.",
            "OK."
        ],
        [
            "So here's the outline for today's talk.",
            "First tell you about some examples of parametric models that you have probably seen by now.",
            "Some in earlier parts of the of the summer school, and then if you a bit of introduction of Bayesian nonparametrics an as an example, I'll kind of go through in a bit more detail.",
            "Particular model called the Infinite mixture model and then I'll introduce the mean part of this talk, which is the derivative process.",
            "OK."
        ],
        [
            "So some examples of parametric models, so I think you guys have seen this from Carl Rasmussen's talk, right?",
            "So suppose that we want to do supervised."
        ],
        [
            "Learning and we want to learn a function from some input space X2 and output space Y.",
            "So for example, the function might look like this.",
            "And we are given a bunch of training data, so X one is here and why one is the height of this point, for example.",
            "And this X2 and Y2 and so on, right?",
            "And then we want to fit a function which goes through this set of lines."
        ],
        [
            "So the typical parametric way of doing things would be.",
            "Let's start with a set of basis functions, right?",
            "So this is a set of functions which can which describe kind of basic shapes from which we can build our functions from, right?",
            "So then we say that we will parameterized function F of X using a set of parameters W as some linear combination of the basis functions.",
            "And here, of course, the parameters RW an if we're just doing something very simple, we might say we want to find a set of parameters.",
            "Search that our training output Yi is as close as possible to the function evaluated at the training input XI.",
            "An of course will use the squared norm because it's the simplest one and want to minimize this quantity over our training sets.",
            "Right?",
            "So that's very simple, but will be a Bayesian in this lecture.",
            "So just following calls footsteps.",
            "So instead will rephrase this using a probabilistic model.",
            "In particular what we would say is that our output, why I given our input XI an R parameters W?",
            "Will be simply we will have a mean given by the function evaluated at XI.",
            "And plus some noise term epsilon I which is drawn from some Gaussian distribution.",
            "And then we'll also place a prior on our parameters, so WK will be some, some Gaussian will be Gaussian distributed with a 0 mean and some other variance term Town Square.",
            "And what we want to do is compute the posterior as you've heard in lecture."
        ],
        [
            "OK, so let's just look at the form of the function that we're fitting is a linear combination of K basis functions.",
            "So firstly, what basis functions should we use, right?",
            "So that's kind of a number of questions that we can ask ourselves whether this is a good model or not.",
            "Firstly, what basis functions to use?",
            "You know you could be using, say, Gaussian shaped basis functions then that basically gives us the assumption that.",
            "All functions are smooth, right?",
            "Or we could use things like that kind of triangular shaped and then that might give us piecewise linear functions for example.",
            "The second question is how many basis functions do we use, right?",
            "So the innocence?",
            "This number of basis functions K tells us the complexity class of our approximation of our class of functions that we are approximating the true function with.",
            "Thirdly, do we actually have a belief that?",
            "The true function F star can be expressed as a linear combination of basis functions for some set of parameters W. So this is typically unlikely because as we know from cows lecture, the typical true function is typically very complicated and very hard to approximate anyway, so that's very unlikely that we can come up with a finite set of basis functions that can approximate our true function without knowing it.",
            "Without knowing the true function to begin with.",
            "And finally, we even believe that the noise process here is Gaussian, right?",
            "What?",
            "What if we don't actually believe that the noise processes goes in?",
            "What can we do about this so this kind of questions about?",
            "So we're kind of starting off with this parametric family or functions, and these are questions about whether we want to go beyond this particular parametric family an of course, Gaussian process is a way of going beyond a parametric family to a non parametric family.",
            "Metric family functions.",
            "So here's another example.",
            "Say we want to do density estimation OK.",
            "So this is an unsupervised learning task now.",
            "An we are we are we supposed that there's some true underlying density F star say something like this an from which we."
        ],
        [
            "Have obtained IID samples from so this.",
            "This forms are training set XI.",
            "So the dots here are kind of the ID samples from F star and what we want to do is to somehow recover the underlying density.",
            "Here App Store from our training set.",
            "Again, the simplest thing we might want to do is to assume some sort of exponential family distribution, say so for example, something like a Gaussian.",
            "So this is the Gaussian parameterized by its mean and its covariance.",
            "It has a normalization term up here and then.",
            "Here is the.",
            "Well, everybody knows this I think anyway, so what's the problem with the Gaussian?",
            "Firstly, is Union model right?",
            "So for something like this, where is multimodal?",
            "It will definitely not be able to fit the data, so you just give it a single bump instead of three bumps.",
            "Secondly, the shape is quite restrictive, right?",
            "So in high dimensions we know that the shape of a Gaussian basically looks at.",
            "Flat pancake or ball ball or something?",
            "Rugby rugby balls.",
            "I'm.",
            "And of course it holds.",
            "It has things like pretty light tails.",
            "Basically as X goes to Infinity then the probability of X goes down really quickly as a function of X.",
            "Instead, well, we might want to do is to use a mixture model, right?",
            "So will parameterise a density F of X using a mixture of K different Gaussians for example.",
            "An paije here are our.",
            "Oh, I guess it should be \u03c0 K. That's why there's a typo.",
            "Should be.",
            "The mixing proportion is the probability that any particular data point will belong to mixture components K. And make sure component K is given by a Gaussian with mean mu K and the covariance of Sigma K. Again, we might want to ask questions about whether we want to be restricted to this parametric class of densities.",
            "So do we really believe that the true density is a mixture of components?",
            "Again, unlikely not to be the case.",
            "And even if we do believe there is some issue OK components, how many major components do we want to use, right?",
            "It's kind of hard to pick a key using model selection or solve techniques.",
            "OK."
        ],
        [
            "So yet another example.",
            "So here we are.",
            "We want to do latent variable modeling right?",
            "So say we have N vectors of.",
            "And vector observations X one to XN.",
            "An we want to model each as a linear combination of K latent sources or factors or features or something so XI.",
            "Is going to be some from of K from one to Big K. Of basis function Lambda K * y IK.",
            "So why here is going to be our latent source activity?",
            "Plus some nice term epsilon.",
            "I so examples of this sort of latent variable models include things like principle components, analysis, factor analysis, independent independent components analysis, right?",
            "So these are pretty standard techniques in machine learning and statistics.",
            "Um?",
            "Again, we might want to ask how many sources are there really in our data, right?",
            "Do we really believe that there are case sources in our data?",
            "How do we find a K even if we do believe that it's OK, an another question might be what prior distribution should we use for these sources?",
            "OK."
        ],
        [
            "And.",
            "Finally, yet another example of a parametric model that you now know all about after they flies lectures is topic modeling with latent additional allocation, right?",
            "So here we the task is that we want to infer topics from a document corpus topics.",
            "Now being kind of a set of words that tend to Co occur together in our in different documents and the.",
            "The model basically goes as follows, so we have outer plate here that denotes document OK and Pi J is going to be a distribution over topics for that document J.",
            "And then for every word in a document, what we'll do is we'll first pick a topic by drawing from a multinomial given by \u03c0 J and then we'll draw a word given the topic indicator that Ji.",
            "From a distribution given from distributions parameterized by Theta K, so I think probably.",
            "The notation I use is a bit different than device lecture, but you can easily translate within them an again we will try to be Bayesian, so we'll place priors on Theta and of course by itself has a directly prior and will place additional prior on Theta 2.",
            "Again, we might want to ask how many topics can we find from the corpus.",
            "We even believe that our corpus only has K topics.",
            "Maybe it has modern K, or maybe it has infinitely many topics possibly.",
            "Right, so these are all parametric models.",
            "So now will go into a basin on Parametrics actually.",
            "Is there any questions on the model so far?",
            "No, OK, it's pretty simple stuff I guess, so will in fact see that for each of the four examples that we'll give, we'll try to derive nonparametric models for each of them.",
            "You're already know the answer for one of them, right?",
            "So for the regression case, it's just a Gaussian process.",
            "So."
        ],
        [
            "Bayesian nonparametrics, so that's kind of a few philosophical points about basic nonparametrics.",
            "Why we want to do based on parametrics.",
            "Firstly.",
            "We tend to."
        ],
        [
            "Leave that models are almost never correct for our real data.",
            "And what happens if they are?",
            "What happens if they're not correct?",
            "How do we deal with model Misfit?",
            "We can try to do things like the following.",
            "So firstly we can try to quantify how far away is our model from the true model.",
            "Some self measure of optimality of fitted model.",
            "Secondly, we want to do things like model selection or averaging so we have a.",
            "Have a sequence of models of increasing complexity and we want to find the model of the right complexity to describe our data.",
            "Or in the case of averaging, we Mount the average over this sequence of models have a Bayesian averaging.",
            "Certainly we might want to not restrict ourselves to a small set of models now, right?",
            "So to a small model class, we might want to increase the flexibility of our model class.",
            "An basic nonparametric models are basically parametric models whose flexibility has been increased whole lot so that they are, in a sense, able to capture almost any.",
            "Data set that we might encounter.",
            "OK. And they are also good solutions from the perspective of model selection or averaging.",
            "In fact, there is a different way of approaching this problem of finding a good.",
            "Complexity follow for model.",
            "In fact it just says integrate everything out and not worry about overfitting."
        ],
        [
            "So you don't have to do model selection or averaging or something, so it makes your life much simpler.",
            "So let's in fact go into a bit of detail about why we might not want to do model selection or averaging, at least from the Bayesian perspective, right?",
            "So let's just imagine that we have our data X again, so X one to XN and we have a sequence of models MK, each parameterized by Theta K for K = 1, two and three, and so on.",
            "So to do model selection or averaging we will.",
            "We want to compute the marginal probability of our data, so P of X given MK is simply the integral.",
            "Over Theta K of the probability of our data under our model, an hour set of parameters.",
            "An there should be a P in here, so P of data.",
            "Given MK, I'm sorry.",
            "Typos again, so is the likelihood multiplied by the prior over Theta and we integrate over data.",
            "So that's our marginal probability of our data integrating all of parameters and then we want to do model selection, in which case we want to find.",
            "The Model MK, which maximizes the probability of our data given our model.",
            "Or if we want to be more Bayesian, we want to.",
            "Computes the posterior over model index K, an hour parameters Theta K by just multiplying the prior over Theta and K. By the likelihood and then normalizing.",
            "So it turns out that this marginal probability is very hard to compute for almost all models of interest.",
            "I'm not sure whether Ian talked about whether it's hard to compute this.",
            "Probably Zubin did.",
            "So for us to do model selection or averaging will always need to compute this thing right so?",
            "And the idea is that for model selection is to prevent overfitting or an underfitting by finding a model of the right complexity class to explain our data.",
            "And because it's because this marginal probability is hard to compute, is expensive to compute, we don't really want to do this too often.",
            "So, so there's one insight that was noted by a few people, including color Smithson Zuben.",
            "Most recently I think not that recently I guess.",
            "People before that too, like Dave Mackay and revenue.",
            "Is that because we are integrating all of our parameters when we do this computation?",
            "When we're being Bayesian.",
            "We're not going to overfit anyways because we've integrated out of our parameters.",
            "There's nothing to fit through our data so that we can't really overfit.",
            "If we can't really overfit, then what is it?",
            "What is to prevent us from using a really large model, right?",
            "Because if we start off with a really large model then we won't under fit to our data.",
            "And we won't overfit either because we're being Bayesian, right?",
            "So?",
            "So that's kind of the basic idea of why we want to do basic nonparametrics from this perspective."
        ],
        [
            "So right?",
            "So what is a nonparametric model nonparametric model is basically a. Parametric model that's really large, so where the number of parameters is either infinite or increases with the data that we see.",
            "We can also think of it as a model over and some sort of infinite dimensional function or measure space so you've seen an example of this in the case of a Gaussian process, right?",
            "So call places a prior distribution over all possible functions that's given by a Gaussian process.",
            "So this is an infinite dimensional space.",
            "In fact it's a countably infinite uncountably infinite dimensional space race.",
            "We need to have a latent variable.",
            "For every input.",
            "Of the function.",
            "Um?",
            "Another way of thinking about what is an operating model is a family of distributions.",
            "That is dense in some large space, so a dense in the sense that if you give me any.",
            "Any other if you give me any distribution, I can find a distribution in this family that's as close as you want to to your distribution, OK?",
            "And so the idea is that with a parametric family, we're parameterising it with a finite number of parameters.",
            "So we can think of family of distributions as a very low dimensional manifold in our.",
            "In this large space of possible distributions, and if you have a small if you have a low dimensional manifold, you're not going to be able to.",
            "To approximate any distribution to arbitrary accuracy.",
            "So with nonparametric models, we are dealing with a family of distributions that so large that it can approximate any distribution to any arbitrary accuracy.",
            "So why we want to deal with nonparametric models in a Bayesian theory of learning?",
            "It is a really nice way of sidestepping model selection averaging.",
            "It's we can also think of it as kind of a broad class of priors that doesn't restrict the model to a parametric family, so it allows the data the data to speak for itself, right?",
            "So if you have lots of data then you'll get very close to the distribution that is underlying the data.",
            "So.",
            "I personally find it well all of these different views to be cover good ways of thinking about what isn't.",
            "Nonparametric model but we are faced with this computational issue of we have a really large parameter parameter space.",
            "Now right infinite dimensional parameter space.",
            "How do we deal with it?",
            "Computationally we can fit all of these parameters onto our computer.",
            "Nor do we really want to because it's just going to take ages to do anything with this.",
            "So that's got two ways of doing this.",
            "One is to marginalized all but a finite all by a finite number of parameters from our model, right?",
            "So we saw this again with the Gaussian process, right?",
            "So if you observe any training points, then you really only need to compute the posterior of the function.",
            "On this end, training inputs.",
            "Rather than all the other potential input points.",
            "Another way of doing this is to define this infinite space implicitly.",
            "This is a bit similar to the kernel trick an will be applying two kind of like pretty fundamental theorems.",
            "1, graph consistency theorem and the other is definitely theorem and I'll actually talk about this later.",
            "And Peter Robbins will go into detail about these two things too.",
            "OK, so we've actually seen."
        ],
        [
            "Gaussian process is already right, so.",
            "Gaussian process is basically a random function right F from input space to R. Such that if you're given any finite set of input points X one to XN, then if you look at the joint distribution of F evaluated at X1F evaluated at X2 all the way until F evaluated at XN, then this vector here this random vector, because the function is random, we have a fixed input but random function, so this vectors random 2.",
            "Is given a Gaussian distribution with a mean given by M of X1.",
            "Vector which is M of X1 until Emma Vexen.",
            "So this is our main vector where M is our main function.",
            "And we have a covariance matrix given by a covariance function C evaluated at our input points.",
            "So here.",
            "The way we think of what a random function is is that it's basically a collection of random variables.",
            "So for every X potential input X in our input space big X, we have random variable F of X and this is a really large set of random variables, it's A.",
            "Is cost a stochastic process is basically a set of random variables?",
            "Another way, so this is a view of the Gaussian of a Gaussian process as.",
            "A model with an infinite number of parameters.",
            "So we have uncountably infinite number of parameters here 1F of X for every X.",
            "This view here.",
            "Is of a Gaussian process where the number of parameters.",
            "Growth with our data set right as our data set N grows.",
            "The number of parameters that we're dealing with here also grows linearly in our.",
            "In yeah, in fact, the number of parameters equals to the number of training input points.",
            "Another way in which we could express the Gaussian process is as follows.",
            "We can in fact show that in fact call later today will tell you about how you can express F of X.",
            "Which is a.",
            "Which is drawn from a Gaussian process as.",
            "A finite sum of a finite linear combination of basis functions.",
            "But then we are going to take the number of basis functions to Infinity.",
            "So this is yet another view of Gaussian process where we think of it as we start off with the parametric model and then we take the number of parameters in this parametric model to go to Infinity.",
            "OK, so we'll see kind of like parallels of this in all the different cases that we see.",
            "OK.",
            "I'll skip this.",
            "Actually, I'd like to talk a bit about this.",
            "So the way we define the Gaussian process here, right?",
            "So we say that for any input for any set of input points X one to XN we have some vector.",
            "Here an will say that this vector is given a Gaussian prior."
        ],
        [
            "Alright.",
            "So we're not really guaranteed that this set of.",
            "Priors one for any set of inputs to be somehow consistent with each other, and it turns out that Luckily, because we're dealing with Gaussians, he ran when you marginalized out random variables under Gaussian, you still get a Gaussian and out right, and because of this consistency property of Gaussian distributions, we know that all of these marginal distributions are consistent with each other.",
            "And because they are consistent.",
            "We can apply this common graph consistency theorem, which kind of guarantees that Gaussian processes exist, IE.",
            "This whole stochastic process actually exists, so there will be a bit further information and Peter audiences lectures.",
            "Any questions?",
            "So far, yes, does it mean that there is a unique solution that these marginals or there could be made?",
            "It's unique, yes?",
            "In fact, Peter will tell you a lot more about that kind of issues with this construction.",
            "Actually, in that I don't really go into details.",
            "He'll tell you about it.",
            "Yes, so this is Gaussian process or no nonparametric Bayesian model or not, yes it is.",
            "Yes so.",
            "But we still have some parameters we have to believe that all this is Gaussian.",
            "Yes, that's right so.",
            "A nonparametric model is not really a well.",
            "The way I think about it is not really a model without any parameters.",
            "In fact, it has a model that's really primarily in that has a lot of parameters.",
            "Actually, I'm not sure whether that makes sense.",
            "It's simply not a parametric model in the traditional sense of a finite number of parameters, right?",
            "So and the other issue is that, of course, we're still assuming that all of this random variables have a Gaussian prior.",
            "And that's kind of in a sense unavoidable, because at some point we have to make some assumptions about how our.",
            "Our data looks like right?",
            "What is what is a reasonable assumption about the world and we have to make certain assumptions that in the case of nonparametric models, we do have to make quite a bit of assumptions about.",
            "That the things are kind of nice in some ways, like a conjugate for example because.",
            "And closed under this marginalization so?",
            "Yeah, so we are nonparametric.",
            "You can think of it as we are not presently in some directions but parametric in some other directions.",
            "OK, so so you've seen all of that right?",
            "So with."
        ],
        [
            "Process and stuff.",
            "So I'll tell you now about infinite mixture models.",
            "I don't think we've seen this in, at least in this summer school yet, so we will again take the same approach, where in fact, what we'll do now is will actually start off with a parametric mixture model and then take the number of components to go to Infinity."
        ],
        [
            "To form our non parametric mixture model.",
            "OK, so we're being based in here, right?",
            "So let's so the first thing we have to do is of course being easy.",
            "We have to place priors over parameters.",
            "And of course afterwards we want to compute posteriors using Markov chain Monte Carlo, which you've learned already from Ian's lecture.",
            "So.",
            "Right, so here's how we'll parameterized our mixture model.",
            "Firstly, will introduce latent variable XI, Zac I, which indicates which components data item XI is going to belong to.",
            "An will of course.",
            "Gives I applier which is simply a multinomial with a vector of probabilities given by mixing proportions, right?",
            "So the probability of?",
            "Data Item XI belonging to component K is simply going to be the case.",
            "Entry in this pie vector.",
            "And then, given that we've chosen which component XI is going to belong to.",
            "Will say that XI, given that I equals 2K, will be drawn from a Gaussian with a mean of mu K and the covariance of Sigma K. So here's the graphical model.",
            "We have our plates over our data items.",
            "I goes from one to N. Here's our data Vector XI an here's our cluster indicator variables I and that is going to be drawn from a multinomial parameterized by \u03c0.",
            "And this data Star K is basically mu K. Anne Sigma K put together.",
            "So those are parameters for mixture component K. An of course we want to introduce priors for both the mixing proportions as well as well as our component parameters as well.",
            "So we'll introduce for simply, for simplicity, conjugate priors for parameters so.",
            "Here we have multinomial for pie, so the conjugate prior for Pi will be Additionally with.",
            "And then I'm going to assume that the parameters of the direction they are given by Alpha over K. Until Dot Alpha over key.",
            "So it's a symmetric originally.",
            "In which the we have a K vector as our as a parameter.",
            "As a parameter vector an, it's a symmetric richley, so every every entry of this vector just as Alpha over K. We'll see that this Alpha divided by K is kind of important.",
            "Next, like why we want to do this?",
            "And the conjugate prior for a Gaussian distribution.",
            "So from you can Sigma K is going to be what's called normal inverse inverse.",
            "We shut distribution.",
            "And let's not worry about the.",
            "The parameters in there so you can look up Wikipedia for normal investigation.",
            "I'm pretty sure you.",
            "You'll find at least you find yeah anyways.",
            "So this thing I'll denote by H. This is our prior distribution over our parameters of our mixing components.",
            "So call Russell's also the one who.",
            "Most recently we derived this infinite limit prior to count as a whole bunch of people who have done the same, except one of those models that keeps on getting rediscovered.",
            "Time and again."
        ],
        [
            "So, so that's our model."
        ],
        [
            "That's our prior.",
            "So now we want to compute the posterior and what we will do is give sampling very simple an because everything is conjugate, we can write down."
        ],
        [
            "Almost immediately, the conditional probabilities required for Gibbs sampling algorithm so.",
            "First, we'll look at the conditional distributions for the parameters, so for \u03c0.",
            "It's conditional distribution given everything else is going to be just.",
            "It's going to only depend on that, basically because if you look at this graphical model, the only way in which interacts with all the other random variables down here is true that an because the duration is conjugate, the posterior of Pi, given that is again a Dirichlet with updated parameters.",
            "So this notation I got from the fly South one set is simply the number of data items that's assigned to cluster one.",
            "In general, any case that is the number of components that assigned to cluster K, the number of data points that's assigned to classification.",
            "Questions.",
            "And also because our prior over the parameters is conjugate too, so the posterior over.",
            "Parameters for mixture component K. Given all the other latent variables is also going to be a normal inverse normal inverse Wishart with some updated parameters.",
            "So again, let's not worry about this this bit.",
            "So what I'm interested in is actually this bit here.",
            "And then so we've.",
            "I've described to you, what's the?",
            "What's the conditional distribution for \u03c0 and Theta?",
            "And now we also want to write down the conditional distribution for that.",
            "An turns out to be also very simple, so the.",
            "The conditional probability of ZI equal to K given all the other latent variables.",
            "Is simply going to be proportional to Pi K. Times end of XI.",
            "Given the parameters of component key.",
            "So we can think of this term as follows.",
            "Pi case the conditional.",
            "Prior that that I is going to take on value K, it is in fact the prior falls.",
            "I taking on value K and this is this term.",
            "Here is just the likelihood of our data.",
            "If that I takes on value K. And of course you need to normalize this to compute the posterior over that I.",
            "So this is quite similar to.",
            "Computing the responsibility when you do EM algorithm for a mixture model, right?",
            "An an as we saw with latent richly allocation, this gives sampling.",
            "Distribution is not as efficient as one in which we.",
            "As a collapsed Gibbs sampling algorithm where we integrate out the parameters, So what we'll do now is to integrate our Pi and integrate out the mixing component parameters two and.",
            "The only thing we will now give sample over the cluster indicator variables that I.",
            "And if you go, if you go through the math, you see that.",
            "The.",
            "Conditional probability of that I equal to K given all the other variables is simply going to be proportional to the product of two terms again, so this time here is placed the same role as Pi K is going to be the conditional prior.",
            "That zetty is going to take on value K given all the given the current assignment of all the others at ICE.",
            "OK. And it's in fact quite quite interpretable.",
            "What it says is the following.",
            "So we will simply count out the number of times among all the all the other cluster indicator variables of the number of times.",
            "That cluster K was chosen as the cluster to explain the data so.",
            "This is this is the number of times that cluster K was chosen among all the other guys.",
            "And then we're going to add a pseudo count that's given by Alpha over K. Two discounts.",
            "And then we're going to normalize by.",
            "Basically some over small K of this term here, so that turns out to be just a constant Alpha plus N -- 1 N being the North minus one being the number of.",
            "Other.",
            "Data items the total number of data items is North.",
            "We are we are not.",
            "Including the current data item I so we only have N -- 1 data items left.",
            "Among these, some and then multiplied by a likelihood term again, and this likelihood term simply says, what's the probability of XI given all the other data item that's currently assigned to cluster K. And this we can compute this because again, we have assumed a conjugate prior for our Gaussian distribution.",
            "So this turns out to work very well.",
            "I'll show you a little demo actually just off this.",
            "Keep sampling going.",
            "King hopefully this works OK, so we're running MCMC sampler here an at every iteration we're going to update our parameters right and.",
            "Given the parameters we can, we can basically compute the density for that particular set of parameters, and of course the parameters are moving around in parameter space in Gibbs sampling, so the density is kind of like wobbling around as well.",
            "And now we can start adding data points in here, so yes.",
            "Oh yeah, the yeah the blue line here is the is the.",
            "Estimate is the current estimate of the density given the current set of parameters and the black line here is basically a running average of the most recent blue lines, so it's kind of telling you what's the average density under posterior sofa.",
            "OK, so I've added two data points in here and you can see that there's a lot more mass in this region now right?",
            "And then the.",
            "The average density is kind of like more less picked around here, right?",
            "So let's add a few more data points here, see.",
            "So hopefully you'll start producing bimodal distribution, right?",
            "So this is collapsed Gibbs sampling running.",
            "You can actually see it running.",
            "So what happens if we add more data points now?",
            "OK, so seems to be OK, right?",
            "So we have three clusters of points and you can kind of see.",
            "The three different modes.",
            "Actually, if I add more data points in here.",
            "We might actually be able to see the tree clusters more clearly, I hope.",
            "I'm just going to add lots of data points.",
            "OK, so we see three clusters, so this is nice and working fine and stuff right?",
            "OK, So what happens now if we add another cluster, right?",
            "I'm still running going a bit slow.",
            "Is there?",
            "Yeah, OK, so so does anybody see any problem with this right now?",
            "Not enough mass.",
            "That's true, so that's kind of like.",
            "It.",
            "It's not quite able to fit all four clusters, right?",
            "How many clusters do you think is our in our mixture model tree right?",
            "Yes, obviously, so you can see that we are this finite mixture model is kind of going getting into trouble now, right?",
            "Because it's not flexible enough to capture this very simple data set.",
            "Yes, it's a finite mixture.",
            "Three, yes, so.",
            "And if we try to fit it to four clusters, we're going to see, of course, that it will try to.",
            "Model this for clusters using tree mixture components.",
            "So so right now it's kind of like merge these two clusters together and this manage to have a component for each of those classes.",
            "For each of these two clusters.",
            "So it's kind of working.",
            "But not quite right.",
            "So what we'll do next is basically to take our finite mixture now and take the number of components in this mixture to Infinity so that it can deal with any number of clusters that you give it basically.",
            "So.",
            "Yeah.",
            "Stop.",
            "So that's a finite mixture model."
        ],
        [
            "Yeah, the final mission mall.",
            "So now we want to take Keita Infinity.",
            "OK, so before we take a two to Infinity, what we'll do is imagine simply a really large K. So what happens when we have a really large number of missing components?",
            "Firstly, we only have N data points and could be big.",
            "1000 by K could be even bigger like a million or something, right, so?",
            "On number of mix.",
            "Mixture components K. We can imagine it to be much larger than the number of data points that we have.",
            "Because every data in the worst case, every data point can be assigned to its own individual cluster.",
            "So that's really only.",
            "Ann clusters that will be used to explain the data out of this really large number of.",
            "Major components right so?",
            "What this says is that most of the components in our mixture will simply be empty, so they're not going to be associated with data at all, and what we will try to do is actually to lump all of this empty components together.",
            "So if we want the first equation here, I'm just going to take sorry.",
            "This thing here and copy it over here.",
            "Right, so the probability.",
            "So in our Gibbs sampler, the probability that.",
            "Data item I is going to be assigned to cluster K given the assignments of all the other clusters is going to be proportional to again, this conditional prior of.",
            "Data item I being assigned to component K * A likelihood of.",
            "Off the data item given all the other data items that is currently assigned to cluster key.",
            "Um?",
            "And so for the for the clusters for which.",
            "For the occupied clusters for so far the clusters for that have that are currently that that currently have data item assigned to them.",
            "So in other words, where this NK term is not zero is some positive integer, right?",
            "So for those clusters then.",
            "This thing is is fine, right?",
            "So we have.",
            "Alpha over K. So K is a very large integer, so this Alpha over K is going to be a very small number now.",
            "Plus a positive integer, so this time he is not going to be 0 and that term here is not going to be 0.",
            "So this thing is perfectly well defined.",
            "Right, So what about for the empty clusters now?",
            "So for the clusters for which there's actually no data associated with those clusters, this NK term is going to be 0 right?",
            "And of course, we're going to be in trouble because we're going to have probably Alpha over K term out here.",
            "That's going to be really small, so the probability that.",
            "A data item being assigned to that particular cluster will be arbitrarily small, as K becomes very very large.",
            "So what happens then?",
            "Well, we can't really do anything, but what we can do is to say that if we lump all of these different empty clusters together.",
            "Right, so that's a lot of them.",
            "And hopefully it will cancel off this small probability Alpha divided by K. And it does happen.",
            "So what's the you can workout the probability that data item I being assigned to an empty cluster is going to be proportional to Alpha over K multiplied by the number of empty clusters in our mixture model.",
            "And K star here is the number of occupied clusters and K -- K star is more or less K because K is much larger than North, right?",
            "So this term here is going to be OK and that term is going to be OK as well because it's simply going to be the marginal probability of data item XI where we integrate out the parameters for that cluster.",
            "So as K goes to Infinity, Now let's see what happens so.",
            "As Big K goes to Infinity, this term is going to go to 0, right?",
            "Anne.",
            "This term here is going to go to one.",
            "Because K star is bounded by N and K -- K start over K is going to be more or less one.",
            "For large key, yes.",
            "Excited.",
            "This thing it doesn't depend on K right?",
            "Because they are all simply the marginal probability of the data given no other data points.",
            "No, no yeah we don't.",
            "Yeah, we don't need a case here, I guess yeah."
        ],
        [
            "OK, so we do that.",
            "Yeah, so that time goes to 0.",
            "This time goes to one.",
            "So now the probability of data items I be assigned to cluster K is simply going to be proportional to the current number of data items that's assigned to cluster K. Multiplied by the likelihood and then divided by some other normalization constants.",
            "And the probability of this data item being assigned to an empty cluster.",
            "So you can think of this as I guess I should call this components rather than clusters.",
            "So the probability that data item I is going to be assigned to.",
            "And empty components is going to be proportional to Alpha times.",
            "The probability of XI.",
            "Marginal probability of exile where we have integrated the parameters of the components.",
            "And we have N -- 1 plus Alpha.",
            "Now here this constant, so we can just ignore it.",
            "So we can think of this as introducing a new component to explain the data.",
            "And this is basically reusing a component to explain this particular data points.",
            "An of course the maximum number of components that this model will use is at most N. So now we can go back to our DP demo.",
            "So we do the same right is exactly the same samples from the prior.",
            "And you kind of see that it also has bumps off structures.",
            "It looks quite similar to the finite mixture model.",
            "At the two data points here, we're going to get a lot more math here now.",
            "But of course we always have mass going outside 2.",
            "So let's try to create as many clusters as we can.",
            "I'm not sure whether this will work, we see.",
            "So OK, so.",
            "First we need more data points because there's currently very few data points here, so it kind of has decided that it doesn't really need that many clusters to explain.",
            "Explain so if you add more and more data points here, hopefully you create a cluster down here.",
            "OK, Ann, among these two classes again, it's it's saying that there's not enough data points here, so I'm not going to use two clusters to explain it.",
            "I'll just use one or one cluster.",
            "So let's add another bunch of points here.",
            "And hopefully it will create a false cluster, right?",
            "So now you see that you know it's.",
            "Decided to use four clusters.",
            "Let's add.",
            "Add more points here now and hopefully it will start to split this cluster into 2.",
            "So it seems to be working right?",
            "So it's decided that there's five classes you can add as more as many classes as you want, and you just keep on using more and more clusters as as it goes along.",
            "So, so that's a infinite mixture model.",
            "Yes.",
            "What?",
            "Well, it's just doing Gibbs sampling.",
            "The question is how do you control the tradeoff when to introduce new clusters, when to not introduce new clusters?",
            "While it's basically just doing Gibbs sampling, so is this exploring the posterior Ann?",
            "When there's not so, remember this diagram where you have like.",
            "I think this is from actually calls lecture work right.",
            "So.",
            "So if you have a small model class then it will explain a small set of a small.",
            "Set of data sets very well.",
            "And then if you have large model plus it will explain the largest datasets, but then of course it has to spread the probability mass around so this thing is kind of similar.",
            "If you have a small number of data points then it will decide that if you just use a small number of mixture components.",
            "To explain that particular.",
            "Dataset well and as you increase as you increase the number of data points, it then realizes that it cannot actually use a small number of clusters to explain it, so it will just increase the number of clusters and it's kind of doing this automatically.",
            "Yes.",
            "The number of data points.",
            "The posterior or the prior?",
            "Yes yeah.",
            "Yes, OK, it turns out that it turns out that the prior problem.",
            "The prior distribution over clusters over the number of clusters that the model wants to use.",
            "If you only tell it an datapoints is going to scale as Alpha times log N, the mean of the number.",
            "That there is a parametric form in fact.",
            "Well, it looks like.",
            "It looks a bit like.",
            "Something like this?",
            "So this may be Alpha log.",
            "And it's going to be OK.",
            "It's not, it's not standard form at all.",
            "Album yes yes.",
            "It also depends on the parameter of, so this parameter Alpha in fact says the larger Alpha is, the more it expects to use.",
            "The number of components it expects to use.",
            "And you can kind of see this from.",
            "From the slight right.",
            "From this slide.",
            "So it's basically saying that the chance of.",
            "Any data item starting its own cluster is going to be larger now.",
            "If you have a large Alpha and so you will see more components in your mixture model in your posterior.",
            "Any other questions yes.",
            "Messages asked about the prior over the number of yes.",
            "Is that the same thing that you can see?",
            "This is the kind of model averaging.",
            "Yes, I guess you could think of it as a kind of model averaging, except that.",
            "The easiest way of thinking.",
            "Thinking about this model is not as model averaging.",
            "But as.",
            "Simply, you have one single model with very large number of clusters, right?",
            "And then when you run your Gibbs sampler it decides not to use most of the clusters.",
            "It will just decide to use a small subset of the clusters.",
            "But even if it were.",
            "Strange.",
            "The number of data points.",
            "Yes, but that's kind of.",
            "Similar to.",
            "That's kind of similar to Gaussian processes too, right?",
            "So you're saying that as the number of data points grows, you expect that to be more.",
            "Information in your data sets, and so you do want to use more and more clusters to explain your data set.",
            "So the number of clusters do grow, so in this case it grows as long as.",
            "OK. Anymore questions yes yes yes.",
            "This business.",
            "Funny thing about what we mean by number of clusters right?",
            "Which means people might be confused about.",
            "So if we have one data point.",
            "The number of possible ways of clustering one data point into clusters is just one, right?",
            "Whereas the number of clusters you might actually believe are in the data in the population of possible data you could observe could still be infected.",
            "Right?",
            "So you know their data point is like a person and you're clustering them.",
            "Their food tastes or something like that in two different kinds of people, like different kinds of food.",
            "Then you know the one person will only belong to one cluster.",
            "So maybe that's a yeah OK, so it's when we say the number of components is kind of saying.",
            "Not the number of components that is in the whole population, but only among the.",
            "Data points that we see only among the people that we see.",
            "Yes, but how do you pick Alpha?",
            "Or do you have a prior on that in the code I just fixed Alpha but you can in fact place apprion, often integrate over.",
            "So in fact this model is quite sensitive to the the setting of Alpha an.",
            "In practice I always put a priority Alpha sample over that 2.",
            "Sensible priors people typically use a gamma problem on Alpha because it tends to work pretty well.",
            "OK.",
            "So that's an infinite mixture model, right?"
        ],
        [
            "So.",
            "Yeah, actually I should.",
            "Go faster, otherwise I want to cover the additional process so when we did this actual infinite limit, right?",
            "The actual infinite limit of the model actually doesn't really make sense, and the reason is basically actually referred to this already, which is that for any particular component mixture components, the prior probability that it will be assigned to.",
            "That any data point will be assigned to it is going to approach zero, and if the prior probability is going to be 0, the posterior is going to be 0 as well.",
            "So in this sense any particular mixture component is not going to be assigned to the data, which is kind of reasonable, right?",
            "So it's kind of like.",
            "A lottery system.",
            "You have a very large number of major components, and they're all fighting for this small set of data points to explain.",
            "Of course, only a small number of them will win, and all the rest will not be assigned.",
            "Any data points and is just going to be unhappy I guess.",
            "So in the Gibbs sampler we kind of bypass this by saying by count lumping all of these different empty clusters together, right?",
            "They're going to form a union and say we want a few data points to explain, and then when the Gibbs sampler assigns them this new data data point is going to pick one of the clusters.",
            "To explain this, this new data point.",
            "So that's kind of a.",
            "This is kind of a bit dumb.",
            "Not very nice in the sense that we are deriving this algorithm and then we are taking the algorithm to the infinite limit and what we want to do in the rest of this lecture is actually to look at better ways of making this infinite limit precise.",
            "Yeah, so that's kind of two ways of looking at this.",
            "Firstly as Zuben already.",
            "Talked about what we can do is to look at the clustering structure that is induced by this richly prior over mixing proportions.",
            "OK, so if you have any data points right this end data points will only be assigned to at most N clusters, so we can just look at how this Model partition this N data points into a small number of clusters and we can look at the prior over this clustering structure that's induced by this richly.",
            "Prior over mixing proportions, and then that's going to turn out to be a process called the Chinese restaurant process.",
            "Another way is that we actually want to reorder the components.",
            "So that those with larger mixing proportions tend to occur first.",
            "And then we can take the infinite limit and this turns out to be equivalent to something called the stick breaking construction.",
            "You see lots of different metaphors in basic nonparametrics, an explain what is meant by a stick breaking and what's meant by a Chinese restaurant later, yes.",
            "So so in the mix in the mixture model that we have here."
        ],
        [
            "We are assuming a prior for PIE which is a symmetric Dirichlet write an recall from day flies lecture that if you draw a sample from this symmetric Dirichlet is not going to be symmetric, right?",
            "So Pi one could take on value 0.2 and \u03c0 two could be 0.01.",
            "Pie tree could be 0.5 and so on.",
            "And then if you imagine reordering those after you've sampled from Italy, so that Taiwan is tends to be larger than Pi 2.",
            "And Pi 2 tends to be larger than pie tree and so on.",
            "And now you can take the infinite limit.",
            "OK."
        ],
        [
            "So it turns out that both of these different views are both of these are different views of this.",
            "Object called the duration, a process which will spend the next 25 minutes on an additional processors can basically be thought of as this infinite dimensional duration distribution, just as the Gaussian process can be thought of as a infinite dimensional Gaussian distribution.",
            "And this gives sampler that we just derived.",
            "Here is basically the.",
            "The basic gift sampler for additional processed mixture model.",
            "So this has anybody.",
            "Does anybody know a little bit about?"
        ],
        [
            "Measure theory.",
            "Great, most people have so, but I guess not everybody has, so I'll still go through the very basic concepts.",
            "So that's all we need really.",
            "So a measure.",
            "Is basically."
        ],
        [
            "You know, think of it as a ruler.",
            "It tells you how large a set is, right so?",
            "It's on the real line.",
            "The typical measure just says what is the length of an interval?",
            "What's the length of a subset of the real line?",
            "And if you think about.",
            "A measure ask of this function which just tells you what is the length of a subset.",
            "Then it has to satisfy a few really basic properties, which is kind of reasonable right?",
            "Firstly, the length of an empty subset is of course going to be 0.",
            "Right, and then if we have disjoint subsets?",
            "Then the length of the Union is going to be the sum of the lengths of the individual subsets.",
            "That's kind of reasonable to write and.",
            "A probability measure is simply one where the total length of the whole space is going to be one.",
            "So the length now is going to be the probability of an event.",
            "A subset is an event, so.",
            "The probability of.",
            "Over the whole event, space is going to be one because it has to integrate to one as a probability distribution.",
            "And it turns out that we can't really form most spaces that we're interested in.",
            "For example, the real line.",
            "We can't actually define a measure over all possible subsets of the real line, so we have to restrict ourselves ourselves to kind of family of subsets that.",
            "Termed measurable.",
            "An so that was called a Sigma algebra.",
            "So a Sigma algebra is simply a family of subsets of a set Theta such that.",
            "Of course it cannot be empty.",
            "We there has to be some things for which you can measure.",
            "An if a subset is in our Sigma algebra, then it's compliments has to be in it as well.",
            "And then finally.",
            "If we have a sequence of subsets, then their union is also going to be our.",
            "Is also going to be measurable.",
            "OK, so the fact that this is a sequence is kind of important, so you can't have a uncountable union of things, so any countable union of measurable sets is also measurable basically.",
            "Um?",
            "Right so then finally.",
            "Well, let's not worry about what's meant by a measurable function.",
            "It in fact just says that if you have two measurable spaces, Theta and Delta, then a function is measurable.",
            "If its inverse images are measurable for any.",
            "Measurable subset A of the second of the output space."
        ],
        [
            "Anne.",
            "So now let's see.",
            "Restrict ourselves to probability measures now and think about what is meant by a random variable.",
            "Turns out that a random variable is actually not random at all, it's simply.",
            "A measurable function.",
            "OK so.",
            "The way you can think of this as is as follows, so.",
            "So how would you implement?",
            "A function in Matlab say or in C that that generates, say, random draws from from a gamma distribution.",
            "Right, so the way you would do is you would actually write down this function, which takes.",
            "Takes a sequence of.",
            "Takes actually.",
            "Actually I'm not sure how do you generate from a distribution.",
            "So here's a here's an easier one.",
            "So how would you generate from a Gaussian with a mean of 10?",
            "Anybody can?",
            "Tell me how.",
            "How would you implement static?",
            "At 10 right subtract.",
            "Yeah, OK, yes, so that's right.",
            "So we assume that we have some random number generator.",
            "So in this case random number generator that generates zero mean gaussian's.",
            "And then we simply add 10 to it.",
            "So if you think of the function itself that you've just written is a deterministic function is it's not deterministic, it's a function itself is fixed, right?",
            "And it takes random bits and it returns to you random numbers from the distribution that you're interested in and.",
            "This is kind of the same way that you can think of a random variable, so the probability measure here on this space data, Sigma.",
            "So Sigma.",
            "Here's our Sigma algebra is going to be our random number generator.",
            "And our random variable X is going to be a function which takes our random number generator and spits out random variables.",
            "Random samples of this variable.",
            "OK.",
            "So and then finally, a stochastic process is basically a collection of random variables.",
            "So say XI, one for each I.",
            "And the distinguishing property of a stochastic process from, say, a graphical model is that this index set I can be infinite, and in fact it can be uncountably infinite as we saw in the case of a Gaussian process.",
            "Right, so this turns out to be a tricky issue, so.",
            "Is that how do you even define this uncountably infinite number of random variables?",
            "And how can even ensure that they exist?",
            "And this is where Kolmogorov consistency theorem comes into play, so it tells you that if you can define a joint distribution on any finite subset.",
            "That's consistent in some way.",
            "Then you can extend it to the whole stochastic process.",
            "But of course, there's caveats.",
            "Again, which filtering will tell you about.",
            "An so it turns out that stochastic processes from the call of many based on non parametric models.",
            "So things like Gaussian processes and today will be talking about judicial processes and on the next lecture tell you a bit about beta processes and parts and processes.",
            "So another thing which I like you too.",
            "To remember is that here I simply define a measure.",
            "As a function from our Sigma algebra to the positive real numbers.",
            "Right, so it's just a function, so.",
            "On the next slide way, I said, why is this a function OK, two slides from now."
        ],
        [
            "So I need to re introduce to you as additional distribution.",
            "I think you're all very familiar with this now.",
            "After about 2 hours of talking about distributions.",
            "So it's simply a distribution over this K dimensional probability simplex, a probability simplex being a set of vectors of length K that such that every entry is non zero and they sum to one.",
            "And we say that a random variable Pi wanted by K is richly distributed with this parameters.",
            "If we can write the density in the following way.",
            "Right?",
            "This is a density on the simplex.",
            "An another way of thinking of what is original distribution is that it's basically equivalent to taking a set of independent gamma variables.",
            "And then just normalizing them.",
            "So that so that the entries sum to One South.",
            "In particular, we're going to take gamma K to be drawn from a gamma distribution with the shape of Lambda K and the scale of 1.",
            "And then we're just going to take this as independent, and then we're going to just normalize it, and then this is going to form a sample from our original distribution with exactly the same set of parameters."
        ],
        [
            "So here's a diagrams for to show how do the richly distributions look like when the when the when the Lambda parameters are all equal to 1, we have a uniform distribution over the simplex.",
            "When the Lambda parameters are greater than one.",
            "Then we have.",
            "Coffee, a unimodal distribution over the simplex, so.",
            "Red here and black here means high probability, high density and yellow and white means low low density and the mean of this distribution simply going to be the Lambda vector.",
            "If you take the Lambda."
        ],
        [
            "Back then you normalize them, then that's going to be the mean of the delay distribution.",
            "And the sum of the Lambda vector.",
            "Is going to be the more or less the inverse?"
        ],
        [
            "Variants of it.",
            "So the spread around the mean.",
            "The interesting thing is that when the lambdas go below 0, so in this case it's 0.7 zero point 7, zero point 7 then.",
            "It's not a unimodal distribution anymore.",
            "In fact, the math gets concentrated at the corners of the probability simplex, and this is another way of saying that the derivative distributions sparse.",
            "When Lambda is less than.",
            "One right, because it's kind of saying that.",
            "So one of these corners means that one of the entries is close to one and both of the other entries are going to be close to 0."
        ],
        [
            "Right, so I'll just tell you what's the judicial process?",
            "Is simply a random probability measure.",
            "Such that for any finite set of partitions of our space, so we start off with a space Theta.",
            "An if we if we take any finite set of measurable partitions, So what that means is that we have a finite number of subsets such that they are all disjoint and their union forms the whole space.",
            "OK, then we can form this vector G of A1 and 2G of a K. This is a random factor because G is random, right?",
            "So we have fixed our finite measurable partition 8128K.",
            "But G is a random probability measure, so this is a random vector.",
            "OK. And this is a random vector that has to sum to one an.",
            "Every entry has to be non negative.",
            "Because it's a probability measure.",
            "And we say that this G. It's going to be a draft is going to be additional process if each of these vectors is going to be directly distributed with parameters given by Lambda A1 until Lambda AKA where Lambda is some some measure over our space, yes?",
            "So what exactly do you mean by a random random?",
            "I mean yes.",
            "Yes, yeah it is so.",
            "It's basically if you take any fixed subsets.",
            "Right then the probability that is going to assign to this subset is going to be random.",
            "So another way of thinking about this is, recall that a probability measure is simply a function from our Sigma algebra to the positive real numbers.",
            "Write an just as in the Gaussian process we have defined as a random function as.",
            "So in the Gaussian process we have F of X.",
            "This is a random function.",
            "If.",
            "Yeah, if every entry here is random and there's a joint distribution over all of this.",
            "A random variables, so in this case we have G which is a measure, so it's a function from the measurable subsets to the positive lines.",
            "So we have G of.",
            "A.",
            "Where a is a measurable set, so it belongs to this Sigma algebra over our space.",
            "So we have again an uncountably.",
            "Infinite number of random variables, one for every measurable subset.",
            "An they all have to satisfy the property that if you take any measurable partition then that.",
            "Joints distribution over this finite number of.",
            "Random variables coming from this infinite set.",
            "This is going to be a jewishly distributed.",
            "So here's come for a visualization of a partition of a space being the rectangle.",
            "So this above family of distributions here, in which for every partition we have Additionally distribution over.",
            "The partition turns out to be consistent, so I'll yes, sorry I missed this, but what do you mean by the dot of the unions in law?",
            "In the second?",
            "It just means that they are disjoint completely.",
            "Yeah, they don't have intersection.",
            "So because this family of distributions is consistent given by this thing, we can now.",
            "Apply Kolmogorov consistency theorem to say that to show that there isn't that the Dirichlet process, this exist OK.",
            "So how do we see that this?",
            "Lee after running out of time, really.",
            "Al OK.",
            "So yeah, so maybe I'll try to go to why is it that this rich data distributions are consistent?",
            "And then I might.",
            "Finish up this in the next lecture.",
            "As eraser.",
            "OK, so.",
            "So what's meant by?"
        ],
        [
            "Consistent here.",
            "Um?",
            "So imagine that we have our space right?",
            "An OK, let's look at one partition, say.",
            "Something like this?",
            "OK, this is going to be a 18283.",
            "So according to the definition.",
            "Here.",
            "We're going to have G of.",
            "A1.",
            "In fact, this G. Of K2G RAT.",
            "Right, so GFA one is simply the random mass.",
            "There's going to be assigned to this a one partition and G of a two simply going to be the random Masters assigned here and here is G of a tree.",
            "Alright, so this vector we want it to be richly distributed.",
            "With parameters given by Lambda A1.",
            "#8 two.",
            "Lambda.",
            "A tree OK?",
            "And now let's look at another partition.",
            "Say something like this is BB1 and B2.",
            "OK, so again, we're going to have the same thing where G of B1.",
            "Your feet too.",
            "We want.",
            "This vector to be directly distributed.",
            "With parameters Lambda B1.",
            "Honda V2 OK. An by consistency, what I mean is that we want that to be a single function.",
            "A single random variable.",
            "This random variable is a function, right?",
            "So a single function G. That makes both of this.",
            "Things.",
            "Makes the marginal distribution over makes these two.",
            "Makes both of these marginal distributions hold OK. And we can kind of see this if we just do the following.",
            "So we will take a common refinement of both of these partitions so.",
            "Here's our.",
            "A partition.",
            "And here's a big partition.",
            "Write something like that so I'll form a common refinement.",
            "So I'll call this CCC1C2C3C4C5.",
            "So we see that a two is simply the Union of these two, and a tree is the Union of this two, and B1 is the union of this tree.",
            "Ann B2 is the union of these two.",
            "So.",
            "Now for this partition we also have this property holding.",
            "So what we see is that G of C1G of C2G of C3.",
            "Jessie fall this thing is deliciously.",
            "Alpha bit long to write down.",
            "Lambda A1C1C.",
            "Lambda C2 the city.",
            "C4C5 OK, so this thing has to be directly distributed.",
            "For this common refinement as well.",
            "And we know that if we take.",
            "G of C2 and G of C4 and we add up.",
            "Those two entries were going to get G of a one.",
            "Right, because it's G itself is a is a measure, so the measure.",
            "So a 2 being the disjoint union of C2 and C4, the measure of.",
            "A2 is going to be the measure of C2 plus the measure of C4 and similarly G of a tree is going to be equal to G of C3 plus.",
            "We'll see 5.",
            "And then G of B1 here is going to be some of this one, this one and that one.",
            "As GOP, this can be some of those two.",
            "It turns out that the duration distribution has this really nice property where if you take any two entries in the original distribution and you add them up.",
            "Then the resulting random factor is also directly distributed where you simply take the corresponding entry or the parameters.",
            "Then you add them up.",
            "So Lambda A1.",
            "So this G of a 2 here is going to be some of C4 and C5, so we're going to just take Lambda C4C5.",
            "We're going to add them.",
            "And that's going to form our corresponding entry of arguably the parameter for our original distribution, and Similarly for this two.",
            "So I got it wrong.",
            "So this goes to.",
            "There's two and this goes to that to write an similarly for over here too.",
            "So Lambda B1 is going to be Lambda, C1C2 and C3.",
            "And this is going to.",
            "Right, so since this is the richly distributed with those parameters, when when we add up.",
            "The entries of this vector this smaller vector is also going to be distributed with exactly the parameters that we started off with, and Similarly for this, and because we both of these are simply marginal distributions coming from the same.",
            "Joints distribution over this set of five random variables we know that this one is going to be consistent with this.",
            "Right, they're both marginals of the same joint distribution.",
            "OK, and because of this consistency we can now apply Kolmogorov consistency theorem.",
            "Yes, consistency.",
            "I'm saying we can put probability and we can define measures on that probability measures satisfy the conditions.",
            "So what we what guaranteed to exist by the theorem, does it?",
            "That theorem say certain kind of measure space exists or so."
        ],
        [
            "Yeah, so this there in fact says that there is.",
            "A.",
            "Measure space with a particular Sigma algebra for which.",
            "For which any finite.",
            "Set of random variables is forest.",
            "The marginal distribution for that finite self for any finite server of random variables, as is given by you right when you define it?",
            "So I started off by saying that, OK, I want.",
            "At random probability, measure G is a stochastic process that satisfies all of these properties for any partition.",
            "This thing has to be directly distributed and this consistency Theorem says that that is in fact.",
            "So I haven't really defined for you.",
            "What is the joint distribution over the whole random probability measure?",
            "Gee, I've only told you what the finite marginals look like, and this theorem basically says that there is a.",
            "A joint distribution over the whole of G such that.",
            "The finite dimensional marginals under this stochastic process.",
            "IS has marginals given by?",
            "Given by this richly basically, but I think Peter would really go into detail about what's the problem that I haven't really told you.",
            "The problem.",
            "I've saved the.",
            "The.",
            "Let's do it.",
            "Yeah, I'll see."
        ],
        [
            "Peter OK so.",
            "Yep, So what we're going to do now?",
            "Yes.",
            "Lambda is a measure.",
            "It doesn't have to sum to one.",
            "Right?",
            "So what we're going to do now is the following so.",
            "I'm gonna start without my space.",
            "And then.",
            "I'm going to come like recursively partition this space into smaller into more and more refined partitions.",
            "So you can imagine first splitting it into two and then splitting into four and then 8.",
            "And so on.",
            "OK and?",
            "At any step of this.",
            "Process we are going to have additional distribution over this finite partition.",
            "But if we keep on splitting each of these subsets into smaller into a.",
            "Smaller and smaller sets.",
            "At some point we're going to see that we will be able to define all a.",
            "The probability measure over the whole space.",
            "And that's what we're going to do.",
            "OK so here.",
            "My space is gonna be.",
            "Just a unit interval between zero and one.",
            "And the area of this rectangle here so this thing goes from zero to 1 here as well.",
            "So the area under this rectangle is going to be the probability mass get.",
            "That's going to be assigned to this unit interval.",
            "And of course this area here is going to be 1 * 1 is going to be one, so that's OK, right?",
            "So we start off with a with a probability measure that assigns probability of 1 to the whole space, yes?",
            "We mentioned that we sampled from the replay process.",
            "Sample constitutes apart finite partition of the space and the probability measure know it.",
            "A sample is a whole probability measure, so that on any finite partition.",
            "Yeah, yeah, it's for all for all finite partitions, so here will try to draw a richly process.",
            "So what this thing says is that the.",
            "For this drawing the judicial process, the total mass that is going to assign to the unit interval, it's going to be one which is OK because it has to be a probability measure.",
            "So the next thing we do is we're going to take this unit interval.",
            "We're going to split into 2.",
            "And then we're going to ask how much mass is it going to assign to?",
            "Between zero and.",
            "And a half and how much mass does it want to assign to?",
            "1/2 to one.",
            "And we can actually sample this.",
            "Just put it over here.",
            "OK, so now it says that it wants to assign a bit more mass to the to the first half of the interval as opposed to the second half and the amount is curve.",
            "Push this mess up.",
            "Here is the same as the amount that is pushed it down here, so the total mass is still one.",
            "Now we're going to take the first half of the interval, and we're going to ask how much mass do you want to assign to the.",
            "First quarter versus the second quarter, and then for the second half we ask how much math do you want to assign to the third quarter versus the fourth quarter.",
            "So we might do this so in the first two quarters is kind of like basically the same.",
            "Right, that's the same amount of mass in the first two quarters, but then is saying that the fourth quarter it wants to assign just a little bit of mass two, and then we can keep on repeating this, splitting each interval into halves.",
            "Anne, if you do this lots of time.",
            "OK OK, what do we see?",
            "So we're going to see that.",
            "So this is in fact a random sample from a directly process.",
            "OK, so it's going to.",
            "In fact, BS series of of this Delta functions Delta function being.",
            "Point mass at.",
            "Is this saying that with probability given by the height of this thing is going to take on value exactly given by the by the location of that of that spike there?",
            "OK.",
            "So.",
            "And with a smaller probability, it's going to be given a value.",
            "That's given by the location of that Spike.",
            "What?",
            "Speed slow.",
            "I'm, I'll wrap up soon."
        ],
        [
            "OK so I said Oh yeah, OK. Yeah, maybe I'll stop here actually.",
            "So, So what this says is that.",
            "We know that the richly processes exist, and we know that if we generate samples from it then it will always look like a weighted sum of point methods, and I'll next week I will."
        ],
        [
            "Actually, try to go to some.",
            "The parameters of the dish."
        ],
        [
            "The process, as well as different representations of it before I go onto.",
            "To the other processes.",
            "Thank you, yes.",
            "Any point?",
            "Yes, in fact it turns out there is always countably infinite number of points masses.",
            "When you draw from the judicial process.",
            "Yeah, so in fact you always look like this.",
            "So a sample from the duration process is always going to be an infinite sum of.",
            "Points methods each located at the data key and weight.",
            "The mass of that point mass is going to be given by paikea.",
            "OK. Yeah, so we'll go into representations of the judicial process next week.",
            "Any other questions?",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello hello.",
                    "label": 0
                },
                {
                    "sent": "Can everybody hear me with my mic?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll first like to thank the organizers for giving me the opportunity to to tell you about the area that I've been working on.",
                    "label": 0
                },
                {
                    "sent": "My name is Eyt.",
                    "label": 0
                },
                {
                    "sent": "I'm at Gatsby unit just South of here in London, and I'll be telling you about an introduction to Bayesian nonparametric modeling.",
                    "label": 1
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "Next week Peter Banks will also tell you about some foundational stuff on basic nonparametrics as well, so this lecture will be kind of more introductory.",
                    "label": 0
                },
                {
                    "sent": "Just telling you giving a bit you a bit of an idea of how what are basic nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "And then after Peter Robbins Lecture, I will tell you a bit more more advanced models.",
                    "label": 0
                },
                {
                    "sent": "On Thursday.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the outline for today's talk.",
                    "label": 0
                },
                {
                    "sent": "First tell you about some examples of parametric models that you have probably seen by now.",
                    "label": 0
                },
                {
                    "sent": "Some in earlier parts of the of the summer school, and then if you a bit of introduction of Bayesian nonparametrics an as an example, I'll kind of go through in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "Particular model called the Infinite mixture model and then I'll introduce the mean part of this talk, which is the derivative process.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some examples of parametric models, so I think you guys have seen this from Carl Rasmussen's talk, right?",
                    "label": 0
                },
                {
                    "sent": "So suppose that we want to do supervised.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning and we want to learn a function from some input space X2 and output space Y.",
                    "label": 0
                },
                {
                    "sent": "So for example, the function might look like this.",
                    "label": 0
                },
                {
                    "sent": "And we are given a bunch of training data, so X one is here and why one is the height of this point, for example.",
                    "label": 1
                },
                {
                    "sent": "And this X2 and Y2 and so on, right?",
                    "label": 0
                },
                {
                    "sent": "And then we want to fit a function which goes through this set of lines.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the typical parametric way of doing things would be.",
                    "label": 0
                },
                {
                    "sent": "Let's start with a set of basis functions, right?",
                    "label": 1
                },
                {
                    "sent": "So this is a set of functions which can which describe kind of basic shapes from which we can build our functions from, right?",
                    "label": 0
                },
                {
                    "sent": "So then we say that we will parameterized function F of X using a set of parameters W as some linear combination of the basis functions.",
                    "label": 1
                },
                {
                    "sent": "And here, of course, the parameters RW an if we're just doing something very simple, we might say we want to find a set of parameters.",
                    "label": 0
                },
                {
                    "sent": "Search that our training output Yi is as close as possible to the function evaluated at the training input XI.",
                    "label": 0
                },
                {
                    "sent": "An of course will use the squared norm because it's the simplest one and want to minimize this quantity over our training sets.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 1
                },
                {
                    "sent": "So that's very simple, but will be a Bayesian in this lecture.",
                    "label": 0
                },
                {
                    "sent": "So just following calls footsteps.",
                    "label": 0
                },
                {
                    "sent": "So instead will rephrase this using a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "In particular what we would say is that our output, why I given our input XI an R parameters W?",
                    "label": 0
                },
                {
                    "sent": "Will be simply we will have a mean given by the function evaluated at XI.",
                    "label": 0
                },
                {
                    "sent": "And plus some noise term epsilon I which is drawn from some Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And then we'll also place a prior on our parameters, so WK will be some, some Gaussian will be Gaussian distributed with a 0 mean and some other variance term Town Square.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is compute the posterior as you've heard in lecture.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's just look at the form of the function that we're fitting is a linear combination of K basis functions.",
                    "label": 0
                },
                {
                    "sent": "So firstly, what basis functions should we use, right?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a number of questions that we can ask ourselves whether this is a good model or not.",
                    "label": 0
                },
                {
                    "sent": "Firstly, what basis functions to use?",
                    "label": 1
                },
                {
                    "sent": "You know you could be using, say, Gaussian shaped basis functions then that basically gives us the assumption that.",
                    "label": 0
                },
                {
                    "sent": "All functions are smooth, right?",
                    "label": 0
                },
                {
                    "sent": "Or we could use things like that kind of triangular shaped and then that might give us piecewise linear functions for example.",
                    "label": 1
                },
                {
                    "sent": "The second question is how many basis functions do we use, right?",
                    "label": 0
                },
                {
                    "sent": "So the innocence?",
                    "label": 0
                },
                {
                    "sent": "This number of basis functions K tells us the complexity class of our approximation of our class of functions that we are approximating the true function with.",
                    "label": 0
                },
                {
                    "sent": "Thirdly, do we actually have a belief that?",
                    "label": 0
                },
                {
                    "sent": "The true function F star can be expressed as a linear combination of basis functions for some set of parameters W. So this is typically unlikely because as we know from cows lecture, the typical true function is typically very complicated and very hard to approximate anyway, so that's very unlikely that we can come up with a finite set of basis functions that can approximate our true function without knowing it.",
                    "label": 0
                },
                {
                    "sent": "Without knowing the true function to begin with.",
                    "label": 0
                },
                {
                    "sent": "And finally, we even believe that the noise process here is Gaussian, right?",
                    "label": 1
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What if we don't actually believe that the noise processes goes in?",
                    "label": 0
                },
                {
                    "sent": "What can we do about this so this kind of questions about?",
                    "label": 0
                },
                {
                    "sent": "So we're kind of starting off with this parametric family or functions, and these are questions about whether we want to go beyond this particular parametric family an of course, Gaussian process is a way of going beyond a parametric family to a non parametric family.",
                    "label": 0
                },
                {
                    "sent": "Metric family functions.",
                    "label": 0
                },
                {
                    "sent": "So here's another example.",
                    "label": 0
                },
                {
                    "sent": "Say we want to do density estimation OK.",
                    "label": 0
                },
                {
                    "sent": "So this is an unsupervised learning task now.",
                    "label": 0
                },
                {
                    "sent": "An we are we are we supposed that there's some true underlying density F star say something like this an from which we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have obtained IID samples from so this.",
                    "label": 0
                },
                {
                    "sent": "This forms are training set XI.",
                    "label": 0
                },
                {
                    "sent": "So the dots here are kind of the ID samples from F star and what we want to do is to somehow recover the underlying density.",
                    "label": 0
                },
                {
                    "sent": "Here App Store from our training set.",
                    "label": 0
                },
                {
                    "sent": "Again, the simplest thing we might want to do is to assume some sort of exponential family distribution, say so for example, something like a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this is the Gaussian parameterized by its mean and its covariance.",
                    "label": 0
                },
                {
                    "sent": "It has a normalization term up here and then.",
                    "label": 0
                },
                {
                    "sent": "Here is the.",
                    "label": 0
                },
                {
                    "sent": "Well, everybody knows this I think anyway, so what's the problem with the Gaussian?",
                    "label": 0
                },
                {
                    "sent": "Firstly, is Union model right?",
                    "label": 0
                },
                {
                    "sent": "So for something like this, where is multimodal?",
                    "label": 0
                },
                {
                    "sent": "It will definitely not be able to fit the data, so you just give it a single bump instead of three bumps.",
                    "label": 0
                },
                {
                    "sent": "Secondly, the shape is quite restrictive, right?",
                    "label": 0
                },
                {
                    "sent": "So in high dimensions we know that the shape of a Gaussian basically looks at.",
                    "label": 0
                },
                {
                    "sent": "Flat pancake or ball ball or something?",
                    "label": 0
                },
                {
                    "sent": "Rugby rugby balls.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And of course it holds.",
                    "label": 0
                },
                {
                    "sent": "It has things like pretty light tails.",
                    "label": 0
                },
                {
                    "sent": "Basically as X goes to Infinity then the probability of X goes down really quickly as a function of X.",
                    "label": 0
                },
                {
                    "sent": "Instead, well, we might want to do is to use a mixture model, right?",
                    "label": 1
                },
                {
                    "sent": "So will parameterise a density F of X using a mixture of K different Gaussians for example.",
                    "label": 0
                },
                {
                    "sent": "An paije here are our.",
                    "label": 0
                },
                {
                    "sent": "Oh, I guess it should be \u03c0 K. That's why there's a typo.",
                    "label": 0
                },
                {
                    "sent": "Should be.",
                    "label": 0
                },
                {
                    "sent": "The mixing proportion is the probability that any particular data point will belong to mixture components K. And make sure component K is given by a Gaussian with mean mu K and the covariance of Sigma K. Again, we might want to ask questions about whether we want to be restricted to this parametric class of densities.",
                    "label": 0
                },
                {
                    "sent": "So do we really believe that the true density is a mixture of components?",
                    "label": 1
                },
                {
                    "sent": "Again, unlikely not to be the case.",
                    "label": 0
                },
                {
                    "sent": "And even if we do believe there is some issue OK components, how many major components do we want to use, right?",
                    "label": 0
                },
                {
                    "sent": "It's kind of hard to pick a key using model selection or solve techniques.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yet another example.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "We want to do latent variable modeling right?",
                    "label": 0
                },
                {
                    "sent": "So say we have N vectors of.",
                    "label": 1
                },
                {
                    "sent": "And vector observations X one to XN.",
                    "label": 0
                },
                {
                    "sent": "An we want to model each as a linear combination of K latent sources or factors or features or something so XI.",
                    "label": 1
                },
                {
                    "sent": "Is going to be some from of K from one to Big K. Of basis function Lambda K * y IK.",
                    "label": 0
                },
                {
                    "sent": "So why here is going to be our latent source activity?",
                    "label": 0
                },
                {
                    "sent": "Plus some nice term epsilon.",
                    "label": 1
                },
                {
                    "sent": "I so examples of this sort of latent variable models include things like principle components, analysis, factor analysis, independent independent components analysis, right?",
                    "label": 0
                },
                {
                    "sent": "So these are pretty standard techniques in machine learning and statistics.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Again, we might want to ask how many sources are there really in our data, right?",
                    "label": 1
                },
                {
                    "sent": "Do we really believe that there are case sources in our data?",
                    "label": 0
                },
                {
                    "sent": "How do we find a K even if we do believe that it's OK, an another question might be what prior distribution should we use for these sources?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Finally, yet another example of a parametric model that you now know all about after they flies lectures is topic modeling with latent additional allocation, right?",
                    "label": 0
                },
                {
                    "sent": "So here we the task is that we want to infer topics from a document corpus topics.",
                    "label": 1
                },
                {
                    "sent": "Now being kind of a set of words that tend to Co occur together in our in different documents and the.",
                    "label": 0
                },
                {
                    "sent": "The model basically goes as follows, so we have outer plate here that denotes document OK and Pi J is going to be a distribution over topics for that document J.",
                    "label": 0
                },
                {
                    "sent": "And then for every word in a document, what we'll do is we'll first pick a topic by drawing from a multinomial given by \u03c0 J and then we'll draw a word given the topic indicator that Ji.",
                    "label": 0
                },
                {
                    "sent": "From a distribution given from distributions parameterized by Theta K, so I think probably.",
                    "label": 0
                },
                {
                    "sent": "The notation I use is a bit different than device lecture, but you can easily translate within them an again we will try to be Bayesian, so we'll place priors on Theta and of course by itself has a directly prior and will place additional prior on Theta 2.",
                    "label": 0
                },
                {
                    "sent": "Again, we might want to ask how many topics can we find from the corpus.",
                    "label": 1
                },
                {
                    "sent": "We even believe that our corpus only has K topics.",
                    "label": 0
                },
                {
                    "sent": "Maybe it has modern K, or maybe it has infinitely many topics possibly.",
                    "label": 0
                },
                {
                    "sent": "Right, so these are all parametric models.",
                    "label": 0
                },
                {
                    "sent": "So now will go into a basin on Parametrics actually.",
                    "label": 0
                },
                {
                    "sent": "Is there any questions on the model so far?",
                    "label": 0
                },
                {
                    "sent": "No, OK, it's pretty simple stuff I guess, so will in fact see that for each of the four examples that we'll give, we'll try to derive nonparametric models for each of them.",
                    "label": 0
                },
                {
                    "sent": "You're already know the answer for one of them, right?",
                    "label": 0
                },
                {
                    "sent": "So for the regression case, it's just a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bayesian nonparametrics, so that's kind of a few philosophical points about basic nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Why we want to do based on parametrics.",
                    "label": 0
                },
                {
                    "sent": "Firstly.",
                    "label": 0
                },
                {
                    "sent": "We tend to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leave that models are almost never correct for our real data.",
                    "label": 1
                },
                {
                    "sent": "And what happens if they are?",
                    "label": 0
                },
                {
                    "sent": "What happens if they're not correct?",
                    "label": 0
                },
                {
                    "sent": "How do we deal with model Misfit?",
                    "label": 1
                },
                {
                    "sent": "We can try to do things like the following.",
                    "label": 1
                },
                {
                    "sent": "So firstly we can try to quantify how far away is our model from the true model.",
                    "label": 0
                },
                {
                    "sent": "Some self measure of optimality of fitted model.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we want to do things like model selection or averaging so we have a.",
                    "label": 0
                },
                {
                    "sent": "Have a sequence of models of increasing complexity and we want to find the model of the right complexity to describe our data.",
                    "label": 1
                },
                {
                    "sent": "Or in the case of averaging, we Mount the average over this sequence of models have a Bayesian averaging.",
                    "label": 0
                },
                {
                    "sent": "Certainly we might want to not restrict ourselves to a small set of models now, right?",
                    "label": 0
                },
                {
                    "sent": "So to a small model class, we might want to increase the flexibility of our model class.",
                    "label": 0
                },
                {
                    "sent": "An basic nonparametric models are basically parametric models whose flexibility has been increased whole lot so that they are, in a sense, able to capture almost any.",
                    "label": 0
                },
                {
                    "sent": "Data set that we might encounter.",
                    "label": 1
                },
                {
                    "sent": "OK. And they are also good solutions from the perspective of model selection or averaging.",
                    "label": 0
                },
                {
                    "sent": "In fact, there is a different way of approaching this problem of finding a good.",
                    "label": 0
                },
                {
                    "sent": "Complexity follow for model.",
                    "label": 0
                },
                {
                    "sent": "In fact it just says integrate everything out and not worry about overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you don't have to do model selection or averaging or something, so it makes your life much simpler.",
                    "label": 0
                },
                {
                    "sent": "So let's in fact go into a bit of detail about why we might not want to do model selection or averaging, at least from the Bayesian perspective, right?",
                    "label": 0
                },
                {
                    "sent": "So let's just imagine that we have our data X again, so X one to XN and we have a sequence of models MK, each parameterized by Theta K for K = 1, two and three, and so on.",
                    "label": 1
                },
                {
                    "sent": "So to do model selection or averaging we will.",
                    "label": 0
                },
                {
                    "sent": "We want to compute the marginal probability of our data, so P of X given MK is simply the integral.",
                    "label": 0
                },
                {
                    "sent": "Over Theta K of the probability of our data under our model, an hour set of parameters.",
                    "label": 0
                },
                {
                    "sent": "An there should be a P in here, so P of data.",
                    "label": 0
                },
                {
                    "sent": "Given MK, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Typos again, so is the likelihood multiplied by the prior over Theta and we integrate over data.",
                    "label": 0
                },
                {
                    "sent": "So that's our marginal probability of our data integrating all of parameters and then we want to do model selection, in which case we want to find.",
                    "label": 0
                },
                {
                    "sent": "The Model MK, which maximizes the probability of our data given our model.",
                    "label": 0
                },
                {
                    "sent": "Or if we want to be more Bayesian, we want to.",
                    "label": 0
                },
                {
                    "sent": "Computes the posterior over model index K, an hour parameters Theta K by just multiplying the prior over Theta and K. By the likelihood and then normalizing.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this marginal probability is very hard to compute for almost all models of interest.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure whether Ian talked about whether it's hard to compute this.",
                    "label": 0
                },
                {
                    "sent": "Probably Zubin did.",
                    "label": 0
                },
                {
                    "sent": "So for us to do model selection or averaging will always need to compute this thing right so?",
                    "label": 1
                },
                {
                    "sent": "And the idea is that for model selection is to prevent overfitting or an underfitting by finding a model of the right complexity class to explain our data.",
                    "label": 1
                },
                {
                    "sent": "And because it's because this marginal probability is hard to compute, is expensive to compute, we don't really want to do this too often.",
                    "label": 0
                },
                {
                    "sent": "So, so there's one insight that was noted by a few people, including color Smithson Zuben.",
                    "label": 0
                },
                {
                    "sent": "Most recently I think not that recently I guess.",
                    "label": 0
                },
                {
                    "sent": "People before that too, like Dave Mackay and revenue.",
                    "label": 0
                },
                {
                    "sent": "Is that because we are integrating all of our parameters when we do this computation?",
                    "label": 0
                },
                {
                    "sent": "When we're being Bayesian.",
                    "label": 0
                },
                {
                    "sent": "We're not going to overfit anyways because we've integrated out of our parameters.",
                    "label": 0
                },
                {
                    "sent": "There's nothing to fit through our data so that we can't really overfit.",
                    "label": 0
                },
                {
                    "sent": "If we can't really overfit, then what is it?",
                    "label": 0
                },
                {
                    "sent": "What is to prevent us from using a really large model, right?",
                    "label": 0
                },
                {
                    "sent": "Because if we start off with a really large model then we won't under fit to our data.",
                    "label": 0
                },
                {
                    "sent": "And we won't overfit either because we're being Bayesian, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the basic idea of why we want to do basic nonparametrics from this perspective.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So right?",
                    "label": 0
                },
                {
                    "sent": "So what is a nonparametric model nonparametric model is basically a. Parametric model that's really large, so where the number of parameters is either infinite or increases with the data that we see.",
                    "label": 1
                },
                {
                    "sent": "We can also think of it as a model over and some sort of infinite dimensional function or measure space so you've seen an example of this in the case of a Gaussian process, right?",
                    "label": 0
                },
                {
                    "sent": "So call places a prior distribution over all possible functions that's given by a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So this is an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In fact it's a countably infinite uncountably infinite dimensional space race.",
                    "label": 0
                },
                {
                    "sent": "We need to have a latent variable.",
                    "label": 0
                },
                {
                    "sent": "For every input.",
                    "label": 0
                },
                {
                    "sent": "Of the function.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Another way of thinking about what is an operating model is a family of distributions.",
                    "label": 1
                },
                {
                    "sent": "That is dense in some large space, so a dense in the sense that if you give me any.",
                    "label": 0
                },
                {
                    "sent": "Any other if you give me any distribution, I can find a distribution in this family that's as close as you want to to your distribution, OK?",
                    "label": 0
                },
                {
                    "sent": "And so the idea is that with a parametric family, we're parameterising it with a finite number of parameters.",
                    "label": 0
                },
                {
                    "sent": "So we can think of family of distributions as a very low dimensional manifold in our.",
                    "label": 0
                },
                {
                    "sent": "In this large space of possible distributions, and if you have a small if you have a low dimensional manifold, you're not going to be able to.",
                    "label": 0
                },
                {
                    "sent": "To approximate any distribution to arbitrary accuracy.",
                    "label": 0
                },
                {
                    "sent": "So with nonparametric models, we are dealing with a family of distributions that so large that it can approximate any distribution to any arbitrary accuracy.",
                    "label": 1
                },
                {
                    "sent": "So why we want to deal with nonparametric models in a Bayesian theory of learning?",
                    "label": 0
                },
                {
                    "sent": "It is a really nice way of sidestepping model selection averaging.",
                    "label": 1
                },
                {
                    "sent": "It's we can also think of it as kind of a broad class of priors that doesn't restrict the model to a parametric family, so it allows the data the data to speak for itself, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have lots of data then you'll get very close to the distribution that is underlying the data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "I personally find it well all of these different views to be cover good ways of thinking about what isn't.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric model but we are faced with this computational issue of we have a really large parameter parameter space.",
                    "label": 0
                },
                {
                    "sent": "Now right infinite dimensional parameter space.",
                    "label": 0
                },
                {
                    "sent": "How do we deal with it?",
                    "label": 1
                },
                {
                    "sent": "Computationally we can fit all of these parameters onto our computer.",
                    "label": 0
                },
                {
                    "sent": "Nor do we really want to because it's just going to take ages to do anything with this.",
                    "label": 0
                },
                {
                    "sent": "So that's got two ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "One is to marginalized all but a finite all by a finite number of parameters from our model, right?",
                    "label": 0
                },
                {
                    "sent": "So we saw this again with the Gaussian process, right?",
                    "label": 0
                },
                {
                    "sent": "So if you observe any training points, then you really only need to compute the posterior of the function.",
                    "label": 0
                },
                {
                    "sent": "On this end, training inputs.",
                    "label": 0
                },
                {
                    "sent": "Rather than all the other potential input points.",
                    "label": 0
                },
                {
                    "sent": "Another way of doing this is to define this infinite space implicitly.",
                    "label": 0
                },
                {
                    "sent": "This is a bit similar to the kernel trick an will be applying two kind of like pretty fundamental theorems.",
                    "label": 0
                },
                {
                    "sent": "1, graph consistency theorem and the other is definitely theorem and I'll actually talk about this later.",
                    "label": 0
                },
                {
                    "sent": "And Peter Robbins will go into detail about these two things too.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've actually seen.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gaussian process is already right, so.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process is basically a random function right F from input space to R. Such that if you're given any finite set of input points X one to XN, then if you look at the joint distribution of F evaluated at X1F evaluated at X2 all the way until F evaluated at XN, then this vector here this random vector, because the function is random, we have a fixed input but random function, so this vectors random 2.",
                    "label": 1
                },
                {
                    "sent": "Is given a Gaussian distribution with a mean given by M of X1.",
                    "label": 0
                },
                {
                    "sent": "Vector which is M of X1 until Emma Vexen.",
                    "label": 0
                },
                {
                    "sent": "So this is our main vector where M is our main function.",
                    "label": 0
                },
                {
                    "sent": "And we have a covariance matrix given by a covariance function C evaluated at our input points.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 1
                },
                {
                    "sent": "The way we think of what a random function is is that it's basically a collection of random variables.",
                    "label": 0
                },
                {
                    "sent": "So for every X potential input X in our input space big X, we have random variable F of X and this is a really large set of random variables, it's A.",
                    "label": 0
                },
                {
                    "sent": "Is cost a stochastic process is basically a set of random variables?",
                    "label": 0
                },
                {
                    "sent": "Another way, so this is a view of the Gaussian of a Gaussian process as.",
                    "label": 0
                },
                {
                    "sent": "A model with an infinite number of parameters.",
                    "label": 0
                },
                {
                    "sent": "So we have uncountably infinite number of parameters here 1F of X for every X.",
                    "label": 1
                },
                {
                    "sent": "This view here.",
                    "label": 0
                },
                {
                    "sent": "Is of a Gaussian process where the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Growth with our data set right as our data set N grows.",
                    "label": 0
                },
                {
                    "sent": "The number of parameters that we're dealing with here also grows linearly in our.",
                    "label": 0
                },
                {
                    "sent": "In yeah, in fact, the number of parameters equals to the number of training input points.",
                    "label": 0
                },
                {
                    "sent": "Another way in which we could express the Gaussian process is as follows.",
                    "label": 0
                },
                {
                    "sent": "We can in fact show that in fact call later today will tell you about how you can express F of X.",
                    "label": 0
                },
                {
                    "sent": "Which is a.",
                    "label": 0
                },
                {
                    "sent": "Which is drawn from a Gaussian process as.",
                    "label": 0
                },
                {
                    "sent": "A finite sum of a finite linear combination of basis functions.",
                    "label": 0
                },
                {
                    "sent": "But then we are going to take the number of basis functions to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this is yet another view of Gaussian process where we think of it as we start off with the parametric model and then we take the number of parameters in this parametric model to go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll see kind of like parallels of this in all the different cases that we see.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'll skip this.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'd like to talk a bit about this.",
                    "label": 0
                },
                {
                    "sent": "So the way we define the Gaussian process here, right?",
                    "label": 0
                },
                {
                    "sent": "So we say that for any input for any set of input points X one to XN we have some vector.",
                    "label": 0
                },
                {
                    "sent": "Here an will say that this vector is given a Gaussian prior.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So we're not really guaranteed that this set of.",
                    "label": 0
                },
                {
                    "sent": "Priors one for any set of inputs to be somehow consistent with each other, and it turns out that Luckily, because we're dealing with Gaussians, he ran when you marginalized out random variables under Gaussian, you still get a Gaussian and out right, and because of this consistency property of Gaussian distributions, we know that all of these marginal distributions are consistent with each other.",
                    "label": 1
                },
                {
                    "sent": "And because they are consistent.",
                    "label": 1
                },
                {
                    "sent": "We can apply this common graph consistency theorem, which kind of guarantees that Gaussian processes exist, IE.",
                    "label": 1
                },
                {
                    "sent": "This whole stochastic process actually exists, so there will be a bit further information and Peter audiences lectures.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So far, yes, does it mean that there is a unique solution that these marginals or there could be made?",
                    "label": 0
                },
                {
                    "sent": "It's unique, yes?",
                    "label": 0
                },
                {
                    "sent": "In fact, Peter will tell you a lot more about that kind of issues with this construction.",
                    "label": 0
                },
                {
                    "sent": "Actually, in that I don't really go into details.",
                    "label": 0
                },
                {
                    "sent": "He'll tell you about it.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is Gaussian process or no nonparametric Bayesian model or not, yes it is.",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "But we still have some parameters we have to believe that all this is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right so.",
                    "label": 0
                },
                {
                    "sent": "A nonparametric model is not really a well.",
                    "label": 0
                },
                {
                    "sent": "The way I think about it is not really a model without any parameters.",
                    "label": 0
                },
                {
                    "sent": "In fact, it has a model that's really primarily in that has a lot of parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm not sure whether that makes sense.",
                    "label": 0
                },
                {
                    "sent": "It's simply not a parametric model in the traditional sense of a finite number of parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So and the other issue is that, of course, we're still assuming that all of this random variables have a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of in a sense unavoidable, because at some point we have to make some assumptions about how our.",
                    "label": 0
                },
                {
                    "sent": "Our data looks like right?",
                    "label": 0
                },
                {
                    "sent": "What is what is a reasonable assumption about the world and we have to make certain assumptions that in the case of nonparametric models, we do have to make quite a bit of assumptions about.",
                    "label": 0
                },
                {
                    "sent": "That the things are kind of nice in some ways, like a conjugate for example because.",
                    "label": 0
                },
                {
                    "sent": "And closed under this marginalization so?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we are nonparametric.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as we are not presently in some directions but parametric in some other directions.",
                    "label": 0
                },
                {
                    "sent": "OK, so so you've seen all of that right?",
                    "label": 0
                },
                {
                    "sent": "So with.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process and stuff.",
                    "label": 0
                },
                {
                    "sent": "So I'll tell you now about infinite mixture models.",
                    "label": 0
                },
                {
                    "sent": "I don't think we've seen this in, at least in this summer school yet, so we will again take the same approach, where in fact, what we'll do now is will actually start off with a parametric mixture model and then take the number of components to go to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To form our non parametric mixture model.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're being based in here, right?",
                    "label": 0
                },
                {
                    "sent": "So let's so the first thing we have to do is of course being easy.",
                    "label": 0
                },
                {
                    "sent": "We have to place priors over parameters.",
                    "label": 1
                },
                {
                    "sent": "And of course afterwards we want to compute posteriors using Markov chain Monte Carlo, which you've learned already from Ian's lecture.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, so here's how we'll parameterized our mixture model.",
                    "label": 0
                },
                {
                    "sent": "Firstly, will introduce latent variable XI, Zac I, which indicates which components data item XI is going to belong to.",
                    "label": 0
                },
                {
                    "sent": "An will of course.",
                    "label": 0
                },
                {
                    "sent": "Gives I applier which is simply a multinomial with a vector of probabilities given by mixing proportions, right?",
                    "label": 0
                },
                {
                    "sent": "So the probability of?",
                    "label": 0
                },
                {
                    "sent": "Data Item XI belonging to component K is simply going to be the case.",
                    "label": 0
                },
                {
                    "sent": "Entry in this pie vector.",
                    "label": 1
                },
                {
                    "sent": "And then, given that we've chosen which component XI is going to belong to.",
                    "label": 0
                },
                {
                    "sent": "Will say that XI, given that I equals 2K, will be drawn from a Gaussian with a mean of mu K and the covariance of Sigma K. So here's the graphical model.",
                    "label": 0
                },
                {
                    "sent": "We have our plates over our data items.",
                    "label": 0
                },
                {
                    "sent": "I goes from one to N. Here's our data Vector XI an here's our cluster indicator variables I and that is going to be drawn from a multinomial parameterized by \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And this data Star K is basically mu K. Anne Sigma K put together.",
                    "label": 0
                },
                {
                    "sent": "So those are parameters for mixture component K. An of course we want to introduce priors for both the mixing proportions as well as well as our component parameters as well.",
                    "label": 0
                },
                {
                    "sent": "So we'll introduce for simply, for simplicity, conjugate priors for parameters so.",
                    "label": 1
                },
                {
                    "sent": "Here we have multinomial for pie, so the conjugate prior for Pi will be Additionally with.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to assume that the parameters of the direction they are given by Alpha over K. Until Dot Alpha over key.",
                    "label": 0
                },
                {
                    "sent": "So it's a symmetric originally.",
                    "label": 0
                },
                {
                    "sent": "In which the we have a K vector as our as a parameter.",
                    "label": 0
                },
                {
                    "sent": "As a parameter vector an, it's a symmetric richley, so every every entry of this vector just as Alpha over K. We'll see that this Alpha divided by K is kind of important.",
                    "label": 0
                },
                {
                    "sent": "Next, like why we want to do this?",
                    "label": 0
                },
                {
                    "sent": "And the conjugate prior for a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So from you can Sigma K is going to be what's called normal inverse inverse.",
                    "label": 0
                },
                {
                    "sent": "We shut distribution.",
                    "label": 0
                },
                {
                    "sent": "And let's not worry about the.",
                    "label": 0
                },
                {
                    "sent": "The parameters in there so you can look up Wikipedia for normal investigation.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure you.",
                    "label": 0
                },
                {
                    "sent": "You'll find at least you find yeah anyways.",
                    "label": 0
                },
                {
                    "sent": "So this thing I'll denote by H. This is our prior distribution over our parameters of our mixing components.",
                    "label": 0
                },
                {
                    "sent": "So call Russell's also the one who.",
                    "label": 0
                },
                {
                    "sent": "Most recently we derived this infinite limit prior to count as a whole bunch of people who have done the same, except one of those models that keeps on getting rediscovered.",
                    "label": 0
                },
                {
                    "sent": "Time and again.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so that's our model.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's our prior.",
                    "label": 0
                },
                {
                    "sent": "So now we want to compute the posterior and what we will do is give sampling very simple an because everything is conjugate, we can write down.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Almost immediately, the conditional probabilities required for Gibbs sampling algorithm so.",
                    "label": 0
                },
                {
                    "sent": "First, we'll look at the conditional distributions for the parameters, so for \u03c0.",
                    "label": 0
                },
                {
                    "sent": "It's conditional distribution given everything else is going to be just.",
                    "label": 0
                },
                {
                    "sent": "It's going to only depend on that, basically because if you look at this graphical model, the only way in which interacts with all the other random variables down here is true that an because the duration is conjugate, the posterior of Pi, given that is again a Dirichlet with updated parameters.",
                    "label": 0
                },
                {
                    "sent": "So this notation I got from the fly South one set is simply the number of data items that's assigned to cluster one.",
                    "label": 0
                },
                {
                    "sent": "In general, any case that is the number of components that assigned to cluster K, the number of data points that's assigned to classification.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "And also because our prior over the parameters is conjugate too, so the posterior over.",
                    "label": 0
                },
                {
                    "sent": "Parameters for mixture component K. Given all the other latent variables is also going to be a normal inverse normal inverse Wishart with some updated parameters.",
                    "label": 0
                },
                {
                    "sent": "So again, let's not worry about this this bit.",
                    "label": 0
                },
                {
                    "sent": "So what I'm interested in is actually this bit here.",
                    "label": 0
                },
                {
                    "sent": "And then so we've.",
                    "label": 0
                },
                {
                    "sent": "I've described to you, what's the?",
                    "label": 0
                },
                {
                    "sent": "What's the conditional distribution for \u03c0 and Theta?",
                    "label": 0
                },
                {
                    "sent": "And now we also want to write down the conditional distribution for that.",
                    "label": 0
                },
                {
                    "sent": "An turns out to be also very simple, so the.",
                    "label": 0
                },
                {
                    "sent": "The conditional probability of ZI equal to K given all the other latent variables.",
                    "label": 0
                },
                {
                    "sent": "Is simply going to be proportional to Pi K. Times end of XI.",
                    "label": 0
                },
                {
                    "sent": "Given the parameters of component key.",
                    "label": 0
                },
                {
                    "sent": "So we can think of this term as follows.",
                    "label": 0
                },
                {
                    "sent": "Pi case the conditional.",
                    "label": 0
                },
                {
                    "sent": "Prior that that I is going to take on value K, it is in fact the prior falls.",
                    "label": 0
                },
                {
                    "sent": "I taking on value K and this is this term.",
                    "label": 0
                },
                {
                    "sent": "Here is just the likelihood of our data.",
                    "label": 0
                },
                {
                    "sent": "If that I takes on value K. And of course you need to normalize this to compute the posterior over that I.",
                    "label": 0
                },
                {
                    "sent": "So this is quite similar to.",
                    "label": 0
                },
                {
                    "sent": "Computing the responsibility when you do EM algorithm for a mixture model, right?",
                    "label": 0
                },
                {
                    "sent": "An an as we saw with latent richly allocation, this gives sampling.",
                    "label": 0
                },
                {
                    "sent": "Distribution is not as efficient as one in which we.",
                    "label": 1
                },
                {
                    "sent": "As a collapsed Gibbs sampling algorithm where we integrate out the parameters, So what we'll do now is to integrate our Pi and integrate out the mixing component parameters two and.",
                    "label": 0
                },
                {
                    "sent": "The only thing we will now give sample over the cluster indicator variables that I.",
                    "label": 0
                },
                {
                    "sent": "And if you go, if you go through the math, you see that.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability of that I equal to K given all the other variables is simply going to be proportional to the product of two terms again, so this time here is placed the same role as Pi K is going to be the conditional prior.",
                    "label": 0
                },
                {
                    "sent": "That zetty is going to take on value K given all the given the current assignment of all the others at ICE.",
                    "label": 0
                },
                {
                    "sent": "OK. And it's in fact quite quite interpretable.",
                    "label": 0
                },
                {
                    "sent": "What it says is the following.",
                    "label": 0
                },
                {
                    "sent": "So we will simply count out the number of times among all the all the other cluster indicator variables of the number of times.",
                    "label": 0
                },
                {
                    "sent": "That cluster K was chosen as the cluster to explain the data so.",
                    "label": 0
                },
                {
                    "sent": "This is this is the number of times that cluster K was chosen among all the other guys.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to add a pseudo count that's given by Alpha over K. Two discounts.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to normalize by.",
                    "label": 0
                },
                {
                    "sent": "Basically some over small K of this term here, so that turns out to be just a constant Alpha plus N -- 1 N being the North minus one being the number of.",
                    "label": 0
                },
                {
                    "sent": "Other.",
                    "label": 0
                },
                {
                    "sent": "Data items the total number of data items is North.",
                    "label": 0
                },
                {
                    "sent": "We are we are not.",
                    "label": 0
                },
                {
                    "sent": "Including the current data item I so we only have N -- 1 data items left.",
                    "label": 0
                },
                {
                    "sent": "Among these, some and then multiplied by a likelihood term again, and this likelihood term simply says, what's the probability of XI given all the other data item that's currently assigned to cluster K. And this we can compute this because again, we have assumed a conjugate prior for our Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So this turns out to work very well.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a little demo actually just off this.",
                    "label": 0
                },
                {
                    "sent": "Keep sampling going.",
                    "label": 0
                },
                {
                    "sent": "King hopefully this works OK, so we're running MCMC sampler here an at every iteration we're going to update our parameters right and.",
                    "label": 0
                },
                {
                    "sent": "Given the parameters we can, we can basically compute the density for that particular set of parameters, and of course the parameters are moving around in parameter space in Gibbs sampling, so the density is kind of like wobbling around as well.",
                    "label": 0
                },
                {
                    "sent": "And now we can start adding data points in here, so yes.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, the yeah the blue line here is the is the.",
                    "label": 0
                },
                {
                    "sent": "Estimate is the current estimate of the density given the current set of parameters and the black line here is basically a running average of the most recent blue lines, so it's kind of telling you what's the average density under posterior sofa.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've added two data points in here and you can see that there's a lot more mass in this region now right?",
                    "label": 0
                },
                {
                    "sent": "And then the.",
                    "label": 0
                },
                {
                    "sent": "The average density is kind of like more less picked around here, right?",
                    "label": 0
                },
                {
                    "sent": "So let's add a few more data points here, see.",
                    "label": 0
                },
                {
                    "sent": "So hopefully you'll start producing bimodal distribution, right?",
                    "label": 1
                },
                {
                    "sent": "So this is collapsed Gibbs sampling running.",
                    "label": 0
                },
                {
                    "sent": "You can actually see it running.",
                    "label": 0
                },
                {
                    "sent": "So what happens if we add more data points now?",
                    "label": 0
                },
                {
                    "sent": "OK, so seems to be OK, right?",
                    "label": 0
                },
                {
                    "sent": "So we have three clusters of points and you can kind of see.",
                    "label": 0
                },
                {
                    "sent": "The three different modes.",
                    "label": 0
                },
                {
                    "sent": "Actually, if I add more data points in here.",
                    "label": 0
                },
                {
                    "sent": "We might actually be able to see the tree clusters more clearly, I hope.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to add lots of data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so we see three clusters, so this is nice and working fine and stuff right?",
                    "label": 0
                },
                {
                    "sent": "OK, So what happens now if we add another cluster, right?",
                    "label": 0
                },
                {
                    "sent": "I'm still running going a bit slow.",
                    "label": 0
                },
                {
                    "sent": "Is there?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so so does anybody see any problem with this right now?",
                    "label": 0
                },
                {
                    "sent": "Not enough mass.",
                    "label": 0
                },
                {
                    "sent": "That's true, so that's kind of like.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "It's not quite able to fit all four clusters, right?",
                    "label": 0
                },
                {
                    "sent": "How many clusters do you think is our in our mixture model tree right?",
                    "label": 0
                },
                {
                    "sent": "Yes, obviously, so you can see that we are this finite mixture model is kind of going getting into trouble now, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's not flexible enough to capture this very simple data set.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a finite mixture.",
                    "label": 0
                },
                {
                    "sent": "Three, yes, so.",
                    "label": 0
                },
                {
                    "sent": "And if we try to fit it to four clusters, we're going to see, of course, that it will try to.",
                    "label": 0
                },
                {
                    "sent": "Model this for clusters using tree mixture components.",
                    "label": 0
                },
                {
                    "sent": "So so right now it's kind of like merge these two clusters together and this manage to have a component for each of those classes.",
                    "label": 0
                },
                {
                    "sent": "For each of these two clusters.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of working.",
                    "label": 0
                },
                {
                    "sent": "But not quite right.",
                    "label": 0
                },
                {
                    "sent": "So what we'll do next is basically to take our finite mixture now and take the number of components in this mixture to Infinity so that it can deal with any number of clusters that you give it basically.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Stop.",
                    "label": 0
                },
                {
                    "sent": "So that's a finite mixture model.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, the final mission mall.",
                    "label": 0
                },
                {
                    "sent": "So now we want to take Keita Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK, so before we take a two to Infinity, what we'll do is imagine simply a really large K. So what happens when we have a really large number of missing components?",
                    "label": 0
                },
                {
                    "sent": "Firstly, we only have N data points and could be big.",
                    "label": 0
                },
                {
                    "sent": "1000 by K could be even bigger like a million or something, right, so?",
                    "label": 0
                },
                {
                    "sent": "On number of mix.",
                    "label": 0
                },
                {
                    "sent": "Mixture components K. We can imagine it to be much larger than the number of data points that we have.",
                    "label": 0
                },
                {
                    "sent": "Because every data in the worst case, every data point can be assigned to its own individual cluster.",
                    "label": 0
                },
                {
                    "sent": "So that's really only.",
                    "label": 0
                },
                {
                    "sent": "Ann clusters that will be used to explain the data out of this really large number of.",
                    "label": 0
                },
                {
                    "sent": "Major components right so?",
                    "label": 0
                },
                {
                    "sent": "What this says is that most of the components in our mixture will simply be empty, so they're not going to be associated with data at all, and what we will try to do is actually to lump all of this empty components together.",
                    "label": 1
                },
                {
                    "sent": "So if we want the first equation here, I'm just going to take sorry.",
                    "label": 0
                },
                {
                    "sent": "This thing here and copy it over here.",
                    "label": 0
                },
                {
                    "sent": "Right, so the probability.",
                    "label": 0
                },
                {
                    "sent": "So in our Gibbs sampler, the probability that.",
                    "label": 0
                },
                {
                    "sent": "Data item I is going to be assigned to cluster K given the assignments of all the other clusters is going to be proportional to again, this conditional prior of.",
                    "label": 0
                },
                {
                    "sent": "Data item I being assigned to component K * A likelihood of.",
                    "label": 0
                },
                {
                    "sent": "Off the data item given all the other data items that is currently assigned to cluster key.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so for the for the clusters for which.",
                    "label": 0
                },
                {
                    "sent": "For the occupied clusters for so far the clusters for that have that are currently that that currently have data item assigned to them.",
                    "label": 0
                },
                {
                    "sent": "So in other words, where this NK term is not zero is some positive integer, right?",
                    "label": 0
                },
                {
                    "sent": "So for those clusters then.",
                    "label": 0
                },
                {
                    "sent": "This thing is is fine, right?",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 1
                },
                {
                    "sent": "Alpha over K. So K is a very large integer, so this Alpha over K is going to be a very small number now.",
                    "label": 0
                },
                {
                    "sent": "Plus a positive integer, so this time he is not going to be 0 and that term here is not going to be 0.",
                    "label": 0
                },
                {
                    "sent": "So this thing is perfectly well defined.",
                    "label": 0
                },
                {
                    "sent": "Right, So what about for the empty clusters now?",
                    "label": 0
                },
                {
                    "sent": "So for the clusters for which there's actually no data associated with those clusters, this NK term is going to be 0 right?",
                    "label": 0
                },
                {
                    "sent": "And of course, we're going to be in trouble because we're going to have probably Alpha over K term out here.",
                    "label": 0
                },
                {
                    "sent": "That's going to be really small, so the probability that.",
                    "label": 0
                },
                {
                    "sent": "A data item being assigned to that particular cluster will be arbitrarily small, as K becomes very very large.",
                    "label": 1
                },
                {
                    "sent": "So what happens then?",
                    "label": 0
                },
                {
                    "sent": "Well, we can't really do anything, but what we can do is to say that if we lump all of these different empty clusters together.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's a lot of them.",
                    "label": 1
                },
                {
                    "sent": "And hopefully it will cancel off this small probability Alpha divided by K. And it does happen.",
                    "label": 0
                },
                {
                    "sent": "So what's the you can workout the probability that data item I being assigned to an empty cluster is going to be proportional to Alpha over K multiplied by the number of empty clusters in our mixture model.",
                    "label": 0
                },
                {
                    "sent": "And K star here is the number of occupied clusters and K -- K star is more or less K because K is much larger than North, right?",
                    "label": 0
                },
                {
                    "sent": "So this term here is going to be OK and that term is going to be OK as well because it's simply going to be the marginal probability of data item XI where we integrate out the parameters for that cluster.",
                    "label": 0
                },
                {
                    "sent": "So as K goes to Infinity, Now let's see what happens so.",
                    "label": 0
                },
                {
                    "sent": "As Big K goes to Infinity, this term is going to go to 0, right?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This term here is going to go to one.",
                    "label": 0
                },
                {
                    "sent": "Because K star is bounded by N and K -- K start over K is going to be more or less one.",
                    "label": 0
                },
                {
                    "sent": "For large key, yes.",
                    "label": 0
                },
                {
                    "sent": "Excited.",
                    "label": 0
                },
                {
                    "sent": "This thing it doesn't depend on K right?",
                    "label": 0
                },
                {
                    "sent": "Because they are all simply the marginal probability of the data given no other data points.",
                    "label": 0
                },
                {
                    "sent": "No, no yeah we don't.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we don't need a case here, I guess yeah.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that time goes to 0.",
                    "label": 0
                },
                {
                    "sent": "This time goes to one.",
                    "label": 0
                },
                {
                    "sent": "So now the probability of data items I be assigned to cluster K is simply going to be proportional to the current number of data items that's assigned to cluster K. Multiplied by the likelihood and then divided by some other normalization constants.",
                    "label": 0
                },
                {
                    "sent": "And the probability of this data item being assigned to an empty cluster.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as I guess I should call this components rather than clusters.",
                    "label": 0
                },
                {
                    "sent": "So the probability that data item I is going to be assigned to.",
                    "label": 0
                },
                {
                    "sent": "And empty components is going to be proportional to Alpha times.",
                    "label": 0
                },
                {
                    "sent": "The probability of XI.",
                    "label": 0
                },
                {
                    "sent": "Marginal probability of exile where we have integrated the parameters of the components.",
                    "label": 0
                },
                {
                    "sent": "And we have N -- 1 plus Alpha.",
                    "label": 0
                },
                {
                    "sent": "Now here this constant, so we can just ignore it.",
                    "label": 0
                },
                {
                    "sent": "So we can think of this as introducing a new component to explain the data.",
                    "label": 0
                },
                {
                    "sent": "And this is basically reusing a component to explain this particular data points.",
                    "label": 0
                },
                {
                    "sent": "An of course the maximum number of components that this model will use is at most N. So now we can go back to our DP demo.",
                    "label": 1
                },
                {
                    "sent": "So we do the same right is exactly the same samples from the prior.",
                    "label": 0
                },
                {
                    "sent": "And you kind of see that it also has bumps off structures.",
                    "label": 0
                },
                {
                    "sent": "It looks quite similar to the finite mixture model.",
                    "label": 0
                },
                {
                    "sent": "At the two data points here, we're going to get a lot more math here now.",
                    "label": 0
                },
                {
                    "sent": "But of course we always have mass going outside 2.",
                    "label": 0
                },
                {
                    "sent": "So let's try to create as many clusters as we can.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure whether this will work, we see.",
                    "label": 0
                },
                {
                    "sent": "So OK, so.",
                    "label": 0
                },
                {
                    "sent": "First we need more data points because there's currently very few data points here, so it kind of has decided that it doesn't really need that many clusters to explain.",
                    "label": 0
                },
                {
                    "sent": "Explain so if you add more and more data points here, hopefully you create a cluster down here.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann, among these two classes again, it's it's saying that there's not enough data points here, so I'm not going to use two clusters to explain it.",
                    "label": 0
                },
                {
                    "sent": "I'll just use one or one cluster.",
                    "label": 0
                },
                {
                    "sent": "So let's add another bunch of points here.",
                    "label": 0
                },
                {
                    "sent": "And hopefully it will create a false cluster, right?",
                    "label": 0
                },
                {
                    "sent": "So now you see that you know it's.",
                    "label": 0
                },
                {
                    "sent": "Decided to use four clusters.",
                    "label": 0
                },
                {
                    "sent": "Let's add.",
                    "label": 0
                },
                {
                    "sent": "Add more points here now and hopefully it will start to split this cluster into 2.",
                    "label": 0
                },
                {
                    "sent": "So it seems to be working right?",
                    "label": 0
                },
                {
                    "sent": "So it's decided that there's five classes you can add as more as many classes as you want, and you just keep on using more and more clusters as as it goes along.",
                    "label": 0
                },
                {
                    "sent": "So, so that's a infinite mixture model.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just doing Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "The question is how do you control the tradeoff when to introduce new clusters, when to not introduce new clusters?",
                    "label": 0
                },
                {
                    "sent": "While it's basically just doing Gibbs sampling, so is this exploring the posterior Ann?",
                    "label": 0
                },
                {
                    "sent": "When there's not so, remember this diagram where you have like.",
                    "label": 0
                },
                {
                    "sent": "I think this is from actually calls lecture work right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So if you have a small model class then it will explain a small set of a small.",
                    "label": 0
                },
                {
                    "sent": "Set of data sets very well.",
                    "label": 0
                },
                {
                    "sent": "And then if you have large model plus it will explain the largest datasets, but then of course it has to spread the probability mass around so this thing is kind of similar.",
                    "label": 0
                },
                {
                    "sent": "If you have a small number of data points then it will decide that if you just use a small number of mixture components.",
                    "label": 0
                },
                {
                    "sent": "To explain that particular.",
                    "label": 0
                },
                {
                    "sent": "Dataset well and as you increase as you increase the number of data points, it then realizes that it cannot actually use a small number of clusters to explain it, so it will just increase the number of clusters and it's kind of doing this automatically.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The number of data points.",
                    "label": 0
                },
                {
                    "sent": "The posterior or the prior?",
                    "label": 0
                },
                {
                    "sent": "Yes yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, it turns out that it turns out that the prior problem.",
                    "label": 0
                },
                {
                    "sent": "The prior distribution over clusters over the number of clusters that the model wants to use.",
                    "label": 0
                },
                {
                    "sent": "If you only tell it an datapoints is going to scale as Alpha times log N, the mean of the number.",
                    "label": 0
                },
                {
                    "sent": "That there is a parametric form in fact.",
                    "label": 0
                },
                {
                    "sent": "Well, it looks like.",
                    "label": 0
                },
                {
                    "sent": "It looks a bit like.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "So this may be Alpha log.",
                    "label": 0
                },
                {
                    "sent": "And it's going to be OK.",
                    "label": 0
                },
                {
                    "sent": "It's not, it's not standard form at all.",
                    "label": 0
                },
                {
                    "sent": "Album yes yes.",
                    "label": 0
                },
                {
                    "sent": "It also depends on the parameter of, so this parameter Alpha in fact says the larger Alpha is, the more it expects to use.",
                    "label": 0
                },
                {
                    "sent": "The number of components it expects to use.",
                    "label": 0
                },
                {
                    "sent": "And you can kind of see this from.",
                    "label": 0
                },
                {
                    "sent": "From the slight right.",
                    "label": 0
                },
                {
                    "sent": "From this slide.",
                    "label": 0
                },
                {
                    "sent": "So it's basically saying that the chance of.",
                    "label": 0
                },
                {
                    "sent": "Any data item starting its own cluster is going to be larger now.",
                    "label": 0
                },
                {
                    "sent": "If you have a large Alpha and so you will see more components in your mixture model in your posterior.",
                    "label": 0
                },
                {
                    "sent": "Any other questions yes.",
                    "label": 0
                },
                {
                    "sent": "Messages asked about the prior over the number of yes.",
                    "label": 0
                },
                {
                    "sent": "Is that the same thing that you can see?",
                    "label": 0
                },
                {
                    "sent": "This is the kind of model averaging.",
                    "label": 0
                },
                {
                    "sent": "Yes, I guess you could think of it as a kind of model averaging, except that.",
                    "label": 0
                },
                {
                    "sent": "The easiest way of thinking.",
                    "label": 0
                },
                {
                    "sent": "Thinking about this model is not as model averaging.",
                    "label": 0
                },
                {
                    "sent": "But as.",
                    "label": 0
                },
                {
                    "sent": "Simply, you have one single model with very large number of clusters, right?",
                    "label": 0
                },
                {
                    "sent": "And then when you run your Gibbs sampler it decides not to use most of the clusters.",
                    "label": 0
                },
                {
                    "sent": "It will just decide to use a small subset of the clusters.",
                    "label": 0
                },
                {
                    "sent": "But even if it were.",
                    "label": 0
                },
                {
                    "sent": "Strange.",
                    "label": 0
                },
                {
                    "sent": "The number of data points.",
                    "label": 0
                },
                {
                    "sent": "Yes, but that's kind of.",
                    "label": 0
                },
                {
                    "sent": "Similar to.",
                    "label": 0
                },
                {
                    "sent": "That's kind of similar to Gaussian processes too, right?",
                    "label": 0
                },
                {
                    "sent": "So you're saying that as the number of data points grows, you expect that to be more.",
                    "label": 0
                },
                {
                    "sent": "Information in your data sets, and so you do want to use more and more clusters to explain your data set.",
                    "label": 0
                },
                {
                    "sent": "So the number of clusters do grow, so in this case it grows as long as.",
                    "label": 0
                },
                {
                    "sent": "OK. Anymore questions yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "This business.",
                    "label": 0
                },
                {
                    "sent": "Funny thing about what we mean by number of clusters right?",
                    "label": 0
                },
                {
                    "sent": "Which means people might be confused about.",
                    "label": 0
                },
                {
                    "sent": "So if we have one data point.",
                    "label": 0
                },
                {
                    "sent": "The number of possible ways of clustering one data point into clusters is just one, right?",
                    "label": 0
                },
                {
                    "sent": "Whereas the number of clusters you might actually believe are in the data in the population of possible data you could observe could still be infected.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So you know their data point is like a person and you're clustering them.",
                    "label": 0
                },
                {
                    "sent": "Their food tastes or something like that in two different kinds of people, like different kinds of food.",
                    "label": 0
                },
                {
                    "sent": "Then you know the one person will only belong to one cluster.",
                    "label": 0
                },
                {
                    "sent": "So maybe that's a yeah OK, so it's when we say the number of components is kind of saying.",
                    "label": 0
                },
                {
                    "sent": "Not the number of components that is in the whole population, but only among the.",
                    "label": 0
                },
                {
                    "sent": "Data points that we see only among the people that we see.",
                    "label": 0
                },
                {
                    "sent": "Yes, but how do you pick Alpha?",
                    "label": 0
                },
                {
                    "sent": "Or do you have a prior on that in the code I just fixed Alpha but you can in fact place apprion, often integrate over.",
                    "label": 0
                },
                {
                    "sent": "So in fact this model is quite sensitive to the the setting of Alpha an.",
                    "label": 0
                },
                {
                    "sent": "In practice I always put a priority Alpha sample over that 2.",
                    "label": 0
                },
                {
                    "sent": "Sensible priors people typically use a gamma problem on Alpha because it tends to work pretty well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's an infinite mixture model, right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually I should.",
                    "label": 0
                },
                {
                    "sent": "Go faster, otherwise I want to cover the additional process so when we did this actual infinite limit, right?",
                    "label": 0
                },
                {
                    "sent": "The actual infinite limit of the model actually doesn't really make sense, and the reason is basically actually referred to this already, which is that for any particular component mixture components, the prior probability that it will be assigned to.",
                    "label": 1
                },
                {
                    "sent": "That any data point will be assigned to it is going to approach zero, and if the prior probability is going to be 0, the posterior is going to be 0 as well.",
                    "label": 0
                },
                {
                    "sent": "So in this sense any particular mixture component is not going to be assigned to the data, which is kind of reasonable, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like.",
                    "label": 0
                },
                {
                    "sent": "A lottery system.",
                    "label": 0
                },
                {
                    "sent": "You have a very large number of major components, and they're all fighting for this small set of data points to explain.",
                    "label": 0
                },
                {
                    "sent": "Of course, only a small number of them will win, and all the rest will not be assigned.",
                    "label": 0
                },
                {
                    "sent": "Any data points and is just going to be unhappy I guess.",
                    "label": 0
                },
                {
                    "sent": "So in the Gibbs sampler we kind of bypass this by saying by count lumping all of these different empty clusters together, right?",
                    "label": 1
                },
                {
                    "sent": "They're going to form a union and say we want a few data points to explain, and then when the Gibbs sampler assigns them this new data data point is going to pick one of the clusters.",
                    "label": 0
                },
                {
                    "sent": "To explain this, this new data point.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a bit dumb.",
                    "label": 0
                },
                {
                    "sent": "Not very nice in the sense that we are deriving this algorithm and then we are taking the algorithm to the infinite limit and what we want to do in the rest of this lecture is actually to look at better ways of making this infinite limit precise.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so that's kind of two ways of looking at this.",
                    "label": 0
                },
                {
                    "sent": "Firstly as Zuben already.",
                    "label": 0
                },
                {
                    "sent": "Talked about what we can do is to look at the clustering structure that is induced by this richly prior over mixing proportions.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have any data points right this end data points will only be assigned to at most N clusters, so we can just look at how this Model partition this N data points into a small number of clusters and we can look at the prior over this clustering structure that's induced by this richly.",
                    "label": 0
                },
                {
                    "sent": "Prior over mixing proportions, and then that's going to turn out to be a process called the Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "Another way is that we actually want to reorder the components.",
                    "label": 0
                },
                {
                    "sent": "So that those with larger mixing proportions tend to occur first.",
                    "label": 1
                },
                {
                    "sent": "And then we can take the infinite limit and this turns out to be equivalent to something called the stick breaking construction.",
                    "label": 0
                },
                {
                    "sent": "You see lots of different metaphors in basic nonparametrics, an explain what is meant by a stick breaking and what's meant by a Chinese restaurant later, yes.",
                    "label": 0
                },
                {
                    "sent": "So so in the mix in the mixture model that we have here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are assuming a prior for PIE which is a symmetric Dirichlet write an recall from day flies lecture that if you draw a sample from this symmetric Dirichlet is not going to be symmetric, right?",
                    "label": 0
                },
                {
                    "sent": "So Pi one could take on value 0.2 and \u03c0 two could be 0.01.",
                    "label": 0
                },
                {
                    "sent": "Pie tree could be 0.5 and so on.",
                    "label": 0
                },
                {
                    "sent": "And then if you imagine reordering those after you've sampled from Italy, so that Taiwan is tends to be larger than Pi 2.",
                    "label": 0
                },
                {
                    "sent": "And Pi 2 tends to be larger than pie tree and so on.",
                    "label": 0
                },
                {
                    "sent": "And now you can take the infinite limit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that both of these different views are both of these are different views of this.",
                    "label": 1
                },
                {
                    "sent": "Object called the duration, a process which will spend the next 25 minutes on an additional processors can basically be thought of as this infinite dimensional duration distribution, just as the Gaussian process can be thought of as a infinite dimensional Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "And this gives sampler that we just derived.",
                    "label": 0
                },
                {
                    "sent": "Here is basically the.",
                    "label": 0
                },
                {
                    "sent": "The basic gift sampler for additional processed mixture model.",
                    "label": 0
                },
                {
                    "sent": "So this has anybody.",
                    "label": 0
                },
                {
                    "sent": "Does anybody know a little bit about?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measure theory.",
                    "label": 0
                },
                {
                    "sent": "Great, most people have so, but I guess not everybody has, so I'll still go through the very basic concepts.",
                    "label": 0
                },
                {
                    "sent": "So that's all we need really.",
                    "label": 0
                },
                {
                    "sent": "So a measure.",
                    "label": 0
                },
                {
                    "sent": "Is basically.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, think of it as a ruler.",
                    "label": 0
                },
                {
                    "sent": "It tells you how large a set is, right so?",
                    "label": 0
                },
                {
                    "sent": "It's on the real line.",
                    "label": 0
                },
                {
                    "sent": "The typical measure just says what is the length of an interval?",
                    "label": 0
                },
                {
                    "sent": "What's the length of a subset of the real line?",
                    "label": 0
                },
                {
                    "sent": "And if you think about.",
                    "label": 0
                },
                {
                    "sent": "A measure ask of this function which just tells you what is the length of a subset.",
                    "label": 0
                },
                {
                    "sent": "Then it has to satisfy a few really basic properties, which is kind of reasonable right?",
                    "label": 0
                },
                {
                    "sent": "Firstly, the length of an empty subset is of course going to be 0.",
                    "label": 0
                },
                {
                    "sent": "Right, and then if we have disjoint subsets?",
                    "label": 0
                },
                {
                    "sent": "Then the length of the Union is going to be the sum of the lengths of the individual subsets.",
                    "label": 0
                },
                {
                    "sent": "That's kind of reasonable to write and.",
                    "label": 0
                },
                {
                    "sent": "A probability measure is simply one where the total length of the whole space is going to be one.",
                    "label": 1
                },
                {
                    "sent": "So the length now is going to be the probability of an event.",
                    "label": 0
                },
                {
                    "sent": "A subset is an event, so.",
                    "label": 0
                },
                {
                    "sent": "The probability of.",
                    "label": 0
                },
                {
                    "sent": "Over the whole event, space is going to be one because it has to integrate to one as a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that we can't really form most spaces that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "For example, the real line.",
                    "label": 0
                },
                {
                    "sent": "We can't actually define a measure over all possible subsets of the real line, so we have to restrict ourselves ourselves to kind of family of subsets that.",
                    "label": 0
                },
                {
                    "sent": "Termed measurable.",
                    "label": 0
                },
                {
                    "sent": "An so that was called a Sigma algebra.",
                    "label": 0
                },
                {
                    "sent": "So a Sigma algebra is simply a family of subsets of a set Theta such that.",
                    "label": 1
                },
                {
                    "sent": "Of course it cannot be empty.",
                    "label": 0
                },
                {
                    "sent": "We there has to be some things for which you can measure.",
                    "label": 0
                },
                {
                    "sent": "An if a subset is in our Sigma algebra, then it's compliments has to be in it as well.",
                    "label": 0
                },
                {
                    "sent": "And then finally.",
                    "label": 0
                },
                {
                    "sent": "If we have a sequence of subsets, then their union is also going to be our.",
                    "label": 0
                },
                {
                    "sent": "Is also going to be measurable.",
                    "label": 0
                },
                {
                    "sent": "OK, so the fact that this is a sequence is kind of important, so you can't have a uncountable union of things, so any countable union of measurable sets is also measurable basically.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right so then finally.",
                    "label": 0
                },
                {
                    "sent": "Well, let's not worry about what's meant by a measurable function.",
                    "label": 0
                },
                {
                    "sent": "It in fact just says that if you have two measurable spaces, Theta and Delta, then a function is measurable.",
                    "label": 0
                },
                {
                    "sent": "If its inverse images are measurable for any.",
                    "label": 0
                },
                {
                    "sent": "Measurable subset A of the second of the output space.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So now let's see.",
                    "label": 0
                },
                {
                    "sent": "Restrict ourselves to probability measures now and think about what is meant by a random variable.",
                    "label": 0
                },
                {
                    "sent": "Turns out that a random variable is actually not random at all, it's simply.",
                    "label": 0
                },
                {
                    "sent": "A measurable function.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "The way you can think of this as is as follows, so.",
                    "label": 0
                },
                {
                    "sent": "So how would you implement?",
                    "label": 0
                },
                {
                    "sent": "A function in Matlab say or in C that that generates, say, random draws from from a gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so the way you would do is you would actually write down this function, which takes.",
                    "label": 0
                },
                {
                    "sent": "Takes a sequence of.",
                    "label": 0
                },
                {
                    "sent": "Takes actually.",
                    "label": 0
                },
                {
                    "sent": "Actually I'm not sure how do you generate from a distribution.",
                    "label": 0
                },
                {
                    "sent": "So here's a here's an easier one.",
                    "label": 0
                },
                {
                    "sent": "So how would you generate from a Gaussian with a mean of 10?",
                    "label": 0
                },
                {
                    "sent": "Anybody can?",
                    "label": 0
                },
                {
                    "sent": "Tell me how.",
                    "label": 0
                },
                {
                    "sent": "How would you implement static?",
                    "label": 0
                },
                {
                    "sent": "At 10 right subtract.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, yes, so that's right.",
                    "label": 0
                },
                {
                    "sent": "So we assume that we have some random number generator.",
                    "label": 0
                },
                {
                    "sent": "So in this case random number generator that generates zero mean gaussian's.",
                    "label": 0
                },
                {
                    "sent": "And then we simply add 10 to it.",
                    "label": 0
                },
                {
                    "sent": "So if you think of the function itself that you've just written is a deterministic function is it's not deterministic, it's a function itself is fixed, right?",
                    "label": 0
                },
                {
                    "sent": "And it takes random bits and it returns to you random numbers from the distribution that you're interested in and.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the same way that you can think of a random variable, so the probability measure here on this space data, Sigma.",
                    "label": 1
                },
                {
                    "sent": "So Sigma.",
                    "label": 0
                },
                {
                    "sent": "Here's our Sigma algebra is going to be our random number generator.",
                    "label": 1
                },
                {
                    "sent": "And our random variable X is going to be a function which takes our random number generator and spits out random variables.",
                    "label": 0
                },
                {
                    "sent": "Random samples of this variable.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So and then finally, a stochastic process is basically a collection of random variables.",
                    "label": 1
                },
                {
                    "sent": "So say XI, one for each I.",
                    "label": 1
                },
                {
                    "sent": "And the distinguishing property of a stochastic process from, say, a graphical model is that this index set I can be infinite, and in fact it can be uncountably infinite as we saw in the case of a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "Right, so this turns out to be a tricky issue, so.",
                    "label": 0
                },
                {
                    "sent": "Is that how do you even define this uncountably infinite number of random variables?",
                    "label": 0
                },
                {
                    "sent": "And how can even ensure that they exist?",
                    "label": 0
                },
                {
                    "sent": "And this is where Kolmogorov consistency theorem comes into play, so it tells you that if you can define a joint distribution on any finite subset.",
                    "label": 0
                },
                {
                    "sent": "That's consistent in some way.",
                    "label": 0
                },
                {
                    "sent": "Then you can extend it to the whole stochastic process.",
                    "label": 0
                },
                {
                    "sent": "But of course, there's caveats.",
                    "label": 0
                },
                {
                    "sent": "Again, which filtering will tell you about.",
                    "label": 0
                },
                {
                    "sent": "An so it turns out that stochastic processes from the call of many based on non parametric models.",
                    "label": 0
                },
                {
                    "sent": "So things like Gaussian processes and today will be talking about judicial processes and on the next lecture tell you a bit about beta processes and parts and processes.",
                    "label": 0
                },
                {
                    "sent": "So another thing which I like you too.",
                    "label": 0
                },
                {
                    "sent": "To remember is that here I simply define a measure.",
                    "label": 0
                },
                {
                    "sent": "As a function from our Sigma algebra to the positive real numbers.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just a function, so.",
                    "label": 0
                },
                {
                    "sent": "On the next slide way, I said, why is this a function OK, two slides from now.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I need to re introduce to you as additional distribution.",
                    "label": 0
                },
                {
                    "sent": "I think you're all very familiar with this now.",
                    "label": 0
                },
                {
                    "sent": "After about 2 hours of talking about distributions.",
                    "label": 0
                },
                {
                    "sent": "So it's simply a distribution over this K dimensional probability simplex, a probability simplex being a set of vectors of length K that such that every entry is non zero and they sum to one.",
                    "label": 1
                },
                {
                    "sent": "And we say that a random variable Pi wanted by K is richly distributed with this parameters.",
                    "label": 0
                },
                {
                    "sent": "If we can write the density in the following way.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "This is a density on the simplex.",
                    "label": 1
                },
                {
                    "sent": "An another way of thinking of what is original distribution is that it's basically equivalent to taking a set of independent gamma variables.",
                    "label": 1
                },
                {
                    "sent": "And then just normalizing them.",
                    "label": 0
                },
                {
                    "sent": "So that so that the entries sum to One South.",
                    "label": 0
                },
                {
                    "sent": "In particular, we're going to take gamma K to be drawn from a gamma distribution with the shape of Lambda K and the scale of 1.",
                    "label": 0
                },
                {
                    "sent": "And then we're just going to take this as independent, and then we're going to just normalize it, and then this is going to form a sample from our original distribution with exactly the same set of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a diagrams for to show how do the richly distributions look like when the when the when the Lambda parameters are all equal to 1, we have a uniform distribution over the simplex.",
                    "label": 0
                },
                {
                    "sent": "When the Lambda parameters are greater than one.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "Coffee, a unimodal distribution over the simplex, so.",
                    "label": 0
                },
                {
                    "sent": "Red here and black here means high probability, high density and yellow and white means low low density and the mean of this distribution simply going to be the Lambda vector.",
                    "label": 0
                },
                {
                    "sent": "If you take the Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back then you normalize them, then that's going to be the mean of the delay distribution.",
                    "label": 0
                },
                {
                    "sent": "And the sum of the Lambda vector.",
                    "label": 0
                },
                {
                    "sent": "Is going to be the more or less the inverse?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variants of it.",
                    "label": 0
                },
                {
                    "sent": "So the spread around the mean.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that when the lambdas go below 0, so in this case it's 0.7 zero point 7, zero point 7 then.",
                    "label": 0
                },
                {
                    "sent": "It's not a unimodal distribution anymore.",
                    "label": 0
                },
                {
                    "sent": "In fact, the math gets concentrated at the corners of the probability simplex, and this is another way of saying that the derivative distributions sparse.",
                    "label": 0
                },
                {
                    "sent": "When Lambda is less than.",
                    "label": 0
                },
                {
                    "sent": "One right, because it's kind of saying that.",
                    "label": 0
                },
                {
                    "sent": "So one of these corners means that one of the entries is close to one and both of the other entries are going to be close to 0.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so I'll just tell you what's the judicial process?",
                    "label": 0
                },
                {
                    "sent": "Is simply a random probability measure.",
                    "label": 0
                },
                {
                    "sent": "Such that for any finite set of partitions of our space, so we start off with a space Theta.",
                    "label": 1
                },
                {
                    "sent": "An if we if we take any finite set of measurable partitions, So what that means is that we have a finite number of subsets such that they are all disjoint and their union forms the whole space.",
                    "label": 0
                },
                {
                    "sent": "OK, then we can form this vector G of A1 and 2G of a K. This is a random factor because G is random, right?",
                    "label": 0
                },
                {
                    "sent": "So we have fixed our finite measurable partition 8128K.",
                    "label": 1
                },
                {
                    "sent": "But G is a random probability measure, so this is a random vector.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is a random vector that has to sum to one an.",
                    "label": 0
                },
                {
                    "sent": "Every entry has to be non negative.",
                    "label": 0
                },
                {
                    "sent": "Because it's a probability measure.",
                    "label": 0
                },
                {
                    "sent": "And we say that this G. It's going to be a draft is going to be additional process if each of these vectors is going to be directly distributed with parameters given by Lambda A1 until Lambda AKA where Lambda is some some measure over our space, yes?",
                    "label": 0
                },
                {
                    "sent": "So what exactly do you mean by a random random?",
                    "label": 0
                },
                {
                    "sent": "I mean yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah it is so.",
                    "label": 0
                },
                {
                    "sent": "It's basically if you take any fixed subsets.",
                    "label": 0
                },
                {
                    "sent": "Right then the probability that is going to assign to this subset is going to be random.",
                    "label": 0
                },
                {
                    "sent": "So another way of thinking about this is, recall that a probability measure is simply a function from our Sigma algebra to the positive real numbers.",
                    "label": 0
                },
                {
                    "sent": "Write an just as in the Gaussian process we have defined as a random function as.",
                    "label": 0
                },
                {
                    "sent": "So in the Gaussian process we have F of X.",
                    "label": 0
                },
                {
                    "sent": "This is a random function.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if every entry here is random and there's a joint distribution over all of this.",
                    "label": 0
                },
                {
                    "sent": "A random variables, so in this case we have G which is a measure, so it's a function from the measurable subsets to the positive lines.",
                    "label": 0
                },
                {
                    "sent": "So we have G of.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Where a is a measurable set, so it belongs to this Sigma algebra over our space.",
                    "label": 0
                },
                {
                    "sent": "So we have again an uncountably.",
                    "label": 0
                },
                {
                    "sent": "Infinite number of random variables, one for every measurable subset.",
                    "label": 0
                },
                {
                    "sent": "An they all have to satisfy the property that if you take any measurable partition then that.",
                    "label": 0
                },
                {
                    "sent": "Joints distribution over this finite number of.",
                    "label": 0
                },
                {
                    "sent": "Random variables coming from this infinite set.",
                    "label": 0
                },
                {
                    "sent": "This is going to be a jewishly distributed.",
                    "label": 0
                },
                {
                    "sent": "So here's come for a visualization of a partition of a space being the rectangle.",
                    "label": 0
                },
                {
                    "sent": "So this above family of distributions here, in which for every partition we have Additionally distribution over.",
                    "label": 0
                },
                {
                    "sent": "The partition turns out to be consistent, so I'll yes, sorry I missed this, but what do you mean by the dot of the unions in law?",
                    "label": 0
                },
                {
                    "sent": "In the second?",
                    "label": 0
                },
                {
                    "sent": "It just means that they are disjoint completely.",
                    "label": 1
                },
                {
                    "sent": "Yeah, they don't have intersection.",
                    "label": 0
                },
                {
                    "sent": "So because this family of distributions is consistent given by this thing, we can now.",
                    "label": 0
                },
                {
                    "sent": "Apply Kolmogorov consistency theorem to say that to show that there isn't that the Dirichlet process, this exist OK.",
                    "label": 0
                },
                {
                    "sent": "So how do we see that this?",
                    "label": 0
                },
                {
                    "sent": "Lee after running out of time, really.",
                    "label": 0
                },
                {
                    "sent": "Al OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so maybe I'll try to go to why is it that this rich data distributions are consistent?",
                    "label": 0
                },
                {
                    "sent": "And then I might.",
                    "label": 0
                },
                {
                    "sent": "Finish up this in the next lecture.",
                    "label": 0
                },
                {
                    "sent": "As eraser.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So what's meant by?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consistent here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So imagine that we have our space right?",
                    "label": 0
                },
                {
                    "sent": "An OK, let's look at one partition, say.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "OK, this is going to be a 18283.",
                    "label": 0
                },
                {
                    "sent": "So according to the definition.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We're going to have G of.",
                    "label": 0
                },
                {
                    "sent": "A1.",
                    "label": 0
                },
                {
                    "sent": "In fact, this G. Of K2G RAT.",
                    "label": 0
                },
                {
                    "sent": "Right, so GFA one is simply the random mass.",
                    "label": 0
                },
                {
                    "sent": "There's going to be assigned to this a one partition and G of a two simply going to be the random Masters assigned here and here is G of a tree.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this vector we want it to be richly distributed.",
                    "label": 0
                },
                {
                    "sent": "With parameters given by Lambda A1.",
                    "label": 0
                },
                {
                    "sent": "#8 two.",
                    "label": 0
                },
                {
                    "sent": "Lambda.",
                    "label": 0
                },
                {
                    "sent": "A tree OK?",
                    "label": 0
                },
                {
                    "sent": "And now let's look at another partition.",
                    "label": 0
                },
                {
                    "sent": "Say something like this is BB1 and B2.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, we're going to have the same thing where G of B1.",
                    "label": 0
                },
                {
                    "sent": "Your feet too.",
                    "label": 0
                },
                {
                    "sent": "We want.",
                    "label": 0
                },
                {
                    "sent": "This vector to be directly distributed.",
                    "label": 0
                },
                {
                    "sent": "With parameters Lambda B1.",
                    "label": 0
                },
                {
                    "sent": "Honda V2 OK. An by consistency, what I mean is that we want that to be a single function.",
                    "label": 0
                },
                {
                    "sent": "A single random variable.",
                    "label": 0
                },
                {
                    "sent": "This random variable is a function, right?",
                    "label": 0
                },
                {
                    "sent": "So a single function G. That makes both of this.",
                    "label": 0
                },
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "Makes the marginal distribution over makes these two.",
                    "label": 0
                },
                {
                    "sent": "Makes both of these marginal distributions hold OK. And we can kind of see this if we just do the following.",
                    "label": 0
                },
                {
                    "sent": "So we will take a common refinement of both of these partitions so.",
                    "label": 0
                },
                {
                    "sent": "Here's our.",
                    "label": 0
                },
                {
                    "sent": "A partition.",
                    "label": 0
                },
                {
                    "sent": "And here's a big partition.",
                    "label": 0
                },
                {
                    "sent": "Write something like that so I'll form a common refinement.",
                    "label": 0
                },
                {
                    "sent": "So I'll call this CCC1C2C3C4C5.",
                    "label": 0
                },
                {
                    "sent": "So we see that a two is simply the Union of these two, and a tree is the Union of this two, and B1 is the union of this tree.",
                    "label": 0
                },
                {
                    "sent": "Ann B2 is the union of these two.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now for this partition we also have this property holding.",
                    "label": 0
                },
                {
                    "sent": "So what we see is that G of C1G of C2G of C3.",
                    "label": 0
                },
                {
                    "sent": "Jessie fall this thing is deliciously.",
                    "label": 0
                },
                {
                    "sent": "Alpha bit long to write down.",
                    "label": 0
                },
                {
                    "sent": "Lambda A1C1C.",
                    "label": 0
                },
                {
                    "sent": "Lambda C2 the city.",
                    "label": 0
                },
                {
                    "sent": "C4C5 OK, so this thing has to be directly distributed.",
                    "label": 0
                },
                {
                    "sent": "For this common refinement as well.",
                    "label": 0
                },
                {
                    "sent": "And we know that if we take.",
                    "label": 0
                },
                {
                    "sent": "G of C2 and G of C4 and we add up.",
                    "label": 0
                },
                {
                    "sent": "Those two entries were going to get G of a one.",
                    "label": 0
                },
                {
                    "sent": "Right, because it's G itself is a is a measure, so the measure.",
                    "label": 0
                },
                {
                    "sent": "So a 2 being the disjoint union of C2 and C4, the measure of.",
                    "label": 0
                },
                {
                    "sent": "A2 is going to be the measure of C2 plus the measure of C4 and similarly G of a tree is going to be equal to G of C3 plus.",
                    "label": 0
                },
                {
                    "sent": "We'll see 5.",
                    "label": 0
                },
                {
                    "sent": "And then G of B1 here is going to be some of this one, this one and that one.",
                    "label": 0
                },
                {
                    "sent": "As GOP, this can be some of those two.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the duration distribution has this really nice property where if you take any two entries in the original distribution and you add them up.",
                    "label": 0
                },
                {
                    "sent": "Then the resulting random factor is also directly distributed where you simply take the corresponding entry or the parameters.",
                    "label": 0
                },
                {
                    "sent": "Then you add them up.",
                    "label": 0
                },
                {
                    "sent": "So Lambda A1.",
                    "label": 0
                },
                {
                    "sent": "So this G of a 2 here is going to be some of C4 and C5, so we're going to just take Lambda C4C5.",
                    "label": 0
                },
                {
                    "sent": "We're going to add them.",
                    "label": 0
                },
                {
                    "sent": "And that's going to form our corresponding entry of arguably the parameter for our original distribution, and Similarly for this two.",
                    "label": 0
                },
                {
                    "sent": "So I got it wrong.",
                    "label": 0
                },
                {
                    "sent": "So this goes to.",
                    "label": 0
                },
                {
                    "sent": "There's two and this goes to that to write an similarly for over here too.",
                    "label": 0
                },
                {
                    "sent": "So Lambda B1 is going to be Lambda, C1C2 and C3.",
                    "label": 0
                },
                {
                    "sent": "And this is going to.",
                    "label": 0
                },
                {
                    "sent": "Right, so since this is the richly distributed with those parameters, when when we add up.",
                    "label": 0
                },
                {
                    "sent": "The entries of this vector this smaller vector is also going to be distributed with exactly the parameters that we started off with, and Similarly for this, and because we both of these are simply marginal distributions coming from the same.",
                    "label": 0
                },
                {
                    "sent": "Joints distribution over this set of five random variables we know that this one is going to be consistent with this.",
                    "label": 0
                },
                {
                    "sent": "Right, they're both marginals of the same joint distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and because of this consistency we can now apply Kolmogorov consistency theorem.",
                    "label": 0
                },
                {
                    "sent": "Yes, consistency.",
                    "label": 0
                },
                {
                    "sent": "I'm saying we can put probability and we can define measures on that probability measures satisfy the conditions.",
                    "label": 0
                },
                {
                    "sent": "So what we what guaranteed to exist by the theorem, does it?",
                    "label": 0
                },
                {
                    "sent": "That theorem say certain kind of measure space exists or so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so this there in fact says that there is.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Measure space with a particular Sigma algebra for which.",
                    "label": 0
                },
                {
                    "sent": "For which any finite.",
                    "label": 0
                },
                {
                    "sent": "Set of random variables is forest.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution for that finite self for any finite server of random variables, as is given by you right when you define it?",
                    "label": 0
                },
                {
                    "sent": "So I started off by saying that, OK, I want.",
                    "label": 0
                },
                {
                    "sent": "At random probability, measure G is a stochastic process that satisfies all of these properties for any partition.",
                    "label": 1
                },
                {
                    "sent": "This thing has to be directly distributed and this consistency Theorem says that that is in fact.",
                    "label": 0
                },
                {
                    "sent": "So I haven't really defined for you.",
                    "label": 0
                },
                {
                    "sent": "What is the joint distribution over the whole random probability measure?",
                    "label": 0
                },
                {
                    "sent": "Gee, I've only told you what the finite marginals look like, and this theorem basically says that there is a.",
                    "label": 0
                },
                {
                    "sent": "A joint distribution over the whole of G such that.",
                    "label": 0
                },
                {
                    "sent": "The finite dimensional marginals under this stochastic process.",
                    "label": 0
                },
                {
                    "sent": "IS has marginals given by?",
                    "label": 0
                },
                {
                    "sent": "Given by this richly basically, but I think Peter would really go into detail about what's the problem that I haven't really told you.",
                    "label": 0
                },
                {
                    "sent": "The problem.",
                    "label": 0
                },
                {
                    "sent": "I've saved the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Let's do it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll see.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Peter OK so.",
                    "label": 0
                },
                {
                    "sent": "Yep, So what we're going to do now?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Lambda is a measure.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to sum to one.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do now is the following so.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna start without my space.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "I'm going to come like recursively partition this space into smaller into more and more refined partitions.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine first splitting it into two and then splitting into four and then 8.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "OK and?",
                    "label": 0
                },
                {
                    "sent": "At any step of this.",
                    "label": 0
                },
                {
                    "sent": "Process we are going to have additional distribution over this finite partition.",
                    "label": 0
                },
                {
                    "sent": "But if we keep on splitting each of these subsets into smaller into a.",
                    "label": 1
                },
                {
                    "sent": "Smaller and smaller sets.",
                    "label": 0
                },
                {
                    "sent": "At some point we're going to see that we will be able to define all a.",
                    "label": 0
                },
                {
                    "sent": "The probability measure over the whole space.",
                    "label": 0
                },
                {
                    "sent": "And that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "OK so here.",
                    "label": 0
                },
                {
                    "sent": "My space is gonna be.",
                    "label": 0
                },
                {
                    "sent": "Just a unit interval between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And the area of this rectangle here so this thing goes from zero to 1 here as well.",
                    "label": 0
                },
                {
                    "sent": "So the area under this rectangle is going to be the probability mass get.",
                    "label": 0
                },
                {
                    "sent": "That's going to be assigned to this unit interval.",
                    "label": 0
                },
                {
                    "sent": "And of course this area here is going to be 1 * 1 is going to be one, so that's OK, right?",
                    "label": 1
                },
                {
                    "sent": "So we start off with a with a probability measure that assigns probability of 1 to the whole space, yes?",
                    "label": 0
                },
                {
                    "sent": "We mentioned that we sampled from the replay process.",
                    "label": 1
                },
                {
                    "sent": "Sample constitutes apart finite partition of the space and the probability measure know it.",
                    "label": 0
                },
                {
                    "sent": "A sample is a whole probability measure, so that on any finite partition.",
                    "label": 1
                },
                {
                    "sent": "Yeah, yeah, it's for all for all finite partitions, so here will try to draw a richly process.",
                    "label": 0
                },
                {
                    "sent": "So what this thing says is that the.",
                    "label": 0
                },
                {
                    "sent": "For this drawing the judicial process, the total mass that is going to assign to the unit interval, it's going to be one which is OK because it has to be a probability measure.",
                    "label": 0
                },
                {
                    "sent": "So the next thing we do is we're going to take this unit interval.",
                    "label": 0
                },
                {
                    "sent": "We're going to split into 2.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to ask how much mass is it going to assign to?",
                    "label": 0
                },
                {
                    "sent": "Between zero and.",
                    "label": 0
                },
                {
                    "sent": "And a half and how much mass does it want to assign to?",
                    "label": 0
                },
                {
                    "sent": "1/2 to one.",
                    "label": 0
                },
                {
                    "sent": "And we can actually sample this.",
                    "label": 0
                },
                {
                    "sent": "Just put it over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it says that it wants to assign a bit more mass to the to the first half of the interval as opposed to the second half and the amount is curve.",
                    "label": 0
                },
                {
                    "sent": "Push this mess up.",
                    "label": 0
                },
                {
                    "sent": "Here is the same as the amount that is pushed it down here, so the total mass is still one.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to take the first half of the interval, and we're going to ask how much mass do you want to assign to the.",
                    "label": 0
                },
                {
                    "sent": "First quarter versus the second quarter, and then for the second half we ask how much math do you want to assign to the third quarter versus the fourth quarter.",
                    "label": 0
                },
                {
                    "sent": "So we might do this so in the first two quarters is kind of like basically the same.",
                    "label": 0
                },
                {
                    "sent": "Right, that's the same amount of mass in the first two quarters, but then is saying that the fourth quarter it wants to assign just a little bit of mass two, and then we can keep on repeating this, splitting each interval into halves.",
                    "label": 0
                },
                {
                    "sent": "Anne, if you do this lots of time.",
                    "label": 0
                },
                {
                    "sent": "OK OK, what do we see?",
                    "label": 1
                },
                {
                    "sent": "So we're going to see that.",
                    "label": 0
                },
                {
                    "sent": "So this is in fact a random sample from a directly process.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's going to.",
                    "label": 0
                },
                {
                    "sent": "In fact, BS series of of this Delta functions Delta function being.",
                    "label": 0
                },
                {
                    "sent": "Point mass at.",
                    "label": 0
                },
                {
                    "sent": "Is this saying that with probability given by the height of this thing is going to take on value exactly given by the by the location of that of that spike there?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And with a smaller probability, it's going to be given a value.",
                    "label": 0
                },
                {
                    "sent": "That's given by the location of that Spike.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Speed slow.",
                    "label": 0
                },
                {
                    "sent": "I'm, I'll wrap up soon.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I said Oh yeah, OK. Yeah, maybe I'll stop here actually.",
                    "label": 0
                },
                {
                    "sent": "So, So what this says is that.",
                    "label": 0
                },
                {
                    "sent": "We know that the richly processes exist, and we know that if we generate samples from it then it will always look like a weighted sum of point methods, and I'll next week I will.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, try to go to some.",
                    "label": 0
                },
                {
                    "sent": "The parameters of the dish.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The process, as well as different representations of it before I go onto.",
                    "label": 1
                },
                {
                    "sent": "To the other processes.",
                    "label": 0
                },
                {
                    "sent": "Thank you, yes.",
                    "label": 0
                },
                {
                    "sent": "Any point?",
                    "label": 0
                },
                {
                    "sent": "Yes, in fact it turns out there is always countably infinite number of points masses.",
                    "label": 1
                },
                {
                    "sent": "When you draw from the judicial process.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so in fact you always look like this.",
                    "label": 0
                },
                {
                    "sent": "So a sample from the duration process is always going to be an infinite sum of.",
                    "label": 0
                },
                {
                    "sent": "Points methods each located at the data key and weight.",
                    "label": 0
                },
                {
                    "sent": "The mass of that point mass is going to be given by paikea.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, so we'll go into representations of the judicial process next week.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}