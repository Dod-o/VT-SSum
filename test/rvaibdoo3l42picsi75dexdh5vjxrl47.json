{
    "id": "rvaibdoo3l42picsi75dexdh5vjxrl47",
    "title": "The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures",
    "info": {
        "author": [
            "Joseph Anderson, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_anderson_learning/",
    "segmentation": [
        [
            "Good afternoon, I'm Joe Anderson and I'll be presenting our paper called the More the merrier.",
            "The blessing of dimensionality for learning large Gaussian mixtures.",
            "This is joint work with Mikhail Belkin, Navin Goyal, Louise Rademacher, and James Voss and Louise and James are with us today."
        ],
        [
            "So.",
            "To understand the main message of our paper.",
            "We first need to discuss the curse of dimensionality and it's a common obstacle in modern computational applications that the problems intrinsically become very difficult as the dimension decreases, and this is more than just like.",
            "Decreasing because the dimension is increasing, but like quantum jumps in difficulty.",
            "So a common approach when better tools aren't available.",
            "Is to preprocess your data and this is often to reduce the dimension.",
            "And you can do some sort of analysis on the lower dimensional data and propagate the results back to your original.",
            "Situation."
        ],
        [
            "So the blessing of dimensionality.",
            "Is.",
            "What we call an unusual phenomenon because we show that there are problems that become much easier as the ambient dimension of your data increases, and in this sense, dimensionality reduction could be conservative, considered harmful to your ability to efficiently solve a problem.",
            "And anytime we say efficient, we will mean learnable in polynomial time and say one over your probability of error etc.",
            "And specifically in this talk we will discuss the parameter estimation for a Gaussian mixture model.",
            "And show that this is generically hard in low dimension and easy in high dimension, and we'll discuss what that means later and there is in fact an exponential gap between the two."
        ],
        [
            "So what is learning a Gaussian mixture model is the situation where you're given samples with a density which is the weighted average of several N dimensional Gaussian densities, each with its own mean vector covariance matrix and await.",
            "And when you draw a sample from this model, you get a sample from one of the Gaussians and we have these here.",
            "These actually have.",
            "This is Mew, Mew and mew.",
            "You can see this too, but so we have three Gaussians and when you draw sample you get a point from one of them, but you don't know which one, and so your goal is to estimate the weights.",
            "The means in the covariances.",
            "And so specifically, we'll discuss a blessing of dimensionality for the case of recovery of just the means in the weights, even."
        ],
        [
            "So low dimensional hardness is the first sort of half of the blessing.",
            "And this is that.",
            "In fact, in low dimensions, your ability to efficiently estimate a Gaussian mixture fails generically, which is to say for dense enough sets.",
            "There are two Gaussian mixtures over those sets, so centered on each point.",
            "So that the distributions are very close.",
            "But yet very different in parameter space.",
            "So you can't distinguish the two.",
            "And this lower bound will be applied through a reduction.",
            "And will be related to the problem of independent component analysis."
        ],
        [
            "So the low dimensional hardness theorem looks like this, and it might seem rather cumbersome, but the main message is just like on the previous slide.",
            "If you take a square points from the end dimensional hypercube with high probability, you can construct two disjoint subsets of these K square points and two Gaussian mixtures over them that are exponentially close in total variation distance.",
            "That's this guy here and they'll be exponentially close in K over log K. To the 1 / N, so there's a lot there you can sort of ignore the log and 1st consider the case N = 1.",
            "So in low dimension your exponential inverse exponential in K. But as your dimension increases you can see how this degrades.",
            "And.",
            "So you have very close statistical distributions, but the means are going to be well separated by your choice of these two disjoint subsets.",
            "And the implication of this is just the fact that any algorithm will require an inverse number in the total variation distance.",
            "Just to distinguish these two distributions.",
            "So you're sort of in a hopeless situation for efficient learnability.",
            "The technique used to prove this just to.",
            "Sort of give you the taste without any more details is that using properties of reproducing kernel Hilbert spaces to bound the differences between smooth functions and their Gaussian kernel interpolants, which essentially construct you Gaussian mixtures, and then you can choose your points carefully enough that you get equal equal size sets of means."
        ],
        [
            "So the other half of the blessing is well, remember that it's hard and low dimension and easy in high dimension.",
            "And so for a fixed P you can learn order N to the P. Gaussian means in a Gaussian mixture.",
            "Using smooth polynomial time and sample size will talk a little more about that that, but the theorem is as follows.",
            "If each Gaussian is the same covariance matrix.",
            "And you know it beforehand, you can estimate the means using sample and time that's polynomial and the dimension number of components inverse in this conditioning parameter S. And the other natural parameters to the problem.",
            "So what does this 1 / S?",
            "It's a generalization of the smaller singular value of the matrix of means in the mixture.",
            "And we'll talk a little bit more about that as well.",
            "And the fact that.",
            "When we say it's easy, generically, is that 1 / S. Is generically not too large in high enough dimension.",
            "And."
        ],
        [
            "This is the more precise way in the case where your fixed P = 2.",
            "If you want to learn N square Gaussians in Dimension N, the probability so.",
            "Your set of means can be in the smooth sense.",
            "You perturb the setting means, so this is any set of means and you add a perturbation matrix E which is just N by K. Here's some order in square.",
            "You add this to M and the probability that the condition number is low is some inverse polynomial in the dimension.",
            "So as the dimension increases, your condition number generically allows you to learn the mixture efficiently.",
            "And this was mentioned earlier.",
            "But and if you know what it is, S is actually the case.",
            "Singular value of the second catcher out power.",
            "So yeah, you perturb and the probability of the second catcher or power is small.",
            "Is inverse polynomial?",
            "So."
        ],
        [
            "Summarize the blessing as such, is you really have to keep the two fold in mind that it's hard and low dimension and easy in high dimension, so it's sort of a blessing.",
            "That is only a blessing when your dimension is high enough.",
            "And it is still hard generically in low dimension."
        ],
        [
            "So some previous work on Gaussian mixture, specifically.",
            "Of course, Gaussian mixtures have a very historic significance in statistical community, but the first algorithms in computer science with full sample and time analysis was done by Descoped in 1999 and which used a projection to lower dimension, which we alluded to earlier.",
            "And required a separation condition on the means.",
            "So once this work was done, there was a entire sequence of work using similar projection techniques and whose goal was to reduce this separation assumption.",
            "And finally, in 2010 the work of Belkin and Sinha and Moitra Valiant both gave polynomial time algorithms to learn Gaussian mixtures for fixed number of components.",
            "And they also use lower dimensional projections and Additionally, motor and Valiant gave in.",
            "21 dimensional mixtures which were exponentially cost close in distribution.",
            "So this is as you asked earlier, you're very close in distribution, but the parameters you're learning are actually very different.",
            "So if you wanted to do some sort of inference on those means, you get the wrong sort of answer.",
            "So this showed an exponential dependence was necessary in one dimension.",
            "But in 2012 soon Kokota gave a surprising result where they are able to learn Dega D Gaussians in D dimensions.",
            "Officially, without separation assumptions, but instead their complexity is related to the smallest singular value of the matrix of means."
        ],
        [
            "And there are simultaneous work by bus, garage Charikar Moitra Envision Raghavan.",
            "In this year's talk, which was done independently to learn large Gaussian mixtures, it in high dimension and they used to smooth analysis for particular decomposition.",
            "Algorithm and their work handles unknown access line covariance which has a slightly higher running time and but a little lower probability of error ultimately."
        ],
        [
            "So I'd like to discuss now the blessing of dimensionality was ultimately a side effect of our original attempt to learn Gaussian mixture models, and the relation came from this independent component analysis problem, which has a long history in the machine learning signal processing community, and the model is as follows.",
            "You get observations of a vector S, which is a matrix a times a signal S plus some noise.",
            "And you typically assume Ada.",
            "The noise is Gaussian and S has independent coordinates.",
            "And the goal is to recover your mixing matrix.",
            "And we.",
            "Learn a Gaussian mixture with an ICA algorithm.",
            "When you know the covariance matrix beforehand and it's the same for the Gaussians."
        ],
        [
            "And the basic idea of this reduction is this probabilistic technique of personalization.",
            "So if you have a distribution over discrete set and you generate a pass on random variable R. And you draw our samples from the set.",
            "You'll have.",
            "A mixture, a discrete mixture where each component is.",
            "Selected a person number of times.",
            "With rate Lambda times its weight in the original discrete distribution.",
            "And this will be illustrated shortly.",
            "But if you're set, X is a set of vectors and you draw the samples and you add them together, you can write it as y = a S, where a is just the matrix of X&S.",
            "Is this vector where each coordinate counts the number of times each vector is selected."
        ],
        [
            "So this is a nice bridge to Gaussian mixture model in the following way you start with a mixture.",
            "You generate your posts on random variable, generate R samples from the mixture, add them together and you get something that looks like this.",
            "Once you have this, you can choose a threshold two you can see cut off the sample so you reject, say all the samples above this threshold.",
            "And then add noise to this.",
            "Equal to.",
            "The value at this threshold to normalize out the Gaussian.",
            "So there are the same covariance essentially, and this is where you need to know the covariance and it has to be the same for all of them as you see here.",
            "And this is approximately from a product distribution.",
            "And so this gives you exactly why it goes S plus ETA in the ICA model and.",
            "You can use any ICA algorithm to learn it.",
            "If the ICA ICA algorithm has your polynomial time guarantees and specifically we use the algorithm of a coil compellent shell.",
            "So."
        ],
        [
            "In the end.",
            "We show that in high dimension.",
            "You can learn efficiently very large mixtures of Gaussians polynomial.",
            "And in low dimension it fails generically.",
            "Which in open sort of question is defined more instances of this and prove it really where dimensionality reduction can in fact hurt your ability to solve the problem efficiently.",
            "And.",
            "As we mentioned earlier, the reduction that GMM reduces to ICA means that ICA has this low dimensional restriction as well.",
            "And if there are no questions, well, any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, I'm Joe Anderson and I'll be presenting our paper called the More the merrier.",
                    "label": 0
                },
                {
                    "sent": "The blessing of dimensionality for learning large Gaussian mixtures.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Mikhail Belkin, Navin Goyal, Louise Rademacher, and James Voss and Louise and James are with us today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To understand the main message of our paper.",
                    "label": 0
                },
                {
                    "sent": "We first need to discuss the curse of dimensionality and it's a common obstacle in modern computational applications that the problems intrinsically become very difficult as the dimension decreases, and this is more than just like.",
                    "label": 1
                },
                {
                    "sent": "Decreasing because the dimension is increasing, but like quantum jumps in difficulty.",
                    "label": 0
                },
                {
                    "sent": "So a common approach when better tools aren't available.",
                    "label": 1
                },
                {
                    "sent": "Is to preprocess your data and this is often to reduce the dimension.",
                    "label": 1
                },
                {
                    "sent": "And you can do some sort of analysis on the lower dimensional data and propagate the results back to your original.",
                    "label": 0
                },
                {
                    "sent": "Situation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the blessing of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "What we call an unusual phenomenon because we show that there are problems that become much easier as the ambient dimension of your data increases, and in this sense, dimensionality reduction could be conservative, considered harmful to your ability to efficiently solve a problem.",
                    "label": 0
                },
                {
                    "sent": "And anytime we say efficient, we will mean learnable in polynomial time and say one over your probability of error etc.",
                    "label": 1
                },
                {
                    "sent": "And specifically in this talk we will discuss the parameter estimation for a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "And show that this is generically hard in low dimension and easy in high dimension, and we'll discuss what that means later and there is in fact an exponential gap between the two.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is learning a Gaussian mixture model is the situation where you're given samples with a density which is the weighted average of several N dimensional Gaussian densities, each with its own mean vector covariance matrix and await.",
                    "label": 0
                },
                {
                    "sent": "And when you draw a sample from this model, you get a sample from one of the Gaussians and we have these here.",
                    "label": 0
                },
                {
                    "sent": "These actually have.",
                    "label": 0
                },
                {
                    "sent": "This is Mew, Mew and mew.",
                    "label": 0
                },
                {
                    "sent": "You can see this too, but so we have three Gaussians and when you draw sample you get a point from one of them, but you don't know which one, and so your goal is to estimate the weights.",
                    "label": 0
                },
                {
                    "sent": "The means in the covariances.",
                    "label": 0
                },
                {
                    "sent": "And so specifically, we'll discuss a blessing of dimensionality for the case of recovery of just the means in the weights, even.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So low dimensional hardness is the first sort of half of the blessing.",
                    "label": 0
                },
                {
                    "sent": "And this is that.",
                    "label": 0
                },
                {
                    "sent": "In fact, in low dimensions, your ability to efficiently estimate a Gaussian mixture fails generically, which is to say for dense enough sets.",
                    "label": 1
                },
                {
                    "sent": "There are two Gaussian mixtures over those sets, so centered on each point.",
                    "label": 0
                },
                {
                    "sent": "So that the distributions are very close.",
                    "label": 1
                },
                {
                    "sent": "But yet very different in parameter space.",
                    "label": 1
                },
                {
                    "sent": "So you can't distinguish the two.",
                    "label": 1
                },
                {
                    "sent": "And this lower bound will be applied through a reduction.",
                    "label": 0
                },
                {
                    "sent": "And will be related to the problem of independent component analysis.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the low dimensional hardness theorem looks like this, and it might seem rather cumbersome, but the main message is just like on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "If you take a square points from the end dimensional hypercube with high probability, you can construct two disjoint subsets of these K square points and two Gaussian mixtures over them that are exponentially close in total variation distance.",
                    "label": 1
                },
                {
                    "sent": "That's this guy here and they'll be exponentially close in K over log K. To the 1 / N, so there's a lot there you can sort of ignore the log and 1st consider the case N = 1.",
                    "label": 0
                },
                {
                    "sent": "So in low dimension your exponential inverse exponential in K. But as your dimension increases you can see how this degrades.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "So you have very close statistical distributions, but the means are going to be well separated by your choice of these two disjoint subsets.",
                    "label": 1
                },
                {
                    "sent": "And the implication of this is just the fact that any algorithm will require an inverse number in the total variation distance.",
                    "label": 0
                },
                {
                    "sent": "Just to distinguish these two distributions.",
                    "label": 1
                },
                {
                    "sent": "So you're sort of in a hopeless situation for efficient learnability.",
                    "label": 0
                },
                {
                    "sent": "The technique used to prove this just to.",
                    "label": 0
                },
                {
                    "sent": "Sort of give you the taste without any more details is that using properties of reproducing kernel Hilbert spaces to bound the differences between smooth functions and their Gaussian kernel interpolants, which essentially construct you Gaussian mixtures, and then you can choose your points carefully enough that you get equal equal size sets of means.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other half of the blessing is well, remember that it's hard and low dimension and easy in high dimension.",
                    "label": 0
                },
                {
                    "sent": "And so for a fixed P you can learn order N to the P. Gaussian means in a Gaussian mixture.",
                    "label": 0
                },
                {
                    "sent": "Using smooth polynomial time and sample size will talk a little more about that that, but the theorem is as follows.",
                    "label": 1
                },
                {
                    "sent": "If each Gaussian is the same covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "And you know it beforehand, you can estimate the means using sample and time that's polynomial and the dimension number of components inverse in this conditioning parameter S. And the other natural parameters to the problem.",
                    "label": 1
                },
                {
                    "sent": "So what does this 1 / S?",
                    "label": 0
                },
                {
                    "sent": "It's a generalization of the smaller singular value of the matrix of means in the mixture.",
                    "label": 1
                },
                {
                    "sent": "And we'll talk a little bit more about that as well.",
                    "label": 0
                },
                {
                    "sent": "And the fact that.",
                    "label": 0
                },
                {
                    "sent": "When we say it's easy, generically, is that 1 / S. Is generically not too large in high enough dimension.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the more precise way in the case where your fixed P = 2.",
                    "label": 1
                },
                {
                    "sent": "If you want to learn N square Gaussians in Dimension N, the probability so.",
                    "label": 0
                },
                {
                    "sent": "Your set of means can be in the smooth sense.",
                    "label": 0
                },
                {
                    "sent": "You perturb the setting means, so this is any set of means and you add a perturbation matrix E which is just N by K. Here's some order in square.",
                    "label": 0
                },
                {
                    "sent": "You add this to M and the probability that the condition number is low is some inverse polynomial in the dimension.",
                    "label": 0
                },
                {
                    "sent": "So as the dimension increases, your condition number generically allows you to learn the mixture efficiently.",
                    "label": 0
                },
                {
                    "sent": "And this was mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "But and if you know what it is, S is actually the case.",
                    "label": 0
                },
                {
                    "sent": "Singular value of the second catcher out power.",
                    "label": 1
                },
                {
                    "sent": "So yeah, you perturb and the probability of the second catcher or power is small.",
                    "label": 0
                },
                {
                    "sent": "Is inverse polynomial?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summarize the blessing as such, is you really have to keep the two fold in mind that it's hard and low dimension and easy in high dimension, so it's sort of a blessing.",
                    "label": 0
                },
                {
                    "sent": "That is only a blessing when your dimension is high enough.",
                    "label": 1
                },
                {
                    "sent": "And it is still hard generically in low dimension.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some previous work on Gaussian mixture, specifically.",
                    "label": 0
                },
                {
                    "sent": "Of course, Gaussian mixtures have a very historic significance in statistical community, but the first algorithms in computer science with full sample and time analysis was done by Descoped in 1999 and which used a projection to lower dimension, which we alluded to earlier.",
                    "label": 0
                },
                {
                    "sent": "And required a separation condition on the means.",
                    "label": 0
                },
                {
                    "sent": "So once this work was done, there was a entire sequence of work using similar projection techniques and whose goal was to reduce this separation assumption.",
                    "label": 0
                },
                {
                    "sent": "And finally, in 2010 the work of Belkin and Sinha and Moitra Valiant both gave polynomial time algorithms to learn Gaussian mixtures for fixed number of components.",
                    "label": 1
                },
                {
                    "sent": "And they also use lower dimensional projections and Additionally, motor and Valiant gave in.",
                    "label": 0
                },
                {
                    "sent": "21 dimensional mixtures which were exponentially cost close in distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is as you asked earlier, you're very close in distribution, but the parameters you're learning are actually very different.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to do some sort of inference on those means, you get the wrong sort of answer.",
                    "label": 0
                },
                {
                    "sent": "So this showed an exponential dependence was necessary in one dimension.",
                    "label": 0
                },
                {
                    "sent": "But in 2012 soon Kokota gave a surprising result where they are able to learn Dega D Gaussians in D dimensions.",
                    "label": 0
                },
                {
                    "sent": "Officially, without separation assumptions, but instead their complexity is related to the smallest singular value of the matrix of means.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are simultaneous work by bus, garage Charikar Moitra Envision Raghavan.",
                    "label": 1
                },
                {
                    "sent": "In this year's talk, which was done independently to learn large Gaussian mixtures, it in high dimension and they used to smooth analysis for particular decomposition.",
                    "label": 0
                },
                {
                    "sent": "Algorithm and their work handles unknown access line covariance which has a slightly higher running time and but a little lower probability of error ultimately.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd like to discuss now the blessing of dimensionality was ultimately a side effect of our original attempt to learn Gaussian mixture models, and the relation came from this independent component analysis problem, which has a long history in the machine learning signal processing community, and the model is as follows.",
                    "label": 1
                },
                {
                    "sent": "You get observations of a vector S, which is a matrix a times a signal S plus some noise.",
                    "label": 0
                },
                {
                    "sent": "And you typically assume Ada.",
                    "label": 0
                },
                {
                    "sent": "The noise is Gaussian and S has independent coordinates.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to recover your mixing matrix.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "Learn a Gaussian mixture with an ICA algorithm.",
                    "label": 0
                },
                {
                    "sent": "When you know the covariance matrix beforehand and it's the same for the Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the basic idea of this reduction is this probabilistic technique of personalization.",
                    "label": 1
                },
                {
                    "sent": "So if you have a distribution over discrete set and you generate a pass on random variable R. And you draw our samples from the set.",
                    "label": 1
                },
                {
                    "sent": "You'll have.",
                    "label": 0
                },
                {
                    "sent": "A mixture, a discrete mixture where each component is.",
                    "label": 0
                },
                {
                    "sent": "Selected a person number of times.",
                    "label": 0
                },
                {
                    "sent": "With rate Lambda times its weight in the original discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "And this will be illustrated shortly.",
                    "label": 0
                },
                {
                    "sent": "But if you're set, X is a set of vectors and you draw the samples and you add them together, you can write it as y = a S, where a is just the matrix of X&S.",
                    "label": 1
                },
                {
                    "sent": "Is this vector where each coordinate counts the number of times each vector is selected.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a nice bridge to Gaussian mixture model in the following way you start with a mixture.",
                    "label": 0
                },
                {
                    "sent": "You generate your posts on random variable, generate R samples from the mixture, add them together and you get something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Once you have this, you can choose a threshold two you can see cut off the sample so you reject, say all the samples above this threshold.",
                    "label": 0
                },
                {
                    "sent": "And then add noise to this.",
                    "label": 1
                },
                {
                    "sent": "Equal to.",
                    "label": 0
                },
                {
                    "sent": "The value at this threshold to normalize out the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So there are the same covariance essentially, and this is where you need to know the covariance and it has to be the same for all of them as you see here.",
                    "label": 0
                },
                {
                    "sent": "And this is approximately from a product distribution.",
                    "label": 1
                },
                {
                    "sent": "And so this gives you exactly why it goes S plus ETA in the ICA model and.",
                    "label": 0
                },
                {
                    "sent": "You can use any ICA algorithm to learn it.",
                    "label": 0
                },
                {
                    "sent": "If the ICA ICA algorithm has your polynomial time guarantees and specifically we use the algorithm of a coil compellent shell.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the end.",
                    "label": 0
                },
                {
                    "sent": "We show that in high dimension.",
                    "label": 1
                },
                {
                    "sent": "You can learn efficiently very large mixtures of Gaussians polynomial.",
                    "label": 1
                },
                {
                    "sent": "And in low dimension it fails generically.",
                    "label": 1
                },
                {
                    "sent": "Which in open sort of question is defined more instances of this and prove it really where dimensionality reduction can in fact hurt your ability to solve the problem efficiently.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As we mentioned earlier, the reduction that GMM reduces to ICA means that ICA has this low dimensional restriction as well.",
                    "label": 0
                },
                {
                    "sent": "And if there are no questions, well, any questions.",
                    "label": 0
                }
            ]
        }
    }
}