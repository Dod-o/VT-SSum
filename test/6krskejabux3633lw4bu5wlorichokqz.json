{
    "id": "6krskejabux3633lw4bu5wlorichokqz",
    "title": "Reinforcement Learning in Decentralized Stochastic Control Systems with Partial History Sharing",
    "info": {
        "author": [
            "Jalal Arabneydi, School of Computer Science, McGill University"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_arabneydi_history_sharing/",
    "segmentation": [
        [
            "Hello everyone, my name is Arab media and today I'm going to talk about my recent new work joint with my Prophet, sorority Amazon on Reinforcement learning in Multi Agent system with partially story sharing and this work has been accepted in American control conference and it contains lots of technicality but for the matter of this talk or just skip all of them, I'll just give you the high level idea."
        ],
        [
            "So in this talk are interested in systems with multi agent systems where agents would like to collaborate with each other to accomplish a common task awhile.",
            "First they do they have different information?",
            "That is, they have decentralized information, plus they do not know the complete model of the system.",
            "They either know it partially or do not know it at all.",
            "So multi agent system arise in many applications and the basic advantages of them or.",
            "As follows, they distribute the computational resources and capacities they provide maintainability, flexibility, and robustness.",
            "An implementation wise.",
            "Sometimes there are efficient.",
            "There are efficient physically an econonmic Lee.",
            "But there."
        ],
        [
            "Is a big challenge here, and that challenge is coming from the decentralized nature of the problem that causes discrepancy in perspectives.",
            "The discrepancy in perspective makes establishing compression among agents are conceptually challenging, even in the model known case.",
            "In general, these problems belong to next complex could be class.",
            "So as you can imagine, finding the solution when the model is partially known or not known at all would be harder."
        ],
        [
            "So here I will formulate the problem of multi agent system as follows.",
            "We have an environment that evolved in a Markovian manner as T we just get this as T. Is there state of environment an its evolution is influenced by the action of each agent and agents and agents would get some feedback of informations denoted by I and at each time.",
            "Each agent has observation or it."
        ],
        [
            "And so each agent Maps decide based on a control law which Maps the available information to its action in space, and this mapping is called control law and we call all the collection of all these control log control strategy.",
            "And we assume that agents observed immediate reward.",
            "And because an immediate reward is a function of a state of the environment and actions, we consider Infinite Horizon.",
            "Discounted case an or main objective is to develop a model based or model free reinforcement learning algorithm that guarantees epsilon optimal strategy.",
            "An means that we're looking for a strategy.",
            "A star such that its performance is with an Epsilon neighborhood of.",
            "Optimal performance."
        ],
        [
            "So in this work we restrict attention to a large class of multi agent system that is called partially story sharing.",
            "Basically, says that consider the information of agent.",
            "I can split it in two terms, Siti an MIT.",
            "The See T is the common information, that is the information shared between all agents.",
            "This is the common knowledge and the rest would be just local information.",
            "When it.",
            "Three properties A1 says that the common formation has to be nested.",
            "It means the common knowledge will not be destroyed.",
            "It will just continue.",
            "It will be increasing means that the next, information is the current common formation palazeti, which is the common observation at time T. The second property is that the local information is a function of current.",
            "Local information, current actions and next observation.",
            "And the size of the size of the common observation and the local information are uniformly bounded in time.",
            "These assumptions are very mild and a large class of problems satisfy these.",
            "Properties such as delayed sharing, control, sharing, mean field sharing, etc.",
            "And note that the common information is allowed to be empty, set here."
        ],
        [
            "So our methodology has two main step in a first, so we have the multi agent system in the first step we do the common information approach and convert the multi agent system to an equivalent centralized mom DP.",
            "And the rest in the second step, we need to find reinforcement learning algorithm for palm DP and any palm DP for any reinforcement learning for palm DP would work from here onwards but we have our own algorithm."
        ],
        [
            "Basically, the dotted version is.",
            "It is the second step.",
            "This is also a novel approach for reinforcement learning and palm DP, so there's just to capture the salient feature of our approaches that we guarantee epsilon optimality or class of problem compasses a large class of multi agent system various palm DP.",
            "Our algorithm may be used in the second step, an approach that we use for the second step is a novel approach.",
            "Basically we introduce an.",
            "New notion that we call incrementally expanding representation, using which we convert the palm DP to countable asset MDP, then approximate accountable SMTP with finite state MDP and then we can use any generic finalist air enforcement learning algorithm on that MVP and get that epsilon optimal path."
        ],
        [
            "So what is the common formation approach?",
            "The common formation approach says that OK.",
            "Consider a virtual coordinator that.",
            "That observed the comment in formation part and prescribes partial function that Maps the local information of each agent to action space.",
            "Basically the prescription of Agent I tells Agent I what to do, which it's all local information and the coordinator job is to find these prescription.",
            "So here the PSI is the coordinator strategy and beta one to beta an order prescription.",
            "For simplicity, we just denote them by by beta.",
            "And this is and it is shown to be the equivalent central US palm DP and total expected reward would be in this format."
        ],
        [
            "So like other palm DP, so let our to be the reachable subsets of this palm DP with action beta.",
            "From now on we are in the coordinated system in this.",
            "Which is surrounded by this red lines we are working in this asset up in this set up the action are our beta or prescriptions and observation is zed.",
            "Once we find a solution we translate the problem back to original model."
        ],
        [
            "So in a second step, so we so we propose an approximate palm DP.",
            "Our algorithm.",
            "Basically we introduce a new notion, incrementally expanding representation short IR.",
            "It's a three tuple and it is like an onion with the layers and the first term says that the X and want to Infinity is a sequence of finite expanding sets.",
            "As you can see Anna 2nd.",
            "Property says that for any action beta, an for any observations Ed.",
            "If X is in the layer NTH, the at most it can go to the to the N plus one layer.",
            "It cannot jump to the further layers.",
            "An app till the F is a dynamic through these layers and we need a surjective function B that Maps X the reachable set for the.",
            "Distance a purposes, and the model known case.",
            "We typically don't need lemma one, but here because we are working an partial known case or unknown case, we need lemma one which says that for every multi agent system with partially story sharing, there exists at least one rar whose X until the F do not depend on unknowns, but B may depend on."
        ],
        [
            "So now we construct accountable as that MVP simply by status state.",
            "SpaceX Action Space G, which is the space of all prescriptions a dynamic still the F and reward which is defined using this subjective function B and then we can truncate the whole like all these layers and the answer stab an call it the college Approximate Finalist MVP at North Layer.",
            "And so here we need to make sure that all transition probabilities, or in a way that they constitute valid probability distribution, and then we can use any generic finalists.",
            "Our algorithm on this MDP, and we assume that the data the generic can be just Q learning any other finalists RL kind of edges to optimal strategy of other science.",
            "And once we do this, we translate all the solution back to original.",
            "Multi agent system.",
            "Our main theorem says that let JS star be the optimal performance of the original multi Agent system until the JB.",
            "The performance under the Learning Strategy and their distance is less than epsilon N where epsilon N is bounded by this exponential terms."
        ],
        [
            "So here's an algorithm.",
            "So the very good advantage of our work is that because our approach is based on the common information, Now every agent can run this algorithm in independently in a decentralized manner.",
            "So let's see what happens at first step.",
            "Given epsilon, they choose agents, choose an such that this inequality happens.",
            "Then they went North is decided.",
            "So they construct Delta N, particularly estate XN and dynamic still laugh.",
            "At iteration K, the generic algorithm like let's say Q learning chooses prescriptions.",
            "The joint prescriptions an.",
            "I need to mention that all agents have access to common random generator in order to consistently Xplorer system.",
            "And based on the chosen prescription, now each agent's given its own local information takes an action.",
            "Based on the text taken action as the system or environment evolves, it occurs on incurs reward R and generates knew observation.",
            "Apart of observation goes to local information and a part of that.",
            "As it K is a common knowledge and everyone observes that so everyone can consistently compute the next state as follows and then.",
            "And four and four step.",
            "So the Q learning, let's say learn the coordinated strategy based on observing reward by performing prescription beta at estate SKN translating to status cable, asthma and we keep doing this until the IRL converges."
        ],
        [
            "So here is an example.",
            "So here at this example is Multiaccess Broadcasting channel is a benchmark example.",
            "In this in the centralized control systems it says we have two users.",
            "H has a buffer of size 1 and there is a packets arrive at each user with rate independent Bernoulli process, right PQ, P1P2, let's say, and the objective is to transmit the packet.",
            "Through a shared channel, if they both have packets and transmit, there will be a collision and the Packers remain will remain in the packets.",
            "So now and it's set up that we do not know the rate so that the model is partially partially known.",
            "We do not know the rates an the user has decentralizing formation.",
            "Each agent I does not know the state of the other agents and the common information here is they know the history of the action that has been taken by both of them.",
            "And so the objective is to maximize the throughput."
        ],
        [
            "So by performing the common information approach at the first step, we get an equivalent centralized palm DP.",
            "Here is that reachable set and you can see the oldest state depends on the rates.",
            "There are model dependent by using the notion of IR will remove that dependency and we get a countable set.",
            "MDP here B1 and B2 do not their arbitrary number.",
            "And so in this space, now in MDP we can do the planning now because it does not depend on the known unknowns.",
            "So."
        ],
        [
            "Here as a result, and this shows that learning processes and this is specific example, the optimal solution is a recurrent class.",
            "There are red lines and it says that the probability rate of.",
            "The rate of arrival of 1 user is 0.3 and the other one is 0.6.",
            "Even though this one is just twice then this the other one, the optimal strategy says the one with the higher probability has to transmit three times more than the other, so we will get much more complicated solution when we have lower probability where.",
            "It's worth the risk to transfer.",
            "Transmit simultaneously is worth the risk of collision."
        ],
        [
            "So, and this is summary, so we've given up silent represented the model based or model free reinforcement learning.",
            "That guarantees epsilon optimal before large class of multi agent system with partial story sharing.",
            "Or approach has 2 minus the common information approach plus any palm DP IRL.",
            "And for this part Depomed Aprl, we provide a novel approach for model known and unknown model.",
            "And we.",
            "We developed a multi agent Q learning for them ABC and the obtain error is actually a little bit conservative and in practice the actual error is less than this."
        ],
        [
            "So Anne thank you and this is the paper accepted in American Control conference.",
            "How does the complexity change as the number of agents increases in the system?",
            "And that's a good question actually.",
            "So this approach is a very general approach and it works for all of this.",
            "Almost all of this centralized control system, and worst case scenario, where will have next, even in the model known case we have next, there exists like class of the centralized control problem.",
            "Where the computational complexity is very tractable, like actually recently like a year ago.",
            "It's a new work again by myself and my officer.",
            "We introduced a class of problem with non classical information structure and whose computational complexity is polynomial and the number of agents.",
            "So we have centralized control problem for which the computational complexity is tractable."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Arab media and today I'm going to talk about my recent new work joint with my Prophet, sorority Amazon on Reinforcement learning in Multi Agent system with partially story sharing and this work has been accepted in American control conference and it contains lots of technicality but for the matter of this talk or just skip all of them, I'll just give you the high level idea.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk are interested in systems with multi agent systems where agents would like to collaborate with each other to accomplish a common task awhile.",
                    "label": 1
                },
                {
                    "sent": "First they do they have different information?",
                    "label": 0
                },
                {
                    "sent": "That is, they have decentralized information, plus they do not know the complete model of the system.",
                    "label": 1
                },
                {
                    "sent": "They either know it partially or do not know it at all.",
                    "label": 1
                },
                {
                    "sent": "So multi agent system arise in many applications and the basic advantages of them or.",
                    "label": 0
                },
                {
                    "sent": "As follows, they distribute the computational resources and capacities they provide maintainability, flexibility, and robustness.",
                    "label": 0
                },
                {
                    "sent": "An implementation wise.",
                    "label": 0
                },
                {
                    "sent": "Sometimes there are efficient.",
                    "label": 0
                },
                {
                    "sent": "There are efficient physically an econonmic Lee.",
                    "label": 0
                },
                {
                    "sent": "But there.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a big challenge here, and that challenge is coming from the decentralized nature of the problem that causes discrepancy in perspectives.",
                    "label": 0
                },
                {
                    "sent": "The discrepancy in perspective makes establishing compression among agents are conceptually challenging, even in the model known case.",
                    "label": 1
                },
                {
                    "sent": "In general, these problems belong to next complex could be class.",
                    "label": 1
                },
                {
                    "sent": "So as you can imagine, finding the solution when the model is partially known or not known at all would be harder.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I will formulate the problem of multi agent system as follows.",
                    "label": 0
                },
                {
                    "sent": "We have an environment that evolved in a Markovian manner as T we just get this as T. Is there state of environment an its evolution is influenced by the action of each agent and agents and agents would get some feedback of informations denoted by I and at each time.",
                    "label": 0
                },
                {
                    "sent": "Each agent has observation or it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so each agent Maps decide based on a control law which Maps the available information to its action in space, and this mapping is called control law and we call all the collection of all these control log control strategy.",
                    "label": 1
                },
                {
                    "sent": "And we assume that agents observed immediate reward.",
                    "label": 1
                },
                {
                    "sent": "And because an immediate reward is a function of a state of the environment and actions, we consider Infinite Horizon.",
                    "label": 0
                },
                {
                    "sent": "Discounted case an or main objective is to develop a model based or model free reinforcement learning algorithm that guarantees epsilon optimal strategy.",
                    "label": 1
                },
                {
                    "sent": "An means that we're looking for a strategy.",
                    "label": 0
                },
                {
                    "sent": "A star such that its performance is with an Epsilon neighborhood of.",
                    "label": 0
                },
                {
                    "sent": "Optimal performance.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we restrict attention to a large class of multi agent system that is called partially story sharing.",
                    "label": 0
                },
                {
                    "sent": "Basically, says that consider the information of agent.",
                    "label": 0
                },
                {
                    "sent": "I can split it in two terms, Siti an MIT.",
                    "label": 0
                },
                {
                    "sent": "The See T is the common information, that is the information shared between all agents.",
                    "label": 0
                },
                {
                    "sent": "This is the common knowledge and the rest would be just local information.",
                    "label": 0
                },
                {
                    "sent": "When it.",
                    "label": 0
                },
                {
                    "sent": "Three properties A1 says that the common formation has to be nested.",
                    "label": 0
                },
                {
                    "sent": "It means the common knowledge will not be destroyed.",
                    "label": 0
                },
                {
                    "sent": "It will just continue.",
                    "label": 0
                },
                {
                    "sent": "It will be increasing means that the next, information is the current common formation palazeti, which is the common observation at time T. The second property is that the local information is a function of current.",
                    "label": 0
                },
                {
                    "sent": "Local information, current actions and next observation.",
                    "label": 1
                },
                {
                    "sent": "And the size of the size of the common observation and the local information are uniformly bounded in time.",
                    "label": 1
                },
                {
                    "sent": "These assumptions are very mild and a large class of problems satisfy these.",
                    "label": 1
                },
                {
                    "sent": "Properties such as delayed sharing, control, sharing, mean field sharing, etc.",
                    "label": 1
                },
                {
                    "sent": "And note that the common information is allowed to be empty, set here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our methodology has two main step in a first, so we have the multi agent system in the first step we do the common information approach and convert the multi agent system to an equivalent centralized mom DP.",
                    "label": 0
                },
                {
                    "sent": "And the rest in the second step, we need to find reinforcement learning algorithm for palm DP and any palm DP for any reinforcement learning for palm DP would work from here onwards but we have our own algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, the dotted version is.",
                    "label": 0
                },
                {
                    "sent": "It is the second step.",
                    "label": 0
                },
                {
                    "sent": "This is also a novel approach for reinforcement learning and palm DP, so there's just to capture the salient feature of our approaches that we guarantee epsilon optimality or class of problem compasses a large class of multi agent system various palm DP.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm may be used in the second step, an approach that we use for the second step is a novel approach.",
                    "label": 1
                },
                {
                    "sent": "Basically we introduce an.",
                    "label": 0
                },
                {
                    "sent": "New notion that we call incrementally expanding representation, using which we convert the palm DP to countable asset MDP, then approximate accountable SMTP with finite state MDP and then we can use any generic finalist air enforcement learning algorithm on that MVP and get that epsilon optimal path.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the common formation approach?",
                    "label": 0
                },
                {
                    "sent": "The common formation approach says that OK.",
                    "label": 0
                },
                {
                    "sent": "Consider a virtual coordinator that.",
                    "label": 1
                },
                {
                    "sent": "That observed the comment in formation part and prescribes partial function that Maps the local information of each agent to action space.",
                    "label": 1
                },
                {
                    "sent": "Basically the prescription of Agent I tells Agent I what to do, which it's all local information and the coordinator job is to find these prescription.",
                    "label": 0
                },
                {
                    "sent": "So here the PSI is the coordinator strategy and beta one to beta an order prescription.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, we just denote them by by beta.",
                    "label": 1
                },
                {
                    "sent": "And this is and it is shown to be the equivalent central US palm DP and total expected reward would be in this format.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So like other palm DP, so let our to be the reachable subsets of this palm DP with action beta.",
                    "label": 1
                },
                {
                    "sent": "From now on we are in the coordinated system in this.",
                    "label": 0
                },
                {
                    "sent": "Which is surrounded by this red lines we are working in this asset up in this set up the action are our beta or prescriptions and observation is zed.",
                    "label": 0
                },
                {
                    "sent": "Once we find a solution we translate the problem back to original model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a second step, so we so we propose an approximate palm DP.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically we introduce a new notion, incrementally expanding representation short IR.",
                    "label": 0
                },
                {
                    "sent": "It's a three tuple and it is like an onion with the layers and the first term says that the X and want to Infinity is a sequence of finite expanding sets.",
                    "label": 1
                },
                {
                    "sent": "As you can see Anna 2nd.",
                    "label": 0
                },
                {
                    "sent": "Property says that for any action beta, an for any observations Ed.",
                    "label": 0
                },
                {
                    "sent": "If X is in the layer NTH, the at most it can go to the to the N plus one layer.",
                    "label": 0
                },
                {
                    "sent": "It cannot jump to the further layers.",
                    "label": 0
                },
                {
                    "sent": "An app till the F is a dynamic through these layers and we need a surjective function B that Maps X the reachable set for the.",
                    "label": 1
                },
                {
                    "sent": "Distance a purposes, and the model known case.",
                    "label": 0
                },
                {
                    "sent": "We typically don't need lemma one, but here because we are working an partial known case or unknown case, we need lemma one which says that for every multi agent system with partially story sharing, there exists at least one rar whose X until the F do not depend on unknowns, but B may depend on.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we construct accountable as that MVP simply by status state.",
                    "label": 0
                },
                {
                    "sent": "SpaceX Action Space G, which is the space of all prescriptions a dynamic still the F and reward which is defined using this subjective function B and then we can truncate the whole like all these layers and the answer stab an call it the college Approximate Finalist MVP at North Layer.",
                    "label": 1
                },
                {
                    "sent": "And so here we need to make sure that all transition probabilities, or in a way that they constitute valid probability distribution, and then we can use any generic finalists.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm on this MDP, and we assume that the data the generic can be just Q learning any other finalists RL kind of edges to optimal strategy of other science.",
                    "label": 1
                },
                {
                    "sent": "And once we do this, we translate all the solution back to original.",
                    "label": 0
                },
                {
                    "sent": "Multi agent system.",
                    "label": 0
                },
                {
                    "sent": "Our main theorem says that let JS star be the optimal performance of the original multi Agent system until the JB.",
                    "label": 1
                },
                {
                    "sent": "The performance under the Learning Strategy and their distance is less than epsilon N where epsilon N is bounded by this exponential terms.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the very good advantage of our work is that because our approach is based on the common information, Now every agent can run this algorithm in independently in a decentralized manner.",
                    "label": 0
                },
                {
                    "sent": "So let's see what happens at first step.",
                    "label": 0
                },
                {
                    "sent": "Given epsilon, they choose agents, choose an such that this inequality happens.",
                    "label": 0
                },
                {
                    "sent": "Then they went North is decided.",
                    "label": 0
                },
                {
                    "sent": "So they construct Delta N, particularly estate XN and dynamic still laugh.",
                    "label": 0
                },
                {
                    "sent": "At iteration K, the generic algorithm like let's say Q learning chooses prescriptions.",
                    "label": 1
                },
                {
                    "sent": "The joint prescriptions an.",
                    "label": 0
                },
                {
                    "sent": "I need to mention that all agents have access to common random generator in order to consistently Xplorer system.",
                    "label": 1
                },
                {
                    "sent": "And based on the chosen prescription, now each agent's given its own local information takes an action.",
                    "label": 0
                },
                {
                    "sent": "Based on the text taken action as the system or environment evolves, it occurs on incurs reward R and generates knew observation.",
                    "label": 0
                },
                {
                    "sent": "Apart of observation goes to local information and a part of that.",
                    "label": 1
                },
                {
                    "sent": "As it K is a common knowledge and everyone observes that so everyone can consistently compute the next state as follows and then.",
                    "label": 0
                },
                {
                    "sent": "And four and four step.",
                    "label": 0
                },
                {
                    "sent": "So the Q learning, let's say learn the coordinated strategy based on observing reward by performing prescription beta at estate SKN translating to status cable, asthma and we keep doing this until the IRL converges.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "So here at this example is Multiaccess Broadcasting channel is a benchmark example.",
                    "label": 1
                },
                {
                    "sent": "In this in the centralized control systems it says we have two users.",
                    "label": 0
                },
                {
                    "sent": "H has a buffer of size 1 and there is a packets arrive at each user with rate independent Bernoulli process, right PQ, P1P2, let's say, and the objective is to transmit the packet.",
                    "label": 1
                },
                {
                    "sent": "Through a shared channel, if they both have packets and transmit, there will be a collision and the Packers remain will remain in the packets.",
                    "label": 0
                },
                {
                    "sent": "So now and it's set up that we do not know the rate so that the model is partially partially known.",
                    "label": 0
                },
                {
                    "sent": "We do not know the rates an the user has decentralizing formation.",
                    "label": 0
                },
                {
                    "sent": "Each agent I does not know the state of the other agents and the common information here is they know the history of the action that has been taken by both of them.",
                    "label": 1
                },
                {
                    "sent": "And so the objective is to maximize the throughput.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by performing the common information approach at the first step, we get an equivalent centralized palm DP.",
                    "label": 0
                },
                {
                    "sent": "Here is that reachable set and you can see the oldest state depends on the rates.",
                    "label": 1
                },
                {
                    "sent": "There are model dependent by using the notion of IR will remove that dependency and we get a countable set.",
                    "label": 0
                },
                {
                    "sent": "MDP here B1 and B2 do not their arbitrary number.",
                    "label": 0
                },
                {
                    "sent": "And so in this space, now in MDP we can do the planning now because it does not depend on the known unknowns.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here as a result, and this shows that learning processes and this is specific example, the optimal solution is a recurrent class.",
                    "label": 0
                },
                {
                    "sent": "There are red lines and it says that the probability rate of.",
                    "label": 0
                },
                {
                    "sent": "The rate of arrival of 1 user is 0.3 and the other one is 0.6.",
                    "label": 0
                },
                {
                    "sent": "Even though this one is just twice then this the other one, the optimal strategy says the one with the higher probability has to transmit three times more than the other, so we will get much more complicated solution when we have lower probability where.",
                    "label": 0
                },
                {
                    "sent": "It's worth the risk to transfer.",
                    "label": 0
                },
                {
                    "sent": "Transmit simultaneously is worth the risk of collision.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, and this is summary, so we've given up silent represented the model based or model free reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That guarantees epsilon optimal before large class of multi agent system with partial story sharing.",
                    "label": 1
                },
                {
                    "sent": "Or approach has 2 minus the common information approach plus any palm DP IRL.",
                    "label": 0
                },
                {
                    "sent": "And for this part Depomed Aprl, we provide a novel approach for model known and unknown model.",
                    "label": 1
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "We developed a multi agent Q learning for them ABC and the obtain error is actually a little bit conservative and in practice the actual error is less than this.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Anne thank you and this is the paper accepted in American Control conference.",
                    "label": 1
                },
                {
                    "sent": "How does the complexity change as the number of agents increases in the system?",
                    "label": 0
                },
                {
                    "sent": "And that's a good question actually.",
                    "label": 0
                },
                {
                    "sent": "So this approach is a very general approach and it works for all of this.",
                    "label": 0
                },
                {
                    "sent": "Almost all of this centralized control system, and worst case scenario, where will have next, even in the model known case we have next, there exists like class of the centralized control problem.",
                    "label": 0
                },
                {
                    "sent": "Where the computational complexity is very tractable, like actually recently like a year ago.",
                    "label": 0
                },
                {
                    "sent": "It's a new work again by myself and my officer.",
                    "label": 0
                },
                {
                    "sent": "We introduced a class of problem with non classical information structure and whose computational complexity is polynomial and the number of agents.",
                    "label": 0
                },
                {
                    "sent": "So we have centralized control problem for which the computational complexity is tractable.",
                    "label": 0
                }
            ]
        }
    }
}